<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#30740;&#31350;&#22914;&#20309;&#37327;&#21270;DNN&#30340;&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#19981;&#21516;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10527</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#32500;&#24230;&#19981;&#30830;&#23450;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multidimensional Uncertainty Quantification for Deep Neural Networks. (arXiv:2304.10527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10527
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22914;&#20309;&#37327;&#21270;DNN&#30340;&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#19981;&#21516;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#26512;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#33647;&#29289;&#30740;&#31350;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#19981;&#21516;&#26681;&#26412;&#21407;&#22240;&#23548;&#33268;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#24050;&#32463;&#34987;&#35748;&#20026;&#26159;DNN&#22312;&#23547;&#25214;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#31283;&#20581;&#21644;&#21487;&#20449;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#20005;&#37325;&#38556;&#30861;&#12290;&#32570;&#20047;&#23545;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#32771;&#34385;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#24517;&#35201;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#24230;&#37327;DNN&#30340;&#19981;&#21516;&#19981;&#30830;&#23450;&#22240;&#32032;&#65292;&#24182;&#23558;&#23427;&#20204;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#19981;&#21516;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have received tremendous attention and achieved great success in various applications, such as image and video analysis, natural language processing, recommendation systems, and drug discovery. However, inherent uncertainties derived from different root causes have been realized as serious hurdles for DNNs to find robust and trustworthy solutions for real-world problems. A lack of consideration of such uncertainties may lead to unnecessary risk. For example, a self-driving autonomous car can misdetect a human on the road. A deep learning-based medical assistant may misdiagnose cancer as a benign tumor.  In this work, we study how to measure different uncertainty causes for DNNs and use them to solve diverse decision-making problems more effectively. In the first part of this thesis, we develop a general learning framework to quantify multiple types of uncertainties caused by different root causes, such as vacuity (i.e., uncertainty due to a lack of evidence)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#25104;&#21151;&#30340;&#23398;&#20064;Narrow One-Hidden-Layer ReLU&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20551;&#35774;&#65292;&#24182;&#20351;&#29992;&#20102;&#20998;&#26512;&#39640;&#38454;&#30697;&#24352;&#37327;&#30340;&#38543;&#26426;&#25910;&#32553;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#21487;&#20197;&#21457;&#29616;&#21333;&#20010;&#31070;&#32463;&#20803;&#12290;</title><link>http://arxiv.org/abs/2304.10524</link><description>&lt;p&gt;
&#23398;&#20064;&#31364;&#30340;&#21333;&#38544;&#34255;&#23618;ReLU&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Narrow One-Hidden-Layer ReLU Networks. (arXiv:2304.10524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#25104;&#21151;&#30340;&#23398;&#20064;Narrow One-Hidden-Layer ReLU&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20551;&#35774;&#65292;&#24182;&#20351;&#29992;&#20102;&#20998;&#26512;&#39640;&#38454;&#30697;&#24352;&#37327;&#30340;&#38543;&#26426;&#25910;&#32553;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#21487;&#20197;&#21457;&#29616;&#21333;&#20010;&#31070;&#32463;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#8212;&#8212;&#20851;&#20110;&#22312;$d$&#32500;&#36755;&#20837;&#19978;&#30340;&#39640;&#26031;&#20998;&#24067;&#20013;&#65292;&#23398;&#20064;$k$&#20010;ReLU&#28608;&#27963;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;$k$&#20026;&#24120;&#25968;&#26102;&#25104;&#21151;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#25152;&#26377;&#20043;&#21069;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#23398;&#20064;&#22120;&#37117;&#38656;&#35201;&#23545;&#32593;&#32476;&#36827;&#34892;&#39069;&#22806;&#30340;&#20551;&#35774;&#65292;&#27604;&#22914;&#27491;&#31995;&#25968;&#31995;&#21512;&#25110;&#38544;&#34255;&#26435;&#37325;&#21521;&#37327;&#30340;&#30697;&#38453;&#33391;&#22909;&#23450;&#20041;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20998;&#26512;&#39640;&#38454;&#30697;&#24352;&#37327;&#30340;&#38543;&#26426;&#25910;&#32553;&#12290;&#25105;&#20204;&#37319;&#29992;&#22810;&#23610;&#24230;&#20998;&#26512;&#26469;&#35777;&#26126;&#36275;&#22815;&#25509;&#36817;&#30340;&#31070;&#32463;&#20803;&#21487;&#20197;&#34987;&#21512;&#24182;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#35268;&#36991;&#20102;&#20197;&#21069;&#24037;&#20316;&#20013;&#23384;&#22312;&#30340;&#23450;&#24615;&#38382;&#39064;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35774;&#35745;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#26469;&#21457;&#29616;&#21333;&#20010;&#31070;&#32463;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the well-studied problem of learning a linear combination of $k$ ReLU activations with respect to a Gaussian distribution on inputs in $d$ dimensions. We give the first polynomial-time algorithm that succeeds whenever $k$ is a constant. All prior polynomial-time learners require additional assumptions on the network, such as positive combining coefficients or the matrix of hidden weight vectors being well-conditioned.  Our approach is based on analyzing random contractions of higher-order moment tensors. We use a multi-scale analysis to argue that sufficiently close neurons can be collapsed together, sidestepping the conditioning issues present in prior work. This allows us to design an iterative procedure to discover individual neurons.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#23545;&#27604;&#35843;&#25972;(MAE-CT)&#65292;&#21033;&#29992;&#26368;&#36817;&#37051;&#23545;&#27604;&#23398;&#20064;&#65288;NNCLR&#65289;&#22312;&#26631;&#35760;&#25968;&#25454;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19979;&#28216;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#22270;&#20687;&#30340;&#20016;&#23500;&#29305;&#24449;&#32858;&#31867;&#25104;&#23545;&#35937;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.10520</link><description>&lt;p&gt;
&#23545;&#27604;&#35843;&#33410;: &#24110;&#21161;&#36951;&#24536;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#19968;&#28857;&#23567;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget. (arXiv:2304.10520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#23545;&#27604;&#35843;&#25972;(MAE-CT)&#65292;&#21033;&#29992;&#26368;&#36817;&#37051;&#23545;&#27604;&#23398;&#20064;&#65288;NNCLR&#65289;&#22312;&#26631;&#35760;&#25968;&#25454;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19979;&#28216;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#22270;&#20687;&#30340;&#20016;&#23500;&#29305;&#24449;&#32858;&#31867;&#25104;&#23545;&#35937;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#22914;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36755;&#20837;&#30340;&#20016;&#23500;&#34920;&#31034;&#12290;&#20294;&#26159;&#65292;&#20026;&#20102;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#30001;&#20110;&#20854;&#20016;&#23500;&#30340;&#29305;&#24449;&#19981;&#20165;&#25429;&#33719;&#20102;&#23545;&#35937;&#32780;&#19988;&#36824;&#21253;&#25324;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#32972;&#26223;&#65292;&#22240;&#27492;&#23427;&#20204;&#38656;&#35201;&#36275;&#22815;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23454;&#20363;&#36776;&#21035;&#26041;&#27861;&#20391;&#37325;&#20110;&#23545;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#23558;MIM&#30340;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#19982;ID&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#32570;&#23569;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#19979;&#28216;&#20998;&#31867;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#23545;&#27604;&#35843;&#25972;&#65288;MAE-CT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39034;&#24207;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26368;&#36817;&#37051;&#23545;&#27604;&#23398;&#20064;&#65288;NNCLR&#65289;&#24212;&#29992;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;MAE&#12290;MAE-CT&#35843;&#25972;&#20102;&#20016;&#23500;&#30340;&#29305;&#24449;&#65292;&#20351;&#23427;&#20204;&#24418;&#25104;&#23545;&#35937;&#30340;&#35821;&#20041;&#32858;&#31867;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#26631;&#31614;&#12290;&#24212;&#29992;&#20110;&#22823;&#22411;&#21644;&#24040;&#22411;Vision Transformer&#65288;ViT&#65289;&#27169;&#22411;&#26102;&#65292;MAE-CT&#22312;&#32447;&#24615;&#25506;&#27979;&#65292;k-&#22343;&#20540;&#32858;&#31867;&#21644;&#21322;&#30417;&#30563;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#21305;&#37197;&#25110;&#36229;&#36234;&#20102;&#22312;ImageNet&#19978;&#35757;&#32451;&#30340;&#20808;&#21069;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE), efficiently learn a rich representation of the input. However, for adapting to downstream tasks, they require a sufficient amount of labeled data since their rich features capture not only objects but also less relevant image background. In contrast, Instance Discrimination (ID) methods focus on objects. In this work, we study how to combine the efficiency and scalability of MIM with the ability of ID to perform downstream classification in the absence of large amounts of labeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning (MAE-CT), a sequential approach that applies Nearest Neighbor Contrastive Learning (NNCLR) to a pre-trained MAE. MAE-CT tunes the rich features such that they form semantic clusters of objects without using any labels. Applied to large and huge Vision Transformer (ViT) models, MAE-CT matches or excels previous self-supervised methods trained on ImageNet in linear probing, k
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;SAM&#22312;&#21508;&#31181;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#28857;&#25552;&#31034;&#19979;&#20854;&#34920;&#29616;&#39640;&#24230;&#21464;&#21270;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2304.10517</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#65306;&#19968;&#39033;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model for Medical Image Analysis: an Experimental Study. (arXiv:2304.10517v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;SAM&#22312;&#21508;&#31181;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#28857;&#25552;&#31034;&#19979;&#20854;&#34920;&#29616;&#39640;&#24230;&#21464;&#21270;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#27880;&#37322;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#33719;&#21462;&#25104;&#26412;&#65292;&#35757;&#32451;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;Segment Anything Model&#65288;SAM&#65289;&#26159;&#19968;&#31181;&#22522;&#30784;&#27169;&#22411;&#65292;&#32463;&#36807;&#36229;&#36807;10&#20159;&#20010;&#27880;&#37322;&#30340;&#35757;&#32451;&#65292;&#20027;&#35201;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;&#65292;&#26088;&#22312;&#33021;&#22815;&#20197;&#20132;&#20114;&#26041;&#24335;&#20998;&#21106;&#29992;&#25143;&#23450;&#20041;&#30340;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#23613;&#31649;SAM&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#19981;&#28165;&#26970;&#35813;&#27169;&#22411;&#22312;&#36716;&#25442;&#21040;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#26102;&#20250;&#21463;&#21040;&#22810;&#22823;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;SAM&#22312;&#21508;&#31181;&#27169;&#24577;&#21644;&#35299;&#21078;&#23398;&#30340;11&#20010;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#26041;&#27861;&#29983;&#25104;&#28857;&#25552;&#31034;&#26469;&#27169;&#25311;&#20132;&#20114;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAM&#22522;&#20110;&#21333;&#28857;&#25552;&#31034;&#30340;&#34920;&#29616;&#22312;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26041;&#38754;&#39640;&#24230;&#21464;&#21270;&#65292;&#21363;&#20174;&#33034;&#26609;MRI&#25968;&#25454;&#38598;&#30340;0.1135&#21040;&#39627;&#20851;&#33410;X&#23556;&#32447;&#25968;&#25454;&#38598;&#30340;0.8650&#12290;
&lt;/p&gt;
&lt;p&gt;
Training segmentation models for medical images continues to be challenging due to the limited availability and acquisition expense of data annotations. Segment Anything Model (SAM) is a foundation model trained on over 1 billion annotations, predominantly for natural images, that is intended to be able to segment the user-defined object of interest in an interactive manner. Despite its impressive performance on natural images, it is unclear how the model is affected when shifting to medical image domains. Here, we perform an extensive evaluation of SAM's ability to segment medical images on a collection of 11 medical imaging datasets from various modalities and anatomies. In our experiments, we generated point prompts using a standard method that simulates interactive segmentation. Experimental results show that SAM's performance based on single prompts highly varies depending on the task and the dataset, i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset, eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20154;&#33041;&#26680;&#24515;-&#21608;&#36793;&#21407;&#21017;&#30340;&#31867;&#33041;&#35774;&#35745;&#21407;&#21017;&#20197;&#25351;&#23548;CNN&#30340;&#35774;&#35745;&#12290;&#36890;&#36807;&#22312;&#32593;&#32476;&#24067;&#32447;&#27169;&#24335;&#21644;&#21367;&#31215;&#25805;&#20316;&#20013;&#23454;&#29616;&#26680;&#24515;-&#21608;&#36793;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;CP-CNN&#26550;&#26500;&#65292;&#23427;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.10515</link><description>&lt;p&gt;
CP-CNN: &#22522;&#20110;&#26680;&#24515;-&#21608;&#36793;&#21407;&#21017;&#25351;&#23548;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CP-CNN: Core-Periphery Principle Guided Convolutional Neural Network. (arXiv:2304.10515v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20154;&#33041;&#26680;&#24515;-&#21608;&#36793;&#21407;&#21017;&#30340;&#31867;&#33041;&#35774;&#35745;&#21407;&#21017;&#20197;&#25351;&#23548;CNN&#30340;&#35774;&#35745;&#12290;&#36890;&#36807;&#22312;&#32593;&#32476;&#24067;&#32447;&#27169;&#24335;&#21644;&#21367;&#31215;&#25805;&#20316;&#20013;&#23454;&#29616;&#26680;&#24515;-&#21608;&#36793;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;CP-CNN&#26550;&#26500;&#65292;&#23427;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#30340;&#21457;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#20854;&#26550;&#26500;&#35774;&#35745;&#65292;&#21363;&#32593;&#32476;&#24067;&#32447;&#27169;&#24335;&#12290;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;(NAS)&#36890;&#36807;&#33258;&#21160;&#21270;&#25628;&#32034;&#26368;&#20248;&#32593;&#32476;&#26550;&#26500;&#25512;&#36827;&#20102;&#36825;&#19968;&#36827;&#23637;&#65292;&#20294;&#25152;&#24471;&#21040;&#30340;&#32593;&#32476;&#23454;&#20363;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#21487;&#33021;&#26080;&#27861;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25506;&#32034;&#36328;&#20219;&#21153;&#36890;&#29992;&#30340;&#32593;&#32476;&#35774;&#35745;&#21407;&#21017;&#26159;&#26356;&#23454;&#38469;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#20154;&#33041;&#32593;&#32476;&#30340;&#26680;&#24515;-&#21608;&#36793;&#23646;&#24615;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31867;&#33041;&#35774;&#35745;&#21407;&#21017;&#65292;&#20197;&#25351;&#23548;CNN&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20174;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#36825;&#20123;&#30740;&#31350;&#34920;&#26126;&#20154;&#24037;&#21644;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#22312;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;&#26041;&#38754;&#21487;&#33021;&#20855;&#26377;&#20849;&#21516;&#30340;&#21407;&#21017;&#12290;&#25105;&#20204;&#22312;&#32593;&#32476;&#24067;&#32447;&#27169;&#24335;&#30340;&#35774;&#35745;&#21644;&#21367;&#31215;&#25805;&#20316;&#30340;&#31232;&#30095;&#21270;&#20013;&#23454;&#29616;&#20102;&#26680;&#24515;-&#21608;&#36793;&#21407;&#21017;&#12290;&#32467;&#26524;&#65292;&#20351;&#29992;&#26680;&#24515;-&#21608;&#36793;&#21407;&#21017;&#25351;&#23548;&#30340;CNN (CP-CNN)&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of convolutional neural networks (CNNs) can be largely attributed to the design of its architecture, i.e., the network wiring pattern. Neural architecture search (NAS) advances this by automating the search for the optimal network architecture, but the resulting network instance may not generalize well in different tasks. To overcome this, exploring network design principles that are generalizable across tasks is a more practical solution. In this study, We explore a novel brain-inspired design principle based on the core-periphery property of the human brain network to guide the design of CNNs. Our work draws inspiration from recent studies suggesting that artificial and biological neural networks may have common principles in optimizing network architecture. We implement the core-periphery principle in the design of network wiring patterns and the sparsification of the convolution operation. The resulting core-periphery principle guided CNNs (CP-CNNs) are evaluated on t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#30693;&#35782;&#24863;&#30693;BERT&#27169;&#22411;&#65292;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#21457;&#24067;&#30340;&#19982;&#21512;&#25104;&#38463;&#29255;&#31867;&#29289;&#36136;&#30456;&#20851;&#30340;&#24086;&#23376;&#65292;&#20197;&#20102;&#35299;&#29992;&#25143;&#23545;&#19981;&#21516;&#21512;&#25104;&#38463;&#29255;&#31867;&#29289;&#36136;&#30340;&#24773;&#24863;&#21644;&#24773;&#32490;&#65292;&#20174;&#32780;&#24110;&#21161;&#39044;&#27979;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2304.10512</link><description>&lt;p&gt;
&#8220;&#25105;&#20204;&#33021;&#26816;&#27979;&#20986;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#21527;&#65311;&#8221;&#65306;&#26469;&#33258;&#26263;&#32593;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#30693;&#35782;&#21644;&#26102;&#38388;&#24863;&#30693;&#20998;&#31867;&#30740;&#31350;&#12290;(arXiv:2304.10512v1[cs.LG])
&lt;/p&gt;
&lt;p&gt;
"Can We Detect Substance Use Disorder?": Knowledge and Time Aware Classification on Social Media from Darkweb. (arXiv:2304.10512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#30693;&#35782;&#24863;&#30693;BERT&#27169;&#22411;&#65292;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#21457;&#24067;&#30340;&#19982;&#21512;&#25104;&#38463;&#29255;&#31867;&#29289;&#36136;&#30456;&#20851;&#30340;&#24086;&#23376;&#65292;&#20197;&#20102;&#35299;&#29992;&#25143;&#23545;&#19981;&#21516;&#21512;&#25104;&#38463;&#29255;&#31867;&#29289;&#36136;&#30340;&#24773;&#24863;&#21644;&#24773;&#32490;&#65292;&#20174;&#32780;&#24110;&#21161;&#39044;&#27979;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#32654;&#22269;&#30340;&#38463;&#29255;&#31867;&#21644;&#29289;&#36136;&#28389;&#29992;&#38382;&#39064;&#26085;&#30410;&#20005;&#37325;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#38463;&#29255;&#31867;&#21361;&#26426;&#8221;&#12290;&#29289;&#36136;&#20351;&#29992;&#21644;&#24515;&#29702;&#20581;&#24247;&#20043;&#38388;&#30340;&#20851;&#31995;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20854;&#20013;&#19968;&#31181;&#21487;&#33021;&#30340;&#20851;&#31995;&#26159;&#65306;&#29289;&#36136;&#28389;&#29992;&#23548;&#33268;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#35777;&#25454;&#65292;&#23548;&#33268;&#21512;&#27861;&#36884;&#24452;&#36141;&#20080;&#30340;&#38463;&#29255;&#31867;&#29289;&#36136;&#24456;&#38590;&#33719;&#24471;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#31038;&#20132;&#23186;&#20307;&#19978;&#26377;&#20851;&#29289;&#36136;&#20351;&#29992;&#30340;&#24086;&#23376;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#21152;&#23494;&#24066;&#22330;&#38144;&#21806;&#30340;&#21512;&#25104;&#38463;&#29255;&#31867;&#29289;&#36136;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#33647;&#29289;&#28389;&#29992;&#26412;&#20307;&#35770;&#65292;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#30693;&#35782;&#24863;&#30693;BERT&#27169;&#22411;&#29983;&#25104;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#24773;&#24863;&#21644;&#24773;&#32490;&#65292;&#20197;&#20102;&#35299;&#29992;&#25143;&#23545;&#31038;&#20132;&#23186;&#20307;&#30340;&#30475;&#27861;&#65292;&#30740;&#31350;&#38382;&#39064;&#65292;&#20363;&#22914;&#65306;&#20154;&#20204;&#23545;&#21738;&#31181;&#21512;&#25104;&#38463;&#29255;&#31867;&#29289;&#36136;&#25345;&#20048;&#35266;&#24577;&#24230;&#12289;&#20013;&#31435;&#24577;&#24230;&#25110;&#32773;&#28040;&#26497;&#24577;&#24230;&#65311;&#25110;&#32773;&#21738;&#20123;&#33647;&#29289;&#24341;&#36215;&#20102;&#24656;&#24807;&#21644;&#24754;&#20260;&#65311;&#20154;&#20204;&#21916;&#27426;&#21738;&#20123;&#33647;&#29289;&#65292;&#25110;&#32773;&#23545;&#21738;&#20123;&#33647;&#29289;&#24863;&#28608;&#65311;&#21738;&#20123;&#33647;&#29289;&#20154;&#20204;&#25345;&#28040;&#26497;&#24577;&#24230;&#65311;
&lt;/p&gt;
&lt;p&gt;
Opioid and substance misuse is rampant in the United States today, with the phenomenon known as the "opioid crisis". The relationship between substance use and mental health has been extensively studied, with one possible relationship being: substance misuse causes poor mental health. However, the lack of evidence on the relationship has resulted in opioids being largely inaccessible through legal means. This study analyzes the substance use posts on social media with opioids being sold through crypto market listings. We use the Drug Abuse Ontology, state-of-the-art deep learning, and knowledge-aware BERT-based models to generate sentiment and emotion for the social media posts to understand users' perceptions on social media by investigating questions such as: which synthetic opioids people are optimistic, neutral, or negative about? or what kind of drugs induced fear and sorrow? or what kind of drugs people love or are thankful about? or which drugs people think negatively about? or 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OutCenTR&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#20013;&#21487;&#33021;&#34987;&#21033;&#29992;&#30340;&#28431;&#27934;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#38477;&#32500;&#25216;&#26415;OutCenTR&#65292;&#21487;&#20197;&#22686;&#24378;&#22522;&#26412;&#30340;&#31163;&#32676;&#28857;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;OutCenTR&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;5&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.10511</link><description>&lt;p&gt;
OutCenTR: &#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#39044;&#27979;&#28431;&#27934;&#21033;&#29992;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OutCenTR: A novel semi-supervised framework for predicting exploits of vulnerabilities in high-dimensional datasets. (arXiv:2304.10511v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OutCenTR&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#20013;&#21487;&#33021;&#34987;&#21033;&#29992;&#30340;&#28431;&#27934;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#38477;&#32500;&#25216;&#26415;OutCenTR&#65292;&#21487;&#20197;&#22686;&#24378;&#22522;&#26412;&#30340;&#31163;&#32676;&#28857;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;OutCenTR&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#37117;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#28431;&#27934;&#34987;&#25253;&#36947;&#65292;&#28982;&#32780;&#36825;&#20123;&#28431;&#27934;&#24182;&#19981;&#37117;&#26159;&#30456;&#21516;&#30340;&#65292;&#26377;&#20123;&#27604;&#20854;&#20182;&#30340;&#26356;&#20855;&#26377;&#38024;&#23545;&#24615;&#12290;&#27491;&#30830;&#20272;&#35745;&#28431;&#27934;&#34987;&#21033;&#29992;&#30340;&#21487;&#33021;&#24615;&#26159;&#31995;&#32479;&#31649;&#29702;&#21592;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26412;&#25991;&#21033;&#29992;&#31163;&#32676;&#28857;&#26816;&#27979;&#25216;&#26415;&#26469;&#39044;&#27979;&#39640;&#24230;&#19981;&#24179;&#34913;&#21644;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#20013;&#21487;&#33021;&#34987;&#21033;&#29992;&#30340;&#28431;&#27934;&#65292;&#20363;&#22914;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#32500;&#25216;&#26415;OutCenTR&#65292;&#21487;&#20197;&#22686;&#24378;&#22522;&#26412;&#30340;&#31163;&#32676;&#28857;&#26816;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;4&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;12&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;OutCenTR&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;PCA&#21644;GRP&#31561;&#26368;&#20808;&#36827;&#30340;&#38477;&#32500;&#25216;&#26415;&#30456;&#27604;&#65292;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
An ever-growing number of vulnerabilities are reported every day. Yet these vulnerabilities are not all the same; Some are more targeted than others. Correctly estimating the likelihood of a vulnerability being exploited is a critical task for system administrators. This aids the system administrators in prioritizing and patching the right vulnerabilities. Our work makes use of outlier detection techniques to predict vulnerabilities that are likely to be exploited in highly imbalanced and high-dimensional datasets such as the National Vulnerability Database. We propose a dimensionality reduction technique, OutCenTR, that enhances the baseline outlier detection models. We further demonstrate the effectiveness and efficiency of OutCenTR empirically with 4 benchmark and 12 synthetic datasets. The results of our experiments show on average a 5-fold improvement of F1 score in comparison with state-of-the-art dimensionality reduction techniques such as PCA and GRP.
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#32531;&#35299;&#21270;&#23398;&#25968;&#25454;&#34987;&#29992;&#20110;&#24320;&#21457;&#35782;&#21035;&#26032;&#22411;&#27602;&#32032;&#25110;&#21270;&#23398;&#25112;&#21058;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#21452;&#37325;&#29992;&#36884;&#39118;&#38505;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#30041;&#26377;&#21033;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#30340;&#25928;&#29992;&#21306;&#22495;&#30340;&#21516;&#26102;&#65292;&#23545;&#25935;&#24863;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#30340;&#28155;&#21152;&#22122;&#38899;&#65292;&#35813;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#24573;&#30053;&#25935;&#24863;&#25968;&#25454;&#20250;&#22686;&#21152;&#27169;&#22411;&#30340;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.10510</link><description>&lt;p&gt;
&#32531;&#35299;&#21452;&#37325;&#29992;&#36884;&#39118;&#38505;&#30340;&#21270;&#23398;&#25968;&#25454;&#23457;&#26597;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Censoring chemical data to mitigate dual use risk. (arXiv:2304.10510v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10510
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#32531;&#35299;&#21270;&#23398;&#25968;&#25454;&#34987;&#29992;&#20110;&#24320;&#21457;&#35782;&#21035;&#26032;&#22411;&#27602;&#32032;&#25110;&#21270;&#23398;&#25112;&#21058;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#21452;&#37325;&#29992;&#36884;&#39118;&#38505;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#30041;&#26377;&#21033;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#30340;&#25928;&#29992;&#21306;&#22495;&#30340;&#21516;&#26102;&#65292;&#23545;&#25935;&#24863;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#30340;&#28155;&#21152;&#22122;&#38899;&#65292;&#35813;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#24573;&#30053;&#25935;&#24863;&#25968;&#25454;&#20250;&#22686;&#21152;&#27169;&#22411;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21452;&#37325;&#29992;&#36884;&#65288;&#27169;&#22411;&#26082;&#21487;&#20197;&#29992;&#20110;&#26377;&#30410;&#29992;&#36884;&#21448;&#21487;&#20197;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#65289;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22312;&#21270;&#23398;&#39046;&#22495;&#65292;&#30001;&#20110;&#21547;&#26377;&#25935;&#24863;&#26631;&#31614;&#65288;&#22914;&#27602;&#29702;&#23398;&#20449;&#24687;&#65289;&#30340;&#21270;&#23398;&#25968;&#25454;&#38598;&#21487;&#33021;&#34987;&#29992;&#20110;&#24320;&#21457;&#35782;&#21035;&#26032;&#22411;&#27602;&#32032;&#25110;&#21270;&#23398;&#25112;&#21058;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#36825;&#24050;&#25104;&#20026;&#19968;&#20010;&#29305;&#21035;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#32531;&#35299;&#21452;&#37325;&#29992;&#36884;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#36873;&#25321;&#24615;&#22320;&#28155;&#21152;&#22122;&#38899;&#21040;&#25968;&#25454;&#38598;&#20013;&#65292;&#21516;&#26102;&#20445;&#30041;&#26377;&#21033;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#30340;&#25928;&#29992;&#21306;&#22495;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#26368;&#23567;&#20108;&#20056;&#27861;&#12289;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26377;&#25928;&#24615;, &#21457;&#29616;&#26377;&#36873;&#25321;&#24615;&#28155;&#21152;&#22122;&#22768;&#25968;&#25454;&#21487;&#20197;&#24341;&#20837;&#27169;&#22411;&#30340;&#26041;&#24046;&#21644;&#39044;&#27979;&#25935;&#24863;&#26631;&#31614;&#30340;&#20559;&#24046;&#65292;&#36825;&#34920;&#26126;&#21487;&#20197;&#23454;&#29616;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#30340;&#23433;&#20840;&#20849;&#20139;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24573;&#30053;&#25935;&#24863;&#25968;&#25454;&#36890;&#24120;&#20250;&#22686;&#21152;&#27169;&#22411;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dual use of machine learning applications, where models can be used for both beneficial and malicious purposes, presents a significant challenge. This has recently become a particular concern in chemistry, where chemical datasets containing sensitive labels (e.g. toxicological information) could be used to develop predictive models that identify novel toxins or chemical warfare agents. To mitigate dual use risks, we propose a model-agnostic method of selectively noising datasets while preserving the utility of the data for training deep neural networks in a beneficial region. We evaluate the effectiveness of the proposed method across least squares, a multilayer perceptron, and a graph neural network. Our findings show selectively noised datasets can induce model variance and bias in predictions for sensitive labels with control, suggesting the safe sharing of datasets containing sensitive information is feasible. We also find omitting sensitive data often increases model variance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#39057;&#39044;&#35757;&#32451;Transformer&#65292;&#23427;&#20351;&#29992;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#23558;&#35270;&#39057;&#36716;&#25442;&#25104;&#36890;&#29992;&#34920;&#31034;&#65292;&#21487;&#20197;&#32467;&#21512;&#22810;&#31181;&#27169;&#24335;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10505</link><description>&lt;p&gt;
&#35270;&#39057;&#39044;&#35757;&#32451;Transformer: &#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#19987;&#23478;&#28151;&#21512;&#20307;
&lt;/p&gt;
&lt;p&gt;
Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts. (arXiv:2304.10505v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#39057;&#39044;&#35757;&#32451;Transformer&#65292;&#23427;&#20351;&#29992;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#23558;&#35270;&#39057;&#36716;&#25442;&#25104;&#36890;&#29992;&#34920;&#31034;&#65292;&#21487;&#20197;&#32467;&#21512;&#22810;&#31181;&#27169;&#24335;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35270;&#39057;&#39044;&#35757;&#32451;Transformer (VPT)&#12290;VPT&#20351;&#29992;&#26469;&#33258;&#20043;&#21069;&#24037;&#20316;&#30340;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#23558;&#35270;&#39057;&#36716;&#25442;&#25104;&#32039;&#20945;&#30340;&#23884;&#20837;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#39592;&#24178;&#32593;&#32476;&#22522;&#20110;&#21442;&#32771;Flan-T5-11B&#26550;&#26500;&#65292;&#23398;&#20064;&#35270;&#39057;&#30340;&#36890;&#29992;&#34920;&#31034;&#24418;&#24335;&#65292;&#36825;&#26159;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#24635;&#21644;&#12290;&#23427;&#20351;&#29992;&#33258;&#22238;&#24402;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#36890;&#36807;&#39044;&#27979;YouTube&#35270;&#39057;&#20013;&#35762;&#35805;&#30340;&#21333;&#35789;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;&#27599;&#20010;&#20219;&#21153;&#35757;&#32451;&#20840;&#36830;&#25509;&#39044;&#27979;&#22836;&#65292;&#22312;&#26631;&#20934;&#30340;&#19979;&#28216;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#8220;&#23884;&#20837;-&gt;&#39592;&#24178;&#32593;&#32476;-&gt;&#39044;&#27979;&#22836;&#8221;&#35774;&#35745;&#27169;&#24335;&#20013;&#20351;&#29992;&#22810;&#20010;&#20923;&#32467;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#30740;&#31350;&#65292;&#32780;&#20854;&#20182;&#30740;&#31350;&#37117;&#26159;&#35757;&#32451;&#33258;&#24049;&#30340;&#32852;&#21512;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#26174;&#24335;&#30340;&#22330;&#26223;&#22270;&#20449;&#24687;&#65292;&#21253;&#21547;&#20102;&#27604;&#24403;&#21069;SOTA Merlot Reserve&#26356;&#22810;&#30340;&#27169;&#24577;&#12290;&#20986;&#20110;&#36825;&#20004;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#30456;&#20449;&#23427;&#21487;&#20197;&#23558;&#19990;&#30028;&#19978;&#26368;&#22909;&#30340;&#24320;&#28304;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;SOTA&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Video Pre-trained Transformer. VPT uses four SOTA encoder models from prior work to convert a video into a sequence of compact embeddings. Our backbone, based on a reference Flan-T5-11B architecture, learns a universal representation of the video that is a non-linear sum of the encoder models. It learns using an autoregressive causal language modeling loss by predicting the words spoken in YouTube videos. Finally, we evaluate on standard downstream benchmarks by training fully connected prediction heads for each task. To the best of our knowledge, this is the first use of multiple frozen SOTA models as encoders in an "embedding -&gt; backbone -&gt; prediction head" design pattern - all others have trained their own joint encoder models. Additionally, we include more modalities than the current SOTA, Merlot Reserve, by adding explicit Scene Graph information. For these two reasons, we believe it could combine the world's best open-source models to achieve SOTA performance. Initial 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#29575;&#35889;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#21306;&#22495;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#40065;&#26834;&#33258;&#36866;&#24212;&#27874;&#26463;&#25104;&#24418;&#25216;&#26415;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#24178;&#25200;&#21152;&#22122;&#22768;&#25104;&#20998;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10502</link><description>&lt;p&gt;
&#22522;&#20110;&#21151;&#29575;&#35889;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#21306;&#22495;&#30340;&#40065;&#26834;&#33258;&#36866;&#24212;&#27874;&#26463;&#25104;&#24418;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Study of Robust Adaptive Beamforming with Covariance Matrix Reconstruction Based on Power Spectral Estimation and Uncertainty Region. (arXiv:2304.10502v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#29575;&#35889;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#21306;&#22495;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#40065;&#26834;&#33258;&#36866;&#24212;&#27874;&#26463;&#25104;&#24418;&#25216;&#26415;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#24178;&#25200;&#21152;&#22122;&#22768;&#25104;&#20998;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24178;&#25200;&#21152;&#22122;&#22768;&#25104;&#20998;&#30340;&#21151;&#29575;&#35889;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#21306;&#22495;&#65288;PSEUR&#65289;&#30340;&#19968;&#33268;&#32447;&#24615;&#38453;&#21015;&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#40065;&#26834;&#33258;&#36866;&#24212;&#27874;&#26463;&#25104;&#24418;&#25216;&#26415;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#25214;&#21040;&#27599;&#20010;&#30636;&#26102;&#30340;&#24178;&#25200;&#35282;&#24230;&#21306;&#38388;&#65292;&#22522;&#20110;&#37319;&#29992;&#30340;&#24178;&#25200;&#26041;&#21521;&#30340;&#31354;&#38388;&#19981;&#30830;&#23450;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#23545;&#24178;&#25200;&#21644;&#22122;&#22768;&#25104;&#20998;&#21151;&#29575;&#30340;&#20272;&#35745;&#24341;&#20837;&#20102;&#19968;&#20010;&#21151;&#29575;&#35889;&#65292;&#20174;&#32780;&#20801;&#35768;&#24320;&#21457;&#19968;&#31181;&#40065;&#26834;&#30340;IPN&#21327;&#26041;&#24046;&#30697;&#38453;&#37325;&#24314;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#20004;&#20010;&#20027;&#35201;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#21253;&#21547;&#24178;&#25200;&#26041;&#21521;&#30340;&#35282;&#24230;&#21306;&#22495;&#22522;&#20110;&#38453;&#21015;&#25968;&#25454;&#30340;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#26356;&#26032;&#12290;&#20854;&#27425;&#65292;&#25152;&#25552;&#20986;&#30340;IPN-PSEUR&#26041;&#27861;&#36991;&#20813;&#20102;&#20272;&#35745;&#24178;&#25200;&#21306;&#38388;&#25152;&#26377;&#21487;&#33021;&#26041;&#21521;&#30340;&#21151;&#29575;&#35889;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#40065;&#26834;&#24615;&#21644;&#33258;&#36866;&#24212;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#27874;&#26463;&#25104;&#24418;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, a simple and effective robust adaptive beamforming technique is proposed for uniform linear arrays, which is based on the power spectral estimation and uncertainty region (PSEUR) of the interference plus noise (IPN) components. In particular, two algorithms are presented to find the angular sector of interference in every snapshot based on the adopted spatial uncertainty region of the interference direction. Moreover, a power spectrum is introduced based on the estimation of the power of interference and noise components, which allows the development of a robust approach to IPN covariance matrix reconstruction. The proposed method has two main advantages. First, an angular region that contains the interference direction is updated based on the statistics of the array data. Secondly, the proposed IPN-PSEUR method avoids estimating the power spectrum of the whole range of possible directions of the interference sector. Simulation results show that the performance of the pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20351;&#29992; Transformer &#27169;&#22411;&#36827;&#34892;&#24418;&#24335;&#35821;&#35328;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26368;&#22522;&#26412;&#30340;&#29305;&#24615;&#8212;&#8212;&#26415;&#35821;&#21644;&#31867;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Transformer &#30340;&#31867;&#22411;&#25512;&#29702;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TTI &#27169;&#22411;&#22312;&#25512;&#29702;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10500</link><description>&lt;p&gt;
&#20351;&#29992; Transformer &#27169;&#22411;&#36827;&#34892; Simply Typed Lambda Calculus &#30340;&#31867;&#22411;&#25512;&#29702;&#65306;&#28145;&#24230;&#23398;&#20064;&#22312;&#20195;&#30721;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transformer Models for Type Inference in the Simply Typed Lambda Calculus: A Case Study in Deep Learning for Code. (arXiv:2304.10500v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20351;&#29992; Transformer &#27169;&#22411;&#36827;&#34892;&#24418;&#24335;&#35821;&#35328;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26368;&#22522;&#26412;&#30340;&#29305;&#24615;&#8212;&#8212;&#26415;&#35821;&#21644;&#31867;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Transformer &#30340;&#31867;&#22411;&#25512;&#29702;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TTI &#27169;&#22411;&#22312;&#25512;&#29702;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21644;&#24418;&#24335;&#35821;&#35328;&#30340;&#20132;&#21449;&#39046;&#22495;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#31995;&#32479;&#24615;&#22320;&#25506;&#32034;&#20351;&#29992; Transformer &#27169;&#22411;&#25512;&#29702;&#26377;&#31867;&#22411; lambda &#28436;&#31639;&#26041;&#38754;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#36825;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26377;&#20004;&#20010;&#21407;&#22240;&#12290;&#39318;&#20808;&#65292;&#26377;&#31867;&#22411;&#30340; lambda &#28436;&#31639;&#26159;&#32534;&#31243;&#35821;&#35328;&#30340;&#22522;&#30784;&#12290;&#23558;&#21508;&#31181;&#26377;&#31867;&#22411; lambda &#28436;&#31639;&#19982;&#26377;&#25928;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#20851;&#32852;&#30340;&#19968;&#32452;&#21551;&#21457;&#24335;&#26041;&#27861;&#20250;&#25552;&#20379;&#19968;&#31181;&#23558;&#35821;&#35328;&#29305;&#24449;&#65288;&#20363;&#22914;&#22810;&#24577;&#24615;&#65292;&#23376;&#31867;&#22411;&#65292;&#32487;&#25215;&#31561;&#65289;&#26144;&#23556;&#21040;&#26550;&#26500;&#36873;&#25321;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;Transformer &#27169;&#22411;&#24191;&#27867;&#29992;&#20110;&#24212;&#29992;&#20110;&#20195;&#30721;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#65292;&#20294;&#20854;&#35774;&#35745;&#21644;&#36229;&#21442;&#25968;&#31354;&#38388;&#22312;&#32534;&#31243;&#35821;&#35328;&#24212;&#29992;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#20063;&#35768;&#26159;&#32534;&#31243;&#35821;&#35328;&#26368;&#31616;&#21333;&#21644;&#26368;&#22522;&#26412;&#30340;&#29305;&#24615;&#65292;&#21363;&#26415;&#35821;&#21644;&#31867;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26469;&#25506;&#32034;&#36825;&#19968;&#28857;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite a growing body of work at the intersection of deep learning and formal languages, there has been relatively little systematic exploration of transformer models for reasoning about typed lambda calculi. This is an interesting area of inquiry for two reasons. First, typed lambda calculi are the lingua franc of programming languages. A set of heuristics that relate various typed lambda calculi to effective neural architectures would provide a systematic method for mapping language features (e.g., polymorphism, subtyping, inheritance, etc.) to architecture choices. Second, transformer models are widely used in deep learning architectures applied to code, but the design and hyperparameter space for them is large and relatively unexplored in programming language applications. Therefore, we suggest a benchmark that allows us to explore exactly this through perhaps the simplest and most fundamental property of a programming language: the relationship between terms and types. Consequent
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#26500;&#35937;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#21463;&#21040;&#36136;&#30097;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20173;&#26159;&#19968;&#20010;&#26377;&#21147;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2304.10494</link><description>&lt;p&gt;
&#26080;&#38480;&#29289;&#29702;&#29492;&#23376;&#65306;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#26500;&#35937;&#29983;&#25104;&#20013;&#30495;&#30340;&#27604;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Infinite Physical Monkey: Do Deep Learning Methods Really Perform Better in Conformation Generation?. (arXiv:2304.10494v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10494
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#26500;&#35937;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#21463;&#21040;&#36136;&#30097;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20173;&#26159;&#19968;&#20010;&#26377;&#21147;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#35937;&#29983;&#25104;&#26159;&#33647;&#29289;&#21457;&#29616;&#21644;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#26377;&#26426;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#65292;&#29305;&#21035;&#26159;&#22312;&#30495;&#31354;&#21644;&#34507;&#30333;&#36136;&#21475;&#34955;&#29615;&#22659;&#20013;&#65292;&#19982;&#33647;&#29289;&#35774;&#35745;&#26368;&#30456;&#20851;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#26696;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#36825;&#19968;&#39046;&#22495;&#65292;&#21253;&#25324;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#65288;&#22312;&#30495;&#31354;&#20013;&#65289;&#21644;&#32467;&#21512;&#20301;&#23039;&#29983;&#25104;&#65288;&#22312;&#34507;&#30333;&#36136;&#21475;&#34955;&#20013;&#65289;&#12290;&#21069;&#32773;&#25171;&#36133;&#20102;&#20256;&#32479;&#30340;ETKDG&#26041;&#27861;&#65292;&#32780;&#21518;&#32773;&#36798;&#21040;&#20102;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#20998;&#23376;&#23545;&#25509;&#36719;&#20214;&#30456;&#20284;&#30340;&#31934;&#24230;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#36136;&#30097;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#26159;&#21542;&#36890;&#36807;&#26080;&#21442;&#25968;&#26041;&#27861;&#26356;&#22909;&#22320;&#22312;&#20998;&#23376;&#26500;&#35937;&#29983;&#25104;&#20013;&#34920;&#29616;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20182;&#20204;&#35774;&#35745;&#30340;&#26041;&#27861;&#26377;&#28857;&#31867;&#20284;&#20110;&#33879;&#21517;&#30340;&#26080;&#38480;&#29492;&#23376;&#23450;&#29702;&#65292;&#36825;&#20123;&#29492;&#23376;&#29978;&#33267;&#35013;&#22791;&#20102;&#29289;&#29702;&#25945;&#32946;&#12290;&#26412;&#25991;&#26088;&#22312;&#35752;&#35770;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#31616;&#21333;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#20173;&#28982;&#26159;&#26500;&#35937;&#29983;&#25104;&#38382;&#39064;&#20013;&#30340;&#26377;&#21147;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformation Generation is a fundamental problem in drug discovery and cheminformatics. And organic molecule conformation generation, particularly in vacuum and protein pocket environments, is most relevant to drug design. Recently, with the development of geometric neural networks, the data-driven schemes have been successfully applied in this field, both for molecular conformation generation (in vacuum) and binding pose generation (in protein pocket). The former beats the traditional ETKDG method, while the latter achieves similar accuracy compared with the widely used molecular docking software. Although these methods have shown promising results, some researchers have recently questioned whether deep learning (DL) methods perform better in molecular conformation generation via a parameter-free method. To our surprise, what they have designed is some kind analogous to the famous infinite monkey theorem, the monkeys that are even equipped with physics education. To discuss the feasib
&lt;/p&gt;</description></item><item><title>CoProver&#26159;&#19968;&#31181;&#35777;&#26126;&#26500;&#36896;&#25512;&#33616;&#31995;&#32479;&#65292;&#33021;&#22815;&#20174;&#35777;&#26126;&#26500;&#36896;&#36807;&#31243;&#20013;&#30340;&#36807;&#21435;&#25805;&#20316;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;&#23384;&#20648;&#22312;ITP&#20013;&#30340;&#20851;&#20110;&#20197;&#21069;&#35777;&#26126;&#30340;&#30693;&#35782;&#26469;&#25552;&#20379;&#26377;&#29992;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2304.10486</link><description>&lt;p&gt;
CoProver: &#19968;&#31181;&#35777;&#26126;&#26500;&#36896;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CoProver: A Recommender System for Proof Construction. (arXiv:2304.10486v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10486
&lt;/p&gt;
&lt;p&gt;
CoProver&#26159;&#19968;&#31181;&#35777;&#26126;&#26500;&#36896;&#25512;&#33616;&#31995;&#32479;&#65292;&#33021;&#22815;&#20174;&#35777;&#26126;&#26500;&#36896;&#36807;&#31243;&#20013;&#30340;&#36807;&#21435;&#25805;&#20316;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#25506;&#32034;&#23384;&#20648;&#22312;ITP&#20013;&#30340;&#20851;&#20110;&#20197;&#21069;&#35777;&#26126;&#30340;&#30693;&#35782;&#26469;&#25552;&#20379;&#26377;&#29992;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#23450;&#29702;&#35777;&#26126;&#24037;&#20855;(ITPs)&#26159;&#24418;&#24335;&#21270;&#26041;&#27861;&#19987;&#23478;&#24037;&#20855;&#24211;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;, &#29992;&#20110;&#26500;&#24314;&#21644;(&#24418;&#24335;)&#39564;&#35777;&#35777;&#26126;. &#35777;&#26126;&#30340;&#22797;&#26434;&#24615;&#21644;&#36890;&#24120;&#38656;&#35201;&#30340;&#19987;&#19994;&#27700;&#24179;&#24448;&#24448;&#20250;&#38459;&#30861;ITPs&#30340;&#37319;&#29992;. &#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#24037;&#20316;&#30740;&#31350;&#20102;&#23558;&#22522;&#20110;ITP&#29992;&#25143;&#27963;&#21160;&#36319;&#36394;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#21487;&#34892;&#36884;&#24452;&#30340;&#26041;&#27861;. &#34429;&#28982;&#36825;&#26159;&#19968;&#26465;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#32447;, &#20294;&#20173;&#28982;&#26377;&#35768;&#22810;&#38382;&#39064;&#38656;&#35201;&#20154;&#31867;&#30340;&#30417;&#30563;&#25165;&#33021;&#23436;&#20840;&#23436;&#25104;&#65292;&#22240;&#27492;&#23558;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#21327;&#21161;&#29992;&#25143;&#25552;&#20379;&#26377;&#29992;&#24314;&#35758;&#21487;&#33021;&#26356;&#20026;&#26377;&#30410;&#12290;&#36319;&#38543;&#29992;&#25143;&#21327;&#21161;&#30340;&#24605;&#36335;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CoProver&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;transformers&#30340;&#35777;&#26126;&#25512;&#33616;&#31995;&#32479;&#65292;&#33021;&#22815;&#20174;&#35777;&#26126;&#26500;&#36896;&#36807;&#31243;&#20013;&#30340;&#36807;&#21435;&#25805;&#20316;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#25506;&#32034;&#23384;&#20648;&#22312;ITP&#20013;&#30340;&#20851;&#20110;&#20197;&#21069;&#35777;&#26126;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive Theorem Provers (ITPs) are an indispensable tool in the arsenal of formal method experts as a platform for construction and (formal) verification of proofs. The complexity of the proofs in conjunction with the level of expertise typically required for the process to succeed can often hinder the adoption of ITPs. A recent strain of work has investigated methods to incorporate machine learning models trained on ITP user activity traces as a viable path towards full automation. While a valuable line of investigation, many problems still require human supervision to be completed fully, thus applying learning methods to assist the user with useful recommendations can prove more fruitful.  Following the vein of user assistance, we introduce CoProver, a proof recommender system based on transformers, capable of learning from past actions during proof construction, all while exploring knowledge stored in the ITP concerning previous proofs. CoProver employs a neurally learnt sequenc
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#35201;&#29942;&#39048;&#22312;&#20110;&#39640;&#26102;&#38388;&#24046;&#35823;&#24046;&#30340;&#39564;&#35777;&#38598;&#19978;&#20986;&#29616;&#20102;&#20005;&#37325;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10466</link><description>&lt;p&gt;
&#39640;&#25928;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38656;&#35201;&#25233;&#21046;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Efficient Deep Reinforcement Learning Requires Regulating Overfitting. (arXiv:2304.10466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10466
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#35201;&#29942;&#39048;&#22312;&#20110;&#39640;&#26102;&#38388;&#24046;&#35823;&#24046;&#30340;&#39564;&#35777;&#38598;&#19978;&#20986;&#29616;&#20102;&#20005;&#37325;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#25910;&#38598;&#26377;&#38480;&#30340;&#25968;&#25454;&#36827;&#34892;&#31574;&#30053;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#38656;&#35201;&#27491;&#30830;&#30340;&#27491;&#21017;&#21270;&#25216;&#24039;&#25165;&#33021;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#21033;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#26816;&#39564;&#20960;&#31181;&#20551;&#35774;&#65292;&#22914;&#38750;&#31283;&#24577;&#24615;&#12289;&#36807;&#24230;&#21160;&#20316;&#20998;&#24067;&#20559;&#31227;&#21644;&#36807;&#25311;&#21512;&#31561;&#65292;&#35797;&#22270;&#29702;&#35299;&#22312;&#26679;&#26412;&#39640;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#20027;&#35201;&#30340;&#29942;&#39048;&#12290;&#25105;&#20204;&#23545;DeepMind&#25511;&#21046;&#22871;&#20214;&#65288;DMC&#65289;&#20219;&#21153;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#19968;&#31181;&#26377;&#25511;&#21046;&#12289;&#31995;&#32479;&#30340;&#26041;&#24335;&#23637;&#31034;&#20102;&#23545;&#36716;&#25442;&#30340;&#39564;&#35777;&#38598;&#30340;&#39640;&#26102;&#38388;&#24046;&#65288;TD&#65289;&#35823;&#24046;&#26159;&#20005;&#37325;&#24433;&#21709;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30340;&#20027;&#35201;&#32618;&#39745;&#31096;&#39318;&#65292;&#32780;&#20808;&#21069;&#30340;&#26041;&#27861;......(&#26410;&#23436;&#25972;&#32763;&#35793;)
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms that learn policies by trial-and-error must learn from limited amounts of data collected by actively interacting with the environment. While many prior works have shown that proper regularization techniques are crucial for enabling data-efficient RL, a general understanding of the bottlenecks in data-efficient RL has remained unclear. Consequently, it has been difficult to devise a universal technique that works well across all domains. In this paper, we attempt to understand the primary bottleneck in sample-efficient deep RL by examining several potential hypotheses such as non-stationarity, excessive action distribution shift, and overfitting. We perform thorough empirical analysis on state-based DeepMind control suite (DMC) tasks in a controlled and systematic way to show that high temporal-difference (TD) error on the validation set of transitions is the main culprit that severely affects the performance of deep RL algorithms, and prior method
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35282;&#24230;&#35745;&#31639;&#30340;&#21160;&#24577;&#23398;&#20064;&#29575;&#26041;&#24335;&#65292;&#29992;&#20110;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20013;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#36873;&#25321;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#34987;&#35777;&#26126;&#26159;&#21487;&#20197;&#25910;&#25947;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.10457</link><description>&lt;p&gt;
&#22522;&#20110;&#35282;&#24230;&#30340;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#23398;&#20064;&#29575;
&lt;/p&gt;
&lt;p&gt;
Angle based dynamic learning rate for gradient descent. (arXiv:2304.10457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35282;&#24230;&#35745;&#31639;&#30340;&#21160;&#24577;&#23398;&#20064;&#29575;&#26041;&#24335;&#65292;&#29992;&#20110;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20013;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#36873;&#25321;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#34987;&#35777;&#26126;&#26159;&#21487;&#20197;&#25910;&#25947;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#21462;&#20998;&#31867;&#20219;&#21153;&#19978;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#24403;&#21069;&#26799;&#24230;&#19982;&#26032;&#26799;&#24230;&#20043;&#38388;&#30340;&#35282;&#24230;&#26469;&#36873;&#25321;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#39033;&#26399;&#26395;&#30340;&#34928;&#20943;&#26469;&#36873;&#25321;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22312;&#24403;&#21069;&#26799;&#24230;&#22402;&#30452;&#26041;&#21521;&#19978;&#35745;&#31639;&#20986;&#26469;&#30340;&#26032;&#26799;&#24230;&#65292;&#20174;&#32780;&#24110;&#21161;&#25105;&#20204;&#26681;&#25454;&#35282;&#24230;&#21382;&#21490;&#35760;&#24405;&#24471;&#21040;&#26356;&#22909;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#22240;&#27492;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;ResNet&#12289;DenseNet&#12289;EfficientNet&#21644;VGG&#31561;&#27969;&#34892;&#30340;&#22270;&#20687;&#20998;&#31867;&#26550;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#37117;&#33719;&#24471;&#20102;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#25910;&#25947;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our work, we propose a novel yet simple approach to obtain an adaptive learning rate for gradient-based descent methods on classification tasks. Instead of the traditional approach of selecting adaptive learning rates via the decayed expectation of gradient-based terms, we use the angle between the current gradient and the new gradient: this new gradient is computed from the direction orthogonal to the current gradient, which further helps us in determining a better adaptive learning rate based on angle history, thereby, leading to relatively better accuracy compared to the existing state-of-the-art optimizers. On a wide variety of benchmark datasets with prominent image classification architectures such as ResNet, DenseNet, EfficientNet, and VGG, we find that our method leads to the highest accuracy in most of the datasets. Moreover, we prove that our method is convergent.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35748;&#35777;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#25200;&#21160;&#36793;&#30028;&#19979;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#65292;&#21516;&#26102;&#38024;&#23545;RS&#26041;&#27861;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35748;&#35777;&#24615;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10446</link><description>&lt;p&gt;
&#22312;&#22810;&#20010;&#25200;&#21160;&#32422;&#26463;&#20013;&#35777;&#26126;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certified Adversarial Robustness Within Multiple Perturbation Bounds. (arXiv:2304.10446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35748;&#35777;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#25200;&#21160;&#36793;&#30028;&#19979;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#65292;&#21516;&#26102;&#38024;&#23545;RS&#26041;&#27861;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35748;&#35777;&#24615;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#65288;RS&#65289;&#26159;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#25269;&#24481;&#26041;&#27861;&#65292;&#23427;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36890;&#36807;&#39044;&#27979;&#22312;&#36755;&#20837;&#19978;&#30340;&#38543;&#26426;&#22122;&#22768;&#25200;&#21160;&#19979;&#26368;&#21487;&#33021;&#30340;&#20998;&#31867;&#26469;&#21019;&#24314;&#24179;&#28369;&#30340;&#20998;&#31867;&#22120;&#12290;&#34429;&#28982;&#26368;&#21021;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#23545;$\ell_2$&#33539;&#25968;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#29992;&#20174;&#39640;&#26031;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22122;&#22768;&#65292;&#20294;&#38543;&#21518;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#22122;&#22768;&#20998;&#24067;&#20063;&#21487;&#20197;&#23548;&#33268;&#23545;&#20854;&#20182;$\ell_p$&#33539;&#25968;&#36793;&#30028;&#30340;&#40065;&#26834;&#24615;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#29305;&#23450;&#30340;&#22122;&#22768;&#20998;&#24067;&#26159;&#38450;&#24481;&#32473;&#23450;$\ell_p$&#33539;&#25968;&#25915;&#20987;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#22810;&#20010;&#25200;&#21160;&#30028;&#38480;&#30340;&#35748;&#35777;&#24615;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;\textit{&#35748;&#35777;&#26041;&#26696;}&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20351;&#29992;&#19981;&#21516;&#22122;&#22768;&#20998;&#24067;&#33719;&#24471;&#30340;&#35777;&#20070;&#65292;&#20197;&#22312;&#22810;&#20010;&#25200;&#21160;&#30028;&#38480;&#19979;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;\textit{&#35757;&#32451;&#22122;&#22768;&#20998;&#24067;}&#65292;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;\textit{&#21487;&#35777;&#26126;&#30340;&#36793;&#30028;}&#29992;&#20110;RS&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#21516;&#26102;&#23454;&#29616;&#22810;&#20010;&#25200;&#21160;&#30028;&#38480;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#21333;&#20010;&#24179;&#28369;&#30340;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22810;&#20010;&#25200;&#21160;&#30028;&#38480;&#19979;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized smoothing (RS) is a well known certified defense against adversarial attacks, which creates a smoothed classifier by predicting the most likely class under random noise perturbations of inputs during inference. While initial work focused on robustness to $\ell_2$ norm perturbations using noise sampled from a Gaussian distribution, subsequent works have shown that different noise distributions can result in robustness to other $\ell_p$ norm bounds as well. In general, a specific noise distribution is optimal for defending against a given $\ell_p$ norm based attack. In this work, we aim to improve the certified adversarial robustness against multiple perturbation bounds simultaneously. Towards this, we firstly present a novel \textit{certification scheme}, that effectively combines the certificates obtained using different noise distributions to obtain optimal results against multiple perturbation bounds. We further propose a novel \textit{training noise distribution} along wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22312;&#26684;&#28857;&#37327;&#23376;&#33394;&#21160;&#21147;&#23398;&#20013;&#20351;&#29992;&#35268;&#33539;&#31561;&#21464;&#27744;&#21270;&#23618;&#21487;&#20197;&#36890;&#36807;&#20285;&#36797;&#37329;&#26500;&#36896;&#31895;&#32593;&#26684;&#35268;&#33539;&#22330;&#28040;&#38500;&#20020;&#30028;&#20943;&#24930;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2304.10438</link><description>&lt;p&gt;
&#26684;&#28857;&#37327;&#23376;&#33394;&#21160;&#21147;&#23398;&#39044;&#26465;&#20214;&#31639;&#23376;&#20013;&#30340;&#35268;&#33539;&#31561;&#21464;&#27744;&#21270;&#23618;
&lt;/p&gt;
&lt;p&gt;
Gauge-equivariant pooling layers for preconditioners in lattice QCD. (arXiv:2304.10438v1 [hep-lat])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22312;&#26684;&#28857;&#37327;&#23376;&#33394;&#21160;&#21147;&#23398;&#20013;&#20351;&#29992;&#35268;&#33539;&#31561;&#21464;&#27744;&#21270;&#23618;&#21487;&#20197;&#36890;&#36807;&#20285;&#36797;&#37329;&#26500;&#36896;&#31895;&#32593;&#26684;&#35268;&#33539;&#22330;&#28040;&#38500;&#20020;&#30028;&#20943;&#24930;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#65292;&#35268;&#33539;&#31561;&#21464;&#30340;&#27744;&#21270;&#21644;&#21453;&#27744;&#21270;&#23618;&#22312;&#26684;&#28857;&#37327;&#23376;&#33394;&#21160;&#21147;&#23398;&#30340;&#22810;&#37325;&#32593;&#26684;&#39044;&#26465;&#20214;&#27169;&#22411;&#20013;&#21487;&#20197;&#19982;&#20256;&#32479;&#30340;&#38480;&#21046;&#21644;&#24310;&#25299;&#23618;&#19968;&#26679;&#26377;&#25928;&#12290;&#36825;&#20123;&#23618;&#22312;&#31895;&#32593;&#26684;&#19978;&#24341;&#20837;&#20102;&#35268;&#33539;&#33258;&#30001;&#24230;&#65292;&#20801;&#35768;&#22312;&#31895;&#32593;&#26684;&#19978;&#20351;&#29992;&#26126;&#30830;&#30340;&#35268;&#33539;&#31561;&#21464;&#23618;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#31895;&#31890;&#24230;&#35268;&#33539;&#22330;&#30340;&#26500;&#36896;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#39044;&#26465;&#20214;&#22120;&#27169;&#22411;&#20013;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#20285;&#36797;&#37329;&#26500;&#36896;&#31895;&#32593;&#26684;&#35268;&#33539;&#22330;&#30340;&#32452;&#21512;&#22810;&#37325;&#32593;&#26684;&#31070;&#32463;&#32593;&#32476;&#28040;&#38500;&#20102;&#20020;&#30028;&#20943;&#24930;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate that gauge-equivariant pooling and unpooling layers can perform as well as traditional restriction and prolongation layers in multigrid preconditioner models for lattice QCD. These layers introduce a gauge degree of freedom on the coarse grid, allowing for the use of explicitly gauge-equivariant layers on the coarse grid. We investigate the construction of coarse-grid gauge fields and study their efficiency in the preconditioner model. We show that a combined multigrid neural network using a Galerkin construction for the coarse-grid gauge field eliminates critical slowing down.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174; softmax &#21333;&#20803;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#26426;&#21046; inspired &#30340; softmax &#22238;&#24402;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#29992;&#20110;&#25511;&#21046;&#28508;&#22312;&#20989;&#25968;&#30340;&#36827;&#23637;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10411</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#26426;&#21046;&#30340; softmax &#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Attention Scheme Inspired Softmax Regression. (arXiv:2304.10411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174; softmax &#21333;&#20803;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#26426;&#21046; inspired &#30340; softmax &#22238;&#24402;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#29992;&#20110;&#25511;&#21046;&#28508;&#22312;&#20989;&#25968;&#30340;&#36827;&#23637;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#32473;&#20154;&#31867;&#31038;&#20250;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#21464;&#38761;&#12290;LLMs &#30340;&#20851;&#38190;&#35745;&#31639;&#20043;&#19968;&#26159; softmax &#21333;&#20803;&#12290;&#36825;&#20010;&#25805;&#20316;&#22312; LLMs &#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#27169;&#22411;&#22312;&#32473;&#23450;&#36755;&#20837;&#21333;&#35789;&#24207;&#21015;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#21487;&#33021;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#25110;&#30701;&#35821;&#30340;&#20998;&#24067;&#12290;&#36825;&#20010;&#20998;&#24067;&#28982;&#21518;&#29992;&#26469;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#25110;&#30701;&#35821;&#65292;&#22522;&#20110;&#27169;&#22411;&#20998;&#37197;&#30340;&#27010;&#29575;&#12290;softmax &#21333;&#20803;&#22312;&#35757;&#32451; LLMs &#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#27169;&#22411;&#36890;&#36807;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#24046;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#22312;&#20984;&#20248;&#21270;&#39046;&#22495;&#65292;&#20363;&#22914;&#20351;&#29992;&#20013;&#24515;&#36335;&#24452;&#27861;&#35299;&#20915;&#32447;&#24615;&#35268;&#21010;&#65292;softmax &#20989;&#25968;&#24050;&#32463;&#25104;&#20026;&#25511;&#21046;&#28508;&#22312;&#20989;&#25968;&#30340;&#36827;&#23637;&#21644;&#31283;&#23450;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174; softmax &#21333;&#20803;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010; softmax &#22238;&#24402;&#38382;&#39064;&#12290;&#24418;&#24335;&#19978;&#35762;&#65292;&#32473;&#23450;&#19968;&#20010;&#30697;&#38453; $A \in \mathbb{R}^{n \times d}$ &#21644;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made transformed changes for human society. One of the key computation in LLMs is the softmax unit. This operation is important in LLMs because it allows the model to generate a distribution over possible next words or phrases, given a sequence of input words. This distribution is then used to select the most likely next word or phrase, based on the probabilities assigned by the model. The softmax unit plays a crucial role in training LLMs, as it allows the model to learn from the data by adjusting the weights and biases of the neural network.  In the area of convex optimization such as using central path method to solve linear programming. The softmax function has been used a crucial tool for controlling the progress and stability of potential function [Cohen, Lee and Song STOC 2019, Brand SODA 2020].  In this work, inspired the softmax unit, we define a softmax regression problem. Formally speaking, given a matrix $A \in \mathbb{R}^{n \times d}$ and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; MLGCN&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#22330;&#26223;&#20013;&#21516;&#31867;&#20559;&#22909;&#30340;&#35821;&#20041;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MLGCN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10398</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-label Node Classification On Graph-Structured Data. (arXiv:2304.10398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; MLGCN&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#22330;&#26223;&#20013;&#21516;&#31867;&#20559;&#22909;&#30340;&#35821;&#20041;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MLGCN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#20013;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#25913;&#36827;&#12290;&#34429;&#28982;&#36825;&#20123;&#36827;&#23637;&#22312;&#22810;&#31867;&#20998;&#31867;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#23637;&#31034;&#65292;&#20294;&#19968;&#20010;&#26356;&#26222;&#36941;&#21644;&#29616;&#23454;&#30340;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#33410;&#28857;&#21487;&#33021;&#26377;&#22810;&#20010;&#26631;&#31614;&#65292;&#19968;&#30452;&#20197;&#26469;&#21463;&#21040;&#20102;&#24456;&#23569;&#20851;&#27880;&#12290;&#22312;&#36827;&#34892;&#20851;&#20110;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#30340;&#37325;&#28857;&#30740;&#31350;&#30340;&#39318;&#35201;&#25361;&#25112;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#26631;&#31614;&#22270;&#25968;&#25454;&#38598;&#25968;&#37327;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19977;&#20010;&#30495;&#23454;&#30340;&#29983;&#29289;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#26631;&#31614;&#22270;&#29983;&#25104;&#22120;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#21487;&#35843;&#23646;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;&#39640;&#26631;&#31614;&#30456;&#20284;&#24615;&#65288;&#39640;&#21516;&#31867;&#20559;&#22909;&#65289;&#36890;&#24120;&#34987;&#24402;&#22240;&#20110;GNN&#30340;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#22810;&#26631;&#31614;&#22330;&#26223;&#24182;&#19981;&#36981;&#24490;&#30446;&#21069;&#20026;&#22810;&#31867;&#22330;&#26223;&#23450;&#20041;&#30340;&#21516;&#31867;&#20559;&#22909;&#21644;&#24322;&#31867;&#20559;&#22909;&#30340;&#24120;&#35268;&#35821;&#20041;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#38500;&#20102;&#20026;&#22810;&#26631;&#31614;&#22330;&#26223;&#23450;&#20041;&#21516;&#31867;&#20559;&#22909;&#22806;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;MLGCN&#65288;&#22810;&#26631;&#31614;&#22270;&#21367;&#31215;&#32593;&#32476;&#65289;&#65292;&#26469;&#22788;&#29702;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MLGCN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have shown state-of-the-art improvements in node classification tasks on graphs. While these improvements have been largely demonstrated in a multi-class classification scenario, a more general and realistic scenario in which each node could have multiple labels has so far received little attention. The first challenge in conducting focused studies on multi-label node classification is the limited number of publicly available multi-label graph datasets. Therefore, as our first contribution, we collect and release three real-world biological datasets and develop a multi-label graph generator to generate datasets with tunable properties. While high label similarity (high homophily) is usually attributed to the success of GNNs, we argue that a multi-label scenario does not follow the usual semantics of homophily and heterophily so far defined for a multi-class scenario. As our second contribution, besides defining homophily for the multi-label scenario, we dev
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; C-qGAN &#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#37327;&#23376;&#30005;&#36335;&#32467;&#26500;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#29366;&#24577;&#20934;&#22791;&#36807;&#31243;&#65292;&#21487;&#20197;&#21033;&#29992;&#35813;&#26041;&#27861;&#21152;&#36895;&#33945;&#29305;&#21345;&#32599;&#20998;&#26512;&#31561;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20122;&#24335;&#26399;&#26435;&#34893;&#29983;&#21697;&#23450;&#20215;&#30340;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.10382</link><description>&lt;p&gt;
&#23398;&#20064;&#38543;&#26426;&#36807;&#31243;&#30340;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditional Generative Models for Learning Stochastic Processes. (arXiv:2304.10382v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10382
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; C-qGAN &#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#37327;&#23376;&#30005;&#36335;&#32467;&#26500;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#29366;&#24577;&#20934;&#22791;&#36807;&#31243;&#65292;&#21487;&#20197;&#21033;&#29992;&#35813;&#26041;&#27861;&#21152;&#36895;&#33945;&#29305;&#21345;&#32599;&#20998;&#26512;&#31561;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20122;&#24335;&#26399;&#26435;&#34893;&#29983;&#21697;&#23450;&#20215;&#30340;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26465;&#20214;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;C-qGAN&#65289;&#12290;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20005;&#26684;&#37319;&#29992;&#37327;&#23376;&#30005;&#36335;&#65292;&#22240;&#27492;&#34987;&#35777;&#26126;&#33021;&#22815;&#27604;&#24403;&#21069;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#29366;&#24577;&#20934;&#22791;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#28508;&#21147;&#21152;&#36895;&#33945;&#29305;&#21345;&#32599;&#20998;&#26512;&#31561;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#23637;&#31034;&#20102;&#32593;&#32476;&#22312;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21518;&#65292;&#23558;&#35813;&#25216;&#26415;&#24212;&#29992;&#20110;&#23450;&#20215;&#20122;&#24335;&#26399;&#26435;&#34893;&#29983;&#21697;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#20854;&#20182;&#36335;&#24452;&#30456;&#20851;&#26399;&#26435;&#25171;&#19979;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
A framework to learn a multi-modal distribution is proposed, denoted as the Conditional Quantum Generative Adversarial Network (C-qGAN). The neural network structure is strictly within a quantum circuit and, as a consequence, is shown to represents a more efficient state preparation procedure than current methods. This methodology has the potential to speed-up algorithms, such as Monte Carlo analysis. In particular, after demonstrating the effectiveness of the network in the learning task, the technique is applied to price Asian option derivatives, providing the foundation for further research on other path-dependent options.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#28436;&#21592;&#26550;&#26500;&#65292;&#21033;&#29992;&#26174;&#33879;&#24615;&#21521;&#37327;&#37325;&#29992;&#29615;&#22659;&#30340;&#26465;&#20214;&#29366;&#24577;&#26469;&#25552;&#39640;&#26465;&#20214;&#21327;&#21516;&#34892;&#20026;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20195;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10375</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#26377;&#26465;&#20214;&#21327;&#21516;&#34892;&#20026;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Interpretability for Conditional Coordinated Behavior in Multi-Agent Reinforcement Learning. (arXiv:2304.10375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10375
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#28436;&#21592;&#26550;&#26500;&#65292;&#21033;&#29992;&#26174;&#33879;&#24615;&#21521;&#37327;&#37325;&#29992;&#29615;&#22659;&#30340;&#26465;&#20214;&#29366;&#24577;&#26469;&#25552;&#39640;&#26465;&#20214;&#21327;&#21516;&#34892;&#20026;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20195;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26550;&#26500;&#65292;&#31216;&#20026;&#22522;&#20110;&#26465;&#20214;&#27880;&#24847;&#21147;(DA6-X)&#30340;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#28436;&#21592;&#26550;&#26500;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#26465;&#20214;&#21327;&#21516;&#34892;&#20026;&#21487;&#35299;&#37322;&#24615;&#12290;&#20854;&#22522;&#26412;&#21407;&#29702;&#28041;&#21450;&#37325;&#29992;&#26174;&#33879;&#24615;&#21521;&#37327;&#65292;&#35813;&#21521;&#37327;&#34920;&#31034;&#29615;&#22659;&#30340;&#26465;&#20214;&#29366;&#24577;&#65292;&#20363;&#22914;&#20195;&#29702;&#30340;&#20840;&#23616;&#20301;&#32622;&#12290;&#22240;&#27492;&#65292;&#20855;&#26377;&#23884;&#20837;&#20854;&#31574;&#30053;&#20013;&#30340;DA6-X&#28789;&#27963;&#24615;&#30340;&#20195;&#29702;&#36890;&#36807;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#32771;&#34385;&#26465;&#20214;&#29366;&#24577;&#20013;&#30340;&#38468;&#21152;&#20449;&#24687;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#23545;&#35937;&#25910;&#38598;&#28216;&#25103;&#20013;&#23558;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;DA6-X&#30340;&#27880;&#24847;&#26435;&#37325;&#65292;&#25105;&#20204;&#30830;&#35748;&#20195;&#29702;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#24773;&#22659;&#20381;&#36182;&#24615;&#21327;&#21516;&#34892;&#20026;&#65292;&#36890;&#36807;&#27491;&#30830;&#35782;&#21035;&#21508;&#31181;&#26465;&#20214;&#29366;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model-free reinforcement learning architecture, called distributed attentional actor architecture after conditional attention (DA6-X), to provide better interpretability of conditional coordinated behaviors. The underlying principle involves reusing the saliency vector, which represents the conditional states of the environment, such as the global position of agents. Hence, agents with DA6-X flexibility built into their policy exhibit superior performance by considering the additional information in the conditional states during the decision-making process. The effectiveness of the proposed method was experimentally evaluated by comparing it with conventional methods in an objects collection game. By visualizing the attention weights from DA6-X, we confirmed that agents successfully learn situation-dependent coordinated behaviors by correctly identifying various conditional states, leading to improved interpretability of agents along with superior performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24102;&#20551;&#35774;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#65288;NSRwH&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#23450;&#20041;&#30340;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#21040;&#29983;&#25104;&#20998;&#26512;&#34920;&#36798;&#24335;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#30446;&#26631;&#34920;&#36798;&#24335;&#20013;&#20351;&#29992;&#30340;&#25968;&#23398;&#31526;&#21495;&#31867;&#22411;&#21644;&#33539;&#22260;&#65292;&#24182;&#23545;&#34920;&#36798;&#24335;&#30340;&#22797;&#26434;&#24615;&#26045;&#21152;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2304.10336</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Controllable Neural Symbolic Regression. (arXiv:2304.10336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24102;&#20551;&#35774;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#65288;NSRwH&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#29992;&#25143;&#23450;&#20041;&#30340;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#21040;&#29983;&#25104;&#20998;&#26512;&#34920;&#36798;&#24335;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#30446;&#26631;&#34920;&#36798;&#24335;&#20013;&#20351;&#29992;&#30340;&#25968;&#23398;&#31526;&#21495;&#31867;&#22411;&#21644;&#33539;&#22260;&#65292;&#24182;&#23545;&#34920;&#36798;&#24335;&#30340;&#22797;&#26434;&#24615;&#26045;&#21152;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31526;&#21495;&#22238;&#24402;&#20013;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#20998;&#26512;&#34920;&#36798;&#24335;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#25968;&#23398;&#31526;&#21495;&#65288;&#20363;&#22914;&#36816;&#31639;&#31526;&#12289;&#21464;&#37327;&#21644;&#24120;&#37327;&#65289;&#30340;&#24773;&#20917;&#19979;&#31934;&#30830;&#22320;&#25311;&#21512;&#23454;&#39564;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#33021;&#34920;&#36798;&#24335;&#30340;&#32452;&#21512;&#31354;&#38388;&#24456;&#22823;&#65292;&#20256;&#32479;&#36827;&#21270;&#31639;&#27861;&#24456;&#38590;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#25214;&#21040;&#27491;&#30830;&#30340;&#34920;&#36798;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#65288;NSR&#65289;&#31639;&#27861;&#65292;&#33021;&#22815;&#24555;&#36895;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#24182;&#29983;&#25104;&#20998;&#26512;&#34920;&#36798;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30446;&#21069;&#36824;&#32570;&#20047;&#23558;&#29992;&#25143;&#23450;&#20041;&#30340;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#20854;&#20013;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#31181;&#33021;&#21147;&#22312;&#33258;&#28982;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#36890;&#24120;&#26159;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21517;&#20026;&#24102;&#20551;&#35774;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#65288;NSRwH&#65289;&#65292;&#23427;&#20801;&#35768;&#26174;&#24335;&#22320;&#23558;&#20851;&#20110;&#30495;&#23454;&#34920;&#36798;&#24335;&#26399;&#26395;&#32467;&#26500;&#30340;&#20551;&#35774;&#32435;&#20837;&#39044;&#27979;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In symbolic regression, the goal is to find an analytical expression that accurately fits experimental data with the minimal use of mathematical symbols such as operators, variables, and constants. However, the combinatorial space of possible expressions can make it challenging for traditional evolutionary algorithms to find the correct expression in a reasonable amount of time. To address this issue, Neural Symbolic Regression (NSR) algorithms have been developed that can quickly identify patterns in the data and generate analytical expressions. However, these methods, in their current form, lack the capability to incorporate user-defined prior knowledge, which is often required in natural sciences and engineering fields. To overcome this limitation, we propose a novel neural symbolic regression method, named Neural Symbolic Regression with Hypothesis (NSRwH) that enables the explicit incorporation of assumptions about the expected structure of the ground-truth expression into the pre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#22270;&#24418;&#34920;&#31034;&#20013;&#34920;&#36848;&#23398;&#20064;&#34562;&#31389;&#32593;&#32476;&#35206;&#30422;&#30340;&#20219;&#21153;&#65292;&#24212;&#29992;&#20102;&#26368;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26694;&#26550;&#26082;&#21487;&#20197;&#29983;&#25104;&#22810;&#20010;KPI&#30340;&#36136;&#37327;&#21333;&#20803;&#26684;&#37197;&#32622;&#23884;&#20837;&#65292;&#21516;&#26102;&#33021;&#22815;&#25512;&#24191;&#21040;&#22823;&#22411;&#65288;&#38754;&#21521;&#21306;&#22495;&#65289;&#22330;&#26223;&#65292;&#32473;&#20986;&#38750;&#24120;&#23569;&#30340;&#26631;&#35760;&#21333;&#20803;&#26684;&#12290;</title><link>http://arxiv.org/abs/2304.10328</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20174;&#23454;&#38469;&#32593;&#32476;&#37197;&#32622;&#20013;&#23398;&#20064;&#34562;&#31389;&#32593;&#32476;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
Learning Cellular Coverage from Real Network Configurations using GNNs. (arXiv:2304.10328v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#22270;&#24418;&#34920;&#31034;&#20013;&#34920;&#36848;&#23398;&#20064;&#34562;&#31389;&#32593;&#32476;&#35206;&#30422;&#30340;&#20219;&#21153;&#65292;&#24212;&#29992;&#20102;&#26368;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26694;&#26550;&#26082;&#21487;&#20197;&#29983;&#25104;&#22810;&#20010;KPI&#30340;&#36136;&#37327;&#21333;&#20803;&#26684;&#37197;&#32622;&#23884;&#20837;&#65292;&#21516;&#26102;&#33021;&#22815;&#25512;&#24191;&#21040;&#22823;&#22411;&#65288;&#38754;&#21521;&#21306;&#22495;&#65289;&#22330;&#26223;&#65292;&#32473;&#20986;&#38750;&#24120;&#23569;&#30340;&#26631;&#35760;&#21333;&#20803;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34562;&#31389;&#35206;&#30422;&#36136;&#37327;&#20272;&#35745;&#26159;&#33258;&#32452;&#32455;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#22312;&#32593;&#32476;&#35774;&#35745;&#21644;&#20248;&#21270;&#26399;&#38388;&#24456;&#23569;&#25552;&#20379;&#30830;&#23450;&#30340;&#22522;&#30784;&#20107;&#23454;&#65292;&#28145;&#24230;&#23398;&#20064;&#25903;&#25345;&#30340;&#35206;&#30422;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#38754;&#31215;&#65292;&#27492;&#22806;&#65292;&#20182;&#20204;&#26080;&#27861;&#29983;&#25104;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#23884;&#20837;&#20197;&#20805;&#20998;&#25429;&#25417;&#21333;&#20803;&#26684;&#37197;&#32622;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#22270;&#24418;&#34920;&#31034;&#20013;&#34920;&#36848;&#35813;&#20219;&#21153;&#65292;&#20197;&#20415;&#25105;&#20204;&#21487;&#20197;&#24212;&#29992;&#34920;&#29616;&#20986;&#33394;&#30340;&#26368;&#26032;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#26082;&#21487;&#20197;&#29983;&#25104;&#22810;&#20010;KPI&#30340;&#36136;&#37327;&#21333;&#20803;&#26684;&#37197;&#32622;&#23884;&#20837;&#65292;&#21516;&#26102;&#25105;&#20204;&#26174;&#31034;&#23427;&#33021;&#22815;&#25512;&#24191;&#21040;&#22823;&#22411;&#65288;&#38754;&#21521;&#21306;&#22495;&#65289;&#22330;&#26223;&#65292;&#32473;&#20986;&#38750;&#24120;&#23569;&#30340;&#26631;&#35760;&#21333;&#20803;&#26684;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#20351;&#29992;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular coverage quality estimation has been a critical task for self-organized networks. In real-world scenarios, deep-learning-powered coverage quality estimation methods cannot scale up to large areas due to little ground truth can be provided during network design &amp; optimization. In addition they fall short in produce expressive embeddings to adequately capture the variations of the cells' configurations. To deal with this challenge, we formulate the task in a graph representation and so that we can apply state-of-the-art graph neural networks, that show exemplary performance. We propose a novel training framework that can both produce quality cell configuration embeddings for estimating multiple KPIs, while we show it is capable of generalising to large (area-wide) scenarios given very few labeled cells. We show that our framework yields comparable accuracy with models that have been trained using massively labeled samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;ADAM&#21644;RMSprop&#30340;&#20108;&#38454;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;GAN&#65292;&#22312;&#19981;&#38656;&#35201;&#35299;&#20915;&#32447;&#24615;&#31995;&#32479;&#25110;&#28151;&#21512;&#20108;&#38454;&#23548;&#25968;&#39033;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#26356;&#24555;&#30340;&#30456;&#20284;&#31934;&#24230;&#65292;&#20135;&#29983;&#20102;&#26356;&#22909;&#25110;&#21487;&#27604;&#30340;Inception&#20998;&#25968;&#21644;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#19982;&#19968;&#38454;&#26041;&#27861;&#30456;&#27604;&#20135;&#29983;&#20102;&#26174;&#33879;&#26356;&#22909;&#30340;Inception&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.10317</link><description>&lt;p&gt;
GAN&#30340;&#33258;&#36866;&#24212;&#20849;&#35782;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive Consensus Optimization Method for GANs. (arXiv:2304.10317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;ADAM&#21644;RMSprop&#30340;&#20108;&#38454;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;GAN&#65292;&#22312;&#19981;&#38656;&#35201;&#35299;&#20915;&#32447;&#24615;&#31995;&#32479;&#25110;&#28151;&#21512;&#20108;&#38454;&#23548;&#25968;&#39033;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#26356;&#24555;&#30340;&#30456;&#20284;&#31934;&#24230;&#65292;&#20135;&#29983;&#20102;&#26356;&#22909;&#25110;&#21487;&#27604;&#30340;Inception&#20998;&#25968;&#21644;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#19982;&#19968;&#38454;&#26041;&#27861;&#30456;&#27604;&#20135;&#29983;&#20102;&#26174;&#33879;&#26356;&#22909;&#30340;Inception&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ADAM&#21644;RMSprop&#30340;&#20108;&#38454;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#22312;&#33719;&#24471;&#30456;&#20284;&#31934;&#24230;&#26102;&#27604;&#20854;&#20182;&#33879;&#21517;&#30340;&#20108;&#38454;&#26041;&#27861;&#26356;&#24555;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#38656;&#35201;&#35299;&#20915;&#32447;&#24615;&#31995;&#32479;&#65292;&#20063;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#28151;&#21512;&#20108;&#38454;&#23548;&#25968;&#39033;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#23545;&#24212;&#20110;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#22266;&#23450;&#28857;&#36845;&#20195;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#19982;&#20854;&#20182;&#26368;&#26032;&#25552;&#20986;&#30340;&#26368;&#20808;&#36827;&#30340;&#20108;&#38454;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#26356;&#22909;&#25110;&#21487;&#27604;&#30340;Inception&#20998;&#25968;&#65292;&#20197;&#21450;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;&#19982;&#19968;&#38454;&#26041;&#27861;&#65288;&#22914;ADAM&#65289;&#30456;&#27604;&#65292;&#23427;&#20135;&#29983;&#20102;&#26174;&#33879;&#26356;&#22909;&#30340;Inception&#20998;&#25968;&#12290;&#35813;&#26041;&#27861;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#22914;FFHQ&#12289;LSUN&#12289;CIFAR10&#12289;MNIST&#21644;Fashion MNIST&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#39564;&#35777;\footnote{&#24050;&#34987;IJCNN 2023&#25509;&#21463;}&#12290;&#20195;&#30721;&#65306;\url{https://github.com/misterpawan/acom}
&lt;/p&gt;
&lt;p&gt;
We propose a second order gradient based method with ADAM and RMSprop for the training of generative adversarial networks. The proposed method is fastest to obtain similar accuracy when compared to prominent second order methods. Unlike state-of-the-art recent methods, it does not require solving a linear system, or it does not require additional mixed second derivative terms. We derive the fixed point iteration corresponding to proposed method, and show that the proposed method is convergent. The proposed method produces better or comparable inception scores, and comparable quality of images compared to other recently proposed state-of-the-art second order methods. Compared to first order methods such as ADAM, it produces significantly better inception scores. The proposed method is compared and validated on popular datasets such as FFHQ, LSUN, CIFAR10, MNIST, and Fashion MNIST for image generation tasks\footnote{Accepted in IJCNN 2023}. Codes: \url{https://github.com/misterpawan/acom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#33258;&#30417;&#30563;&#21644;&#35270;&#35273;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#23545;&#30005;&#24433;&#31080;&#25151;&#36827;&#34892;&#39044;&#27979;&#65292;&#23545;&#20855;&#26377;&#20869;&#23481;&#20851;&#38190;&#35789;&#30340;&#30005;&#24433;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.10311</link><description>&lt;p&gt;
&#37319;&#29992;&#33258;&#30417;&#30563;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#30005;&#24433;&#31080;&#25151;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Movie Box Office Prediction With Self-Supervised and Visually Grounded Pretraining. (arXiv:2304.10311v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#33258;&#30417;&#30563;&#21644;&#35270;&#35273;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#23545;&#30005;&#24433;&#31080;&#25151;&#36827;&#34892;&#39044;&#27979;&#65292;&#23545;&#20855;&#26377;&#20869;&#23481;&#20851;&#38190;&#35789;&#30340;&#30005;&#24433;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#24433;&#25237;&#36164;&#39118;&#38505;&#36739;&#39640;&#65292;&#22240;&#20026;&#30005;&#24433;&#31080;&#25151;&#25910;&#20837;&#21576;&#29616;&#38271;&#23614;&#21644;&#21452;&#23792;&#20998;&#24067;&#12290;&#20934;&#30830;&#39044;&#27979;&#30005;&#24433;&#31080;&#25151;&#25910;&#20837;&#21487;&#20197;&#20943;&#36731;&#19981;&#30830;&#23450;&#24615;&#24182;&#40723;&#21169;&#25237;&#36164;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#23545;&#28436;&#21592;&#12289;&#23548;&#28436;&#21644;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30456;&#20851;&#20851;&#38190;&#35789;&#30340;&#26377;&#25928;&#34920;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#30005;&#24433;&#28023;&#25253;&#20013;&#30340;&#23545;&#35937;&#24320;&#22987;&#23454;&#29616;&#35270;&#35273;&#30456;&#20851;&#20851;&#38190;&#35789;&#20316;&#20026;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;35,794&#37096;&#30005;&#24433;&#30340;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#30417;&#30563;&#35757;&#32451;&#21644;&#35270;&#35273;&#30456;&#20851;&#20851;&#38190;&#35789;&#30340;&#39044;&#35757;&#32451;&#37117;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#22909;&#22788;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20855;&#26377;&#20869;&#23481;&#20851;&#38190;&#35789;&#30340;&#30005;&#24433;&#19978;&#36827;&#34892;&#35270;&#35273;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#65292;&#30456;&#23545;&#20110;&#20855;&#26377;&#30456;&#21516;&#26550;&#26500;&#30340;&#24494;&#35843;BERT&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;14.5%&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investments in movie production are associated with a high level of risk as movie revenues have long-tailed and bimodal distributions. Accurate prediction of box-office revenue may mitigate the uncertainty and encourage investment. However, learning effective representations for actors, directors, and user-generated content-related keywords remains a challenging open problem. In this work, we investigate the effects of self-supervised pretraining and propose visual grounding of content keywords in objects from movie posters as a pertaining objective. Experiments on a large dataset of 35,794 movies demonstrate significant benefits of self-supervised training and visual grounding. In particular, visual grounding pretraining substantially improves learning on movies with content keywords and achieves 14.5% relative performance gains compared to a finetuned BERT model with identical architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26032;&#22411;&#30340;&#21160;&#24577;&#20998;&#37197;&#38382;&#39064;&#8212;&#8212;&#20572;&#27490;&#36172;&#21338;&#26426;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#23545;&#20110;&#32463;&#20856;&#30340;Gittins&#25351;&#25968;&#20998;&#35299;&#32467;&#26524;&#21644;&#26368;&#26032;&#32467;&#26524;&#30340;&#26032;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2304.10302</link><description>&lt;p&gt;
&#20572;&#27490;&#22810;&#33218;&#36172;&#21338;&#26426;&#27169;&#22411;&#30340;&#26368;&#20248;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Activation of Halting Multi-Armed Bandit Models. (arXiv:2304.10302v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26032;&#22411;&#30340;&#21160;&#24577;&#20998;&#37197;&#38382;&#39064;&#8212;&#8212;&#20572;&#27490;&#36172;&#21338;&#26426;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#23545;&#20110;&#32463;&#20856;&#30340;Gittins&#25351;&#25968;&#20998;&#35299;&#32467;&#26524;&#21644;&#26368;&#26032;&#32467;&#26524;&#30340;&#26032;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21160;&#24577;&#20998;&#37197;&#38382;&#39064;&#8212;&#8212;&#20572;&#27490;&#36172;&#21338;&#26426;&#27169;&#22411;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#23545;&#20110;&#32463;&#20856;&#30340;Gittins&#25351;&#25968;&#20998;&#35299;&#32467;&#26524;&#21644;&#20316;&#32773;&#22312;&#8220;&#26222;&#36941;&#25240;&#26087;&#21644;&#25215;&#35834;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#8221;&#30340;&#26368;&#26032;&#32467;&#26524;&#30340;&#26032;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study new types of dynamic allocation problems the {\sl Halting Bandit} models. As an application, we obtain new proofs for the classic Gittins index decomposition result and recent results of the authors in `Multi-armed bandits under general depreciation and commitment.'
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;SARF&#65292;&#36890;&#36807;&#21033;&#29992;&#21516;&#20041;&#20851;&#31995;&#36741;&#21161;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#20851;&#31995;&#25512;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#65292;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#36229;&#36234;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10297</link><description>&lt;p&gt;
SARF: &#21033;&#29992;&#21516;&#20041;&#20851;&#31995;&#36741;&#21161;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SARF: Aliasing Relation Assisted Self-Supervised Learning for Few-shot Relation Reasoning. (arXiv:2304.10297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;SARF&#65292;&#36890;&#36807;&#21033;&#29992;&#21516;&#20041;&#20851;&#31995;&#36741;&#21161;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#20851;&#31995;&#25512;&#29702;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#65292;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#36229;&#36234;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#38024;&#23545;&#23569;&#37327;&#25968;&#25454;&#12289;&#38271;&#23614;&#30340;&#20851;&#31995;&#25512;&#29702;&#65288;FS-KGR&#65289;&#36817;&#24180;&#26469;&#22240;&#20854;&#23454;&#29992;&#24615;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#25163;&#21160;&#26500;&#24314;&#20803;&#20851;&#31995;&#38598;&#26469;&#39044;&#35757;&#32451;&#65292;&#23548;&#33268;&#20102;&#22823;&#37327;&#30340;&#21171;&#21160;&#25104;&#26412;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#34987;&#35270;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#20294;&#22312;FS-KGR&#20219;&#21153;&#20013;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#21033;&#29992;&#19982;&#30446;&#26631;&#25968;&#25454;&#31232;&#23569;&#20851;&#31995;&#20855;&#26377;&#31867;&#20284;&#19978;&#19979;&#25991;&#35821;&#20041;&#30340;&#21516;&#20041;&#20851;&#31995;&#65288;AR&#65289;&#30340;&#26377;&#30410;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21516;&#20041;&#20851;&#31995;&#36741;&#21161;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;SARF&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#35774;&#35745;&#20102;&#22235;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;SSL&#25512;&#29702;&#27169;&#22359;&#12289;AR&#36741;&#21161;&#26426;&#21046;&#12289;&#34701;&#21512;&#27169;&#22359;&#21644;&#35780;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#39318;&#20808;&#20197;&#29983;&#25104;&#24335;&#30340;&#26041;&#24335;&#29983;&#25104;&#20849;&#29616;&#27169;&#24335;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#35774;&#35745;AR&#36741;&#21161;&#26426;&#21046;&#26469;&#25429;&#25417;&#25968;&#25454;&#31232;&#23569;&#20851;&#31995;&#30340;&#30456;&#20284;&#19978;&#19979;&#25991;&#35821;&#20041;&#12290;&#28982;&#21518;&#65292;&#36825;&#20004;&#20010;&#34920;&#31034;&#34987;&#34701;&#21512;&#36215;&#26469;&#29983;&#25104;&#32508;&#21512;&#29305;&#24449;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#35780;&#20998;&#20989;&#25968;&#26469;&#39044;&#27979;&#22522;&#20110;&#27492;&#29305;&#24449;&#34920;&#31034;&#30340;&#30446;&#26631;&#20851;&#31995;&#12290;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SARF&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot relation reasoning on knowledge graphs (FS-KGR) aims to infer long-tail data-poor relations, which has drawn increasing attention these years due to its practicalities. The pre-training of previous methods needs to manually construct the meta-relation set, leading to numerous labor costs. Self-supervised learning (SSL) is treated as a solution to tackle the issue, but still at an early stage for FS-KGR task. Moreover, most of the existing methods ignore leveraging the beneficial information from aliasing relations (AR), i.e., data-rich relations with similar contextual semantics to the target data-poor relation. Therefore, we proposed a novel Self-Supervised Learning model by leveraging Aliasing Relations to assist FS-KGR, termed SARF. Concretely, four main components are designed in our model, i.e., SSL reasoning module, AR-assisted mechanism, fusion module, and scoring function. We first generate the representation of the co-occurrence patterns in a generative manner. Meanwh
&lt;/p&gt;</description></item><item><title>OptoGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#20165;&#21253;&#21547;&#35299;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#20027;&#20840;&#23616;&#35774;&#35745;&#25506;&#32034;&#65292;&#21516;&#26102;&#36873;&#25321;&#26448;&#26009;&#21644;&#21402;&#24230;&#65292;&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#21453;&#21521;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.10294</link><description>&lt;p&gt;
OptoGPT&#65306;&#19968;&#31181;&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#21453;&#21521;&#35774;&#35745;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OptoGPT: A Foundation Model for Inverse Design in Optical Multilayer Thin Film Structures. (arXiv:2304.10294v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10294
&lt;/p&gt;
&lt;p&gt;
OptoGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#20165;&#21253;&#21547;&#35299;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#20027;&#20840;&#23616;&#35774;&#35745;&#25506;&#32034;&#65292;&#21516;&#26102;&#36873;&#25321;&#26448;&#26009;&#21644;&#21402;&#24230;&#65292;&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#21453;&#21521;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#21363;&#21487;&#35299;&#20915;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#24341;&#39046;&#30740;&#31350;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#36866;&#29992;&#20110;&#20809;&#23398;&#22810;&#23618;&#34180;&#33180;&#32467;&#26500;&#21453;&#21521;&#35774;&#35745;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#24403;&#21069;&#30340;&#21453;&#21521;&#35774;&#35745;&#31639;&#27861;&#35201;&#20040;&#19981;&#33021;&#25506;&#32034;&#20840;&#23616;&#35774;&#35745;&#31354;&#38388;&#65292;&#35201;&#20040;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Opto Generative Pretrained Transformer&#65288;OptoGPT&#65289;&#12290;OptoGPT&#26159;&#19968;&#20010;&#20165;&#21253;&#21547;&#35299;&#30721;&#22120;&#30340;Transformer&#65292;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#30340;&#39057;&#35889;&#30446;&#26631;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;&#35774;&#35745;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#32452;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;1000&#19975;&#20010;&#35774;&#35745;&#65289;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;: 1&#65289;&#33258;&#20027;&#20840;&#23616;&#35774;&#35745;&#25506;&#32034;&#65292;&#36890;&#36807;&#30830;&#23450;&#23618;&#25968;&#65288;&#39640;&#36798;20&#23618;&#65289;&#65292;&#21516;&#26102;&#36873;&#25321;&#27599;&#20010;&#23618;&#30340;&#26448;&#26009;&#65288;&#39640;&#36798;18&#31181;&#19981;&#21516;&#31867;&#22411;&#65289;&#21644;&#21402;&#24230;&#65307;2&#65289;&#39640;&#25928;&#30340;&#32467;&#26500;&#39068;&#33394;&#35774;&#35745;&#65292;&#21560;&#25910;&#22120;&#65292;&#28388;&#27874;&#22120;&#65292;&#20998;&#24067;&#21453;&#23556;&#38236;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are large machine learning models that can tackle various downstream tasks once trained on diverse and large-scale data, leading research trends in natural language processing, computer vision, and reinforcement learning. However, no foundation model exists for optical multilayer thin film structure inverse design. Current inverse design algorithms either fail to explore the global design space or suffer from low computational efficiency. To bridge this gap, we propose the Opto Generative Pretrained Transformer (OptoGPT). OptoGPT is a decoder-only transformer that auto-regressively generates designs based on specific spectrum targets. Trained on a large dataset of 10 million designs, our model demonstrates remarkable capabilities: 1) autonomous global design exploration by determining the number of layers (up to 20) while selecting the material (up to 18 distinct types) and thickness at each layer, 2) efficient designs for structural color, absorbers, filters, distrib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#31616;&#21333;&#30340;&#24341;&#23548;&#21453;&#39304;&#25511;&#21046;&#22120;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#35774;&#23450;&#28857;&#25511;&#21046;&#38382;&#39064;&#30340;&#26356;&#22909;&#28608;&#21169;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#20811;&#26381;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2304.10289</link><description>&lt;p&gt;
&#36741;&#21161;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#35774;&#23450;&#28857;&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Aiding reinforcement learning for set point control. (arXiv:2304.10289v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#31616;&#21333;&#30340;&#24341;&#23548;&#21453;&#39304;&#25511;&#21046;&#22120;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#35774;&#23450;&#28857;&#25511;&#21046;&#38382;&#39064;&#30340;&#26356;&#22909;&#28608;&#21169;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#20811;&#26381;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#24456;&#22810;&#39046;&#22495;&#33719;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#30340;&#31639;&#27861;&#20173;&#28982;&#19981;&#33021;&#24456;&#22909;&#22320;&#35299;&#20915;&#30475;&#20284;&#31616;&#21333;&#30340;&#35774;&#23450;&#28857;&#21453;&#39304;&#25511;&#21046;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#65292;&#23398;&#24471;&#30340;&#25511;&#21046;&#22120;&#21487;&#33021;&#26080;&#27861;&#22312;&#21021;&#22987;&#38454;&#27573;&#20805;&#20998;&#22320;&#28608;&#21169;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#22240;&#27492;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#25165;&#33021;&#33719;&#24471;&#36275;&#22815;&#20449;&#24687;&#29992;&#20110;&#23398;&#20064;&#20986;&#36739;&#22909;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22312;&#20110;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#31616;&#21333;&#30340;&#24341;&#23548;&#21453;&#39304;&#25511;&#21046;&#22120;&#65288;&#20363;&#22914;&#27604;&#20363;&#25511;&#21046;&#22120;&#65289;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#22312;&#35774;&#23450;&#28857;&#25511;&#21046;&#38382;&#39064;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#28608;&#21169;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#36825;&#22312;&#24555;&#36895;&#20934;&#30830;&#30340;&#23454;&#26102;&#25511;&#21046;&#29615;&#22659;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32463;&#36807;&#27169;&#25311;&#21644;&#23454;&#38469;&#30340;&#21452;&#27700;&#31665;&#31995;&#32479;&#27979;&#35797;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning has made great improvements, state-of-the-art algorithms can still struggle with seemingly simple set-point feedback control problems. One reason for this is that the learned controller may not be able to excite the system dynamics well enough initially, and therefore it can take a long time to get data that is informative enough to learn for good control. The paper contributes by augmentation of reinforcement learning with a simple guiding feedback controller, for example, a proportional controller. The key advantage in set point control is a much improved excitation that improves the convergence properties of the reinforcement learning controller significantly. This can be very important in real-world control where quick and accurate convergence is needed. The proposed method is evaluated with simulation and on a real-world double tank process with promising results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20197;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#20915;&#31574;&#37117;&#20114;&#30456;&#20851;&#32852;&#30340;&#22240;&#32032;&#26469;&#34920;&#24449;&#19968;&#20010;&#23454;&#20363;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#20272;&#35745;&#20998;&#31867;&#38169;&#35823;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.10284</link><description>&lt;p&gt;
&#19968;&#31181;&#20272;&#31639;&#24182;&#35299;&#37322;&#20998;&#31867;&#22120;&#19981;&#30830;&#23450;&#24615;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Meta-heuristic Approach to Estimate and Explain Classifier Uncertainty. (arXiv:2304.10284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20197;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#20915;&#31574;&#37117;&#20114;&#30456;&#20851;&#32852;&#30340;&#22240;&#32032;&#26469;&#34920;&#24449;&#19968;&#20010;&#23454;&#20363;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#20272;&#35745;&#20998;&#31867;&#38169;&#35823;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#26159;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37319;&#29992;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#23450;&#24615;&#30740;&#31350;&#34920;&#26126;&#65292;&#32456;&#31471;&#29992;&#25143;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#38656;&#35201;&#33021;&#22815;&#22312;&#20915;&#31574;&#26102;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#27169;&#22411;&#65292;&#20197;&#20351;&#29992;&#25143;&#30693;&#36947;&#20309;&#26102;&#24573;&#30053;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37327;&#21270;&#20915;&#31574;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#19981;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#23601;&#26159;&#20381;&#36182;&#20110;&#19981;&#23481;&#26131;&#35753;&#26222;&#36890;&#20154;&#25110;&#32456;&#31471;&#29992;&#25143;&#29702;&#35299;&#30340;&#22797;&#26434;&#32479;&#35745;&#25512;&#23548;&#65292;&#36825;&#20351;&#23427;&#20204;&#22312;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#26102;&#19981;&#22826;&#26377;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#31867;&#29420;&#31435;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#20915;&#31574;&#37117;&#20114;&#30456;&#20851;&#32852;&#30340;&#22240;&#32032;&#26469;&#34920;&#24449;&#19968;&#20010;&#23454;&#20363;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#20123;&#24230;&#37327;&#34987;&#38598;&#25104;&#21040;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#35813;&#26694;&#26550;&#20272;&#35745;&#20102;&#20998;&#31867;&#38169;&#35823;&#39118;&#38505;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#37492;&#21035;&#37027;&#20123;&#26377;&#21487;&#33021;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#23454;&#20363;&#26041;&#38754;&#65292;&#34920;&#29616;&#20248;&#20110;&#39044;&#27979;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust is a crucial factor affecting the adoption of machine learning (ML) models. Qualitative studies have revealed that end-users, particularly in the medical domain, need models that can express their uncertainty in decision-making allowing users to know when to ignore the model's recommendations. However, existing approaches for quantifying decision-making uncertainty are not model-agnostic, or they rely on complex statistical derivations that are not easily understood by laypersons or end-users, making them less useful for explaining the model's decision-making process. This work proposes a set of class-independent meta-heuristics that can characterize the complexity of an instance in terms of factors are mutually relevant to both human and ML decision-making. The measures are integrated into a meta-learning framework that estimates the risk of misclassification. The proposed framework outperformed predicted probabilities in identifying instances at risk of being misclassified. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#24605;&#24819;&#26469;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#20808;&#21069;&#21453;&#39304;&#25511;&#21046;&#22120;&#26469;&#24110;&#21161;&#25391;&#24133;&#25506;&#32034;&#65292;&#20351;&#29992;&#31215;&#20998;&#35823;&#24046;&#65292;&#23545;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#24819;&#27861;&#21487;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#65292;&#20351;&#24471;&#35757;&#32451;&#20986;&#26469;&#30340;&#35774;&#23450;&#28857;&#25511;&#21046;&#22120;&#26356;&#21152;&#40065;&#26834;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#30495;&#23454;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.10277</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#38750;&#32447;&#24615;&#35774;&#23450;&#28857;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Robust nonlinear set-point control with reinforcement learning. (arXiv:2304.10277v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#24605;&#24819;&#26469;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#20808;&#21069;&#21453;&#39304;&#25511;&#21046;&#22120;&#26469;&#24110;&#21161;&#25391;&#24133;&#25506;&#32034;&#65292;&#20351;&#29992;&#31215;&#20998;&#35823;&#24046;&#65292;&#23545;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#24819;&#27861;&#21487;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#65292;&#20351;&#24471;&#35757;&#32451;&#20986;&#26469;&#30340;&#35774;&#23450;&#28857;&#25511;&#21046;&#22120;&#26356;&#21152;&#40065;&#26834;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#30495;&#23454;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#38750;&#32447;&#24615;&#25511;&#21046;&#38382;&#39064;&#20013;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32463;&#24120;&#22312;&#30475;&#20284;&#31616;&#21333;&#30340;&#35774;&#23450;&#28857;&#25511;&#21046;&#38382;&#39064;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#35748;&#20026;&#19977;&#20010;&#24605;&#24819;&#21487;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363; 1&#65289;&#21033;&#29992;&#20808;&#21069;&#21453;&#39304;&#25511;&#21046;&#22120;&#26469;&#24110;&#21161;&#25391;&#24133;&#25506;&#32034;&#12290;2&#65289;&#20351;&#29992;&#31215;&#20998;&#35823;&#24046;&#12290;3&#65289;&#23545;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#24605;&#24819;&#20849;&#21516;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#35774;&#23450;&#28857;&#25511;&#21046;&#22120;&#65292;&#23427;&#23545;&#24314;&#27169;&#35823;&#24046;&#26356;&#21152;&#40065;&#26834;&#65292;&#22240;&#27492;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#30495;&#23454;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#12290;&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#25903;&#25345;&#20102;&#36825;&#19968;&#35770;&#26029;&#65292;&#23454;&#39564;&#20351;&#29992;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#38750;&#32447;&#24615;&#32423;&#32852;&#32592;&#36827;&#31243;&#21644;&#27169;&#25311;&#30340;&#24378;&#38750;&#32447;&#24615; pH &#25511;&#21046;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has recently been an increased interest in reinforcement learning for nonlinear control problems. However standard reinforcement learning algorithms can often struggle even on seemingly simple set-point control problems. This paper argues that three ideas can improve reinforcement learning methods even for highly nonlinear set-point control problems: 1) Make use of a prior feedback controller to aid amplitude exploration. 2) Use integrated errors. 3) Train on model ensembles. Together these ideas lead to more efficient training, and a trained set-point controller that is more robust to modelling errors and thus can be directly deployed to real-world nonlinear systems. The claim is supported by experiments with a real-world nonlinear cascaded tank process and a simulated strongly nonlinear pH-control system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#32447;&#24615;&#33258;&#36866;&#24212;&#25511;&#21046;&#65292;&#23558;&#35266;&#23519;&#22120;&#21644;&#25511;&#21046;&#22120;&#20998;&#20026;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20351;&#25511;&#21046;&#22120;&#20855;&#26377;&#21487;&#29702;&#35299;&#30340;&#32467;&#26500;&#65292;&#26174;&#33879;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.10276</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35266;&#23519;&#22120;-&#21453;&#39304;-&#21069;&#39304;&#25511;&#21046;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Observer-Feedback-Feedforward Controller Structures in Reinforcement Learning. (arXiv:2304.10276v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#32447;&#24615;&#33258;&#36866;&#24212;&#25511;&#21046;&#65292;&#23558;&#35266;&#23519;&#22120;&#21644;&#25511;&#21046;&#22120;&#20998;&#20026;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20351;&#25511;&#21046;&#22120;&#20855;&#26377;&#21487;&#29702;&#35299;&#30340;&#32467;&#26500;&#65292;&#26174;&#33879;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#32447;&#24615;&#33258;&#36866;&#24212;&#25511;&#21046;&#12290;&#37325;&#28857;&#26159;&#37096;&#20998;&#21487;&#35266;&#27979;&#31995;&#32479;&#65292;&#20351;&#29992;&#21333;&#29420;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#29366;&#24577;&#21644;&#21069;&#39304;&#35266;&#23519;&#22120;&#20197;&#21450;&#29366;&#24577;&#21453;&#39304;&#21644;&#21069;&#39304;&#25511;&#21046;&#12290;&#35266;&#23519;&#22120;&#21160;&#24577;&#30001;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#65292;&#32780;&#25511;&#21046;&#22120;&#21017;&#20351;&#29992;&#26631;&#20934;&#32593;&#32476;&#12290;&#22914;&#26412;&#25991;&#25152;&#36848;&#65292;&#36825;&#23548;&#33268;&#20102;&#35266;&#23519;&#22120;&#21160;&#24577;&#20998;&#31163;&#20026;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#37096;&#20998;&#21644;&#29366;&#24577;&#21453;&#39304;&#21040;&#21453;&#39304;&#21644;&#21069;&#39304;&#32593;&#32476;&#37096;&#20998;&#12290;&#36825;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#20351;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#19982;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#20855;&#26377;&#8220;&#21487;&#29702;&#35299;&#30340;&#8221;&#32467;&#26500;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32467;&#26500;&#20855;&#26377;&#35757;&#32451;&#36895;&#24230;&#26174;&#33879;&#21152;&#24555;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#21253;&#25324;&#21069;&#39304;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#19982;&#29366;&#24577;&#21069;&#39304;&#30456;&#20851;
&lt;/p&gt;
&lt;p&gt;
The paper proposes the use of structured neural networks for reinforcement learning based nonlinear adaptive control. The focus is on partially observable systems, with separate neural networks for the state and feedforward observer and the state feedback and feedforward controller. The observer dynamics are modelled by recurrent neural networks while a standard network is used for the controller. As discussed in the paper, this leads to a separation of the observer dynamics to the recurrent neural network part, and the state feedback to the feedback and feedforward network. The structured approach reduces the computational complexity and gives the reinforcement learning based controller an {\em understandable} structure as compared to when one single neural network is used. As shown by simulation the proposed structure has the additional and main advantage that the training becomes significantly faster. Two ways to include feedforward structure are presented, one related to state feed
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#19968;&#33268;&#24615;&#29983;&#25104;&#23545;&#25239;&#26041;&#27861;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#36712;&#36857;&#27169;&#20223;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;DATI&#65292;&#21487;&#20197;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#20195;&#34920;&#24615;&#36712;&#36857;&#65292;&#35782;&#21035;&#20132;&#36890;&#20013;&#24322;&#24120;&#36816;&#21160;&#27169;&#24335;&#65292;&#24182;&#22312;&#21508;&#31181;&#21512;&#25104;&#30340;&#21442;&#32771;&#36712;&#36857;&#23478;&#26063;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.10260</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#20223;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#30340;&#20195;&#34920;&#24615;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Learning Representative Trajectories of Dynamical Systems via Domain-Adaptive Imitation. (arXiv:2304.10260v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10260
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#19968;&#33268;&#24615;&#29983;&#25104;&#23545;&#25239;&#26041;&#27861;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#36712;&#36857;&#27169;&#20223;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;DATI&#65292;&#21487;&#20197;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#20195;&#34920;&#24615;&#36712;&#36857;&#65292;&#35782;&#21035;&#20132;&#36890;&#20013;&#24322;&#24120;&#36816;&#21160;&#27169;&#24335;&#65292;&#24182;&#22312;&#21508;&#31181;&#21512;&#25104;&#30340;&#21442;&#32771;&#36712;&#36857;&#23478;&#26063;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36866;&#24212;&#30340;&#36712;&#36857;&#27169;&#20223;&#26159;&#19968;&#31181;&#26576;&#20123;&#25429;&#39135;&#21160;&#29289;&#20026;&#29983;&#23384;&#23398;&#20064;&#30340;&#25216;&#33021;&#65292;&#36890;&#36807;&#23558;&#21160;&#24577;&#20449;&#24687;&#20174;&#19968;&#20010;&#22495;&#65288;&#23427;&#20204;&#30340;&#36895;&#24230;&#21644;&#36716;&#21521;&#26041;&#21521;&#65289;&#26144;&#23556;&#21040;&#19981;&#21516;&#30340;&#22495;&#65288;&#31227;&#21160;&#29454;&#29289;&#30340;&#24403;&#21069;&#20301;&#32622;&#65289;&#12290;&#20855;&#26377;&#27492;&#25216;&#33021;&#30340;&#26234;&#33021;&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#23398;&#20064;&#27169;&#20223;&#20195;&#34920;&#24615;&#36712;&#36857;&#21518;&#35782;&#21035;&#20132;&#36890;&#20013;&#30340;&#24322;&#24120;&#36816;&#21160;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;DATI&#65292;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#19968;&#33268;&#24615;&#29983;&#25104;&#23545;&#25239;&#26041;&#27861;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#36712;&#36857;&#27169;&#20223;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#21512;&#25104;&#30340;&#21442;&#32771;&#36712;&#36857;&#23478;&#26063;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DATI&#22312;&#27169;&#20223;&#23398;&#20064;&#21644;&#26368;&#20248;&#25511;&#21046;&#20013;&#30340;&#22522;&#20934;&#26041;&#27861;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#20445;&#25345;&#30456;&#21516;&#30340;&#27599;&#19968;&#39033;&#20219;&#21153;&#30340;&#36229;&#21442;&#25968;&#12290;&#36890;&#36807;&#21457;&#29616;&#28023;&#19978;&#20132;&#36890;&#30340;&#24322;&#24120;&#36816;&#21160;&#27169;&#24335;&#65292;&#23427;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#24050;&#24471;&#21040;&#35777;&#26126;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#22312;&#39046;&#22495;&#36866;&#24212;&#36712;&#36857;&#27169;&#20223;&#20013;&#30340;&#24212;&#29992;&#24320;&#36767;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-adaptive trajectory imitation is a skill that some predators learn for survival, by mapping dynamic information from one domain (their speed and steering direction) to a different domain (current position of the moving prey). An intelligent agent with this skill could be exploited for a diversity of tasks, including the recognition of abnormal motion in traffic once it has learned to imitate representative trajectories. Towards this direction, we propose DATI, a deep reinforcement learning agent designed for domain-adaptive trajectory imitation using a cycle-consistent generative adversarial method. Our experiments on a variety of synthetic families of reference trajectories show that DATI outperforms baseline methods for imitation learning and optimal control in this setting, keeping the same per-task hyperparameters. Its generalization to a real-world scenario is shown through the discovery of abnormal motion patterns in maritime traffic, opening the door for the use of deep r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#21019;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#25163;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#23558;&#21360;&#24230;&#25163;&#35821;&#36716;&#25442;&#20026;&#25991;&#26412;&#25110;&#35821;&#38899;&#12290;CNN&#27169;&#22411;&#23545;&#20110;&#38745;&#24577;&#25163;&#35821;&#30340;&#35782;&#21035;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#26159;&#36890;&#36807;&#30417;&#25511;&#21160;&#24577;&#25163;&#21183;&#65292;LSTM&#27169;&#22411;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.10256</link><description>&lt;p&gt;
&#20351;&#29992;Mediapipe Holistic&#35782;&#21035;&#21360;&#24230;&#25163;&#35821;
&lt;/p&gt;
&lt;p&gt;
Indian Sign Language Recognition Using Mediapipe Holistic. (arXiv:2304.10256v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#21019;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#25163;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#23558;&#21360;&#24230;&#25163;&#35821;&#36716;&#25442;&#20026;&#25991;&#26412;&#25110;&#35821;&#38899;&#12290;CNN&#27169;&#22411;&#23545;&#20110;&#38745;&#24577;&#25163;&#35821;&#30340;&#35782;&#21035;&#34920;&#29616;&#36739;&#22909;&#65292;&#20294;&#26159;&#36890;&#36807;&#30417;&#25511;&#21160;&#24577;&#25163;&#21183;&#65292;LSTM&#27169;&#22411;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32843;&#21713;&#20154;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#38754;&#20020;&#30528;&#27807;&#36890;&#38556;&#30861;&#12290;&#20182;&#20204;&#21548;&#19981;&#21040;&#22768;&#38899;&#65292;&#19982;&#19981;&#25026;&#25163;&#35821;&#30340;&#20154;&#27807;&#36890;&#22256;&#38590;&#65292;&#20174;&#32780;&#22312;&#25945;&#32946;&#12289;&#32844;&#19994;&#21644;&#31038;&#20132;&#31561;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#25216;&#26415;&#21487;&#20197;&#25552;&#20379;&#26367;&#20195;&#30340;&#27807;&#36890;&#28192;&#36947;&#65292;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#19968;&#31181;&#26377;&#21161;&#20110;&#32843;&#20154;&#19982;&#21548;&#21147;&#27491;&#24120;&#20154;&#36827;&#34892;&#27807;&#36890;&#30340;&#25216;&#26415;&#26159;&#25163;&#35821;&#35782;&#21035;&#12290;&#25105;&#20204;&#23558;&#21019;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#25163;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#23558;&#21360;&#24230;&#25163;&#35821;&#36716;&#25442;&#20026;&#25991;&#26412;&#25110;&#35821;&#38899;&#12290;&#25105;&#20204;&#23558;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#65292;&#24182;&#27604;&#36739;CNN&#21644;LSTM&#27169;&#22411;&#12290;&#30001;&#20110;&#23384;&#22312;&#38745;&#24577;&#21644;&#21160;&#24577;&#25163;&#35821;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#21306;&#20998;&#23427;&#20204;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;CNN&#27169;&#22411;&#27604;LSTM&#27169;&#22411;&#26356;&#33021;&#25429;&#25417;&#23383;&#27597;&#21644;&#23383;&#31526;&#65292;&#26356;&#36866;&#29992;&#20110;&#38745;&#24577;&#25163;&#35821;&#30340;&#35782;&#21035;&#65292;&#20294;&#36890;&#36807;&#30417;&#25511;&#25163;&#21183;&#65292;&#23427;&#21448;&#36229;&#36234;&#20102;CNN&#12290;
&lt;/p&gt;
&lt;p&gt;
Deaf individuals confront significant communication obstacles on a daily basis. Their inability to hear makes it difficult for them to communicate with those who do not understand sign language. Moreover, it presents difficulties in educational, occupational, and social contexts. By providing alternative communication channels, technology can play a crucial role in overcoming these obstacles. One such technology that can facilitate communication between deaf and hearing individuals is sign language recognition. We will create a robust system for sign language recognition in order to convert Indian Sign Language to text or speech. We will evaluate the proposed system and compare CNN and LSTM models. Since there are both static and gesture sign languages, a robust model is required to distinguish between them. In this study, we discovered that a CNN model captures letters and characters for recognition of static sign language better than an LSTM model, but it outperforms CNN by monitorin
&lt;/p&gt;</description></item><item><title>PED-ANOVA &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; f-ANOVA &#20844;&#24335;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#23376;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#35745;&#31639;&#36229;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#26377;&#21161;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.10255</link><description>&lt;p&gt;
PED-ANOVA: &#22312;&#20219;&#24847;&#23376;&#31354;&#38388;&#20013;&#39640;&#25928;&#37327;&#21270;&#36229;&#21442;&#25968;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces. (arXiv:2304.10255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10255
&lt;/p&gt;
&lt;p&gt;
PED-ANOVA &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; f-ANOVA &#20844;&#24335;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#23376;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#35745;&#31639;&#36229;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#26377;&#21161;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#27969;&#34892;&#20351;&#24471;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#23545;&#20110;&#35757;&#32451;&#24378;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#21448;&#20005;&#37325;&#20381;&#36182;&#20110;&#20102;&#35299;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#20316;&#29992;&#12290;&#36825;&#28608;&#21457;&#20102;&#20851;&#20110;&#36229;&#21442;&#25968;&#37325;&#35201;&#24615;&#30340;&#30740;&#31350;&#65292;&#20363;&#22914;&#20351;&#29992;&#21151;&#33021;&#26041;&#24046;&#20998;&#26512; (f-ANOVA) &#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340; f-ANOVA &#20844;&#24335;&#19981;&#36866;&#29992;&#20110;&#31639;&#27861;&#35774;&#35745;&#24072;&#26368;&#30456;&#20851;&#30340;&#23376;&#31354;&#38388;&#65292;&#20363;&#22914;&#30001;&#26368;&#20339;&#24615;&#33021;&#23450;&#20041;&#30340;&#23376;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;&#20219;&#24847;&#23376;&#31354;&#38388;&#30340; f-ANOVA &#20844;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#20351;&#29992; Pearson &#25955;&#24230; (PED) &#23454;&#29616;&#36229;&#21442;&#25968;&#37325;&#35201;&#24615;&#30340;&#38381;&#24335;&#35745;&#31639;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#20010;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026; PED-ANOVA&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#35782;&#21035;&#19981;&#21516;&#23376;&#31354;&#38388;&#20013;&#37325;&#35201;&#30340;&#36229;&#21442;&#25968;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#26497;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rise in popularity of Hyperparameter Optimization (HPO) for deep learning has highlighted the role that good hyperparameter (HP) space design can play in training strong models. In turn, designing a good HP space is critically dependent on understanding the role of different HPs. This motivates research on HP Importance (HPI), e.g., with the popular method of functional ANOVA (f-ANOVA). However, the original f-ANOVA formulation is inapplicable to the subspaces most relevant to algorithm designers, such as those defined by top performance. To overcome this problem, we derive a novel formulation of f-ANOVA for arbitrary subspaces and propose an algorithm that uses Pearson divergence (PED) to enable a closed-form computation of HPI. We demonstrate that this new algorithm, dubbed PED-ANOVA, is able to successfully identify important HPs in different subspaces while also being extremely computationally efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#25968;&#25454;&#22686;&#24378;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#35780;&#20272;&#20102;&#25193;&#25955;&#27169;&#22411;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20165;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#20110;&#26368;&#24378;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2304.10253</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#35270;&#35282;&#30340;&#25193;&#25955;&#27169;&#22411;&#19982;&#26816;&#32034;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A data augmentation perspective on diffusion models and retrieval. (arXiv:2304.10253v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#25968;&#25454;&#22686;&#24378;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#35780;&#20272;&#20102;&#25193;&#25955;&#27169;&#22411;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20165;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#29992;&#20110;&#26368;&#24378;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#20174;&#25991;&#26412;&#26597;&#35810;&#20013;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20197;&#21033;&#29992;&#36825;&#20123;&#29983;&#25104;&#33021;&#21147;&#26469;&#22686;&#24378;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#26159;&#22312;&#22823;&#22411;&#22024;&#26434;&#30340;&#30417;&#30563;&#27880;&#37322;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22686;&#24378;&#36807;&#31243;&#20013;&#20351;&#29992;&#38468;&#21152;&#25968;&#25454;&#26159;&#21542;&#33021;&#22815;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#24182;&#30740;&#31350;&#26032;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#20854;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#30410;&#12290;&#34429;&#28982;&#25105;&#20204;&#21457;&#29616;&#65292;&#38024;&#23545;&#30446;&#26631;&#25968;&#25454;&#20010;&#24615;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#20248;&#20110;&#26356;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#20294;&#25105;&#20204;&#20063;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#26368;&#36817;&#37051;&#26816;&#32034;&#36807;&#31243;&#65292;&#21487;&#20197;&#23548;&#33268;&#26356;&#24378;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models excel at generating photorealistic images from text-queries. Naturally, many approaches have been proposed to use these generative abilities to augment training datasets for downstream tasks, such as classification. However, diffusion models are themselves trained on large noisily supervised, but nonetheless, annotated datasets. It is an open question whether the generalization capabilities of diffusion models beyond using the additional data of the pre-training process for augmentation lead to improved downstream performance. We perform a systematic evaluation of existing methods to generate images from diffusion models and study new extensions to assess their benefit for data augmentation. While we find that personalizing diffusion models towards the target data outperforms simpler prompting strategies, we also show that using the training data of the diffusion model alone, via a simple nearest neighbor retrieval procedure, leads to even stronger downstream performan
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#38477;&#27700;&#38598;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.10251</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#20195;&#26367;&#38477;&#27700;&#38598;&#21512;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards replacing precipitation ensemble predictions systems using machine learning. (arXiv:2304.10251v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10251
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#38477;&#27700;&#38598;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20854;&#20182;&#27668;&#35937;&#22330;&#30456;&#27604;&#65292;&#38477;&#27700;&#39044;&#27979;&#19981;&#22815;&#20934;&#30830;&#65292;&#22240;&#20026;&#24433;&#21709;&#38477;&#27700;&#20998;&#24067;&#21644;&#24378;&#24230;&#30340;&#20960;&#20010;&#20851;&#38190;&#36807;&#31243;&#21457;&#29983;&#22312;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#21487;&#35299;&#20915;&#23610;&#24230;&#20197;&#19979;&#12290;&#36825;&#35201;&#27714;&#20351;&#29992;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#27169;&#25311;&#12290;&#20026;&#20102;&#29983;&#25104;&#19982;&#39044;&#27979;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#65292;&#21516;&#26102;&#36816;&#34892;&#27169;&#25311;&#30340;&#38598;&#21512;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#25104;&#26412;&#26159;&#19968;&#20010;&#38480;&#21046;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#26367;&#20195;&#20174;&#27169;&#25311;&#29983;&#25104;&#38598;&#21512;&#31995;&#32479;&#30340;&#36235;&#21183;&#26159;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#39640;&#20998;&#36776;&#29575;&#38598;&#21512;&#36816;&#34892;&#30340;&#25968;&#25454;&#19981;&#21487;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#38477;&#27700;&#30340;&#38598;&#21512;&#27668;&#35937;&#39044;&#27979;&#65292;&#32780;&#19981;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23398;&#20064;&#22797;&#26434;&#30340;&#38477;&#27700;&#27169;&#24335;&#65292;&#24182;&#20135;&#29983;&#22810;&#26679;&#21270;&#12289;&#30495;&#23454;&#30340;&#38477;&#27700;&#22330;&#65292;&#20174;&#32780;&#20135;&#29983;&#30495;&#23454;&#30340;&#38477;&#27700;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precipitation forecasts are less accurate compared to other meteorological fields because several key processes affecting precipitation distribution and intensity occur below the resolved scale of global weather prediction models. This requires to use higher resolution simulations. To generate an uncertainty prediction associated with the forecast, ensembles of simulations are run simultaneously. However, the computational cost is a limiting factor here. Thus, instead of generating an ensemble system from simulations there is a trend of using neural networks. Unfortunately the data for high resolution ensemble runs is not available. We propose a new approach to generating ensemble weather predictions for high-resolution precipitation without requiring high-resolution training data. The method uses generative adversarial networks to learn the complex patterns of precipitation and produce diverse and realistic precipitation fields, allowing to generate realistic precipitation ensemble me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#34394;&#20551;&#20449;&#24687;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;-&#22270;&#20687;&#23545;&#20013;&#21306;&#20998;&#27491;&#23447;&#21644;&#34394;&#20551;&#26032;&#38395;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.10249</link><description>&lt;p&gt;
&#21033;&#29992;&#25991;&#26412;-&#22270;&#20687;&#23545;&#27604;&#27169;&#22411;&#33258;&#21160;&#26816;&#27979;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Text-image Contrastive Models for Automatic Detection of Online Misinformation. (arXiv:2304.10249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#34394;&#20551;&#20449;&#24687;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;-&#22270;&#20687;&#23545;&#20013;&#21306;&#20998;&#27491;&#23447;&#21644;&#34394;&#20551;&#26032;&#38395;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#32593;&#31449;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#26032;&#38395;&#25991;&#31456;&#30340;&#25968;&#37327;&#22312;&#32593;&#19978;&#36805;&#36895;&#20256;&#25773;&#65292;&#22240;&#27492;&#21487;&#33021;&#23384;&#22312;&#22823;&#35268;&#27169;&#30340;&#28508;&#22312;&#34394;&#20551;&#20449;&#24687;&#12290;&#34429;&#28982;&#22823;&#37327;&#30340;&#30740;&#31350;&#24050;&#32463;&#24212;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#26679;&#30340;&#20869;&#23481;&#65292;&#20294;&#32570;&#20047;&#40644;&#37329;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#24050;&#32463;&#38459;&#30861;&#20102;&#21457;&#23637;&#12290;&#20998;&#26512;&#21333;&#20010;&#25968;&#25454;&#26684;&#24335;&#65292;&#21363;&#34394;&#20551;&#30340;&#25991;&#26412;&#25551;&#36848;&#25110;&#34394;&#20551;&#30340;&#22270;&#20687;&#65292;&#26159;&#30446;&#21069;&#30740;&#31350;&#30340;&#20027;&#27969;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#34394;&#20551;&#20449;&#24687;&#36890;&#24120;&#20197;&#25991;&#26412;-&#22270;&#20687;&#23545;&#30340;&#24418;&#24335;&#24418;&#25104;&#65292;&#26032;&#38395;&#25991;&#31456;/&#26032;&#38395;&#26631;&#39064;&#34987;&#25551;&#36848;&#20026;&#25991;&#26412;&#20869;&#23481;&#65292;&#24182;&#36890;&#24120;&#36319;&#38543;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#37492;&#20110;&#23545;&#27809;&#26377;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#29305;&#24449;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#23545;&#27604;&#23398;&#20064;&#20316;&#20026;&#33258;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#23545;&#27604;&#23398;&#20064;&#22312;&#34394;&#20551;&#20449;&#24687;&#35782;&#21035;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25991;&#26412;-&#22270;&#20687;&#23545;&#27604;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#21306;&#20998;&#27491;&#23447;&#21644;&#34394;&#20551;&#26032;&#38395;&#20013;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20351;&#29992;&#25991;&#26412;-&#22270;&#20687;&#23545;&#26816;&#27979;&#20551;&#26032;&#38395;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As growing usage of social media websites in the recent decades, the amount of news articles spreading online rapidly, resulting in an unprecedented scale of potentially fraudulent information. Although a plenty of studies have applied the supervised machine learning approaches to detect such content, the lack of gold standard training data has hindered the development. Analysing the single data format, either fake text description or fake image, is the mainstream direction for the current research. However, the misinformation in real-world scenario is commonly formed as a text-image pair where the news article/news title is described as text content, and usually followed by the related image. Given the strong ability of learning features without labelled data, contrastive learning, as a self-learning approach, has emerged and achieved success on the computer vision. In this paper, our goal is to explore the constrastive learning in the domain of misinformation identification. We devel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32553;&#20943;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#23545;&#31216;&#23574;&#23792;&#24352;&#37327;&#19978;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#38750;&#24179;&#20961;&#30456;&#20851;&#24615;&#24773;&#20917;&#19979;&#30340;&#31934;&#30830;&#34920;&#29616;&#65292;&#21487;&#29992;&#20110;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#20449;&#21495;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10248</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#23545;&#31216;&#23574;&#23792;&#24352;&#37327;&#19978;&#30340;Hotelling&#32553;&#20943;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hotelling Deflation on Large Symmetric Spiked Tensors. (arXiv:2304.10248v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32553;&#20943;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#23545;&#31216;&#23574;&#23792;&#24352;&#37327;&#19978;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#38750;&#24179;&#20961;&#30456;&#20851;&#24615;&#24773;&#20917;&#19979;&#30340;&#31934;&#30830;&#34920;&#29616;&#65292;&#21487;&#29992;&#20110;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#20449;&#21495;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#24212;&#29992;&#20110;&#20272;&#35745;&#21463;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#27745;&#26579;&#30340;&#22823;&#24352;&#37327;&#20013;&#21253;&#21547;&#30340;&#20302;&#31209;&#23545;&#31216;&#23574;&#23792;&#26102;&#30340;&#32553;&#20943;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#20551;&#23450;&#23574;&#23792;&#20998;&#37327;&#23384;&#22312;&#38750;&#24179;&#20961;&#65288;&#22266;&#23450;&#65289;&#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#23545;&#32553;&#20943;&#31639;&#27861;&#22312;&#22823;&#32500;&#24773;&#20917;&#19979;&#34920;&#29616;&#30340;&#31934;&#30830;&#21051;&#30011;&#65292;&#20854;&#20013;&#21253;&#25324;&#36890;&#36807;&#36830;&#32493;&#30340;&#31209;-1&#36924;&#36817;&#33719;&#24471;&#30340;&#21521;&#37327;&#30340;&#23545;&#40784;&#24773;&#20917;&#21450;&#20854;&#20272;&#35745;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21487;&#35753;&#20154;&#20204;&#29702;&#35299;&#22122;&#22768;&#24178;&#25200;&#19979;&#30340;&#32553;&#20943;&#26426;&#21046;&#65292;&#24182;&#21487;&#29992;&#20110;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#20449;&#21495;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the deflation algorithm when applied to estimate a low-rank symmetric spike contained in a large tensor corrupted by additive Gaussian noise. Specifically, we provide a precise characterization of the large-dimensional performance of deflation in terms of the alignments of the vectors obtained by successive rank-1 approximation and of their estimated weights, assuming non-trivial (fixed) correlations among spike components. Our analysis allows an understanding of the deflation mechanism in the presence of noise and can be exploited for designing more efficient signal estimation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#28388;&#27874;&#24863;&#30693;MPC&#65292;&#22522;&#20110;&#29366;&#24577;&#20272;&#35745;&#22120;&#30340;&#36861;&#36394;&#24230;&#24809;&#32602;&#20449;&#24687;&#25439;&#22833;&#65292;&#21487;&#20197;&#24555;&#36895;&#35268;&#21010;&#24182;&#22312;&#22810;&#26041;&#38754;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10246</link><description>&lt;p&gt;
&#28388;&#27874;&#24863;&#30693;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Filter-Aware Model-Predictive Control. (arXiv:2304.10246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#28388;&#27874;&#24863;&#30693;MPC&#65292;&#22522;&#20110;&#29366;&#24577;&#20272;&#35745;&#22120;&#30340;&#36861;&#36394;&#24230;&#24809;&#32602;&#20449;&#24687;&#25439;&#22833;&#65292;&#21487;&#20197;&#24555;&#36895;&#35268;&#21010;&#24182;&#22312;&#22810;&#26041;&#38754;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#38382;&#39064;&#22312;&#20943;&#23569;&#25104;&#26412;&#21644;&#25910;&#38598;&#20449;&#24687;&#20043;&#38388;&#23384;&#22312;&#24179;&#34913;&#12290;&#21487;&#20197;&#36890;&#36807;&#22312;&#20449;&#24565;&#31354;&#38388;&#20013;&#36827;&#34892;&#35268;&#21010;&#26469;&#26368;&#20248;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#36825;&#36890;&#24120;&#26159;&#26497;&#20854;&#26114;&#36149;&#30340;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046; (MPC) &#37319;&#29992;&#20351;&#29992;&#29366;&#24577;&#20272;&#35745;&#22120;&#24418;&#25104;&#20851;&#20110;&#29366;&#24577;&#30340;&#20449;&#20208;&#65292;&#28982;&#21518;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#36827;&#34892;&#35268;&#21010;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36825;&#22312;&#35268;&#21010;&#26399;&#38388;&#24573;&#30053;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#35266;&#23519;&#32467;&#26524;&#65292;&#22240;&#27492;&#19981;&#33021;&#31215;&#26497;&#22686;&#21152;&#25110;&#20445;&#25345;&#20854;&#33258;&#36523;&#29366;&#24577;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#20449;&#24565;&#31354;&#38388;&#20013;&#36827;&#34892;&#35268;&#21010;&#21644;&#23436;&#20840;&#24573;&#30053;&#20854;&#21160;&#24577;&#20043;&#38388;&#25214;&#21040;&#20102;&#19968;&#20010;&#25240;&#34935;&#26041;&#26696;&#65292;&#21482;&#32771;&#34385;&#20854;&#26410;&#26469;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#28388;&#27874;&#24863;&#30693;MPC&#65292;&#36890;&#36807;&#25105;&#20204;&#25152;&#35859;&#30340;&#8221;&#36861;&#36394;&#24230;&#8220;&#65292;&#21363;&#29366;&#24577;&#20272;&#35745;&#22120;&#30340;&#39044;&#26399;&#35823;&#24046;&#65292;&#24809;&#32602;&#20449;&#24687;&#25439;&#22833;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#25311;&#21487;&#20197;&#23558;&#8220;&#36861;&#36394;&#24230;&#8221;&#21387;&#32553;&#25104;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#35268;&#21010;&#12290;&#22312;&#28041;&#21450;&#35270;&#35273;&#23548;&#33322;&#65292;&#29616;&#23454;&#30340;&#26085;&#24120;&#29615;&#22659;&#21644;&#19968;&#20010;&#21452;&#36830;&#26438;&#26426;&#22120;&#20154;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially-observable problems pose a trade-off between reducing costs and gathering information. They can be solved optimally by planning in belief space, but that is often prohibitively expensive. Model-predictive control (MPC) takes the alternative approach of using a state estimator to form a belief over the state, and then plan in state space. This ignores potential future observations during planning and, as a result, cannot actively increase or preserve the certainty of its own state estimate. We find a middle-ground between planning in belief space and completely ignoring its dynamics by only reasoning about its future accuracy. Our approach, filter-aware MPC, penalises the loss of information by what we call "trackability", the expected error of the state estimator. We show that model-based simulation allows condensing trackability into a neural network, which allows fast planning. In experiments involving visual navigation, realistic every-day environments and a two-link robot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#39044;&#27979;3D&#22320;&#38663;&#27874;&#20256;&#25773;&#30340;&#22320;&#38754;&#36816;&#21160;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.10242</link><description>&lt;p&gt;
&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#20195;&#29702;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;3D&#22320;&#38663;&#27874;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Fourier Neural Operator Surrogate Model to Predict 3D Seismic Waves Propagation. (arXiv:2304.10242v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#39044;&#27979;3D&#22320;&#38663;&#27874;&#20256;&#25773;&#30340;&#22320;&#38754;&#36816;&#21160;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#31639;&#23376;&#30340;&#20852;&#36215;&#65292;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#37327;&#21270;&#19982;&#39640;&#20445;&#30495;&#25968;&#20540;&#27169;&#25311;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#25110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#65292;&#21463;&#21040;&#39044;&#23450;&#20041;&#37197;&#32622;&#30340;&#23616;&#38480;&#65292;&#24182;&#19988;&#21482;&#33021;&#39044;&#27979;&#35299;&#30340;&#35299;&#12290;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#65292;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#19981;&#21516;&#21442;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#19968;&#33324;&#35299;&#65292;&#20363;&#22914;&#24377;&#24615;&#27874;&#26041;&#31243;&#12290;&#22320;&#38663;&#23398;&#20013;&#24212;&#29992;&#31070;&#32463;&#31639;&#23376;&#30340;&#24212;&#29992;&#24456;&#23569;&#65292;&#37117;&#20165;&#38480;&#20110;&#20108;&#32500;&#35774;&#32622;&#65292;&#23613;&#31649;&#19977;&#32500;&#25928;&#24212;&#30340;&#37325;&#35201;&#24615;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#26469;&#39044;&#27979;&#26469;&#33258;3D&#22320;&#36136;&#25551;&#36848;&#30340;&#22320;&#38754;&#36816;&#21160;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#20195;&#30721;SEM3D&#24314;&#31435;&#20102;&#19968;&#20010;&#30001;30,000&#20010;&#19981;&#21516;&#22320;&#36136;&#26500;&#36896;&#29983;&#25104;&#30340;&#22320;&#38754;&#36816;&#21160;&#30340;&#24191;&#27867;&#25968;&#25454;&#24211;&#12290;&#20511;&#21161;&#36825;&#20010;&#25968;&#25454;&#24211;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;FNO&#20195;&#29702;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#30001;&#26032;&#22320;&#36136;&#26500;&#36896;&#24341;&#36215;&#30340;&#22320;&#38754;&#36816;&#21160;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent rise of neural operators, scientific machine learning offers new solutions to quantify uncertainties associated with high-fidelity numerical simulations. Traditional neural networks, such as Convolutional Neural Networks (CNN) or Physics-Informed Neural Networks (PINN), are restricted to the prediction of solutions in a predefined configuration. With neural operators, one can learn the general solution of Partial Differential Equations, such as the elastic wave equation, with varying parameters. There have been very few applications of neural operators in seismology. All of them were limited to two-dimensional settings, although the importance of three-dimensional (3D) effects is well known.  In this work, we apply the Fourier Neural Operator (FNO) to predict ground motion time series from a 3D geological description. We used a high-fidelity simulation code, SEM3D, to build an extensive database of ground motions generated by 30,000 different geologies. With this databa
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#29983;&#25104;&#22810;&#31181;&#39118;&#26684;&#21644;&#35270;&#35282;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#36827;&#19968;&#27493;&#24494;&#35843;&#39592;&#24178;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10226</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#20083;&#33146;X&#32447;&#25668;&#24433;&#22270;&#20687;&#20998;&#26512;&#30340;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization for Mammographic Image Analysis via Contrastive Learning. (arXiv:2304.10226v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10226
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#29983;&#25104;&#22810;&#31181;&#39118;&#26684;&#21644;&#35270;&#35282;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#36827;&#19968;&#27493;&#24494;&#35843;&#39592;&#24178;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;X&#32447;&#25668;&#24433;&#22270;&#20687;&#20998;&#26512;&#26159;&#21307;&#23398;&#24433;&#20687;&#23398;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#20855;&#26377;&#22810;&#26679;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19981;&#21516;&#21378;&#21830;&#30340;&#22270;&#20687;&#39118;&#26684;&#65292;&#36825;&#24448;&#24448;&#38656;&#35201;&#38750;&#24120;&#24222;&#22823;&#30340;&#26679;&#26412;&#38598;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#21040;&#19981;&#21516;&#21378;&#21830;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mammographic image analysis is a fundamental problem in the computer-aided diagnosis scheme, which has recently made remarkable progress with the advance of deep learning. However, the construction of a deep learning model requires training data that are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, mammogram collection from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning models to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor styles. Afterward, the backbone network is then recalibrated to the downstream tasks of mas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#20139;&#20102;&#20316;&#32773;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#23545;&#36164;&#20135;&#31649;&#29702;&#20135;&#29983;&#24433;&#21709;&#30340;&#35266;&#28857;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#34913;&#37327;&#29305;&#23450;&#22522;&#37329;&#26159;&#21542;&#30495;&#27491;&#24320;&#21457;&#20102;AI&#30340;&#31616;&#21333;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2304.10212</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#38761;&#21629;&#23545;&#36164;&#20135;&#31649;&#29702;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The impact of the AI revolution on asset management. (arXiv:2304.10212v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#20139;&#20102;&#20316;&#32773;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#23545;&#36164;&#20135;&#31649;&#29702;&#20135;&#29983;&#24433;&#21709;&#30340;&#35266;&#28857;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#34913;&#37327;&#29305;&#23450;&#22522;&#37329;&#26159;&#21542;&#30495;&#27491;&#24320;&#21457;&#20102;AI&#30340;&#31616;&#21333;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#26426;&#22120;&#20855;&#22791;&#20102;&#26480;&#20986;&#30340;&#33021;&#21147;&#65306;&#23427;&#20204;&#21487;&#20197;&#38405;&#35835;&#21644;&#29702;&#35299;&#33258;&#30001;&#27969;&#21160;&#30340;&#25991;&#26412;&#65292;&#19982;&#20154;&#31867;&#36827;&#34892;&#25512;&#29702;&#21644;&#20132;&#28041;&#65292;&#32763;&#35793;&#19981;&#21516;&#35821;&#35328;&#30340;&#25991;&#26412;&#65292;&#23398;&#20064;&#22914;&#20309;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#31561;&#31561;&#12290;&#22914;&#20170;&#65292;&#26426;&#22120;&#24050;&#32463;&#22312;&#30284;&#30151;&#26816;&#27979;&#12289;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#12289;&#33647;&#29289;&#35774;&#35745;&#12289;&#26680;&#32858;&#21464;&#21453;&#24212;&#22534;&#25511;&#21046;&#31561;&#26041;&#38754;&#23454;&#29616;&#20102;&#38761;&#21629;&#24615;&#31361;&#30772;&#12290;&#34429;&#28982;&#36825;&#20123;&#33021;&#21147;&#20173;&#22788;&#20110;&#33804;&#33469;&#38454;&#27573;&#65292;&#20294;&#23427;&#20204;&#22312;&#25345;&#32493;&#23436;&#21892;&#21644;&#24212;&#29992;&#20013;&#30340;&#25216;&#26415;&#24433;&#21709;&#20960;&#20046;&#23558;&#22312;&#20154;&#31867;&#27963;&#21160;&#30340;&#20960;&#20046;&#25152;&#26377;&#31038;&#20250;&#21644;&#32463;&#27982;&#39046;&#22495;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#36825;&#26159;&#25105;&#20204;&#20197;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#12290;&#26412;&#25991;&#23558;&#20998;&#20139;&#25105;&#30340;&#35266;&#28857;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#24433;&#21709;&#36164;&#20135;&#31649;&#29702;&#65292;&#25105;&#23558;&#25552;&#20379;&#19968;&#20010;&#24605;&#32500;&#26694;&#26550;&#65292;&#20351;&#35835;&#32773;&#21487;&#20197;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#26631;&#20934;&#26469;&#35780;&#20272;&#19968;&#20010;&#29305;&#23450;&#22522;&#37329;&#26159;&#21542;&#30495;&#27491;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Recent progress in deep learning, a special form of machine learning, has led to remarkable capabilities machines can now be endowed with: they can read and understand free flowing text, reason and bargain with human counterparts, translate texts between languages, learn how to take decisions to maximize certain outcomes, etc. Today, machines have revolutionized the detection of cancer, the prediction of protein structures, the design of drugs, the control of nuclear fusion reactors etc. Although these capabilities are still in their infancy, it seems clear that their continued refinement and application will result in a technological impact on nearly all social and economic areas of human activity, the likes of which we have not seen before. In this article, I will share my view as to how AI will likely impact asset management in general and I will provide a mental framework that will equip readers with a simple criterion to assess whether and to what degree a given fund really exploi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;S&#21442;&#25968;&#27169;&#24335;&#25104;&#21151;&#23454;&#29616;&#23545;Cu&#20114;&#36830;&#32570;&#38519;&#30340;&#38750;&#30772;&#22351;&#24615;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#22312;&#21516;&#26102;&#20998;&#26512;&#26681;&#26412;&#21407;&#22240;&#21644;&#20005;&#37325;&#24615;&#26041;&#38754;&#20855;&#26377;&#20808;&#36827;&#24615;&#65292;&#20855;&#22791;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#20934;&#30830;&#24230;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#31561;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.10207</link><description>&lt;p&gt;
SREL&#65306;&#22522;&#20110;S&#21442;&#25968;&#27169;&#24335;&#30340;&#38108;&#20114;&#36830;&#38750;&#30772;&#22351;&#25925;&#38556;&#35786;&#26029;&#30340;&#20005;&#37325;&#24615;&#35780;&#32423;&#38598;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SREL: Severity Rating Ensemble Learning for Non-Destructive Fault Diagnosis of Cu Interconnects using S-parameter Patterns. (arXiv:2304.10207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;S&#21442;&#25968;&#27169;&#24335;&#25104;&#21151;&#23454;&#29616;&#23545;Cu&#20114;&#36830;&#32570;&#38519;&#30340;&#38750;&#30772;&#22351;&#24615;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#22312;&#21516;&#26102;&#20998;&#26512;&#26681;&#26412;&#21407;&#22240;&#21644;&#20005;&#37325;&#24615;&#26041;&#38754;&#20855;&#26377;&#20808;&#36827;&#24615;&#65292;&#20855;&#22791;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#20934;&#30830;&#24230;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#31561;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22788;&#29702;&#22120;&#30340;&#24037;&#20316;&#39057;&#29575;&#21644;&#26102;&#38047;&#36895;&#24230;&#36880;&#24180;&#25552;&#39640;&#65292;&#20114;&#36830;&#23545;&#25972;&#20010;&#30005;&#23376;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#37117;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#26816;&#27979;&#21644;&#35786;&#26029;&#20114;&#36830;&#25925;&#38556;&#23545;&#30005;&#23376;&#20581;&#24247;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#30005;&#20449;&#21495;&#20316;&#20026;&#39044;&#27979;&#22240;&#23376;&#30340;&#29616;&#26377;&#30740;&#31350;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#26080;&#27861;&#21306;&#20998;&#32570;&#38519;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#26368;&#32456;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#30772;&#22351;&#24615;&#35780;&#20272;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#30340;&#24178;&#25200;&#32780;&#23548;&#33268;&#35823;&#35686;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#23545;Cu&#20114;&#36830;&#32570;&#38519;&#30340;&#38750;&#30772;&#22351;&#24615;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#23454;&#29616;&#20102;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#20934;&#30830;&#24230;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#30005;&#20449;&#21495;&#27169;&#24335;&#21516;&#26102;&#20998;&#26512;&#26681;&#26412;&#21407;&#22240;&#21644;&#20005;&#37325;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;S&#21442;&#25968;&#27169;&#24335;&#20855;&#26377;&#25925;&#38556;&#35786;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
As operating frequencies and clock speeds in processors have increased over the years, interconnects affect both the reliability and performance of entire electronic systems. Fault detection and diagnosis of the interconnects are crucial for prognostics and health management (PHM) of electronics. However, existing research works utilizing electrical signals as prognostic factors have limitations, such as the inability to distinguish the root cause of defects, which eventually requires additional destructive evaluation, and vulnerability to noise that results in a false alarm. Herein, we realize the non-destructive detection and diagnosis of defects in Cu interconnects, achieving early detection, high diagnostic accuracy, and noise robustness. To the best of our knowledge, this study first simultaneously analyzes the root cause and severity using electrical signal patterns. In this paper, we experimentally show that S-parameter patterns have the ability for fault diagnosis and they are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#38388;&#27493;&#26426;&#21046;&#23454;&#29616;MC-dropout&#30340;&#39640;&#25928;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#20026;&#31070;&#32463;&#24418;&#24577;&#23398;&#30828;&#20214;&#30340;&#33410;&#33021;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#30740;&#31350;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.10191</link><description>&lt;p&gt;
&#22522;&#20110;MC-dropout&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Uncertainty Estimation in Spiking Neural Networks via MC-dropout. (arXiv:2304.10191v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#38388;&#27493;&#26426;&#21046;&#23454;&#29616;MC-dropout&#30340;&#39640;&#25928;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#20026;&#31070;&#32463;&#24418;&#24577;&#23398;&#30828;&#20214;&#30340;&#33410;&#33021;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#30740;&#31350;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNN)&#20316;&#20026;&#29983;&#29289;&#31070;&#32463;&#20803;&#31232;&#30095;&#21644;&#20107;&#20214;&#39537;&#21160;&#36890;&#20449;&#27169;&#22411;&#30340;&#27169;&#25311;&#22120;&#24050;&#24341;&#36215;&#20851;&#27880;&#65292;&#24182;&#22240;&#27492;&#22312;&#31070;&#32463;&#24418;&#24577;&#23398;&#30828;&#20214;&#20013;&#26174;&#31034;&#20986;&#36234;&#26469;&#36234;&#22810;&#30340;&#33410;&#33021;&#24212;&#29992;&#21069;&#26223;&#12290;&#19982;&#32463;&#20856;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#19968;&#26679;&#65292;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#39640;&#39118;&#38505;&#24212;&#29992;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12289;&#21307;&#23398;&#35786;&#26029;&#21644;&#39640;&#39057;&#20132;&#26131;&#65289;&#30340;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;SNNs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35752;&#35770;&#24456;&#23569;&#65292;&#32780;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;SNNs&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MC-dropout&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;SNNs&#30340;&#26102;&#38388;&#27493;&#26426;&#21046;&#65292;&#20197;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#24335;&#23454;&#29616;MC-dropout&#65292;&#19981;&#20250;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#24341;&#20837;&#37325;&#22823;&#24320;&#38144;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#39640;&#20934;&#30830;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) have gained attention as models of sparse and event-driven communication of biological neurons, and as such have shown increasing promise for energy-efficient applications in neuromorphic hardware. As with classical artificial neural networks (ANNs), predictive uncertainties are important for decision making in high-stakes applications, such as autonomous vehicles, medical diagnosis, and high frequency trading. Yet, discussion of uncertainty estimation in SNNs is limited, and approaches for uncertainty estimation in artificial neural networks (ANNs) are not directly applicable to SNNs. Here, we propose an efficient Monte Carlo(MC)-dropout based approach for uncertainty estimation in SNNs. Our approach exploits the time-step mechanism of SNNs to enable MC-dropout in a computationally efficient manner, without introducing significant overheads during training and inference while demonstrating high accuracy and uncertainty quality.
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;&#21046;&#36896;&#26159;&#24037;&#19994;4.0&#30340;&#19968;&#37096;&#20998;&#65292;&#23558;&#29983;&#20135;&#35774;&#22791;&#21644;&#32593;&#32476;&#26234;&#33021;&#32467;&#21512;&#65292;&#20294;&#30001;&#20110;&#29616;&#26377;&#25805;&#20316;&#27969;&#31243;&#30340;&#28431;&#27934;&#65292;&#20351;&#24471;&#26234;&#33021;&#21046;&#36896;&#25104;&#20026;&#32593;&#32476;&#23041;&#32961;&#30340;&#20027;&#35201;&#30446;&#26631;&#65292;&#32593;&#32476;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2304.10180</link><description>&lt;p&gt;
&#26234;&#33021;&#21046;&#36896;&#20013;&#30340;&#32593;&#32476;&#23433;&#20840;&#65288;&#23041;&#32961;&#12289;&#39118;&#26223;&#12289;&#25361;&#25112;&#65289;
&lt;/p&gt;
&lt;p&gt;
Cyber Security in Smart Manufacturing (Threats, Landscapes Challenges). (arXiv:2304.10180v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10180
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#21046;&#36896;&#26159;&#24037;&#19994;4.0&#30340;&#19968;&#37096;&#20998;&#65292;&#23558;&#29983;&#20135;&#35774;&#22791;&#21644;&#32593;&#32476;&#26234;&#33021;&#32467;&#21512;&#65292;&#20294;&#30001;&#20110;&#29616;&#26377;&#25805;&#20316;&#27969;&#31243;&#30340;&#28431;&#27934;&#65292;&#20351;&#24471;&#26234;&#33021;&#21046;&#36896;&#25104;&#20026;&#32593;&#32476;&#23041;&#32961;&#30340;&#20027;&#35201;&#30446;&#26631;&#65292;&#32593;&#32476;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;4.0&#26159;&#20449;&#24687;&#25216;&#26415;&#21644;&#36816;&#33829;&#25216;&#26415;&#20004;&#20010;&#19990;&#30028;&#20013;&#36229;&#36830;&#36890;&#25968;&#23383;&#21270;&#24037;&#19994;&#30340;&#32452;&#21512;&#12290;&#26234;&#33021;&#21046;&#36896;&#21033;&#29992;&#36825;&#31181;&#26426;&#20250;&#65292;&#23558;&#21046;&#36896;&#35774;&#22791;&#30340;&#26234;&#33021;&#24615;&#19982;&#32593;&#32476;&#23618;&#38754;&#30340;&#26234;&#33021;&#24615;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#25805;&#20316;&#27969;&#31243;&#30340;&#28431;&#27934;&#65292;&#26234;&#33021;&#21046;&#36896;&#29616;&#22312;&#25104;&#20026;&#32593;&#32476;&#23041;&#32961;&#30340;&#20027;&#35201;&#30446;&#26631;&#12290;&#30001;&#20110;&#26234;&#33021;&#21046;&#36896;&#35206;&#30422;&#20102;&#20174;&#29289;&#29702;&#31995;&#32479;&#21040;&#22686;&#26448;&#21046;&#36896;&#12289;&#33258;&#20027;&#36710;&#36742;&#12289;&#22522;&#20110;&#20113;&#30340;IIoT&#65288;&#24037;&#19994;&#29289;&#32852;&#32593;&#65289;&#20877;&#21040;&#26426;&#22120;&#20154;&#21046;&#36896;&#31561;&#24191;&#27867;&#30340;&#29983;&#20135;&#34892;&#19994;&#39046;&#22495;&#65292;&#32593;&#32476;&#23041;&#32961;&#34987;&#32622;&#20110;&#26174;&#33879;&#22320;&#20301;&#65292;&#20851;&#20110;&#22914;&#20309;&#36890;&#36807;&#32593;&#32476;&#36830;&#25509;&#21046;&#36896;&#36164;&#28304;&#12289;&#22914;&#20309;&#20026;&#24037;&#21378;&#29983;&#20135;&#25972;&#20010;&#36807;&#31243;&#38142;&#26465;&#36827;&#34892;&#25972;&#21512;&#31561;&#38382;&#39064;&#20135;&#29983;&#20102;&#36136;&#30097;&#12290;&#32593;&#32476;&#23433;&#20840;&#30340;&#26426;&#23494;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#21487;&#29992;&#24615;&#23545;&#20110;&#27491;&#24120;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industry 4.0 is a blend of the hyper-connected digital industry within two world of Information Technology (IT) and Operational Technology (OT). With this amalgamate opportunity, smart manufacturing involves production assets with the manufacturing equipment having its own intelligence, while the system-wide intelligence is provided by the cyber layer. However Smart manufacturing now becomes one of the prime targets of cyber threats due to vulnerabilities in the existing process of operation. Since smart manufacturing covers a vast area of production industries from cyber physical system to additive manufacturing, to autonomous vehicles, to cloud based IIoT (Industrial IoT), to robotic production, cyber threat stands out with this regard questioning about how to connect manufacturing resources by network, how to integrate a whole process chain for a factory production etc. Cybersecurity confidentiality, integrity and availability expose their essential existence for the proper operatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#30446;&#26631;&#26469;&#35268;&#33539;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#20108;&#38454;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#23454;&#29616;&#26041;&#27861;&#26469;&#20248;&#21270;&#25152;&#25552;&#20986;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2304.10177</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#20108;&#38454;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Regularizing Second-Order Influences for Continual Learning. (arXiv:2304.10177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#30446;&#26631;&#26469;&#35268;&#33539;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#20108;&#38454;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#23454;&#29616;&#26041;&#27861;&#26469;&#20248;&#21270;&#25152;&#25552;&#20986;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#38750;&#38745;&#24577;&#25968;&#25454;&#27969;&#65292;&#32780;&#19981;&#20250;&#28798;&#38590;&#24615;&#22320;&#24536;&#35760;&#20197;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#30446;&#21069;&#30340;&#22238;&#25918;&#26041;&#27861;&#36890;&#36807;&#22312;&#20445;&#23384;&#24050;&#32463;&#30475;&#36807;&#30340;&#25968;&#25454;&#30340;&#23567;&#32531;&#20914;&#21306;&#19978;&#28436;&#22863;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38590;&#39064;&#65292;&#38656;&#35201;&#19968;&#20010;&#31934;&#32454;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36873;&#25321;&#26041;&#26696;&#36890;&#24120;&#21482;&#23547;&#27714;&#26368;&#22823;&#21270;&#27491;&#22312;&#36827;&#34892;&#30340;&#36873;&#25321;&#30340;&#25928;&#29992;&#65292;&#24573;&#30053;&#20102;&#22312;&#36873;&#25321;&#30340;&#36830;&#32493;&#22238;&#21512;&#20043;&#38388;&#30340;&#24178;&#25200;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#24433;&#21709;&#20989;&#25968;&#30340;&#26694;&#26550;&#19979;&#21078;&#26512;&#20102;&#39034;&#24207;&#36873;&#25321;&#27493;&#39588;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#35774;&#27861;&#35782;&#21035;&#20986;&#19968;&#31867;&#26032;&#30340;&#20108;&#38454;&#24433;&#21709;&#65292;&#23427;&#23558;&#36880;&#28176;&#25918;&#22823;&#37325;&#22797;&#32531;&#20914;&#21306;&#20013;&#30340;&#20598;&#28982;&#20559;&#24046;&#65292;&#24182;&#25439;&#23475;&#36873;&#25321;&#36807;&#31243;&#12290;&#20026;&#20102;&#35268;&#33539;&#20108;&#38454;&#25928;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#30446;&#26631;&#65292;&#20854;&#36824;&#19982;&#20004;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#26631;&#20934;&#20855;&#26377;&#26126;&#26174;&#30340;&#32852;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#25152;&#25552;&#20986;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning aims to learn on non-stationary data streams without catastrophically forgetting previous knowledge. Prevalent replay-based methods address this challenge by rehearsing on a small buffer holding the seen data, for which a delicate sample selection strategy is required. However, existing selection schemes typically seek only to maximize the utility of the ongoing selection, overlooking the interference between successive rounds of selection. Motivated by this, we dissect the interaction of sequential selection steps within a framework built on influence functions. We manage to identify a new class of second-order influences that will gradually amplify incidental bias in the replay buffer and compromise the selection process. To regularize the second-order effects, a novel selection objective is proposed, which also has clear connections to two widely adopted criteria. Furthermore, we present an efficient implementation for optimizing the proposed criterion. Experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26435;&#37325;&#38170;&#23450;&#26469;&#23454;&#29616;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#21487;&#36991;&#20813;&#22312;&#20248;&#21270;&#29615;&#22659;&#19979;&#24573;&#30053;&#25110;&#24536;&#35760;&#26399;&#26395;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#21487;&#25805;&#32437;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10176</link><description>&lt;p&gt;
&#36890;&#36807;&#26435;&#37325;&#38170;&#23450;&#23454;&#29616;&#40065;&#26834;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Robust Deep Reinforcement Learning Scheduling via Weight Anchoring. (arXiv:2304.10176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26435;&#37325;&#38170;&#23450;&#26469;&#23454;&#29616;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#21487;&#36991;&#20813;&#22312;&#20248;&#21270;&#29615;&#22659;&#19979;&#24573;&#30053;&#25110;&#24536;&#35760;&#26399;&#26395;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23398;&#20064;&#26041;&#27861;&#20174;&#20223;&#30495;&#21040;&#29616;&#23454;&#20013;&#36328;&#36234;&#40511;&#27807;&#26102;&#65292;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26435;&#37325;&#38170;&#23450;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#24050;&#30693;&#65292;&#24182;&#29992;&#20110;&#22521;&#20859;&#21644;&#22266;&#23450;&#31070;&#32463;&#32593;&#32476;&#20013;&#26399;&#26395;&#30340;&#34892;&#20026;&#12290;&#21487;&#20197;&#20351;&#29992;&#26435;&#37325;&#38170;&#23450;&#26041;&#27861;&#25214;&#21040;&#19968;&#20010;&#19982;&#21478;&#19968;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#25509;&#36817;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#12290;&#36825;&#26679;&#65292;&#22312;&#20248;&#21270;&#29615;&#22659;&#19979;&#36827;&#34892;&#23398;&#20064;&#26102;&#65292;&#19981;&#20250;&#24573;&#30053;&#25110;&#24536;&#35760;&#26399;&#26395;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#28151;&#21512;&#30340;QoS&#39640;&#25928;&#31163;&#25955;&#36164;&#28304;&#35843;&#24230;&#19982;&#19981;&#39057;&#32321;&#30340;&#20248;&#20808;&#28040;&#24687;&#30340;&#31034;&#20363;&#26469;&#28436;&#31034;&#27492;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19982;&#22686;&#21152;&#20223;&#30495;&#29615;&#22659;&#30340;&#29616;&#26377;&#25216;&#26415;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Questions remain on the robustness of data-driven learning methods when crossing the gap from simulation to reality. We utilize weight anchoring, a method known from continual learning, to cultivate and fixate desired behavior in Neural Networks. Weight anchoring may be used to find a solution to a learning problem that is nearby the solution of another learning problem. Thereby, learning can be carried out in optimal environments without neglecting or unlearning desired behavior. We demonstrate this approach on the example of learning mixed QoS-efficient discrete resource scheduling with infrequent priority messages. Results show that this method provides performance comparable to the state of the art of augmenting a simulation environment, alongside significantly increased robustness and steerability.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#38376;&#25511;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;&#26368;&#32456;&#24635;&#32467;&#20102;&#24320;&#21457;&#28145;&#24230;&#37327;&#23376;&#23398;&#20064;&#30340;&#21069;&#26223;&#21644;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2304.10159</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Using Hybrid Quantum Neural Network. (arXiv:2304.10159v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10159
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#38376;&#25511;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;&#26368;&#32456;&#24635;&#32467;&#20102;&#24320;&#21457;&#28145;&#24230;&#37327;&#23376;&#23398;&#20064;&#30340;&#21069;&#26223;&#21644;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#23545;&#20110;&#20419;&#36827;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22788;&#29702;&#26356;&#39640;&#25968;&#25454;&#32500;&#24230;&#25110;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24635;&#20307;&#35757;&#32451;&#21442;&#25968;&#30340;&#38480;&#21046;&#20855;&#26377;&#24378;&#28872;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#38376;&#25511;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230; Q-Learning &#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#20854;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#24182;&#22521;&#35757;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#26032;&#30340; Qiskit &#21644; PyTorch &#26694;&#26550;&#30340;&#26032;&#22411; PQC&#65292;&#20197;&#19982;&#23436;&#20840;&#32463;&#20856;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#65292;&#24102;&#25110;&#19981;&#24102;&#38598;&#25104; PQC&#12290;&#30740;&#31350;&#26368;&#21518;&#24635;&#32467;&#20102;&#20854;&#20851;&#20110;&#24320;&#21457;&#28145;&#24230;&#37327;&#23376;&#23398;&#20064;&#35299;&#20915;&#36855;&#23467;&#38382;&#39064;&#25110;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#21069;&#26223;&#21644;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computation has a strong implication for advancing the current limitation of machine learning algorithms to deal with higher data dimensions or reducing the overall training parameters for a deep neural network model. Based on a gate-based quantum computer, a parameterized quantum circuit was designed to solve a model-free reinforcement learning problem with the deep-Q learning method. This research has investigated and evaluated its potential. Therefore, a novel PQC based on the latest Qiskit and PyTorch framework was designed and trained to compare with a full-classical deep neural network with and without integrated PQC. At the end of the research, the research draws its conclusion and prospects on developing deep quantum learning in solving a maze problem or other reinforcement learning problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;K&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#21464;&#20307;&#65292;&#21487;&#20197;&#30830;&#20445;&#26368;&#36817;&#37051;&#23621;&#30830;&#23454;&#25509;&#36817;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#24182;&#22312;&#36807;&#31243;&#20013;&#25214;&#21040;K&#20540;&#12290;&#19982;&#26631;&#20934;KNN&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#22312;&#23460;&#20869;&#25351;&#32441;&#23450;&#20301;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.10151</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;K&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#65306;&#25512;&#23548;&#21644;&#22312;&#22522;&#20110;&#31163;&#23376;&#36801;&#31227;&#35889;&#30340;&#23460;&#20869;&#23450;&#20301;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible K Nearest Neighbors Classifier: Derivation and Application for Ion-mobility Spectrometry-based Indoor Localization. (arXiv:2304.10151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;K&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#21464;&#20307;&#65292;&#21487;&#20197;&#30830;&#20445;&#26368;&#36817;&#37051;&#23621;&#30830;&#23454;&#25509;&#36817;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#24182;&#22312;&#36807;&#31243;&#20013;&#25214;&#21040;K&#20540;&#12290;&#19982;&#26631;&#20934;KNN&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#22312;&#23460;&#20869;&#25351;&#32441;&#23450;&#20301;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
K&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#24191;&#27867;&#24212;&#29992;&#20110;&#25351;&#32441;&#23450;&#20301;&#25110;&#21307;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#23427;&#22522;&#20110;K&#20010;&#26631;&#35760;&#26679;&#26412;&#65292;&#21363;&#26368;&#36817;&#37051;&#23621;&#30340;&#31867;&#25104;&#21592;&#36523;&#20221;&#65292;&#20915;&#23450;&#26410;&#26631;&#35760;&#26679;&#26412;&#30340;&#31867;&#25104;&#21592;&#36523;&#20221;&#12290;K&#30340;&#36873;&#25321;&#19968;&#30452;&#26159;&#21508;&#31181;&#30740;&#31350;&#21644;&#25552;&#20986;KNN&#21464;&#20307;&#30340;&#20027;&#39064;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#21464;&#20307;&#34987;&#35777;&#26126;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#21464;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KNN&#21464;&#20307;&#65292;&#30830;&#20445;K&#20010;&#26368;&#36817;&#37051;&#23621;&#30830;&#23454;&#25509;&#36817;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#24182;&#22312;&#36807;&#31243;&#20013;&#25214;&#21040;K&#20540;&#12290;&#35813;&#31639;&#27861;&#22312;&#29702;&#35770;&#24773;&#26223;&#21644;&#22522;&#20110;&#31163;&#23376;&#36801;&#31227;&#35889;&#25351;&#32441;&#30340;&#23460;&#20869;&#23450;&#20301;&#26041;&#38754;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#31639;&#27861;&#22312;&#19982;KNN&#21516;&#26679;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#27604;KNN&#26356;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The K Nearest Neighbors (KNN) classifier is widely used in many fields such as fingerprint-based localization or medicine. It determines the class membership of unlabelled sample based on the class memberships of the K labelled samples, the so-called nearest neighbors, that are closest to the unlabelled sample. The choice of K has been the topic of various studies and proposed KNN-variants. Yet no variant has been proven to outperform all other variants. In this paper a new KNN-variant is proposed which ensures that the K nearest neighbors are indeed close to the unlabelled sample and finds K along the way. The proposed algorithm is tested and compared to the standard KNN in theoretical scenarios and for indoor localization based on ion-mobility spectrometry fingerprints. It achieves a higher classification accuracy than the KNN in the tests, while requiring having the same computational demand.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#36807;&#29109;&#27491;&#21017;&#21270;&#26469;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#30340;&#38590;&#24230;&#65292;&#24182;&#26681;&#25454;&#26679;&#26412;&#38590;&#24230;&#24809;&#32602;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2304.10127</link><description>&lt;p&gt;
&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23398;&#20064;&#26679;&#26412;&#38590;&#24230;&#20197;&#25552;&#39640;&#27169;&#22411;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning Sample Difficulty from Pre-trained Models for Reliable Prediction. (arXiv:2304.10127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#36807;&#29109;&#27491;&#21017;&#21270;&#26469;&#35745;&#31639;&#35757;&#32451;&#26679;&#26412;&#30340;&#38590;&#24230;&#65292;&#24182;&#26681;&#25454;&#26679;&#26412;&#38590;&#24230;&#24809;&#32602;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#31181;&#22330;&#26223;&#21644;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22914;&#20309;&#21033;&#29992;&#23427;&#20204;&#26469;&#25552;&#39640;&#19979;&#28216;&#27169;&#22411;&#30340;&#39044;&#27979;&#21487;&#38752;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#22312;&#22266;&#26377;&#26679;&#26412;&#38590;&#24230;&#21644;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#20570;&#20986;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#26679;&#26412;&#38590;&#24230;&#24863;&#30693;&#30340;&#29109;&#27491;&#21017;&#21270;&#26469;&#25351;&#23548;&#19979;&#28216;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#65292;&#19981;&#20250;&#36807;&#24230;&#25311;&#21512;&#19979;&#28216;&#35757;&#32451;&#38598;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#39640;&#26031;&#24314;&#27169;&#21644;&#30456;&#23545;&#39532;&#27663;&#36317;&#31163;&#30340;&#35745;&#31639;&#26469;&#27979;&#37327;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#38590;&#24230;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36890;&#36807;&#26681;&#25454;&#26679;&#26412;&#30340;&#38590;&#24230;&#33258;&#36866;&#24212;&#22320;&#24809;&#32602;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#25105;&#20204;&#21516;&#26102;&#25552;&#39640;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained models have achieved remarkable success in a variety of scenarios and applications, but how to leverage them to improve the prediction reliability of downstream models is undesirably under-explored. Moreover, modern neural networks have been found to be poorly calibrated and make overconfident predictions regardless of inherent sample difficulty and data uncertainty. To address this issue, we propose to utilize large-scale pre-trained models to guide downstream model training with sample difficulty-aware entropy regularization. Pre-trained models that have been exposed to large-scale datasets and do not overfit the downstream training classes enable us to measure each training sample difficulty via feature-space Gaussian modeling and relative Mahalanobis distance computation. Importantly, by adaptively penalizing overconfident prediction based on the sample's difficulty, we simultaneously improve accuracy and uncertainty calibration on various challenging benchm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#32806;&#20026;&#22810;&#20010;&#31616;&#21333;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#32463;&#20856;&#30340;&#21069;&#21521;&#35757;&#32451;&#21644;&#35774;&#35745;&#30340;&#21453;&#21521;&#35757;&#32451;&#12290;&#27599;&#20010;&#27169;&#22359;&#37117;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#31639;&#27861;&#22312;&#21069;&#21521;&#35757;&#32451;&#20013;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21453;&#21521;&#35757;&#32451;&#26426;&#21046;&#26469;&#20351;&#21069;&#38754;&#30340;&#27169;&#22359;&#33021;&#22815;&#24863;&#30693;&#21518;&#38754;&#30340;&#27169;&#22359;&#65292;&#20174;&#32780;&#20805;&#20998;&#35757;&#32451;&#27973;&#23618;&#27169;&#22359;&#21644;&#26356;&#28145;&#23618;&#30340;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2304.10126</link><description>&lt;p&gt;
&#35299;&#32806;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#31616;&#21333;&#30340;GNN&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decouple Graph Neural Networks: Train Multiple Simple GNNs Simultaneously Instead of One. (arXiv:2304.10126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#32806;&#20026;&#22810;&#20010;&#31616;&#21333;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#32463;&#20856;&#30340;&#21069;&#21521;&#35757;&#32451;&#21644;&#35774;&#35745;&#30340;&#21453;&#21521;&#35757;&#32451;&#12290;&#27599;&#20010;&#27169;&#22359;&#37117;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#31639;&#27861;&#22312;&#21069;&#21521;&#35757;&#32451;&#20013;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21453;&#21521;&#35757;&#32451;&#26426;&#21046;&#26469;&#20351;&#21069;&#38754;&#30340;&#27169;&#22359;&#33021;&#22815;&#24863;&#30693;&#21518;&#38754;&#30340;&#27169;&#22359;&#65292;&#20174;&#32780;&#20805;&#20998;&#35757;&#32451;&#27973;&#23618;&#27169;&#22359;&#21644;&#26356;&#28145;&#23618;&#30340;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23384;&#22312;&#20005;&#37325;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#33410;&#28857;&#20381;&#36182;&#38543;&#30528;&#23618;&#25968;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#36825;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#20351;&#24471;GNN&#30340;&#35757;&#32451;&#36890;&#24120;&#24456;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#22810;&#23618;GNN&#35299;&#32806;&#20026;&#22810;&#20010;&#31616;&#21333;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#30001;&#32463;&#20856;&#30340;&#21069;&#21521;&#35757;&#32451;&#65288;FT&#65289;&#21644;&#35774;&#35745;&#30340;&#21453;&#21521;&#35757;&#32451;&#65288;BT&#65289;&#32452;&#25104;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#19979;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#31639;&#27861;&#22312;FT&#20013;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;&#19981;&#20250;&#25197;&#26354;&#22270;&#24418;&#20449;&#24687;&#12290;&#20026;&#36991;&#20813;FT&#30340;&#21482;&#21333;&#21521;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#20805;&#20998;&#35757;&#32451;&#27973;&#23618;&#27169;&#22359;&#21644;&#26356;&#28145;&#23618;&#30340;&#27169;&#22359;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21453;&#21521;&#35757;&#32451;&#26426;&#21046;&#65292;&#20351;&#21069;&#38754;&#30340;&#27169;&#22359;&#33021;&#22815;&#24863;&#30693;&#21518;&#38754;&#30340;&#27169;&#22359;&#12290;&#36825;&#31181;&#21453;&#21521;&#35757;&#32451;&#24341;&#20837;&#20102;&#21453;&#21521;&#20449;&#24687;&#20256;&#36882;&#21040;&#35299;&#32806;&#27169;&#22359;&#20013;&#65292;&#21516;&#26102;&#20063;&#20250;&#26377;&#21069;&#21521;&#20449;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNN) suffer from severe inefficiency. It is mainly caused by the exponential growth of node dependency with the increase of layers. It extremely limits the application of stochastic optimization algorithms so that the training of GNN is usually time-consuming. To address this problem, we propose to decouple a multi-layer GNN as multiple simple modules for more efficient training, which is comprised of classical forward training (FT)and designed backward training (BT). Under the proposed framework, each module can be trained efficiently in FT by stochastic algorithms without distortion of graph information owing to its simplicity. To avoid the only unidirectional information delivery of FT and sufficiently train shallow modules with the deeper ones, we develop a backward training mechanism that makes the former modules perceive the latter modules. The backward training introduces the reversed information delivery into the decoupled modules as well as the forward i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#31216;&#36827;&#21270;&#35757;&#32451;&#65288;AET&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#35843;&#25972;&#65288;ADA&#65289;&#21644;&#29615;&#22659;&#38543;&#26426;&#21270;&#65288;ER&#65289;&#20248;&#21270;AET&#36807;&#31243;&#65292;&#20351;&#24471;AI&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;AMP&#28216;&#25103;&#20013;&#20987;&#36133;&#39030;&#32423;&#20154;&#31867;&#29609;&#23478;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#20154;&#31867;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.10124</link><description>&lt;p&gt;
&#29992;&#22810;&#26234;&#33021;&#20307;&#38750;&#23545;&#31216;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#25484;&#25569;&#38750;&#23545;&#31216;&#22810;&#20154;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Mastering Asymmetrical Multiplayer Game with Multi-Agent Asymmetric-Evolution Reinforcement Learning. (arXiv:2304.10124v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10124
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#31216;&#36827;&#21270;&#35757;&#32451;&#65288;AET&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#35843;&#25972;&#65288;ADA&#65289;&#21644;&#29615;&#22659;&#38543;&#26426;&#21270;&#65288;ER&#65289;&#20248;&#21270;AET&#36807;&#31243;&#65292;&#20351;&#24471;AI&#21487;&#20197;&#22312;&#22797;&#26434;&#30340;AMP&#28216;&#25103;&#20013;&#20987;&#36133;&#39030;&#32423;&#20154;&#31867;&#29609;&#23478;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#20154;&#31867;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#23545;&#31216;&#22810;&#20154;&#28216;&#25103;&#65288;AMP&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#28216;&#25103;&#31867;&#22411;&#65292;&#28041;&#21450;&#22810;&#31181;&#31867;&#22411;&#30340;&#20195;&#29702;&#22312;&#28216;&#25103;&#20013;&#30456;&#20114;&#31454;&#20105;&#25110;&#21512;&#20316;&#12290;&#30001;&#20110;&#38750;&#23545;&#31216;&#29615;&#22659;&#20013;&#26377;&#19981;&#24179;&#34913;&#30340;&#29305;&#24615;&#65292;&#22240;&#27492;&#20351;&#29992;&#20856;&#22411;&#30340;&#33258;&#25105;&#21338;&#24328;&#35757;&#32451;&#26041;&#27861;&#35757;&#32451;&#24378;&#22823;&#30340;&#20195;&#29702;&#20197;&#20987;&#36133;&#39030;&#32423;&#20154;&#31867;&#29609;&#23478;&#22312;AMP&#28216;&#25103;&#20013;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#23545;&#31216;&#36827;&#21270;&#35757;&#32451;&#65288;AET&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#35757;&#32451;&#22810;&#31181;&#20195;&#29702;&#22312;AMP&#28216;&#25103;&#20013;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#36866;&#24212;&#25968;&#25454;&#35843;&#25972;&#65288;ADA&#65289;&#21644;&#29615;&#22659;&#38543;&#26426;&#21270;&#65288;ER&#65289;&#26469;&#20248;&#21270;AET&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21517;&#20026;Tom&#65286;Jerry&#30340;&#22797;&#26434;AMP&#28216;&#25103;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;&#30340;AI&#23545;65&#20010;&#27604;&#36187;&#21462;&#24471;&#20102;98.5&#65285;&#30340;&#32988;&#29575;&#65292;&#32780;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#20154;&#31867;&#25968;&#25454;&#12290;&#28040;&#34701;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22359;&#26377;&#30410;&#20110;&#35813;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Asymmetrical multiplayer (AMP) game is a popular game genre which involves multiple types of agents competing or collaborating with each other in the game. It is difficult to train powerful agents that can defeat top human players in AMP games by typical self-play training method because of unbalancing characteristics in their asymmetrical environments. We propose asymmetric-evolution training (AET), a novel multi-agent reinforcement learning framework that can train multiple kinds of agents simultaneously in AMP game. We designed adaptive data adjustment (ADA) and environment randomization (ER) to optimize the AET process. We tested our method in a complex AMP game named Tom \&amp; Jerry, and our AIs trained without using any human data can achieve a win rate of 98.5% against top human players over 65 matches. The ablation experiments indicated that the proposed modules are beneficial to the framework.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35762;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26816;&#27979;&#37319;&#36141;&#27450;&#35784;&#65292;&#38024;&#23545;&#27599;&#20010;&#37319;&#36141;&#20107;&#20214;&#20351;&#29992;9&#20010;&#29305;&#23450;&#29305;&#24449;&#26500;&#24314;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#27979;&#35797;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#26816;&#27979;&#37319;&#36141;&#27450;&#35784;&#26041;&#38754;&#26159;&#26377;&#29992;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.10105</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#21160;&#37319;&#36141;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automatic Procurement Fraud Detection with Machine Learning. (arXiv:2304.10105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35762;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26816;&#27979;&#37319;&#36141;&#27450;&#35784;&#65292;&#38024;&#23545;&#27599;&#20010;&#37319;&#36141;&#20107;&#20214;&#20351;&#29992;9&#20010;&#29305;&#23450;&#29305;&#24449;&#26500;&#24314;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#27979;&#35797;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#26816;&#27979;&#37319;&#36141;&#27450;&#35784;&#26041;&#38754;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#37319;&#36141;&#27450;&#35784;&#19968;&#30452;&#26159;&#20960;&#20046;&#25152;&#26377;&#33258;&#30001;&#24066;&#22330;&#30340;&#19968;&#20010;&#20005;&#37325;&#38382;&#39064;&#65292;&#20294;&#23457;&#35745;&#37096;&#38376;&#20173;&#28982;&#24378;&#28872;&#20381;&#36182;&#26469;&#33258;&#30693;&#24773;&#20154;&#22763;&#30340;&#25253;&#21578;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#19982;&#25105;&#20204;&#24951;&#24936;&#30340;&#21512;&#20316;&#20249;&#20276;&#39034;&#20016;&#36895;&#36816;&#20998;&#20139;2015&#24180;&#33267;2017&#24180;&#20854;&#20844;&#21496;&#36827;&#34892;&#30340;&#37319;&#36141;&#30456;&#20851;&#25968;&#25454;&#24211;&#30340;&#35775;&#38382;&#26435;&#21518;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#24110;&#21161;&#23457;&#35745;&#20013;&#22269;&#30446;&#21069;&#26368;&#28145;&#21051;&#30340;&#29359;&#32618;&#20043;&#19968;&#65292;&#21363;&#37319;&#36141;&#27450;&#35784;&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#37319;&#36141;&#20107;&#20214;&#34920;&#31034;&#20026;9&#20010;&#29305;&#23450;&#29305;&#24449;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#35782;&#21035;&#21487;&#30097;&#37319;&#36141;&#24182;&#23545;&#20854;&#27450;&#35784;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#37319;&#36141;&#25968;&#25454;&#24211;&#25910;&#38598;&#30340;50000&#20010;&#26679;&#26412;&#19978;&#27979;&#35797;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#23613;&#31649;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#20294;&#23545;&#20110;&#26816;&#27979;&#37319;&#36141;&#27450;&#35784;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although procurement fraud is always a critical problem in almost every free market, audit departments still have a strong reliance on reporting from informed sources when detecting them. With our generous cooperator, SF Express, sharing the access to the database related with procurements took place from 2015 to 2017 in their company, our team studies how machine learning techniques could help with the audition of one of the most profound crime among current chinese market, namely procurement frauds. By representing each procurement event as 9 specific features, we construct neural network models to identify suspicious procurements and classify their fraud types. Through testing our models over 50000 samples collected from the procurement database, we have proven that such models -- despite having space for improvements -- are useful in detecting procurement frauds.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;AUC&#20998;&#25968;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#21160;&#37327;&#31639;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10101</link><description>&lt;p&gt;
&#32852;&#37030;&#32452;&#21512;&#28145;&#24230;AUC&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Federated Compositional Deep AUC Maximization. (arXiv:2304.10101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;AUC&#20998;&#25968;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#21160;&#37327;&#31639;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30001;&#20110;&#24179;&#34913;&#38544;&#31169;&#21644;&#22823;&#35268;&#27169;&#23398;&#20064;&#30340;&#25215;&#35834;&#32780;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65307;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#22788;&#29702;&#24179;&#34913;&#25968;&#25454;&#38382;&#39064;&#19978;&#65292;&#32780;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#19981;&#21516;&#31867;&#21035;&#20013;&#30340;&#26679;&#26412;&#25968;&#37327;&#39640;&#24230;&#19981;&#24179;&#34913;&#65292;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#36828;&#20302;&#20110;&#29702;&#24819;&#27700;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20998;&#25968;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;AUC&#26368;&#22823;&#21270;&#38382;&#39064;&#20316;&#20026;&#32852;&#37030;&#32452;&#21512;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#34920;&#36848;&#65292;&#24182;&#24320;&#21457;&#20102;&#26412;&#22320;&#38543;&#26426;&#32452;&#21512;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#21160;&#37327;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#30340;&#30028;&#38480;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#22914;&#27492;&#26377;&#21033;&#29702;&#35770;&#32467;&#26524;&#30340;&#24037;&#20316;&#12290;&#26368;&#21518;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has attracted increasing attention due to the promise of balancing privacy and large-scale learning; numerous approaches have been proposed. However, most existing approaches focus on problems with balanced data, and prediction performance is far from satisfactory for many real-world applications where the number of samples in different classes is highly imbalanced. To address this challenging problem, we developed a novel federated learning method for imbalanced data by directly optimizing the area under curve (AUC) score. In particular, we formulate the AUC maximization problem as a federated compositional minimax optimization problem, develop a local stochastic compositional gradient descent ascent with momentum algorithm, and provide bounds on the computational and communication complexities of our algorithm. To the best of our knowledge, this is the first work to achieve such favorable theoretical results. Finally, extensive experimental results confirm the effi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#35760;&#24518;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702; (2M)&#65292;&#23427;&#32467;&#21512;&#20102;&#24773;&#33410;&#35760;&#24518;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#28857;&#26469;&#25552;&#39640;&#23398;&#20064;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10098</link><description>&lt;p&gt;
&#21452;&#35760;&#24518;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Two-Memory Reinforcement Learning. (arXiv:2304.10098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#35760;&#24518;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702; (2M)&#65292;&#23427;&#32467;&#21512;&#20102;&#24773;&#33410;&#35760;&#24518;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#28857;&#26469;&#25552;&#39640;&#23398;&#20064;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#32463;&#39564;&#24615;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#22870;&#21169;&#20449;&#24687;&#20256;&#25773;&#21644;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#26356;&#26032;&#30340;&#36895;&#24230;&#36739;&#24930;&#65292;&#23427;&#20542;&#21521;&#20110;&#23398;&#20064;&#24471;&#27604;&#36739;&#24930;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38750;&#21442;&#25968;&#21270;&#30340;&#24773;&#33410;&#35760;&#24518;&#25552;&#20379;&#20102;&#30456;&#23545;&#36739;&#24555;&#30340;&#23398;&#20064;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#19981;&#38656;&#35201;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#26368;&#22823;&#24773;&#33410;&#22238;&#25253;&#20316;&#20026;&#29366;&#24577;-&#21160;&#20316;&#20540;&#36827;&#34892;&#34892;&#21160;&#36873;&#25321;&#12290;&#24773;&#33410;&#35760;&#24518;&#21644;&#24378;&#21270;&#23398;&#20064;&#37117;&#26377;&#21508;&#33258;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20154;&#31867;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#22810;&#20010;&#35760;&#24518;&#31995;&#32479;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#20174;&#20013;&#33719;&#30410;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#35760;&#24518;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;2M&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#24773;&#33410;&#35760;&#24518;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#28857;&#12290; 2M &#20195;&#29702;&#21033;&#29992;&#24773;&#33410;&#35760;&#24518;&#37096;&#20998;&#30340;&#36895;&#24230;&#21644;&#24378;&#21270;&#23398;&#20064;&#37096;&#20998;&#30340;&#26368;&#20248;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#30456;&#20114;&#34917;&#20805;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weaknesses. Notably, humans can leverage multiple memory systems concurrently during learning and benefit from all of them. In this work, we propose a method called Two-Memory reinforcement learning agent (2M) that combines episodic memory and reinforcement learning to distill both of their strengths. The 2M agent exploits the speed of the episodic memory part and the optimality and the generalization capacity of the reinforcement learning part to complement each other. Our experiments demonstrate that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#24046;&#20998;&#22270;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#26410;&#33021;&#26126;&#30830;&#24314;&#27169;&#36793;&#26102;&#24207;&#29366;&#24577;&#21644;&#25552;&#21462;&#20840;&#23616;&#32467;&#26500;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10079</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#24102;&#26377;&#36793;&#26102;&#24207;&#29366;&#24577;&#30340;&#24490;&#29615;Transformer
&lt;/p&gt;
&lt;p&gt;
Recurrent Transformer for Dynamic Graph Representation Learning with Edge Temporal States. (arXiv:2304.10079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#24046;&#20998;&#22270;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#26410;&#33021;&#26126;&#30830;&#24314;&#27169;&#36793;&#26102;&#24207;&#29366;&#24577;&#21644;&#25552;&#21462;&#20840;&#23616;&#32467;&#26500;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#22270;&#25968;&#25454;&#20998;&#26512;&#30340;&#24191;&#27867;&#38656;&#27714;&#65292;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#27491;&#25104;&#20026;&#19968;&#39033;&#36235;&#21183;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#20219;&#21153;&#12290;&#23613;&#31649;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23637;&#29616;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#26126;&#30830;&#22320;&#23545;&#33410;&#28857;&#29305;&#24449;&#38543;&#26102;&#38388;&#29255;&#27573;&#30340;&#36793;&#26102;&#24207;&#29366;&#24577;&#20135;&#29983;&#24433;&#21709;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;GNNs&#30340;&#20869;&#22312;over-smoothing&#32570;&#38519;&#65292;&#23427;&#20204;&#24456;&#38590;&#25552;&#21462;&#20840;&#23616;&#32467;&#26500;&#29305;&#24449;&#65292;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24490;&#29615;&#24046;&#20998;&#22270;&#21464;&#25442;&#22120;&#65288;RDGT&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#39318;&#20808;&#20026;&#27599;&#20010;&#24555;&#29031;&#20013;&#30340;&#36793;&#20998;&#37197;&#20102;&#21508;&#31181;&#31867;&#22411;&#21644;&#26435;&#37325;&#65292;&#20197;&#26126;&#30830;&#22320;&#35828;&#26126;&#23427;&#20204;&#30340;&#29305;&#23450;&#26102;&#38388;&#29366;&#24577;&#65292;&#28982;&#21518;&#37319;&#29992;&#22686;&#24378;&#32467;&#26500;&#30340;&#22270;&#21464;&#25442;&#22120;&#26469;&#36890;&#36807;&#24490;&#29615;&#23398;&#20064;&#33539;&#24335;&#25429;&#33719;&#26102;&#38388;&#33410;&#28857;&#34920;&#31034;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph representation learning is growing as a trending yet challenging research task owing to the widespread demand for graph data analysis in real world applications. Despite the encouraging performance of many recent works that build upon recurrent neural networks (RNNs) and graph neural networks (GNNs), they fail to explicitly model the impact of edge temporal states on node features over time slices. Additionally, they are challenging to extract global structural features because of the inherent over-smoothing disadvantage of GNNs, which further restricts the performance. In this paper, we propose a recurrent difference graph transformer (RDGT) framework, which firstly assigns the edges in each snapshot with various types and weights to illustrate their specific temporal states explicitly, then a structure-reinforced graph transformer is employed to capture the temporal node representations by a recurrent learning paradigm. Experimental results on four real-world datasets d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#25216;&#24039;&#65292;&#29992;&#20110;&#22312;&#22810;&#33410;&#28857;&#20219;&#21153;&#19978;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21306;&#20998;&#30446;&#26631;&#33410;&#28857;&#21644;&#20854;&#20182;&#33410;&#28857;&#30340;&#26041;&#24335;&#25913;&#36827;&#20102;&#20197;&#24448;&#30452;&#25509;&#32858;&#21512;&#21508;&#33410;&#28857;&#34920;&#31034;&#30340;&#32570;&#38519;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#33410;&#28857;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.10074</link><description>&lt;p&gt;
&#21033;&#29992;&#26631;&#31614;&#25216;&#24039;&#22312;&#22810;&#33410;&#28857;&#20219;&#21153;&#19978;&#25913;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Improving Graph Neural Networks on Multi-node Tasks with Labeling Tricks. (arXiv:2304.10074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#25216;&#24039;&#65292;&#29992;&#20110;&#22312;&#22810;&#33410;&#28857;&#20219;&#21153;&#19978;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21306;&#20998;&#30446;&#26631;&#33410;&#28857;&#21644;&#20854;&#20182;&#33410;&#28857;&#30340;&#26041;&#24335;&#25913;&#36827;&#20102;&#20197;&#24448;&#30452;&#25509;&#32858;&#21512;&#21508;&#33410;&#28857;&#34920;&#31034;&#30340;&#32570;&#38519;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#33410;&#28857;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#22810;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#30340;&#29702;&#35770;&#65292;&#20854;&#20013;&#25105;&#20204;&#26377;&#20852;&#36259;&#23398;&#20064;&#30001;&#22810;&#20010;&#33410;&#28857;&#32452;&#25104;&#30340;&#33410;&#28857;&#38598;&#30340;&#34920;&#31034;&#65292;&#22914;&#19968;&#20010;&#38142;&#25509;&#12290;&#29616;&#26377;&#30340;GNN&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#23398;&#20064;&#21333;&#20010;&#33410;&#28857;&#34920;&#31034;&#12290;&#24403;&#25105;&#20204;&#24819;&#35201;&#23398;&#20064;&#28041;&#21450;&#22810;&#20010;&#33410;&#28857;&#30340;&#33410;&#28857;&#38598;&#34920;&#31034;&#26102;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#30452;&#25509;&#32858;&#21512;GNN&#33719;&#24471;&#30340;&#21333;&#33410;&#28857;&#34920;&#31034;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#22522;&#26412;&#23616;&#38480;&#24615;&#65292;&#21363;&#19981;&#33021;&#25429;&#25417;&#33410;&#28857;&#38598;&#20013;&#22810;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#20063;&#35748;&#20026;&#30452;&#25509;&#32858;&#21512;&#21508;&#20010;&#33410;&#28857;&#30340;&#34920;&#31034;&#26080;&#27861;&#20026;&#22810;&#20010;&#33410;&#28857;&#20135;&#29983;&#26377;&#25928;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#19968;&#20010;&#30452;&#25509;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21306;&#20998;&#30446;&#26631;&#33410;&#28857;&#21644;&#20854;&#20182;&#33410;&#28857;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26631;&#31614;&#25216;&#24039;&#8221;&#65292;&#23427;&#39318;&#20808;&#26681;&#25454;&#19982;&#30446;&#26631;&#33410;&#28857;&#38598;&#30340;&#20851;&#31995;&#26631;&#35760;&#22270;&#20013;&#30340;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a theory of using graph neural networks (GNNs) for \textit{multi-node representation learning}, where we are interested in learning a representation for a set of more than one node such as a link. Existing GNNs are mainly designed to learn single-node representations. When we want to learn a node-set representation involving multiple nodes, a common practice in previous works is to directly aggregate the single-node representations obtained by a GNN. In this paper, we show a fundamental limitation of such an approach, namely the inability to capture the dependence among multiple nodes in a node set, and argue that directly aggregating individual node representations fails to produce an effective joint representation for multiple nodes. A straightforward solution is to distinguish target nodes from others. Formalizing this idea, we propose \text{labeling trick}, which first labels nodes in the graph according to their relationships with the target node set befo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#27700;&#26524;&#37319;&#25688;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#35782;&#21035;&#34955;&#20498;&#31354;&#20107;&#20214;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20892;&#20316;&#29289;&#31649;&#29702;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10068</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#27700;&#26524;&#37319;&#25688;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Fruit Picker Activity Recognition with Wearable Sensors and Machine Learning. (arXiv:2304.10068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#27700;&#26524;&#37319;&#25688;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#35782;&#21035;&#34955;&#20498;&#31354;&#20107;&#20214;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20892;&#20316;&#29289;&#31649;&#29702;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24212;&#29992;&#65292;&#22522;&#20110;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#26816;&#27979;&#27700;&#26524;&#37319;&#25688;&#27963;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25163;&#27573;&#26469;&#26816;&#27979;&#37319;&#25688;&#34955;&#20498;&#31354;&#20107;&#20214;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#25968;&#25454;&#26631;&#27880;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#27169;&#22411;&#21644;&#28145;&#24230;&#24490;&#29615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#19982;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20934;&#30830;&#29575;&#20026;97.92%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#39640;&#31934;&#24230;&#35782;&#21035;&#20102;&#34955;&#20498;&#31354;&#20107;&#20214;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#36827;&#34892;&#20892;&#20316;&#29289;&#31649;&#29702;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a novel application of detecting fruit picker activities based on time series data generated from wearable sensors. During harvesting, fruit pickers pick fruit into wearable bags and empty these bags into harvesting bins located in the orchard. Once full, these bins are quickly transported to a cooled pack house to improve the shelf life of picked fruits. For farmers and managers, the knowledge of when a picker bag is emptied is important for managing harvesting bins more effectively to minimise the time the picked fruit is left out in the heat (resulting in reduced shelf life). We propose a means to detect these bag-emptying events using human activity recognition with wearable sensors and machine learning methods. We develop a semi-supervised approach to labelling the data. A feature-based machine learning ensemble model and a deep recurrent convolutional neural network are developed and tested on a real-world dataset. When compared, the neural network achiev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20986;Allegro&#26550;&#26500;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;GPU&#24182;&#34892;&#21270;&#23454;&#29616;&#20102;&#23545;&#26497;&#22823;&#20998;&#23376;&#30340;&#39640;&#31934;&#24230;&#27169;&#25311;&#65292;&#33021;&#22815;&#23545;&#31354;&#21069;&#22797;&#26434;&#30340;&#32467;&#26500;&#36827;&#34892;&#37327;&#23376;&#20445;&#30495;&#21160;&#21147;&#23398;&#25551;&#36848;&#65292;&#24182;&#25104;&#21151;&#36827;&#34892;&#20102;&#23545;4400&#19975;&#21407;&#23376;&#30340;HIV&#22806;&#22771;&#30340;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2304.10061</link><description>&lt;p&gt;
&#23558;&#28145;&#24230;&#31561;&#21464;&#27169;&#22411;&#30340;&#39046;&#20808;&#31934;&#24230;&#25193;&#23637;&#21040;&#30495;&#23454;&#23610;&#23544;&#29983;&#29289;&#20998;&#23376;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Scaling the leading accuracy of deep equivariant models to biomolecular simulations of realistic size. (arXiv:2304.10061v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20986;Allegro&#26550;&#26500;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;GPU&#24182;&#34892;&#21270;&#23454;&#29616;&#20102;&#23545;&#26497;&#22823;&#20998;&#23376;&#30340;&#39640;&#31934;&#24230;&#27169;&#25311;&#65292;&#33021;&#22815;&#23545;&#31354;&#21069;&#22797;&#26434;&#30340;&#32467;&#26500;&#36827;&#34892;&#37327;&#23376;&#20445;&#30495;&#21160;&#21147;&#23398;&#25551;&#36848;&#65292;&#24182;&#25104;&#21151;&#36827;&#34892;&#20102;&#23545;4400&#19975;&#21407;&#23376;&#30340;HIV&#22806;&#22771;&#30340;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#26032;&#30340;&#27169;&#22411;&#26550;&#26500;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#20197;&#21450;&#38024;&#23545;&#39640;&#25928;GPU&#21033;&#29992;&#29575;&#36827;&#34892;&#20248;&#21270;&#30340;&#27169;&#22411;&#21644;&#23454;&#29616;&#65292;&#23558;&#28145;&#24230;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#39046;&#20808;&#31934;&#24230;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#21457;&#25381;&#21040;&#26497;&#33268;&#35745;&#31639;&#35268;&#27169;&#12290;Allegro&#26550;&#26500;&#25104;&#21151;&#35299;&#20915;&#20102;&#21407;&#23376;&#27169;&#25311;&#30340;&#31934;&#24230;-&#36895;&#24230;&#26435;&#34913;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#31354;&#21069;&#22797;&#26434;&#32467;&#26500;&#30340;&#37327;&#23376;&#20445;&#30495;&#21160;&#21147;&#23398;&#25551;&#36848;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;Allegro&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#36827;&#34892;&#20102;&#32435;&#31186;&#32423;&#34507;&#30333;&#21160;&#21147;&#23398;&#31283;&#23450;&#27169;&#25311;&#65292;&#24182;&#22312;Perlmutter&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#25193;&#23637;&#21040;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#12289;&#25152;&#26377;&#21407;&#23376;&#26126;&#30830;&#28342;&#21058;&#21270;&#30340;4400&#19975;&#21407;&#23376;&#32467;&#26500;&#30340;HIV&#22806;&#22771;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;100&#19975;&#21407;&#23376;&#30340;&#20248;&#24322;&#24378;&#25193;&#23637;&#24615;&#21644;5120&#20010;A100 GPU&#30340;70%&#24369;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work brings the leading accuracy, sample efficiency, and robustness of deep equivariant neural networks to the extreme computational scale. This is achieved through a combination of innovative model architecture, massive parallelization, and models and implementations optimized for efficient GPU utilization. The resulting Allegro architecture bridges the accuracy-speed tradeoff of atomistic simulations and enables description of dynamics in structures of unprecedented complexity at quantum fidelity. To illustrate the scalability of Allegro, we perform nanoseconds-long stable simulations of protein dynamics and scale up to a 44-million atom structure of a complete, all-atom, explicitly solvated HIV capsid on the Perlmutter supercomputer. We demonstrate excellent strong scaling up to 100 million atoms and 70% weak scaling to 5120 A100 GPUs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968; $\mathcal{L}_{\sigma}$ &#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20316;&#22312;&#32447;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#30340;&#40065;&#26834;&#26367;&#20195;&#26041;&#26696;&#12290;&#24182;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#36873;&#25321;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26080;&#23481;&#37327;&#20381;&#36182;&#30340;&#26368;&#20248;&#24615;&#25910;&#25947;&#24615;&#20197;&#21450;&#24378;&#25910;&#25947;&#30340;&#26368;&#20248;&#23481;&#37327;&#20381;&#36182;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.10060</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Optimality of Robust Online Learning. (arXiv:2304.10060v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968; $\mathcal{L}_{\sigma}$ &#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20316;&#22312;&#32447;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#30340;&#40065;&#26834;&#26367;&#20195;&#26041;&#26696;&#12290;&#24182;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#36873;&#25321;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26080;&#23481;&#37327;&#20381;&#36182;&#30340;&#26368;&#20248;&#24615;&#25910;&#25947;&#24615;&#20197;&#21450;&#24378;&#25910;&#25947;&#30340;&#26368;&#20248;&#23481;&#37327;&#20381;&#36182;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#19978;&#20351;&#29992;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968; $\mathcal{L}_{\sigma}$ &#36827;&#34892;&#22238;&#24402;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#20010;&#28041;&#21450;&#21040;&#32553;&#25918;&#21442;&#25968; $\sigma&gt;0$ &#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#35206;&#30422;&#19968;&#31995;&#21015;&#24120;&#29992;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#38024;&#23545;&#22312;&#32447;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#30340;&#40065;&#26834;&#26367;&#20195;&#26041;&#26696;&#65292;&#26088;&#22312;&#20272;&#35745;&#26465;&#20214;&#22343;&#20540;&#20989;&#25968;&#12290;&#22312;&#36873;&#25321;&#36866;&#24403;&#30340; $\sigma$ &#21644;&#27493;&#38271;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#22312;&#32447;&#31639;&#27861;&#30340;&#26368;&#32456;&#36845;&#20195;&#21487;&#20197;&#22312;&#22343;&#26041;&#36317;&#31163;&#19978;&#23454;&#29616;&#26080;&#23481;&#37327;&#20381;&#36182;&#30340;&#25910;&#25947;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#24050;&#30693;&#24213;&#23618;&#20989;&#25968;&#31354;&#38388;&#30340;&#20854;&#20182;&#20449;&#24687;&#65292;&#21017;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#24378;&#25910;&#25947;&#30340;&#26368;&#20248;&#23481;&#37327;&#20381;&#36182;&#36895;&#29575;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20004;&#20010;&#32467;&#26524;&#37117;&#26159;&#22312;&#32447;&#23398;&#20064;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study an online learning algorithm with a robust loss function $\mathcal{L}_{\sigma}$ for regression over a reproducing kernel Hilbert space (RKHS). The loss function $\mathcal{L}_{\sigma}$ involving a scaling parameter $\sigma&gt;0$ can cover a wide range of commonly used robust losses. The proposed algorithm is then a robust alternative for online least squares regression aiming to estimate the conditional mean function. For properly chosen $\sigma$ and step size, we show that the last iterate of this online algorithm can achieve optimal capacity independent convergence in the mean square distance. Moreover, if additional information on the underlying function space is known, we also establish optimal capacity dependent rates for strong convergence in RKHS. To the best of our knowledge, both of the two results are new to the existing literature of online learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;HyperTuner&#65292;&#23427;&#26159;&#19968;&#20010;&#36328;&#23618;&#22810;&#30446;&#26631;&#36229;&#21442;&#25968;&#33258;&#21160;&#35843;&#25972;&#26694;&#26550;&#65292;&#21487;&#21516;&#26102;&#32771;&#34385;&#27169;&#22411;&#36229;&#21442;&#25968;&#21644;&#31995;&#32479;&#21442;&#25968;&#12290;HyperTuner&#20351;&#29992;MOPIR&#31639;&#27861;&#36827;&#34892;&#22810;&#30446;&#26631;&#21442;&#25968;&#37325;&#35201;&#24615;&#25490;&#24207;&#65292;&#28982;&#21518;&#21033;&#29992;ADUMBO&#31639;&#27861;&#25214;&#21040;Pareto&#26368;&#20248;&#37197;&#32622;&#38598;&#21512;&#65292;&#24110;&#21161;&#25968;&#25454;&#20998;&#26512;&#26381;&#21153;&#25552;&#20379;&#21830;&#35299;&#20915;&#39640;&#32500;&#40657;&#30418;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10051</link><description>&lt;p&gt;
HyperTuner&#65306;&#25968;&#25454;&#20998;&#26512;&#26381;&#21153;&#30340;&#36328;&#23618;&#22810;&#30446;&#26631;&#36229;&#21442;&#25968;&#33258;&#21160;&#35843;&#25972;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HyperTuner: A Cross-Layer Multi-Objective Hyperparameter Auto-Tuning Framework for Data Analytic Services. (arXiv:2304.10051v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;HyperTuner&#65292;&#23427;&#26159;&#19968;&#20010;&#36328;&#23618;&#22810;&#30446;&#26631;&#36229;&#21442;&#25968;&#33258;&#21160;&#35843;&#25972;&#26694;&#26550;&#65292;&#21487;&#21516;&#26102;&#32771;&#34385;&#27169;&#22411;&#36229;&#21442;&#25968;&#21644;&#31995;&#32479;&#21442;&#25968;&#12290;HyperTuner&#20351;&#29992;MOPIR&#31639;&#27861;&#36827;&#34892;&#22810;&#30446;&#26631;&#21442;&#25968;&#37325;&#35201;&#24615;&#25490;&#24207;&#65292;&#28982;&#21518;&#21033;&#29992;ADUMBO&#31639;&#27861;&#25214;&#21040;Pareto&#26368;&#20248;&#37197;&#32622;&#38598;&#21512;&#65292;&#24110;&#21161;&#25968;&#25454;&#20998;&#26512;&#26381;&#21153;&#25552;&#20379;&#21830;&#35299;&#20915;&#39640;&#32500;&#40657;&#30418;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#38500;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#22806;&#65292;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#21644;&#33021;&#32791;&#31561;&#35843;&#33410;&#24847;&#22270;&#20063;&#20540;&#24471;&#21560;&#24341;&#25968;&#25454;&#20998;&#26512;&#26381;&#21153;&#25552;&#20379;&#21830;&#30340;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#21040;&#27169;&#22411;&#36229;&#21442;&#25968;&#21644;&#31995;&#32479;&#21442;&#25968;&#65292;&#25191;&#34892;&#36328;&#23618;&#22810;&#30446;&#26631;&#36229;&#21442;&#25968;&#33258;&#21160;&#35843;&#25972;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;HyperTuner&#12290;&#20026;&#20102;&#35299;&#20915;&#35813;&#39640;&#32500;&#40657;&#30418;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;HyperTuner&#39318;&#20808;&#20351;&#29992;MOPIR&#31639;&#27861;&#36827;&#34892;&#22810;&#30446;&#26631;&#21442;&#25968;&#37325;&#35201;&#24615;&#25490;&#24207;&#65292;&#28982;&#21518;&#21033;&#29992;&#25552;&#20986;&#30340;ADUMBO&#31639;&#27861;&#25214;&#21040;Pareto&#26368;&#20248;&#37197;&#32622;&#38598;&#21512;&#12290;&#22312;&#27599;&#20010;&#36845;&#20195;&#36807;&#31243;&#20013;&#65292;ADUMBO&#36890;&#36807;&#26368;&#22823;&#21270;&#19968;&#20010;&#26032;&#30340;&#35774;&#35745;&#33391;&#22909;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#33258;&#21160;&#29983;&#25104;Pareto&#20505;&#36873;&#38598;&#21512;&#65292;&#24182;&#36873;&#25321;&#26368;&#26377;&#21069;&#36884;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyper-parameters optimization (HPO) is vital for machine learning models. Besides model accuracy, other tuning intentions such as model training time and energy consumption are also worthy of attention from data analytic service providers. Hence, it is essential to take both model hyperparameters and system parameters into consideration to execute cross-layer multi-objective hyperparameter auto-tuning. Towards this challenging target, we propose HyperTuner in this paper. To address the formulated high-dimensional black-box multi-objective optimization problem, HyperTuner first conducts multi-objective parameter importance ranking with its MOPIR algorithm and then leverages the proposed ADUMBO algorithm to find the Pareto-optimal configuration set. During each iteration, ADUMBO selects the most promising configuration from the generated Pareto candidate set via maximizing a new well-designed metric, which can adaptively leverage the uncertainty as well as the predicted mean across all t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#27169;&#24577;&#32479;&#35745;&#26041;&#27861;&#30340;&#26368;&#20248;&#26680;&#20989;&#25968;&#30340;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#65288;&#22810;&#20803;&#65289;&#26368;&#20248;&#26680;&#20989;&#25968;&#65292;&#22312;&#26576;&#20010;&#26680;&#20989;&#25968;&#31867;&#21035;&#20013;&#20351;&#24471;&#20854;&#35299;&#26512;&#24471;&#21040;&#30340;&#28176;&#36817;&#35823;&#24046;&#20934;&#21017;&#26368;&#23567;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.10046</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#30340;&#27169;&#24577;&#32479;&#35745;&#26041;&#27861;&#30340;&#26368;&#20248;&#26680;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Optimal Kernel for Kernel-Based Modal Statistical Methods. (arXiv:2304.10046v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#27169;&#24577;&#32479;&#35745;&#26041;&#27861;&#30340;&#26368;&#20248;&#26680;&#20989;&#25968;&#30340;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#65288;&#22810;&#20803;&#65289;&#26368;&#20248;&#26680;&#20989;&#25968;&#65292;&#22312;&#26576;&#20010;&#26680;&#20989;&#25968;&#31867;&#21035;&#20013;&#20351;&#24471;&#20854;&#35299;&#26512;&#24471;&#21040;&#30340;&#28176;&#36817;&#35823;&#24046;&#20934;&#21017;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#27169;&#24577;&#32479;&#35745;&#26041;&#27861;&#21253;&#25324;&#27169;&#24577;&#20272;&#35745;&#12289;&#22238;&#24402;&#21644;&#32858;&#31867;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;&#25152;&#20351;&#29992;&#30340;&#26680;&#20989;&#25968;&#21644;&#24102;&#23485;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#20989;&#25968;&#36873;&#25321;&#23545;&#36825;&#20123;&#26041;&#27861;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#29305;&#21035;&#22320;&#65292;&#24403;&#20351;&#29992;&#26368;&#20248;&#24102;&#23485;&#26102;&#65292;&#26412;&#25991;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#19968;&#31181;&#65288;&#22810;&#20803;&#65289;&#26368;&#20248;&#26680;&#20989;&#25968;&#65292;&#20854;&#22312;&#23450;&#20041;&#20026;&#20854;&#31526;&#21495;&#21464;&#21270;&#25968;&#37327;&#30340;&#26576;&#20010;&#26680;&#20989;&#25968;&#31867;&#21035;&#20013;&#65292;&#20351;&#24471;&#20854;&#35299;&#26512;&#24471;&#21040;&#30340;&#28176;&#36817;&#35823;&#24046;&#20934;&#21017;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel-based modal statistical methods include mode estimation, regression, and clustering. Estimation accuracy of these methods depends on the kernel used as well as the bandwidth. We study effect of the selection of the kernel function to the estimation accuracy of these methods. In particular, we theoretically show a (multivariate) optimal kernel that minimizes its analytically-obtained asymptotic error criterion when using an optimal bandwidth, among a certain kernel class defined via the number of its sign changes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20221;&#28151;&#21512;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#36890;&#36807;&#22270;&#24418;&#22686;&#24378;&#24471;&#21040;&#30340;&#19981;&#21516;&#20294;&#30456;&#20284;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#26631;&#31614;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#25429;&#33719;&#12290;</title><link>http://arxiv.org/abs/2304.10045</link><description>&lt;p&gt;
ID-MixGCL: &#22522;&#20110;&#36523;&#20221;&#28151;&#21512;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ID-MixGCL: Identity Mixup for Graph Contrastive Learning. (arXiv:2304.10045v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20221;&#28151;&#21512;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#36890;&#36807;&#22270;&#24418;&#22686;&#24378;&#24471;&#21040;&#30340;&#19981;&#21516;&#20294;&#30456;&#20284;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#26631;&#31614;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#25429;&#33719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#23637;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#26159;&#27604;&#36739;&#21516;&#19968;&#20010;&#22270;&#24418;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#8220;&#35270;&#22270;&#8221;&#20197;&#23398;&#20064;&#33410;&#28857;/&#22270;&#24418;&#34920;&#31034;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#36890;&#36807;&#22270;&#24418;&#22686;&#24378;&#65292;&#21487;&#20197;&#29983;&#25104;&#20960;&#20010;&#32467;&#26500;&#19981;&#21516;&#20294;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#22240;&#27492;&#21407;&#22987;&#21644;&#22686;&#24378;&#30340;&#22270;&#24418;/&#33410;&#28857;&#30340;&#36523;&#20221;&#26631;&#31614;&#24212;&#35813;&#26159;&#30456;&#21516;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#65292;&#20363;&#22914;&#20998;&#23376;&#22270;&#20013;&#23545;&#33410;&#28857;&#25110;&#36793;&#30340;&#20219;&#20309;&#25200;&#21160;&#37117;&#20250;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25913;&#21464;&#22270;&#24418;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#22686;&#24378;&#22270;&#24418;&#32467;&#26500;&#24212;&#35813;&#20276;&#38543;&#30528;&#23545;&#23545;&#27604;&#25439;&#22833;&#20351;&#29992;&#30340;&#26631;&#31614;&#30340;&#36866;&#24212;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ID-MixGCL&#65292;&#23427;&#20801;&#35768;&#21516;&#26102;&#35843;&#33410;&#36755;&#20837;&#22270;&#24418;&#21644;&#30456;&#24212;&#30340;&#36523;&#20221;&#26631;&#31614;&#65292;&#20855;&#26377;&#21487;&#25511;&#30340;&#25913;&#21464;&#31243;&#24230;&#65292;&#20174;&#32780;&#25429;&#33719;&#32454;&#31890;&#24230;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently developed graph contrastive learning (GCL) approaches compare two different "views" of the same graph in order to learn node/graph representations. The core assumption of these approaches is that by graph augmentation, it is possible to generate several structurally different but semantically similar graph structures, and therefore, the identity labels of the original and augmented graph/nodes should be identical. However, in this paper, we observe that this assumption does not always hold, for example, any perturbation to nodes or edges in a molecular graph will change the graph labels to some degree. Therefore, we believe that augmenting the graph structure should be accompanied by an adaptation of the labels used for the contrastive loss. Based on this idea, we propose ID-MixGCL, which allows for simultaneous modulation of both the input graph and the corresponding identity labels, with a controllable degree of change, leading to the capture of fine-grained representations 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33041;&#32959;&#30244;&#20998;&#31867;&#21644;&#20998;&#21106;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;MRI&#22270;&#20687;&#20013;&#30340;&#32959;&#30244;&#20998;&#20026;&#22235;&#31867;&#65292;&#24182;&#23558;&#32959;&#30244;&#20174;&#22270;&#20687;&#20013;&#31934;&#30830;&#22320;&#20998;&#21106;&#20986;&#26469;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#20020;&#24202;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.10039</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33041;&#32959;&#30244;&#22810;&#20998;&#31867;&#21644;&#20998;&#21106;&#22312;MRO&#22270;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Brain tumor multi classification and segmentation in MRO images using deep learning. (arXiv:2304.10039v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33041;&#32959;&#30244;&#20998;&#31867;&#21644;&#20998;&#21106;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;MRI&#22270;&#20687;&#20013;&#30340;&#32959;&#30244;&#20998;&#20026;&#22235;&#31867;&#65292;&#24182;&#23558;&#32959;&#30244;&#20174;&#22270;&#20687;&#20013;&#31934;&#30830;&#22320;&#20998;&#21106;&#20986;&#26469;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#20020;&#24202;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25195;&#25551;&#20013;&#23545;&#33041;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#21106;&#12290;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;EfficientNetB1&#26550;&#26500;&#65292;&#32463;&#36807;&#35757;&#32451;&#65292;&#23558;&#22270;&#20687;&#20998;&#20026;&#22235;&#31867;&#65306;&#33041;&#33180;&#30244;&#65292;&#33014;&#36136;&#30244;&#65292;&#22402;&#20307;&#33146;&#30244;&#21644;&#26080;&#32959;&#30244;&#12290;&#20998;&#21106;&#27169;&#22411;&#22522;&#20110;U-Net&#26550;&#26500;&#65292;&#32463;&#36807;&#35757;&#32451;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20174;MRI&#22270;&#20687;&#20013;&#20998;&#21106;&#20986;&#32959;&#30244;&#12290;&#35813;&#27169;&#22411;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#20998;&#21106;&#25351;&#26631;&#65292;&#34920;&#26126;&#20854;&#22312;&#33041;&#32959;&#30244;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#20020;&#24202;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a deep learning model for the classification and segmentation of brain tumors from magnetic resonance imaging (MRI) scans. The classification model is based on the EfficientNetB1 architecture and is trained to classify images into four classes: meningioma, glioma, pituitary adenoma, and no tumor. The segmentation model is based on the U-Net architecture and is trained to accurately segment the tumor from the MRI images. The models are evaluated on a publicly available dataset and achieve high accuracy and segmentation metrics, indicating their potential for clinical use in the diagnosis and treatment of brain tumors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#20998;&#24067;&#22806;&#26816;&#27979;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#21487;&#20197;&#20998;&#35299;&#25104;&#20219;&#21153;&#20869;&#39044;&#27979;&#21644;&#20219;&#21153; ID &#39044;&#27979;&#65292;&#24182;&#19988;&#20219;&#21153; ID &#39044;&#27979;&#19982;&#20998;&#24067;&#22806;&#26816;&#27979;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2304.10038</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#25345;&#32493;&#23398;&#20064;&#65306;&#32479;&#19968;&#26032;&#39062;&#24615;&#26816;&#27979;&#19982;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Open-World Continual Learning: Unifying Novelty Detection and Continual Learning. (arXiv:2304.10038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#20998;&#24067;&#22806;&#26816;&#27979;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#21487;&#20197;&#20998;&#35299;&#25104;&#20219;&#21153;&#20869;&#39044;&#27979;&#21644;&#20219;&#21153; ID &#39044;&#27979;&#65292;&#24182;&#19988;&#20219;&#21153; ID &#39044;&#27979;&#19982;&#20998;&#24067;&#22806;&#26816;&#27979;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528; AI agent &#22312;&#26410;&#30693;&#25110;&#26032;&#22855;&#30340;&#30495;&#23454;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#23427;&#20204;&#38656;&#35201;&#20855;&#22791; (1) &#35748;&#35782;&#24050;&#32463;&#23398;&#20064;&#36807;&#30340;&#29289;&#20307;&#21644;&#26816;&#27979;&#21040;&#20043;&#21069;&#26410;&#35265;&#25110;&#23398;&#20064;&#30340;&#29289;&#20307;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450; (2) &#22686;&#37327;&#22320;&#23398;&#20064;&#26032;&#29289;&#21697;&#65292;&#36880;&#28176;&#21464;&#24471;&#26356;&#26377;&#30693;&#35782;&#21644;&#26356;&#24378;&#22823;&#12290; (1) &#31216;&#20026;&#26032;&#39062;&#24615;&#26816;&#27979;&#25110;&#20998;&#24067;&#22806; (OOD) &#26816;&#27979;&#65292;&#32780; (2) &#31216;&#20026;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064; (CIL)&#65292;&#26159;&#25345;&#32493;&#23398;&#20064; (CL) &#30340;&#19968;&#31181;&#35774;&#32622;&#12290;&#22312;&#29616;&#26377;&#30340;&#30740;&#31350;&#20013;&#65292;OOD &#26816;&#27979;&#21644; CIL &#34987;&#35270;&#20026;&#20004;&#20010;&#23436;&#20840;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102; OOD &#26816;&#27979;&#23454;&#38469;&#19978;&#23545;&#20110; CIL &#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034; CIL &#21487;&#20197;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65306;&#20219;&#21153;&#20869;&#39044;&#27979; (WP) &#21644;&#20219;&#21153; ID &#39044;&#27979;(TP)&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#20102; TP &#19982; OOD &#26816;&#27979;&#30456;&#20851;&#12290;&#20851;&#38190;&#30340;&#29702;&#35770;&#32467;&#26524;&#26159;&#65292;&#26080;&#35770; WP &#21644; OOD &#26816;&#27979;&#65288;&#25110; TP&#65289;&#26159;&#21542;&#30001; CIL &#31639;&#27861;&#26174;&#24335;&#25110;&#38544;&#24335;&#22320;&#23450;&#20041;&#65292;&#22909;&#30340; WP &#21644;&#33391;&#22909;&#30340; OOD &#26816;&#27979;&#25110; TP &#24635;&#26159;&#23384;&#22312;&#23884;&#20837;&#22312;&#20219;&#20309; CIL &#31639;&#27861;&#20013;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI agents are increasingly used in the real open world with unknowns or novelties, they need the ability to (1) recognize objects that (i) they have learned and (ii) detect items that they have not seen or learned before, and (2) learn the new items incrementally to become more and more knowledgeable and powerful. (1) is called novelty detection or out-of-distribution (OOD) detection and (2) is called class incremental learning (CIL), which is a setting of continual learning (CL). In existing research, OOD detection and CIL are regarded as two completely different problems. This paper theoretically proves that OOD detection actually is necessary for CIL. We first show that CIL can be decomposed into two sub-problems: within-task prediction (WP) and task-id prediction (TP). We then prove that TP is correlated with OOD detection. The key theoretical result is that regardless of whether WP and OOD detection (or TP) are defined explicitly or implicitly by a CIL algorithm, good WP and go
&lt;/p&gt;</description></item><item><title>&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#25552;&#20379;&#20102;&#20174;&#22797;&#26434;&#31995;&#32479;&#30456;&#20851;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#20840;&#38754;&#26550;&#26500;&#65292;&#36890;&#36807;&#35299;&#20915;&#29616;&#26377;&#24037;&#20316;&#30340;&#31526;&#21495;&#21644;&#26415;&#35821;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#24212;&#29992;&#31185;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#24320;&#25299;&#26032;&#23616;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.10031</link><description>&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#30340;&#26550;&#26500;&#65306;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Architectures of Topological Deep Learning: A Survey on Topological Neural Networks. (arXiv:2304.10031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10031
&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#25552;&#20379;&#20102;&#20174;&#22797;&#26434;&#31995;&#32479;&#30456;&#20851;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#20840;&#38754;&#26550;&#26500;&#65292;&#36890;&#36807;&#35299;&#20915;&#29616;&#26377;&#24037;&#20316;&#30340;&#31526;&#21495;&#21644;&#26415;&#35821;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#24212;&#29992;&#31185;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#24320;&#25299;&#26032;&#23616;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#30028;&#20013;&#20805;&#28385;&#20102;&#22797;&#26434;&#30340;&#31995;&#32479;&#65292;&#20854;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#23384;&#22312;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65306;&#20174;&#31038;&#20132;&#32593;&#32476;&#20013;&#20010;&#20307;&#20043;&#38388;&#30340;&#31038;&#20132;&#20114;&#21160;&#21040;&#34507;&#30333;&#36136;&#20013;&#21407;&#23376;&#20043;&#38388;&#30340;&#38745;&#30005;&#30456;&#20114;&#20316;&#29992;&#12290;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#65288;TDL&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#21644;&#20174;&#36825;&#20123;&#31995;&#32479;&#30456;&#20851;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#22914;&#39044;&#27979;&#19968;&#20010;&#20154;&#23646;&#20110;&#21738;&#20010;&#31038;&#21306;&#25110;&#39044;&#27979;&#19968;&#20010;&#34507;&#30333;&#36136;&#26159;&#21542;&#21487;&#20197;&#25104;&#20026;&#21512;&#29702;&#30340;&#33647;&#29289;&#24320;&#21457;&#38774;&#28857;&#12290;TDL&#24050;&#32463;&#35777;&#26126;&#25317;&#26377;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#30340;&#20248;&#21183;&#65292;&#36825;&#20026;&#22312;&#24212;&#29992;&#31185;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#24320;&#25299;&#26032;&#23616;&#38754;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;&#28982;&#32780;&#65292;TDL&#25991;&#29486;&#30340;&#24555;&#36895;&#22686;&#38271;&#20063;&#23548;&#33268;&#20102;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65288;TNN&#65289;&#20307;&#31995;&#32467;&#26500;&#31526;&#21495;&#21644;&#26415;&#35821;&#19978;&#30340;&#19981;&#19968;&#33268;&#12290;&#36825;&#23545;&#20110;&#24314;&#31435;&#22312;&#29616;&#26377;&#24037;&#20316;&#22522;&#30784;&#19978;&#21644;&#23558;TNN&#37096;&#32626;&#21040;&#26032;&#30340;&#29616;&#23454;&#38382;&#39064;&#20013;&#37117;&#26159;&#19968;&#20010;&#30495;&#27491;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#29702;&#35299;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The natural world is full of complex systems characterized by intricate relations between their components: from social interactions between individuals in a social network to electrostatic interactions between atoms in a protein. Topological Deep Learning (TDL) provides a comprehensive framework to process and extract knowledge from data associated with these systems, such as predicting the social community to which an individual belongs or predicting whether a protein can be a reasonable target for drug development. TDL has demonstrated theoretical and practical advantages that hold the promise of breaking ground in the applied sciences and beyond. However, the rapid growth of the TDL literature has also led to a lack of unification in notation and language across Topological Neural Network (TNN) architectures. This presents a real obstacle for building upon existing works and for deploying TNNs to new real-world problems. To address this issue, we provide an accessible introduction 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#38450;&#24481;&#26041;&#27861;&#8212;&#8212;Jedi&#65292;&#35813;&#26041;&#27861;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#34917;&#19969;&#23450;&#20301;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#26234;&#33021;&#32534;&#30721;&#22120;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#34917;&#19969;&#30340;&#23450;&#20301;&#31934;&#24230;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#25239;&#24615;&#34917;&#19969;&#30340;&#36890;&#29992;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2304.10029</link><description>&lt;p&gt;
Jedi: &#22522;&#20110;&#20449;&#24687;&#29109;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#23450;&#20301;&#19982;&#28165;&#38500;
&lt;/p&gt;
&lt;p&gt;
Jedi: Entropy-based Localization and Removal of Adversarial Patches. (arXiv:2304.10029v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#38450;&#24481;&#26041;&#27861;&#8212;&#8212;Jedi&#65292;&#35813;&#26041;&#27861;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#34917;&#19969;&#23450;&#20301;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#26234;&#33021;&#32534;&#30721;&#22120;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#34917;&#19969;&#30340;&#23450;&#20301;&#31934;&#24230;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#25239;&#24615;&#34917;&#19969;&#30340;&#36890;&#29992;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#24615;&#29289;&#29702;&#34917;&#19969;&#24050;&#32463;&#25104;&#21151;&#22320;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#30772;&#22351;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#26799;&#24230;&#25110;&#29305;&#24449;&#20998;&#26512;&#30340;&#38450;&#24481;&#24050;&#32463;&#34987;&#26368;&#36817;&#30340;&#22522;&#20110;GAN&#30340;&#25915;&#20987;&#25152;&#30772;&#22351;&#65292;&#36825;&#20123;&#25915;&#20987;&#29983;&#25104;&#20102;&#33258;&#28982;&#20027;&#20041;&#30340;&#34917;&#19969;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jedi&#65292;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#23545;&#25239;&#24615;&#34917;&#19969;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#30495;&#23454;&#30340;&#34917;&#19969;&#25915;&#20987;&#20855;&#26377;&#24377;&#24615;&#12290;Jedi&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#34917;&#19969;&#23450;&#20301;&#38382;&#39064;&#65307;&#21033;&#29992;&#20102;&#20004;&#20010;&#26032;&#24819;&#27861;&#65306;(1) &#23427;&#36890;&#36807;&#29109;&#20998;&#26512;&#25913;&#36827;&#20102;&#28508;&#22312;&#34917;&#19969;&#21306;&#22495;&#30340;&#35782;&#21035;&#65306;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#34917;&#19969;&#30340;&#29109;&#26159;&#39640;&#30340;&#65292;&#21363;&#20351;&#22312;&#33258;&#28982;&#20027;&#20041;&#30340;&#34917;&#19969;&#20013;&#20063;&#26159;&#22914;&#27492;&#65307;(2) &#23427;&#21033;&#29992;&#33021;&#22815;&#20174;&#39640;&#29109;&#20869;&#26680;&#20013;&#23436;&#25104;&#34917;&#19969;&#21306;&#22495;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#26469;&#25913;&#21892;&#23545;&#25239;&#24615;&#34917;&#19969;&#30340;&#23450;&#20301;&#12290;Jedi&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#23450;&#20301;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#23545;&#20110;&#25104;&#21151;&#20462;&#22797;&#22270;&#20687;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;Jedi&#26159;&#38024;&#23545;&#34917;&#19969;&#29305;&#23450;&#30340;&#65292;&#24182;&#19981;&#38656;&#35201;&#20808;&#21069;&#20851;&#20110;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#22240;&#27492;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#25239;&#24615;&#34917;&#19969;&#30340;&#36890;&#29992;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world adversarial physical patches were shown to be successful in compromising state-of-the-art models in a variety of computer vision applications. Existing defenses that are based on either input gradient or features analysis have been compromised by recent GAN-based attacks that generate naturalistic patches. In this paper, we propose Jedi, a new defense against adversarial patches that is resilient to realistic patch attacks. Jedi tackles the patch localization problem from an information theory perspective; leverages two new ideas: (1) it improves the identification of potential patch regions using entropy analysis: we show that the entropy of adversarial patches is high, even in naturalistic patches; and (2) it improves the localization of adversarial patches, using an autoencoder that is able to complete patch regions from high entropy kernels. Jedi achieves high-precision adversarial patch localization, which we show is critical to successfully repair the images. Since Jed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#21462;&#20102;&#32852;&#37030;&#33406;&#28363;&#30149;&#26381;&#21153;&#25968;&#25454;&#21644;&#33406;&#28363;&#30149;&#30417;&#27979;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31995;&#21015;&#26469;&#35782;&#21035;&#26410;&#34987;&#30417;&#27979;&#21040;&#30340;&#26381;&#21153;&#25968;&#25454;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#30340;&#33406;&#28363;&#30149;&#20256;&#25773;&#39044;&#38450;&#25552;&#20379;&#25968;&#25454;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.10023</link><description>&lt;p&gt;
&#23545;&#20110;&#30417;&#27979;&#32780;&#35328;&#22826;&#30149;&#24577;&#65306;&#32852;&#37030;&#33406;&#28363;&#30149;&#26381;&#21153;&#25968;&#25454;&#33021;&#21542;&#25552;&#39640;&#32852;&#37030;&#33406;&#28363;&#30149;&#30417;&#27979;&#24037;&#20316;&#65311;
&lt;/p&gt;
&lt;p&gt;
Too sick for surveillance: Can federal HIV service data improve federal HIV surveillance efforts?. (arXiv:2304.10023v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#21462;&#20102;&#32852;&#37030;&#33406;&#28363;&#30149;&#26381;&#21153;&#25968;&#25454;&#21644;&#33406;&#28363;&#30149;&#30417;&#27979;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31995;&#21015;&#26469;&#35782;&#21035;&#26410;&#34987;&#30417;&#27979;&#21040;&#30340;&#26381;&#21153;&#25968;&#25454;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#30340;&#33406;&#28363;&#30149;&#20256;&#25773;&#39044;&#38450;&#25552;&#20379;&#25968;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#35328;&#65306;&#30446;&#21069;&#25972;&#21512;&#32852;&#37030;&#33406;&#28363;&#30149;&#26381;&#21153;&#25968;&#25454;&#21644;&#33406;&#28363;&#30149;&#30417;&#27979;&#30340;&#20215;&#20540;&#23578;&#26410;&#24471;&#30693;&#12290;&#19978;&#28216;&#21644;&#23436;&#25972;&#30340;&#26696;&#20363;&#25429;&#33719;&#23545;&#20110;&#26410;&#26469;&#30340;&#33406;&#28363;&#30149;&#20256;&#25773;&#39044;&#38450;&#33267;&#20851;&#37325;&#35201;&#12290;&#26041;&#27861;&#65306;&#26412;&#30740;&#31350;&#23558;2005&#24180;&#33267;2018&#24180;&#30340;Ryan White&#12289;&#31038;&#20250;&#20445;&#38556;&#27531;&#30142;&#20445;&#38505;&#12289;&#21307;&#30103;&#20445;&#38505;&#12289;&#20799;&#31461;&#20581;&#24247;&#20445;&#38505;&#35745;&#21010;&#21644;&#21307;&#30103;&#34917;&#21161;&#30340;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#19982;&#30142;&#30149;&#25511;&#21046;&#19982;&#39044;&#38450;&#20013;&#24515;&#30340;&#33406;&#28363;&#30149;&#30417;&#27979;&#25968;&#25454;&#25353;&#20154;&#21475;&#32479;&#35745;&#32858;&#21512;&#65292;&#21457;&#29616;&#26381;&#21153;&#32479;&#35745;&#37327;&#36229;&#36807;&#30417;&#27979;&#32479;&#35745;&#37327;&#30340;&#32858;&#21512;&#21363;&#20026;&#30417;&#27979;&#26410;&#30693;&#65292;&#32780;&#24050;&#30830;&#23450;&#26381;&#21153;&#30340;&#20505;&#36873;&#32858;&#21512;&#12290;&#20351;&#29992;&#20998;&#24067;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31995;&#21015;&#26469;&#30830;&#23450;&#22312;&#32858;&#21512;&#20013;&#30417;&#27979;&#30149;&#20363;&#36229;&#36807;&#26381;&#21153;&#30149;&#20363;&#30340;&#30417;&#27979;&#26410;&#30693;&#65292;&#21363;&#30417;&#27979;&#26410;&#30693;&#26381;&#21153;&#24050;&#30693;&#65288;SUSK&#65289;&#20505;&#36873;&#32858;&#21512;&#12290;&#32467;&#26524;&#65306;&#21307;&#30103;&#20445;&#38505;&#26377;&#26368;&#22810;&#30340;SUSK&#20505;&#36873;&#32858;&#21512;&#12290;&#21307;&#30103;&#34917;&#21161;&#21487;&#33021;&#26377;&#20505;&#36873;&#30340;SUSK&#32858;&#21512;&#65292;&#20854;&#20013;&#30340;&#30149;&#20363;&#25509;&#36817;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduction: The value of integrating federal HIV services data with HIV surveillance is currently unknown. Upstream and complete case capture is essential in preventing future HIV transmission. Methods: This study integrated Ryan White, Social Security Disability Insurance, Medicare, Children Health Insurance Programs and Medicaid demographic aggregates from 2005 to 2018 for people living with HIV and compared them with Centers for Disease Control and Prevention HIV surveillance by demographic aggregate. Surveillance Unknown, Service Known (SUSK) candidate aggregates were identified from aggregates where services aggregate volumes exceeded surveillance aggregate volumes. A distribution approach and a deep learning model series were used to identify SUSK candidate aggregates where surveillance cases exceeded services cases in aggregate. Results: Medicare had the most candidate SUSK aggregates. Medicaid may have candidate SUSK aggregates where cases approach parity with surveillance. D
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#25968;&#23383;&#23402;&#29983;&#22270;(DTG)&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#21487;&#20197;&#23436;&#20840;&#33258;&#21160;&#21270;&#21644;&#39046;&#22495;&#26080;&#20851;&#22320;&#26500;&#24314;&#25968;&#23383;&#23402;&#29983;&#65292;&#37319;&#29992;&#20102;&#25968;&#25454;&#39537;&#21160;&#21644;&#22270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#25968;&#23383;&#23402;&#29983;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.10018</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#22270;&#65306;&#29289;&#32852;&#32593;&#19990;&#30028;&#33258;&#21160;&#39046;&#22495;&#26080;&#20851;&#24314;&#35774;&#12289;&#34701;&#21512;&#21644;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Digital Twin Graph: Automated Domain-Agnostic Construction, Fusion, and Simulation of IoT-Enabled World. (arXiv:2304.10018v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10018
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#25968;&#23383;&#23402;&#29983;&#22270;(DTG)&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#21487;&#20197;&#23436;&#20840;&#33258;&#21160;&#21270;&#21644;&#39046;&#22495;&#26080;&#20851;&#22320;&#26500;&#24314;&#25968;&#23383;&#23402;&#29983;&#65292;&#37319;&#29992;&#20102;&#25968;&#25454;&#39537;&#21160;&#21644;&#22270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#25968;&#23383;&#23402;&#29983;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#28023;&#37327;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#36890;&#20449;&#65292;&#21019;&#36896;&#20102;&#26500;&#24314;&#25968;&#23383;&#23402;&#29983;&#20197;&#21453;&#26144;&#21644;&#27169;&#25311;&#22797;&#26434;&#29289;&#29702;&#19990;&#30028;&#30340;&#26426;&#20250;&#12290;&#38271;&#26399;&#20197;&#26469;&#65292;&#25968;&#23383;&#23402;&#29983;&#34987;&#35748;&#20026;&#20005;&#37325;&#20381;&#36182;&#20110;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36825;&#23548;&#33268;&#20102;&#39640;&#38376;&#27099;&#21644;&#32531;&#24930;&#30340;&#24320;&#21457;&#65292;&#22240;&#20026;&#20154;&#24037;&#19987;&#23478;&#31232;&#32570;&#21644;&#25104;&#26412;&#26114;&#36149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23383;&#23402;&#29983;&#22270;(DTG)&#65292;&#36825;&#26159;&#19968;&#20010;&#19982;&#22788;&#29702;&#26694;&#26550;&#30456;&#20851;&#32852;&#30340;&#36890;&#29992;&#25968;&#25454;&#32467;&#26500;&#65292;&#21487;&#20197;&#20197;&#23436;&#20840;&#33258;&#21160;&#21270;&#21644;&#39046;&#22495;&#26080;&#20851;&#30340;&#26041;&#24335;&#26500;&#24314;&#25968;&#23383;&#23402;&#29983;&#12290;&#36825;&#39033;&#24037;&#20316;&#20195;&#34920;&#20102;&#39318;&#27425;&#23581;&#35797;&#37319;&#29992;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#21644;&#65288;&#38750;&#20256;&#32479;&#30340;&#65289;&#22270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#25968;&#23383;&#23402;&#29983;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advances of IoT developments, copious sensor data are communicated through wireless networks and create the opportunity of building Digital Twins to mirror and simulate the complex physical world. Digital Twin has long been believed to rely heavily on domain knowledge, but we argue that this leads to a high barrier of entry and slow development due to the scarcity and cost of human experts. In this paper, we propose Digital Twin Graph (DTG), a general data structure associated with a processing framework that constructs digital twins in a fully automated and domain-agnostic manner. This work represents the first effort that takes a completely data-driven and (unconventional) graph learning approach to addresses key digital twin challenges.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21457;&#29616;&#20102;&#36895;&#36890;&#19990;&#30028;&#32426;&#24405;&#25913;&#36827;&#30340;&#24130;&#24459;&#27169;&#24335;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#21457;&#29616;&#25552;&#39640;&#20102;&#39044;&#27979;&#36895;&#36890;&#19990;&#30028;&#32426;&#24405;&#31934;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#19978;&#24471;&#21040;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ML&#22522;&#20934;&#36828;&#26410;&#39281;&#21644;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25913;&#36827;&#20855;&#26377;&#31361;&#28982;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10004</link><description>&lt;p&gt;
&#36895;&#36890;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24130;&#24459;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Power Law Trends in Speedrunning and Machine Learning. (arXiv:2304.10004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10004
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21457;&#29616;&#20102;&#36895;&#36890;&#19990;&#30028;&#32426;&#24405;&#25913;&#36827;&#30340;&#24130;&#24459;&#27169;&#24335;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#21457;&#29616;&#25552;&#39640;&#20102;&#39044;&#27979;&#36895;&#36890;&#19990;&#30028;&#32426;&#24405;&#31934;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#19978;&#24471;&#21040;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ML&#22522;&#20934;&#36828;&#26410;&#39281;&#21644;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25913;&#36827;&#20855;&#26377;&#31361;&#28982;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36895;&#36890;&#19990;&#30028;&#32426;&#24405;&#30340;&#25913;&#36827;&#20013;&#23384;&#22312;&#24130;&#24459;&#27169;&#24335;&#12290;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#26410;&#35299;&#20915;&#38382;&#39064;&#65306;&#22914;&#20309;&#22312;&#39044;&#27979;&#26576;&#20010;&#26102;&#38388;&#36328;&#24230;&#65288;&#22914;&#19968;&#20010;&#26376;&#65289;&#20869;&#30340;&#36895;&#36890;&#19990;&#30028;&#32426;&#24405;&#26102;&#65292;&#25552;&#39640;&#22522;&#32447;&#39044;&#27979;&#19981;&#25913;&#36827;&#30340;&#31934;&#24230;&#65311;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25928;&#24212;&#27169;&#22411;&#65292;&#22312;&#39044;&#27979;&#26679;&#26412;&#22806;&#30340;&#19990;&#30028;&#32426;&#24405;&#25913;&#36827;&#30340;&#30456;&#23545;&#22343;&#26041;&#35823;&#24046;&#19978;&#65292;&#25105;&#20204;&#22312;$p&lt;10^{-5}$&#30340;&#26174;&#33879;&#24615;&#27700;&#24179;&#19978;&#25552;&#39640;&#20102;&#22522;&#32447;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;&#23613;&#31649;&#20351;&#29992;&#30340;&#25968;&#25454;&#28857;&#36828;&#23569;&#20110;&#20808;&#21069;&#34920;&#29616;&#26368;&#20339;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#39044;&#27979;&#27169;&#22411;&#65292;&#20294;&#30456;&#21516;&#30340;&#35774;&#32622;&#22312;$p=0.15$&#30340;&#26174;&#33879;&#24615;&#27700;&#24179;&#19978;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#24182;&#21462;&#24471;&#20102;&#36229;&#36807;&#22522;&#32447;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35299;&#37322;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35748;&#20026;1&#65289;ML&#22522;&#20934;&#36828;&#26410;&#39281;&#21644;&#65292;2&#65289;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31361;&#28982;&#22823;&#24133;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
We find that improvements in speedrunning world records follow a power law pattern. Using this observation, we answer an outstanding question from previous work: How do we improve on the baseline of predicting no improvement when forecasting speedrunning world records out to some time horizon, such as one month? Using a random effects model, we improve on this baseline for relative mean square error made on predicting out-of-sample world record improvements as the comparison metric at a $p &lt; 10^{-5}$ significance level. The same set-up improves \textit{even} on the ex-post best exponential moving average forecasts at a $p = 0.15$ significance level while having access to substantially fewer data points. We demonstrate the effectiveness of this approach by applying it to Machine Learning benchmarks and achieving forecasts that exceed a baseline. Finally, we interpret the resulting model to suggest that 1) ML benchmarks are far from saturation and 2) sudden large improvements in Machine 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#27169;&#22411;&#21644;&#27169;&#22411;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#20010;&#24615;&#21270;&#32925;&#32032;&#21058;&#37327;&#25511;&#21046;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#27835;&#30103;&#21644;&#26080;&#27169;&#22411;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#23433;&#20840;&#24615;&#21644;&#21151;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.10000</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#20010;&#24615;&#21270;&#32925;&#32032;&#21058;&#37327;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Model Based Reinforcement Learning for Personalized Heparin Dosing. (arXiv:2304.10000v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#27169;&#22411;&#21644;&#27169;&#22411;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#20010;&#24615;&#21270;&#32925;&#32032;&#21058;&#37327;&#25511;&#21046;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#27835;&#30103;&#21644;&#26080;&#27169;&#22411;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#23433;&#20840;&#24615;&#21644;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22312;&#23616;&#37096;&#20449;&#24687;&#19979;&#23433;&#20840;&#22320;&#20248;&#21270;&#31995;&#32479;&#12290;&#35745;&#31639;&#24739;&#32773;&#30340;&#32925;&#32032;&#21058;&#37327;&#65292;&#30001;&#20110;&#26080;&#27861;&#30452;&#25509;&#27979;&#37327;&#24739;&#32773;&#34880;&#28082;&#20013;&#32925;&#32032;&#30340;&#27987;&#24230;&#65292;&#19988;&#19981;&#21516;&#24739;&#32773;&#20195;&#35874;&#32925;&#32032;&#30340;&#36895;&#29575;&#21508;&#19981;&#30456;&#21516;&#65292;&#22240;&#27492;&#20855;&#26377;&#37096;&#20998;&#21487;&#30693;&#30340;&#29366;&#24577;&#21644;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#33509;&#37096;&#20998;&#32467;&#26500;&#30340;&#21160;&#24577;&#26159;&#24050;&#30693;&#30340;&#65292;&#21017;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#22522;&#30784;&#30340;&#26041;&#27861;&#25552;&#20379;&#23433;&#20840;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#20248;&#21270;&#20010;&#24615;&#21270;&#32925;&#32032;&#21058;&#37327;&#25511;&#21046;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#39044;&#27979;&#27169;&#22411;&#39044;&#27979;&#26410;&#26469;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#23398;&#20064;&#20010;&#24615;&#21270;&#32925;&#32032;&#21058;&#37327;&#25511;&#21046;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#32925;&#32032;&#25511;&#21046;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#27492;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#19982;&#26631;&#20934;&#27835;&#30103;&#21644;&#26080;&#27169;&#22411;&#26041;&#27861;&#30456;&#27604;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#23433;&#20840;&#24615;&#21644;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in sequential decision making is optimizing systems safely under partial information. While much of the literature has focused on the cases of either partially known states or partially known dynamics, it is further exacerbated in cases where both states and dynamics are partially known. Computing heparin doses for patients fits this paradigm since the concentration of heparin in the patient cannot be measured directly and the rates at which patients metabolize heparin vary greatly between individuals. While many proposed solutions are model free, they require complex models and have difficulty ensuring safety. However, if some of the structure of the dynamics is known, a model based approach can be leveraged to provide safe policies. In this paper we propose such a framework to address the challenge of optimizing personalized heparin doses. We use a predictive model parameterized individually by patient to predict future therapeutic effects. We then leverage this model
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36190;&#25104;&#31080;&#30340;&#22810;&#33719;&#32988;&#32773;&#25237;&#31080;&#30340;&#23454;&#20363;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#34920;&#24615;&#25237;&#31080;&#35268;&#21017;&#36873;&#25321;&#33719;&#32988;&#32773;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20943;&#23569;&#35757;&#32451;&#38598;&#30340;&#25968;&#25454;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2304.09995</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#31080;&#30340;&#23454;&#20363;&#31579;&#36873;&#26041;&#27861;&#65306;&#20351;&#29992;&#22522;&#20110;&#36190;&#25104;&#31080;&#30340;&#22810;&#33719;&#32988;&#32773;&#25237;&#31080;
&lt;/p&gt;
&lt;p&gt;
Data as voters: instance selection using approval-based multi-winner voting. (arXiv:2304.09995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09995
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36190;&#25104;&#31080;&#30340;&#22810;&#33719;&#32988;&#32773;&#25237;&#31080;&#30340;&#23454;&#20363;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#34920;&#24615;&#25237;&#31080;&#35268;&#21017;&#36873;&#25321;&#33719;&#32988;&#32773;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20943;&#23569;&#35757;&#32451;&#38598;&#30340;&#25968;&#25454;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;&#25110;&#25968;&#25454;&#25366;&#25496;&#65289;&#20013;&#30340;&#23454;&#20363;&#31579;&#36873;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#20851;&#20110;&#22522;&#20110;&#36190;&#25104;&#31080;&#30340;&#22810;&#33719;&#32988;&#32773;&#36873;&#20030;&#20013;&#20195;&#34920;&#24615;&#34920;&#24449;&#30340;&#32467;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#23454;&#20363;&#25198;&#28436;&#36873;&#27665;&#21644;&#20505;&#36873;&#20154;&#30340;&#21452;&#37325;&#35282;&#33394;&#12290;&#27599;&#20010;&#35757;&#32451;&#38598;&#20013;&#30340;&#23454;&#20363;&#65288;&#20316;&#20026;&#36873;&#27665;&#65289;&#36190;&#25104;&#20854;&#26412;&#22320;&#38598;&#21512;&#20013;&#30340;&#23454;&#20363;&#65288;&#25198;&#28436;&#20505;&#36873;&#20154;&#30340;&#35282;&#33394;&#65289;&#65288;&#38500;&#33258;&#36523;&#20197;&#22806;&#30340;&#23454;&#20363;&#65289;&#65292;&#36825;&#20010;&#27010;&#24565;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#23384;&#22312;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20195;&#34920;&#24615;&#25237;&#31080;&#35268;&#21017;&#36873;&#25321;&#36873;&#20030;&#33719;&#32988;&#32773;&#65292;&#24182;&#20316;&#20026;&#20943;&#23569;&#35757;&#32451;&#38598;&#20013;&#30340;&#25968;&#25454;&#23454;&#20363;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to the instance selection problem in machine learning (or data mining). Our approach is based on recent results on (proportional) representation in approval-based multi-winner elections. In our model, instances play a double role as voters and candidates. Each instance in the training set (acting as a voter) approves of the instances (playing the role of candidates) belonging to its local set (except itself), a concept already existing in the literature. We then select the election winners using a representative voting rule, and such winners are the data instances kept in the reduced training set.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-RNN&#28151;&#21512;&#29305;&#24449;&#34701;&#21512;&#24314;&#27169;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#38745;&#24577;&#21644;&#21160;&#24577;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#22810;&#20010;CNN&#21644;RNN&#27169;&#22411;&#65292;&#22312;&#31934;&#24230;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.09994</link><description>&lt;p&gt;
&#22522;&#20110;LSTM-DeepLabv3+&#21644;&#26102;&#31354;&#29305;&#24449;&#34701;&#21512;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#27169;&#22411;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian Optimization with Spatiotemporal feature fusion. (arXiv:2304.09994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-RNN&#28151;&#21512;&#29305;&#24449;&#34701;&#21512;&#24314;&#27169;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#38745;&#24577;&#21644;&#21160;&#24577;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#22810;&#20010;CNN&#21644;RNN&#27169;&#22411;&#65292;&#22312;&#31934;&#24230;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22240;&#20854;&#30456;&#23545;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#36880;&#28176;&#25104;&#20026;&#27969;&#34892;&#30340;&#27946;&#27700;&#39044;&#27979;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21333;&#29420;&#30340;&#31354;&#38388;&#25110;&#26102;&#38388;&#29305;&#24449;&#20998;&#26512;&#65292;&#24182;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#32500;&#24230;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CNN-RNN&#30340;&#28151;&#21512;&#29305;&#24449;&#34701;&#21512;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22478;&#24066;&#27946;&#27700;&#39044;&#27979;&#65292;&#23558;CNN&#22312;&#22788;&#29702;&#31354;&#38388;&#29305;&#24449;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;RNN&#22312;&#20998;&#26512;&#19981;&#21516;&#32500;&#24230;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#30340;&#20248;&#21183;&#25972;&#21512;&#36215;&#26469;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#36827;&#34892;&#38745;&#24577;&#21644;&#21160;&#24577;&#30340;&#27946;&#27700;&#39044;&#27979;&#12290;&#24212;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#30830;&#23450;&#19971;&#20010;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#27946;&#27700;&#39537;&#21160;&#22240;&#32032;&#65292;&#24182;&#30830;&#23450;&#26368;&#20339;&#32452;&#21512;&#31574;&#30053;&#12290;&#36890;&#36807;&#32467;&#21512;&#22235;&#20010;CNN&#65288;FCN&#65292;UNet&#65292;SegNet&#65292;DeepLabv3+&#65289;&#21644;&#19977;&#20010;RNN&#65288;LSTM&#65292;BiLSTM&#65292;GRU&#65289;&#65292;&#26368;&#20248;&#28151;&#21512;&#27169;&#22411;&#34987;&#30830;&#23450;&#20026;LSTM-DeepLabv3+&#12290;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65288;MAE&#12289;RMSE&#12289;NSE&#21644;KGE&#29575;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have become increasingly popular for flood prediction due to their superior accuracy and efficiency compared to traditional methods. However, current machine learning methods often rely on separate spatial or temporal feature analysis and have limitations on the types, number, and dimensions of input data. This study presented a CNN-RNN hybrid feature fusion modelling approach for urban flood prediction, which integrated the strengths of CNNs in processing spatial features and RNNs in analyzing different dimensions of time sequences. This approach allowed for both static and dynamic flood predictions. Bayesian optimization was applied to identify the seven most influential flood-driven factors and determine the best combination strategy. By combining four CNNs (FCN, UNet, SegNet, DeepLabv3+) and three RNNs (LSTM, BiLSTM, GRU), the optimal hybrid model was identified as LSTM-DeepLabv3+. This model achieved the highest prediction accuracy (MAE, RMSE, NSE, and KGE wer
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#30830;&#23450;&#24615;&#21021;&#22987;&#21270;&#36807;&#31243;&#65288;CKmeans&#21644;FCKmeans&#65289;&#20197;&#25913;&#36827;Kmeans&#32858;&#31867;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#36807;&#31243;&#22312;&#32858;&#31867;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;Kmeans&#21644;Kmeans++&#12290;</title><link>http://arxiv.org/abs/2304.09989</link><description>&lt;p&gt;
CKmeans&#21644;FCKmeans&#65306;&#20351;&#29992;&#25317;&#25380;&#36317;&#31163;&#30340;Kmeans&#31639;&#27861;&#30340;&#20004;&#31181;&#30830;&#23450;&#24615;&#21021;&#22987;&#21270;&#36807;&#31243; &#65288;arXiv&#65306;2304.09989v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
CKmeans and FCKmeans : Two Deterministic Initialization Procedures For Kmeans Algorithm Using Crowding Distance. (arXiv:2304.09989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#30830;&#23450;&#24615;&#21021;&#22987;&#21270;&#36807;&#31243;&#65288;CKmeans&#21644;FCKmeans&#65289;&#20197;&#25913;&#36827;Kmeans&#32858;&#31867;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#36807;&#31243;&#22312;&#32858;&#31867;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;Kmeans&#21644;Kmeans++&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20462;&#25913;&#21518;&#30340;&#25317;&#25380;&#36317;&#31163;&#30340;K-means&#32858;&#31867;&#30340;&#26032;&#22411;&#30830;&#23450;&#24615;&#21021;&#22987;&#21270;&#36807;&#31243;&#65292;&#20998;&#21035;&#31216;&#20026;CKmeans&#21644;FCKmeans&#12290;&#36825;&#20123;&#36807;&#31243;&#21033;&#29992;&#26356;&#23494;&#38598;&#30340;&#28857;&#20316;&#20026;&#21021;&#22987;&#36136;&#24515;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32858;&#31867;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;Kmeans&#21644;Kmeans ++&#12290;CKmeans&#21644;FCKmeans&#30340;&#26377;&#25928;&#24615;&#24402;&#22240;&#20110;&#23427;&#20204;&#22522;&#20110;&#20462;&#25913;&#21518;&#30340;&#25317;&#25380;&#36317;&#31163;&#36873;&#25321;&#26356;&#22909;&#30340;&#21021;&#22987;&#36136;&#24515;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20026;&#25913;&#36827;K-means&#32858;&#31867;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two novel deterministic initialization procedures for K-means clustering based on a modified crowding distance. The procedures, named CKmeans and FCKmeans, use more crowded points as initial centroids. Experimental studies on multiple datasets demonstrate that the proposed approach outperforms Kmeans and Kmeans++ in terms of clustering accuracy. The effectiveness of CKmeans and FCKmeans is attributed to their ability to select better initial centroids based on the modified crowding distance. Overall, the proposed approach provides a promising alternative for improving K-means clustering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22235;&#38754;&#20307;&#21644; Delaunay &#34920;&#31034;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#21487;&#23454;&#29616;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#22810;&#25509;&#36817;&#34920;&#38754;&#30340;&#32454;&#33410;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#27604;&#22522;&#20110;&#28857;&#30340;&#34920;&#31034;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09987</link><description>&lt;p&gt;
Tetra-NeRF&#65306;&#20351;&#29992;&#22235;&#38754;&#20307;&#34920;&#31034;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra. (arXiv:2304.09987v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22235;&#38754;&#20307;&#21644; Delaunay &#34920;&#31034;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#21487;&#23454;&#29616;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#22810;&#25509;&#36817;&#34920;&#38754;&#30340;&#32454;&#33410;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#27604;&#22522;&#20110;&#28857;&#30340;&#34920;&#31034;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330; (NeRF) &#26159;&#19968;&#31181;&#38750;&#24120;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#21644;&#19977;&#32500;&#37325;&#26500;&#38382;&#39064;&#12290;NeRF &#24120;&#29992;&#30340;&#22330;&#26223;&#34920;&#31034;&#26159;&#23558;&#22330;&#26223;&#30340;&#19968;&#33268;&#30340;&#22522;&#20110;&#20307;&#32032;&#30340;&#32454;&#20998;&#19982; MLP &#32467;&#21512;&#36215;&#26469;&#12290;&#26412;&#25991;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#30340;&#65288;&#31232;&#30095;&#65289;&#28857;&#20113;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Delaunay &#34920;&#31034;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#65292;&#32780;&#38750;&#19968;&#33268;&#30340;&#32454;&#20998;&#25110;&#22522;&#20110;&#28857;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#34920;&#31034;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24039;&#22937;&#22320;&#32467;&#21512;&#20102;&#19977;&#32500;&#20960;&#20309;&#22788;&#29702;&#12289;&#19977;&#35282;&#24418;&#28210;&#26579;&#21644;&#29616;&#20195;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#27010;&#24565;&#12290;&#19982;&#22522;&#20110;&#20307;&#32032;&#30340;&#34920;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22810;&#25509;&#36817;&#34920;&#38754;&#30340;&#22330;&#26223;&#32454;&#33410;&#12290;&#19982;&#22522;&#20110;&#28857;&#30340;&#34920;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRFs) are a very recent and very popular approach for the problems of novel view synthesis and 3D reconstruction. A popular scene representation used by NeRFs is to combine a uniform, voxel-based subdivision of the scene with an MLP. Based on the observation that a (sparse) point cloud of the scene is often available, this paper proposes to use an adaptive representation based on tetrahedra and a Delaunay representation instead of the uniform subdivision or point-based representations. We show that such a representation enables efficient training and leads to state-of-the-art results. Our approach elegantly combines concepts from 3D geometry processing, triangle-based rendering, and modern neural radiance fields. Compared to voxel-based representations, ours provides more detail around parts of the scene likely to be close to the surface. Compared to point-based representations, our approach achieves better performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#29983;&#23384;&#20998;&#26512;&#26469;&#35780;&#20215;&#20986;&#38498;&#21518;&#35780;&#20272;&#21644;&#31649;&#29702;&#65288;E/M&#65289;&#26381;&#21153;&#23545;&#38450;&#27490;&#20877;&#20303;&#38498;&#25110;&#27515;&#20129;&#30340;&#24433;&#21709;&#65292;&#36991;&#20813;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24184;&#23384;&#32773;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20010;&#26696;&#31649;&#29702;&#26381;&#21153;&#22312;&#20943;&#23569;&#20877;&#20303;&#38498;&#26041;&#38754;&#26368;&#20026;&#26377;&#25928;&#65292;&#23588;&#20854;&#23545;&#20110;&#20986;&#38498;&#21518;&#21040;&#38271;&#26399;&#25252;&#29702;&#26426;&#26500;&#30340;&#24739;&#32773;&#20197;&#21450;&#20837;&#38498;&#21069;&#26377;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#30340;&#24739;&#32773;&#12290;</title><link>http://arxiv.org/abs/2304.09981</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24322;&#36136;&#24615;&#24184;&#23384;&#32773;&#20559;&#24046;&#26657;&#27491;&#27835;&#30103;&#25928;&#26524;&#30740;&#31350;&#65292;&#26088;&#22312;&#25351;&#27966;&#20986;&#38498;&#21518;&#39044;&#38450;&#20877;&#20303;&#38498;&#30340;&#24178;&#39044;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
Interpretable (not just posthoc-explainable) heterogeneous survivor bias-corrected treatment effects for assignment of postdischarge interventions to prevent readmissions. (arXiv:2304.09981v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09981
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#29983;&#23384;&#20998;&#26512;&#26469;&#35780;&#20215;&#20986;&#38498;&#21518;&#35780;&#20272;&#21644;&#31649;&#29702;&#65288;E/M&#65289;&#26381;&#21153;&#23545;&#38450;&#27490;&#20877;&#20303;&#38498;&#25110;&#27515;&#20129;&#30340;&#24433;&#21709;&#65292;&#36991;&#20813;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24184;&#23384;&#32773;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20010;&#26696;&#31649;&#29702;&#26381;&#21153;&#22312;&#20943;&#23569;&#20877;&#20303;&#38498;&#26041;&#38754;&#26368;&#20026;&#26377;&#25928;&#65292;&#23588;&#20854;&#23545;&#20110;&#20986;&#38498;&#21518;&#21040;&#38271;&#26399;&#25252;&#29702;&#26426;&#26500;&#30340;&#24739;&#32773;&#20197;&#21450;&#20837;&#38498;&#21069;&#26377;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#30340;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37319;&#29992;&#29983;&#23384;&#20998;&#26512;&#26469;&#37327;&#21270;&#20986;&#38498;&#21518;&#35780;&#20272;&#21644;&#31649;&#29702;&#26381;&#21153;&#22312;&#38450;&#27490;&#20303;&#38498;&#25110;&#27515;&#20129;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21040;&#36825;&#20010;&#38382;&#39064;&#19978;&#30340;&#19968;&#20010;&#29305;&#23450;&#38519;&#38449;&#65292;&#37027;&#23601;&#26159;&#22240;&#20026;&#24184;&#23384;&#32773;&#20559;&#24046;&#32780;&#23548;&#33268;&#30340;&#25928;&#26524;&#36807;&#39640; -- &#36825;&#31181;&#36807;&#39640;&#30340;&#20272;&#35745;&#21487;&#33021;&#19982;&#20154;&#32676;&#20013;&#30340;&#24322;&#36136;&#24615;&#28151;&#28102;&#22240;&#32032;&#26377;&#20851;&#12290;&#36825;&#31181;&#20559;&#24046;&#20043;&#25152;&#20197;&#20135;&#29983;&#65292;&#26159;&#22240;&#20026;&#20026;&#20102;&#22312;&#20986;&#38498;&#21518;&#25509;&#21463;&#24178;&#39044;&#65292;&#19968;&#20010;&#20154;&#24517;&#39035;&#22312;&#20171;&#20837;&#26399;&#20869;&#27809;&#26377;&#20877;&#27425;&#20303;&#38498;&#12290;&#22312;&#24471;&#20986;&#36825;&#31181;&#24187;&#24433;&#25928;&#24212;&#30340;&#34920;&#36798;&#24335;&#21518;&#65292;&#25105;&#20204;&#22312;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#29983;&#23384;&#26694;&#26550;&#20013;&#25511;&#21046;&#20102;&#36825;&#20010;&#21644;&#20854;&#20182;&#20559;&#24046;&#12290;&#25105;&#20204;&#30830;&#23450;&#20010;&#26696;&#31649;&#29702;&#26381;&#21153;&#23545;&#20110;&#25972;&#20307;&#20943;&#23569;&#20877;&#20303;&#38498;&#20855;&#26377;&#26368;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20986;&#38498;&#21518;&#21040;&#38271;&#26399;&#25252;&#29702;&#26426;&#26500;&#30340;&#24739;&#32773;&#65292;&#22312;&#20837;&#38498;&#21069;&#19968;&#20010;&#23395;&#24230;&#26377;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#30340;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We used survival analysis to quantify the impact of postdischarge evaluation and management (E/M) services in preventing hospital readmission or death. Our approach avoids a specific pitfall of applying machine learning to this problem, which is an inflated estimate of the effect of interventions, due to survivors bias -- where the magnitude of inflation may be conditional on heterogeneous confounders in the population. This bias arises simply because in order to receive an intervention after discharge, a person must not have been readmitted in the intervening period. After deriving an expression for this phantom effect, we controlled for this and other biases within an inherently interpretable Bayesian survival framework. We identified case management services as being the most impactful for reducing readmissions overall, particularly for patients discharged to long term care facilities, with high resource utilization in the quarter preceding admission.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32473;Transformer&#27169;&#22411;&#22686;&#21152;&#20004;&#20010;&#31616;&#21333;&#30340;&#24402;&#32435;&#23398;&#20064;&#20559;&#35265;&#25506;&#31350;&#20102;&#23398;&#20064;&#21644;&#39044;&#27979;&#31616;&#21333;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#21457;&#29616;&#36825;&#20123;&#20559;&#35265;&#30340;&#24110;&#21161;&#65292;&#21516;&#26102;&#25351;&#20986;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#26377;&#21161;&#20110;&#20154;&#31867;&#22312;&#22806;&#25512;&#33021;&#21147;&#26041;&#38754;&#30340;&#24402;&#32435;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.09979</link><description>&lt;p&gt;
&#36229;&#36234;Transformer&#30340;&#20989;&#25968;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Transformers for Function Learning. (arXiv:2304.09979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32473;Transformer&#27169;&#22411;&#22686;&#21152;&#20004;&#20010;&#31616;&#21333;&#30340;&#24402;&#32435;&#23398;&#20064;&#20559;&#35265;&#25506;&#31350;&#20102;&#23398;&#20064;&#21644;&#39044;&#27979;&#31616;&#21333;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#21457;&#29616;&#36825;&#20123;&#20559;&#35265;&#30340;&#24110;&#21161;&#65292;&#21516;&#26102;&#25351;&#20986;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#26377;&#21161;&#20110;&#20154;&#31867;&#22312;&#22806;&#25512;&#33021;&#21147;&#26041;&#38754;&#30340;&#24402;&#32435;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#39044;&#27979;&#31616;&#21333;&#20989;&#25968;&#30340;&#33021;&#21147;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#24320;&#22987;&#20351;&#29992;Transformer&#26550;&#26500;&#26469;&#25506;&#32034;&#36825;&#31181;&#33021;&#21147;&#65292;&#28982;&#32780;&#20173;&#19981;&#28165;&#26970;&#36825;&#26159;&#21542;&#36275;&#20197;&#37325;&#29616;&#20154;&#20204;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22686;&#21152;&#20004;&#20010;&#31616;&#21333;&#30340;&#24402;&#32435;&#23398;&#20064;&#20559;&#35265;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#36825;&#20123;&#20559;&#35265;&#30452;&#25509;&#26469;&#33258;&#20110;&#35748;&#30693;&#31185;&#23398;&#20013;&#26368;&#36817;&#30340;&#25277;&#35937;&#25512;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#25253;&#36947;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#20123;&#20559;&#35265;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#21487;&#33021;&#26377;&#21161;&#20110;&#20154;&#31867;&#22806;&#25512;&#33021;&#21147;&#30340;&#24402;&#32435;&#23398;&#20064;&#20559;&#35265;&#30340;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn and predict simple functions is a key aspect of human intelligence. Recent works have started to explore this ability using transformer architectures, however it remains unclear whether this is sufficient to recapitulate the extrapolation abilities of people in this domain. Here, we propose to address this gap by augmenting the transformer architecture with two simple inductive learning biases, that are directly adapted from recent models of abstract reasoning in cognitive science. The results we report demonstrate that these biases are helpful in the context of large neural network models, as well as shed light on the types of inductive learning biases that may contribute to human abilities in extrapolation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32958;&#31227;&#26893;&#38382;&#39064;&#65292;&#21487;&#20197;&#36817;&#20284;&#35299;&#20915;NP&#38590;&#38382;&#39064;&#65292;&#24182;&#19988;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.09975</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32958;&#31227;&#26893;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving the Kidney-Exchange Problem via Graph Neural Networks with No Supervision. (arXiv:2304.09975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32958;&#31227;&#26893;&#38382;&#39064;&#65292;&#21487;&#20197;&#36817;&#20284;&#35299;&#20915;NP&#38590;&#38382;&#39064;&#65292;&#24182;&#19988;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36817;&#20284;&#35299;&#20915;&#20102;&#22270;&#19978;&#30340;&#32958;&#31227;&#26893;&#38382;&#39064;&#65288;KEP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;NP&#38590;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#28041;&#21450;&#21040;&#20174;&#19968;&#32452;&#32958;&#33039;&#20379;&#20307;&#21644;&#31561;&#24453;&#32958;&#33039;&#25424;&#36192;&#30340;&#24739;&#32773;&#20013;&#65292;&#36873;&#25321;&#19968;&#32452;&#25424;&#36192;&#20197;&#26368;&#22823;&#21270;&#31227;&#26893;&#25968;&#37327;&#21644;&#36136;&#37327;&#65292;&#21516;&#26102;&#36981;&#23432;&#19968;&#20123;&#26377;&#20851;&#36825;&#20123;&#25424;&#36192;&#23433;&#25490;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#35813;&#25216;&#26415;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#31532;&#19968;&#27493;&#26159;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65307;&#31532;&#20108;&#27493;&#26159;&#30830;&#23450;&#24615;&#30340;&#38750;&#23398;&#20064;&#25628;&#32034;&#21551;&#21457;&#24335;&#65292;&#23427;&#21033;&#29992;GNN&#30340;&#36755;&#20986;&#26469;&#26597;&#25214;&#36335;&#24452;&#21644;&#24490;&#29615;&#12290;&#20026;&#20102;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#36824;&#23454;&#29616;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#26041;&#27861;&#65292;&#20351;&#29992;&#25972;&#25968;&#35268;&#21010;&#12289;&#20004;&#20010;&#36138;&#24515;&#25628;&#32034;&#21551;&#21457;&#24335;&#65292;&#19981;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22359;&#30340;GNN&#12290;&#25105;&#20204;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#21363;&#22522;&#20110;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new learning-based approach for approximately solving the Kidney-Exchange Problem (KEP), an NP-hard problem on graphs. The problem consists of, given a pool of kidney donors and patients waiting for kidney donations, optimally selecting a set of donations to optimize the quantity and quality of transplants performed while respecting a set of constraints about the arrangement of these donations. The proposed technique consists of two main steps: the first is a Graph Neural Network (GNN) trained without supervision; the second is a deterministic non-learned search heuristic that uses the output of the GNN to find paths and cycles. To allow for comparisons, we also implemented and tested an exact solution method using integer programming, two greedy search heuristics without the machine learning module, and the GNN alone without a heuristic. We analyze and compare the methods and conclude that the learning-based two-stage approach is the best solution quality, outp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21152;&#36895;&#20026;&#22810;&#20010;&#23458;&#25143;&#31471;&#36816;&#34892;&#36793;&#32536;&#26381;&#21153;&#22120;DNN&#12290;&#25209;&#22788;&#29702;&#22810;&#20010;DNN&#35831;&#27714;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#22788;&#29702;&#26102;&#38388;&#12290;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21327;&#20316;&#26041;&#27861;&#26469;&#35843;&#24230;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#30340;DNN&#35831;&#27714;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22788;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.09961</link><description>&lt;p&gt;
&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Scheduling DNNs on Edge Servers. (arXiv:2304.09961v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21152;&#36895;&#20026;&#22810;&#20010;&#23458;&#25143;&#31471;&#36816;&#34892;&#36793;&#32536;&#26381;&#21153;&#22120;DNN&#12290;&#25209;&#22788;&#29702;&#22810;&#20010;DNN&#35831;&#27714;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#22788;&#29702;&#26102;&#38388;&#12290;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21327;&#20316;&#26041;&#27861;&#26469;&#35843;&#24230;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#30340;DNN&#35831;&#27714;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22788;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#24050;&#32463;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#35270;&#39057;&#20998;&#26512;&#20219;&#21153;&#20013;&#12290;&#36825;&#20123;&#20219;&#21153;&#35201;&#27714;&#23454;&#26102;&#21709;&#24212;&#65292;&#30001;&#20110;&#31227;&#21160;&#35774;&#22791;&#22788;&#29702;&#33021;&#21147;&#26377;&#38480;&#65292;&#25903;&#25345;&#27492;&#31867;&#23454;&#26102;&#20998;&#26512;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#23558;&#22788;&#29702;&#31163;&#32447;&#21040;&#36793;&#32536;&#26381;&#21153;&#22120;&#12290;&#26412;&#25991;&#32771;&#23519;&#22914;&#20309;&#21152;&#36895;&#20026;&#22810;&#20010;&#23458;&#25143;&#31471;&#36816;&#34892;&#36793;&#32536;&#26381;&#21153;&#22120;DNN&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25209;&#22788;&#29702;&#22810;&#20010;DNN&#35831;&#27714;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#22788;&#29702;&#26102;&#38388;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#31639;&#27861;&#65292;&#20197;&#21033;&#29992;&#36816;&#34892;&#30456;&#21516;DNN&#30340;&#25152;&#26377;&#35831;&#27714;&#30340;&#25209;&#22788;&#29702;&#20248;&#21183;&#12290;&#36825;&#24456;&#26377;&#35828;&#26381;&#21147;&#65292;&#22240;&#20026;&#21482;&#26377;&#23569;&#25968;DNN&#65292;&#35768;&#22810;&#35831;&#27714;&#20542;&#21521;&#20110;&#20351;&#29992;&#21516;&#19968;&#20010;DNN&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#30340;&#30446;&#26631;&#65292;&#22914;&#26368;&#23567;&#21270;&#23436;&#25104;&#26102;&#38388;&#25110;&#26368;&#22823;&#21270;&#21450;&#26102;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#25105;&#20204;&#30340;&#31639;&#27861;&#20197;&#22788;&#29702;&#20351;&#29992;&#19981;&#21516;DNN&#30340;&#20855;&#26377;&#25110;&#19981;&#20855;&#26377;&#20849;&#20139;&#23618;&#30340;&#35831;&#27714;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21327;&#20316;&#26041;&#27861;&#26469;&#35843;&#24230;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#30340;DNN&#35831;&#27714;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22788;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have been widely used in various video analytic tasks. These tasks demand real-time responses. Due to the limited processing power on mobile devices, a common way to support such real-time analytics is to offload the processing to an edge server. This paper examines how to speed up the edge server DNN processing for multiple clients. In particular, we observe batching multiple DNN requests significantly speeds up the processing time. Based on this observation, we first design a novel scheduling algorithm to exploit the batching benefits of all requests that run the same DNN. This is compelling since there are only a handful of DNNs and many requests tend to use the same DNN. Our algorithms are general and can support different objectives, such as minimizing the completion time or maximizing the on-time ratio. We then extend our algorithm to handle requests that use different DNNs with or without shared layers. Finally, we develop a collaborative approach to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.09960</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#28508;&#22312;&#31354;&#38388;&#29702;&#35770;&#23545;&#24212;&#26032;&#20852;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24182;&#19981;&#26159;&#38543;&#26426;&#29983;&#25104;&#65292;&#32780;&#26159;&#20026;&#20102;&#20256;&#36882;&#20449;&#24687;&#12290;&#35821;&#35328;&#19982;&#20854;&#24213;&#23618;&#21547;&#20041;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#20851;&#32852;&#65292;&#22312;&#20854;&#30456;&#20851;&#24615;&#26041;&#38754;&#26377;&#30528;&#20005;&#37325;&#20559;&#24046;&#30340;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#31232;&#30095;&#24615;&#65292;&#36825;&#20123;&#39640;&#23792;&#20540;&#24688;&#22909;&#19982;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#21305;&#37197;&#12290;&#38543;&#30528;&#22823;&#25968;&#25454;&#21644;&#22823;&#27169;&#22411;&#19978;&#35757;&#32451;&#30340;LLMs&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#29616;&#22312;&#21487;&#20197;&#31934;&#30830;&#35780;&#20272;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#25506;&#32034;&#32852;&#21512;&#20998;&#24067;&#31232;&#30095;&#32467;&#26500;&#23454;&#29616;&#26377;&#25928;&#25512;&#29702;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#20998;&#31867;&#20026;&#26126;&#30830;&#19982;{\epsilon}-&#27169;&#31946;&#65292;&#24182;&#25552;&#20986;&#23450;&#37327;&#32467;&#26524;&#65292;&#20197;&#34920;&#26126;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#65288;&#20363;&#22914;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#65289;&#37117;&#21487;&#20197;&#24402;&#22240;&#20110;&#23545;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#36164;&#20135;&#29305;&#23450;&#22240;&#32032;&#22312;&#39044;&#27979;&#34892;&#19994;&#22238;&#25253;&#21644;&#27979;&#37327;&#34892;&#19994;&#29305;&#23450;&#39118;&#38505;&#28322;&#20215;&#26041;&#38754;&#33719;&#24471;&#26356;&#22823;&#32463;&#27982;&#25910;&#30410;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#22312;&#32447;&#38598;&#25104;&#31639;&#27861;&#26469;&#23398;&#20064;&#20248;&#21270;&#39044;&#27979;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#21644;&#21487;&#33021;&#30340;&#40657;&#30418;&#27169;&#22411;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2304.09947</link><description>&lt;p&gt;
&#22312;&#32447;&#27169;&#22411;&#38598;&#25104;&#23545;&#26368;&#20248;&#39044;&#27979;&#24615;&#33021;&#30340;&#24212;&#29992;&#21644;&#34892;&#19994;&#36718;&#25442;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Online Ensemble of Models for Optimal Predictive Performance with Applications to Sector Rotation Strategy. (arXiv:2304.09947v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09947
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#36164;&#20135;&#29305;&#23450;&#22240;&#32032;&#22312;&#39044;&#27979;&#34892;&#19994;&#22238;&#25253;&#21644;&#27979;&#37327;&#34892;&#19994;&#29305;&#23450;&#39118;&#38505;&#28322;&#20215;&#26041;&#38754;&#33719;&#24471;&#26356;&#22823;&#32463;&#27982;&#25910;&#30410;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#22312;&#32447;&#38598;&#25104;&#31639;&#27861;&#26469;&#23398;&#20064;&#20248;&#21270;&#39044;&#27979;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#21644;&#21487;&#33021;&#30340;&#40657;&#30418;&#27169;&#22411;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#20135;&#29305;&#23450;&#22240;&#32032;&#36890;&#24120;&#29992;&#20110;&#39044;&#27979;&#37329;&#34701;&#22238;&#25253;&#24182;&#37327;&#21270;&#36164;&#20135;&#29305;&#23450;&#39118;&#38505;&#28322;&#20215;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35777;&#26126;&#65292;&#36825;&#20123;&#22240;&#32032;&#21253;&#21547;&#30340;&#20449;&#24687;&#21487;&#20197;&#22312;&#39044;&#27979;&#34892;&#19994;&#22238;&#25253;&#21644;&#27979;&#37327;&#34892;&#19994;&#29305;&#23450;&#39118;&#38505;&#28322;&#20215;&#26041;&#38754;&#24102;&#26469;&#26356;&#22823;&#30340;&#32463;&#27982;&#25910;&#30410;&#12290;&#20026;&#20102;&#21033;&#29992;&#19981;&#21516;&#34892;&#19994;&#34920;&#29616;&#30340;&#21333;&#20010;&#27169;&#22411;&#30340;&#24378;&#39044;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#22312;&#32447;&#38598;&#25104;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23398;&#20064;&#20248;&#21270;&#39044;&#27979;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#19981;&#26029;&#36866;&#24212;&#65292;&#36890;&#36807;&#20998;&#26512;&#23427;&#20204;&#26368;&#36817;&#30340;&#39044;&#27979;&#24615;&#33021;&#26469;&#30830;&#23450;&#20010;&#20307;&#27169;&#22411;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;&#36825;&#20351;&#23427;&#29305;&#21035;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#65292;&#28378;&#21160;&#31383;&#21475;&#22238;&#27979;&#31243;&#24207;&#21644;&#21487;&#33021;&#30340;&#40657;&#30418;&#27169;&#22411;&#31995;&#32479;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#26368;&#20248;&#22686;&#30410;&#20989;&#25968;&#65292;&#29992;&#26679;&#26412;&#22806;R&#24179;&#26041;&#24230;&#37327;&#34920;&#36798;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#25512;&#23548;&#20986;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Asset-specific factors are commonly used to forecast financial returns and quantify asset-specific risk premia. Using various machine learning models, we demonstrate that the information contained in these factors leads to even larger economic gains in terms of forecasts of sector returns and the measurement of sector-specific risk premia. To capitalize on the strong predictive results of individual models for the performance of different sectors, we develop a novel online ensemble algorithm that learns to optimize predictive performance. The algorithm continuously adapts over time to determine the optimal combination of individual models by solely analyzing their most recent prediction performance. This makes it particularly suited for time series problems, rolling window backtesting procedures, and systems of potentially black-box models. We derive the optimal gain function, express the corresponding regret bounds in terms of the out-of-sample R-squared measure, and derive optimal le
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#34928;&#36864;&#26399;&#38388;&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#19982;&#31283;&#23450;&#24066;&#22330;&#26377;&#20851;&#30340;&#22240;&#32032;&#21487;&#33021;&#26159;&#20854;&#34920;&#29616;&#20248;&#24322;&#30340;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;ML&#20174;&#19994;&#32773;&#22312;&#34928;&#36864;&#21644;&#22797;&#33487;&#26399;&#38388;&#35780;&#20272;&#20854;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.09937</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#21644;&#21830;&#19994;&#21608;&#26399;
&lt;/p&gt;
&lt;p&gt;
Stock Price Predictability and the Business Cycle via Machine Learning. (arXiv:2304.09937v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09937
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#34928;&#36864;&#26399;&#38388;&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#19982;&#31283;&#23450;&#24066;&#22330;&#26377;&#20851;&#30340;&#22240;&#32032;&#21487;&#33021;&#26159;&#20854;&#34920;&#29616;&#20248;&#24322;&#30340;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;ML&#20174;&#19994;&#32773;&#22312;&#34928;&#36864;&#21644;&#22797;&#33487;&#26399;&#38388;&#35780;&#20272;&#20854;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21830;&#19994;&#21608;&#26399;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#26631;&#26222;500&#25351;&#25968;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#22823;&#22810;&#25968;&#34928;&#36864;&#26399;&#38388;&#65292;ML&#27169;&#22411;&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#32780;&#34928;&#36864;&#21382;&#21490;&#25110;&#26080;&#39118;&#38505;&#21033;&#29575;&#30340;&#21253;&#21547;&#24182;&#19981;&#33021; necessarily &#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#22312;&#35843;&#26597;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#30340;&#34928;&#36864;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#24066;&#22330;&#27874;&#21160;&#24615;&#36739;&#20854;&#20182;&#34928;&#36864;&#26399;&#36739;&#20302;&#12290;&#36825;&#24847;&#21619;&#30528;&#34920;&#29616;&#25552;&#39640;&#19981;&#26159;&#30001;&#20110;ML&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#32780;&#26159;&#22240;&#20026;&#26377;&#25928;&#30340;&#36135;&#24065;&#25919;&#31574;&#31283;&#23450;&#20102;&#24066;&#22330;&#12290;&#25105;&#20204;&#24314;&#35758;ML&#20174;&#19994;&#32773;&#22312;&#34928;&#36864;&#21644;&#22797;&#33487;&#26399;&#38388;&#35780;&#20272;&#20854;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the impacts of business cycles on machine learning (ML) predictions. Using the S&amp;P 500 index, we find that ML models perform worse during most recessions, and the inclusion of recession history or the risk-free rate does not necessarily improve their performance. Investigating recessions where models perform well, we find that they exhibit lower market volatility than other recessions. This implies that the improved performance is not due to the merit of ML methods but rather factors such as effective monetary policies that stabilized the market. We recommend that ML practitioners evaluate their models during both recessions and expansions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35762;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;ML/DL&#27169;&#22411;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#65292;&#20197;&#24110;&#21161;&#25237;&#36164;&#32773;&#21644;&#37329;&#34701;&#26426;&#26500;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#21644;&#21046;&#23450;&#26377;&#25928;&#30340;&#39118;&#38505;&#31649;&#29702;&#25919;&#31574;&#12290;</title><link>http://arxiv.org/abs/2304.09936</link><description>&lt;p&gt;
&#20351;&#29992;&#25216;&#26415;&#20998;&#26512;&#21644;ML/DL&#27169;&#22411;&#35782;&#21035;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
Identifying Trades Using Technical Analysis and ML/DL Models. (arXiv:2304.09936v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35762;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;ML/DL&#27169;&#22411;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#65292;&#20197;&#24110;&#21161;&#25237;&#36164;&#32773;&#21644;&#37329;&#34701;&#26426;&#26500;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#21644;&#21046;&#23450;&#26377;&#25928;&#30340;&#39118;&#38505;&#31649;&#29702;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32929;&#24066;&#20215;&#26684;&#30340;&#37325;&#35201;&#24615;&#19981;&#35328;&#32780;&#21947;&#12290;&#23545;&#20110;&#25237;&#36164;&#32773;&#21644;&#37329;&#34701;&#26426;&#26500;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20351;&#20182;&#20204;&#33021;&#22815;&#20570;&#20986;&#26126;&#26234;&#30340;&#25237;&#36164;&#20915;&#31574;&#65292;&#31649;&#29702;&#39118;&#38505;&#24182;&#30830;&#20445;&#37329;&#34701;&#31995;&#32479;&#30340;&#31283;&#23450;&#12290;&#20934;&#30830;&#30340;&#32929;&#24066;&#39044;&#27979;&#21487;&#20197;&#24110;&#21161;&#25237;&#36164;&#32773;&#26368;&#22823;&#21270;&#25910;&#30410;&#24182;&#23613;&#37327;&#20943;&#23569;&#25439;&#22833;&#65292;&#32780;&#37329;&#34701;&#26426;&#26500;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#21046;&#23450;&#26377;&#25928;&#30340;&#39118;&#38505;&#31649;&#29702;&#25919;&#31574;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32929;&#24066;&#30340;&#22797;&#26434;&#24615;&#21644;&#24433;&#21709;&#32929;&#31080;&#20215;&#26684;&#30340;&#35832;&#22810;&#22240;&#32032;&#65292;&#32929;&#24066;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#22914;&#28145;&#24230;&#23398;&#20064;&#65292;&#34987;&#29992;&#20110;&#20998;&#26512;&#28023;&#37327;&#25968;&#25454;&#24182;&#27934;&#23519;&#32929;&#24066;&#36816;&#34892;&#34892;&#20026;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#22312;&#20934;&#30830;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#36825;&#20010;&#39046;&#22495;&#20173;&#26377;&#35768;&#22810;&#30740;&#31350;&#38656;&#35201;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The importance of predicting stock market prices cannot be overstated. It is a pivotal task for investors and financial institutions as it enables them to make informed investment decisions, manage risks, and ensure the stability of the financial system. Accurate stock market predictions can help investors maximize their returns and minimize their losses, while financial institutions can use this information to develop effective risk management policies. However, stock market prediction is a challenging task due to the complex nature of the stock market and the multitude of factors that can affect stock prices. As a result, advanced technologies such as deep learning are being increasingly utilized to analyze vast amounts of data and provide valuable insights into the behavior of the stock market. While deep learning has shown promise in accurately predicting stock prices, there is still much research to be done in this area.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#29366;&#24577;&#28966;&#34385;&#26816;&#27979;&#65292;&#21033;&#29992;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26816;&#27979;&#29366;&#24577;&#28966;&#34385;&#65292;&#24182;&#36890;&#36807;&#22810;&#23618;&#20010;&#24615;&#21270;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#32771;&#34385;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25429;&#25417;&#20010;&#20307;&#20043;&#38388;&#30340;&#24515;&#29702;&#21644;&#34892;&#20026;&#21453;&#24212;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.09928</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#29366;&#24577;&#28966;&#34385;&#26816;&#27979;&#65306;&#22522;&#20110;&#35821;&#35328;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Personalized State Anxiety Detection: An Empirical Study with Linguistic Biomarkers and A Machine Learning Pipeline. (arXiv:2304.09928v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09928
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#29366;&#24577;&#28966;&#34385;&#26816;&#27979;&#65292;&#21033;&#29992;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26816;&#27979;&#29366;&#24577;&#28966;&#34385;&#65292;&#24182;&#36890;&#36807;&#22810;&#23618;&#20010;&#24615;&#21270;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#32771;&#34385;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25429;&#25417;&#20010;&#20307;&#20043;&#38388;&#30340;&#24515;&#29702;&#21644;&#34892;&#20026;&#21453;&#24212;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#31038;&#20132;&#28966;&#34385;&#30151;&#29366;&#30340;&#20154;&#24448;&#24448;&#22312;&#31038;&#20132;&#22330;&#21512;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#29366;&#24577;&#28966;&#34385;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#26816;&#27979;&#29366;&#24577;&#28966;&#34385;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#26159;&#22312;&#25972;&#20010;&#21442;&#19982;&#32773;&#32676;&#20307;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#26410;&#33021;&#25429;&#25417;&#21040;&#20182;&#20204;&#30340;&#24515;&#29702;&#21644;&#34892;&#20026;&#21453;&#24212;&#22312;&#31038;&#20132;&#29615;&#22659;&#20013;&#30340;&#20010;&#20307;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#31532;&#19968;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;35&#21517;&#39640;&#31038;&#20132;&#28966;&#34385;&#21442;&#19982;&#32773;&#22312;&#21508;&#31181;&#31038;&#20132;&#29615;&#22659;&#20013;&#30340;&#35821;&#35328;&#25968;&#25454;&#65292;&#21457;&#29616;&#25968;&#23383;&#35821;&#35328;&#29983;&#29289;&#26631;&#24535;&#29289;&#22312;&#35780;&#20215;&#24615;&#19982;&#38750;&#35780;&#20215;&#24615;&#31038;&#20132;&#29615;&#22659;&#19979;&#20197;&#21450;&#22312;&#20855;&#26377;&#19981;&#21516;&#29305;&#36136;&#24515;&#29702;&#30151;&#29366;&#30340;&#20010;&#20307;&#20043;&#38388;&#26174;&#33879;&#19981;&#21516;&#65292;&#36825;&#34920;&#26126;&#20010;&#24615;&#21270;&#26041;&#27861;&#26816;&#27979;&#29366;&#24577;&#28966;&#34385;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#31532;&#20108;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#25968;&#25454;&#21644;&#31532;&#19968;&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#26469;&#24314;&#31435;&#22810;&#23618;&#20010;&#24615;&#21270;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#26469;&#26816;&#27979;&#29366;&#24577;&#28966;&#34385;&#65292;&#32771;&#34385;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individuals high in social anxiety symptoms often exhibit elevated state anxiety in social situations. Research has shown it is possible to detect state anxiety by leveraging digital biomarkers and machine learning techniques. However, most existing work trains models on an entire group of participants, failing to capture individual differences in their psychological and behavioral responses to social contexts. To address this concern, in Study 1, we collected linguistic data from N=35 high socially anxious participants in a variety of social contexts, finding that digital linguistic biomarkers significantly differ between evaluative vs. non-evaluative social contexts and between individuals having different trait psychological symptoms, suggesting the likely importance of personalized approaches to detect state anxiety. In Study 2, we used the same data and results from Study 1 to model a multilayer personalized machine learning pipeline to detect state anxiety that considers contextu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#20998;&#26512;&#20102;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;220&#20010;&#25919;&#27835;&#39046;&#34966;&#30340;YouTube&#35270;&#39057;&#65292;&#24635;&#32467;&#20102;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.09914</link><description>&lt;p&gt;
&#26623;&#23376;&#25919;&#27835;&#30340;&#38754;&#23380;&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27604;&#36739;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning. (arXiv:2304.09914v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#20998;&#26512;&#20102;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;220&#20010;&#25919;&#27835;&#39046;&#34966;&#30340;YouTube&#35270;&#39057;&#65292;&#24635;&#32467;&#20102;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23186;&#20307;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#25919;&#27835;&#20449;&#24687;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20256;&#25773;&#21644;&#28040;&#36153;&#26041;&#24335;&#65292;&#36825;&#31181;&#36716;&#21464;&#20419;&#20351;&#25919;&#27835;&#20154;&#29289;&#37319;&#21462;&#26032;&#30340;&#31574;&#30053;&#26469;&#25429;&#25417;&#21644;&#20445;&#25345;&#36873;&#27665;&#30340;&#27880;&#24847;&#21147;&#12290;&#36825;&#20123;&#31574;&#30053;&#24448;&#24448;&#20381;&#36182;&#20110;&#24773;&#24863;&#35828;&#26381;&#21644;&#21560;&#24341;&#12290;&#38543;&#30528;&#34394;&#25311;&#31354;&#38388;&#20013;&#35270;&#35273;&#20869;&#23481;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#24456;&#22810;&#25919;&#27835;&#27807;&#36890;&#20063;&#34987;&#26631;&#24535;&#30528;&#21796;&#36215;&#24773;&#24863;&#30340;&#35270;&#39057;&#20869;&#23481;&#21644;&#22270;&#20687;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;&#29616;&#26377;&#35757;&#32451;&#22909;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25552;&#20379;&#30340;Python&#24211;fer&#65292;&#24212;&#29992;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#65292;&#23545;&#25551;&#32472;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;&#25919;&#27835;&#39046;&#34966;&#30340;220&#20010;YouTube&#35270;&#39057;&#26679;&#26412;&#36827;&#34892;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#36820;&#22238;&#24773;&#32490;&#20998;&#25968;&#65292;&#27599;&#19968;&#24103;&#37117;&#20195;&#34920;6&#31181;&#24773;&#32490;&#29366;&#24577;&#65288;&#24868;&#24594;&#65292;&#21388;&#24694;&#65292;&#24656;&#24807;&#65292;&#24555;&#20048;&#65292;&#24754;&#20260;&#21644;&#24778;&#35766;&#65289;&#21644;&#19968;&#20010;&#20013;&#24615;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online media has revolutionized the way political information is disseminated and consumed on a global scale, and this shift has compelled political figures to adopt new strategies of capturing and retaining voter attention. These strategies often rely on emotional persuasion and appeal, and as visual content becomes increasingly prevalent in virtual space, much of political communication too has come to be marked by evocative video content and imagery. The present paper offers a novel approach to analyzing material of this kind. We apply a deep-learning-based computer-vision algorithm to a sample of 220 YouTube videos depicting political leaders from 15 different countries, which is based on an existing trained convolutional neural network architecture provided by the Python library fer. The algorithm returns emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#20869;&#26680;&#21305;&#37197;&#38382;&#39064;&#65292;&#22312;Pegasos&#31639;&#27861;&#30340;&#22522;&#30784;&#19978;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#35299;&#20915;&#20102;&#35813;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#38750;&#31283;&#23450;&#21270;&#20869;&#26680;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09899</link><description>&lt;p&gt;
&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#37327;&#23376;&#20869;&#26680;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Quantum Kernel Alignment with Stochastic Gradient Descent. (arXiv:2304.09899v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#20869;&#26680;&#21305;&#37197;&#38382;&#39064;&#65292;&#22312;Pegasos&#31639;&#27861;&#30340;&#22522;&#30784;&#19978;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#35299;&#20915;&#20102;&#35813;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#38750;&#31283;&#23450;&#21270;&#20869;&#26680;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#26377;&#26395;&#23454;&#29616;&#35299;&#20915;&#26576;&#20123;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#37327;&#23376;&#21152;&#36895;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#25214;&#21040;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#22909;&#30340;&#37327;&#23376;&#20869;&#26680;&#65292;&#36825;&#20010;&#20219;&#21153;&#34987;&#31216;&#20026;&#20869;&#26680;&#21305;&#37197;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;Pegasos&#31639;&#27861;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;Pegasos&#26159;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#35299;&#20915;&#25903;&#25345;&#21521;&#37327;&#26426;&#20248;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;Pegasos&#25299;&#23637;&#21040;&#37327;&#23376;&#22330;&#26223;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;&#20869;&#26680;&#21305;&#37197;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#20197;&#21069;&#36890;&#36807;&#22312;&#22806;&#37096;&#20248;&#21270;&#24490;&#29615;&#20013;&#35757;&#32451;QSVM&#26469;&#25191;&#34892;&#20869;&#26680;&#21305;&#37197;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;Pegasos&#21516;&#27493;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#21305;&#37197;&#20869;&#26680;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20197;&#39640;&#31934;&#24230;&#23545;&#40784;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#65292;&#24182;&#19988;&#20248;&#20110;&#29616;&#26377;&#30340;&#37327;&#23376;&#20869;&#26680;&#23545;&#40784;&#25216;&#26415;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Pegasos&#22312;&#38750;&#31283;&#23450;&#21270;&#20869;&#26680;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#65292;&#22312;&#37027;&#37324;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum support vector machines have the potential to achieve a quantum speedup for solving certain machine learning problems. The key challenge for doing so is finding good quantum kernels for a given data set -- a task called kernel alignment. In this paper we study this problem using the Pegasos algorithm, which is an algorithm that uses stochastic gradient descent to solve the support vector machine optimization problem. We extend Pegasos to the quantum case and and demonstrate its effectiveness for kernel alignment. Unlike previous work which performs kernel alignment by training a QSVM within an outer optimization loop, we show that using Pegasos it is possible to simultaneously train the support vector machine and align the kernel. Our experiments show that this approach is capable of aligning quantum feature maps with high accuracy, and outperforms existing quantum kernel alignment techniques. Specifically, we demonstrate that Pegasos is particularly effective for non-stationar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#35813;&#20998;&#25968;&#25429;&#25417;&#20102;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.09875</link><description>&lt;p&gt;
GREAT&#20998;&#25968;&#65306;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models. (arXiv:2304.09875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#35813;&#20998;&#25968;&#25429;&#25417;&#20102;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23545;&#20110;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#32858;&#21512;&#19968;&#32452;&#25968;&#25454;&#26679;&#26412;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#32467;&#26524;&#19978;&#65292;&#20197;&#35780;&#20272;&#21644;&#25490;&#21517;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23616;&#37096;&#32479;&#35745;&#37327;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#20195;&#34920;&#22522;&#30784;&#26410;&#30693;&#25968;&#25454;&#20998;&#24067;&#30340;&#30495;&#27491;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;GREAT&#20998;&#25968;&#27491;&#24335;&#20855;&#26377;&#19968;&#20010;&#20840;&#23616;&#32479;&#35745;&#37327;&#30340;&#29289;&#29702;&#24847;&#20041;&#65292;&#25429;&#25417;&#26469;&#33258;&#29983;&#25104;&#27169;&#22411;&#30340;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#12290;&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#35780;&#20272;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26679;&#26412;&#22343;&#20540;&#19982;&#30495;&#23454;&#22343;&#20540;&#20043;&#38388;&#30340;&#27010;&#29575;&#20445;&#35777;&#12290;GREAT&#20998;&#25968;&#26377;&#20960;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;&#20351;&#29992;GREAT&#20998;&#25968;&#36827;&#34892;&#40065;&#26834;&#24615;&#35780;&#20272;&#39640;&#25928;&#32780;&#19988;&#35268;&#27169;&#21487;&#25193;&#23637;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current studies on adversarial robustness mainly focus on aggregating local robustness results from a set of data samples to evaluate and rank different models. However, the local statistics may not well represent the true global robustness of the underlying unknown data distribution. To address this challenge, this paper makes the first attempt to present a new framework, called GREAT Score , for global robustness evaluation of adversarial perturbation using generative models. Formally, GREAT Score carries the physical meaning of a global statistic capturing a mean certified attack-proof perturbation level over all samples drawn from a generative model. For finite-sample evaluation, we also derive a probabilistic guarantee on the sample complexity and the difference between the sample mean and the true mean. GREAT Score has several advantages: (1) Robustness evaluations using GREAT Score are efficient and scalable to large models, by sparing the need of running adversarial attacks. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#30340;&#20559;&#24207;&#38598;&#21512;&#25551;&#36848;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#20559;&#24207;&#29256;&#26412;&#30340;&#21333;&#32431;&#28145;&#24230;&#65292;&#29992;&#20110;&#27604;&#36739;&#22522;&#20110;&#22810;&#32500;&#24615;&#33021;&#24230;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#19982;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#19981;&#21516;&#65292;&#20026;&#20998;&#31867;&#22120;&#27604;&#36739;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2304.09872</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#30340;&#20559;&#24207;&#38598;&#21512;&#30340;&#25551;&#36848;&#24615;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Depth Functions for Partial Orders with a Descriptive Analysis of Machine Learning Algorithms. (arXiv:2304.09872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#30340;&#20559;&#24207;&#38598;&#21512;&#25551;&#36848;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#20559;&#24207;&#29256;&#26412;&#30340;&#21333;&#32431;&#28145;&#24230;&#65292;&#29992;&#20110;&#27604;&#36739;&#22522;&#20110;&#22810;&#32500;&#24615;&#33021;&#24230;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#19982;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#19981;&#21516;&#65292;&#20026;&#20998;&#31867;&#22120;&#27604;&#36739;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#23545;&#20559;&#24207;&#38598;&#21512;&#36827;&#34892;&#25551;&#36848;&#24615;&#20998;&#26512;&#12290;&#23613;&#31649;&#28145;&#24230;&#20989;&#25968;&#22312;&#32447;&#24615;&#21644;&#24230;&#37327;&#31354;&#38388;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#26159;&#23545;&#20110;&#20559;&#24207;&#31561;&#38750;&#26631;&#20934;&#25968;&#25454;&#31867;&#22411;&#30340;&#28145;&#24230;&#20989;&#25968;&#30340;&#35752;&#35770;&#21364;&#24456;&#23569;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#33879;&#21517;&#30340;&#21333;&#32431;&#28145;&#24230;&#30340;&#20559;&#24207;&#29256;&#26412;-&#26080;&#24182;&#36890;&#29992;&#28145;&#24230;&#65288;ufg depth&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340; ufg depth &#26469;&#27604;&#36739;&#22522;&#20110;&#22810;&#32500;&#24615;&#33021;&#24230;&#37327;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20998;&#26512;&#19981;&#21516;&#20998;&#31867;&#22120;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26377;&#24076;&#26395;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#26377;&#24456;&#22823;&#19981;&#21516;&#65292;&#22240;&#27492;&#20026;&#20998;&#31867;&#22120;&#27604;&#36739;&#30340;&#28608;&#28872;&#36777;&#35770;&#22686;&#21152;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for descriptively analyzing sets of partial orders based on the concept of depth functions. Despite intensive studies of depth functions in linear and metric spaces, there is very little discussion on depth functions for non-standard data types such as partial orders. We introduce an adaptation of the well-known simplicial depth to the set of all partial orders, the union-free generic (ufg) depth. Moreover, we utilize our ufg depth for a comparison of machine learning algorithms based on multidimensional performance measures. Concretely, we analyze the distribution of different classifier performances over a sample of standard benchmark data sets. Our results promisingly demonstrate that our approach differs substantially from existing benchmarking approaches and, therefore, adds a new perspective to the vivid debate on the comparison of classifiers.
&lt;/p&gt;</description></item><item><title>Adam&#20248;&#21270;&#31639;&#27861;&#22312;&#22823;&#25209;&#37327;&#30340;&#35757;&#32451;&#19979;&#23481;&#26131;&#20986;&#29616;&#19981;&#31283;&#23450;&#29616;&#35937;&#65292;&#24182;&#23548;&#33268;&#35757;&#32451;&#24322;&#24120;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35813;&#29616;&#35937;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.09871</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#20013;Adam&#19981;&#31283;&#23450;&#24615;&#30340;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Theory on Adam Instability in Large-Scale Machine Learning. (arXiv:2304.09871v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09871
&lt;/p&gt;
&lt;p&gt;
Adam&#20248;&#21270;&#31639;&#27861;&#22312;&#22823;&#25209;&#37327;&#30340;&#35757;&#32451;&#19979;&#23481;&#26131;&#20986;&#29616;&#19981;&#31283;&#23450;&#29616;&#35937;&#65292;&#24182;&#23548;&#33268;&#35757;&#32451;&#24322;&#24120;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35813;&#29616;&#35937;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20043;&#21069;&#26410;&#34987;&#35299;&#37322;&#30340;&#29616;&#35937;&#30340;&#29702;&#35770;&#65292;&#35813;&#29616;&#35937;&#20986;&#29616;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#30340;&#21457;&#25955;&#34892;&#20026;&#20013;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#20027;&#27969;&#30340;&#20248;&#21270;&#31639;&#27861; Adam &#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040; Adam &#21487;&#33021;&#20250;&#36827;&#20837;&#19968;&#31181;&#29366;&#24577;&#65292;&#20854;&#20013;&#21442;&#25968;&#26356;&#26032;&#21521;&#37327;&#26377;&#27604;&#36739;&#22823;&#30340;&#33539;&#25968;&#65292;&#24182;&#19988;&#19982;&#35757;&#32451;&#25439;&#22833;&#26223;&#35266;&#19979;&#30340;&#19979;&#38477;&#26041;&#21521;&#22522;&#26412;&#26080;&#20851;&#65292;&#20174;&#32780;&#23548;&#33268;&#21457;&#25955;&#12290;&#36825;&#31181;&#29616;&#35937;&#26356;&#23481;&#26131;&#22312;&#22823;&#25209;&#37327;&#24773;&#20917;&#19979;&#20986;&#29616;&#65292;&#36825;&#20063;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#20856;&#22411;&#35774;&#32622;&#12290;&#20026;&#20102;&#35777;&#26126;&#35813;&#29702;&#35770;&#65292;&#25105;&#20204;&#23545;&#35268;&#27169;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;70&#20159;&#65292;300&#20159;&#65292;650&#20159;&#21644;5460&#20159;&#21442;&#25968;&#65289;&#36827;&#34892;&#20102;&#35757;&#32451;&#36816;&#34892;&#30340;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a theory for the previously unexplained divergent behavior noticed in the training of large language models. We argue that the phenomenon is an artifact of the dominant optimization algorithm used for training, called Adam. We observe that Adam can enter a state in which the parameter update vector has a relatively large norm and is essentially uncorrelated with the direction of descent on the training loss landscape, leading to divergence. This artifact is more likely to be observed in the training of a deep model with a large batch size, which is the typical setting of large-scale language model training. To argue the theory, we present observations from the training runs of the language models of different scales: 7 billion, 30 billion, 65 billion, and 546 billion parameters.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;HARL&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;&#38480;&#21046;&#65292;&#21516;&#26102;&#36890;&#36807;&#24341;&#20837;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#20998;&#35299;&#24341;&#29702;&#21644;&#24207;&#21015;&#26356;&#26032;&#26041;&#26696;&#65292;&#24314;&#31435;&#20102;&#24322;&#26500;&#26234;&#33021;&#20307;&#20449;&#20219;&#21306;&#22495;&#23398;&#20064;&#65288;HATRL&#65289;&#31639;&#27861;&#21450;&#20854;&#26131;&#22788;&#29702;&#30340;&#36924;&#36817;&#26041;&#24335; HATRPO &#21644; HAPPO&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#26500;&#26234;&#33021;&#20307;&#38236;&#20687;&#23398;&#20064;&#65288;HAML&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21152;&#24378;&#20102;&#23545;HATRPO&#21644;HAPPO&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.09870</link><description>&lt;p&gt;
&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous-Agent Reinforcement Learning. (arXiv:2304.09870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09870
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;HARL&#65289;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;&#38480;&#21046;&#65292;&#21516;&#26102;&#36890;&#36807;&#24341;&#20837;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#20998;&#35299;&#24341;&#29702;&#21644;&#24207;&#21015;&#26356;&#26032;&#26041;&#26696;&#65292;&#24314;&#31435;&#20102;&#24322;&#26500;&#26234;&#33021;&#20307;&#20449;&#20219;&#21306;&#22495;&#23398;&#20064;&#65288;HATRL&#65289;&#31639;&#27861;&#21450;&#20854;&#26131;&#22788;&#29702;&#30340;&#36924;&#36817;&#26041;&#24335; HATRPO &#21644; HAPPO&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#26500;&#26234;&#33021;&#20307;&#38236;&#20687;&#23398;&#20064;&#65288;HAML&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21152;&#24378;&#20102;&#23545;HATRPO&#21644;HAPPO&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21442;&#25968;&#20849;&#20139;&#65292;&#36825;&#23558;&#23427;&#20204;&#38480;&#21046;&#22312;&#21516;&#36136;&#24322;&#26500;&#26234;&#33021;&#20307;&#35774;&#32622;&#19979;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#32570;&#20047;&#25910;&#25947;&#20445;&#35777;&#12290;&#20026;&#20102;&#22312;&#19968;&#33324;&#30340;&#24322;&#26500;&#26234;&#33021;&#20307;&#35774;&#32622;&#19979;&#23454;&#29616;&#26377;&#25928;&#30340;&#21327;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#30340;&#24322;&#26500;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;HARL&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26680;&#24515;&#26159;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#20998;&#35299;&#24341;&#29702;&#21644;&#24207;&#21015;&#26356;&#26032;&#26041;&#26696;&#12290;&#22522;&#20110;&#36825;&#20123;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32463;&#36807;&#39564;&#35777;&#30340;&#26080;&#21442;&#25968;&#20849;&#20139;&#32422;&#26463;&#30340;&#24322;&#26500;&#26234;&#33021;&#20307;&#20449;&#20219;&#21306;&#22495;&#23398;&#20064;&#65288;HATRL&#65289;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#26131;&#22788;&#29702;&#30340;&#36924;&#36817;&#26041;&#24335;&#24471;&#20986;&#20102;HATRPO&#21644;HAPPO&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#24322;&#26500;&#26234;&#33021;&#20307;&#38236;&#20687;&#23398;&#20064;&#65288;HAML&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#21152;&#24378;&#20102;&#23545;HATRPO&#21644;HAPPO&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The necessity for cooperation among intelligent machines has popularised cooperative multi-agent reinforcement learning (MARL) in AI research. However, many research endeavours heavily rely on parameter sharing among agents, which confines them to only homogeneous-agent setting and leads to training instability and lack of convergence guarantees. To achieve effective cooperation in the general heterogeneous-agent setting, we propose Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the aforementioned issues. Central to our findings are the multi-agent advantage decomposition lemma and the sequential update scheme. Based on these, we develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL) that is free of parameter-sharing constraint, and derive HATRPO and HAPPO by tractable approximations. Furthermore, we discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML), which strengthens theoretical guarantees for HATRPO and HAPP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36827;&#21270;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#38543;&#26426;&#25490;&#21517;&#33258;&#36866;&#24212;&#24179;&#34913;&#22870;&#21169;&#21644;&#32422;&#26463;&#36829;&#35268;&#65292;&#24182;&#21516;&#26102;&#36890;&#36807;&#32500;&#25252;&#19968;&#32452;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#31995;&#25968;&#21450;&#19968;&#20010;&#32422;&#26463;&#32531;&#20914;&#21306;&#26469;&#38480;&#21046;&#31574;&#30053;&#34892;&#20026;&#12290;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09869</link><description>&lt;p&gt;
&#36827;&#21270;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Evolving Constrained Reinforcement Learning Policy. (arXiv:2304.09869v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36827;&#21270;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#38543;&#26426;&#25490;&#21517;&#33258;&#36866;&#24212;&#24179;&#34913;&#22870;&#21169;&#21644;&#32422;&#26463;&#36829;&#35268;&#65292;&#24182;&#21516;&#26102;&#36890;&#36807;&#32500;&#25252;&#19968;&#32452;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#31995;&#25968;&#21450;&#19968;&#20010;&#32422;&#26463;&#32531;&#20914;&#21306;&#26469;&#38480;&#21046;&#31574;&#30053;&#34892;&#20026;&#12290;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#24050;&#34987;&#29992;&#20110;&#28436;&#21270;&#20986;&#19968;&#32452;&#25191;&#34892;&#32773;&#65292;&#20197;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#20307;&#39564;&#26469;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#20174;&#32780;&#35299;&#20915;&#26102;&#38388;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#24182;&#25552;&#39640;&#25506;&#32034;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#32422;&#26463;&#38382;&#39064;&#26102;&#65292;&#24456;&#38590;&#24179;&#34913;&#22870;&#21169;&#21644;&#32422;&#26463;&#36829;&#35268;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36827;&#21270;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#65288;ECRL&#65289;&#31639;&#27861;&#65292;&#37319;&#29992;&#38543;&#26426;&#25490;&#21517;&#33258;&#36866;&#24212;&#24179;&#34913;&#22870;&#21169;&#21644;&#32422;&#26463;&#36829;&#35268;&#65292;&#24182;&#21516;&#26102;&#36890;&#36807;&#32500;&#25252;&#19968;&#32452;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#31995;&#25968;&#21450;&#19968;&#20010;&#32422;&#26463;&#32531;&#20914;&#21306;&#26469;&#38480;&#21046;&#31574;&#30053;&#34892;&#20026;&#12290;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ECRL&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#28040;&#34701;&#20998;&#26512;&#34920;&#26126;&#24341;&#20837;&#38543;&#26426;&#25490;&#21517;&#21644;&#32422;&#26463;&#32531;&#20914;&#21306;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary algorithms have been used to evolve a population of actors to generate diverse experiences for training reinforcement learning agents, which helps to tackle the temporal credit assignment problem and improves the exploration efficiency. However, when adapting this approach to address constrained problems, balancing the trade-off between the reward and constraint violation is hard. In this paper, we propose a novel evolutionary constrained reinforcement learning (ECRL) algorithm, which adaptively balances the reward and constraint violation with stochastic ranking, and at the same time, restricts the policy's behaviour by maintaining a set of Lagrange relaxation coefficients with a constraint buffer. Extensive experiments on robotic control benchmarks show that our ECRL achieves outstanding performance compared to state-of-the-art algorithms. Ablation analysis shows the benefits of introducing stochastic ranking and constraint buffer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#32858;&#31867;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.09868</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?. (arXiv:2304.09868v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20294;&#26159;&#30001;&#20110;&#20854;&#35745;&#31639;&#26114;&#36149;&#30340;&#31751;&#20998;&#37197;&#27493;&#39588;&#65292;&#23427;&#38754;&#20020;&#30528;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#21387;&#32553;&#25104;&#23569;&#37327;&#35889;&#34920;&#31034;&#30340;&#32858;&#21512;&#25968;&#25454;&#28857;&#65292;&#28982;&#21518;&#22312;&#21387;&#32553;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#26631;&#20934;&#30340;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#65292;&#26368;&#21518;&#23558;&#21387;&#32553;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#32467;&#26524;&#26144;&#23556;&#22238;&#21407;&#22987;&#25968;&#25454;&#38598;&#20197;&#21457;&#29616;&#31751;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support vector clustering is an important clustering method. However, it suffers from a scalability issue due to its computational expensive cluster assignment step. In this paper we accelertate the support vector clustering via spectrum-preserving data compression. Specifically, we first compress the original data set into a small amount of spectrally representative aggregated data points. Then, we perform standard support vector clustering on the compressed data set. Finally, we map the clustering results of the compressed data set back to discover the clusters in the original data set. Our extensive experimental results on real-world data set demonstrate dramatically speedups over standard support vector clustering without sacrificing clustering quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;Graph Deviation Network (GDN)&#26469;&#25429;&#25417;&#27827;&#27969;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#31354;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22791;&#29992;&#24322;&#24120;&#38408;&#20540;&#26631;&#20934;GDN+&#65292;&#20197;&#23454;&#29616;&#23545;&#27700;&#36136;&#30340;&#20934;&#30830;&#25345;&#32493;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.09367</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27827;&#27969;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network-Based Anomaly Detection for River Network Systems. (arXiv:2304.09367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;Graph Deviation Network (GDN)&#26469;&#25429;&#25417;&#27827;&#27969;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#31354;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22791;&#29992;&#24322;&#24120;&#38408;&#20540;&#26631;&#20934;GDN+&#65292;&#20197;&#23454;&#29616;&#23545;&#27700;&#36136;&#30340;&#20934;&#30830;&#25345;&#32493;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#26159;&#27827;&#27969;&#32593;&#32476;&#30340;&#29983;&#21629;&#32447;&#65292;&#20854;&#36136;&#37327;&#23545;&#32500;&#25252;&#27700;&#29983;&#24577;&#31995;&#32479;&#21644;&#20154;&#31867;&#31038;&#20250;&#37117;&#26377;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#22330;&#20256;&#24863;&#22120;&#25216;&#26415;&#36234;&#26469;&#36234;&#20381;&#36182;&#23454;&#26102;&#30417;&#27979;&#27700;&#36136;&#12290;&#24322;&#24120;&#26816;&#27979;&#26159;&#35782;&#21035;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#38169;&#35823;&#27169;&#24335;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#65292;&#21363;&#20351;&#22312;&#27491;&#24120;&#24773;&#20917;&#19979;&#20063;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27827;&#27969;&#32593;&#32476;&#20256;&#24863;&#22120;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#23545;&#20110;&#31934;&#30830;&#25345;&#32493;&#30417;&#27979;&#27700;&#36136;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#8212;&#8212;&#26368;&#36817;&#25552;&#20986;&#30340;Graph Deviation Network (GDN)&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#39044;&#27979;&#26469;&#25429;&#25417;&#20256;&#24863;&#22120;&#20043;&#38388;&#22797;&#26434;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;&#25105;&#20204;&#26681;&#25454;&#25152;&#23398;&#22270;&#24418;&#25552;&#20986;&#20102;&#27169;&#22411;GDN+&#30340;&#22791;&#29992;&#24322;&#24120;&#38408;&#20540;&#26631;&#20934;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#20223;&#30495;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Water is the lifeblood of river networks, and its quality plays a crucial role in sustaining both aquatic ecosystems and human societies. Real-time monitoring of water quality is increasingly reliant on in-situ sensor technology. Anomaly detection is crucial for identifying erroneous patterns in sensor data, but can be a challenging task due to the complexity and variability of the data, even under normal conditions. This paper presents a solution to the challenging task of anomaly detection for river network sensor data, which is essential for the accurate and continuous monitoring of water quality. We use a graph neural network model, the recently proposed Graph Deviation Network (GDN), which employs graph attention-based forecasting to capture the complex spatio-temporal relationships between sensors. We propose an alternate anomaly threshold criteria for the model, GDN+, based on the learned graph. To evaluate the model's efficacy, we introduce new benchmarking simulation experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#21644;AI&#26041;&#27861;&#39044;&#27979;DED&#25171;&#21360;&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#23454;&#26102;&#39044;&#27979;&#27599;&#20010;&#27779;&#20811;&#22622;&#23572;&#20013;&#30340;&#27668;&#23380;&#23384;&#22312;&#65292;&#26159;&#19968;&#20010;&#37325;&#22823;&#39134;&#36291;&#12290;</title><link>http://arxiv.org/abs/2304.08658</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#22312;DED&#25171;&#21360;SS316L&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
In-situ surface porosity prediction in DED (directed energy deposition) printed SS316L parts using multimodal sensor fusion. (arXiv:2304.08658v1 [physics.app-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#21644;AI&#26041;&#27861;&#39044;&#27979;DED&#25171;&#21360;&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#23454;&#26102;&#39044;&#27979;&#27599;&#20010;&#27779;&#20811;&#22622;&#23572;&#20013;&#30340;&#27668;&#23380;&#23384;&#22312;&#65292;&#26159;&#19968;&#20010;&#37325;&#22823;&#39134;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#22768;&#21457;&#23556;&#65288;AE&#65289;&#31561;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#26102;&#39057;&#27169;&#24335;&#19982;DED&#36807;&#31243;&#20013;&#30340;&#23380;&#38553;&#29575;&#24418;&#25104;&#36827;&#34892;&#39640;&#31354;&#38388;&#65288;0.5mm&#65289;&#21644;&#26102;&#38388;&#65288;&lt;1ms&#65289;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#20013;&#30340;LIME&#65288;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#38750;&#29305;&#23450;&#24615;&#35299;&#37322;&#65289;&#65292;&#23558;AE&#20013;&#30340;&#26576;&#20123;&#39640;&#39057;&#27874;&#24418;&#29305;&#24449;&#24402;&#22240;&#20110;DED&#36807;&#31243;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#23380;&#38553;&#24418;&#25104;&#36884;&#24452;&#65306;&#39134;&#28293;&#20107;&#20214;&#21644;&#20302;&#28909;&#37327;&#36755;&#20837;&#19979;&#30456;&#37051;&#25171;&#21360;&#36712;&#36857;&#30340;&#19981;&#20805;&#20998;&#29076;&#21512;&#12290;&#35813;&#26041;&#27861;&#20026;&#23454;&#26102;&#39044;&#27979;&#27599;&#20010;&#27779;&#20811;&#22622;&#23572;&#65288;0.5mm&#65289;&#20013;&#30340;&#27668;&#23380;&#23384;&#22312;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#26159;&#19982;&#20808;&#21069;&#21162;&#21147;&#30456;&#27604;&#30340;&#19968;&#20010;&#37325;&#22823;&#39134;&#36291;&#12290;&#22312;&#25171;&#21360;&#24182;&#38543;&#21518;&#21152;&#24037;SS316L&#26448;&#26009;&#26679;&#21697;&#26102;&#65292;&#21516;&#27493;&#37319;&#38598;&#20102;&#21253;&#25324;&#21147;&#65292;AE&#65292;&#25391;&#21160;&#21644;&#28201;&#24230;&#22312;&#20869;&#30340;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#29992;&#20110;&#35782;&#21035;&#20004;&#31181;&#23380;&#38553;&#24418;&#25104;&#36884;&#24452;&#30340;AE&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#36827;&#19968;&#27493;&#20998;&#26512;&#36825;&#20123;&#29305;&#24449;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#21644;AI&#26041;&#27861;&#39044;&#27979;DED&#25171;&#21360;&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to relate the time-frequency patterns of acoustic emission (AE) and other multi-modal sensor data collected in a hybrid directed energy deposition (DED) process to the pore formations at high spatial (0.5 mm) and time (&lt; 1ms) resolutions. Adapting an explainable AI method in LIME (Local Interpretable Model-Agnostic Explanations), certain high-frequency waveform signatures of AE are to be attributed to two major pathways for pore formation in a DED process, namely, spatter events and insufficient fusion between adjacent printing tracks from low heat input. This approach opens an exciting possibility to predict, in real-time, the presence of a pore in every voxel (0.5 mm in size) as they are printed, a major leap forward compared to prior efforts. Synchronized multimodal sensor data including force, AE, vibration and temperature were gathered while an SS316L material sample was printed and subsequently machined. A deep convolution neural network classifier was used to ide
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#12289;&#32431;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#22312;$\{0,1\}^d$&#19978;&#20934;&#30830;&#20272;&#35745;&#20108;&#20803;&#31215;&#20998;&#24067;&#30340;&#22343;&#20540;&#65292;&#36798;&#21040;&#20102;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06787</link><description>&lt;p&gt;
&#20108;&#20803;&#31215;&#20998;&#24067;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#32431;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions. (arXiv:2304.06787v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#12289;&#32431;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#22312;$\{0,1\}^d$&#19978;&#20934;&#30830;&#20272;&#35745;&#20108;&#20803;&#31215;&#20998;&#24067;&#30340;&#22343;&#20540;&#65292;&#36798;&#21040;&#20102;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#949;-&#24046;&#20998;&#38544;&#31169;&#12289;&#35745;&#31639;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#24635;&#21464;&#21270;&#36317;&#31163;&#19979;&#20934;&#30830;&#22320;&#20272;&#35745;$\{0,1\}^d$&#19978;&#30340;&#20056;&#31215;&#20998;&#24067;&#30340;&#22343;&#20540;&#65292;&#21516;&#26102;&#22312;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#20869;&#33719;&#24471;&#20102;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#22312;&#26356;&#24369;&#30340;&#38544;&#31169;&#27010;&#24565;&#19979;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35201;&#20040;&#22312;&#25351;&#25968;&#32423;&#36816;&#34892;&#26102;&#38388;&#20869;&#26368;&#20248;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the first $\varepsilon$-differentially private, computationally efficient algorithm that estimates the means of product distributions over $\{0,1\}^d$ accurately in total-variation distance, whilst attaining the optimal sample complexity to within polylogarithmic factors. The prior work had either solved this problem efficiently and optimally under weaker notions of privacy, or had solved it optimally while having exponential running times.
&lt;/p&gt;</description></item><item><title>PIRBN&#26159;&#19968;&#31181;&#23616;&#37096;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#65292;&#36866;&#29992;&#20110;&#27714;&#35299;&#20855;&#26377;&#39640;&#39057;&#29305;&#24449;&#21644;&#19981;&#36866;&#23450;&#35745;&#31639;&#22495;&#30340;PDE&#26041;&#31243;&#65292;&#30456;&#27604;PINN&#26356;&#21152;&#39640;&#25928;&#26377;&#25928;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35757;&#32451;PIRBN&#21487;&#20197;&#25910;&#25947;&#21040;&#39640;&#26031;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.06234</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#24452;&#21521;&#22522;&#32593;&#32476;&#65288;PIRBN&#65289;&#65306;&#29992;&#20110;&#27714;&#35299;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#23616;&#37096;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Physics-informed radial basis network (PIRBN): A local approximation neural network for solving nonlinear PDEs. (arXiv:2304.06234v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06234
&lt;/p&gt;
&lt;p&gt;
PIRBN&#26159;&#19968;&#31181;&#23616;&#37096;&#36924;&#36817;&#31070;&#32463;&#32593;&#32476;&#65292;&#36866;&#29992;&#20110;&#27714;&#35299;&#20855;&#26377;&#39640;&#39057;&#29305;&#24449;&#21644;&#19981;&#36866;&#23450;&#35745;&#31639;&#22495;&#30340;PDE&#26041;&#31243;&#65292;&#30456;&#27604;PINN&#26356;&#21152;&#39640;&#25928;&#26377;&#25928;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35757;&#32451;PIRBN&#21487;&#20197;&#25910;&#25947;&#21040;&#39640;&#26031;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26368;&#36817;&#30340;&#28145;&#20837;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#35757;&#32451;&#21518;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#24448;&#24448;&#26159;&#23616;&#37096;&#36924;&#36817;&#22120;&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#24341;&#21457;&#20102;&#36825;&#31181;&#26032;&#22411;&#30340;&#29289;&#29702;&#20449;&#24687;&#24452;&#21521;&#22522;&#32593;&#32476;&#65288;PIRBN&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#23616;&#37096;&#29305;&#24615;&#12290;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;PIRBN&#21482;&#21253;&#21547;&#19968;&#20010;&#38544;&#34255;&#23618;&#21644;&#19968;&#20010;&#24452;&#21521;&#22522;&#8220;&#28608;&#27963;&#8221;&#20989;&#25968;&#12290;&#22312;&#36866;&#24403;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35757;&#32451;PIRBN&#21487;&#20197;&#25910;&#25947;&#21040;&#39640;&#26031;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#29702;&#35770;&#30740;&#31350;&#20102;PIRBN&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;PIRBN&#30340;&#21021;&#22987;&#21270;&#31574;&#30053;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#22522;&#20110;&#25968;&#20540;&#20363;&#23376;&#65292;PIRBN&#24050;&#34987;&#35777;&#26126;&#27604;PINN&#22312;&#35299;&#20915;&#20855;&#26377;&#39640;&#39057;&#29305;&#24449;&#21644;&#19981;&#36866;&#23450;&#35745;&#31639;&#22495;&#30340;PDE&#26041;&#31243;&#26041;&#38754;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;PINN&#25968;&#23383;&#25216;&#26415;&#65292;&#20363;&#22914;ad...
&lt;/p&gt;
&lt;p&gt;
Our recent intensive study has found that physics-informed neural networks (PINN) tend to be local approximators after training. This observation leads to this novel physics-informed radial basis network (PIRBN), which can maintain the local property throughout the entire training process. Compared to deep neural networks, a PIRBN comprises of only one hidden layer and a radial basis "activation" function. Under appropriate conditions, we demonstrated that the training of PIRBNs using gradient descendent methods can converge to Gaussian processes. Besides, we studied the training dynamics of PIRBN via the neural tangent kernel (NTK) theory. In addition, comprehensive investigations regarding the initialisation strategies of PIRBN were conducted. Based on numerical examples, PIRBN has been demonstrated to be more effective and efficient than PINN in solving PDEs with high-frequency features and ill-posed computational domains. Moreover, the existing PINN numerical techniques, such as ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25968;&#25454;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#22240;&#26524;&#39537;&#21160;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#36807;&#28388;&#25481;&#22240;&#26524;&#34394;&#20551;&#38142;&#25509;&#65292;&#26368;&#32456;&#36755;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#39044;&#27979;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.05294</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#25968;&#25454;&#22240;&#26524;&#25512;&#26029;&#36873;&#25321;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#24378;&#20581;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Selecting Robust Features for Machine Learning Applications using Multidata Causal Discovery. (arXiv:2304.05294v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25968;&#25454;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#22240;&#26524;&#39537;&#21160;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#36807;&#28388;&#25481;&#22240;&#26524;&#34394;&#20551;&#38142;&#25509;&#65292;&#26368;&#32456;&#36755;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#39044;&#27979;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#20581;&#30340;&#29305;&#24449;&#36873;&#25321;&#23545;&#20110;&#21019;&#24314;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#39046;&#22495;&#30693;&#35782;&#26377;&#38480;&#12289;&#28508;&#22312;&#20132;&#20114;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#35774;&#35745;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#36873;&#25321;&#26368;&#20248;&#29305;&#24449;&#38598;&#36890;&#24120;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#25968;&#25454;&#65288;M&#65289;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#21516;&#26102;&#22788;&#29702;&#19968;&#32452;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#22240;&#26524;&#39537;&#21160;&#38598;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;Tigramite Python&#21253;&#20013;&#23454;&#29616;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;PC1&#25110;PCMCI&#12290;&#36825;&#20123;&#31639;&#27861;&#21033;&#29992;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#25512;&#26029;&#22240;&#26524;&#22270;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#23558;&#21097;&#20313;&#22240;&#26524;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;ML&#27169;&#22411;&#65288;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#65292;&#38543;&#26426;&#26862;&#26519;&#65289;&#39044;&#27979;&#30446;&#26631;&#20043;&#21069;&#65292;&#36807;&#28388;&#25481;&#22240;&#26524;&#34394;&#20551;&#38142;&#25509;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#39044;&#27979;&#35199;&#22826;&#24179;&#27915;&#28909;&#24102;&#22320;&#21306;&#30340;&#22320;&#38663;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust feature selection is vital for creating reliable and interpretable Machine Learning (ML) models. When designing statistical prediction models in cases where domain knowledge is limited and underlying interactions are unknown, choosing the optimal set of features is often difficult. To mitigate this issue, we introduce a Multidata (M) causal feature selection approach that simultaneously processes an ensemble of time series datasets and produces a single set of causal drivers. This approach uses the causal discovery algorithms PC1 or PCMCI that are implemented in the Tigramite Python package. These algorithms utilize conditional independence tests to infer parts of the causal graph. Our causal feature selection approach filters out causally-spurious links before passing the remaining causal features as inputs to ML models (Multiple linear regression, Random Forest) that predict the targets. We apply our framework to the statistical intensity prediction of Western Pacific Tropical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#29366;&#24577;&#32858;&#21512;&#26041;&#27861;&#26469;&#38477;&#20302;&#21160;&#24577;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#30340;&#20272;&#35745;&#35745;&#31639;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#39318;&#20808;&#21033;&#29992;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#20272;&#35745;&#20195;&#29702;Q&#20989;&#25968;&#65292;&#28982;&#21518;&#29992;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#37325;&#35201;&#30340;&#29366;&#24577;&#32858;&#21512;&#65292;&#26368;&#32456;&#21033;&#29992;&#23884;&#22871;&#22266;&#23450;&#28857;&#31639;&#27861;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.04916</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#29366;&#24577;&#32858;&#21512;&#26041;&#27861;&#29992;&#20110;&#21160;&#24577;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Data-Driven State Aggregation Approach for Dynamic Discrete Choice Models. (arXiv:2304.04916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#29366;&#24577;&#32858;&#21512;&#26041;&#27861;&#26469;&#38477;&#20302;&#21160;&#24577;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#30340;&#20272;&#35745;&#35745;&#31639;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#39318;&#20808;&#21033;&#29992;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#20272;&#35745;&#20195;&#29702;Q&#20989;&#25968;&#65292;&#28982;&#21518;&#29992;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#37325;&#35201;&#30340;&#29366;&#24577;&#32858;&#21512;&#65292;&#26368;&#32456;&#21033;&#29992;&#23884;&#22871;&#22266;&#23450;&#28857;&#31639;&#27861;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21160;&#24577;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#26159;&#20351;&#29992;&#20195;&#29702;&#34892;&#20026;&#25968;&#25454;&#20272;&#35745;&#20195;&#29702;&#22870;&#21169;&#20989;&#25968;&#65288;&#20063;&#31216;&#20026;&#8220;&#32467;&#26500;&#21442;&#25968;&#8221;&#65289;&#30340;&#21442;&#25968;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#38656;&#35201;&#21160;&#24577;&#35268;&#21010;&#65292;&#36825;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#36873;&#25321;&#21644;&#32858;&#21512;&#29366;&#24577;&#65292;&#38477;&#20302;&#20102;&#20272;&#35745;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28789;&#27963;&#30340;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20272;&#35745;&#20195;&#29702;Q&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#20272;&#35745;&#30340;Q&#20989;&#25968;&#65292;&#20197;&#21450;&#19968;&#20010;&#32858;&#31867;&#31639;&#27861;&#65292;&#36873;&#25321;&#20102;&#19968;&#20123;&#26368;&#20026;&#37325;&#35201;&#30340;&#29366;&#24577;&#65292;&#36825;&#20123;&#29366;&#24577;&#23545;&#20110;&#39537;&#21160;Q&#20989;&#25968;&#30340;&#21464;&#21270;&#26368;&#20026;&#20851;&#38190;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21033;&#29992;&#36825;&#20123;&#34987;&#36873;&#25321;&#30340;&#8220;&#32858;&#21512;&#8221;&#29366;&#24577;&#65292;&#25105;&#20204;&#20351;&#29992;&#24120;&#29992;&#30340;&#23884;&#22871;&#22266;&#23450;&#28857;&#31639;&#27861;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#25152;&#25552;&#20986;&#30340;&#20108;&#38454;&#27573;&#26041;&#27861;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We study dynamic discrete choice models, where a commonly studied problem involves estimating parameters of agent reward functions (also known as "structural" parameters), using agent behavioral data. Maximum likelihood estimation for such models requires dynamic programming, which is limited by the curse of dimensionality. In this work, we present a novel algorithm that provides a data-driven method for selecting and aggregating states, which lowers the computational and sample complexity of estimation. Our method works in two stages. In the first stage, we use a flexible inverse reinforcement learning approach to estimate agent Q-functions. We use these estimated Q-functions, along with a clustering algorithm, to select a subset of states that are the most pivotal for driving changes in Q-functions. In the second stage, with these selected "aggregated" states, we conduct maximum likelihood estimation using a commonly used nested fixed-point algorithm. The proposed two-stage approach 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;SAM&#19982;&#20256;&#32479;&#26041;&#27861;BET&#22312;MRI&#33041;&#25552;&#21462;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;SAM&#22312;&#20449;&#21495;&#19981;&#22343;&#21248;&#12289;&#38750;&#31561;&#21521;&#24615;&#20998;&#36776;&#29575;&#25110;&#30149;&#21464;&#38752;&#36817;&#33041;&#22806;&#21306;&#22495;&#21644;&#33041;&#33180;&#26102;&#25928;&#26524;&#26356;&#20339;&#65292;SAM&#34920;&#29616;&#20986;&#26356;&#20934;&#30830;&#12289;&#26356;&#20581;&#22766;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.04738</link><description>&lt;p&gt;
SAM&#19982;BET&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30913;&#20849;&#25391;&#22270;&#20687;&#33041;&#25552;&#21462;&#21644;&#20998;&#21106;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SAM vs BET: A Comparative Study for Brain Extraction and Segmentation of Magnetic Resonance Images using Deep Learning. (arXiv:2304.04738v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04738
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;SAM&#19982;&#20256;&#32479;&#26041;&#27861;BET&#22312;MRI&#33041;&#25552;&#21462;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;SAM&#22312;&#20449;&#21495;&#19981;&#22343;&#21248;&#12289;&#38750;&#31561;&#21521;&#24615;&#20998;&#36776;&#29575;&#25110;&#30149;&#21464;&#38752;&#36817;&#33041;&#22806;&#21306;&#22495;&#21644;&#33041;&#33180;&#26102;&#25928;&#26524;&#26356;&#20339;&#65292;SAM&#34920;&#29616;&#20986;&#26356;&#20934;&#30830;&#12289;&#26356;&#20581;&#22766;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#25552;&#21462;&#26159;&#31070;&#32463;&#25104;&#20687;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#28041;&#21450;&#20351;&#29992;MRI&#25968;&#25454;&#23558;&#33041;&#32452;&#32455;&#19982;&#38750;&#33041;&#32452;&#32455;&#20998;&#31163;&#12290;FSL&#30340;&#33041;&#25552;&#21462;&#24037;&#20855;&#65288;BET&#65289;&#26159;&#24403;&#21069;&#30340;&#37329;&#26631;&#20934;&#65292;&#20294;&#30001;&#20110;&#22270;&#20687;&#36136;&#37327;&#38382;&#39064;&#23481;&#26131;&#20986;&#38169;&#12290;Meta AI&#30340;Segment Anything Model&#65288;SAM&#65289;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#38646;&#26679;&#26412;&#20998;&#21106;&#28508;&#21147;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;SAM&#21644;BET&#22312;&#19981;&#21516;&#30340;&#33041;&#25195;&#25551;&#20013;&#30340;&#33041;&#25552;&#21462;&#25928;&#26524;&#65292;&#32771;&#34385;&#20102;&#22270;&#20687;&#36136;&#37327;&#12289;MRI&#24207;&#21015;&#21644;&#30149;&#21464;&#20301;&#32622;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#21442;&#25968;&#26041;&#38754;&#65292;SAM&#32988;&#36807;BET&#65292;&#29305;&#21035;&#26159;&#22312;&#20449;&#21495;&#19981;&#22343;&#21248;&#12289;&#38750;&#31561;&#21521;&#24615;&#20307;&#32032;&#20998;&#36776;&#29575;&#25110;&#30149;&#21464;&#38752;&#36817;&#33041;&#22806;&#21306;&#22495;&#21644;&#33041;&#33180;&#30340;&#24773;&#20917;&#19979;&#12290;SAM&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#26126;&#20854;&#22312;&#33041;&#25552;&#21462;&#21644;&#20998;&#21106;&#24212;&#29992;&#20013;&#20855;&#26377;&#26356;&#20934;&#30830;&#12289;&#26356;&#20581;&#22766;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain extraction is a critical preprocessing step in neuroimaging studies, involving the separation of brain tissue from non-brain tissue using MRI data. FSL's Brain Extraction Tool (BET) is the current gold standard but is prone to errors due to image quality issues. The Segment Anything Model (SAM) by Meta AI has shown promising zero-shot segmentation potential. This paper compares SAM with BET for brain extraction on diverse brain scans, considering image quality, MRI sequences, and lesion locations. Results demonstrate that SAM outperforms BET in various evaluation parameters, particularly in cases with signal inhomogeneities, non-isotropic voxel resolutions, or lesions near the brain's outer regions and meninges. SAM's superior performance indicates its potential as a more accurate, robust, and versatile tool for brain extraction and segmentation applications.
&lt;/p&gt;</description></item><item><title>TransPimLib&#25552;&#20379;&#20102;&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#19978;&#39640;&#25928;&#30340;&#36229;&#36234;&#20989;&#25968;&#35745;&#31639;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;PIM&#31995;&#32479;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#25903;&#25345;&#26356;&#24191;&#27867;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.01951</link><description>&lt;p&gt;
TransPimLib&#65306;&#29992;&#20110;&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#19978;&#39640;&#25928;&#30340;&#36229;&#36234;&#20989;&#25968;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems. (arXiv:2304.01951v1 [cs.MS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01951
&lt;/p&gt;
&lt;p&gt;
TransPimLib&#25552;&#20379;&#20102;&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#19978;&#39640;&#25928;&#30340;&#36229;&#36234;&#20989;&#25968;&#35745;&#31639;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;PIM&#31995;&#32479;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#25903;&#25345;&#26356;&#24191;&#27867;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#29305;&#21035;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#22120;&#20869;&#23384;&#31995;&#32479;&#65288;PIM&#65289;&#25215;&#35834;&#20943;&#36731;&#29616;&#20195;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30495;&#23454;PIM&#31995;&#32479;&#26377;&#19968;&#20010;&#20869;&#22312;&#30340;&#21155;&#21183;&#65292;&#21363;&#23427;&#20204;&#30340;&#30828;&#20214;&#27604;&#20256;&#32479;&#30340;&#22788;&#29702;&#22120;&#65288;CPU&#12289;GPU&#65289;&#26356;&#21152;&#21463;&#38480;&#65292;&#22240;&#20026;&#22312;&#20869;&#23384;&#38468;&#36817;&#25110;&#20869;&#37096;&#26500;&#24314;&#22788;&#29702;&#20803;&#20214;&#30340;&#38590;&#24230;&#21644;&#25104;&#26412;&#24456;&#39640;&#12290;&#22240;&#27492;&#65292;&#36890;&#29992;&#30340;PIM&#26550;&#26500;&#25903;&#25345;&#30456;&#24403;&#26377;&#38480;&#30340;&#25351;&#20196;&#38598;&#65292;&#24182;&#19988;&#38590;&#20197;&#25191;&#34892;&#22797;&#26434;&#30340;&#25805;&#20316;&#65292;&#20363;&#22914;&#36229;&#36234;&#20989;&#25968;&#21644;&#20854;&#20182;&#38590;&#20197;&#35745;&#31639;&#30340;&#25805;&#20316;&#65288;&#20363;&#22914;&#24179;&#26041;&#26681;&#65289;&#12290;&#36825;&#20123;&#25805;&#20316;&#23545;&#20110;&#19968;&#20123;&#29616;&#20195;&#24037;&#20316;&#36127;&#36733;&#23588;&#20854;&#37325;&#35201;&#65292;&#20363;&#22914;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#20026;&#20102;&#22312;&#36890;&#29992;&#30340;PIM&#31995;&#32479;&#20013;&#25552;&#20379;&#23545;&#36229;&#36234;&#65288;&#21644;&#20854;&#20182;&#38590;&#20197;&#35745;&#31639;&#65289;&#20989;&#25968;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TransPimLib&#65292;&#36825;&#26159;&#19968;&#20010;&#24211;&#65292;&#25552;&#20379;&#22522;&#20110;CORDIC&#21644;LUT&#30340;&#19977;&#35282;&#20989;&#25968;&#12289;&#21452;&#26354;&#20989;&#25968;&#12289;&#25351;&#25968;&#12289;&#23545;&#25968;&#12289;&#24179;&#26041;&#26681;&#31561;&#38590;&#20197;&#35745;&#31639;&#30340;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Processing-in-memory (PIM) promises to alleviate the data movement bottleneck in modern computing systems. However, current real-world PIM systems have the inherent disadvantage that their hardware is more constrained than in conventional processors (CPU, GPU), due to the difficulty and cost of building processing elements near or inside the memory. As a result, general-purpose PIM architectures support fairly limited instruction sets and struggle to execute complex operations such as transcendental functions and other hard-to-calculate operations (e.g., square root). These operations are particularly important for some modern workloads, e.g., activation functions in machine learning applications.  In order to provide support for transcendental (and other hard-to-calculate) functions in general-purpose PIM systems, we present \emph{TransPimLib}, a library that provides CORDIC-based and LUT-based methods for trigonometric functions, hyperbolic functions, exponentiation, logarithm, squar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31934;&#36873;&#32508;&#36848;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#32463;&#27982;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#20998;&#26512;&#30340;&#25991;&#31456;&#65292;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22788;&#29702;&#38750;&#20256;&#32479;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12289;&#25429;&#25417;&#24378;&#38750;&#32447;&#24615;&#24615;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#20248;&#21183;&#65292;&#25104;&#20026;&#35745;&#37327;&#32463;&#27982;&#23398;&#23478;&#24037;&#20855;&#31665;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2304.00086</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#32463;&#27982;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65306;&#20309;&#26102;&#12289;&#20160;&#20040;&#21644;&#22914;&#20309;&#36816;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Economics Research: When What and How?. (arXiv:2304.00086v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31934;&#36873;&#32508;&#36848;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#32463;&#27982;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#20998;&#26512;&#30340;&#25991;&#31456;&#65292;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22788;&#29702;&#38750;&#20256;&#32479;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12289;&#25429;&#25417;&#24378;&#38750;&#32447;&#24615;&#24615;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#20248;&#21183;&#65292;&#25104;&#20026;&#35745;&#37327;&#32463;&#27982;&#23398;&#23478;&#24037;&#20855;&#31665;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#36827;&#34892;&#32463;&#27982;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#20998;&#26512;&#30340;&#37325;&#35201;&#32463;&#27982;&#26399;&#21002;&#19978;&#21457;&#34920;&#30340;&#25991;&#31456;&#36827;&#34892;&#20102;&#31934;&#36873;&#32508;&#36848;&#12290;&#32508;&#36848;&#22238;&#31572;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#20309;&#26102;&#22312;&#32463;&#27982;&#23398;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#65288;2&#65289;&#24120;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#20160;&#20040;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22914;&#20309;&#23558;&#23427;&#20204;&#29992;&#20110;&#32463;&#27982;&#24212;&#29992;&#12290;&#32508;&#36848;&#24378;&#35843;&#20102;&#26426;&#22120;&#23398;&#20064;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#38750;&#20256;&#32479;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12289;&#25429;&#25417;&#24378;&#38750;&#32447;&#24615;&#24615;&#21644;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#38750;&#20256;&#32479;&#25968;&#25454;&#65292;&#32780;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#20256;&#32479;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#35745;&#37327;&#32463;&#27982;&#23398;&#27169;&#22411;&#22312;&#20998;&#26512;&#20302;&#22797;&#26434;&#24615;&#25968;&#25454;&#26102;&#21487;&#33021;&#36275;&#22815;&#65292;&#20294;&#30001;&#20110;&#24555;&#36895;&#25968;&#23383;&#21270;&#21644;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#29486;&#65292;&#32463;&#27982;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#26426;&#22120;&#23398;&#20064;&#27491;&#25104;&#20026;&#35745;&#37327;&#32463;&#27982;&#23398;&#23478;&#24037;&#20855;&#31665;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article provides a curated review of selected papers published in prominent economics journals that use machine learning (ML) tools for research and policy analysis. The review focuses on three key questions: (1) when ML is used in economics, (2) what ML models are commonly preferred, and (3) how they are used for economic applications. The review highlights that ML is particularly used in processing nontraditional and unstructured data, capturing strong nonlinearity, and improving prediction accuracy. Deep learning models are suitable for nontraditional data, whereas ensemble learning models are preferred for traditional datasets. While traditional econometric models may suffice for analyzing low-complexity data, the increasing complexity of economic data due to rapid digitalization and the growing literature suggest that ML is becoming an essential addition to the econometrician's toolbox.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;Bayesian&#33258;&#30001;&#33021;&#26159;&#26377;&#30028;&#30340;&#65292;&#35828;&#26126;Bayesian&#24191;&#20041;&#35823;&#24046;&#19981;&#20250;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2303.15739</link><description>&lt;p&gt;
&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#36125;&#21494;&#26031;&#33258;&#30001;&#33021;
&lt;/p&gt;
&lt;p&gt;
Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized Cases. (arXiv:2303.15739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#35777;&#26126;&#20102;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;Bayesian&#33258;&#30001;&#33021;&#26159;&#26377;&#30028;&#30340;&#65292;&#35828;&#26126;Bayesian&#24191;&#20041;&#35823;&#24046;&#19981;&#20250;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#21487;&#29992;&#20110;&#20272;&#35745;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#26410;&#30693;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#24615;&#33021;&#23578;&#26410;&#20174;&#29702;&#35770;&#35282;&#24230;&#23436;&#20840;&#28548;&#28165;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#19981;&#21487;&#35782;&#21035;&#30340;&#21644;&#22855;&#24322;&#30340;&#23398;&#20064;&#26426;&#22120;&#12290;&#27492;&#22806;&#65292;ReLU&#20989;&#25968;&#19981;&#21487;&#24494;&#65292;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#20195;&#25968;&#25110;&#35299;&#26512;&#26041;&#27861;&#26080;&#27861;&#24212;&#29992;&#20110;&#23427;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#35777;&#26126;&#20102;Bayesian&#33258;&#30001;&#33021;&#26159;&#26377;&#30028;&#30340;&#65292;&#21363;&#20351;&#23618;&#25968;&#27604;&#20272;&#35745;&#26410;&#30693;&#25968;&#25454;&#29983;&#25104;&#20989;&#25968;&#25152;&#24517;&#38656;&#30340;&#23618;&#25968;&#26356;&#22810;&#12290;&#30001;&#20110;Bayesian&#24191;&#20041;&#35823;&#24046;&#31561;&#20110;&#26679;&#26412;&#22823;&#23567;&#30340;&#33258;&#30001;&#33021;&#22686;&#21152;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#32467;&#26524;&#20063;&#34920;&#26126;&#65292;Bayesian&#24191;&#20041;&#35823;&#24046;&#19981;&#20250;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many research fields in artificial intelligence, it has been shown that deep neural networks are useful to estimate unknown functions on high dimensional input spaces. However, their generalization performance is not yet completely clarified from the theoretical point of view because they are nonidentifiable and singular learning machines. Moreover, a ReLU function is not differentiable, to which algebraic or analytic methods in singular learning theory cannot be applied. In this paper, we study a deep ReLU neural network in overparametrized cases and prove that the Bayesian free energy, which is equal to the minus log marginal likelihoodor the Bayesian stochastic complexity, is bounded even if the number of layers are larger than necessary to estimate an unknown data-generating function. Since the Bayesian generalization error is equal to the increase of the free energy as a function of a sample size, our result also shows that the Bayesian generalization error does not increase ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#65292;&#29992;&#20110;&#21516;&#26102;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#21644;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#25552;&#39640;&#26410;&#30693;&#23545;&#35937;&#22312;&#32972;&#26223;&#20013;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#26368;&#20339;&#26694;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13769</link><description>&lt;p&gt;
&#26410;&#30693;&#21957;&#25506;&#22120;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#65306;&#19981;&#35201;&#23545;&#26410;&#30693;&#23545;&#35937;&#35270;&#32780;&#19981;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects. (arXiv:2303.13769v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#65292;&#29992;&#20110;&#21516;&#26102;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#21644;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#25552;&#39640;&#26410;&#30693;&#23545;&#35937;&#22312;&#32972;&#26223;&#20013;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#26368;&#20339;&#26694;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25552;&#20986;&#30340;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#21644;&#24320;&#25918;&#38598;&#26816;&#27979;&#22312;&#23547;&#25214;&#20174;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#24182;&#23558;&#20854;&#19982;&#24050;&#30693;&#31867;&#21035;&#21306;&#20998;&#24320;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#23545;&#20174;&#24050;&#30693;&#31867;&#21035;&#21521;&#26410;&#30693;&#31867;&#21035;&#30340;&#30693;&#35782;&#20256;&#36882;&#30340;&#30740;&#31350;&#38656;&#35201;&#26356;&#28145;&#20837;&#65292;&#20174;&#32780;&#23548;&#33268;&#25506;&#27979;&#38544;&#34255;&#22312;&#32972;&#26223;&#20013;&#30340;&#26410;&#30693;&#29289;&#20307;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#26469;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#65292;&#20165;&#20351;&#29992;&#24050;&#30693;&#31867;&#21035;&#26679;&#26412;&#36827;&#34892;&#30417;&#30563;&#21644;&#36991;&#20813;&#22312;&#32972;&#26223;&#20013;&#19981;&#36866;&#24403;&#22320;&#21387;&#21046;&#26410;&#30693;&#29289;&#20307;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20174;&#24050;&#30693;&#29289;&#20307;&#23398;&#20064;&#21040;&#30340;&#36825;&#31181;&#32622;&#20449;&#24230;&#20998;&#25968;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#29289;&#20307;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#36827;&#19968;&#27493;&#38480;&#21046;&#32972;&#26223;&#20013;&#38750;&#29289;&#20307;&#26679;&#26412;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#30001;&#20110;&#32570;&#20047;&#23427;&#20204;&#22312;&#35757;&#32451;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#30340;&#26368;&#20339;&#26694;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
The recently proposed open-world object and open-set detection achieve a breakthrough in finding never-seen-before objects and distinguishing them from class-known ones. However, their studies on knowledge transfer from known classes to unknown ones need to be deeper, leading to the scanty capability for detecting unknowns hidden in the background. In this paper, we propose the unknown sniffer (UnSniffer) to find both unknown and known objects. Firstly, the generalized object confidence (GOC) score is introduced, which only uses class-known samples for supervision and avoids improper suppression of unknowns in the background. Significantly, such confidence score learned from class-known objects can be generalized to unknown ones. Additionally, we propose a negative energy suppression loss to further limit the non-object samples in the background. Next, the best box of each unknown is hard to obtain during inference due to lacking their semantic information in training. To solve this is
&lt;/p&gt;</description></item><item><title>MCTS-GEB&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#37325;&#20889;&#31995;&#32479;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26469;&#26500;&#24314;&#26368;&#20248;&#30340;E&#22270;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;E&#22270;&#26500;&#24314;&#20013;&#30340;&#39034;&#24207;&#38382;&#39064;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.04651</link><description>&lt;p&gt;
MCTS-GEB&#65306;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26159;&#19968;&#20010;&#22909;&#30340;E&#22270;&#26500;&#24314;&#22120;
&lt;/p&gt;
&lt;p&gt;
MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder. (arXiv:2303.04651v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04651
&lt;/p&gt;
&lt;p&gt;
MCTS-GEB&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#37325;&#20889;&#31995;&#32479;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26469;&#26500;&#24314;&#26368;&#20248;&#30340;E&#22270;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;E&#22270;&#26500;&#24314;&#20013;&#30340;&#39034;&#24207;&#38382;&#39064;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#20889;&#31995;&#32479;&#24191;&#27867;&#20351;&#29992;&#31561;&#24335;&#39281;&#21644;&#25216;&#26415;&#26469;&#20248;&#21270;&#37325;&#20889;&#39034;&#24207;&#65292;&#20294;&#26159;&#24403;E&#22270;&#27809;&#26377;&#39281;&#21644;&#26102;&#65292;&#26080;&#27861;&#20195;&#34920;&#25152;&#26377;&#21487;&#33021;&#30340;&#37325;&#20889;&#26426;&#20250;&#65292;&#20250;&#37325;&#26032;&#24341;&#20837;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MCTS-GEB&#65292;&#19968;&#20010;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#20110;E&#22270;&#26500;&#24314;&#30340;&#36890;&#29992;&#37325;&#20889;&#31995;&#32479;&#12290;MCTS-GEB&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#39640;&#25928;&#35268;&#21010;&#26368;&#20248;&#30340;E&#22270;&#26500;&#24314;&#65292;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;E&#22270;&#26500;&#24314;&#38454;&#27573;&#30340;&#39034;&#24207;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#21512;&#29702;&#26102;&#38388;&#20869;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#65292;MCTS-GEB&#37117;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rewrite systems [6, 10, 12] have been widely employing equality saturation [9], which is an optimisation methodology that uses a saturated e-graph to represent all possible sequences of rewrite simultaneously, and then extracts the optimal one. As such, optimal results can be achieved by avoiding the phase-ordering problem. However, we observe that when the e-graph is not saturated, it cannot represent all possible rewrite opportunities and therefore the phase-ordering problem is re-introduced during the construction phase of the e-graph. To address this problem, we propose MCTS-GEB, a domain-general rewrite system that applies reinforcement learning (RL) to e-graph construction. At its core, MCTS-GEB uses a Monte Carlo Tree Search (MCTS) [3] to efficiently plan for the optimal e-graph construction, and therefore it can effectively eliminate the phase-ordering problem at the construction phase and achieve better performance within a reasonable time. Evaluation in two different domains 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#30340;&#35835;&#20986;&#20989;&#25968;&#65292;&#33021;&#22815;&#23558;&#33410;&#28857;&#32423;&#34920;&#31034;&#36716;&#25442;&#20026;&#22270;&#32423;&#21521;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#20998;&#23376;&#24212;&#29992;&#20013;&#21462;&#24471;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.02023</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#25104;&#30340;&#35835;&#20986;&#20989;&#25968;&#30340;&#22270;&#32423;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Graph-level representations using ensemble-based readout functions. (arXiv:2303.02023v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;&#30340;&#35835;&#20986;&#20989;&#25968;&#65292;&#33021;&#22815;&#23558;&#33410;&#28857;&#32423;&#34920;&#31034;&#36716;&#25442;&#20026;&#22270;&#32423;&#21521;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#20998;&#23376;&#24212;&#29992;&#20013;&#21462;&#24471;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#12290;&#26368;&#31361;&#20986;&#30340;&#19968;&#31867;&#27169;&#22411;&#8212;&#8212;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#38597;&#30340;&#26041;&#24335;&#26469;&#25552;&#21462;&#34920;&#36798;&#24615;&#30340;&#33410;&#28857;&#32423;&#34920;&#31034;&#21521;&#37327;&#65292;&#36825;&#20123;&#21521;&#37327;&#21487;&#20197;&#29992;&#26469;&#35299;&#20915;&#33410;&#28857;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#23545;&#29992;&#25143;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20219;&#21153;&#38656;&#35201;&#25972;&#20010;&#22270;&#30340;&#34920;&#31034;&#65292;&#20363;&#22914;&#22312;&#20998;&#23376;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#23558;&#33410;&#28857;&#32423;&#34920;&#31034;&#36716;&#25442;&#20026;&#22270;&#32423;&#21521;&#37327;&#65292;&#24517;&#39035;&#24212;&#29992;&#25152;&#35859;&#30340;&#35835;&#20986;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#26377;&#30340;&#35835;&#20986;&#26041;&#27861;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;&#38750;&#21487;&#35757;&#32451;&#26041;&#27861;&#20197;&#21450;&#22797;&#26434;&#30340;&#21442;&#25968;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#38598;&#25104;&#35835;&#20986;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#36825;&#20123;&#20989;&#25968;&#21487;&#20197;&#32467;&#21512;&#34920;&#31034;&#25110;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#26679;&#30340;&#38598;&#25104;&#21487;&#20197;&#27604;&#31616;&#21333;&#21333;&#19968;&#30340;&#35835;&#20986;&#20989;&#25968;&#34920;&#29616;&#24471;&#26356;&#22909;&#65292;&#25110;&#32773;&#19982;&#22797;&#26434;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#34920;&#29616;&#30456;&#20284;&#65292;&#20294;&#24320;&#38144;&#21482;&#26377;&#23427;&#20204;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph machine learning models have been successfully deployed in a variety of application areas. One of the most prominent types of models - Graph Neural Networks (GNNs) - provides an elegant way of extracting expressive node-level representation vectors, which can be used to solve node-related problems, such as classifying users in a social network. However, many tasks require representations at the level of the whole graph, e.g., molecular applications. In order to convert node-level representations into a graph-level vector, a so-called readout function must be applied. In this work, we study existing readout methods, including simple non-trainable ones, as well as complex, parametrized models. We introduce a concept of ensemble-based readout functions that combine either representations or predictions. Our experiments show that such ensembles allow for better performance than simple single readouts or similar performance as the complex, parametrized ones, but at a fraction of the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;&#12290;</title><link>http://arxiv.org/abs/2303.00396</link><description>&lt;p&gt;
&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;
&lt;/p&gt;
&lt;p&gt;
Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning. (arXiv:2303.00396v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20219;&#21153;&#65292;&#23398;&#20064;&#29305;&#23450;&#20110;&#24207;&#25968;&#20998;&#31867;&#30340;&#33391;&#22909;&#32467;&#26500;&#21270;&#29305;&#24449;&#31354;&#38388;&#26377;&#21161;&#20110;&#24688;&#24403;&#22320;&#25429;&#25417;&#31867;&#20043;&#38388;&#30340;&#24207;&#25968;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20026;&#27599;&#20010;&#24207;&#25968;&#31867;&#23398;&#20064;&#19968;&#20010;&#20195;&#29702;&#65292;&#28982;&#21518;&#36890;&#36807;&#38480;&#21046;&#36825;&#20123;&#20195;&#29702;&#26469;&#35843;&#25972;&#31867;&#30340;&#20840;&#23616;&#24067;&#23616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#30828;&#24067;&#23616;&#32422;&#26463;&#21644;&#36719;&#24067;&#23616;&#32422;&#26463;&#12290;&#30828;&#24067;&#23616;&#32422;&#26463;&#36890;&#36807;&#30452;&#25509;&#25511;&#21046;&#20195;&#29702;&#30340;&#29983;&#25104;&#26469;&#23454;&#29616;&#65292;&#20197;&#24378;&#21046;&#23558;&#20854;&#25918;&#32622;&#22312;&#20005;&#26684;&#30340;&#32447;&#24615;&#24067;&#23616;&#25110;&#21322;&#22278;&#24418;&#24067;&#23616;&#65288;&#21363;&#20005;&#26684;&#24207;&#25968;&#24067;&#23616;&#30340;&#20004;&#31181;&#23454;&#20363;&#65289;&#20013;&#12290;&#36719;&#24067;&#23616;&#32422;&#26463;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#26469;&#23454;&#29616;&#65292;&#35813;&#39033;&#24809;&#32602;&#20559;&#31163;&#29702;&#24819;&#24207;&#25968;&#24067;&#23616;&#30340;&#24773;&#20917;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;CPL&#26041;&#27861;&#22312;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For deep ordinal classification, learning a well-structured feature space specific to ordinal classification is helpful to properly capture the ordinal nature among classes. Intuitively, when Euclidean distance metric is used, an ideal ordinal layout in feature space would be that the sample clusters are arranged in class order along a straight line in space. However, enforcing samples to conform to a specific layout in the feature space is a challenging problem. To address this problem, in this paper, we propose a novel Constrained Proxies Learning (CPL) method, which can learn a proxy for each ordinal class and then adjusts the global layout of classes by constraining these proxies. Specifically, we propose two kinds of strategies: hard layout constraint and soft layout constraint. The hard layout constraint is realized by directly controlling the generation of proxies to force them to be placed in a strict linear layout or semicircular layout (i.e., two instantiations of strict ordi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;80&#20010;&#30495;&#23454;&#21644;&#38543;&#26426;&#32593;&#32476;&#65292;&#30740;&#31350;&#23545;&#27604;&#20102;&#24403;&#21069;8&#31181;&#22522;&#20110;&#27169;&#22359;&#21270;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#19968;&#31181;&#31934;&#30830;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#22312;&#31038;&#21306;&#26816;&#27979;&#20013;&#30340;&#20248;&#21270;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;&#21482;&#26377;16.9%&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#36820;&#22238;&#26368;&#20248;&#21010;&#20998;&#25110;&#30456;&#20284;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.14698</link><description>&lt;p&gt;
&#21551;&#21457;&#24335;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#31639;&#27861;&#24456;&#38590;&#36820;&#22238;&#26368;&#20248;&#21010;&#20998;&#25110;&#30456;&#20284;&#32467;&#26524;&#30340;&#31038;&#21306;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Heuristic Modularity Maximization Algorithms for Community Detection Rarely Return an Optimal Partition or Anything Similar. (arXiv:2302.14698v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14698
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;80&#20010;&#30495;&#23454;&#21644;&#38543;&#26426;&#32593;&#32476;&#65292;&#30740;&#31350;&#23545;&#27604;&#20102;&#24403;&#21069;8&#31181;&#22522;&#20110;&#27169;&#22359;&#21270;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#19968;&#31181;&#31934;&#30830;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#22312;&#31038;&#21306;&#26816;&#27979;&#20013;&#30340;&#20248;&#21270;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;&#21482;&#26377;16.9%&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#36820;&#22238;&#26368;&#20248;&#21010;&#20998;&#25110;&#30456;&#20284;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#35745;&#31639;&#31185;&#23398;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#38382;&#39064;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#35774;&#35745;&#31639;&#27861;&#26469;&#22312;&#32593;&#32476;&#33410;&#28857;&#30340;&#19981;&#21516;&#21010;&#20998;&#20043;&#38388;&#26368;&#22823;&#21270;&#27169;&#22359;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#21508;&#31181;&#32972;&#26223;&#30340;80&#20010;&#30495;&#23454;&#21644;&#38543;&#26426;&#32593;&#32476;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#24403;&#21069;&#21551;&#21457;&#24335;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#31639;&#27861;&#22312;&#36820;&#22238;&#26368;&#22823;&#27169;&#22359;&#21270;&#65288;&#26368;&#20248;&#65289;&#21010;&#20998;&#26041;&#38754;&#30340;&#25104;&#21151;&#31243;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#65288;1&#65289;&#31639;&#27861;&#36755;&#20986;&#27169;&#22359;&#21270;&#19982;&#27599;&#20010;&#36755;&#20837;&#22270;&#30340;&#26368;&#22823;&#27169;&#22359;&#21270;&#20043;&#27604;&#65292;&#20197;&#21450;&#65288;2&#65289;&#23427;&#20204;&#30340;&#36755;&#20986;&#21010;&#20998;&#19982;&#35813;&#22270;&#30340;&#20219;&#20309;&#26368;&#20248;&#21010;&#20998;&#20043;&#38388;&#30340;&#26368;&#22823;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#23558;&#20843;&#31181;&#29616;&#26377;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#19982;&#20840;&#23616;&#26368;&#22823;&#21270;&#27169;&#22359;&#21270;&#30340;&#31934;&#30830;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#24179;&#22343;&#22522;&#20110;&#27169;&#22359;&#21270;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#21482;&#20026;&#32771;&#34385;&#30340;80&#20010;&#22270;&#30340;16.9%&#36820;&#22238;&#26368;&#20248;&#21010;&#20998;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#35843;&#25972;&#21518;&#30340;&#20114;&#20449;&#24687;&#30340;&#32467;&#26524;&#26174;&#31034;&#20986;&#23454;&#36136;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection is a fundamental problem in computational sciences with extensive applications in various fields. The most commonly used methods are the algorithms designed to maximize modularity over different partitions of the network nodes. Using 80 real and random networks from a wide range of contexts, we investigate the extent to which current heuristic modularity maximization algorithms succeed in returning maximum-modularity (optimal) partitions. We evaluate (1) the ratio of the algorithms' output modularity to the maximum modularity for each input graph, and (2) the maximum similarity between their output partition and any optimal partition of that graph. We compare eight existing heuristic algorithms against an exact integer programming method that globally maximizes modularity. The average modularity-based heuristic algorithm returns optimal partitions for only 16.9% of the 80 graphs considered. Additionally, results on adjusted mutual information reveal substantial diss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#20061;&#26001;&#29436;&#30340;&#21367;&#31215;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20132;&#38169;&#38544;&#24335;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#21644;&#25968;&#25454;&#25511;&#21046;&#38376;&#26500;&#36896;&#12290;&#22312;&#35760;&#24518;&#20219;&#21153;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#24207;&#21015;&#26377;&#20960;&#21315;&#21040;&#20960;&#21313;&#19975;&#20010;&#26631;&#35760;&#65292;&#20061;&#26001;&#29436;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#31639;&#23376;&#26356;&#20026;&#31934;&#30830;&#30340;&#34920;&#29616;&#65292;&#36798;&#21040;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#30340;&#27700;&#24179;&#65292;&#24182;&#21462;&#24471;&#20102;&#23494;&#38598;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26368;&#26032;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2302.10866</link><description>&lt;p&gt;
&#20960;&#21315;&#21040;&#20960;&#21313;&#19975;&#20010;&#26631;&#35760;&#30340;&#24207;&#21015;&#25512;&#29702;&#21644;&#35760;&#24518;&#20219;&#21153;&#20013;&#30340;&#21367;&#31215;&#35821;&#35328;&#27169;&#22411; - &#20061;&#26001;&#29436;&#31561;&#32423;: &#36808;&#21521;&#26356;&#22823;&#30340;&#21367;&#31215;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hyena Hierarchy: Towards Larger Convolutional Language Models. (arXiv:2302.10866v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#20061;&#26001;&#29436;&#30340;&#21367;&#31215;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20132;&#38169;&#38544;&#24335;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#21644;&#25968;&#25454;&#25511;&#21046;&#38376;&#26500;&#36896;&#12290;&#22312;&#35760;&#24518;&#20219;&#21153;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#24207;&#21015;&#26377;&#20960;&#21315;&#21040;&#20960;&#21313;&#19975;&#20010;&#26631;&#35760;&#65292;&#20061;&#26001;&#29436;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#31639;&#23376;&#26356;&#20026;&#31934;&#30830;&#30340;&#34920;&#29616;&#65292;&#36798;&#21040;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#30340;&#27700;&#24179;&#65292;&#24182;&#21462;&#24471;&#20102;&#23494;&#38598;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26368;&#26032;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22823;&#22411;Transformer&#30340;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#22312;&#20219;&#24847;&#35268;&#27169;&#19978;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;Transformers&#30340;&#26680;&#24515;&#26500;&#20214;&#8212;&#8212;&#27880;&#24847;&#21147;&#25805;&#20316;&#31526;&#8212;&#8212;&#22312;&#38271;&#24230;&#26041;&#38754;&#21576;&#29616;&#20986;&#20108;&#27425;&#30340;&#25104;&#26412;&#65292;&#38480;&#21046;&#20102;&#21487;&#20197;&#35775;&#38382;&#30340;&#19978;&#19979;&#25991;&#37327;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#20302;&#31209;&#21644;&#31232;&#30095;&#36924;&#36817;&#30340;&#20122;&#20108;&#27425;&#26041;&#27861;&#38656;&#35201;&#19982;&#23494;&#38598;&#30340;&#27880;&#24847;&#21147;&#23618;&#32467;&#21512;&#20351;&#29992;&#26469;&#21305;&#37197;Transformers&#65292;&#34920;&#26126;&#23384;&#22312;&#33021;&#21147;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20061;&#26001;&#29436;&#65292;&#19968;&#31181;&#20122;&#20108;&#27425;&#30340;&#27880;&#24847;&#21147;&#26367;&#20195;&#21697;&#65292;&#36890;&#36807;&#20132;&#38169;&#38544;&#24335;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#21644;&#25968;&#25454;&#25511;&#21046;&#38376;&#26500;&#36896;&#12290;&#22312;&#35760;&#24518;&#20219;&#21153;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#24207;&#21015;&#26377;&#20960;&#21315;&#21040;&#20960;&#21313;&#19975;&#20010;&#26631;&#35760;&#65292;&#20061;&#26001;&#29436;&#30340;&#20934;&#30830;&#24230;&#27604;&#20381;&#36182;&#20110;&#29366;&#24577;&#31354;&#38388;&#21644;&#20854;&#20182;&#38544;&#24335;&#21644;&#26174;&#24335;&#26041;&#27861;&#30340;&#31639;&#23376;&#25552;&#39640;&#20102;50&#20197;&#19978;&#65292;&#36798;&#21040;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#30340;&#27700;&#24179;&#12290;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#20061;&#26001;&#29436;&#24182;&#19981;&#38656;&#35201;&#23494;&#38598;&#27880;&#24847;&#21147;&#30340;&#32467;&#26500;&#65292;&#23601;&#24050;&#32463;&#21462;&#24471;&#20102;&#23494;&#38598;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26368;&#26032;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard dat
&lt;/p&gt;</description></item><item><title>Classy Ensemble&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#27599;&#31867;&#20934;&#30830;&#29575;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#32858;&#21512;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#24182;&#25552;&#20986;Classy Cluster Ensemble&#21644;Classy Evolutionary Ensemble&#20004;&#31181;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.10580</link><description>&lt;p&gt;
Classy Ensemble: &#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#20998;&#31867;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Classy Ensemble: A Novel Ensemble Algorithm for Classification. (arXiv:2302.10580v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10580
&lt;/p&gt;
&lt;p&gt;
Classy Ensemble&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#27599;&#31867;&#20934;&#30830;&#29575;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#32858;&#21512;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#24182;&#25552;&#20986;Classy Cluster Ensemble&#21644;Classy Evolutionary Ensemble&#20004;&#31181;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Classy Ensemble&#65292;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#38598;&#25104;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#27599;&#31867;&#20934;&#30830;&#29575;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#32858;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;153&#20010;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;Classy Ensemble&#20248;&#20110;&#20004;&#31181;&#20854;&#20182;&#33879;&#21517;&#30340;&#32858;&#21512;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;&#39034;&#24207;&#30340;&#20462;&#21098;&#21644;&#22522;&#20110;&#32858;&#31867;&#30340;&#20462;&#21098;&#8212;&#8212;&#20197;&#21450;&#26368;&#36817;&#24341;&#20837;&#30340;lexigarden&#38598;&#25104;&#29983;&#25104;&#22120;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#22686;&#24378;&#26041;&#27861;&#65306;1&#65289;Classy Cluster Ensemble&#65292;&#23558;Classy Ensemble&#21644;&#22522;&#20110;&#32858;&#31867;&#30340;&#20462;&#21098;&#30456;&#32467;&#21512;&#65307;2&#65289;&#28145;&#24230;&#23398;&#20064;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;Classy Ensemble&#22312;&#22235;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;Fashion MNIST&#12289;CIFAR10&#12289;CIFAR100&#21644;ImageNet&#19978;&#30340;&#20248;&#36234;&#24615;&#65307;&#20197;&#21450;3&#65289;Classy Evolutionary Ensemble&#65292;&#20854;&#20013;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#26469;&#36873;&#25321;Classy Ensemble&#20174;&#20013;&#36873;&#25321;&#30340;&#27169;&#22411;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Classy Ensemble, a novel ensemble-generation algorithm for classification tasks, which aggregates models through a weighted combination of per-class accuracy. Tested over 153 machine learning datasets we demonstrate that Classy Ensemble outperforms two other well-known aggregation algorithms -order-based pruning and clustering-based pruning -- as well as the recently introduced lexigarden ensemble generator. We then present three enhancements: 1) Classy Cluster Ensemble, which combines Classy Ensemble and cluster-based pruning; 2) Deep Learning experiments, showing the merits of Classy Ensemble over four image datasets: Fashion MNIST, CIFAR10, CIFAR100, and ImageNet; and 3) Classy Evolutionary Ensemble, wherein an evolutionary algorithm is used to select the set of models which Classy Ensemble picks from.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;10&#20010;&#22478;&#24066;&#30340;&#22823;&#35268;&#27169;&#28014;&#21160;&#36710;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;15&#20998;&#38047;&#20998;&#36776;&#29575;&#30340;&#20132;&#36890;&#36895;&#24230;&#20449;&#24687;&#65292;&#28085;&#30422;&#20102;&#20174;&#20027;&#24178;&#36947;&#21040;&#24403;&#22320;&#34903;&#36947;&#30340;&#25152;&#26377;&#34903;&#36947;&#32423;&#21035;&#65292;&#20026;&#22478;&#24066;&#20132;&#36890;&#36816;&#33829;&#21644;&#35268;&#21010;&#25552;&#20379;&#20102;&#37325;&#35201;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2302.08761</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#28014;&#21160;&#36710;&#25968;&#25454;&#30340;10&#20010;&#22478;&#24066;&#37117;&#24066;&#36335;&#27573;&#20132;&#36890;&#36895;&#24230;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Metropolitan Segment Traffic Speeds from Massive Floating Car Data in 10 Cities. (arXiv:2302.08761v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;10&#20010;&#22478;&#24066;&#30340;&#22823;&#35268;&#27169;&#28014;&#21160;&#36710;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;15&#20998;&#38047;&#20998;&#36776;&#29575;&#30340;&#20132;&#36890;&#36895;&#24230;&#20449;&#24687;&#65292;&#28085;&#30422;&#20102;&#20174;&#20027;&#24178;&#36947;&#21040;&#24403;&#22320;&#34903;&#36947;&#30340;&#25152;&#26377;&#34903;&#36947;&#32423;&#21035;&#65292;&#20026;&#22478;&#24066;&#20132;&#36890;&#36816;&#33829;&#21644;&#35268;&#21010;&#25552;&#20379;&#20102;&#37325;&#35201;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20998;&#26512;&#23545;&#22478;&#24066;&#36816;&#33829;&#21644;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36229;&#20986;&#29615;&#36335;&#26816;&#27979;&#22120;&#33539;&#22260;&#30340;&#22478;&#24066;&#20132;&#36890;&#23494;&#38598;&#25968;&#25454;&#20173;&#28982;&#24456;&#23569;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#28014;&#21160;&#36710;&#36742;&#25968;&#25454;&#38598;&#65292;&#21363;&#8220;10&#20010;&#22478;&#24066;&#30340;&#37117;&#24066;&#36335;&#27573;&#20132;&#36890;&#36895;&#24230;&#25968;&#25454;&#38598;&#8221;&#65292;&#21487;&#29992;&#20110;&#20840;&#29699;10&#20010;&#22478;&#24066;&#65292;&#24182;&#20855;&#26377;&#27599;&#20010;&#37117;&#24066;&#21306;&#22495;1500&#22810;&#24179;&#26041;&#20844;&#37324;&#30340;15&#20998;&#38047;&#20998;&#36776;&#29575;&#30340;&#25910;&#38598;&#21608;&#26399;&#65292;&#25910;&#38598;&#26102;&#38388;&#20026;2019-2021&#24180;&#65292;&#35206;&#30422;&#20174;&#20027;&#24178;&#36947;&#21040;&#24403;&#22320;&#34903;&#36947;&#30340;&#25152;&#26377;&#34903;&#36947;&#32423;&#21035;&#30340;&#20132;&#36890;&#20449;&#24687;&#12290;&#35813;&#25968;&#25454;&#38598;&#21033;&#29992;&#24037;&#19994;&#35268;&#27169;&#30340;&#28014;&#21160;&#36710;&#36742;Traffic4cast&#25968;&#25454;&#65292;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#30340;&#26102;&#31354;&#32858;&#21512;&#25552;&#20379;&#20102;&#36895;&#24230;&#21644;&#36710;&#36742;&#35745;&#25968;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#39640;&#25928;&#30340;&#21305;&#37197;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;OpenStreetMap&#36335;&#32593;&#22270;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic analysis is crucial for urban operations and planning, while the availability of dense urban traffic data beyond loop detectors is still scarce. We present a large-scale floating vehicle dataset of per-street segment traffic information, Metropolitan Segment Traffic Speeds from Massive Floating Car Data in 10 Cities (MeTS-10), available for 10 global cities with a 15-minute resolution for collection periods ranging between 108 and 361 days in 2019-2021 and covering more than 1500 square kilometers per metropolitan area. MeTS-10 features traffic speed information at all street levels from main arterials to local streets for Antwerp, Bangkok, Barcelona, Berlin, Chicago, Istanbul, London, Madrid, Melbourne and Moscow. The dataset leverages the industrial-scale floating vehicle Traffic4cast data with speeds and vehicle counts provided in a privacy-preserving spatio-temporal aggregation. We detail the efficient matching approach mapping the data to the OpenStreetMap road graph. We e
&lt;/p&gt;</description></item><item><title>InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08624</link><description>&lt;p&gt;
InstructABSA: &#22522;&#20110;&#25351;&#20196;&#23398;&#20064;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08624
&lt;/p&gt;
&lt;p&gt;
InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;InstructABSA&#65292;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;Aspect Based Sentiment Analysis (ABSA) &#25152;&#26377;&#23376;&#20219;&#21153;&#65288;Aspect Term Extraction (ATE)&#65292;Aspect Term Sentiment Classification (ATSC)&#65292;&#20197;&#21450;Joint Task modeling&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#24341;&#20837;&#20102;&#27491;&#38754;&#12289;&#36127;&#38754;&#12289;&#21644;&#20013;&#24615;&#30340;&#20363;&#23376;&#65292;&#24182;&#20351;&#29992;&#25351;&#20196;&#26469;&#35843;&#25972;&#27599;&#20010;ABSA&#23376;&#20219;&#21153;&#30340;&#27169;&#22411;&#65288;Tk-Instruct&#65289;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#22312;Sem Eval 2014&#12289;2015&#21644;2016&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#19977;&#20010;ABSA&#23376;&#20219;&#21153;&#65288;ATE&#12289;ATSC&#21644;Joint Task&#65289;&#19978;&#65292;InstructABSA&#22312;&#24615;&#33021;&#19978;&#37117;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;SOTA&#65289;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#34920;&#29616;&#36229;&#36807;&#20102;7&#20493;&#22823;&#30340;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;Rest14 ATE&#23376;&#20219;&#21153;&#19978;&#65292;InstructABSA&#36229;&#36807;&#20102;SOTA 7.31%&#30340;&#24471;&#20998;&#65292;Rest15 ATSC&#23376;&#20219;&#21153;&#19978;&#20063;&#26377;&#25552;&#21319;&#65292;&#24182;&#19988;&#22312;Lapt14 Joint Task&#19978;&#30340;&#34920;&#29616;&#25552;&#21319;&#20102;8.63%&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#23545;&#20110;&#25152;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;InstructABSA&#20855;&#26377;&#24378;&#22823;&#30340;&#26032;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present InstructABSA, Aspect Based Sentiment Analysis (ABSA) using the instruction learning paradigm for all ABSA subtasks: Aspect Term Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint Task modeling. Our method introduces positive, negative, and neutral examples to each training sample, and instruction tunes the model (Tk-Instruct) for each ABSA subtask, yielding significant performance improvements. Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on all three ABSA subtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE subtask by 7.31% points, Rest15 ATSC subtask by and on the Lapt14 Joint Task by 8.63% points. Our results also suggest a strong generalization ability to new domains across all three subtasks
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#22797;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26368;&#21518;&#20960;&#23618;&#36827;&#34892;&#27010;&#29575;&#21270;&#22788;&#29702;&#65292;&#37327;&#21270;&#21644;&#32435;&#20837;&#19981;&#30830;&#23450;&#24615;&#24182;&#26377;&#21161;&#20110;&#20915;&#23450;&#35745;&#31639;&#39044;&#31639;&#30340;&#30830;&#23450;&#12290;</title><link>http://arxiv.org/abs/2302.06359</link><description>&lt;p&gt;
&#20462;&#22797;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fixing Overconfidence in Dynamic Neural Networks. (arXiv:2302.06359v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#22797;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26368;&#21518;&#20960;&#23618;&#36827;&#34892;&#27010;&#29575;&#21270;&#22788;&#29702;&#65292;&#37327;&#21270;&#21644;&#32435;&#20837;&#19981;&#30830;&#23450;&#24615;&#24182;&#26377;&#21161;&#20110;&#20915;&#23450;&#35745;&#31639;&#39044;&#31639;&#30340;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26681;&#25454;&#36755;&#20837;&#38590;&#24230;&#21160;&#24577;&#35843;&#25972;&#35745;&#31639;&#20195;&#20215;&#65292;&#25215;&#35834;&#32531;&#35299;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22823;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;uncertainty estimates&#30340;&#36136;&#37327;&#36739;&#24046;&#65292;&#24456;&#38590;&#21306;&#20998;hard&#21644;easy&#30340;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#21518;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#35745;&#31639;&#26377;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#26368;&#21518;&#20960;&#23618;&#36827;&#34892;&#27010;&#29575;&#21270;&#22788;&#29702;&#65292;&#20805;&#20998;&#37327;&#21270;&#21644;&#32435;&#20837;aleatoric&#21644;epistemic uncertainty&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#30830;&#23450;&#35745;&#31639;&#39044;&#31639;&#26102;&#26377;&#21161;&#20110;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;CIFAR-100&#12289;ImageNet&#21644;Caltech-256&#26041;&#38754;&#23637;&#31034;&#20102;&#20934;&#30830;&#24615;&#12289;&#25429;&#33719;&#19981;&#30830;&#23450;&#24615;&#21644;&#26657;&#20934;&#35823;&#24046;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic neural networks are a recent technique that promises a remedy for the increasing size of modern deep learning models by dynamically adapting their computational cost to the difficulty of the inputs. In this way, the model can adjust to a limited computational budget. However, the poor quality of uncertainty estimates in deep learning models makes it difficult to distinguish between hard and easy samples. To address this challenge, we present a computationally efficient approach for post-hoc uncertainty quantification in dynamic neural networks. We show that adequately quantifying and accounting for both aleatoric and epistemic uncertainty through a probabilistic treatment of the last layers improves the predictive performance and aids decision-making when determining the computational budget. In the experiments, we show improvements on CIFAR-100, ImageNet, and Caltech-256 in terms of accuracy, capturing uncertainty, and calibration error.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#20449;&#24687;&#22330;&#29702;&#35770;(IFT)&#21040;&#29289;&#29702;&#20449;&#24687;&#22330;&#29702;&#35770;(PIFT)&#65292;&#23558;&#25551;&#36848;&#22330;&#30340;&#29289;&#29702;&#23450;&#24459;&#30340;&#20449;&#24687;&#32534;&#30721;&#20026;&#20989;&#25968;&#20808;&#39564;&#12290;&#20174;&#36825;&#20010;PIFT&#24471;&#20986;&#30340;&#21518;&#39564;&#19982;&#20219;&#20309;&#25968;&#20540;&#26041;&#26696;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2301.07609</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#30693;&#35782;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#30340;&#20449;&#24687;&#22330;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification. (arXiv:2301.07609v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#20449;&#24687;&#22330;&#29702;&#35770;(IFT)&#21040;&#29289;&#29702;&#20449;&#24687;&#22330;&#29702;&#35770;(PIFT)&#65292;&#23558;&#25551;&#36848;&#22330;&#30340;&#29289;&#29702;&#23450;&#24459;&#30340;&#20449;&#24687;&#32534;&#30721;&#20026;&#20989;&#25968;&#20808;&#39564;&#12290;&#20174;&#36825;&#20010;PIFT&#24471;&#20986;&#30340;&#21518;&#39564;&#19982;&#20219;&#20309;&#25968;&#20540;&#26041;&#26696;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#32467;&#21512;&#29289;&#29702;&#23398;&#30693;&#35782;&#26159;&#24314;&#27169;&#31995;&#32479;&#30340;&#24378;&#26377;&#21147;&#25216;&#26415;&#12290;&#27492;&#31867;&#27169;&#22411;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23558;&#27979;&#37327;&#32467;&#26524;&#19982;&#24050;&#30693;&#29289;&#29702;&#23450;&#24459;&#30456;&#32467;&#21512;&#65292;&#39640;&#25928;&#22320;&#27714;&#35299;&#22522;&#26412;&#22330;&#12290;&#30001;&#20110;&#35768;&#22810;&#31995;&#32479;&#21253;&#21547;&#26410;&#30693;&#20803;&#32032;&#65292;&#22914;&#32570;&#22833;&#21442;&#25968;&#12289;&#22024;&#26434;&#25968;&#25454;&#25110;&#19981;&#23436;&#25972;&#30340;&#29289;&#29702;&#23450;&#24459;&#65292;&#22240;&#27492;&#36825;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;&#22788;&#29702;&#25152;&#26377;&#21464;&#37327;&#30340;&#24120;&#35265;&#25216;&#26415;&#36890;&#24120;&#21462;&#20915;&#20110;&#29992;&#20110;&#36817;&#20284;&#21518;&#39564;&#30340;&#25968;&#20540;&#26041;&#26696;&#65292;&#24182;&#19988;&#24076;&#26395;&#26377;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#12290;&#20449;&#24687;&#22330;&#29702;&#35770;&#65288;IFT&#65289;&#25552;&#20379;&#20102;&#23545;&#19981;&#19968;&#23450;&#26159;&#39640;&#26031;&#22330;&#30340;&#22330;&#36827;&#34892;&#32479;&#35745;&#23398;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25551;&#36848;&#22330;&#30340;&#29289;&#29702;&#23450;&#24459;&#30340;&#20449;&#24687;&#32534;&#30721;&#20026;&#20989;&#25968;&#20808;&#39564;&#26469;&#25193;&#23637;IFT&#21040;&#29289;&#29702;&#20449;&#24687;&#22330;&#29702;&#35770;&#65288;PIFT&#65289;&#12290;&#20174;&#36825;&#20010;PIFT&#24471;&#20986;&#30340;&#21518;&#39564;&#19982;&#20219;&#20309;&#25968;&#20540;&#26041;&#26696;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven approaches coupled with physical knowledge are powerful techniques to model systems. The goal of such models is to efficiently solve for the underlying field by combining measurements with known physical laws. As many systems contain unknown elements, such as missing parameters, noisy data, or incomplete physical laws, this is widely approached as an uncertainty quantification problem. The common techniques to handle all the variables typically depend on the numerical scheme used to approximate the posterior, and it is desirable to have a method which is independent of any such discretization. Information field theory (IFT) provides the tools necessary to perform statistics over fields that are not necessarily Gaussian. We extend IFT to physics-informed IFT (PIFT) by encoding the functional priors with information about the physical laws which describe the field. The posteriors derived from this PIFT remain independent of any numerical scheme and can capture multiple modes,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Nystr&#246;m&#22411;&#26041;&#27861;&#30340;&#39044;&#22788;&#29702;&#22120;&#26500;&#24314;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#25968;&#25454;&#33539;&#22260;&#20869;&#39640;&#25928;&#37325;&#26500;&#26680;&#26426;&#22120;&#21147;&#22330;&#65292;&#24182;&#22312;&#24102;&#26377;&#25968;&#19975;&#20010;&#22521;&#35757;&#28857;&#30340;&#21270;&#23398;&#31995;&#32479;&#20013;&#33719;&#24471;&#20102;&#31283;&#23450;&#21644;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.12737</link><description>&lt;p&gt;
&#29992;&#36229;&#32447;&#24615;&#25910;&#25947;&#37325;&#26500;&#22522;&#20110;&#26680;&#30340;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Kernel-based Machine Learning Force Fields with Super-linear Convergence. (arXiv:2212.12737v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Nystr&#246;m&#22411;&#26041;&#27861;&#30340;&#39044;&#22788;&#29702;&#22120;&#26500;&#24314;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#25968;&#25454;&#33539;&#22260;&#20869;&#39640;&#25928;&#37325;&#26500;&#26680;&#26426;&#22120;&#21147;&#22330;&#65292;&#24182;&#22312;&#24102;&#26377;&#25968;&#19975;&#20010;&#22521;&#35757;&#28857;&#30340;&#21270;&#23398;&#31995;&#32479;&#20013;&#33719;&#24471;&#20102;&#31283;&#23450;&#21644;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#26426;&#22120;&#22312;&#37327;&#23376;&#21270;&#23398;&#39046;&#22495;&#25345;&#32493;&#21462;&#24471;&#36827;&#23637;&#65292;&#23588;&#20854;&#22312;&#21147;&#22330;&#37325;&#26500;&#30340;&#20302;&#25968;&#25454;&#33539;&#22260;&#20869;&#24050;&#34987;&#35777;&#26126;&#25104;&#21151;&#12290;&#36825;&#26159;&#22240;&#20026;&#21487;&#20197;&#23558;&#35768;&#22810;&#38024;&#23545;&#29289;&#29702;&#23545;&#31216;&#24615;&#30340;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;&#21512;&#24182;&#21040;&#26680;&#20989;&#25968;&#20013;&#20197;&#34917;&#20607;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#20294;&#26159;&#65292;&#26680;&#26426;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#20854;&#20108;&#27425;&#20869;&#23384;&#21644;&#19982;&#35757;&#32451;&#28857;&#25968;&#25104;&#31435;&#26041;&#20851;&#31995;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#24050;&#30693;&#36845;&#20195;&#30340;Krylov&#23376;&#31354;&#38388;&#27714;&#35299;&#22120;&#21487;&#20197;&#20811;&#26381;&#36825;&#20123;&#36127;&#25285;&#65292;&#20294;&#23427;&#20204;&#30340;&#25910;&#25947;&#20851;&#38190;&#21462;&#20915;&#20110;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#22120;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#24456;&#38590;&#23454;&#29616;&#12290;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#22120;&#38656;&#35201;&#20197;&#35745;&#31639;&#20415;&#23452;&#21644;&#25968;&#20540;&#40065;&#26834;&#30340;&#26041;&#24335;&#37096;&#20998;&#39044;&#35299;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;Nystr&#246;m&#22411;&#26041;&#27861;&#31867;&#30340;&#24191;&#27867;&#26041;&#27861;&#65292;&#20197;&#22522;&#20110;&#26368;&#21021;&#26680;&#20989;&#25968;&#30340;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20302;&#31209;&#36817;&#20284;&#26500;&#24314;&#39044;&#22788;&#29702;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel machines have sustained continuous progress in the field of quantum chemistry. In particular, they have proven to be successful in the low-data regime of force field reconstruction. This is because many equivariances and invariances due to physical symmetries can be incorporated into the kernel function to compensate for much larger datasets. So far, the scalability of kernel machines has however been hindered by its quadratic memory and cubical runtime complexity in the number of training points. While it is known, that iterative Krylov subspace solvers can overcome these burdens, their convergence crucially relies on effective preconditioners, which are elusive in practice. Effective preconditioners need to partially pre-solve the learning problem in a computationally cheap and numerically robust manner. Here, we consider the broad class of Nystr\"om-type methods to construct preconditioners based on successively more sophisticated low-rank approximations of the original kerne
&lt;/p&gt;</description></item><item><title>MegaCRN&#26159;&#19968;&#20010;&#20351;&#29992;&#26102;&#31354;&#20803;&#22270;&#23398;&#20064;&#26426;&#21046;&#30340;&#20803;&#22270;&#21367;&#31215;&#36882;&#24402;&#32593;&#32476;&#65292;&#23545;&#20110;&#26102;&#31354;&#24314;&#27169;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.05989</link><description>&lt;p&gt;
MegaCRN&#65306;&#29992;&#20110;&#26102;&#31354;&#24314;&#27169;&#30340;&#20803;&#22270;&#21367;&#31215;&#36882;&#24402;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MegaCRN: Meta-Graph Convolutional Recurrent Network for Spatio-Temporal Modeling. (arXiv:2212.05989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05989
&lt;/p&gt;
&lt;p&gt;
MegaCRN&#26159;&#19968;&#20010;&#20351;&#29992;&#26102;&#31354;&#20803;&#22270;&#23398;&#20064;&#26426;&#21046;&#30340;&#20803;&#22270;&#21367;&#31215;&#36882;&#24402;&#32593;&#32476;&#65292;&#23545;&#20110;&#26102;&#31354;&#24314;&#27169;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#24314;&#27169;&#26159;AI&#31038;&#21306;&#20013;&#37325;&#35201;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26102;&#31354;&#20803;&#22270;&#23398;&#20064;&#20316;&#20026;&#22788;&#29702;&#22270;&#27969;&#20013;&#30340;&#24322;&#36136;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#30340;&#26032;&#22411;&#22270;&#32467;&#26500;&#23398;&#20064;&#26426;&#21046;&#12290;&#36890;&#36807;&#23558;&#20803;&#22270;&#23398;&#20064;&#22120;&#23884;&#20837;GCRN&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#24605;&#24819;&#65292;&#24182;&#21019;&#24314;&#20102;Meta-Graph Convolutional Recurrent Network (MegaCRN)&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;METR-LA&#21644;PEMS-BAY&#65289;&#20197;&#21450;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#38750;&#24179;&#31283;&#29616;&#35937;&#30340;&#22823;&#35268;&#27169;&#26102;&#31354;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#37117;&#22823;&#20026;&#20248;&#36234;&#65288;&#35823;&#24046;&#29575;&#36229;&#36807;27&#65285; MAE&#21644;34&#65285; RMSE&#65289;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#23450;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26126;&#30830;&#22320;&#35299;&#24320;&#20855;&#26377;&#19981;&#21516;&#27169;&#24335;&#30340;&#20301;&#32622;&#21644;&#26102;&#38388;&#25554;&#27133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal modeling as a canonical task of multivariate time series forecasting has been a significant research topic in AI community. To address the underlying heterogeneity and non-stationarity implied in the graph streams, in this study, we propose Spatio-Temporal Meta-Graph Learning as a novel Graph Structure Learning mechanism on spatio-temporal data. Specifically, we implement this idea into Meta-Graph Convolutional Recurrent Network (MegaCRN) by plugging the Meta-Graph Learner powered by a Meta-Node Bank into GCRN encoder-decoder. We conduct a comprehensive evaluation on two benchmark datasets (METR-LA and PEMS-BAY) and a large-scale spatio-temporal dataset that contains a variaty of non-stationary phenomena. Our model outperformed the state-of-the-arts to a large degree on all three datasets (over 27% MAE and 34% RMSE). Besides, through a series of qualitative evaluations, we demonstrate that our model can explicitly disentangle locations and time slots with different patt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#35780;&#35770;&#32773;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#21457;&#29616;&#20351;&#29992;&#35889;&#24402;&#19968;&#21270;(SN)&#21487;&#20197;&#20351;&#35780;&#35770;&#23478;&#26356;&#31283;&#20581;&#22320;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#22870;&#21169;&#22330;&#26223;&#19979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35268;&#33539;&#21270;&#35780;&#35770;&#23478;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#24555;&#36895;&#36731;&#26494;&#22320;&#20174;&#37027;&#20123;&#22797;&#26434;&#24773;&#22659;&#20013;&#23398;&#20064;&#65292;&#36825;&#19968;&#28857;&#23545;&#20110;&#23454;&#29616;&#31283;&#23450;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2212.05331</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35889;&#24402;&#19968;&#21270;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Effects of Spectral Normalization in Multi-agent Reinforcement Learning. (arXiv:2212.05331v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#35780;&#35770;&#32773;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#21457;&#29616;&#20351;&#29992;&#35889;&#24402;&#19968;&#21270;(SN)&#21487;&#20197;&#20351;&#35780;&#35770;&#23478;&#26356;&#31283;&#20581;&#22320;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#22870;&#21169;&#22330;&#26223;&#19979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35268;&#33539;&#21270;&#35780;&#35770;&#23478;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#24555;&#36895;&#36731;&#26494;&#22320;&#20174;&#37027;&#20123;&#22797;&#26434;&#24773;&#22659;&#20013;&#23398;&#20064;&#65292;&#36825;&#19968;&#28857;&#23545;&#20110;&#23454;&#29616;&#31283;&#23450;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#31232;&#30095;&#22870;&#21169;&#22330;&#26223;&#19979;&#65292;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#35770;&#32773;&#23545;&#20110;&#22312;&#31574;&#30053;&#19978;&#23454;&#29616;&#28436;&#21592;-&#35780;&#35770;&#32773;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20004;&#20010;&#22240;&#32032;&#65292;&#23398;&#20064;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#35770;&#32773;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;1&#65289;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#32852;&#21512;&#20316;&#29992;&#31354;&#38388;&#21576;&#25351;&#25968;&#22686;&#38271;&#65307;2&#65289;&#36825;&#20010;&#22240;&#32032;&#32467;&#21512;&#22870;&#21169;&#31232;&#30095;&#21644;&#29615;&#22659;&#22122;&#22768;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#26679;&#26412;&#25968;&#25165;&#33021;&#23454;&#29616;&#20934;&#30830;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#29992;&#35889;&#24402;&#19968;&#21270;(SN)&#23545;&#35780;&#35770;&#23478;&#36827;&#34892;&#35268;&#33539;&#21270;&#65292;&#20351;&#23427;&#33021;&#22815;&#22312;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#19978;&#30340;&#31232;&#30095;&#22870;&#21169;&#22330;&#26223;&#20013;&#26356;&#21152;&#31283;&#20581;&#22320;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35268;&#33539;&#21270;&#30340;&#35780;&#35770;&#23478;&#33021;&#22815;&#24555;&#36895;&#22320;&#20174;&#22797;&#26434;&#30340;SMAC&#21644;RWARE&#39046;&#22495;&#30340;&#31232;&#32570;&#22870;&#21169;&#32463;&#21382;&#20013;&#23398;&#20064;&#21040;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#35780;&#35770;&#23478;&#35268;&#33539;&#21270;&#22312;&#31283;&#23450;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A reliable critic is central to on-policy actor-critic learning. But it becomes challenging to learn a reliable critic in a multi-agent sparse reward scenario due to two factors: 1) The joint action space grows exponentially with the number of agents 2) This, combined with the reward sparseness and environment noise, leads to large sample requirements for accurate learning. We show that regularising the critic with spectral normalization (SN) enables it to learn more robustly, even in multi-agent on-policy sparse reward scenarios. Our experiments show that the regularised critic is quickly able to learn from the sparse rewarding experience in the complex SMAC and RWARE domains. These findings highlight the importance of regularisation in the critic for stable learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PowRL&#26694;&#26550;&#26469;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#26377;&#25928;&#24212;&#23545;&#30005;&#21147;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24773;&#20917;&#65292;&#24182;&#20445;&#25345;&#30005;&#32593;&#30340;&#21487;&#38752;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2212.02397</link><description>&lt;p&gt;
PowRL&#65306;&#29992;&#20110;&#31283;&#20581;&#31649;&#29702;&#30005;&#21147;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PowRL: A Reinforcement Learning Framework for Robust Management of Power Networks. (arXiv:2212.02397v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PowRL&#26694;&#26550;&#26469;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#26377;&#25928;&#24212;&#23545;&#30005;&#21147;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24773;&#20917;&#65292;&#24182;&#20445;&#25345;&#30005;&#32593;&#30340;&#21487;&#38752;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#21508;&#22320;&#30340;&#30005;&#21147;&#32593;&#32476;&#36890;&#36807;&#20026;&#22810;&#20010;&#34892;&#19994;&#12289;&#20225;&#19994;&#21644;&#23478;&#24237;&#28040;&#36153;&#32773;&#25552;&#20379;&#19981;&#38388;&#26029;&#12289;&#21487;&#38752;&#21644;&#26080;&#26242;&#24577;&#30005;&#21147;&#21457;&#25381;&#30528;&#37325;&#35201;&#30340;&#31038;&#20250;&#21644;&#32463;&#27982;&#20316;&#29992;&#12290;&#38543;&#30528;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#30005;&#21160;&#36710;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#21457;&#30005;&#21644;&#39640;&#24230;&#21160;&#24577;&#36127;&#36733;&#38656;&#27714;&#30340;&#20986;&#29616;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#30636;&#24577;&#31283;&#23450;&#38382;&#39064;&#31649;&#29702;&#26469;&#30830;&#20445;&#30005;&#21147;&#32593;&#32476;&#30340;&#31283;&#20581;&#36816;&#34892;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#24182;&#23558;&#20572;&#30005;&#20107;&#20214;&#38480;&#21046;&#22312;&#22320;&#26041;&#33539;&#22260;&#20869;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;PowRL&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#20197;&#32531;&#35299;&#24847;&#22806;&#32593;&#32476;&#20107;&#20214;&#30340;&#24433;&#21709;&#65292;&#24182;&#21487;&#38752;&#22320;&#22312;&#32593;&#32476;&#19978;&#38543;&#26102;&#32500;&#25345;&#30005;&#21147;&#12290;PowRL&#21033;&#29992;&#26032;&#39062;&#30340;&#36229;&#36127;&#33655;&#31649;&#29702;&#21551;&#21457;&#24335;&#20197;&#21450;&#22522;&#20110;RL&#25552;&#20379;&#30340;&#26368;&#20248;&#25299;&#25169;&#36873;&#25321;&#20915;&#31574;&#65292;&#20197;&#30830;&#20445;&#30005;&#32593;&#22312;&#21464;&#21270;&#21644;&#19981;&#30830;&#23450;&#26465;&#20214;&#19979;&#23433;&#20840;&#12289;&#21487;&#38752;&#22320;&#36816;&#34892;&#65288;&#26080;&#36229;&#36733;&#32447;&#36335;&#21644;&#26080;&#20572;&#30005;&#20107;&#20214;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Power grids, across the world, play an important societal and economical role by providing uninterrupted, reliable and transient-free power to several industries, businesses and household consumers. With the advent of renewable power resources and EVs resulting into uncertain generation and highly dynamic load demands, it has become ever so important to ensure robust operation of power networks through suitable management of transient stability issues and localize the events of blackouts. In the light of ever increasing stress on the modern grid infrastructure and the grid operators, this paper presents a reinforcement learning (RL) framework, PowRL, to mitigate the effects of unexpected network events, as well as reliably maintain electricity everywhere on the network at all times. The PowRL leverages a novel heuristic for overload management, along with the RL-guided decision making on optimal topology selection to ensure that the grid is operated safely and reliably (with no overloa
&lt;/p&gt;</description></item><item><title>CEC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#31639;&#27861;&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#65292;&#20854;&#22312;&#20960;&#20010;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#27604;&#26368;&#20808;&#36827;&#30340;RL&#21644;&#35760;&#24518;&#22686;&#24378;RL&#31639;&#27861;&#23398;&#20064;&#26356;&#24555;&#65292;&#26159;&#23398;&#20064;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24555;&#36895;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15183</link><description>&lt;p&gt;
&#36830;&#32493;&#24773;&#26223;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Continuous Episodic Control. (arXiv:2211.15183v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15183
&lt;/p&gt;
&lt;p&gt;
CEC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#31639;&#27861;&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#65292;&#20854;&#22312;&#20960;&#20010;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#27604;&#26368;&#20808;&#36827;&#30340;RL&#21644;&#35760;&#24518;&#22686;&#24378;RL&#31639;&#27861;&#23398;&#20064;&#26356;&#24555;&#65292;&#26159;&#23398;&#20064;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24555;&#36895;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#21487;&#20197;&#29992;&#20110;&#24555;&#36895;&#38145;&#23450;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#39640;&#22870;&#21169;&#30340;&#32463;&#39564;&#12290;&#19982;&#21442;&#25968;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#21442;&#25968;&#38656;&#35201;&#32531;&#24930;&#22320;&#21453;&#21521;&#20256;&#36882;&#22870;&#21169;&#20449;&#21495;&#30340;&#26041;&#27861;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#38656;&#35201;&#21457;&#29616;&#19968;&#27425;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#21518;&#23601;&#21487;&#20197;&#21453;&#22797;&#35299;&#20915;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24773;&#26223;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#23384;&#20648;&#22312;&#31163;&#25955;&#34920;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#36804;&#20170;&#21482;&#24212;&#29992;&#20110;&#31163;&#25955;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#36830;&#32493;&#24773;&#26223;&#25511;&#21046;&#65288;CEC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#24773;&#26223;&#35760;&#24518;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36830;&#32493;&#24615;&#34892;&#21160;&#31354;&#38388;&#38382;&#39064;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#12290;&#22312;&#20960;&#20010;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;RL&#21644;&#35760;&#24518;&#22686;&#24378;RL&#31639;&#27861;&#23398;&#20064;&#26356;&#24555;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#38271;&#26399;&#24615;&#33021;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;CEC&#21487;&#20197;&#26159;&#23398;&#20064;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#24555;&#36895;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-parametric episodic memory can be used to quickly latch onto high-rewarded experience in reinforcement learning tasks. In contrast to parametric deep reinforcement learning approaches in which reward signals need to be back-propagated slowly, these methods only need to discover the solution once, and may then repeatedly solve the task. However, episodic control solutions are stored in discrete tables, and this approach has so far only been applied to discrete action space problems. Therefore, this paper introduces Continuous Episodic Control (CEC), a novel non-parametric episodic memory algorithm for sequential decision making in problems with a continuous action space. Results on several sparse-reward continuous control environments show that our proposed method learns faster than state-of-the-art model-free RL and memory-augmented RL algorithms, while maintaining good long-run performance as well. In short, CEC can be a fast approach for learning in continuous control tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; FedFA &#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#24449;&#38170;&#23450;&#26469;&#23545;&#40784;&#29305;&#24449;&#26144;&#23556;&#24182;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#22312;&#24322;&#26500;&#25968;&#25454;&#26102;&#20998;&#31867;&#22120;&#21644;&#29305;&#24449;&#26144;&#23556;&#20043;&#38388;&#30340;&#24694;&#24615;&#24490;&#29615;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.09299</link><description>&lt;p&gt;
FedFA: &#38024;&#23545;&#24322;&#26500;&#25968;&#25454;&#30340;&#29305;&#24449;&#38170;&#23450;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedFA: Federated Learning with Feature Anchors to Align Features and Classifiers for Heterogeneous Data. (arXiv:2211.09299v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; FedFA &#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#24449;&#38170;&#23450;&#26469;&#23545;&#40784;&#29305;&#24449;&#26144;&#23556;&#24182;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#22312;&#24322;&#26500;&#25968;&#25454;&#26102;&#20998;&#31867;&#22120;&#21644;&#29305;&#24449;&#26144;&#23556;&#20043;&#38388;&#30340;&#24694;&#24615;&#24490;&#29615;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#20132;&#25442;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#23458;&#25143;&#31471;&#23384;&#22312;&#24322;&#26500;&#25968;&#25454;&#26102;&#65292;&#23427;&#20250;&#36973;&#21463;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#24120;&#35265;&#30340;&#26412;&#22320;&#35757;&#32451;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#35774;&#35745;&#29305;&#23450;&#30340;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#26469;&#35268;&#33539;&#26435;&#37325;&#24046;&#24322;&#25110;&#29305;&#24449;&#19981;&#19968;&#33268;&#24615;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#20998;&#31867;&#22120;&#21644;&#29305;&#24449;&#26144;&#23556;&#19981;&#19968;&#33268;&#20043;&#38388;&#30340;&#24694;&#24615;&#24490;&#29615;&#65292;&#23548;&#33268;&#23458;&#25143;&#31471;&#27169;&#22411;&#22312;&#29305;&#24449;&#31354;&#38388;&#21644;&#20998;&#31867;&#22120;&#24046;&#24322;&#30340;&#19981;&#19968;&#33268;&#29305;&#24449;&#31354;&#38388;&#20013;&#26356;&#26032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026; FedFA &#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#22312;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#29305;&#24449;&#38170;&#23450;&#26469;&#23545;&#40784;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#29305;&#24449;&#26144;&#23556;&#24182;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#20351;&#23458;&#25143;&#31471;&#27169;&#22411;&#22312;&#20849;&#20139;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;&#19968;&#33268;&#30340;&#20998;&#31867;&#22120;&#19979;&#26356;&#26032;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#24322;&#26500;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#20462;&#25913;&#21518;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning allows multiple clients to collaboratively train a model without exchanging their data, thus preserving data privacy. Unfortunately, it suffers significant performance degradation under heterogeneous data at clients. Common solutions in local training involve designing a specific auxiliary loss to regularize weight divergence or feature inconsistency. However, we discover that these approaches fall short of the expected performance because they ignore the existence of a vicious cycle between classifier divergence and feature mapping inconsistency across clients, such that client models are updated in inconsistent feature space with diverged classifiers. We then propose a simple yet effective framework named Federated learning with Feature Anchors (FedFA) to align the feature mappings and calibrate classifier across clients during local training, which allows client models updating in a shared feature space with consistent classifiers. We demonstrate that this modific
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#39044;&#27979;&#20989;&#25968;&#19968;&#36215;&#20351;&#29992;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#39044;&#27979;&#20989;&#25968;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#27169;&#22411;&#27531;&#24046;&#12290;&#24341;&#20837;&#20102;Cohort Shapley&#30340;&#31215;&#20998;&#26799;&#24230;&#29256;&#26412;&#65288;IGCS&#65289;&#65292;&#20351;&#24471;&#22312;&#20108;&#20803;&#39044;&#27979;&#22120;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#20351;&#29992;IG&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.08414</link><description>&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#30340;&#26080;&#27169;&#22411;&#21464;&#37327;&#37325;&#35201;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Model free variable importance for high dimensional data. (arXiv:2211.08414v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08414
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#39044;&#27979;&#20989;&#25968;&#19968;&#36215;&#20351;&#29992;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#39044;&#27979;&#20989;&#25968;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#27169;&#22411;&#27531;&#24046;&#12290;&#24341;&#20837;&#20102;Cohort Shapley&#30340;&#31215;&#20998;&#26799;&#24230;&#29256;&#26412;&#65288;IGCS&#65289;&#65292;&#20351;&#24471;&#22312;&#20108;&#20803;&#39044;&#27979;&#22120;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#20351;&#29992;IG&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#26041;&#27861;&#21487;&#19982;&#20219;&#24847;&#39044;&#27979;&#20989;&#25968;&#19968;&#36215;&#20351;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#39044;&#27979;&#20989;&#25968;&#12290;&#36825;&#22312;&#39044;&#27979;&#20989;&#25968;&#26159;&#19987;&#26377;&#30340;&#19988;&#19981;&#21487;&#29992;&#25110;&#26497;&#20854;&#26114;&#36149;&#26102;&#24456;&#26377;&#29992;&#12290;&#24403;&#23545;&#27169;&#22411;&#30340;&#27531;&#24046;&#36827;&#34892;&#30740;&#31350;&#26102;&#20063;&#24456;&#26377;&#29992;&#12290;Cohort Shapley&#65288;CS&#65289;&#26041;&#27861;&#26159;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#20294;&#22312;&#36755;&#20837;&#31354;&#38388;&#30340;&#32500;&#25968;&#19978;&#20855;&#26377;&#25351;&#25968;&#25104;&#26412;&#12290;Frye&#31561;&#20154;&#65288;2020&#65289;&#30340;&#30417;&#30563;&#27969;&#24418;&#19978;Shapley&#26041;&#27861;&#20063;&#26159;&#26080;&#27169;&#22411;&#30340;&#65292;&#20294;&#35201;&#27714;&#36755;&#20837;&#31532;&#20108;&#20010;&#40657;&#21283;&#23376;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#24517;&#39035;&#20026;Shapley&#20540;&#38382;&#39064;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Cohort Shapley&#30340;&#31215;&#20998;&#26799;&#24230;&#65288;IG&#65289;&#29256;&#26412;&#65292;&#31216;&#20026;IGCS&#65292;&#25104;&#26412;&#20026;$\mathcal{O}(nd)$&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#32477;&#22823;&#22810;&#25968;&#30456;&#20851;&#21333;&#20803;&#30340;&#31435;&#26041;&#20307;&#19978;&#65292;IGCS&#20540;&#20989;&#25968;&#25509;&#36817;&#22810;&#32447;&#24615;&#20989;&#25968;&#65292;&#20854;&#20013;IGCS&#21305;&#37197;CS&#12290;IGCS&#30340;&#21478;&#19968;&#20010;&#22909;&#22788;&#26159;&#23427;&#20801;&#35768;&#20351;&#29992;&#20108;&#20803;&#39044;&#27979;&#22120;&#36827;&#34892;IG&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20123;&#38754;&#31215;...
&lt;/p&gt;
&lt;p&gt;
A model-agnostic variable importance method can be used with arbitrary prediction functions. Here we present some model-free methods that do not require access to the prediction function. This is useful when that function is proprietary and not available, or just extremely expensive. It is also useful when studying residuals from a model. The cohort Shapley (CS) method is model-free but has exponential cost in the dimension of the input space. A supervised on-manifold Shapley method from Frye et al. (2020) is also model free but requires as input a second black box model that has to be trained for the Shapley value problem. We introduce an integrated gradient (IG) version of cohort Shapley, called IGCS, with cost $\mathcal{O}(nd)$. We show that over the vast majority of the relevant unit cube that the IGCS value function is close to a multilinear function for which IGCS matches CS. Another benefit of IGCS is that is allows IG methods to be used with binary predictors. We use some area 
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#25104;&#20026;&#28909;&#38376;&#26041;&#27861;&#65292;&#20294;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20250;&#38754;&#20020;&#36866;&#24212;&#31354;&#38388;&#20013;&#30340;&#28418;&#31227;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;"&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#12290;</title><link>http://arxiv.org/abs/2211.02658</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;&#22788;&#29702;&#23398;&#20064;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation. (arXiv:2211.02658v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02658
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#36866;&#24212;&#31995;&#32479;&#20013;&#25104;&#20026;&#28909;&#38376;&#26041;&#27861;&#65292;&#20294;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20250;&#38754;&#20020;&#36866;&#24212;&#31354;&#38388;&#20013;&#30340;&#28418;&#31227;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;"&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064; (ML) &#24050;&#25104;&#20026;&#25903;&#25345;&#33258;&#36866;&#24212;&#30340;&#28909;&#38376;&#26041;&#27861;&#12290;ML &#24050;&#34987;&#29992;&#26469;&#22788;&#29702;&#33258;&#36866;&#24212;&#20013;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#20363;&#22914;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#32500;&#25252;&#26368;&#26032;&#30340;&#36816;&#34892;&#26102;&#27169;&#22411;&#21644;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992; ML &#23384;&#22312;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#38754;&#21521;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31995;&#32479;&#30340;&#19968;&#20010;&#29305;&#21035;&#37325;&#35201;&#30340;&#25361;&#25112;&#65306;&#36866;&#24212;&#31354;&#38388;&#20013;&#30340;&#28418;&#31227;&#12290;&#36890;&#36807;&#36866;&#24212;&#31354;&#38388;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#33258;&#36866;&#24212;&#31995;&#32479;&#22312;&#26576;&#19968;&#29305;&#23450;&#26102;&#38388;&#21487;&#20197;&#36873;&#25321;&#30340;&#36866;&#24212;&#36873;&#39033;&#30340;&#38598;&#21512;&#65292;&#20197;&#26681;&#25454;&#36866;&#24212;&#36873;&#39033;&#30340;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#36866;&#24212;&#12290;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#28304;&#20110;&#24433;&#21709;&#36866;&#24212;&#36873;&#39033;&#36136;&#37327;&#23646;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#28418;&#31227;&#21487;&#33021;&#24847;&#21619;&#30528;&#26368;&#32456;&#27809;&#26377;&#36866;&#24212;&#36873;&#39033;&#33021;&#22815;&#28385;&#36275;&#26368;&#21021;&#30340;&#36866;&#24212;&#30446;&#26631;&#65292;&#20174;&#32780;&#38477;&#20302;&#31995;&#32479;&#30340;&#36136;&#37327;&#65292;&#25110;&#32773;&#21487;&#33021;&#20986;&#29616;&#20801;&#35768;&#22686;&#24378;&#36866;&#24212;&#30446;&#26631;&#30340;&#36866;&#24212;&#36873;&#39033;&#12290;&#22312; ML &#20013;&#65292;&#36825;&#31181;&#28418;&#31227;&#36890;&#24120;&#34987;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#25110;&#23454;&#20363;&#28418;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;&#8221;&#30340;&#26032;&#26041;&#27861;&#12290;&#29983;&#21629;&#21608;&#26399;&#33258;&#36866;&#24212;&#23545; ML powered self-adaptation &#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#36866;&#24212;&#31354;&#38388;&#30340;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, machine learning (ML) has become a popular approach to support self-adaptation. ML has been used to deal with several problems in self-adaptation, such as maintaining an up-to-date runtime model under uncertainty and scalable decision-making. Yet, exploiting ML comes with inherent challenges. In this paper, we focus on a particularly important challenge for learning-based self-adaptive systems: drift in adaptation spaces. With adaptation space we refer to the set of adaptation options a self-adaptive system can select from at a given time to adapt based on the estimated quality properties of the adaptation options. Drift of adaptation spaces originates from uncertainties, affecting the quality properties of the adaptation options. Such drift may imply that eventually no adaptation option can satisfy the initial set of the adaptation goals, deteriorating the quality of the system, or adaptation options may emerge that allow enhancing the adaptation goals. In ML, such shift cor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26032;&#39062;&#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#32954;&#25513;&#27169;&#20449;&#24687;&#26469;&#35782;&#21035;&#26377;&#25928;&#21306;&#22495;&#65292;&#20197;&#23398;&#20064;&#26356;&#26377;&#29992;&#30340;COVID-19&#26816;&#27979;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2211.00313</link><description>&lt;p&gt;
RGMIM: &#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#29992;&#20110;COVID-19&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
RGMIM: Region-Guided Masked Image Modeling for COVID-19 Detection. (arXiv:2211.00313v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26032;&#39062;&#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#32954;&#25513;&#27169;&#20449;&#24687;&#26469;&#35782;&#21035;&#26377;&#25928;&#21306;&#22495;&#65292;&#20197;&#23398;&#20064;&#26356;&#26377;&#29992;&#30340;COVID-19&#26816;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#27491;&#22312;&#24555;&#36895;&#25512;&#36827;&#21307;&#23398;&#39046;&#22495;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#12290;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#25513;&#30422;&#20102;&#19968;&#32452;&#36755;&#20837;&#20687;&#32032;&#24182;&#35797;&#22270;&#39044;&#27979;&#36974;&#30422;&#30340;&#20687;&#32032;&#12290;&#20256;&#32479;&#30340;MIM&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#38543;&#26426;&#25513;&#33180;&#31574;&#30053;&#12290;&#19982;&#26222;&#36890;&#22270;&#20687;&#30456;&#27604;&#65292;&#21307;&#23398;&#22270;&#20687;&#24448;&#24448;&#20855;&#26377;&#29992;&#20110;&#30142;&#30149;&#26816;&#27979;&#30340;&#23567;&#21306;&#22495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#19987;&#27880;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#33258;&#21160;COVID-19&#35782;&#21035;&#26041;&#38754;&#36827;&#34892;&#35780;&#20272;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65288;RGMIM&#65289;&#29992;&#20110;COVID-19&#26816;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25513;&#33180;&#31574;&#30053;&#65292;&#21033;&#29992;&#32954;&#25513;&#27169;&#20449;&#24687;&#26469;&#35782;&#21035;&#26377;&#25928;&#21306;&#22495;&#65292;&#20197;&#23398;&#20064;&#26356;&#26377;&#29992;&#30340;COVID-19&#26816;&#27979;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20116;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65288;MAE&#65292;SKD&#65292;Cross&#65292;BYOL&#21644;SimSiam&#65289;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Self-supervised learning is rapidly advancing computer-aided diagnosis in the medical field. Masked image modeling (MIM) is one of the self-supervised learning methods that masks a subset of input pixels and attempts to predict the masked pixels. Traditional MIM methods often employ a random masking strategy. In comparison to ordinary images, medical images often have a small region of interest for disease detection. Consequently, we focus on fixing the problem in this work, which is evaluated by automatic COVID-19 identification. Methods: In this study, we propose a novel region-guided masked image modeling method (RGMIM) for COVID-19 detection in this paper. In our method, we devise a new masking strategy that employed lung mask information to identify valid regions to learn more useful information for COVID-19 detection. The proposed method was contrasted with five self-supervised learning techniques (MAE, SKD, Cross, BYOL, and, SimSiam). We present a quantitative evaluatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;FedIntR&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#38598;&#25104;&#21040;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25552;&#20379;&#26356;&#32454;&#31890;&#24230;&#30340;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#38480;&#21046;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#20559;&#31163;&#20840;&#23616;&#27169;&#22411;&#30340;&#31243;&#24230;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.15827</link><description>&lt;p&gt;
&#24102;&#26377;&#20013;&#38388;&#34920;&#31034;&#27491;&#21017;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Intermediate Representation Regularization. (arXiv:2210.15827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;FedIntR&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#38598;&#25104;&#21040;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25552;&#20379;&#26356;&#32454;&#31890;&#24230;&#30340;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#38480;&#21046;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#20559;&#31163;&#20840;&#23616;&#27169;&#22411;&#30340;&#31243;&#24230;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#28041;&#21450;&#25968;&#25454;&#25910;&#38598;&#30340;&#38598;&#20013;&#24335;&#27169;&#22411;&#35757;&#32451;&#30456;&#21453;&#65292;&#32852;&#37030;&#23398;&#20064;&#20351;&#36828;&#31243;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#19981;&#26292;&#38706;&#20854;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#26500;&#25968;&#25454;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#24456;&#22810;&#26102;&#20505;&#65292;&#20026;&#20102;&#20445;&#25345;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#38480;&#21046;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#20559;&#31163;&#20840;&#23616;&#27169;&#22411;&#30340;&#31243;&#24230;&#26159;&#19968;&#31181;&#26377;&#25928;&#31574;&#30053;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#36807;&#27491;&#21017;&#21270;&#20840;&#23616;&#21644;&#26412;&#22320;&#23398;&#20064;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#36317;&#31163;&#20197;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#21482;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#26089;&#26399;&#23618;&#25110;&#36755;&#20986;&#23618;&#21069;&#38754;&#30340;&#23618;&#30340;&#34920;&#31034;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;FedIntR&#65292;&#23427;&#36890;&#36807;&#23558;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#38598;&#25104;&#21040;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#20102;&#26356;&#32454;&#31890;&#24230;&#30340;&#27491;&#21017;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FedIntR&#35745;&#31639;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#40723;&#21169;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#20840;&#23616;&#27169;&#22411;&#20043;&#38388;&#30340;&#25509;&#36817;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to centralized model training that involves data collection, federated learning (FL) enables remote clients to collaboratively train a model without exposing their private data. However, model performance usually degrades in FL due to the heterogeneous data generated by clients of diverse characteristics. One promising strategy to maintain good performance is by limiting the local training from drifting far away from the global model. Previous studies accomplish this by regularizing the distance between the representations learned by the local and global models. However, they only consider representations from the early layers of a model or the layer preceding the output layer. In this study, we introduce FedIntR, which provides a more fine-grained regularization by integrating the representations of intermediate layers into the local training process. Specifically, FedIntR computes a regularization term that encourages the closeness between the intermediate layer represent
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20998;&#26512;&#22797;&#26434;&#33258;&#28982;&#36807;&#31243;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#25918;&#26494;&#20102;&#20256;&#32479;&#31616;&#21270;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#21644;&#28789;&#27963;&#30340;&#20989;&#25968;&#36924;&#36817;&#33021;&#21147;&#65292;&#33021;&#22815;&#24212;&#29992;&#20110;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#25913;&#36827;&#25968;&#25454;&#35299;&#37322;&#21147;&#21644;&#25506;&#32034;&#24615;&#20998;&#26512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.12128</link><description>&lt;p&gt;
&#19968;&#31181;&#20998;&#26512;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach to Analyzing Continuous-Time Systems. (arXiv:2209.12128v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12128
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20998;&#26512;&#22797;&#26434;&#33258;&#28982;&#36807;&#31243;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#25918;&#26494;&#20102;&#20256;&#32479;&#31616;&#21270;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#21644;&#28789;&#27963;&#30340;&#20989;&#25968;&#36924;&#36817;&#33021;&#21147;&#65292;&#33021;&#22815;&#24212;&#29992;&#20110;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#25913;&#36827;&#25968;&#25454;&#35299;&#37322;&#21147;&#21644;&#25506;&#32034;&#24615;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#23478;&#36890;&#24120;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#30740;&#31350;&#22797;&#26434;&#30340;&#33258;&#28982;&#36807;&#31243;&#65292;&#20294;&#22238;&#24402;&#20998;&#26512;&#24120;&#24120;&#20551;&#35774;&#36807;&#20110;&#31616;&#21333;&#21270;&#30340;&#21160;&#21147;&#23398;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#22312;&#22797;&#26434;&#36807;&#31243;&#27169;&#22411;&#30340;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25552;&#39640;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#36890;&#24120;&#19981;&#29992;&#20110;&#31185;&#23398;&#20998;&#26512;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#22797;&#26434;&#30340;&#36807;&#31243;&#65292;&#25552;&#20379;&#28789;&#27963;&#30340;&#20989;&#25968;&#36924;&#36817;&#24182;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25918;&#26494;&#20102;&#20256;&#32479;&#31616;&#21270;&#20551;&#35774;&#65288;&#22914;&#32447;&#24615;&#12289;&#24179;&#31283;&#21644;&#21516;&#26041;&#24046;&#24615;&#65289;&#65292;&#36825;&#20123;&#20551;&#35774;&#23545;&#35768;&#22810;&#33258;&#28982;&#31995;&#32479;&#32780;&#35328;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#25968;&#25454;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#36827;&#34892;&#20102;&#27169;&#22411;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#22797;&#26434;&#36830;&#32493;&#21160;&#21147;&#23398;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#34892;&#20026;&#21644;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#25511;&#21046;&#22810;&#31181;&#28151;&#26434;&#23454;&#39564;&#35774;&#32622;&#30340;&#28151;&#26434;&#22240;&#32032;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#29616;&#25506;&#32034;&#24615;&#20998;&#26512;&#20013;&#30340;&#26032;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientists often use observational time series data to study complex natural processes, but regression analyses often assume simplistic dynamics. Recent advances in deep learning have yielded startling improvements to the performance of models of complex processes, but deep learning is generally not used for scientific analysis. Here we show that deep learning can be used to analyze complex processes, providing flexible function approximation while preserving interpretability. Our approach relaxes standard simplifying assumptions (e.g., linearity, stationarity, and homoscedasticity) that are implausible for many natural systems and may critically affect the interpretation of data. We evaluate our model on incremental human language processing, a domain with complex continuous dynamics. We demonstrate substantial improvements on behavioral and neuroimaging data, and we show that our model enables discovery of novel patterns in exploratory analyses, controls for diverse confounds in conf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35782;&#21035;&#21442;&#25968;&#27169;&#22411;&#30340;&#27010;&#29575;&#26080;&#30417;&#30563;&#23398;&#20064;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#23398;&#20064;&#35782;&#21035;&#27169;&#22411;&#65292;&#25429;&#25417;&#35266;&#27979;&#20043;&#38388;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#65292;&#20026;&#22270;&#20687;&#20998;&#31867;&#21644;&#28508;&#22312;&#20998;&#37197;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2209.05661</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#20013;&#30340;&#35782;&#21035;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unsupervised representation learning with recognition-parametrised probabilistic models. (arXiv:2209.05661v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35782;&#21035;&#21442;&#25968;&#27169;&#22411;&#30340;&#27010;&#29575;&#26080;&#30417;&#30563;&#23398;&#20064;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#23398;&#20064;&#35782;&#21035;&#27169;&#22411;&#65292;&#25429;&#25417;&#35266;&#27979;&#20043;&#38388;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#65292;&#20026;&#22270;&#20687;&#20998;&#31867;&#21644;&#28508;&#22312;&#20998;&#37197;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35782;&#21035;&#21442;&#25968;&#27169;&#22411;&#65288;RPM&#65289;&#30340;&#27010;&#29575;&#26080;&#30417;&#30563;&#23398;&#20064;&#26032;&#26041;&#27861;&#65306;&#20316;&#20026;&#20851;&#20110;&#35266;&#23519;&#21464;&#37327;&#21644;&#28508;&#22312;&#21464;&#37327;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#24402;&#19968;&#21270;&#21322;&#21442;&#25968;&#21270;&#20551;&#35774;&#31867;&#12290;&#22312;&#35266;&#23519;&#20540;&#22312;&#32473;&#23450;&#28508;&#22312;&#21464;&#37327;&#30340;&#26465;&#20214;&#19979;&#26159;&#26465;&#20214;&#29420;&#31435;&#30340;&#20851;&#38190;&#20551;&#35774;&#19979;&#65292;RPM&#23558;&#21442;&#25968;&#20808;&#39564;&#21644;&#35266;&#27979;&#26465;&#20214;&#19979;&#30340;&#28508;&#22312;&#20998;&#24067;&#19982;&#38750;&#21442;&#25968;&#35266;&#27979;&#36793;&#32536;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24471;&#21040;&#28789;&#27963;&#30340;&#23398;&#20064;&#35782;&#21035;&#27169;&#22411;&#65292;&#25429;&#25417;&#20102;&#35266;&#27979;&#20043;&#38388;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#26174;&#24335;&#30340;&#21442;&#25968;&#29983;&#25104;&#27169;&#22411;&#12290;&#23545;&#20110;&#31163;&#25955;&#28508;&#21464;&#37327;&#65292;RPM&#20801;&#35768;&#36827;&#34892;&#31934;&#30830;&#30340;&#26368;&#22823;&#20284;&#28982;&#23398;&#20064;&#65292;&#21363;&#20351;&#26159;&#22522;&#20110;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#36830;&#32493;&#28508;&#21464;&#37327;&#24773;&#20917;&#30340;&#26377;&#25928;&#36817;&#20284;&#26041;&#27861;&#12290;&#23454;&#39564;&#23637;&#31034;&#20102;RPM&#22312;&#39640;&#32500;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#23398;&#20064;&#20174;&#24369;&#38388;&#25509;&#30417;&#30563;&#20013;&#30340;&#22270;&#20687;&#20998;&#31867;&#65307;&#30452;&#25509;&#22270;&#20687;&#32423;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new approach to probabilistic unsupervised learning based on the recognition-parametrised model (RPM): a normalised semi-parametric hypothesis class for joint distributions over observed and latent variables. Under the key assumption that observations are conditionally independent given latents, the RPM combines parametric prior and observation-conditioned latent distributions with non-parametric observation marginals. This approach leads to a flexible learnt recognition model capturing latent dependence between observations, without the need for an explicit, parametric generative model. The RPM admits exact maximum-likelihood learning for discrete latents, even for powerful neural-network-based recognition. We develop effective approximations applicable in the continuous-latent case. Experiments demonstrate the effectiveness of the RPM on high-dimensional data, learning image classification from weak indirect supervision; direct image-level latent Dirichlet allocation; 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102; ELBO &#25910;&#25947;&#21040;&#29109;&#21644;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#31867;&#24191;&#27867;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;ELBO &#22312;&#25152;&#26377;&#23398;&#20064;&#30340;&#31283;&#23450;&#28857;&#22788;&#37117;&#31561;&#20110;&#19968;&#31995;&#21015;&#29109;&#30340;&#21644;&#65292;&#20026;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#23646;&#24615;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2209.03077</link><description>&lt;p&gt;
&#20851;&#20110;ELBO&#25910;&#25947;&#21040;&#29109;&#21644;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of the ELBO to Entropy Sums. (arXiv:2209.03077v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102; ELBO &#25910;&#25947;&#21040;&#29109;&#21644;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#31867;&#24191;&#27867;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;ELBO &#22312;&#25152;&#26377;&#23398;&#20064;&#30340;&#31283;&#23450;&#28857;&#22788;&#37117;&#31561;&#20110;&#19968;&#31995;&#21015;&#29109;&#30340;&#21644;&#65292;&#20026;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#23646;&#24615;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#19979;&#30028;&#65288;&#21448;&#31216;ELBO&#25110;&#33258;&#30001;&#33021;&#65289;&#26159;&#35768;&#22810;&#32463;&#20856;&#21644;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#26680;&#24515;&#30446;&#26631;&#12290;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25913;&#21464;&#27169;&#22411;&#21442;&#25968;&#65292;&#20351;&#21464;&#20998;&#19979;&#30028;&#22686;&#21152;&#12290;&#36890;&#24120;&#65292;&#23398;&#20064;&#36827;&#34892;&#21040;&#21442;&#25968;&#25910;&#25947;&#21040;&#25509;&#36817;&#23398;&#20064;&#21160;&#24577;&#30340;&#31283;&#23450;&#28857;&#20540;&#12290;&#22312;&#26412;&#25991;&#30340;&#29702;&#35770;&#36129;&#29486;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65288;&#23545;&#20110;&#19968;&#31867;&#38750;&#24120;&#24191;&#27867;&#30340;&#29983;&#25104;&#27169;&#22411;&#65289;&#65292;&#21464;&#20998;&#19979;&#30028;&#22312;&#25152;&#26377;&#23398;&#20064;&#30340;&#31283;&#23450;&#28857;&#22788;&#22343;&#31561;&#20110;&#19968;&#31995;&#21015;&#29109;&#30340;&#21644;&#12290;&#23545;&#20110;&#20855;&#26377;&#19968;&#32452;&#28508;&#22312;&#21464;&#37327;&#21644;&#19968;&#32452;&#35266;&#27979;&#21464;&#37327;&#30340;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20010;&#21644;&#21253;&#25324;&#19977;&#20010;&#29109;: (A) &#21464;&#20998;&#20998;&#24067;&#30340;&#29109;&#65288;&#24179;&#22343;&#29109;&#65289;&#65292;(B) &#27169;&#22411;&#20808;&#39564;&#20998;&#24067;&#30340;&#36127;&#29109;&#21644; (C) &#21487;&#35266;&#27979;&#20998;&#24067;&#30340;&#65288;&#26399;&#26395;&#65289;&#36127;&#29109;&#12290;&#25152;&#24471;&#21040;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#21253;&#25324;&#65306;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#28857;&#65292;&#22312;&#23398;&#20064;&#30340;&#20219;&#24847;&#38454;&#27573;&#21644;&#21508;&#31181;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#31561;&#30495;&#23454;&#26465;&#20214;&#12290;&#26412;&#30740;&#31350;&#20026;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#23646;&#24615;&#25552;&#20379;&#20102;&#28145;&#20837;&#27934;&#23519;&#65292;&#26159;&#23545;&#20248;&#21270;&#25512;&#29702;&#21644;&#23398;&#20064;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The variational lower bound (a.k.a. ELBO or free energy) is the central objective for many established as well as many novel algorithms for unsupervised learning. Learning algorithms change model parameters such that the variational lower bound increases. Learning usually proceeds until parameters have converged to values close to a stationary point of the learning dynamics. In this purely theoretical contribution, we show that (for a very large class of generative models) the variational lower bound is at all stationary points of learning equal to a sum of entropies. For standard machine learning models with one set of latents and one set observed variables, the sum consists of three entropies: (A) the (average) entropy of the variational distributions, (B) the negative entropy of the model's prior distribution, and (C) the (expected) negative entropy of the observable distributions. The obtained result applies under realistic conditions including: finite numbers of data points, at an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;&#20803;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#24555;&#36895;&#27979;&#35797;&#21521;&#37327;&#29983;&#25104;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;2&#32423;&#39044;&#27979;&#22120;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#26080;&#29992;&#24037;&#20316;&#65292;&#39030;&#23618;&#20803;&#39044;&#27979;&#22120;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;99%</title><link>http://arxiv.org/abs/2207.11312</link><description>&lt;p&gt;
HybMT: &#22522;&#20110;&#28151;&#21512;&#20803;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#24555;&#36895;&#27979;&#35797;&#21521;&#37327;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
HybMT: Hybrid Meta-Predictor based ML Algorithm for Fast Test Vector Generation. (arXiv:2207.11312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;&#20803;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#24555;&#36895;&#27979;&#35797;&#21521;&#37327;&#29983;&#25104;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;2&#32423;&#39044;&#27979;&#22120;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#26080;&#29992;&#24037;&#20316;&#65292;&#39030;&#23618;&#20803;&#39044;&#27979;&#22120;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;99%
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#30005;&#36335;&#27979;&#35797;&#26159;&#19968;&#39033;&#39640;&#24230;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#12290;&#23545;&#20110;&#29616;&#20170;&#30340;&#22797;&#26434;&#35774;&#35745;&#65292;&#36890;&#24120;&#20351;&#29992;&#30830;&#23450;&#24615;&#27979;&#35797;&#29983;&#25104;&#65288;DTG&#65289;&#31639;&#27861;&#29983;&#25104;&#35768;&#22810;&#38590;&#20197;&#26816;&#27979;&#30340;&#25925;&#38556;&#30340;&#27979;&#35797;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#22686;&#21152;&#27979;&#35797;&#35206;&#30422;&#29575;&#24182;&#20943;&#23569;&#24635;&#20307;&#27979;&#35797;&#26102;&#38388;&#12290;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20943;&#23569;&#20102;&#20256;&#32479;&#30340;&#36335;&#24452;&#23548;&#21521;&#20915;&#31574;&#21046;&#23450;&#65288;PODEM&#65289;&#31639;&#27861;&#20013;&#30340;&#26080;&#29992;&#24037;&#20316;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27979;&#35797;&#36136;&#37327;&#12290;&#23545;&#20110;PODEM&#30340;&#21464;&#20307;&#65292;&#24456;&#22810;&#26102;&#20505;&#38656;&#35201;&#22238;&#28335;&#65292;&#22240;&#20026;&#32487;&#32493;&#25191;&#34892;&#24050;&#32463;&#26080;&#27861;&#21462;&#24471;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#22312;&#31639;&#27861;&#25191;&#34892;&#30340;&#19981;&#21516;&#38454;&#27573;&#39044;&#27979;&#26368;&#20339;&#31574;&#30053;&#12290;&#26412;&#25991;&#30340;&#26032;&#36129;&#29486;&#26159;&#19968;&#20010;2&#32423;&#39044;&#27979;&#22120;:&#39030;&#23618;&#26159;&#19968;&#20010;&#20803;&#39044;&#27979;&#22120;&#65292;&#22312;&#36739;&#20302;&#23618;&#20013;&#36873;&#25321;&#20960;&#20010;&#39044;&#27979;&#22120;&#20043;&#19968;&#12290;&#25105;&#20204;&#36873;&#25321;&#32473;&#23450;&#30005;&#36335;&#21644;&#30446;&#26631;&#32593;&#30340;&#26368;&#20339;&#39044;&#27979;&#22120;&#12290;&#21457;&#29616;&#39030;&#23618;&#20803;&#39044;&#27979;&#22120;&#30340;&#20934;&#30830;&#24230;&#20026;99\%&#12290;&#36825;&#23548;&#33268;&#20102;
&lt;/p&gt;
&lt;p&gt;
Testing an integrated circuit (IC) is a highly compute-intensive process. For today's complex designs, tests for many hard-to-detect faults are typically generated using deterministic test generation (DTG) algorithms. Machine Learning (ML) is being increasingly used to increase the test coverage and decrease the overall testing time. Such proposals primarily reduce the wasted work in the classic Path Oriented Decision Making (PODEM) algorithm without compromising on the test quality. With variants of PODEM, many times there is a need to backtrack because further progress cannot be made. There is thus a need to predict the best strategy at different points in the execution of the algorithm. The novel contribution of this paper is a 2-level predictor: the top level is a meta predictor that chooses one of several predictors at the lower level. We choose the best predictor given a circuit and a target net. The accuracy of the top-level meta predictor was found to be 99\%. This leads to a s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.07886</link><description>&lt;p&gt;
&#22522;&#20110;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System. (arXiv:2207.07886v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07886
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#19968;&#31181;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#65292;&#30001;&#20110;&#19981;&#26029;&#35775;&#38382;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#36807;&#31243;&#36890;&#24120;&#20250;&#21463;&#21040;&#20869;&#23384;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20197;&#22788;&#29702;&#22120;&#20026;&#20013;&#24515;&#30340;&#31995;&#32479;&#65288;&#20363;&#22914;CPU&#65292;GPU&#65289;&#22312;&#20869;&#23384;&#21333;&#20803;&#21644;&#22788;&#29702;&#21333;&#20803;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#26041;&#38754;&#23384;&#22312;&#26114;&#36149;&#30340;&#29942;&#39048;&#65292;&#36825;&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#33021;&#37327;&#21644;&#25191;&#34892;&#21608;&#26399;&#12290;&#20855;&#26377;&#22788;&#29702;&#20869;&#23384;&#65288;PIM&#65289;&#21151;&#33021;&#30340;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20102;&#35299;&#29616;&#20195;&#36890;&#29992;PIM&#26550;&#26500;&#21152;&#36895;ML&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#65288;1&#65289;&#22312;&#23454;&#38469;&#36890;&#29992;PIM&#26550;&#26500;&#19978;&#23454;&#29616;&#20102;&#20960;&#31181;&#20195;&#34920;&#24615;&#30340;&#20256;&#32479;ML&#31639;&#27861;&#65288;&#21363;&#32447;&#24615;&#22238;&#24402;&#65292;&#36923;&#36753;&#22238;&#24402;&#65292;&#20915;&#31574;&#26641;&#65292;K-Means&#32858;&#31867;&#65289;&#65292;&#65288;2&#65289;&#20005;&#26684;&#35780;&#20272;&#21644;&#34920;&#24449;&#36825;&#20123;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24615;&#33021;&#21644;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#65288;3&#65289;&#19982;&#23427;&#20204;&#22312;CPU&#21644;GPU&#19978;&#30340;&#30456;&#24212;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#24179;&#21488;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#30456;&#24212;&#30340;CPU&#21644;GPU&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning (ML) algorithms is a computationally intensive process, which is frequently memory-bound due to repeatedly accessing large training datasets. As a result, processor-centric systems (e.g., CPU, GPU) suffer from costly data movement between memory units and processing units, which consumes large amounts of energy and execution cycles. Memory-centric computing systems, i.e., with processing-in-memory (PIM) capabilities, can alleviate this data movement bottleneck.  Our goal is to understand the potential of modern general-purpose PIM architectures to accelerate ML training. To do so, we (1) implement several representative classic ML algorithms (namely, linear regression, logistic regression, decision tree, K-Means clustering) on a real-world general-purpose PIM architecture, (2) rigorously evaluate and characterize them in terms of accuracy, performance and scaling, and (3) compare to their counterpart implementations on CPU and GPU. Our evaluation on a real mem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#36317;&#31163;&#30340;&#36712;&#36857;&#27604;&#36739;&#31639;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;COVID-19&#24739;&#32773;&#29983;&#21629;&#20307;&#24449;&#36712;&#36857;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#25311;&#25972;&#21512;&#21040;&#29616;&#26377;&#31995;&#32479;&#20013;&#20197;&#25913;&#21892;&#20581;&#24247;&#24694;&#21270;&#30340;&#35782;&#21035;&#20934;&#30830;&#24230;&#21644;&#26102;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.07572</link><description>&lt;p&gt;
COVID-19&#24739;&#32773;&#29983;&#21629;&#20307;&#24449;&#36712;&#36857;&#24322;&#24120;&#20540;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Outlier detection of vital sign trajectories from COVID-19 patients. (arXiv:2207.07572v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#36317;&#31163;&#30340;&#36712;&#36857;&#27604;&#36739;&#31639;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;COVID-19&#24739;&#32773;&#29983;&#21629;&#20307;&#24449;&#36712;&#36857;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#25311;&#25972;&#21512;&#21040;&#29616;&#26377;&#31995;&#32479;&#20013;&#20197;&#25913;&#21892;&#20581;&#24247;&#24694;&#21270;&#30340;&#35782;&#21035;&#20934;&#30830;&#24230;&#21644;&#26102;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36712;&#36857;&#27604;&#36739;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#24322;&#24120;&#29983;&#21629;&#20307;&#24449;&#36235;&#21183;&#65292;&#26088;&#22312;&#25913;&#21892;&#23545;&#20581;&#24247;&#24694;&#21270;&#30340;&#35782;&#21035;&#12290;&#36817;&#26469;&#20154;&#20204;&#23545;&#20110;&#29992;&#20110;&#22312;&#23478;&#36828;&#31243;&#30417;&#27979;&#24739;&#32773;&#30340;&#36830;&#32493;&#20329;&#25140;&#24335;&#29983;&#21629;&#20307;&#24449;&#20256;&#24863;&#22120;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#36825;&#20123;&#30417;&#27979;&#22120;&#36890;&#24120;&#19982;&#35686;&#25253;&#31995;&#32479;&#37197;&#23545;&#20351;&#29992;&#65292;&#24403;&#29983;&#21629;&#20307;&#24449;&#27979;&#37327;&#20540;&#36229;&#20986;&#39044;&#23450;&#27491;&#24120;&#33539;&#22260;&#26102;&#35302;&#21457;&#35686;&#25253;&#12290;&#29983;&#21629;&#20307;&#24449;&#36235;&#21183;&#65292;&#22914;&#24515;&#29575;&#22686;&#21152;&#65292;&#36890;&#24120;&#26159;&#20581;&#24247;&#29366;&#20917;&#24694;&#21270;&#30340;&#24449;&#20806;&#65292;&#20294;&#26159;&#24456;&#23569;&#34987;&#32435;&#20837;&#35686;&#25253;&#31995;&#32479;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#27604;&#36739;&#26102;&#38388;&#24207;&#21015;&#36712;&#36857;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#22810;&#21464;&#37327;&#29983;&#21629;&#20307;&#24449;&#26102;&#38388;&#24207;&#21015;&#20998;&#20026;180&#20998;&#38047;&#65292;&#19981;&#37325;&#21472;&#30340;&#26102;&#27573;&#12290;&#28982;&#21518;&#35745;&#31639;&#25152;&#26377;&#26102;&#27573;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#27599;&#20010;&#26102;&#27573;&#30340;&#29305;&#24449;&#20026;&#20854;&#19982;&#25152;&#26377;&#20854;&#20182;&#26102;&#27573;&#20043;&#38388;&#30340;&#24179;&#22343;&#36317;&#31163;&#65288;&#24179;&#22343;&#38142;&#25509;&#36317;&#31163;&#65289;&#65292;&#36890;&#36807;&#30456;&#37051;&#26102;&#27573;&#24418;&#25104;&#32858;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#26469;&#33258;COVID-19&#24739;&#32773;&#30340;&#30495;&#23454;&#25968;&#25454;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#24322;&#24120;&#36712;&#36857;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#36712;&#36857;&#27604;&#36739;&#31639;&#27861;&#21487;&#20197;&#28508;&#22312;&#22320;&#25972;&#21512;&#21040;&#29616;&#26377;&#30340;&#35686;&#25253;&#31995;&#32479;&#20013;&#65292;&#20197;&#25552;&#39640;&#35782;&#21035;COVID-19&#24739;&#32773;&#20581;&#24247;&#24694;&#21270;&#30340;&#31934;&#24230;&#21644;&#26102;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a novel trajectory comparison algorithm to identify abnormal vital sign trends, with the aim of improving recognition of deteriorating health.  There is growing interest in continuous wearable vital sign sensors for monitoring patients remotely at home. These monitors are usually coupled to an alerting system, which is triggered when vital sign measurements fall outside a predefined normal range. Trends in vital signs, such as increasing heart rate, are often indicative of deteriorating health, but are rarely incorporated into alerting systems.  We introduce a dynamic time warp distance-based measure to compare time series trajectories. We split each multi-variable sign time series into 180 minute, non-overlapping epochs. We then calculate the distance between all pairs of epochs. Each epoch is characterized by its mean pairwise distance (average link distance) to all other epochs, with clusters forming with nearby epochs.  We demonstrate in synthetically gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#65292;&#24635;&#32467;&#19982;&#20998;&#31867;&#20026;&#19977;&#32452;&#12290;&#32435;&#20837;&#21270;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#26410;&#26469;&#38656;&#35201;&#26356;&#26131;&#20110;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;MRL&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2207.04869</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#30340;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph-based Molecular Representation Learning. (arXiv:2207.04869v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#65292;&#24635;&#32467;&#19982;&#20998;&#31867;&#20026;&#19977;&#32452;&#12290;&#32435;&#20837;&#21270;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#26410;&#26469;&#38656;&#35201;&#26356;&#26131;&#20110;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;MRL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;(MRL)&#26159;&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#21644;&#21270;&#23398;&#31185;&#23398;&#20043;&#38388;&#32852;&#31995;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#23427;&#23558;&#20998;&#23376;&#32534;&#30721;&#20026;&#25968;&#23383;&#21521;&#37327;&#65292;&#20445;&#30041;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#65292;&#20174;&#32780;&#21487;&#20197;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#24615;&#36136;&#39044;&#27979;&#65289;&#12290;&#26368;&#36817;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#28145;&#24230;&#20998;&#23376;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#36825;&#20123;&#22522;&#20110;&#22270;&#24418;&#30340;&#20998;&#23376;&#34920;&#31034;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#32435;&#20837;&#21270;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;2D&#21644;3D&#20998;&#23376;&#22270;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#20854;&#36755;&#20837;&#23558;MRL&#26041;&#27861;&#24635;&#32467;&#21644;&#20998;&#31867;&#20026;&#19977;&#32452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;MRL&#25903;&#25345;&#30340;&#19968;&#20123;&#20856;&#22411;&#21270;&#23398;&#24212;&#29992;&#31243;&#24207;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#21015;&#20986;&#20102;&#26412;&#25991;&#20013;&#20351;&#29992;&#30340;&#22522;&#20934;&#21644;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#20851;&#20110;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#26356;&#26131;&#20110;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;MRL&#27169;&#22411;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular representation learning (MRL) is a key step to build the connection between machine learning and chemical science. In particular, it encodes molecules as numerical vectors preserving the molecular structures and features, on top of which the downstream tasks (e.g., property prediction) can be performed. Recently, MRL has achieved considerable progress, especially in methods based on deep molecular graph learning. In this survey, we systematically review these graph-based molecular representation techniques, especially the methods incorporating chemical domain knowledge. Specifically, we first introduce the features of 2D and 3D molecular graphs. Then we summarize and categorize MRL methods into three groups based on their input. Furthermore, we discuss some typical chemical applications supported by MRL. To facilitate studies in this fast-developing area, we also list the benchmarks and commonly used datasets in the paper. Finally, we share our thoughts on future research dir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#20197;&#26816;&#27979;DRL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.07813</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Search-Based Testing Approach for Deep Reinforcement Learning Agents. (arXiv:2206.07813v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#20197;&#26816;&#27979;DRL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#21313;&#24180;&#26469;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#31561;&#21508;&#31181;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#29983;&#21629;&#23433;&#20840;&#29615;&#22659;&#20013;&#32463;&#24120;&#34920;&#29616;&#20986;&#38169;&#35823;&#34892;&#20026;&#65292;&#23548;&#33268;&#28508;&#22312;&#30340;&#37325;&#22823;&#38169;&#35823;&#65292;&#22240;&#27492;&#23427;&#20204;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35780;&#20272;DRL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#23545;&#23427;&#20204;&#36827;&#34892;&#27979;&#35797;&#65292;&#20197;&#26816;&#27979;&#21487;&#33021;&#23548;&#33268;&#20851;&#38190;&#25925;&#38556;&#30340;&#25925;&#38556;&#12290;&#36825;&#23601;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#25105;&#20204;&#22914;&#20309;&#26377;&#25928;&#22320;&#27979;&#35797;DRL&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#30340;&#27491;&#30830;&#24615;&#21644;&#36981;&#23432;&#23433;&#20840;&#35201;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#20195;&#29702;&#29983;&#25104;&#28385;&#36275;&#23433;&#20840;&#35201;&#27714;&#30340;&#29366;&#24577;&#24207;&#21015;&#21464;&#21270;&#65292;&#20197;&#25506;&#32034;&#29615;&#22659;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;DRL&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#20445;&#25345;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#27979;&#35797;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One way to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Their main goal is to test the robustness of DRL agents rather than testing the compliance of agents' policies with respect to requirements. Due to the huge state spac
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#29992;&#34394;&#25311;&#36845;&#20195;&#21644;&#24310;&#36831;&#33258;&#36866;&#24212;&#27493;&#38271;&#21487;&#20197;&#22312;&#20219;&#24847;&#24310;&#36831;&#19979;&#25552;&#39640;&#24322;&#27493;SGD&#31639;&#27861;&#24615;&#33021;&#65292;&#20248;&#20110;&#21516;&#27493;&#23567;&#25209;&#37327;SGD&#12290;</title><link>http://arxiv.org/abs/2206.07638</link><description>&lt;p&gt;
&#24322;&#27493;SGD&#22312;&#20219;&#24847;&#24310;&#36831;&#19979;&#22343;&#20248;&#20110;Minibatch SGD
&lt;/p&gt;
&lt;p&gt;
Asynchronous SGD Beats Minibatch SGD Under Arbitrary Delays. (arXiv:2206.07638v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07638
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#29992;&#34394;&#25311;&#36845;&#20195;&#21644;&#24310;&#36831;&#33258;&#36866;&#24212;&#27493;&#38271;&#21487;&#20197;&#22312;&#20219;&#24847;&#24310;&#36831;&#19979;&#25552;&#39640;&#24322;&#27493;SGD&#31639;&#27861;&#24615;&#33021;&#65292;&#20248;&#20110;&#21516;&#27493;&#23567;&#25209;&#37327;SGD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#29616;&#26377;&#20998;&#26512;&#22312;&#20219;&#20309;&#24310;&#36831;&#36739;&#22823;&#26102;&#20250;&#24613;&#21095;&#24694;&#21270;&#65292;&#32473;&#20154;&#20204;&#30041;&#19979;&#20102;&#24615;&#33021;&#20027;&#35201;&#21462;&#20915;&#20110;&#24310;&#36831;&#30340;&#21360;&#35937;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#30456;&#21516;&#30340;&#24322;&#27493; SGD &#31639;&#27861;&#65292;&#26080;&#35770;&#28176;&#21464;&#30340;&#24310;&#36831;&#22914;&#20309;&#65292;&#24615;&#33021;&#20445;&#35777;&#37117;&#35201;&#22909;&#24471;&#22810;&#65292;&#36825;&#21482;&#21462;&#20915;&#20110;&#29992;&#20110;&#23454;&#26045;&#31639;&#27861;&#30340;&#24182;&#34892;&#35774;&#22791;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#20445;&#35777;&#27604;&#29616;&#26377;&#20998;&#26512;&#20005;&#26684;&#26356;&#22909;&#65292;&#21516;&#26102;&#25105;&#20204;&#36824;&#35770;&#35777;&#24322;&#27493; SGD &#22312;&#25105;&#20204;&#32771;&#34385;&#30340;&#24773;&#26223;&#19979;&#20248;&#20110;&#21516;&#27493;&#23567;&#25209;&#37327; SGD&#12290;&#20026;&#20102;&#36827;&#34892;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#34394;&#25311;&#36845;&#20195;&#8221;&#21644;&#24310;&#36831;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;&#26032;&#36882;&#24402;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#24471;&#20986;&#38024;&#23545;&#20984;&#21644;&#38750;&#20984;&#30446;&#26631;&#30340;&#26368;&#20808;&#36827;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existing analysis of asynchronous stochastic gradient descent (SGD) degrades dramatically when any delay is large, giving the impression that performance depends primarily on the delay. On the contrary, we prove much better guarantees for the same asynchronous SGD algorithm regardless of the delays in the gradients, depending instead just on the number of parallel devices used to implement the algorithm. Our guarantees are strictly better than the existing analyses, and we also argue that asynchronous SGD outperforms synchronous minibatch SGD in the settings we consider. For our analysis, we introduce a novel recursion based on "virtual iterates" and delay-adaptive stepsizes, which allow us to derive state-of-the-art guarantees for both convex and non-convex objectives.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDPMS&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#21512;&#25104;&#26412;&#22320;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#22686;&#24378;&#26041;&#27861;&#21487;&#20943;&#36731;&#25968;&#25454;&#24322;&#26500;&#25928;&#24212;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2206.00686</link><description>&lt;p&gt;
&#21463;&#19981;&#21516;ially Private Synthetic Data&#25903;&#25345;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#35774;&#32622;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning in Non-IID Settings Aided by Differentially Private Synthetic Data. (arXiv:2206.00686v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00686
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDPMS&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#21512;&#25104;&#26412;&#22320;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#22686;&#24378;&#26041;&#27861;&#21487;&#20943;&#36731;&#25968;&#25454;&#24322;&#26500;&#25928;&#24212;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated learning, FL)&#26159;&#19968;&#31181;&#33021;&#22815;&#35753;&#28508;&#22312;&#30340;&#22823;&#37327;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#12290;&#22312;FL&#31995;&#32479;&#20013;&#65292;&#26381;&#21153;&#22120;&#36890;&#36807;&#25910;&#38598;&#21644;&#32858;&#21512;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#26356;&#26032;&#26469;&#21327;&#35843;&#21512;&#20316;&#65292;&#32780;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#20173;&#28982;&#20445;&#25345;&#26412;&#22320;&#21644;&#31169;&#23494;&#12290;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#24403;&#26412;&#22320;&#25968;&#25454;&#24322;&#26500;&#26102;&#65292;&#23398;&#20064;&#21040;&#30340;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26126;&#26174;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDPMS(&#32852;&#37030;&#24046;&#20998;&#31169;&#26377;&#22343;&#20540;&#20849;&#20139;)&#65292;&#36825;&#26159;&#19968;&#31181;FL&#31639;&#27861;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#37096;&#32626;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(Variational Auto-encoder, VAE)&#26469;&#21033;&#29992;&#30001;&#20449;&#20219;&#26381;&#21153;&#22120;&#36890;&#20449;&#30340;&#28508;&#21464;&#37327;&#25968;&#25454;&#30340;&#24046;&#20998;&#31169;&#26377;&#22343;&#20540;&#26469;&#29983;&#25104;&#21512;&#25104;&#26412;&#22320;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#22686;&#24378;&#21487;&#20943;&#36731;&#36328;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#24322;&#26500;&#25928;&#24212;&#65292;&#21516;&#26102;&#21448;&#19981;&#20250;&#25439;&#23475;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a privacy-promoting framework that enables potentially large number of clients to collaboratively train machine learning models. In a FL system, a server coordinates the collaboration by collecting and aggregating clients' model updates while the clients' data remains local and private. A major challenge in federated learning arises when the local data is heterogeneous -- the setting in which performance of the learned global model may deteriorate significantly compared to the scenario where the data is identically distributed across the clients. In this paper we propose FedDPMS (Federated Differentially Private Means Sharing), an FL algorithm in which clients deploy variational auto-encoders to augment local datasets with data synthesized using differentially private means of latent data representations communicated by a trusted server. Such augmentation ameliorates effects of data heterogeneity across the clients without compromising privacy. Our experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36830;&#32493;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;(CGNN)&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#26465;&#20214;&#20445;&#35777;CGNN&#26159;&#21333;&#23556;&#30340;&#65292;&#20854;&#29983;&#25104;&#27969;&#24418;&#34987;&#29992;&#20110;&#27714;&#35299;&#21453;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.14627</link><description>&lt;p&gt;
&#36830;&#32493;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continuous Generative Neural Networks. (arXiv:2205.14627v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36830;&#32493;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;(CGNN)&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#26465;&#20214;&#20445;&#35777;CGNN&#26159;&#21333;&#23556;&#30340;&#65292;&#20854;&#29983;&#25104;&#27969;&#24418;&#34987;&#29992;&#20110;&#27714;&#35299;&#21453;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#36830;&#32493;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#65288;CGNN&#65289;&#65292;&#21363;&#36830;&#32493;&#24773;&#22659;&#19979;&#30340;&#29983;&#25104;&#27169;&#22411;&#65306;CGNN&#30340;&#36755;&#20986;&#23646;&#20110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#12290;&#35813;&#26550;&#26500;&#21463;DCGAN&#30340;&#21551;&#21457;&#65292;&#37319;&#29992;&#19968;&#20010;&#20840;&#36830;&#25509;&#23618;&#65292;&#22810;&#20010;&#21367;&#31215;&#23618;&#21644;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#12290;&#22312;&#36830;&#32493;&#30340;$L^2$&#24773;&#22659;&#19979;&#65292;&#27599;&#23618;&#31354;&#38388;&#30340;&#32500;&#24230;&#34987;&#32039;&#25903;&#23567;&#27874;&#30340;&#22810;&#37325;&#20998;&#36776;&#29575;&#20998;&#26512;&#30340;&#23610;&#24230;&#25152;&#20195;&#26367;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#38750;&#32447;&#24615;&#30340;&#26465;&#20214;&#65292;&#20445;&#35777;CGNN&#26159;&#21333;&#23556;&#30340;&#12290;&#35813;&#29702;&#35770;&#24212;&#29992;&#20110;&#21453;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#23548;&#20986;&#19968;&#20010;CGNN&#29983;&#25104;&#27969;&#24418;&#30340;&#65288;&#21487;&#33021;&#38750;&#32447;&#24615;&#30340;&#65289;&#26080;&#38480;&#32500;&#21453;&#38382;&#39064;&#30340;Lipschitz&#31283;&#23450;&#24615;&#20272;&#35745;&#12290;&#21253;&#25324;&#20449;&#21495;&#21435;&#27169;&#31946;&#22312;&#20869;&#30340;&#22810;&#20010;&#25968;&#20540;&#27169;&#25311;&#35777;&#26126;&#24182;&#39564;&#35777;&#20102;&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present and study Continuous Generative Neural Networks (CGNNs), namely, generative models in the continuous setting: the output of a CGNN belongs to an infinite-dimensional function space. The architecture is inspired by DCGAN, with one fully connected layer, several convolutional layers and nonlinear activation functions. In the continuous $L^2$ setting, the dimensions of the spaces of each layer are replaced by the scales of a multiresolution analysis of a compactly supported wavelet. We present conditions on the convolutional filters and on the nonlinearity that guarantee that a CGNN is injective. This theory finds applications to inverse problems, and allows for deriving Lipschitz stability estimates for (possibly nonlinear) infinite-dimensional inverse problems with unknowns belonging to the manifold generated by a CGNN. Several numerical simulations, including signal deblurring, illustrate and validate this approach.
&lt;/p&gt;</description></item><item><title>&#29420;&#31435;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#22312;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#26377;&#25928;&#65292;&#36890;&#36807;&#26356;&#26032;Q&#20989;&#25968;&#21487;&#20197;&#24341;&#23548;&#31574;&#30053;&#25910;&#25947;&#21040;&#31283;&#23450;&#30340;&#32435;&#20160;&#24179;&#34913;&#28857;&#12290;</title><link>http://arxiv.org/abs/2205.14590</link><description>&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#30340;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Independent and Decentralized Learning in Markov Potential Games. (arXiv:2205.14590v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14590
&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#22312;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#26377;&#25928;&#65292;&#36890;&#36807;&#26356;&#26032;Q&#20989;&#25968;&#21487;&#20197;&#24341;&#23548;&#31574;&#30053;&#25910;&#25947;&#21040;&#31283;&#23450;&#30340;&#32435;&#20160;&#24179;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#30340;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#29609;&#23478;&#19981;&#20102;&#35299;&#28216;&#25103;&#27169;&#22411;&#65292;&#20063;&#19981;&#33021;&#36827;&#34892;&#21327;&#35843;&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#29609;&#23478;&#36890;&#36807;&#24322;&#27493;&#26041;&#24335;&#26356;&#26032;&#20182;&#20204;&#30340;&#25171;&#25200;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#20540;&#65292;&#35813;&#20989;&#25968;&#26681;&#25454;&#23454;&#29616;&#30340;&#19968;&#38454;&#27573;&#22870;&#21169;&#35780;&#20272;&#20182;&#20204;&#30340;&#24635;&#20307;&#26465;&#20214;&#20184;&#27454;&#12290;&#28982;&#21518;&#65292;&#29609;&#23478;&#36890;&#36807;&#23558;&#22522;&#20110;&#20272;&#35745;Q&#20989;&#25968;&#30340;&#24179;&#28369;&#26368;&#20248;&#19968;&#38454;&#27573;&#20559;&#24046;&#31574;&#30053;&#32435;&#20837;&#20854;&#31574;&#30053;&#20013;&#26469;&#29420;&#31435;&#22320;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#23398;&#20064;&#21160;&#24577;&#30340;&#20851;&#38190;&#29305;&#24449;&#26159;Q&#20989;&#25968;&#20272;&#35745;&#26159;&#20197;&#27604;&#31574;&#30053;&#26356;&#24555;&#30340;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#26356;&#26032;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#21160;&#24577;&#24341;&#23548;&#30340;&#31574;&#30053;&#22312;&#27010;&#29575;1&#30340;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#30340;&#31283;&#23450;&#32435;&#20160;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#31616;&#21333;&#23398;&#20064;&#21160;&#24577;&#22312;&#36798;&#21040;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#30340;&#31283;&#23450;&#32435;&#20160;&#24179;&#34913;&#26041;&#38754;&#30340;&#21151;&#25928;&#65292;&#21363;&#20351;&#26159;&#22312;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#20195;&#29702;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a multi-agent reinforcement learning dynamics, and analyze its convergence in infinite-horizon discounted Markov potential games. We focus on the independent and decentralized setting, where players do not have knowledge of the game model and cannot coordinate. In each stage, players update their estimate of a perturbed Q-function that evaluates their total contingent payoff based on the realized one-stage reward in an asynchronous manner. Then, players independently update their policies by incorporating a smoothed optimal one-stage deviation strategy based on the estimated Q-function. A key feature of the learning dynamics is that the Q-function estimates are updated at a faster timescale than the policies. We prove that the policies induced by our learning dynamics converge to a stationary Nash equilibrium in Markov potential games with probability 1. Our results highlight the efficacy of simple learning dynamics in reaching a stationary Nash equilibrium even in environme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32473;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#33021;&#22312;&#36866;&#24230;&#25968;&#37327;&#30340;&#32422;&#26463;&#19979;&#20351;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36867;&#33073;&#39640;&#32500;&#39532;&#38797;&#28857;&#12290;</title><link>http://arxiv.org/abs/2205.13753</link><description>&lt;p&gt;
HOUDINI: &#20174;&#36866;&#24230;&#32422;&#26463;&#30340;&#39532;&#38797;&#28857;&#20013;&#36867;&#33073;
&lt;/p&gt;
&lt;p&gt;
HOUDINI: Escaping from Moderately Constrained Saddles. (arXiv:2205.13753v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32473;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#33021;&#22312;&#36866;&#24230;&#25968;&#37327;&#30340;&#32422;&#26463;&#19979;&#20351;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36867;&#33073;&#39640;&#32500;&#39532;&#38797;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32473;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#36866;&#24230;&#25968;&#37327;&#30340;&#32422;&#26463;&#19979;&#20174;&#39640;&#32500;&#39532;&#38797;&#28857;&#20013;&#36867;&#33073;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#32473;&#23450;&#20809;&#28369;&#20989;&#25968;$f \colon \mathbb R^d \to \mathbb R$&#30340;&#26799;&#24230;&#35775;&#38382;&#26435;&#38480;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65288;&#24102;&#22122;&#22768;&#30340;&#65289;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#21487;&#20197;&#22312;&#23545;&#25968;&#20010;&#19981;&#31561;&#24335;&#32422;&#26463;&#19979;&#36867;&#33073;&#39532;&#38797;&#28857;&#12290;&#36825;&#26159;&#23545; Ge &#31561;&#20154;&#30340;&#31361;&#30772;&#24615;&#24037;&#20316;&#30340;&#20027;&#35201;&#24320;&#25918;&#38382;&#39064;&#30340;&#39318;&#27425;&#26377;&#24418;&#36827;&#23637;&#65288;&#26080;&#38656;&#20381;&#36182; NP-Oracle &#25110;&#25913;&#21464;&#23450;&#20041;&#26469;&#20165;&#32771;&#34385;&#29305;&#23450;&#32422;&#26463;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give the first polynomial time algorithms for escaping from high-dimensional saddle points under a moderate number of constraints. Given gradient access to a smooth function $f \colon \mathbb R^d \to \mathbb R$ we show that (noisy) gradient descent methods can escape from saddle points under a logarithmic number of inequality constraints. This constitutes the first tangible progress (without reliance on NP-oracles or altering the definitions to only account for certain constraints) on the main open question of the breakthrough work of Ge et al. who showed an analogous result for unconstrained and equality-constrained problems. Our results hold for both regular and stochastic gradient descent.
&lt;/p&gt;</description></item><item><title>FLEX&#26159;&#19968;&#31181;&#26032;&#22411;KGR&#26694;&#26550;&#65292;&#33021;&#22815;&#30495;&#27491;&#22788;&#29702;&#21253;&#25324;&#21512;&#21462;&#12289;&#26512;&#21462;&#12289;&#21542;&#23450;&#31561;&#25152;&#26377;FOL&#25805;&#20316;&#24182;&#25903;&#25345;&#21508;&#31181;&#29305;&#24449;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FLEX&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.11039</link><description>&lt;p&gt;
FLEX: &#29992;&#20110;&#22797;&#26434;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#29305;&#24449;&#36923;&#36753;&#23884;&#20837;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FLEX: Feature-Logic Embedding Framework for CompleX Knowledge Graph Reasoning. (arXiv:2205.11039v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11039
&lt;/p&gt;
&lt;p&gt;
FLEX&#26159;&#19968;&#31181;&#26032;&#22411;KGR&#26694;&#26550;&#65292;&#33021;&#22815;&#30495;&#27491;&#22788;&#29702;&#21253;&#25324;&#21512;&#21462;&#12289;&#26512;&#21462;&#12289;&#21542;&#23450;&#31561;&#25152;&#26377;FOL&#25805;&#20316;&#24182;&#25903;&#25345;&#21508;&#31181;&#29305;&#24449;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FLEX&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65288;KGR&#65289;&#30340;&#24403;&#21069;&#26368;&#20339;&#27169;&#22411;&#24341;&#20837;&#20960;&#20309;&#23545;&#35937;&#25110;&#27010;&#29575;&#20998;&#24067;&#23558;&#23454;&#20307;&#21644;&#19968;&#38454;&#36923;&#36753;&#65288;FOL&#65289;&#26597;&#35810;&#23884;&#20837;&#21040;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;&#23427;&#20204;&#21487;&#20197;&#24635;&#32467;&#20026;&#19968;&#20010;&#20013;&#24515;-&#23610;&#23544;&#26694;&#26550;&#65288;&#28857;/&#30418;&#24335;/&#38181;&#24418;&#65292;Beta/Gaussian&#20998;&#24067;&#31561;&#65289;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#36923;&#36753;&#23884;&#20837;&#26694;&#26550;&#65288;FLEX&#65289;&#30340;&#26032;&#22411;KGR&#26694;&#26550;&#65292;&#23427;&#26159;&#39318;&#20010;&#30495;&#27491;&#33021;&#22815;&#22788;&#29702;&#21253;&#25324;&#21512;&#21462;&#12289;&#26512;&#21462;&#12289;&#21542;&#23450;&#31561;&#25152;&#26377;FOL&#25805;&#20316;&#24182;&#25903;&#25345;&#21508;&#31181;&#29305;&#24449;&#31354;&#38388;&#30340;KGR&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#29305;&#24449;&#36923;&#36753;&#26694;&#26550;&#30340;&#36923;&#36753;&#37096;&#20998;&#22522;&#20110;&#21521;&#37327;&#36923;&#36753;&#65292;&#33258;&#28982;&#22320;&#24314;&#27169;&#20102;&#25152;&#26377;FOL&#25805;&#20316;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FLEX&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current best performing models for knowledge graph reasoning (KGR) introduce geometry objects or probabilistic distributions to embed entities and first-order logical (FOL) queries into low-dimensional vector spaces. They can be summarized as a center-size framework (point/box/cone, Beta/Gaussian distribution, etc.). However, they have limited logical reasoning ability. And it is difficult to generalize to various features, because the center and size are one-to-one constrained, unable to have multiple centers or sizes. To address these challenges, we instead propose a novel KGR framework named Feature-Logic Embedding framework, FLEX, which is the first KGR framework that can not only TRULY handle all FOL operations including conjunction, disjunction, negation and so on, but also support various feature spaces. Specifically, the logic part of feature-logic framework is based on vector logic, which naturally models all FOL operations. Experiments demonstrate that FLEX significantly outp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;FedCAMS&#65289;&#65292;&#21487;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#37325;&#22797;&#30340;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#21516;&#27493;&#32780;&#20135;&#29983;&#30340;&#22823;&#37327;&#36890;&#20449;&#24320;&#38144;&#21644;&#22522;&#20110; SGD &#30340;&#27169;&#22411;&#26356;&#26032;&#32570;&#20047;&#36866;&#24212;&#24615;&#31561;&#35832;&#22810;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.02719</link><description>&lt;p&gt;
&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Adaptive Federated Learning. (arXiv:2205.02719v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;FedCAMS&#65289;&#65292;&#21487;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#37325;&#22797;&#30340;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#21516;&#27493;&#32780;&#20135;&#29983;&#30340;&#22823;&#37327;&#36890;&#20449;&#24320;&#38144;&#21644;&#22522;&#20110; SGD &#30340;&#27169;&#22411;&#26356;&#26032;&#32570;&#20047;&#36866;&#24212;&#24615;&#31561;&#35832;&#22810;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#24335;&#65292;&#20351;&#24471;&#23458;&#25143;&#31471;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20013;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#20173;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#30001;&#20110;&#37325;&#22797;&#30340;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#21516;&#27493;&#32780;&#20135;&#29983;&#30340;&#22823;&#37327;&#36890;&#20449;&#24320;&#38144;&#20197;&#21450;&#22522;&#20110; SGD &#30340;&#27169;&#22411;&#26356;&#26032;&#32570;&#20047;&#36866;&#24212;&#24615;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#26799;&#24230;&#21387;&#32553;&#25110;&#37327;&#21270;&#26469;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;FedAdam&#31561;&#32852;&#37030;&#29256;&#26412;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#26469;&#22686;&#21152;&#26356;&#22810;&#30340;&#36866;&#24212;&#24615;&#65292;&#20294;&#24403;&#21069;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20173;&#26080;&#27861;&#21516;&#26102;&#35299;&#20915;&#19978;&#36848;&#25152;&#26377;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;FedCAMS&#65289;&#65292;&#20855;&#26377;&#29702;&#35770;&#19978;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a machine learning training paradigm that enables clients to jointly train models without sharing their own localized data. However, the implementation of federated learning in practice still faces numerous challenges, such as the large communication overhead due to the repetitive server-client synchronization and the lack of adaptivity by SGD-based model updates. Despite that various methods have been proposed for reducing the communication cost by gradient compression or quantization, and the federated versions of adaptive optimizers such as FedAdam are proposed to add more adaptivity, the current federated learning framework still cannot solve the aforementioned challenges all at once. In this paper, we propose a novel communication-efficient adaptive federated learning method (FedCAMS) with theoretical convergence guarantees. We show that in the nonconvex stochastic optimization setting, our proposed FedCAMS achieves the same convergence rate of $O(\frac{1}{\s
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#30340;&#20004;&#26679;&#26412;&#26816;&#39564;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20302;&#32500;&#27969;&#24418;&#19978;&#25903;&#25345;&#30340;&#39640;&#32500;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32479;&#35745;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.02043</link><description>&lt;p&gt;
&#27969;&#24418;&#20004;&#26679;&#26412;&#26816;&#39564;&#30740;&#31350;&#65306;&#31070;&#32463;&#32593;&#32476;&#30340;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Manifold Two-Sample Test Study: Integral Probability Metric with Neural Networks. (arXiv:2205.02043v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02043
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#30340;&#20004;&#26679;&#26412;&#26816;&#39564;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20302;&#32500;&#27969;&#24418;&#19978;&#25903;&#25345;&#30340;&#39640;&#32500;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32479;&#35745;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#26679;&#26412;&#26816;&#39564;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#30830;&#23450;&#20004;&#20010;&#35266;&#27979;&#38598;&#21512;&#26159;&#21542;&#36981;&#24490;&#30456;&#21516;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#65288;IPM&#65289;&#30340;&#20004;&#26679;&#26412;&#26816;&#39564;&#65292;&#36866;&#29992;&#20110;&#20302;&#32500;&#27969;&#24418;&#19978;&#25903;&#25345;&#30340;&#39640;&#32500;&#26679;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#26679;&#26412;&#25968;$n$&#21644;&#27969;&#24418;&#20869;&#22312;&#32500;&#24230;$d$&#30340;&#32467;&#26500;&#34920;&#24449;&#20102;&#25152;&#25552;&#20986;&#30340;&#26816;&#39564;&#30340;&#29305;&#24615;&#12290;&#24403;&#32473;&#23450;&#19968;&#20010;&#22270;&#38598;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#27493;&#26816;&#39564;&#20197;&#35782;&#21035;&#19968;&#33324;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20854;&#22312;$n^{-1/\max\{d,2\}}$&#30340;&#39034;&#24207;&#20013;&#23454;&#29616;&#20102;&#31532;&#20108;&#31867;&#22411;&#39118;&#38505;&#12290;&#24403;&#26410;&#32473;&#20986;&#22270;&#38598;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;H\"older IPM&#26816;&#39564;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;$(s,\beta)$&#8208;H\"older&#23494;&#24230;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#20854;&#22312;$n^{-(s+\beta)/d}$&#30340;&#39034;&#24207;&#20013;&#23454;&#29616;&#20102;&#31532;&#20108;&#31867;&#22411;&#39118;&#38505;&#12290;&#20026;&#20102;&#20943;&#36731;&#35780;&#20272;H\"older IPM&#30340;&#37325;&#35745;&#31639;&#36127;&#25285;&#65292;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;H\"older&#20989;&#25968;&#31867;&#12290;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#29702;&#35770;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;H\"older IPM&#27979;&#35797;&#20855;&#26377;&#19968;&#33268;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32479;&#35745;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two-sample tests are important areas aiming to determine whether two collections of observations follow the same distribution or not. We propose two-sample tests based on integral probability metric (IPM) for high-dimensional samples supported on a low-dimensional manifold. We characterize the properties of proposed tests with respect to the number of samples $n$ and the structure of the manifold with intrinsic dimension $d$. When an atlas is given, we propose two-step test to identify the difference between general distributions, which achieves the type-II risk in the order of $n^{-1/\max\{d,2\}}$. When an atlas is not given, we propose H\"older IPM test that applies for data distributions with $(s,\beta)$-H\"older densities, which achieves the type-II risk in the order of $n^{-(s+\beta)/d}$. To mitigate the heavy computation burden of evaluating the H\"older IPM, we approximate the H\"older function class using neural networks. Based on the approximation theory of neural networks, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#26102;&#24207;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20854;&#20182;&#29616;&#20195;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2204.08461</link><description>&lt;p&gt;
&#25506;&#32034;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#21355;&#26143;&#22270;&#20687;&#26102;&#24207;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Temporal Convolutional Neural Networks for Satellite Image Time Series Classification: A survey. (arXiv:2204.08461v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#26102;&#24207;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20854;&#20182;&#29616;&#20195;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#34920;&#38754;&#30340;&#36965;&#24863;&#26102;&#24207;&#22270;&#20687;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#22320;&#34920;&#35206;&#30422;&#22320;&#22270;&#65292;&#20854;&#22312;&#26102;&#31354;&#32500;&#24230;&#19978;&#30340;&#36136;&#37327;&#25345;&#32493;&#25552;&#39640;&#65292;&#23545;&#20110;&#24320;&#21457;&#26088;&#22312;&#20135;&#29983;&#20934;&#30830;&#12289;&#23454;&#26102;&#22320;&#34920;&#35206;&#30422;&#22320;&#22270;&#30340;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#22270;&#20687;&#26102;&#24207;&#22312;&#29983;&#24577;&#31995;&#32479;&#21046;&#22270;&#12289;&#26893;&#34987;&#36807;&#31243;&#30417;&#27979;&#21644;&#20154;&#31867;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#36319;&#36394;&#31561;&#24191;&#27867;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#21355;&#26143;&#22270;&#20687;&#26102;&#24207;&#20998;&#31867;&#30340;&#19968;&#20123;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25928;&#26524;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#21033;&#29992;&#25968;&#25454;&#26102;&#38388;&#32500;&#24230;&#30340;&#26412;&#22320;&#26426;&#21046;&#65307;&#24120;&#24120;&#23548;&#33268;&#22823;&#37327;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#36807;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#19981;&#36275;&#65292;&#26368;&#36817;&#24320;&#22987;&#22312;&#21355;&#26143;&#22270;&#20687;&#26102;&#24207;&#20998;&#31867;&#20013;&#24212;&#29992;&#20102;&#26102;&#24207;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545;&#36825;&#19968;&#26041;&#27861;&#19982;&#20854;&#20182;&#24403;&#20195;&#26102;&#24207;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Satellite Image Time Series (SITS) of the Earth's surface provide detailed land cover maps, with their quality in the spatial and temporal dimensions consistently improving. These image time series are integral for developing systems that aim to produce accurate, up-to-date land cover maps of the Earth's surface. Applications are wide-ranging, with notable examples including ecosystem mapping, vegetation process monitoring and anthropogenic land-use change tracking. Recently proposed methods for SITS classification have demonstrated respectable merit, but these methods tend to lack native mechanisms that exploit the temporal dimension of the data; commonly resulting in extensive data pre-processing contributing to prohibitively long training times. To overcome these shortcomings, Temporal CNNs have recently been employed for SITS classification tasks with encouraging results. This paper seeks to survey this method against a plethora of other contemporary methods for SITS classification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26680;&#26041;&#27861;&#26500;&#36896;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#22312;&#36125;&#21494;&#26031;&#35774;&#32622;&#21644;Neyman-Pearson&#35774;&#32622;&#20013;&#20998;&#21035;&#30740;&#31350;&#20102;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#38169;&#35823;&#27010;&#29575;&#21644;&#25511;&#21046;&#38169;&#35823;&#27010;&#29575;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;MMD&#30340;&#19968;&#31995;&#21015;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2203.12777</link><description>&lt;p&gt;
&#26680;&#40065;&#26834;&#20551;&#35774;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Kernel Robust Hypothesis Testing. (arXiv:2203.12777v2 [eess.SP] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26680;&#26041;&#27861;&#26500;&#36896;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#22312;&#36125;&#21494;&#26031;&#35774;&#32622;&#21644;Neyman-Pearson&#35774;&#32622;&#20013;&#20998;&#21035;&#30740;&#31350;&#20102;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#38169;&#35823;&#27010;&#29575;&#21644;&#25511;&#21046;&#38169;&#35823;&#27010;&#29575;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;MMD&#30340;&#19968;&#31995;&#21015;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#40065;&#26834;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#22312;&#38646;&#20551;&#35774;&#21644;&#22791;&#25321;&#20551;&#35774;&#19979;&#65292;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#34987;&#20551;&#35774;&#22312;&#26576;&#20123;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#65292;&#24182;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#27979;&#35797;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#34920;&#29616;&#26368;&#20248;&#12290;&#26412;&#25991;&#23558;&#20351;&#29992;&#26680;&#26041;&#27861;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26500;&#36896;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#21363;&#20197;&#26469;&#33258;&#38646;&#20551;&#35774;&#21644;&#22791;&#25321;&#20551;&#35774;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#32463;&#39564;&#20998;&#24067;&#20026;&#20013;&#24515;&#65292;&#24182;&#36890;&#36807;&#26680;&#22343;&#20540;&#23884;&#20837;&#30340;&#36317;&#31163;&#26469;&#32422;&#26463;&#65292;&#21363;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#65288;MMD&#65289;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#35774;&#32622;&#21644;Neyman-Pearson&#35774;&#32622;&#12290;&#23545;&#20110;&#36125;&#21494;&#26031;&#35774;&#32622;&#65292;&#21363;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#38169;&#35823;&#27010;&#29575;&#65292;&#24403;&#23383;&#27597;&#34920;&#26159;&#26377;&#38480;&#30340;&#26102;&#65292;&#39318;&#20808;&#24471;&#21040;&#20102;&#26368;&#20339;&#27979;&#35797;&#12290;&#24403;&#23383;&#27597;&#34920;&#26159;&#26080;&#38480;&#30340;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#36817;&#20284;&#26041;&#27861;&#26469;&#37327;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#38169;&#35823;&#27010;&#29575;&#12290;&#23545;&#20110;Neyman-Pearson&#35774;&#32622;&#65292;&#21363;&#30446;&#26631;&#26159;&#22312;&#26368;&#23567;&#21270;&#31532;&#20108;&#31867;&#38169;&#35823;&#27010;&#29575;&#30340;&#21516;&#26102;&#25511;&#21046;&#22312;&#32473;&#23450;&#27700;&#24179;&#19979;&#30340;&#31532;&#19968;&#31867;&#38169;&#35823;&#27010;&#29575;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;MMD&#30340;&#27979;&#35797;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#28176;&#36817;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of robust hypothesis testing is studied, where under the null and the alternative hypotheses, the data-generating distributions are assumed to be in some uncertainty sets, and the goal is to design a test that performs well under the worst-case distributions over the uncertainty sets. In this paper, uncertainty sets are constructed in a data-driven manner using kernel method, i.e., they are centered around empirical distributions of training samples from the null and alternative hypotheses, respectively; and are constrained via the distance between kernel mean embeddings of distributions in the reproducing kernel Hilbert space, i.e., maximum mean discrepancy (MMD). The Bayesian setting and the Neyman-Pearson setting are investigated. For the Bayesian setting where the goal is to minimize the worst-case error probability, an optimal test is firstly obtained when the alphabet is finite. When the alphabet is infinite, a tractable approximation is proposed to quantify the worst
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Transformer&#26500;&#24314;&#20165;&#20351;&#29992;&#27491;&#24120;&#30340;&#26085;&#24535;&#26465;&#30446;&#26469;&#35757;&#32451;&#26032;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#24418;&#24335;&#30340;&#25200;&#21160;&#26469;&#22686;&#24378;&#26085;&#24535;&#65292;&#20197;&#24212;&#23545;&#26085;&#24535;&#26469;&#28304;&#30340;&#24322;&#26500;&#24615;&#21644;&#32570;&#20047;&#26631;&#31614;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#38480;&#21046;&#65307;&#35813;&#27169;&#22411;&#36827;&#19968;&#27493;&#20351;&#29992;&#19968;&#31181;&#26377;&#38480;&#30340;&#26631;&#31614;&#26679;&#26412;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2203.10960</link><description>&lt;p&gt;
&#22522;&#20110;AI&#30340;&#26085;&#24535;&#20998;&#26512;&#22120;: &#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AI based Log Analyser: A Practical Approach. (arXiv:2203.10960v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.10960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Transformer&#26500;&#24314;&#20165;&#20351;&#29992;&#27491;&#24120;&#30340;&#26085;&#24535;&#26465;&#30446;&#26469;&#35757;&#32451;&#26032;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#24418;&#24335;&#30340;&#25200;&#21160;&#26469;&#22686;&#24378;&#26085;&#24535;&#65292;&#20197;&#24212;&#23545;&#26085;&#24535;&#26469;&#28304;&#30340;&#24322;&#26500;&#24615;&#21644;&#32570;&#20047;&#26631;&#31614;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#38480;&#21046;&#65307;&#35813;&#27169;&#22411;&#36827;&#19968;&#27493;&#20351;&#29992;&#19968;&#31181;&#26377;&#38480;&#30340;&#26631;&#31614;&#26679;&#26412;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#20998;&#26512;&#26159;&#36827;&#34892;&#25925;&#38556;&#25110;&#32593;&#32476;&#20107;&#20214;&#26816;&#27979;&#12289;&#35843;&#26597;&#21644;&#25216;&#26415;&#21462;&#35777;&#20998;&#26512;&#20197;&#23454;&#29616;&#31995;&#32479;&#21644;&#32593;&#32476;&#38887;&#24615;&#30340;&#37325;&#35201;&#27963;&#21160;&#12290;AI&#31639;&#27861;&#22312;&#26085;&#24535;&#20998;&#26512;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#21487;&#20197;&#22686;&#24378;&#36825;&#31181;&#22797;&#26434;&#21644;&#32321;&#37325;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#26085;&#24535;&#26469;&#28304;&#30340;&#24322;&#26500;&#24615;&#21644;&#27809;&#26377;&#25110;&#24456;&#23569;&#26631;&#31614;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#24403;&#36825;&#20123;&#26631;&#31614;&#21464;&#24471;&#21487;&#29992;&#26102;&#65292;&#38656;&#35201;&#26356;&#26032;&#20998;&#31867;&#22120;&#12290;&#36825;&#39033;&#22522;&#20110;&#23454;&#36341;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;Transformer&#26500;&#24314;&#26469;&#20165;&#20351;&#29992;&#27491;&#24120;&#30340;&#26085;&#24535;&#26465;&#30446;&#35757;&#32451;&#26032;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#22810;&#31181;&#24418;&#24335;&#30340;&#25200;&#21160;&#26469;&#22686;&#24378;&#26085;&#24535;&#65292;&#20197;&#20316;&#20026;&#33258;&#30417;&#30563;&#35757;&#32451;&#29992;&#20110;&#29305;&#24449;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#36827;&#19968;&#27493;&#20351;&#29992;&#19968;&#31181;&#26377;&#38480;&#30340;&#26631;&#31614;&#26679;&#26412;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#27169;&#20223;&#23454;&#38469;&#24773;&#20917;&#19979;&#26377;&#26631;&#31614;&#21487;&#29992;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of logs is a vital activity undertaken for fault or cyber incident detection, investigation and technical forensics analysis for system and cyber resilience. The potential application of AI algorithms for Log analysis could augment such complex and laborious tasks. However, such solution has its constraints the heterogeneity of log sources and limited to no labels for training a classifier. When such labels become available, the need for the classifier to be updated. This practice-based research seeks to address these challenges with the use of Transformer construct to train a new model with only normal log entries. Log augmentation through multiple forms of perturbation is applied as a form of self-supervised training for feature learning. The model is further finetuned using a form of reinforcement learning with a limited set of label samples to mimic real-world situation with the availability of labels. The experimental results of our model construct show promise with c
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#35889;&#23646;&#24615;&#21435;&#38500;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31070;&#32463;&#34920;&#31034;&#20013;&#21435;&#38500;&#31169;&#26377;&#25110;&#20445;&#25252;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#26356;&#22909;&#30340;&#20027;&#35201;&#20219;&#21153;&#24615;&#33021;&#65292;&#21516;&#26102;&#21482;&#38656;&#23569;&#37327;&#30340;&#34987;&#20445;&#25252;&#23646;&#24615;&#25968;&#25454;&#21363;&#21487;&#21435;&#38500;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2203.07893</link><description>&lt;p&gt;
&#40644;&#37329;&#24182;&#38750;&#19968;&#20999;&#65306;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20445;&#25252;&#23646;&#24615;&#20449;&#24687;&#30340;&#20809;&#35889;&#21435;&#38500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information. (arXiv:2203.07893v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#35889;&#23646;&#24615;&#21435;&#38500;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31070;&#32463;&#34920;&#31034;&#20013;&#21435;&#38500;&#31169;&#26377;&#25110;&#20445;&#25252;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#26356;&#22909;&#30340;&#20027;&#35201;&#20219;&#21153;&#24615;&#33021;&#65292;&#21516;&#26102;&#21482;&#38656;&#23569;&#37327;&#30340;&#34987;&#20445;&#25252;&#23646;&#24615;&#25968;&#25454;&#21363;&#21487;&#21435;&#38500;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;&#20809;&#35889;&#23646;&#24615;&#21435;&#38500;&#26041;&#27861;; SAL&#65289;&#65292;&#29992;&#20110;&#20174;&#31070;&#32463;&#34920;&#31034;&#20013;&#21435;&#38500;&#31169;&#26377;&#25110;&#20445;&#25252;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#30697;&#38453;&#20998;&#35299;&#23558;&#36755;&#20837;&#34920;&#31034;&#25237;&#24433;&#21040;&#19982;&#20445;&#25252;&#20449;&#24687;&#21327;&#26041;&#24046;&#36739;&#23567;&#30340;&#26041;&#21521;&#19978;&#65292;&#32780;&#19981;&#26159;&#20687;&#22240;&#24335;&#20998;&#35299;&#26041;&#27861;&#37027;&#26679;&#26368;&#22823;&#21270;&#21327;&#26041;&#24046;&#12290;&#25105;&#20204;&#20174;&#32447;&#24615;&#20449;&#24687;&#21024;&#38500;&#24320;&#22987;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20869;&#26680;&#23558;&#31639;&#27861;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#20449;&#24687;&#21024;&#38500;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21024;&#38500;&#20445;&#25252;&#20449;&#24687;&#21518;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20445;&#30041;&#20102;&#26356;&#22909;&#30340;&#20027;&#35201;&#20219;&#21153;&#24615;&#33021;&#65292;&#32780;&#19981;&#20687;&#20197;&#21069;&#30340;&#24037;&#20316;&#37027;&#26679;&#21024;&#38500;&#35813;&#20449;&#24687;&#21066;&#24369;&#20102;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#21482;&#38656;&#23569;&#37327;&#30340;&#34987;&#20445;&#25252;&#23646;&#24615;&#25968;&#25454;&#21363;&#21487;&#21435;&#38500;&#26377;&#20851;&#36825;&#20123;&#23646;&#24615;&#30340;&#20449;&#24687;&#65292;&#36825;&#38477;&#20302;&#20102;&#25935;&#24863;&#25968;&#25454;&#30340;&#26292;&#38706;&#65292;&#24182;&#26356;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#12290;&#21487;&#20197;&#22312;https://github.com/jasonshaoshun/SAL&#19978;&#25214;&#21040;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a simple and effective method (Spectral Attribute removaL; SAL) to remove private or guarded information from neural representations. Our method uses matrix decomposition to project the input representations into directions with reduced covariance with the guarded information rather than maximal covariance as factorization methods normally use. We begin with linear information removal and proceed to generalize our algorithm to the case of nonlinear information removal using kernels. Our experiments demonstrate that our algorithm retains better main task performance after removing the guarded information compared to previous work. In addition, our experiments demonstrate that we need a relatively small amount of guarded attribute data to remove information about these attributes, which lowers the exposure to sensitive data and is more suitable for low-resource scenarios. Code is available at https://github.com/jasonshaoshun/SAL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36328;&#23618;&#27880;&#24847;&#21147;&#65288;ACLA&#65289;&#27169;&#22359;&#65292;&#23427;&#21487;&#20197;&#27719;&#24635;&#19981;&#21516;&#23618;&#27425;&#30340;&#29305;&#24449;&#20449;&#24687;&#29992;&#20110;&#38750;&#23616;&#37096;&#27880;&#24847;&#21147;&#30340;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#24674;&#22797;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.03619</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#36328;&#23618;&#27880;&#24847;&#21147;&#26426;&#21046;&#29992;&#20110;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Adaptive Cross-Layer Attention for Image Restoration. (arXiv:2203.03619v3 [eess.IV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36328;&#23618;&#27880;&#24847;&#21147;&#65288;ACLA&#65289;&#27169;&#22359;&#65292;&#23427;&#21487;&#20197;&#27719;&#24635;&#19981;&#21516;&#23618;&#27425;&#30340;&#29305;&#24449;&#20449;&#24687;&#29992;&#20110;&#38750;&#23616;&#37096;&#27880;&#24847;&#21147;&#30340;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#24674;&#22797;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#23616;&#37096;&#27880;&#24847;&#21147;&#27169;&#22359;&#34987;&#35777;&#26126;&#22312;&#22270;&#20687;&#24674;&#22797;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#38750;&#23616;&#37096;&#27880;&#24847;&#21147;&#21333;&#29420;&#22788;&#29702;&#27599;&#20010;&#23618;&#27425;&#30340;&#29305;&#24449;&#65292;&#21487;&#33021;&#20250;&#38169;&#36807;&#19981;&#21516;&#23618;&#27425;&#20043;&#38388;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21487;&#20197;&#27719;&#24635;&#19981;&#21516;&#23618;&#27425;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#36328;&#23618;&#27880;&#24847;&#21147;&#65288;ACLA&#65289;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#22320;&#23558;&#36825;&#31181;&#27880;&#24847;&#21147;&#35774;&#35745;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#20013;&#12290;ACLA&#25552;&#20379;&#20102;&#20004;&#31181;&#33258;&#36866;&#24212;&#35774;&#35745;&#65306;&#65288;1&#65289;&#22312;&#27599;&#19968;&#23618;&#33258;&#36866;&#24212;&#36873;&#25321;&#38750;&#23616;&#37096;&#27880;&#24847;&#21147;&#30340;&#38190;&#65307;&#65288;2&#65289;&#33258;&#21160;&#25628;&#32034;ACLA&#27169;&#22359;&#30340;&#25554;&#20837;&#20301;&#32622;&#12290;&#36890;&#36807;&#36825;&#20004;&#20010;&#33258;&#36866;&#24212;&#35774;&#35745;&#65292;ACLA&#21160;&#24577;&#22320;&#36873;&#25321;&#28789;&#27963;&#25968;&#37327;&#30340;&#38190;&#26469;&#27719;&#24635;&#21069;&#19968;&#23618;&#27425;&#30340;&#29305;&#24449;&#36827;&#34892;&#38750;&#23616;&#37096;&#27880;&#24847;&#21147;&#30340;&#22788;&#29702;&#65292;&#21516;&#26102;&#20445;&#25345;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-local attention module has been proven to be crucial for image restoration. Conventional non-local attention processes features of each layer separately, so it risks missing correlation between features among different layers. To address this problem, we aim to design attention modules that aggregate information from different layers. Instead of finding correlated key pixels within the same layer, each query pixel is encouraged to attend to key pixels at multiple previous layers of the network. In order to efficiently embed such attention design into neural network backbones, we propose a novel Adaptive Cross-Layer Attention (ACLA) module. Two adaptive designs are proposed for ACLA: (1) adaptively selecting the keys for non-local attention at each layer; (2) automatically searching for the insertion locations for ACLA modules. By these two adaptive designs, ACLA dynamically selects a flexible number of keys to be aggregated for non-local attention at previous layer while maintainin
&lt;/p&gt;</description></item><item><title>SpinGNN&#26159;&#19968;&#31181;&#33258;&#26059;&#30456;&#20851;&#30340;&#30456;&#20114;&#20316;&#29992;&#21183;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#30913;&#24615;&#31995;&#32479;&#30340;&#28023;&#26862;&#22561;&#22411;&#33258;&#26059;-&#26230;&#26684;&#30456;&#20114;&#20316;&#29992;&#21644;&#39640;&#38454;&#33258;&#26059;-&#26230;&#26684;&#32806;&#21512;&#65292;&#21516;&#26102;&#25104;&#21151;&#22320;&#27169;&#25311;&#20102;BiFeO3&#20013;&#24494;&#22937;&#30340;&#33258;&#26059;-&#26230;&#26684;&#32806;&#21512;&#12290;</title><link>http://arxiv.org/abs/2203.02853</link><description>&lt;p&gt;
&#33258;&#26059;&#30456;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21183;&#29992;&#20110;&#30913;&#24615;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
Spin-Dependent Graph Neural Network Potential for Magnetic Materials. (arXiv:2203.02853v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02853
&lt;/p&gt;
&lt;p&gt;
SpinGNN&#26159;&#19968;&#31181;&#33258;&#26059;&#30456;&#20851;&#30340;&#30456;&#20114;&#20316;&#29992;&#21183;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#30913;&#24615;&#31995;&#32479;&#30340;&#28023;&#26862;&#22561;&#22411;&#33258;&#26059;-&#26230;&#26684;&#30456;&#20114;&#20316;&#29992;&#21644;&#39640;&#38454;&#33258;&#26059;-&#26230;&#26684;&#32806;&#21512;&#65292;&#21516;&#26102;&#25104;&#21151;&#22320;&#27169;&#25311;&#20102;BiFeO3&#20013;&#24494;&#22937;&#30340;&#33258;&#26059;-&#26230;&#26684;&#32806;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#21457;&#23637;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20998;&#23376;&#21644;&#26230;&#20307;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20026;&#30913;&#24615;&#31995;&#32479;&#21019;&#24314;&#26082;&#32771;&#34385;&#30913;&#30697;&#21448;&#32771;&#34385;&#32467;&#26500;&#33258;&#30001;&#24230;&#30340;&#30456;&#20114;&#20316;&#29992;&#21183;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;SpinGNN&#65292;&#19968;&#31181;&#33258;&#26059;&#30456;&#20851;&#30340;&#30456;&#20114;&#20316;&#29992;&#21183;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#25551;&#36848;&#30913;&#24615;&#31995;&#32479;&#12290;SpinGNN&#30001;&#20004;&#31181;&#31867;&#22411;&#30340;&#36793;&#32536;GNN&#32452;&#25104;&#65306;&#28023;&#26862;&#22561;&#36793;&#32536;GNN&#65288;HEGNN&#65289;&#21644;&#33258;&#26059;&#36317;&#31163;&#36793;&#32536;GNN&#65288;SEGNN&#65289;&#12290;HEGNN&#26088;&#22312;&#25429;&#25417;&#28023;&#26862;&#22561;&#22411;&#33258;&#26059;-&#26230;&#26684;&#30456;&#20114;&#20316;&#29992;&#65292;&#32780;SEGNN&#21017;&#20934;&#30830;&#22320;&#27169;&#25311;&#22810;&#20307;&#21644;&#39640;&#38454;&#33258;&#26059;-&#26230;&#26684;&#32806;&#21512;&#12290;SpinGNN&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#20854;&#22312;&#39640;&#38454;&#33258;&#26059;&#21704;&#23494;&#39039;&#37327;&#21644;&#20004;&#20010;&#22797;&#26434;&#33258;&#26059;-&#26230;&#26684;&#21704;&#23494;&#39039;&#37327;&#30340;&#25311;&#21512;&#31934;&#24230;&#26041;&#38754;&#30340;&#21331;&#36234;&#34920;&#29616;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#23427;&#25104;&#21151;&#22320;&#27169;&#25311;&#20102;BiFeO3&#20013;&#24494;&#22937;&#30340;&#33258;&#26059;-&#26230;&#26684;&#32806;&#21512;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#33258;&#26059;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of machine learning interatomic potentials has immensely contributed to the accuracy of simulations of molecules and crystals. However, creating interatomic potentials for magnetic systems that account for both magnetic moments and structural degrees of freedom remains a challenge. This work introduces SpinGNN, a spin-dependent interatomic potential approach that employs the graph neural network (GNN) to describe magnetic systems. SpinGNN consists of two types of edge GNNs: Heisenberg edge GNN (HEGNN) and spin-distance edge GNN (SEGNN). HEGNN is tailored to capture Heisenberg-type spin-lattice interactions, while SEGNN accurately models multi-body and high-order spin-lattice coupling. The effectiveness of SpinGNN is demonstrated by its exceptional precision in fitting a high-order spin Hamiltonian and two complex spin-lattice Hamiltonians with great precision. Furthermore, it successfully models the subtle spin-lattice coupling in BiFeO3 and performs large-scale spin-la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#24403;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#36739;&#22823;&#26102;&#65292;&#22320;&#29702;&#23616;&#37096;&#21270;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#30340;&#35757;&#32451;&#36827;&#20837;&#25042;&#24816;&#38454;&#27573;&#65292;&#38480;&#21046;&#20102;&#21442;&#25968;&#21464;&#21270;&#36895;&#29575;&#24182;&#20445;&#35777;&#20102;&#30456;&#24212;&#37327;&#23376;&#27169;&#22411;&#30340;&#32447;&#24615;&#36924;&#36817;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2202.08232</link><description>&lt;p&gt;
&#37327;&#23376;&#25042;&#24816;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Quantum Lazy Training. (arXiv:2202.08232v6 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#24403;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#36739;&#22823;&#26102;&#65292;&#22320;&#29702;&#23616;&#37096;&#21270;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#30340;&#35757;&#32451;&#36827;&#20837;&#25042;&#24816;&#38454;&#27573;&#65292;&#38480;&#21046;&#20102;&#21442;&#25968;&#21464;&#21270;&#36895;&#29575;&#24182;&#20445;&#35777;&#20102;&#30456;&#24212;&#37327;&#23376;&#27169;&#22411;&#30340;&#32447;&#24615;&#36924;&#36817;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#36229;&#21442;&#25968;&#27169;&#22411;&#20989;&#25968;&#26102;&#65292;&#26377;&#26102;&#21442;&#25968;&#19981;&#20250;&#21457;&#29983;&#26174;&#30528;&#21464;&#21270;&#65292;&#20445;&#25345;&#25509;&#36817;&#20854;&#21021;&#22987;&#20540;&#12290;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#25042;&#24816;&#35757;&#32451;&#65292;&#24182;&#28608;&#21457;&#20102;&#23545;&#27169;&#22411;&#20989;&#25968;&#22312;&#21021;&#22987;&#21442;&#25968;&#21608;&#22260;&#30340;&#32447;&#24615;&#36924;&#36817;&#30340;&#32771;&#34385;&#12290;&#22312;&#25042;&#24816;&#38454;&#27573;&#65292;&#32447;&#24615;&#36924;&#36817;&#27169;&#25311;&#20102;&#21442;&#25968;&#21270;&#20989;&#25968;&#30340;&#34892;&#20026;&#65292;&#20854;&#30456;&#20851;&#20869;&#26680;&#31216;&#20026;&#20999;&#21521;&#20869;&#26680;&#65292;&#25351;&#23450;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#24050;&#30693;&#22823;&#23485;&#24230;&#65288;&#32463;&#20856;&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#20250;&#20986;&#29616;&#25042;&#24816;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23637;&#31034;&#20102;&#22312;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#36739;&#22823;&#26102;&#65292;&#22320;&#29702;&#23616;&#37096;&#21270;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#30340;&#35757;&#32451;&#36827;&#20837;&#25042;&#24816;&#38454;&#27573;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#22320;&#29702;&#23616;&#37096;&#21270;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#21442;&#25968;&#21464;&#21270;&#36895;&#29575;&#30340;&#38480;&#21046;&#65292;&#20197;&#21450;&#20854;&#30456;&#20851;&#37327;&#23376;&#27169;&#22411;&#30340;&#32447;&#24615;&#36924;&#36817;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the training of over-parameterized model functions via gradient descent, sometimes the parameters do not change significantly and remain close to their initial values. This phenomenon is called lazy training, and motivates consideration of the linear approximation of the model function around the initial parameters. In the lazy regime, this linear approximation imitates the behavior of the parameterized function whose associated kernel, called the tangent kernel, specifies the training performance of the model. Lazy training is known to occur in the case of (classical) neural networks with large widths. In this paper, we show that the training of geometrically local parameterized quantum circuits enters the lazy regime for large numbers of qubits. More precisely, we prove bounds on the rate of changes of the parameters of such a geometrically local parameterized quantum circuit in the training process, and on the precision of the linear approximation of the associated quantum model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#22635;&#34917;&#20102;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#21457;&#29616;&#22312;&#35813;&#22330;&#26223;&#19979;&#20998;&#24067;&#24335;&#21518;&#38376;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2202.03195</link><description>&lt;p&gt;
&#26356;&#22810;&#26159;&#26356;&#22909;&#30340;&#65288;&#22312;&#24456;&#22810;&#24773;&#20917;&#19979;&#65289;&#65306;&#20851;&#20110;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
More is Better (Mostly): On the Backdoor Attacks in Federated Graph Neural Networks. (arXiv:2202.03195v5 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#22635;&#34917;&#20102;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#21457;&#29616;&#22312;&#35813;&#22330;&#26223;&#19979;&#20998;&#24067;&#24335;&#21518;&#38376;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31867;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#39046;&#22495;&#20449;&#24687;&#22788;&#29702;&#26041;&#27861;&#12290;&#30001;&#20110;&#20854;&#20248;&#36234;&#30340;&#23398;&#20064;&#22797;&#26434;&#22270;&#25968;&#25454;&#34920;&#31034;&#30340;&#33021;&#21147;&#65292;GNNs&#26368;&#36817;&#24050;&#25104;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#20998;&#26512;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#21644;&#30417;&#31649;&#38480;&#21046;&#65292;&#20013;&#24515;&#21270;&#30340;GNNs&#22312;&#22788;&#29702;&#19982;&#38544;&#31169;&#26377;&#20851;&#30340;&#25968;&#25454;&#22330;&#26223;&#26102;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#21442;&#19982;&#26041;&#38656;&#35201;&#21327;&#20316;&#35757;&#32451;&#20849;&#20139;&#20840;&#23616;&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#35774;&#32622;&#20013;&#12290;&#23613;&#31649;&#26377;&#19968;&#20123;&#30740;&#31350;&#23558;FL&#24212;&#29992;&#20110;&#35757;&#32451;GNNs&#65288;&#32852;&#37030;GNNs&#65289;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#23427;&#20204;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#32852;&#37030;GNNs&#20013;&#36827;&#34892;&#20004;&#31181;&#31867;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#21363;&#38598;&#20013;&#24335;&#21518;&#38376;&#25915;&#20987;&#65288;CBA&#65289;&#21644;&#20998;&#24067;&#24335;&#21518;&#38376;&#25915;&#20987;&#65288;DBA&#65289;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#24773;&#20917;&#19979;&#65292;DBA&#25915;&#20987;&#25104;&#21151;&#29575;&#27604;CBA&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a class of deep learning-based methods for processing graph domain information. GNNs have recently become a widely used graph analysis method due to their superior ability to learn representations for complex graph data. However, due to privacy concerns and regulation restrictions, centralized GNNs can be difficult to apply to data-sensitive scenarios. Federated learning (FL) is an emerging technology developed for privacy-preserving settings when several parties need to train a shared global model collaboratively. Although several research works have applied FL to train GNNs (Federated GNNs), there is no research on their robustness to backdoor attacks.  This paper bridges this gap by conducting two types of backdoor attacks in Federated GNNs: centralized backdoor attacks (CBA) and distributed backdoor attacks (DBA). Our experiments show that the DBA attack success rate is higher than CBA in almost all evaluated cases. For CBA, the attack success rate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#30417;&#30563;&#65288;SUD&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#30417;&#30563;&#37325;&#24314;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#21046;&#20316;&#29305;&#23450;&#20110;&#25104;&#20687;&#39046;&#22495;&#30340;&#21487;&#24494;&#27491;&#21017;&#21270;&#22120;&#12290;</title><link>http://arxiv.org/abs/2202.02952</link><description>&lt;p&gt;
&#21435;&#22122;&#30417;&#30563;&#12290; (arXiv:2202.02952v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Supervision by Denoising. (arXiv:2202.02952v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#30417;&#30563;&#65288;SUD&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#30417;&#30563;&#37325;&#24314;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#21046;&#20316;&#29305;&#23450;&#20110;&#25104;&#20687;&#39046;&#22495;&#30340;&#21487;&#24494;&#27491;&#21017;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#22270;&#20687;&#37325;&#24314;&#27169;&#22411;&#65292;&#20363;&#22914;&#22522;&#20110; U-Net &#30340;&#27169;&#22411;&#65292;&#22914;&#26524;&#35201;&#20445;&#35777;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21017;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#22270;&#20687;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#25104;&#20687;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#20855;&#26377;&#20687;&#32032;&#25110;&#20307;&#32032;&#32423;&#21035;&#31934;&#24230;&#30340;&#26631;&#35760;&#25968;&#25454;&#24456;&#23569;&#12290;&#22312;&#21307;&#23398;&#25104;&#20687;&#31561;&#39046;&#22495;&#65292;&#30001;&#20110;&#19981;&#23384;&#22312;&#21333;&#19968;&#30340;&#26631;&#20934;&#26631;&#31614;&#65292;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#22823;&#37327;&#30340;&#37325;&#22797;&#21464;&#24322;&#24615;&#65292;&#22240;&#27492;&#35757;&#32451;&#37325;&#24314;&#32593;&#32476;&#20174;&#24102;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#30340;&#31034;&#20363;&#20013;&#23398;&#20064;&#20197;&#26356;&#22909;&#22320;&#27867;&#21270;&#65288;&#31216;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#65289;&#65292;&#26159;&#19968;&#20010;&#23454;&#36341;&#21644;&#29702;&#35770;&#19978;&#30340;&#38382;&#39064;&#12290; &#28982;&#32780;&#65292;&#38024;&#23545;&#22270;&#20687;&#37325;&#24314;&#30340;&#20256;&#32479;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#38024;&#23545;&#26576;&#20123;&#32473;&#23450;&#25104;&#20687;&#38382;&#39064;&#30340;&#21487;&#24494;&#27491;&#21017;&#21270;&#22120;&#65292;&#36825;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#21435;&#22122;&#30417;&#30563;&#8221;&#65288;SUD&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#30417;&#30563;&#37325;&#24314;&#27169;&#22411;&#65292;&#21516;&#26102;&#36991;&#20813;&#25163;&#21160;&#21046;&#20316;&#29305;&#23450;&#20110;&#25104;&#20687;&#39046;&#22495;&#30340;&#21487;&#24494;&#27491;&#21017;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based image reconstruction models, such as those based on the U-Net, require a large set of labeled images if good generalization is to be guaranteed. In some imaging domains, however, labeled data with pixel- or voxel-level label accuracy are scarce due to the cost of acquiring them. This problem is exacerbated further in domains like medical imaging, where there is no single ground truth label, resulting in large amounts of repeat variability in the labels. Therefore, training reconstruction networks to generalize better by learning from both labeled and unlabeled examples (called semi-supervised learning) is problem of practical and theoretical interest. However, traditional semi-supervised learning methods for image reconstruction often necessitate handcrafting a differentiable regularizer specific to some given imaging problem, which can be extremely time-consuming. In this work, we propose "supervision by denoising" (SUD), a framework that enables us to supervise reconst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#21098;&#20999;Gossip&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#35777;&#26126;&#25910;&#25947;&#21040;&#38750;&#20984;&#30446;&#26631;&#30340;$O(\delta_{max} \zeta^2 /\gamma^2)$&#37051;&#22495;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#22823;&#37327;&#25915;&#20987;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2202.01545</link><description>&lt;p&gt;
&#21098;&#20999;Gossip&#22312;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Byzantine-Robust Decentralized Learning via ClippedGossip. (arXiv:2202.01545v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#21098;&#20999;Gossip&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#35777;&#26126;&#25910;&#25947;&#21040;&#38750;&#20984;&#30446;&#26631;&#30340;$O(\delta_{max} \zeta^2 /\gamma^2)$&#37051;&#22495;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#22823;&#37327;&#25915;&#20987;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20219;&#24847;&#36890;&#20449;&#22270;&#19978;&#36827;&#34892;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20998;&#25955;&#24335;&#35757;&#32451;&#30340;&#33392;&#24040;&#20219;&#21153;&#12290;&#19982;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#26381;&#21153;&#22120;&#36827;&#34892;&#36890;&#20449;&#30340;&#26041;&#24335;&#19981;&#21516;&#65292;&#20998;&#25955;&#24335;&#29615;&#22659;&#20013;&#30340;workers&#21482;&#33021;&#19982;&#23427;&#20204;&#30340;&#37051;&#23621;&#20132;&#27969;&#65292;&#36825;&#20351;&#24471;&#36798;&#25104;&#20849;&#35782;&#21644;&#21327;&#20316;&#35757;&#32451;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25308;&#21344;&#24237;&#40065;&#26834;&#20849;&#35782;&#21644;&#20248;&#21270;&#30340;&#21098;&#20999;Gossip&#31639;&#27861;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#21487;&#20197;&#35777;&#26126;&#25910;&#25947;&#21040;&#38750;&#20984;&#30446;&#26631;&#30340;$O(\delta_{max} \zeta^2 /\gamma^2)$&#37051;&#22495;&#30340;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22823;&#37327;&#25915;&#20987;&#19979;&#35777;&#26126;&#20102;&#21098;&#20999;Gossip&#30340;&#40723;&#33310;&#20154;&#24515;&#30340;&#23454;&#35777;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the challenging task of Byzantine-robust decentralized training on arbitrary communication graphs. Unlike federated learning where workers communicate through a server, workers in the decentralized environment can only talk to their neighbors, making it harder to reach consensus and benefit from collaborative training. To address these issues, we propose a ClippedGossip algorithm for Byzantine-robust consensus and optimization, which is the first to provably converge to a $O(\delta_{\max}\zeta^2/\gamma^2)$ neighborhood of the stationary point for non-convex objectives under standard assumptions. Finally, we demonstrate the encouraging empirical performance of ClippedGossip under a large number of attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;DDPG&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#28145;&#24230;&#28145;&#24230;&#23637;&#24320;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#36755;&#20837;&#12290;&#35813;&#26694;&#26550;&#34987;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2201.08477</link><description>&lt;p&gt;
DDPG&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#28145;&#24230;&#28145;&#24230;&#23637;&#24320;&#19982;&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;&#30340;&#20449;&#36947;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DDPG-Driven Deep-Unfolding with Adaptive Depth for Channel Estimation with Sparse Bayesian Learning. (arXiv:2201.08477v3 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;DDPG&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#28145;&#24230;&#28145;&#24230;&#23637;&#24320;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#36755;&#20837;&#12290;&#35813;&#26694;&#26550;&#34987;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23637;&#24320;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30001;&#20110;&#36798;&#21040;&#20102;&#30456;&#23545;&#20302;&#22797;&#26434;&#24230;&#19979;&#28385;&#24847;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#28145;&#24230;&#23637;&#24320;NN&#23545;&#20110;&#25152;&#26377;&#36755;&#20837;&#37117;&#34987;&#38480;&#21046;&#22312;&#22266;&#23450;&#28145;&#24230;&#19978;&#12290;&#28982;&#32780;&#65292;&#25910;&#25947;&#25152;&#38656;&#30340;&#26368;&#20339;&#23618;&#25968;&#38543;&#30528;&#19981;&#21516;&#36755;&#20837;&#32780;&#21464;&#21270;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;DDPG&#39537;&#21160;&#30340;&#36866;&#24212;&#28145;&#24230;&#28145;&#24230;&#23637;&#24320;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#36755;&#20837;&#65292;&#20854;&#20013;&#28145;&#24230;&#23637;&#24320;NN&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26159;&#30001;DDPG&#23398;&#20064;&#30340;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#30001;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26356;&#26032;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#28145;&#24230;&#23637;&#24320;NN&#30340;&#20248;&#21270;&#21464;&#37327;&#12289;&#21487;&#35757;&#32451;&#21442;&#25968;&#21644;&#20307;&#31995;&#32467;&#26500;&#34987;&#35774;&#35745;&#20026;DDPG&#30340;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#29366;&#24577;&#36716;&#31227;&#65292;&#20998;&#21035;&#12290;&#28982;&#21518;&#65292;&#23558;&#35813;&#26694;&#26550;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#30340;&#34920;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep-unfolding neural networks (NNs) have received great attention since they achieve satisfactory performance with relatively low complexity. Typically, these deep-unfolding NNs are restricted to a fixed-depth for all inputs. However, the optimal number of layers required for convergence changes with different inputs. In this paper, we first develop a framework of deep deterministic policy gradient (DDPG)-driven deep-unfolding with adaptive depth for different inputs, where the trainable parameters of deep-unfolding NN are learned by DDPG, rather than updated by the stochastic gradient descent algorithm directly. Specifically, the optimization variables, trainable parameters, and architecture of deep-unfolding NN are designed as the state, action, and state transition of DDPG, respectively. Then, this framework is employed to deal with the channel estimation problem in massive multiple-input multiple-output systems. Specifically, first of all we formulate the channel estimation proble
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#28145;&#24230;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20013;&#36807;&#20110;&#33258;&#20449;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23618;&#26469;&#36991;&#20813;&#20256;&#32479;&#30340;&#39044;&#27979;&#23618;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#20551;&#38451;&#24615;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;</title><link>http://arxiv.org/abs/2112.01360</link><description>&lt;p&gt;
&#36335;&#29992;&#25143;&#26816;&#27979;&#30340;&#27010;&#29575;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Approach for Road-Users Detection. (arXiv:2112.01360v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#28145;&#24230;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20013;&#36807;&#20110;&#33258;&#20449;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23618;&#26469;&#36991;&#20813;&#20256;&#32479;&#30340;&#39044;&#27979;&#23618;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#20551;&#38451;&#24615;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#24847;&#21619;&#30528;&#23545;&#35821;&#20041;&#23545;&#35937;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#36890;&#24120;&#26159;&#22478;&#24066;&#39550;&#39542;&#29615;&#22659;&#30340;&#29305;&#33394;&#65292;&#22914;&#34892;&#20154;&#21644;&#36710;&#36742;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#26631;&#26816;&#27979;&#20013;&#23384;&#22312;&#20551;&#38451;&#24615;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#24102;&#26377;&#36807;&#20110;&#33258;&#20449;&#30340;&#24471;&#20998;&#12290;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20854;&#20182;&#20851;&#38190;&#30340;&#26426;&#22120;&#20154;&#24863;&#30693;&#39046;&#22495;&#65292;&#36825;&#26159;&#38750;&#24120;&#19981;&#24076;&#26395;&#30475;&#21040;&#30340;&#65292;&#22240;&#20026;&#28041;&#21450;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#20013;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#23618;&#65292;&#21521;&#28145;&#24230;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#20013;&#28155;&#21152;&#36825;&#31181;&#27010;&#29575;&#23618;&#12290;&#24314;&#35758;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#20256;&#32479;&#30340;Sigmoid&#25110;Softmax&#39044;&#27979;&#23618;&#65292;&#36825;&#20123;&#23618;&#36890;&#24120;&#20250;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#33021;&#22815;&#20943;&#23569;&#20551;&#38451;&#24615;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#30495;&#38451;&#24615;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;2D-KITTI&#30446;&#26631;&#26816;&#27979;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#20351;&#29992;&#20102;YOLOV4&#21644;S&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection in autonomous driving applications implies that the detection and tracking of semantic objects are commonly native to urban driving environments, as pedestrians and vehicles. One of the major challenges in state-of-the-art deep-learning based object detection are false positives which occur with overconfident scores. This is highly undesirable in autonomous driving and other critical robotic-perception domains because of safety concerns. This paper proposes an approach to alleviate the problem of overconfident predictions by introducing a novel probabilistic layer to deep object detection networks in testing. The suggested approach avoids the traditional Sigmoid or Softmax prediction layer which often produces overconfident predictions. It is demonstrated that the proposed technique reduces overconfidence in the false positives without degrading the performance on the true positives. The approach is validated on the 2D-KITTI objection detection through the YOLOV4 and S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;Transformer&#27169;&#22411;&#65292;&#23558;&#36829;&#32422;&#39044;&#27979;&#38382;&#39064;&#20316;&#20026;&#22810;&#26631;&#31614;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#28909;&#22270;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25439;&#22833;&#20989;&#25968;&#21644;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#36890;&#36947;&#26550;&#26500;&#65292;&#20197;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2111.09902</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#39044;&#27979;&#20013;&#31561;&#24066;&#20540;&#20844;&#21496;&#36829;&#32422;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
A transformer-based model for default prediction in mid-cap corporate markets. (arXiv:2111.09902v4 [q-fin.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;Transformer&#27169;&#22411;&#65292;&#23558;&#36829;&#32422;&#39044;&#27979;&#38382;&#39064;&#20316;&#20026;&#22810;&#26631;&#31614;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#28909;&#22270;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25439;&#22833;&#20989;&#25968;&#21644;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#36890;&#36947;&#26550;&#26500;&#65292;&#20197;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24066;&#20540;&#20302;&#20110;100&#20159;&#32654;&#20803;&#30340;&#19978;&#24066;&#20844;&#21496;&#65292;&#21033;&#29992;30&#24180;&#30340;&#32654;&#22269;&#20013;&#31561;&#24066;&#20540;&#20844;&#21496;&#25968;&#25454;&#38598;&#65292;&#39044;&#27979;&#20854;&#20013;&#26399;&#20869;&#30340;&#36829;&#32422;&#27010;&#29575;&#65292;&#24182;&#30830;&#23450;&#36129;&#29486;&#36829;&#32422;&#39118;&#38505;&#30340;&#25968;&#25454;&#26469;&#28304;&#65288;&#22522;&#26412;&#38754;&#12289;&#24066;&#22330;&#25110;&#23450;&#20215;&#25968;&#25454;&#65289;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#20316;&#20026;&#22810;&#26631;&#31614;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#21644;&#27880;&#24847;&#21147;&#28909;&#22270;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20248;&#21270;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#33258;&#23450;&#20041;&#25439;&#22833;&#20989;&#25968;&#21644;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#36890;&#36947;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study mid-cap companies, i.e. publicly traded companies with less than US $10 billion in market capitalisation. Using a large dataset of US mid-cap companies observed over 30 years, we look to predict the default probability term structure over the medium term and understand which data sources (i.e. fundamental, market or pricing data) contribute most to the default risk. Whereas existing methods typically require that data from different time periods are first aggregated and turned into cross-sectional features, we frame the problem as a multi-label time-series classification problem. We adapt transformer models, a state-of-the-art deep learning model emanating from the natural language processing domain, to the credit risk modelling setting. We also interpret the predictions of these models using attention heat maps. To optimise the model further, we present a custom loss function for multi-label classification and a novel multi-channel architecture with differentia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;VC&#32500;&#20026;$d$&#30340;&#26377;&#21521;&#25311;&#38453;&#30340;&#22797;&#21512;&#20307;&#30340;&#25299;&#25169;&#30340;&#26631;&#35760;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#20026;&#35745;&#31639;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#26679;&#26412;&#21387;&#32553;&#29468;&#24819;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2110.15168</link><description>&lt;p&gt;
&#26377;&#21521;&#25311;&#38453;&#30340;&#22797;&#21512;&#20307;&#30340;&#26631;&#35760;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Labeled sample compression schemes for complexes of oriented matroids. (arXiv:2110.15168v3 [math.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;VC&#32500;&#20026;$d$&#30340;&#26377;&#21521;&#25311;&#38453;&#30340;&#22797;&#21512;&#20307;&#30340;&#25299;&#25169;&#30340;&#26631;&#35760;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#20026;&#35745;&#31639;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#26679;&#26412;&#21387;&#32553;&#29468;&#24819;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#22823;&#23567;&#20026;$d$&#30340;&#36866;&#24403;&#30340;&#26631;&#35760;&#26679;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#29992;&#20110;VC&#32500;&#20026;$d$&#30340;&#26377;&#21521;&#25311;&#38453;&#30340;&#22797;&#21512;&#20307;&#65288;&#31616;&#31216;COM&#65289;&#30340;&#25299;&#25169;&#12290;&#36825;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#20851;&#20110;&#20805;&#20998;&#31867;&#65288;ample classes&#65289;&#30340;Moran&#21644;Warmuth&#30340;&#32467;&#26524;&#12289;&#20851;&#20110;&#36229;&#24179;&#38754;&#20223;&#23556;&#25490;&#21015;&#30340;Ben-David&#21644;Litman&#30340;&#32467;&#26524;&#20197;&#21450;&#20851;&#20110;&#22343;&#21248;&#26377;&#21521;&#25311;&#38453;&#30340;&#22797;&#21512;&#20307;&#30340;&#20316;&#32773;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#26679;&#26412;&#21387;&#32553;&#29468;&#24819;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#36825;&#26159;&#35745;&#31639;&#23398;&#20064;&#29702;&#35770;&#20013;&#26368;&#21476;&#32769;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#20043;&#19968;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26377;&#21521;&#25311;&#38453;&#29702;&#35770;&#21033;&#29992;&#20102;COM&#30340;&#20016;&#23500;&#30340;&#32452;&#21512;&#32454;&#32990;&#32467;&#26500;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23558;COM&#30340;&#39030;&#28857;&#22270;&#24418;&#35270;&#20026;&#37096;&#20998;&#31435;&#26041;&#20307;&#21019;&#24314;&#20102;&#19982;&#24230;&#37327;&#22270;&#24418;&#29702;&#35770;&#30340;&#26377;&#30410;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the topes of a complex of oriented matroids (abbreviated COM) of VC-dimension $d$ admit a proper labeled sample compression scheme of size $d$. This considerably extends results of Moran and Warmuth on ample classes, of Ben-David and Litman on affine arrangements of hyperplanes, and of the authors on complexes of uniform oriented matroids, and is a step towards the sample compression conjecture -- one of the oldest open problems in computational learning theory. On the one hand, our approach exploits the rich combinatorial cell structure of COMs via oriented matroid theory. On the other hand, viewing tope graphs of COMs as partial cubes creates a fruitful link to metric graph theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;BGNNs&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#19977;&#32500;&#39063;&#31890;&#27969;&#21160;&#30340;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2106.11299</link><description>&lt;p&gt;
&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110; 3D &#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Boundary Graph Neural Networks for 3D Simulations. (arXiv:2106.11299v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;BGNNs&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#19977;&#32500;&#39063;&#31890;&#27969;&#21160;&#30340;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#25968;&#25454;&#30340;&#20986;&#29616;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#28982;&#31185;&#23398;&#21644;&#24037;&#31243;&#23398;&#26041;&#38754;&#20855;&#26377;&#20102;&#21487;&#35266;&#30340;&#21160;&#21147;&#65292;&#28982;&#32780;&#23545;&#29289;&#29702;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#36890;&#24120;&#24456;&#22256;&#38590;&#12290;&#20854;&#20013;&#19968;&#20010;&#29305;&#21035;&#26840;&#25163;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#34920;&#31034;&#20960;&#20309;&#36793;&#30028;&#12290;&#19977;&#35282;&#21270;&#30340;&#20960;&#20309;&#36793;&#30028;&#22312;&#24037;&#31243;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#29702;&#35299;&#21644;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#23427;&#20204;&#30340;&#23610;&#23544;&#21644;&#26041;&#21521;&#30340;&#24322;&#36136;&#24615;&#65292;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#36890;&#24120;&#21313;&#20998;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#29702;&#35770;&#26469;&#24314;&#27169;&#31890;&#23376;&#19982;&#36793;&#30028;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#25512;&#23548;&#20986;&#20102;&#25105;&#20204;&#30340;&#26032;&#22411;&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;BGNNs&#65289;&#65292;&#35813;&#32593;&#32476;&#21160;&#24577;&#22320;&#20462;&#25913;&#22270;&#32467;&#26500;&#20197;&#28385;&#36275;&#36793;&#30028;&#26465;&#20214;&#12290;&#26032;&#30340; BGNNs &#22312;&#22797;&#26434;&#30340;&#19977;&#32500;&#39063;&#31890;&#27969;&#21160;&#36807;&#31243;&#65288;&#22914;&#28431;&#26007;&#12289;&#26059;&#36716;&#40723;&#21644;&#25605;&#25292;&#22120;&#65289;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36825;&#20123;&#36807;&#31243;&#37117;&#26159;&#29616;&#20195;&#24037;&#19994;&#26426;&#26800;&#30340;&#26631;&#20934;&#32452;&#20214;&#65292;&#20294;&#20854;&#20960;&#20309;&#24418;&#29366;&#20173;&#28982;&#21313;&#20998;&#22797;&#26434;&#12290;BGNNs &#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#29575;&#26041;&#38754;&#37117;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abundance of data has given machine learning considerable momentum in natural sciences and engineering, though modeling of physical processes is often difficult. A particularly tough problem is the efficient representation of geometric boundaries. Triangularized geometric boundaries are well understood and ubiquitous in engineering applications. However, it is notoriously difficult to integrate them into machine learning approaches due to their heterogeneity with respect to size and orientation. In this work, we introduce an effective theory to model particle-boundary interactions, which leads to our new Boundary Graph Neural Networks (BGNNs) that dynamically modify graph structures to obey boundary conditions. The new BGNNs are tested on complex 3D granular flow processes of hoppers, rotating drums and mixers, which are all standard components of modern industrial machinery but still have complicated geometry. BGNNs are evaluated in terms of computational efficiency as well as pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20010;&#31070;&#32463;&#20803;&#30340;&#26032;&#22411;&#38750;&#21442;&#25968;&#20998;&#20301;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23567;&#26679;&#26412;&#22823;&#23567;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#34920;&#26126;&#20854;&#21487;&#20197;&#28040;&#38500;&#19982;&#27169;&#22411;&#35268;&#33539;&#30456;&#20851;&#30340;&#33258;&#30001;&#24230;&#65292;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2106.03702</link><description>&lt;p&gt;
&#21333;&#20010;&#31070;&#32463;&#20803;&#33021;&#21542;&#23398;&#20064;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can a single neuron learn predictive uncertainty?. (arXiv:2106.03702v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20010;&#31070;&#32463;&#20803;&#30340;&#26032;&#22411;&#38750;&#21442;&#25968;&#20998;&#20301;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23567;&#26679;&#26412;&#22823;&#23567;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#34920;&#26126;&#20854;&#21487;&#20197;&#28040;&#38500;&#19982;&#27169;&#22411;&#35268;&#33539;&#30456;&#20851;&#30340;&#33258;&#30001;&#24230;&#65292;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26088;&#22312;&#20998;&#31163;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#25152;&#35266;&#23519;&#21040;&#30340;&#19990;&#30028;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;&#23458;&#35266;&#32456;&#28857;&#65289;&#19982;&#27169;&#22411;&#35268;&#33539;&#21644;&#35757;&#32451;&#36807;&#31243;&#29992;&#20110;&#39044;&#27979;&#36825;&#31181;&#29366;&#24577;&#30340;&#26041;&#24335;&#30456;&#28151;&#28102;&#30340;&#31243;&#24230;&#65288;&#20027;&#35266;&#25163;&#27573;&#65289;--&#20363;&#22914;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#65292;&#28145;&#24230;&#65292;&#36830;&#25509;&#65292;&#20808;&#39564;&#20998;&#24067;&#65288;&#22914;&#26524;&#27169;&#22411;&#26159;&#36125;&#21494;&#26031;&#30340;&#65289;&#65292;&#26435;&#37325;&#21021;&#22987;&#21270;&#31561;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#20173;&#33021;&#25429;&#33719;&#23458;&#35266;&#32456;&#28857;&#30340;&#21069;&#25552;&#19979;&#65292;&#33021;&#21542;&#28040;&#38500;&#19982;&#36825;&#20123;&#35268;&#33539;&#30456;&#20851;&#30340;&#33258;&#30001;&#24230;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#8212;&#21333;&#20010;&#31070;&#32463;&#20803;&#8212;&#30340;&#36830;&#32493;&#38543;&#26426;&#21464;&#37327;&#30340;&#26032;&#22411;&#38750;&#21442;&#25968;&#20998;&#20301;&#25968;&#20272;&#35745;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#22312;&#21512;&#25104;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#23427;&#30340;&#20248;&#21183;&#65292;&#23427;&#23558;&#36890;&#36807;&#25490;&#24207;&#39034;&#24207;&#32479;&#35745;&#37327;&#24471;&#21040;&#30340;&#20998;&#20301;&#25968;&#20272;&#35745;&#32467;&#26524;&#65288;&#29305;&#21035;&#26159;&#23545;&#20110;&#23567;&#26679;&#26412;&#22823;&#23567;&#65289;&#19982;&#20998;&#20301;&#22238;&#24402;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34987;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#38477;&#20302;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#25968;&#25454;&#20013;&#39044;&#27979;&#26041;&#24046;&#30340;&#20302;&#20272;&#26469;&#20026;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation methods using deep learning approaches strive against separating how uncertain the state of the world manifests to us via measurement (objective end) from the way this gets scrambled with the model specification and training procedure used to predict such state (subjective means) -- e.g., number of neurons, depth, connections, priors (if the model is bayesian), weight initialization, etc. This poses the question of the extent to which one can eliminate the degrees of freedom associated with these specifications and still being able to capture the objective end. Here, a novel non-parametric quantile estimation method for continuous random variables is introduced, based on the simplest neural network architecture with one degree of freedom: a single neuron. Its advantage is first shown in synthetic experiments comparing with the quantile estimation achieved from ranking the order statistics (specifically for small sample size) and with quantile regression. In real-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#22823;&#31751;&#20013;&#36827;&#34892;&#23454;&#39564;&#35774;&#35745;&#65292;&#20272;&#35745;&#21644;&#25512;&#26029;&#26368;&#22823;&#31119;&#21033;&#25919;&#31574;&#65292;&#24182;&#25552;&#20986;&#21333;&#27874;&#23454;&#39564;&#21644;&#22810;&#27874;&#23454;&#39564;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#28322;&#20986;&#25928;&#24212;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2011.08174</link><description>&lt;p&gt;
&#26410;&#30693;&#24178;&#25200;&#23454;&#39564;&#20013;&#30340;&#25919;&#31574;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Policy design in experiments with unknown interference. (arXiv:2011.08174v7 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.08174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#22823;&#31751;&#20013;&#36827;&#34892;&#23454;&#39564;&#35774;&#35745;&#65292;&#20272;&#35745;&#21644;&#25512;&#26029;&#26368;&#22823;&#31119;&#21033;&#25919;&#31574;&#65292;&#24182;&#25552;&#20986;&#21333;&#27874;&#23454;&#39564;&#21644;&#22810;&#27874;&#23454;&#39564;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#28322;&#20986;&#25928;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#28322;&#20986;&#25928;&#24212;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#21644;&#25512;&#26029;&#26368;&#22823;&#31119;&#21033;&#25919;&#31574;&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#23558;&#21333;&#20803;&#32452;&#32455;&#25104;&#26377;&#38480;&#25968;&#37327;&#30340;&#22823;&#31751;&#65292;&#24182;&#22312;&#27599;&#20010;&#31751;&#20869;&#20197;&#26410;&#30693;&#30340;&#26041;&#24335;&#30456;&#20114;&#20316;&#29992;&#12290;&#20316;&#20026;&#31532;&#19968;&#39033;&#36129;&#29486;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#27874;&#23454;&#39564;&#65292;&#36890;&#36807;&#22312;&#31751;&#23545;&#38388;&#20180;&#32454;&#21464;&#21270;&#38543;&#26426;&#21270;&#65292;&#32771;&#34385;&#28322;&#20986;&#25928;&#24212;&#20272;&#35745;&#27835;&#30103;&#27010;&#29575;&#21464;&#21270;&#30340;&#36793;&#38469;&#25928;&#24212;&#12290;&#21033;&#29992;&#36825;&#20010;&#36793;&#38469;&#25928;&#24212;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26816;&#39564;&#25919;&#31574;&#26368;&#20248;&#24615;&#30340;&#27979;&#35797;&#12290;&#20316;&#20026;&#31532;&#20108;&#39033;&#36129;&#29486;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#27874;&#23454;&#39564;&#65292;&#20272;&#35745;&#27835;&#30103;&#35268;&#21017;&#24182;&#26368;&#22823;&#21270;&#31119;&#21033;&#12290;&#26412;&#25991;&#23545;&#26368;&#22823;&#21487;&#36798;&#31119;&#21033;&#20110;&#25152;&#20272;&#35745;&#25919;&#31574;&#35780;&#20272;&#19979;&#31119;&#21033;&#20043;&#38388;&#30340;&#24046;&#24322;&#32473;&#20986;&#20102;&#24378;&#26377;&#21147;&#30340;&#23567;&#26679;&#26412;&#20445;&#35777;&#12290;&#20316;&#32773;&#22312;&#26681;&#25454;&#29616;&#26377;&#20851;&#20110;&#20449;&#24687;&#20256;&#25773;&#21644;&#29616;&#37329;&#36716;&#31227;&#35745;&#21010;&#30340;&#23454;&#39564;&#27169;&#25311;&#21644;&#22823;&#35268;&#27169;&#29616;&#22330;&#23454;&#39564;&#20013;&#25552;&#20379;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies experimental designs for estimation and inference on welfare-maximizing policies in the presence of spillover effects. Units are organized into a finite number of large clusters and interact in unknown ways within each cluster. As a first contribution, I introduce a single-wave experiment that, by carefully varying the randomization across cluster pairs, estimates the marginal effect of a change in treatment probabilities, taking spillover effects into account. Using the marginal effect, I propose a test for policy optimality. As a second contribution, I design a multiple-wave experiment to estimate treatment rules and maximize welfare. I derive strong small-sample guarantees on the difference between the maximum attainable welfare and the welfare evaluated at the estimated policy. I illustrate the method's properties in simulations calibrated to existing experiments on information diffusion and cash-transfer programs, and in a large scale field experiment implemente
&lt;/p&gt;</description></item><item><title>&#26631;&#20934;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;ELBO&#22312;&#31283;&#23450;&#28857;&#22788;&#21487;&#20197;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;&#65292;&#25910;&#25947;&#20110;&#19977;&#20010;&#29109;&#20043;&#21644;&#12290;&#65288;&#20854;&#20013;&#19968;&#20010;&#29109;&#20026;&#20808;&#39564;&#20998;&#24067;&#30340;&#29109;&#65292;&#19968;&#20010;&#20026;&#21487;&#35266;&#27979;&#20998;&#24067;&#30340;&#29109;&#65292;&#19968;&#20010;&#20026;&#21464;&#20998;&#20998;&#24067;&#30340;&#24179;&#22343;&#29109;&#65292;&#25104;&#26524;&#35777;&#26126;&#20102;ELBO&#22312;&#31283;&#23450;&#28857;&#22788;&#31561;&#20110;&#29109;&#12290;&#65289;</title><link>http://arxiv.org/abs/2010.14860</link><description>&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;ELBO&#25910;&#25947;&#20110;&#19977;&#20010;&#29109;&#20043;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ELBO of Variational Autoencoders Converges to a Sum of Three Entropies. (arXiv:2010.14860v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.14860
&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;ELBO&#22312;&#31283;&#23450;&#28857;&#22788;&#21487;&#20197;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;&#65292;&#25910;&#25947;&#20110;&#19977;&#20010;&#29109;&#20043;&#21644;&#12290;&#65288;&#20854;&#20013;&#19968;&#20010;&#29109;&#20026;&#20808;&#39564;&#20998;&#24067;&#30340;&#29109;&#65292;&#19968;&#20010;&#20026;&#21487;&#35266;&#27979;&#20998;&#24067;&#30340;&#29109;&#65292;&#19968;&#20010;&#20026;&#21464;&#20998;&#20998;&#24067;&#30340;&#24179;&#22343;&#29109;&#65292;&#25104;&#26524;&#35777;&#26126;&#20102;ELBO&#22312;&#31283;&#23450;&#28857;&#22788;&#31561;&#20110;&#29109;&#12290;&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAEs)&#30340;&#20013;&#24515;&#30446;&#26631;&#20989;&#25968;&#26159;&#20854;&#21464;&#20998;&#19979;&#30028;(ELBO)&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#26631;&#20934;(&#21363;&#39640;&#26031;)VAEs&#65292;ELBO&#25910;&#25947;&#20110;&#30001;&#19977;&#20010;&#29109;&#20043;&#21644;&#32473;&#20986;&#30340;&#20540;&#65306;(&#20808;&#39564;&#20998;&#24067;&#30340;&#36127;)&#29109;&#12289;&#21487;&#35266;&#27979;&#20998;&#24067;&#30340;&#39044;&#26399;(&#36127;)&#29109;&#20197;&#21450;&#21464;&#20998;&#20998;&#24067;&#30340;&#24179;&#22343;&#29109;(&#21518;&#32773;&#24050;&#32463;&#26159;ELBO&#30340;&#19968;&#37096;&#20998;)&#12290;&#25105;&#20204;&#30340;&#25512;&#23548;&#32467;&#26524;&#31934;&#30830;&#65292;&#36866;&#29992;&#20110;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#23567;&#22411;&#21644;&#22797;&#26434;&#28145;&#24230;&#32593;&#32476;&#65292;&#24182;&#36866;&#29992;&#20110;&#26377;&#38480;&#21644;&#26080;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#28857;&#20197;&#21450;&#20219;&#20309;&#31283;&#23450;&#28857;(&#21253;&#25324;&#23616;&#37096;&#26368;&#22823;&#20540;&#21644;&#38797;&#28857;)&#12290;&#35813;&#32467;&#26524;&#24847;&#21619;&#30528;&#23545;&#20110;&#26631;&#20934;VAEs&#65292;ELBO&#22312;&#31283;&#23450;&#28857;&#26102;&#36890;&#24120;&#21487;&#20197;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;&#65292;&#32780;&#21407;&#22987;ELBO&#38656;&#35201;&#25968;&#20540;&#31215;&#20998;&#36817;&#20284;&#12290;&#20316;&#20026;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;VAEs&#30340;ELBO&#22312;&#31283;&#23450;&#28857;&#22788;&#31561;&#20110;&#29109;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The central objective function of a variational autoencoder (VAE) is its variational lower bound (the ELBO). Here we show that for standard (i.e., Gaussian) VAEs the ELBO converges to a value given by the sum of three entropies: the (negative) entropy of the prior distribution, the expected (negative) entropy of the observable distribution, and the average entropy of the variational distributions (the latter is already part of the ELBO). Our derived analytical results are exact and apply for small as well as for intricate deep networks for encoder and decoder. Furthermore, they apply for finitely and infinitely many data points and at any stationary point (including local maxima and saddle points). The result implies that the ELBO can for standard VAEs often be computed in closed-form at stationary points while the original ELBO requires numerical approximations of integrals. As a main contribution, we provide the proof that the ELBO for VAEs is at stationary points equal to entropy su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#26469;&#25551;&#36848;&#23545;&#25239;&#35757;&#32451;&#20013;&#26799;&#24230;&#30340;&#20248;&#36873;&#26041;&#21521;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#23545;&#40784;&#24773;&#20917;&#65292;&#24182;&#34920;&#26126;&#22312;&#23545;&#40784;&#26041;&#21521;&#19978;&#30340;&#38480;&#21046;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2009.04709</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#30340;&#23545;&#25239;&#35757;&#32451;&#20013;&#26799;&#24230;&#26041;&#21521;&#30340;&#37327;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Preferential Direction of the Model Gradient in Adversarial Training With Projected Gradient Descent. (arXiv:2009.04709v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.04709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#26469;&#25551;&#36848;&#23545;&#25239;&#35757;&#32451;&#20013;&#26799;&#24230;&#30340;&#20248;&#36873;&#26041;&#21521;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#23545;&#40784;&#24773;&#20917;&#65292;&#24182;&#34920;&#26126;&#22312;&#23545;&#40784;&#26041;&#21521;&#19978;&#30340;&#38480;&#21046;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#65292;&#23588;&#20854;&#26159;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;PGD&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#23545;&#25239;&#24615;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#23545;&#25239;&#35757;&#32451;&#21518;&#65292;&#27169;&#22411;&#23545;&#20854;&#36755;&#20837;&#30340;&#26799;&#24230;&#20855;&#26377;&#20248;&#36873;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#23545;&#40784;&#26041;&#21521;&#24182;&#27809;&#26377;&#24471;&#21040;&#25968;&#23398;&#19978;&#30340;&#24456;&#22909;&#25551;&#36848;&#65292;&#36825;&#20351;&#24471;&#20854;&#38590;&#20197;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#65292;&#23558;&#20854;&#35270;&#20026;&#25351;&#21521;&#20915;&#31574;&#31354;&#38388;&#20013;&#26368;&#36817;&#38169;&#35823;&#31867;&#25903;&#25345;&#38598;&#30340;&#26368;&#36817;&#28857;&#30340;&#21521;&#37327;&#26041;&#21521;&#12290;&#20026;&#20102;&#35780;&#20272;&#23545;&#25239;&#35757;&#32451;&#21518;&#27169;&#22411;&#19982;&#27492;&#26041;&#21521;&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#25351;&#26631;&#65292;&#35813;&#25351;&#26631;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26469;&#20135;&#29983;&#26368;&#23567;&#27531;&#24046;&#65292;&#20197;&#25913;&#21464;&#22270;&#20687;&#20013;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;PGD&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#22522;&#32447;&#20855;&#26377;&#26356;&#39640;&#30340;&#23545;&#40784;&#24230;&#65292;&#32780;&#25105;&#20204;&#30340;&#25351;&#26631;&#21576;&#29616;&#27604;&#31454;&#20105;&#25351;&#26631;&#20844;&#24335;&#26356;&#39640;&#30340;&#23545;&#40784;&#24230;&#20540;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24378;&#21046;&#25191;&#34892;&#36825;&#20010;&#23545;&#40784;&#26041;&#21521;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training, especially projected gradient descent (PGD), has proven to be a successful approach for improving robustness against adversarial attacks. After adversarial training, gradients of models with respect to their inputs have a preferential direction. However, the direction of alignment is not mathematically well established, making it difficult to evaluate quantitatively. We propose a novel definition of this direction as the direction of the vector pointing toward the closest point of the support of the closest inaccurate class in decision space. To evaluate the alignment with this direction after adversarial training, we apply a metric that uses generative adversarial networks to produce the smallest residual needed to change the class present in the image. We show that PGD-trained models have a higher alignment than the baseline according to our definition, that our metric presents higher alignment values than a competing metric formulation, and that enforcing this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#30340;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19978;&#23450;&#20041;&#19968;&#20010;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#23558;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#21464;&#25442;&#30340;&#30446;&#26631;&#24207;&#21015;&#19978;&#65292;&#35813;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#30340;&#25311;&#21512;&#24230;&#19978;&#34920;&#29616;&#26174;&#33879;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#22312;&#23545;&#25968;&#20284;&#28982;&#24230;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2008.02144</link><description>&lt;p&gt;
FRMDN: &#22522;&#20110;&#27969;&#30340;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FRMDN: Flow-based Recurrent Mixture Density Network. (arXiv:2008.02144v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.02144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#30340;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19978;&#23450;&#20041;&#19968;&#20010;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#23558;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#21464;&#25442;&#30340;&#30446;&#26631;&#24207;&#21015;&#19978;&#65292;&#35813;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#30340;&#25311;&#21512;&#24230;&#19978;&#34920;&#29616;&#26174;&#33879;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#22312;&#23545;&#25968;&#20284;&#28982;&#24230;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a flow-based recurrent mixture density network (FRMDN) that generalizes recurrent mixture density networks by defining a Gaussian mixture model on a non-linearly transformed target sequence in each time-step. The model significantly improves the fit to image sequences and outperforms other state-of-the-art methods in terms of the log-likelihood.
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#21644;&#24207;&#21015;&#21040;&#24207;&#21015;&#26144;&#23556;&#24212;&#29992;&#20013;&#12290;&#22312;&#36825;&#31867;&#27169;&#22411;&#20013;&#65292;&#30446;&#26631;&#24207;&#21015;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#23494;&#24230;&#30001;&#20855;&#26377;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#24314;&#27169;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19978;&#23450;&#20041;&#19968;&#20010;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#23558;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#21464;&#25442;&#30340;&#30446;&#26631;&#24207;&#21015;&#19978;&#12290;&#38750;&#32447;&#24615;&#21464;&#25442;&#31354;&#38388;&#26159;&#36890;&#36807;&#24402;&#19968;&#21270;&#27969;&#21019;&#24314;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#35813;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#24207;&#21015;&#30340;&#25311;&#21512;&#24230;&#65292;&#29992;&#23545;&#25968;&#20284;&#28982;&#24230;&#37327;&#12290;&#25105;&#20204;&#36824;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;&#19968;&#20123;&#35821;&#38899;&#21644;&#22270;&#20687;&#25968;&#25454;&#65292;&#24182;&#35266;&#23519;&#21040;&#35813;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#22312;&#23545;&#25968;&#20284;&#28982;&#24230;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The class of recurrent mixture density networks is an important class of probabilistic models used extensively in sequence modeling and sequence-to-sequence mapping applications. In this class of models, the density of a target sequence in each time-step is modeled by a Gaussian mixture model with the parameters given by a recurrent neural network. In this paper, we generalize recurrent mixture density networks by defining a Gaussian mixture model on a non-linearly transformed target sequence in each time-step. The non-linearly transformed space is created by normalizing flow. We observed that this model significantly improves the fit to image sequences measured by the log-likelihood. We also applied the proposed model on some speech and image data, and observed that the model has significant modeling power outperforming other state-of-the-art methods in terms of the log-likelihood.
&lt;/p&gt;</description></item></channel></rss>