<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00976</link><description>&lt;p&gt;
&#20855;&#26377;&#21160;&#24577;&#20572;&#27490;&#30340;&#24490;&#29615;Transformer
&lt;/p&gt;
&lt;p&gt;
Recurrent Transformers with Dynamic Halt
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#22312;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#26041;&#38754;&#30340;&#24402;&#32435;&#20559;&#22909;&#8212;&#8212;&#65288;1&#65289;&#31867;&#20284;&#20110;Universal Transformers&#30340;&#28145;&#24230;&#36880;&#23618;&#24490;&#29615;&#26041;&#27861;&#65307;&#21644;&#65288;2&#65289;&#31867;&#20284;&#20110;Temporal Latent Bottleneck&#30340;&#20998;&#22359;&#26102;&#24577;&#24490;&#29615;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#25193;&#23637;&#21644;&#32452;&#21512;&#19978;&#36848;&#26041;&#27861;&#30340;&#26032;&#26041;&#24335;&#65292;&#20363;&#22914;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#22343;&#20540;&#30340;Universal Transformer&#21160;&#24577;&#20572;&#27490;&#26426;&#21046;&#65292;&#24182;&#23558;Universal Transformer&#30340;&#20803;&#32032;&#34701;&#20837;&#21040;Temporal Latent Bottleneck&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#65288;&#22914;Long Range Arena&#65288;LRA&#65289;&#65292;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;&#65292;ListOps&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#27604;&#36739;&#20102;&#27169;&#22411;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;Masked AutoEncoders&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;NeRF-MAE&#29992;&#20110;&#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#65292;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;&#23494;&#38598;&#36755;&#20837;&#65292;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#19977;&#32500;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.01300</link><description>&lt;p&gt;
NeRF-MAE: &#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;Masked AutoEncoders
&lt;/p&gt;
&lt;p&gt;
NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Masked AutoEncoders&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;NeRF-MAE&#29992;&#20110;&#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#65292;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;&#23494;&#38598;&#36755;&#20837;&#65292;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#19977;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31070;&#32463;&#22330;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#33021;&#22815;&#29702;&#35299;&#19977;&#32500;&#35270;&#35273;&#19990;&#30028;&#65292;&#22914;&#25512;&#26029;&#35821;&#20041;&#12289;&#20960;&#20309;&#21644;&#21160;&#24577;&#31561;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#22330;&#22312;&#20174;&#20108;&#32500;&#22270;&#20687;&#20013;&#23494;&#38598;&#34920;&#31034;&#19977;&#32500;&#22330;&#26223;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20855;&#20307;&#20351;&#29992;Masked AutoEncoders&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#23558;transformers&#25193;&#23637;&#21040;&#26032;&#25968;&#25454;&#27169;&#24577;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#25104;&#21151;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#26469;&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#12290;&#25105;&#20204;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;transformer&#30340;&#23494;&#38598;&#36755;&#20837;&#65292;&#19982;&#20854;&#20182;&#19977;&#32500;&#34920;&#31034;&#65288;&#22914;&#28857;&#20113;&#65289;&#36827;&#34892;&#23545;&#27604;&#65292;&#20854;&#20449;&#24687;&#23494;&#24230;&#21487;&#33021;&#19981;&#22343;&#21248;&#65292;&#32780;&#34920;&#31034;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#30001;&#20110;&#23558;masked autoencoders&#24212;&#29992;&#20110;&#31867;&#20284;NeRF&#36825;&#26679;&#30340;&#38544;&#24335;&#34920;&#31034;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#36873;&#25321;&#25552;&#21462;&#19968;&#20010;&#26174;&#24335;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01300v1 Announce Type: cross  Abstract: Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit repres
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#21345;&#36890;&#22270;&#20687;&#26500;&#24314;&#30340;CausalChaos!&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#22240;&#26524;&#38382;&#31572;&#65292;&#36890;&#36807;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#23637;&#31034;&#25361;&#25112;&#24615;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22810;&#20855;&#25361;&#25112;&#24615;&#19988;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2404.01299</link><description>&lt;p&gt;
CausalChaos!&#25968;&#25454;&#38598;&#65306;&#22522;&#20110;&#21160;&#24577;&#35270;&#35273;&#22330;&#26223;&#20013;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#20840;&#38754;&#22240;&#26524;&#34892;&#21160;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01299
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21345;&#36890;&#22270;&#20687;&#26500;&#24314;&#30340;CausalChaos!&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#22240;&#26524;&#38382;&#31572;&#65292;&#36890;&#36807;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#23637;&#31034;&#25361;&#25112;&#24615;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22810;&#20855;&#25361;&#25112;&#24615;&#19988;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#35270;&#39057;&#38382;&#31572;&#65288;QA&#65289;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#28982;&#32780;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#22240;&#26524;&#25512;&#29702;&#20998;&#26512;&#26041;&#38754;&#24448;&#24448;&#32570;&#20047;&#28145;&#24230;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#21033;&#29992;&#21345;&#36890;&#30340;&#29420;&#29305;&#23646;&#24615;&#26500;&#24314;&#20102;CausalChaos!&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22240;&#26524;&#38382;&#31572;&#65288;Why-QA&#65289;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#26631;&#24535;&#24615;&#30340;&#8220;&#29483;&#21644;&#32769;&#40736;&#8221;&#21345;&#36890;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;&#21608;&#21040;&#30340;&#38382;&#39064;&#21644;&#22810;&#23618;&#27425;&#31572;&#26696;&#65292;&#21253;&#21547;&#30528;&#23884;&#20837;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#20013;&#30340;&#26356;&#38271;&#22240;&#26524;&#38142;&#65292;&#21516;&#26102;&#21160;&#30011;&#21407;&#29702;&#20801;&#35768;&#21160;&#30011;&#24072;&#21019;&#36896;&#23450;&#20041;&#26126;&#30830;&#12289;&#26126;&#20102;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36825;&#20123;&#22240;&#32032;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#26356;&#20855;&#25361;&#25112;&#24615;&#20294;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#30828;&#36127;&#37319;&#26679;&#65292;&#21253;&#25324;CausalConfusion&#29256;&#26412;&#12290;&#34429;&#28982;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#25913;&#36827;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#24320;&#25918;&#24335;&#31572;&#26696;&#26041;&#38754;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20026;&#20808;&#36827;/&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#24314;&#27169;&#21644;&#32852;&#21512;&#24314;&#27169;&#31561;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01299v1 Announce Type: cross  Abstract: Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#25552;&#21462;&#22270;&#20687;&#30340;&#39118;&#26684;&#25551;&#36848;&#31526;&#65292;&#36890;&#36807;&#26032;&#25968;&#25454;&#38598;&#21644;&#30740;&#31350;&#21576;&#29616;&#20102;&#19968;&#31181;&#20102;&#35299;&#22270;&#20687;&#39118;&#26684;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.01292</link><description>&lt;p&gt;
&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#27979;&#37327;&#39118;&#26684;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring Style Similarity in Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01292
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#25552;&#21462;&#22270;&#20687;&#30340;&#39118;&#26684;&#25551;&#36848;&#31526;&#65292;&#36890;&#36807;&#26032;&#25968;&#25454;&#38598;&#21644;&#30740;&#31350;&#21576;&#29616;&#20102;&#19968;&#31181;&#20102;&#35299;&#22270;&#20687;&#39118;&#26684;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01292v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25277;&#35937;&#65306;&#29983;&#25104;&#27169;&#22411;&#29616;&#22312;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#24418;&#35774;&#35745;&#24072;&#21644;&#33402;&#26415;&#23478;&#12290;&#20808;&#21069;&#30340;&#20316;&#21697;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20250;&#35760;&#20303;&#24182;&#32463;&#24120;&#22797;&#21046;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#38543;&#30528;&#23427;&#20204;&#30340;&#26222;&#21450;&#22686;&#21152;&#65292;&#27599;&#27425;&#22312;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#29992;&#20110;&#19987;&#19994;&#29992;&#36884;&#20043;&#21069;&#65292;&#25191;&#34892;&#25968;&#25454;&#24211;&#25628;&#32034;&#20197;&#30830;&#23450;&#22270;&#20687;&#30340;&#23646;&#24615;&#26159;&#21542;&#24402;&#22240;&#20110;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#29616;&#26377;&#24037;&#20855;&#26088;&#22312;&#26816;&#32034;&#20855;&#26377;&#30456;&#20284;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35768;&#22810;&#33402;&#26415;&#23478;&#20851;&#27880;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#39118;&#26684;&#22797;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#29702;&#35299;&#21644;&#25552;&#21462;&#22270;&#20687;&#30340;&#39118;&#26684;&#25551;&#36848;&#31526;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20351;&#29992;&#36825;&#26679;&#19968;&#31181;&#27934;&#23519;&#21147;&#31579;&#36873;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#21363;&#39118;&#26684;&#26159;&#22270;&#20687;&#30340;&#20027;&#35266;&#23646;&#24615;&#65292;&#25429;&#25417;&#21040;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#39068;&#33394;&#12289;&#32441;&#29702;&#12289;&#24418;&#29366;&#31561;&#22240;&#32032;&#30340;&#22797;&#26434;&#32780;&#26377;&#24847;&#20041;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01292v1 Announce Type: cross  Abstract: Generative models are now widely used by graphic designers and artists. Prior works have shown that these models remember and often replicate content from their training data during generation. Hence as their proliferation increases, it has become important to perform a database search to determine whether the properties of the image are attributable to specific training data, every time before a generated image is used for professional purposes. Existing tools for this purpose focus on retrieving images of similar semantic content. Meanwhile, many artists are concerned with style replication in text-to-image models. We present a framework for understanding and extracting style descriptors from images. Our framework comprises a new dataset curated using the insight that style is a subjective property of an image that captures complex yet meaningful interactions of factors including but not limited to colors, textures, shapes, etc. We a
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;VQAScore&#65292;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;</title><link>https://arxiv.org/abs/2404.01291</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#19982;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text-to-Visual Generation with Image-to-Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01291
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;VQAScore&#65292;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#26631;&#20934;&#21270;&#22522;&#20934;&#65292;&#32508;&#21512;&#35780;&#20272;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VQAScore&#65292;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#36890;&#36807;&#35745;&#31639;&#23545;&#31616;&#21333;&#30340;&#8220;&#36825;&#24133;&#22270;&#34920;&#29616;&#20986;&#20102;'{&#25991;&#26412;}'&#21527;&#65311;&#8221;&#38382;&#39064;&#30340;&#8220;&#26159;&#8221;&#31572;&#26696;&#30340;&#27010;&#29575;&#26469;&#29983;&#25104;&#23545;&#40784;&#20998;&#25968;&#12290;&#23613;&#31649;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#26356;&#31616;&#21333;&#65292;&#20294;&#20351;&#29992;&#29616;&#25104;&#27169;&#22411;&#35745;&#31639;&#30340;VQAScore&#22312;&#35768;&#22810;&#65288;8&#20010;&#65289;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#22522;&#20934;&#19978;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01291v1 Announce Type: cross  Abstract: Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;TWIN-GPT&#65292;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2404.01273</link><description>&lt;p&gt;
TWIN-GPT: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#35797;&#39564;&#25968;&#23383;&#23402;&#29983;&#20307;
&lt;/p&gt;
&lt;p&gt;
TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01273
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;TWIN-GPT&#65292;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#34394;&#25311;&#20020;&#24202;&#35797;&#39564;&#20135;&#29983;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#65292;&#36825;&#20123;&#35797;&#39564;&#27169;&#25311;&#20102;&#29616;&#23454;&#19990;&#30028;&#24773;&#22659;&#65292;&#26377;&#26395;&#26174;&#33879;&#22686;&#24378;&#24739;&#32773;&#23433;&#20840;&#24615;&#65292;&#21152;&#24555;&#24320;&#21457;&#36895;&#24230;&#65292;&#38477;&#20302;&#25104;&#26412;&#65292;&#24182;&#20026;&#21307;&#30103;&#39046;&#22495;&#30340;&#26356;&#24191;&#27867;&#31185;&#23398;&#30693;&#35782;&#36129;&#29486;&#21147;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;TWIN-GPT&#65292;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01273v1 Announce Type: cross  Abstract: Recently, there has been a burgeoning interest in virtual clinical trials, which simulate real-world scenarios and hold the potential to significantly enhance patient safety, expedite development, reduce costs, and contribute to the broader scientific knowledge in healthcare. Existing research often focuses on leveraging electronic health records (EHRs) to support clinical trial outcome prediction. Yet, trained with limited clinical trial outcome data, existing approaches frequently struggle to perform accurate predictions. Some research has attempted to generate EHRs to augment model development but has fallen short in personalizing the generation for individual patient profiles. Recently, the emergence of large language models has illuminated new possibilities, as their embedded comprehensive clinical knowledge has proven beneficial in addressing medical issues. In this paper, we propose a large language model-based digital twin crea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#25955;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20004;&#31181;&#26041;&#27861;&#35770;&#36827;&#23637;&#65292;&#19968;&#26159;&#22312;CollabDict&#26694;&#26550;&#20013;&#25972;&#21512;&#28145;&#24230;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#65292;&#20108;&#26159;&#25552;&#20379;&#20102;&#20351;&#29992;CollabDict&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22806;&#37096;&#20849;&#20139;&#26102;&#25968;&#25454;&#38544;&#31169;&#27844;&#28431;&#30340;&#25968;&#23398;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2404.01270</link><description>&lt;p&gt;
&#20855;&#26377;&#22806;&#37096;&#38544;&#31169;&#27844;&#38706;&#20998;&#26512;&#30340;&#20998;&#25955;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Decentralized Collaborative Learning Framework with External Privacy Leakage Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#25955;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20004;&#31181;&#26041;&#27861;&#35770;&#36827;&#23637;&#65292;&#19968;&#26159;&#22312;CollabDict&#26694;&#26550;&#20013;&#25972;&#21512;&#28145;&#24230;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#65292;&#20108;&#26159;&#25552;&#20379;&#20102;&#20351;&#29992;CollabDict&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22806;&#37096;&#20849;&#20139;&#26102;&#25968;&#25454;&#38544;&#31169;&#27844;&#28431;&#30340;&#25968;&#23398;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#38544;&#31169;&#32422;&#26463;&#26465;&#20214;&#19979;&#20998;&#25955;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20004;&#31181;&#26041;&#27861;&#35770;&#36827;&#23637;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#19979;&#19968;&#20195;&#21306;&#22359;&#38142;&#24179;&#21488;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#29992;&#20110;&#21327;&#20316;&#35789;&#20856;&#23398;&#20064;&#65288;CollabDict&#65289;&#30340;&#29616;&#26377;&#26694;&#26550;&#36827;&#34892;&#25193;&#23637;&#65292;&#35813;&#26694;&#26550;&#20808;&#21069;&#20165;&#38480;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#32435;&#20837;&#26694;&#26550;&#65292;&#29305;&#21035;&#20851;&#27880;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#22522;&#20110;VAE&#30340;&#24322;&#24120;&#35780;&#20998;&#20989;&#25968;&#19982;&#38750;&#28145;&#24230;&#27169;&#22411;&#20855;&#26377;&#30456;&#21516;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#20840;&#38754;&#30340;&#23450;&#24615;&#27604;&#36739;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;&#8220;&#39044;&#35757;&#32451;&#27169;&#22411;&#8221;&#24191;&#27867;&#20351;&#29992;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;CollabDict&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22806;&#37096;&#20849;&#20139;&#26102;&#30340;&#25968;&#25454;&#38544;&#31169;&#27844;&#28431;&#36827;&#34892;&#20102;&#25968;&#23398;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23558;CollabDict&#26041;&#27861;&#24212;&#29992;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26102;&#65292;&#31526;&#21512;Renyi&#24494;&#20998;&#38544;&#31169;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01270v1 Announce Type: new  Abstract: This paper presents two methodological advancements in decentralized multi-task learning under privacy constraints, aiming to pave the way for future developments in next-generation Blockchain platforms. First, we expand the existing framework for collaborative dictionary learning (CollabDict), which has previously been limited to Gaussian mixture models, by incorporating deep variational autoencoders (VAEs) into the framework, with a particular focus on anomaly detection. We demonstrate that the VAE-based anomaly score function shares the same mathematical structure as the non-deep model, and provide comprehensive qualitative comparison. Second, considering the widespread use of "pre-trained models," we provide a mathematical analysis on data privacy leakage when models trained with CollabDict are shared externally. We show that the CollabDict approach, when applied to Gaussian mixtures, adheres to a Renyi differential privacy criterion
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#35770;&#25991;&#20013;LLM&#20351;&#29992;&#22686;&#21152;&#30340;&#26144;&#23556;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#23398;&#26415;&#20889;&#20316;&#20013;&#30495;&#23454;LLM&#20462;&#25913;&#20869;&#23481;&#27604;&#20363;&#32570;&#22833;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2404.01268</link><description>&lt;p&gt;
&#31185;&#23398;&#35770;&#25991;&#20013;LLM&#20351;&#29992;&#22686;&#21152;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Mapping the Increasing Use of LLMs in Scientific Papers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#35770;&#25991;&#20013;LLM&#20351;&#29992;&#22686;&#21152;&#30340;&#26144;&#23556;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#23398;&#26415;&#20889;&#20316;&#20013;&#30495;&#23454;LLM&#20462;&#25913;&#20869;&#23481;&#27604;&#20363;&#32570;&#22833;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#20986;&#29256;&#36890;&#36807;&#20256;&#25773;&#30740;&#31350;&#25104;&#26524;&#12289;&#20419;&#36827;&#21512;&#20316;&#12289;&#40723;&#21169;&#21487;&#37325;&#22797;&#24615;&#65292;&#24182;&#30830;&#20445;&#31185;&#23398;&#30693;&#35782;&#38543;&#26102;&#38388;&#21487;&#35775;&#38382;&#12289;&#21487;&#39564;&#35777;&#24182;&#19981;&#26029;&#24314;&#31435;&#65292;&#20026;&#31185;&#23398;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#22810;&#23569;&#20154;&#22312;&#23398;&#26415;&#20889;&#20316;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#21450;&#36825;&#31181;&#24037;&#20855;&#23545;&#20840;&#29699;&#31185;&#23398;&#23454;&#36341;&#21487;&#33021;&#20135;&#29983;&#20309;&#31181;&#24433;&#21709;&#36827;&#34892;&#20102;&#22823;&#37327;&#29468;&#27979;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#23545;&#23398;&#26415;&#20889;&#20316;&#20013;&#23454;&#36136;&#24615;&#20462;&#25913;&#25110;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24230;&#37327;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#22312;arXiv&#12289;bioRxiv&#21644;&#33258;&#28982;&#23398;&#25253;&#31995;&#21015;&#26399;&#21002;&#19978;&#30340;950,965&#31687;&#35770;&#25991;&#65288;&#26102;&#38388;&#36328;&#24230;&#20174;2020&#24180;1&#26376;&#33267;2024&#24180;2&#26376;&#65289;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#24615;&#12289;&#22823;&#35268;&#27169;&#30340;&#20998;&#26512;&#65292;&#21033;&#29992;&#20154;&#32676;&#32423;&#21035;&#30340;&#32479;&#35745;&#26694;&#26550;&#26469;&#27979;&#37327;LLM&#20462;&#25913;&#20869;&#23481;&#38543;&#26102;&#38388;&#30340;&#30427;&#34892;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#20272;&#35745;&#26159;&#22312;&#35821;&#26009;&#24211;&#32423;&#21035;&#21450;i
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01268v1 Announce Type: cross  Abstract: Scientific publishing lays the foundation of science by disseminating research findings, fostering collaboration, encouraging reproducibility, and ensuring that scientific knowledge is accessible, verifiable, and built upon over time. Recently, there has been immense speculation about how many people are using large language models (LLMs) like ChatGPT in their academic writing, and to what extent this tool might have an effect on global scientific practices. However, we lack a precise measure of the proportion of academic writing substantially modified or produced by LLMs. To address this gap, we conduct the first systematic, large-scale analysis across 950,965 papers published between January 2020 and February 2024 on the arXiv, bioRxiv, and Nature portfolio journals, using a population-level statistical framework to measure the prevalence of LLM-modified content over time. Our statistical estimation operates on the corpus level and i
&lt;/p&gt;</description></item><item><title>msGFM&#26159;&#19968;&#20010;&#22810;&#20256;&#24863;&#22120;&#22320;&#29702;&#31354;&#38388;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#32479;&#19968;&#22235;&#31181;&#20851;&#38190;&#20256;&#24863;&#22120;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#20855;&#26377;&#22788;&#29702;&#25104;&#23545;&#21644;&#38750;&#25104;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#36328;&#20256;&#24863;&#22120;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#33945;&#29256;&#22270;&#20687;&#24314;&#27169;&#20013;&#21512;&#25104;&#32852;&#21512;&#34920;&#31034;&#65292;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#20256;&#24863;&#22120;&#31867;&#22411;&#19979;&#37117;&#20855;&#26377;&#24378;&#22823;&#24615;&#33021;&#30340;&#32508;&#21512;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.01260</link><description>&lt;p&gt;
&#23558;&#36828;&#31243;&#20256;&#24863;&#22120;&#19982;&#22810;&#20256;&#24863;&#22120;&#22320;&#29702;&#31354;&#38388;&#22522;&#30784;&#27169;&#22411;&#32852;&#31995;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
Bridging Remote Sensors with Multisensor Geospatial Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01260
&lt;/p&gt;
&lt;p&gt;
msGFM&#26159;&#19968;&#20010;&#22810;&#20256;&#24863;&#22120;&#22320;&#29702;&#31354;&#38388;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#32479;&#19968;&#22235;&#31181;&#20851;&#38190;&#20256;&#24863;&#22120;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#20855;&#26377;&#22788;&#29702;&#25104;&#23545;&#21644;&#38750;&#25104;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#36328;&#20256;&#24863;&#22120;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#33945;&#29256;&#22270;&#20687;&#24314;&#27169;&#20013;&#21512;&#25104;&#32852;&#21512;&#34920;&#31034;&#65292;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#20256;&#24863;&#22120;&#31867;&#22411;&#19979;&#37117;&#20855;&#26377;&#24378;&#22823;&#24615;&#33021;&#30340;&#32508;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#29702;&#31354;&#38388;&#20998;&#26512;&#39046;&#22495;&#65292;&#21253;&#25324;&#20809;&#23398;&#21644;&#24494;&#27874;&#25216;&#26415;&#22312;&#20869;&#30340;&#36828;&#31243;&#20256;&#24863;&#22120;&#30340;&#22810;&#26679;&#24615;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#29420;&#29305;&#35266;&#27979;&#33021;&#21147;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;msGFM&#65292;&#19968;&#20010;&#22810;&#20256;&#24863;&#22120;&#22320;&#29702;&#31354;&#38388;&#22522;&#30784;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#32479;&#19968;&#20102;&#26469;&#33258;&#22235;&#31181;&#20851;&#38190;&#20256;&#24863;&#22120;&#27169;&#24577;&#30340;&#25968;&#25454;&#12290;&#36825;&#31181;&#38598;&#25104;&#28085;&#30422;&#20102;&#20004;&#30334;&#19975;&#22810;&#20256;&#24863;&#22120;&#22270;&#20687;&#30340;&#24222;&#22823;&#25968;&#25454;&#38598;&#12290;msGFM&#22312;&#22788;&#29702;&#25104;&#23545;&#21644;&#38750;&#25104;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#26041;&#38754;&#29420;&#20855;&#25165;&#33021;&#12290;&#23545;&#20110;&#26469;&#33258;&#30456;&#21516;&#22320;&#29702;&#20301;&#32622;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#20256;&#24863;&#22120;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#33945;&#29256;&#22270;&#20687;&#24314;&#27169;&#20013;&#23454;&#29616;&#32852;&#21512;&#34920;&#31034;&#30340;&#21512;&#25104;&#12290;msGFM&#32467;&#21512;&#20102;&#22235;&#31181;&#36828;&#31243;&#20256;&#24863;&#22120;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#36866;&#24212;&#21508;&#31181;&#20256;&#24863;&#22120;&#31867;&#22411;&#30340;&#32508;&#21512;&#27169;&#22411;&#12290;msGFM&#22312;&#19968;&#31995;&#21015;&#21333;&#20256;&#24863;&#22120;&#21644;&#22810;&#20256;&#24863;&#22120;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01260v1 Announce Type: cross  Abstract: In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. msGFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. msGFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include sce
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25968;&#27493;&#38271;&#65292;&#36890;&#36807;&#35813;&#27493;&#38271;&#21487;&#25913;&#21892;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#38750;&#20984;&#20989;&#25968;&#19978;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#22312;CNN&#27169;&#22411;&#19979;&#23558;CIFAR100&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;0.9%&#12290;</title><link>https://arxiv.org/abs/2404.01257</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26032;&#23545;&#25968;&#27493;&#38271;
&lt;/p&gt;
&lt;p&gt;
New logarithmic step size for stochastic gradient descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01257
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25968;&#27493;&#38271;&#65292;&#36890;&#36807;&#35813;&#27493;&#38271;&#21487;&#25913;&#21892;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#38750;&#20984;&#20989;&#25968;&#19978;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#22312;CNN&#27169;&#22411;&#19979;&#23558;CIFAR100&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;0.9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26032;&#30340;&#23545;&#25968;&#27493;&#38271;&#30340;&#28201;&#26262;&#37325;&#21551;&#25216;&#26415;&#65292;&#29992;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#12290;&#23545;&#20110;&#20809;&#28369;&#21644;&#38750;&#20984;&#20989;&#25968;&#65292;&#25105;&#20204;&#20026;SGD&#24314;&#31435;&#20102;&#19968;&#20010;$O(\frac{1}{\sqrt{T}})$&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#29616;&#65292;&#20197;&#23637;&#31034;&#26032;&#25552;&#20986;&#30340;&#27493;&#38271;&#22312;FashionMinst&#12289;CIFAR10&#21644;CIFAR100&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#19982;&#20854;&#20182;&#20061;&#31181;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#26102;&#65292;&#26032;&#30340;&#23545;&#25968;&#27493;&#38271;&#21487;&#20197;&#23558;CIFAR100&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#25552;&#39640;$0.9\%$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01257v1 Announce Type: new  Abstract: In this paper, we propose a novel warm restart technique using a new logarithmic step size for the stochastic gradient descent (SGD) approach. For smooth and non-convex functions, we establish an $O(\frac{1}{\sqrt{T}})$ convergence rate for the SGD. We conduct a comprehensive implementation to demonstrate the efficiency of the newly proposed step size on the ~FashionMinst,~ CIFAR10, and CIFAR100 datasets. Moreover, we compare our results with nine other existing approaches and demonstrate that the new logarithmic step size improves test accuracy by $0.9\%$ for the CIFAR100 dataset when we utilize a convolutional neural network (CNN) model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01245</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#26694;&#26550;: &#26530;&#36724;&#12289;&#26816;&#27979;&#25928;&#29575;&#21644;&#26368;&#20248;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;ChatGPT&#20110;2022&#24180;11&#26376;&#25512;&#20986;&#20197;&#26469;&#65292;&#23558;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#32479;&#35745;&#20449;&#21495;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#20063;&#34987;&#31216;&#20026;&#27700;&#21360;&#65292;&#24050;&#34987;&#29992;&#20316;&#20174;&#20854;&#20154;&#31867;&#25776;&#20889;&#23545;&#24212;&#29289;&#19978;&#21487;&#35777;&#26816;&#27979;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#24182;&#35774;&#35745;&#24378;&#22823;&#30340;&#26816;&#27979;&#35268;&#21017;&#12290;&#21463;&#27700;&#21360;&#26816;&#27979;&#30340;&#20551;&#35774;&#26816;&#39564;&#20844;&#24335;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#36873;&#25321;&#25991;&#26412;&#30340;&#26530;&#36724;&#32479;&#35745;&#37327;&#21644;&#30001;LLM&#25552;&#20379;&#32473;&#39564;&#35777;&#22120;&#30340;&#31192;&#23494;&#23494;&#38053;&#65292;&#20197;&#23454;&#29616;&#25511;&#21046;&#35823;&#25253;&#29575;&#65288;&#23558;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;LLM&#29983;&#25104;&#30340;&#38169;&#35823;&#65289;&#12290; &#25509;&#19979;&#26469;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#33719;&#21462;&#28176;&#36817;&#38169;&#35823;&#36127;&#29575;&#65288;&#23558;LLM&#29983;&#25104;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;&#20154;&#31867;&#25776;&#20889;&#30340;&#38169;&#35823;&#65289;&#30340;&#23553;&#38381;&#24418;&#24335;&#34920;&#36798;&#24335;&#26469;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01245v1 Announce Type: cross  Abstract: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#38024;&#23545;&#20998;&#24067;&#22806;&#39044;&#27979;&#30340;&#26368;&#20248;&#23725;&#22238;&#24402;&#27491;&#21017;&#21270;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#24314;&#31435;&#20102;&#30830;&#23450;&#26368;&#20248;&#27491;&#21017;&#21270;&#27700;&#24179;&#30340;&#19968;&#33324;&#26465;&#20214;&#65292;&#25581;&#31034;&#20102;&#19982;&#20998;&#24067;&#20869;&#35774;&#32622;&#30340;&#40092;&#26126;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.01233</link><description>&lt;p&gt;
&#38024;&#23545;&#20998;&#24067;&#22806;&#39044;&#27979;&#30340;&#26368;&#20248;&#23725;&#22238;&#24402;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimal Ridge Regularization for Out-of-Distribution Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01233
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#38024;&#23545;&#20998;&#24067;&#22806;&#39044;&#27979;&#30340;&#26368;&#20248;&#23725;&#22238;&#24402;&#27491;&#21017;&#21270;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#24314;&#31435;&#20102;&#30830;&#23450;&#26368;&#20248;&#27491;&#21017;&#21270;&#27700;&#24179;&#30340;&#19968;&#33324;&#26465;&#20214;&#65292;&#25581;&#31034;&#20102;&#19982;&#20998;&#24067;&#20869;&#35774;&#32622;&#30340;&#40092;&#26126;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;&#20998;&#24067;&#22806;&#39044;&#27979;&#30340;&#26368;&#20248;&#23725;&#22238;&#24402;&#27491;&#21017;&#21270;&#21644;&#26368;&#20248;&#23725;&#39118;&#38505;&#30340;&#34892;&#20026;&#65292;&#20854;&#20013;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#20219;&#24847;&#20559;&#31163;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#30830;&#23450;&#22312;&#21327;&#21464;&#37327;&#21644;&#22238;&#24402;&#20559;&#31227;&#19979;&#26368;&#20248;&#27491;&#21017;&#21270;&#27700;&#24179;&#31526;&#21495;&#30340;&#19968;&#33324;&#26465;&#20214;&#12290;&#36825;&#20123;&#26465;&#20214;&#25429;&#25417;&#20102;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#21327;&#26041;&#24046;&#21644;&#20449;&#21495;&#32467;&#26500;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#22312;&#20998;&#24067;&#20869;&#35774;&#32622;&#30456;&#27604;&#30340;&#40092;&#26126;&#24046;&#24322;&#12290;&#20363;&#22914;&#65292;&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#25110;&#22238;&#24402;&#20559;&#31227;&#19979;&#65292;&#21363;&#20351;&#35757;&#32451;&#29305;&#24449;&#26159;&#21508;&#21521;&#21516;&#24615;&#30340;&#25110;&#35774;&#35745;&#26159;&#27424;&#21442;&#25968;&#21270;&#30340;&#65292;&#36127;&#27491;&#21017;&#21270;&#27700;&#24179;&#20063;&#21487;&#33021;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#32437;&#27178;&#27604;&#20013;&#65292;&#29978;&#33267;&#22312;&#26368;&#20248;&#21270;&#36127;&#27491;&#21017;&#21270;&#27700;&#24179;&#26102;&#65292;&#26368;&#20248;&#35843;&#25972;&#30340;&#39118;&#38505;&#26159;&#21333;&#35843;&#30340;&#65292;&#21363;&#22312;&#20998;&#24067;&#22806;&#35774;&#32622;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#35757;&#32451;&#25968;&#25454;&#27809;&#26377;&#20570;&#20986;&#20219;&#20309;&#24314;&#27169;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01233v1 Announce Type: cross  Abstract: We study the behavior of optimal ridge regularization and optimal ridge risk for out-of-distribution prediction, where the test distribution deviates arbitrarily from the train distribution. We establish general conditions that determine the sign of the optimal regularization level under covariate and regression shifts. These conditions capture the alignment between the covariance and signal structures in the train and test data and reveal stark differences compared to the in-distribution setting. For example, a negative regularization level can be optimal under covariate shift or regression shift, even when the training features are isotropic or the design is underparameterized. Furthermore, we prove that the optimally-tuned risk is monotonic in the data aspect ratio, even in the out-of-distribution setting and when optimizing over negative regularization levels. In general, our results do not make any modeling assumptions for the tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#21518;&#38376;&#25915;&#20987;&#28431;&#27934;&#65292;&#36890;&#36807;&#27745;&#26579;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#22686;&#24378;&#25104;&#21592;&#25512;&#29702;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#25915;&#20987;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01231</link><description>&lt;p&gt;
&#38544;&#31169;&#21518;&#38376;: &#36890;&#36807;&#27745;&#26579;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#25104;&#21592;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#21518;&#38376;&#25915;&#20987;&#28431;&#27934;&#65292;&#36890;&#36807;&#27745;&#26579;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#22686;&#24378;&#25104;&#21592;&#25512;&#29702;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#25915;&#20987;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#23450;&#21046;&#25968;&#25454;&#38598;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#29983;&#25104;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#27169;&#22411;&#24050;&#32463;&#21496;&#31354;&#35265;&#24815;&#12290;&#32593;&#32476;&#19978;&#22522;&#30784;&#27169;&#22411;&#26816;&#26597;&#28857;&#30340;&#24191;&#27867;&#21487;&#29992;&#24615;&#23384;&#22312;&#37325;&#22823;&#39118;&#38505;&#65292;&#21253;&#25324;&#26131;&#21463;&#21518;&#38376;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#28431;&#27934;&#65306;&#38544;&#31169;&#21518;&#38376;&#25915;&#20987;&#12290;&#36825;&#31181;&#40657;&#30418;&#38544;&#31169;&#25915;&#20987;&#26088;&#22312;&#22686;&#24378;&#27169;&#22411;&#24494;&#35843;&#26102;&#20135;&#29983;&#30340;&#38544;&#31169;&#27844;&#38706;&#65306;&#24403;&#21463;&#23475;&#32773;&#24494;&#35843;&#19968;&#20010;&#24102;&#26377;&#21518;&#38376;&#30340;&#27169;&#22411;&#26102;&#65292;&#20182;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#27844;&#38706;&#36895;&#29575;&#20250;&#27604;&#24494;&#35843;&#20856;&#22411;&#27169;&#22411;&#26102;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21253;&#25324;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;CLIP&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#28040;&#34701;&#30740;&#31350;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#24494;&#35843;&#26041;&#27861;&#21644;&#25512;&#29702;&#31574;&#30053;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01231v1 Announce Type: cross  Abstract: It is commonplace to produce application-specific models by fine-tuning large pre-trained models using a small bespoke dataset. The widespread availability of foundation model checkpoints on the web poses considerable risks, including the vulnerability to backdoor attacks. In this paper, we unveil a new vulnerability: the privacy backdoor attack. This black-box privacy attack aims to amplify the privacy leakage that arises when fine-tuning a model: when a victim fine-tunes a backdoored model, their training data will be leaked at a significantly higher rate than if they had fine-tuned a typical model. We conduct extensive experiments on various datasets and models, including both vision-language models (CLIP) and large language models, demonstrating the broad applicability and effectiveness of such an attack. Additionally, we carry out multiple ablation studies with different fine-tuning methods and inference strategies to thoroughly a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;(CoPSL)&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#24085;&#32047;&#25176;&#38598;&#65292;&#36890;&#36807;&#20849;&#20139;&#21644;&#29305;&#23450;&#23618;&#30340;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;MOP&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2404.01224</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#21327;&#21516;&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collaborative Pareto Set Learning in Multiple Multi-Objective Optimization Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01224
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;(CoPSL)&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#24085;&#32047;&#25176;&#38598;&#65292;&#36890;&#36807;&#20849;&#20139;&#21644;&#29305;&#23450;&#23618;&#30340;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;MOP&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;(PSL)&#26159;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20174;&#20559;&#22909;&#21521;&#37327;&#21040;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#30340;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PSL&#26041;&#27861;&#20165;&#38480;&#20110;&#19968;&#27425;&#35299;&#20915;&#21333;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;(MOP)&#12290;&#38754;&#23545;&#22810;&#20010;MOP&#26102;&#65292;&#36825;&#31181;&#38480;&#21046;&#19981;&#20165;&#23548;&#33268;&#26174;&#33879;&#30340;&#20302;&#25928;&#65292;&#32780;&#19988;&#26410;&#33021;&#21033;&#29992;&#27178;&#36328;&#19981;&#21516;MOP&#30340;&#28508;&#22312;&#21327;&#21516;&#25928;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;(CoPSL)&#26694;&#26550;&#65292;&#23427;&#20197;&#21327;&#21516;&#26041;&#24335;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;MOP&#30340;&#24085;&#32047;&#25176;&#38598;&#12290;CoPSL&#37319;&#29992;&#20102;&#19968;&#20010;&#26550;&#26500;&#65292;&#21253;&#25324;&#20849;&#20139;&#21644;MOP&#29305;&#23450;&#23618;&#65292;&#20854;&#20013;&#20849;&#20139;&#23618;&#26088;&#22312;&#21327;&#21516;&#25429;&#25417;MOP&#20043;&#38388;&#30340;&#20844;&#20849;&#20851;&#31995;&#65292;&#32780;MOP&#29305;&#23450;&#23618;&#22788;&#29702;&#36825;&#20123;&#20851;&#31995;&#20197;&#29983;&#25104;&#27599;&#20010;MOP&#30340;&#35299;&#38598;&#12290;&#36825;&#31181;&#21327;&#21516;&#26041;&#27861;&#20351;&#24471;CoPSL&#33021;&#22815;&#39640;&#25928;&#22320;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01224v1 Announce Type: new  Abstract: Pareto Set Learning (PSL) is an emerging research area in multi-objective optimization, focusing on training neural networks to learn the mapping from preference vectors to Pareto optimal solutions. However, existing PSL methods are limited to addressing a single Multi-objective Optimization Problem (MOP) at a time. When faced with multiple MOPs, this limitation not only leads to significant inefficiencies but also fails to exploit the potential synergies across varying MOPs. In this paper, we propose a Collaborative Pareto Set Learning (CoPSL) framework, which simultaneously learns the Pareto sets of multiple MOPs in a collaborative manner. CoPSL employs an architecture consisting of shared and MOP-specific layers, where shared layers aim to capture common relationships among MOPs collaboratively, and MOP-specific layers process these relationships to generate solution sets for each MOP. This collaborative approach enables CoPSL to effi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Feature Splatting&#30340;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#24577;&#22330;&#26223;&#21512;&#25104;&#19982;&#28304;&#33258;&#35270;&#35273;&#35821;&#35328;&#30340;&#20016;&#23500;&#35821;&#20041;&#32479;&#19968;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#23545;&#35937;&#20998;&#35299;&#21644;&#22522;&#20110;&#25991;&#26412;&#26597;&#35810;&#30340;&#29289;&#29702;&#29305;&#24615;&#33258;&#21160;&#21512;&#25104;&#12290;</title><link>https://arxiv.org/abs/2404.01223</link><description>&lt;p&gt;
&#29305;&#24449;&#28857;&#20999;&#29255;&#65306;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#22330;&#26223;&#21512;&#25104;&#21644;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Feature Splatting: Language-Driven Physics-Based Scene Synthesis and Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01223
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Feature Splatting&#30340;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#24577;&#22330;&#26223;&#21512;&#25104;&#19982;&#28304;&#33258;&#35270;&#35273;&#35821;&#35328;&#30340;&#20016;&#23500;&#35821;&#20041;&#32479;&#19968;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#23545;&#35937;&#20998;&#35299;&#21644;&#22522;&#20110;&#25991;&#26412;&#26597;&#35810;&#30340;&#29289;&#29702;&#29305;&#24615;&#33258;&#21160;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;3D&#39640;&#26031;&#21407;&#35821;&#34920;&#31034;&#22330;&#26223;&#22312;&#24314;&#27169;&#38745;&#24577;&#21644;&#21160;&#24577;3D&#22330;&#26223;&#30340;&#22806;&#35266;&#26041;&#38754;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290; &#28982;&#32780;&#65292;&#35768;&#22810;&#22270;&#24418;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#33021;&#22815;&#25805;&#32437;&#23545;&#35937;&#30340;&#22806;&#35266;&#21644;&#29289;&#29702;&#29305;&#24615;&#12290; &#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Feature Splatting&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#24577;&#22330;&#26223;&#21512;&#25104;&#19982;&#28304;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20016;&#23500;&#35821;&#20041;&#32479;&#19968;&#36215;&#26469;&#12290; &#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39640;&#36136;&#37327;&#30340;&#65292;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#35270;&#35273;&#35821;&#35328;&#29305;&#24449;&#25552;&#28860;&#25104;3D&#39640;&#26031;&#65292;&#20174;&#32780;&#21033;&#29992;&#25991;&#26412;&#26597;&#35810;&#23454;&#29616;&#21322;&#33258;&#21160;&#22330;&#26223;&#20998;&#35299;&#12290; &#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#31890;&#23376;&#30340;&#27169;&#25311;&#22120;&#20174;&#38745;&#24577;&#22330;&#26223;&#20013;&#21512;&#25104;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#24577;&#22330;&#26223;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#26448;&#26009;&#23646;&#24615;&#36890;&#36807;&#25991;&#26412;&#26597;&#35810;&#33258;&#21160;&#20998;&#37197;&#12290; &#25105;&#20204;&#21076;&#38500;&#20102;&#22312;&#36825;&#20010;&#27969;&#31243;&#20013;&#20351;&#29992;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20197;&#38416;&#26126;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01223v1 Announce Type: cross  Abstract: Scene representations using 3D Gaussian primitives have produced excellent results in modeling the appearance of static and dynamic 3D scenes. Many graphics applications, however, demand the ability to manipulate both the appearance and the physical properties of objects. We introduce Feature Splatting, an approach that unifies physics-based dynamic scene synthesis with rich semantics from vision language foundation models that are grounded by natural language. Our first contribution is a way to distill high-quality, object-centric vision-language features into 3D Gaussians, that enables semi-automatic scene decomposition using text queries. Our second contribution is a way to synthesize physics-based dynamics from an otherwise static scene using a particle-based simulator, in which material properties are assigned automatically via text queries. We ablate key techniques used in this pipeline, to illustrate the challenge and opportunit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20010;&#23545;&#35937;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#23545;&#35937;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#30446;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#35757;&#32451;&#20195;&#29702;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01220</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#20307;&#20013;&#24515;&#30340;&#20687;&#32032;&#32423;&#30446;&#26631;&#25805;&#32437;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Entity-Centric Reinforcement Learning for Object Manipulation from Pixels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01220
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20010;&#23545;&#35937;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#23545;&#35937;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#30446;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#35757;&#32451;&#20195;&#29702;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25805;&#32437;&#29289;&#20307;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#26631;&#24535;&#65292;&#20063;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20010;&#23545;&#35937;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#26041;&#27861;&#65292;&#24182;&#29992;&#23427;&#26469;&#23398;&#20064;&#20960;&#20010;&#23545;&#35937;&#30340;&#30446;&#26631;&#25805;&#32437;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#23545;&#35937;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#30446;&#26631;&#65288;&#20363;&#22914;&#25353;&#29305;&#23450;&#39034;&#24207;&#31227;&#21160;&#23545;&#35937;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26550;&#26500;&#19982;&#35757;&#32451;&#20195;&#29702;&#30340;&#27867;&#21270;&#33021;&#21147;&#32852;&#31995;&#36215;&#26469;&#65292;&#22522;&#20110;&#22797;&#21512;&#27867;&#21270;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#23398;&#20064;3&#20010;&#23545;&#35937;&#20294;&#33021;&#27867;&#21270;&#21040;&#31867;&#20284;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01220v1 Announce Type: cross  Abstract: Manipulating objects is a hallmark of human intelligence, and an important task in domains such as robotics. In principle, Reinforcement Learning (RL) offers a general approach to learn object manipulation. In practice, however, domains with more than a few objects are difficult for RL agents due to the curse of dimensionality, especially when learning from raw image observations. In this work we propose a structured approach for visual RL that is suitable for representing multiple objects and their interaction, and use it to learn goal-conditioned manipulation of several objects. Key to our method is the ability to handle goals with dependencies between the objects (e.g., moving objects in a certain order). We further relate our architecture to the generalization capability of the trained agent, based on a theoretical result for compositional generalization, and demonstrate agents that learn with 3 objects but generalize to similar ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#24314;&#27169;&#25903;&#25345;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#30142;&#30149;&#25968;&#25454;&#65292;&#20197;&#20415;&#23558;&#20854;&#29992;&#20110;&#21307;&#24072;&#30740;&#31350;&#27963;&#21160;&#65292;&#35299;&#20915;EHRs&#25968;&#25454;&#26684;&#24335;&#22810;&#26679;&#21270;&#30340;&#26631;&#20934;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01218</link><description>&lt;p&gt;
&#25903;&#25345;&#21307;&#24072;&#30740;&#31350;&#27963;&#21160;&#30340;&#31995;&#32479;&#24314;&#27169;&#20197;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#30142;&#30149;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Towards System Modelling to Support Diseases Data Extraction from the Electronic Health Records for Physicians Research Activities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#24314;&#27169;&#25903;&#25345;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#30142;&#30149;&#25968;&#25454;&#65292;&#20197;&#20415;&#23558;&#20854;&#29992;&#20110;&#21307;&#24072;&#30740;&#31350;&#27963;&#21160;&#65292;&#35299;&#20915;EHRs&#25968;&#25454;&#26684;&#24335;&#22810;&#26679;&#21270;&#30340;&#26631;&#20934;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;15&#24180;&#20013;&#65292;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#30340;&#20351;&#29992;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#22240;&#20026;&#23427;&#34987;&#35748;&#20026;&#26159;&#31649;&#29702;&#30149;&#20154;&#25968;&#25454;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;EHRs&#26159;&#19990;&#30028;&#33539;&#22260;&#20869;&#30142;&#30149;&#35786;&#26029;&#21644;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35832;&#22914;&#30740;&#31350;&#20043;&#31867;&#30340;&#27425;&#35201;&#20219;&#21153;&#12290;&#26412;&#25991;&#26088;&#22312;&#20351;&#36825;&#20123;&#25968;&#25454;&#21487;&#29992;&#20110;&#30417;&#27979;&#29305;&#23450;&#20154;&#32676;&#30340;&#30142;&#30149;&#32479;&#35745;&#25968;&#25454;&#31561;&#30740;&#31350;&#27963;&#21160;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26816;&#27979;&#30446;&#26631;&#32676;&#20307;&#30340;&#30149;&#22240;&#19982;&#34892;&#20026;&#21644;&#29983;&#27963;&#26041;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;EHRs&#31995;&#32479;&#30340;&#23616;&#38480;&#20043;&#19968;&#26159;&#25968;&#25454;&#19981;&#20197;&#26631;&#20934;&#26684;&#24335;&#32780;&#26159;&#20197;&#21508;&#31181;&#24418;&#24335;&#21487;&#29992;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#39318;&#20808;&#23558;&#30142;&#30149;&#21517;&#31216;&#21644;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#36716;&#25442;&#20026;&#19968;&#20010;&#26631;&#20934;&#26684;&#24335;&#65292;&#20197;&#20351;&#20854;&#21487;&#29992;&#20110;&#30740;&#31350;&#27963;&#21160;&#12290;&#26377;&#22823;&#37327;&#30340;EHRs&#21487;&#29992;&#65292;&#35299;&#20915;&#26631;&#20934;&#21270;&#38382;&#39064;&#38656;&#35201;&#19968;&#20123;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01218v1 Announce Type: new  Abstract: The use of Electronic Health Records (EHRs) has increased dramatically in the past 15 years, as, it is considered an important source of managing data od patients. The EHRs are primary sources of disease diagnosis and demographic data of patients worldwide. Therefore, the data can be utilized for secondary tasks such as research. This paper aims to make such data usable for research activities such as monitoring disease statistics for a specific population. As a result, the researchers can detect the disease causes for the behavior and lifestyle of the target group. One of the limitations of EHRs systems is that the data is not available in the standard format but in various forms. Therefore, it is required to first convert the names of the diseases and demographics data into one standardized form to make it usable for research activities. There is a large amount of EHRs available, and solving the standardizing issues requires some optim
&lt;/p&gt;</description></item><item><title>&#23558;&#39046;&#22495;&#24494;&#20998;&#26041;&#31243;&#32435;&#20837;&#22270;&#21367;&#31215;&#32593;&#32476;&#21487;&#25552;&#39640;&#20854;&#23545;&#19981;&#21305;&#37197;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01217</link><description>&lt;p&gt;
&#23558;&#39046;&#22495;&#24494;&#20998;&#26041;&#31243;&#32435;&#20837;&#22270;&#21367;&#31215;&#32593;&#32476;&#20197;&#38477;&#20302;&#27867;&#21270;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Incorporating Domain Differential Equations into Graph Convolutional Networks to Lower Generalization Discrepancy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01217
&lt;/p&gt;
&lt;p&gt;
&#23558;&#39046;&#22495;&#24494;&#20998;&#26041;&#31243;&#32435;&#20837;&#22270;&#21367;&#31215;&#32593;&#32476;&#21487;&#25552;&#39640;&#20854;&#23545;&#19981;&#21305;&#37197;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#23545;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#22478;&#24066;&#35268;&#21010;&#21040;&#30123;&#24773;&#31649;&#29702;&#12290;&#26412;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#28085;&#30422;&#25152;&#26377;&#26102;&#31354;&#27169;&#24335;&#30340;&#20805;&#20998;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20570;&#20986;&#30456;&#24403;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#24403;&#35757;&#32451;&#25968;&#25454;&#26469;&#33258;&#19982;&#27979;&#35797;&#25968;&#25454;&#19981;&#21516;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#27491;&#24120;&#22825;&#20132;&#36890;&#27169;&#24335;&#19982;&#33258;&#28982;&#28798;&#23475;&#21518;&#20132;&#36890;&#27169;&#24335;&#65289;&#26102;&#65292;&#29616;&#26377;&#26041;&#27861;&#20250;&#22833;&#36133;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#23558;&#39046;&#22495;&#24494;&#20998;&#26041;&#31243;&#32435;&#20837;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01217v1 Announce Type: cross  Abstract: Ensuring both accuracy and robustness in time series prediction is critical to many applications, ranging from urban planning to pandemic management. With sufficient training data where all spatiotemporal patterns are well-represented, existing deep-learning models can make reasonably accurate predictions. However, existing methods fail when the training data are drawn from different circumstances (e.g., traffic patterns on regular days) compared to test data (e.g., traffic patterns after a natural disaster). Such challenges are usually classified under domain generalization. In this work, we show that one way to address this challenge in the context of spatiotemporal prediction is by incorporating domain differential equations into Graph Convolutional Networks (GCNs). We theoretically derive conditions where GCNs incorporating such domain differential equations are robust to mismatched training and testing data compared to baseline do
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; RECO-SLIP&#65292;&#29992;&#20110;&#22312;&#23646;&#24615;&#22270;&#20013;&#26816;&#27979;&#23646;&#20110;&#26032;&#31867;&#21035;&#30340;&#33410;&#28857;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#23376;&#32676;&#20307;&#36716;&#31227;&#19979;&#30340;&#33410;&#28857;&#26816;&#27979;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2404.01216</link><description>&lt;p&gt;
&#22312;&#23376;&#32676;&#20307;&#36716;&#31227;&#19979;&#30340;&#26032;&#39062;&#33410;&#28857;&#31867;&#21035;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Novel Node Category Detection Under Subpopulation Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01216
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; RECO-SLIP&#65292;&#29992;&#20110;&#22312;&#23646;&#24615;&#22270;&#20013;&#26816;&#27979;&#23646;&#20110;&#26032;&#31867;&#21035;&#30340;&#33410;&#28857;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#23376;&#32676;&#20307;&#36716;&#31227;&#19979;&#30340;&#33410;&#28857;&#26816;&#27979;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#20013;&#65292;&#20998;&#24067;&#36716;&#31227;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#26041;&#24335;&#34920;&#29616;&#65292;&#20363;&#22914;&#26032;&#31867;&#21035;&#30340;&#20986;&#29616;&#21644;&#29616;&#26377;&#31867;&#21035;&#30456;&#23545;&#27604;&#20363;&#30340;&#21464;&#21270;&#12290;&#22312;&#36825;&#31181;&#20998;&#24067;&#36716;&#31227;&#19979;&#65292;&#26816;&#27979;&#23646;&#20110;&#26032;&#31867;&#21035;&#30340;&#33410;&#28857;&#23545;&#20110;&#23433;&#20840;&#25110;&#27934;&#23519;&#21457;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#20855;&#26377;&#36873;&#25321;&#24615;&#38142;&#36335;&#39044;&#27979;&#30340;&#21484;&#22238;&#32422;&#26463;&#20248;&#21270;&#65288;RECO-SLIP&#65289;&#65292;&#29992;&#20110;&#22312;&#23376;&#32676;&#20307;&#36716;&#31227;&#19979;&#26816;&#27979;&#23646;&#24615;&#22270;&#20013;&#23646;&#20110;&#26032;&#31867;&#21035;&#30340;&#33410;&#28857;&#12290;&#36890;&#36807;&#23558;&#21484;&#22238;&#32422;&#26463;&#23398;&#20064;&#26694;&#26550;&#19982;&#39640;&#25928;&#26679;&#26412;&#39044;&#27979;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;RECO-SLIP&#35299;&#20915;&#20102;&#25269;&#25239;&#23376;&#32676;&#20307;&#36716;&#31227;&#21644;&#26377;&#25928;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#21452;&#37325;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;RECO-SLIP&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01216v1 Announce Type: new  Abstract: In real-world graph data, distribution shifts can manifest in various ways, such as the emergence of new categories and changes in the relative proportions of existing categories. It is often important to detect nodes of novel categories under such distribution shifts for safety or insight discovery purposes. We introduce a new approach, Recall-Constrained Optimization with Selective Link Prediction (RECO-SLIP), to detect nodes belonging to novel categories in attributed graphs under subpopulation shifts. By integrating a recall-constrained learning framework with a sample-efficient link prediction mechanism, RECO-SLIP addresses the dual challenges of resilience against subpopulation shifts and the effective exploitation of graph structure. Our extensive empirical evaluation across multiple graph datasets demonstrates the superior performance of RECO-SLIP over existing methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#26426;&#22120;&#36951;&#24536;&#22312;&#20256;&#32479;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#23450;&#20041;&#12289;&#20998;&#31867;&#12289;&#35780;&#20272;&#26631;&#20934;&#20197;&#21450;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.01206</link><description>&lt;p&gt;
&#20256;&#32479;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#65306;&#31616;&#30701;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning for Traditional Models and Large Language Models: A Short Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01206
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#26426;&#22120;&#36951;&#24536;&#22312;&#20256;&#32479;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#23450;&#20041;&#12289;&#20998;&#31867;&#12289;&#35780;&#20272;&#26631;&#20934;&#20197;&#21450;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;&#35268;&#23450;&#30340;&#23454;&#26045;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#38754;&#20020;&#8220;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#8221;&#30340;&#25361;&#25112;&#12290;&#26426;&#22120;&#36951;&#24536;&#24212;&#36816;&#32780;&#29983;&#65292;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#35831;&#27714;&#21024;&#38500;&#25968;&#25454;&#24182;&#20943;&#23569;&#20854;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#26426;&#22120;&#36951;&#24536;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#23545;&#20854;&#26368;&#26032;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#30340;&#20840;&#38754;&#35843;&#26597;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#28145;&#20837;&#25506;&#35752;&#26426;&#22120;&#36951;&#24536;&#65292;&#21253;&#25324;&#23450;&#20041;&#12289;&#20998;&#31867;&#21644;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#25361;&#25112;&#21450;&#20854;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#23545;&#20256;&#32479;&#27169;&#22411;&#21644;LLMs&#19978;&#30340;&#36951;&#24536;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#36951;&#24536;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#32489;&#25928;&#34913;&#37327;&#26631;&#20934;&#12290;&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01206v1 Announce Type: new  Abstract: With the implementation of personal data privacy regulations, the field of machine learning (ML) faces the challenge of the "right to be forgotten". Machine unlearning has emerged to address this issue, aiming to delete data and reduce its impact on models according to user requests. Despite the widespread interest in machine unlearning, comprehensive surveys on its latest advancements, especially in the field of Large Language Models (LLMs) is lacking. This survey aims to fill this gap by providing an in-depth exploration of machine unlearning, including the definition, classification and evaluation criteria, as well as challenges in different environments and their solutions. Specifically, this paper categorizes and investigates unlearning on both traditional models and LLMs, and proposes methods for evaluating the effectiveness and efficiency of unlearning, and standards for performance measurement. This paper reveals the limitations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#20984;&#32422;&#26463;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#25972;&#20307;&#25968;&#25454;&#38598;&#22823;&#23567;&#26080;&#20851;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2404.01200</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#38750;&#20984;&#38543;&#26426;&#32422;&#26463;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Non-convex Stochastic Constrained Distributionally Robust Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#20984;&#32422;&#26463;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#25972;&#20307;&#25968;&#25454;&#38598;&#22823;&#23567;&#26080;&#20851;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#26159;&#38024;&#23545;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#35757;&#32451;&#20581;&#22766;&#27169;&#22411;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#26412;&#25991;&#20851;&#27880;&#20855;&#26377;&#40065;&#26834;&#24615;&#27700;&#24179;&#26126;&#30830;&#29305;&#24449;&#30340;&#32422;&#26463;DRO&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20855;&#26377;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#32422;&#26463;DRO&#19978;&#65292;&#24182;&#25490;&#38500;&#20102;&#20855;&#26377;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#23454;&#36341;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#20026;&#38750;&#20984;&#32422;&#26463;DRO&#24320;&#21457;&#20102;&#19968;&#31181;&#38543;&#26426;&#31639;&#27861;&#21450;&#20854;&#24615;&#33021;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#38543;&#26426;&#31639;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#25972;&#20307;&#25968;&#25454;&#38598;&#22823;&#23567;&#29420;&#31435;&#26080;&#20851;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;&#25105;&#20204;&#20391;&#37325;&#20110;&#23558;Cressie-Read&#23478;&#26063;&#25955;&#24230;&#23450;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#25104;&#20013;&#21253;&#21547;$\chi^2$-&#25955;&#24230;&#20316;&#20026;&#29305;&#20363;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;$\mathcal O(\epsilon^{-3k_*-5})$&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#20102;&#19968;&#20010;$\epsilon$-&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01200v1 Announce Type: cross  Abstract: Distributionally robust optimization (DRO) is a powerful framework for training robust models against data distribution shifts. This paper focuses on constrained DRO, which has an explicit characterization of the robustness level. Existing studies on constrained DRO mostly focus on convex loss function, and exclude the practical and challenging case with non-convex loss function, e.g., neural network. This paper develops a stochastic algorithm and its performance analysis for non-convex constrained DRO. The computational complexity of our stochastic algorithm at each iteration is independent of the overall dataset size, and thus is suitable for large-scale applications. We focus on the general Cressie-Read family divergence defined uncertainty set which includes $\chi^2$-divergences as a special case. We prove that our algorithm finds an $\epsilon$-stationary point with a computational complexity of $\mathcal O(\epsilon^{-3k_*-5})$, wh
&lt;/p&gt;</description></item><item><title>&#23545;&#25913;&#36827;&#30340;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#38543;&#26426;&#22312;&#32447;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20102;&#35299;&#26368;&#20248;&#33218;&#26368;&#22823;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;$O(\sqrt{k} \log k)$&#30340;&#36924;&#36817;&#30456;&#23545;&#20110;&#26368;&#20248;&#12290;</title><link>https://arxiv.org/abs/2404.01198</link><description>&lt;p&gt;
&#23545;&#25913;&#36827;&#30340;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#36817;&#20046;&#26368;&#32039;&#23494;&#30340;&#36924;&#36817;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Nearly-tight Approximation Guarantees for the Improving Multi-Armed Bandits Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01198
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25913;&#36827;&#30340;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#38543;&#26426;&#22312;&#32447;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20102;&#35299;&#26368;&#20248;&#33218;&#26368;&#22823;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;$O(\sqrt{k} \log k)$&#30340;&#36924;&#36817;&#30456;&#23545;&#20110;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#25913;&#36827;&#30340;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#25552;&#20379;&#20102;&#36817;&#20046;&#26368;&#32039;&#23494;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#23454;&#20363;&#26377;$k$&#20010;&#33218;&#65292;&#27599;&#20010;&#33218;&#30340;&#22870;&#21169;&#20989;&#25968;&#37117;&#26159;&#19968;&#20010;&#20985;&#20989;&#25968;&#65292;&#24182;&#19988;&#26159;&#19968;&#20010;&#19982;&#21040;&#30446;&#21069;&#20026;&#27490;&#25289;&#21160;&#35813;&#33218;&#30340;&#27425;&#25968;&#25104;&#22686;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;&#38543;&#26426;&#22312;&#32447;&#31639;&#27861;&#65292;&#37117;&#23384;&#22312;&#19968;&#20010;&#23454;&#20363;&#65292;&#20351;&#20854;&#30456;&#23545;&#20110;&#26368;&#20248;&#22870;&#21169;&#24517;&#39035;&#33267;&#23569;&#25215;&#21463;&#19968;&#20010;$\Omega(\sqrt{k})$&#30340;&#36817;&#20284;&#22240;&#23376;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#38543;&#26426;&#22312;&#32447;&#31639;&#27861;&#65292;&#22914;&#26524;&#20107;&#20808;&#21578;&#30693;&#26368;&#20248;&#33218;&#21487;&#23454;&#29616;&#30340;&#26368;&#22823;&#22870;&#21169;&#65292;&#23601;&#21487;&#20197;&#20445;&#35777;&#19968;&#20010;$O(\sqrt{k})$&#30340;&#36817;&#20284;&#22240;&#23376;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#39069;&#22806;&#20184;&#20986;$O(\log k)$&#30340;&#36817;&#20284;&#22240;&#23376;&#30340;&#20195;&#20215;&#19979;&#65292;&#28040;&#38500;&#36825;&#20010;&#20551;&#35774;&#65292;&#23454;&#29616;&#30456;&#23545;&#20110;&#26368;&#20248;&#30340;&#24635;&#20307;$O(\sqrt{k} \log k)$&#30340;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01198v1 Announce Type: new  Abstract: We give nearly-tight upper and lower bounds for the improving multi-armed bandits problem. An instance of this problem has $k$ arms, each of whose reward function is a concave and increasing function of the number of times that arm has been pulled so far. We show that for any randomized online algorithm, there exists an instance on which it must suffer at least an $\Omega(\sqrt{k})$ approximation factor relative to the optimal reward. We then provide a randomized online algorithm that guarantees an $O(\sqrt{k})$ approximation factor, if it is told the maximum reward achievable by the optimal arm in advance. We then show how to remove this assumption at the cost of an extra $O(\log k)$ approximation factor, achieving an overall $O(\sqrt{k} \log k)$ approximation relative to optimal.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#35825;&#23548;&#30340;&#31070;&#32463;&#25511;&#21046;&#22120;&#26469;&#20943;&#23569;&#22522;&#20110;&#37319;&#26679;&#30340;&#36816;&#21160;&#35268;&#21010;&#20013;&#25152;&#38656;&#26679;&#26412;&#25968;&#37327;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#30896;&#25758;&#36991;&#20813;&#25511;&#21046;&#21644;&#38271;&#35270;&#31243;&#36816;&#21160;&#35268;&#21010;&#30340;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2404.01184</link><description>&lt;p&gt;
&#20855;&#26377;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#35825;&#23548;&#31070;&#32463;&#25511;&#21046;&#22120;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#39640;&#25928;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Efficient Motion Planning for Manipulators with Control Barrier Function-Induced Neural Controller
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01184
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#35825;&#23548;&#30340;&#31070;&#32463;&#25511;&#21046;&#22120;&#26469;&#20943;&#23569;&#22522;&#20110;&#37319;&#26679;&#30340;&#36816;&#21160;&#35268;&#21010;&#20013;&#25152;&#38656;&#26679;&#26412;&#25968;&#37327;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#30896;&#25758;&#36991;&#20813;&#25511;&#21046;&#21644;&#38271;&#35270;&#31243;&#36816;&#21160;&#35268;&#21010;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#30340;&#26426;&#22120;&#20154;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#36890;&#24120;&#20250;&#21463;&#21040;&#26114;&#36149;&#30340;&#30896;&#25758;&#26816;&#26597;&#21644;&#39640;&#37319;&#26679;&#22797;&#26434;&#24615;&#30340;&#22256;&#25200;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#23454;&#26102;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;(CBF)&#20026;&#22522;&#30784;&#30340;&#36716;&#21521;&#25511;&#21046;&#22120;&#65292;&#20197;&#20943;&#23569;&#37319;&#26679;&#24335;&#36816;&#21160;&#35268;&#21010;&#22120;RRT&#20013;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;CBF&#29992;&#20110;&#23454;&#26102;&#36991;&#20813;&#30896;&#25758;&#25511;&#21046;&#30340;&#20248;&#21183;&#21644;RRT&#29992;&#20110;&#38271;&#35270;&#31243;&#36816;&#21160;&#35268;&#21010;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20351;&#29992;CBF&#35825;&#23548;&#30340;&#31070;&#32463;&#25511;&#21046;&#22120;(CBF-INC)&#29983;&#25104;&#25805;&#32437;&#20449;&#21495;&#65292;&#23558;&#31995;&#32479;&#24341;&#23548;&#21521;RRT&#37319;&#26679;&#30340;&#37197;&#32622;&#12290;CBF-INC&#34987;&#23398;&#20064;&#20026;&#31070;&#32463;&#32593;&#32476;&#65292;&#26377;&#20004;&#20010;&#22788;&#29702;&#19981;&#21516;&#36755;&#20837;&#30340;&#21464;&#20307;&#65292;&#20998;&#21035;&#26159;&#65306;&#29366;&#24577;&#65288;&#31526;&#21495;&#36317;&#31163;&#65289;&#36755;&#20837;&#21644;&#26469;&#33258;LiDAR&#30340;&#28857;&#20113;&#36755;&#20837;&#12290;&#22312;&#21518;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#65306;&#23436;&#20840;&#21644;&#37096;&#20998;&#35266;&#23519;&#30340;&#29615;&#22659;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01184v1 Announce Type: cross  Abstract: Sampling-based motion planning methods for manipulators in crowded environments often suffer from expensive collision checking and high sampling complexity, which make them difficult to use in real time. To address this issue, we propose a new generalizable control barrier function (CBF)-based steering controller to reduce the number of samples needed in a sampling-based motion planner RRT. Our method combines the strength of CBF for real-time collision-avoidance control and RRT for long-horizon motion planning, by using CBF-induced neural controller (CBF-INC) to generate control signals that steer the system towards sampled configurations by RRT. CBF-INC is learned as Neural Networks and has two variants handling different inputs, respectively: state (signed distance) input and point-cloud input from LiDAR. In the latter case, we also study two different settings: fully and partially observed environmental information. Compared to man
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BEM&#30340;&#24179;&#34913;&#29109;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#26088;&#22312;&#37325;&#26032;&#24179;&#34913;&#25968;&#25454;&#25968;&#37327;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#31867;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2404.01179</link><description>&lt;p&gt;
BEM&#65306;&#29992;&#20110;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24179;&#34913;&#29109;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BEM&#30340;&#24179;&#34913;&#29109;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#26088;&#22312;&#37325;&#26032;&#24179;&#34913;&#25968;&#25454;&#25968;&#37327;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#31867;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#28151;&#21512;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;LTSSL&#65289;&#20013;&#30340;&#24212;&#29992;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Balanced and Entropy-based Mix&#65288;BEM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#26032;&#24179;&#34913;&#25968;&#25454;&#25968;&#37327;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#31867;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01179v1 Announce Type: cross  Abstract: Data mixing methods play a crucial role in semi-supervised learning (SSL), but their application is unexplored in long-tailed semi-supervised learning (LTSSL). The primary reason is that the in-batch mixing manner fails to address class imbalance. Furthermore, existing LTSSL methods mainly focus on re-balancing data quantity but ignore class-wise uncertainty, which is also vital for class balance. For instance, some classes with sufficient samples might still exhibit high uncertainty due to indistinguishable features. To this end, this paper introduces the Balanced and Entropy-based Mix (BEM), a pioneering mixing approach to re-balance the class distribution of both data quantity and uncertainty. Specifically, we first propose a class balanced mix bank to store data of each class for mixing. This bank samples data based on the estimated quantity distribution, thus re-balancing data quantity. Then, we present an entropy-based learning a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#34701;&#21512;&#27491;&#21017;&#21270;&#22120;&#30340;&#20004;&#27493;&#27861;&#26041;&#27861;&#65292;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#22238;&#24402;&#20013;&#30340;&#27169;&#22411;&#20559;&#31227;&#21644;&#21327;&#21464;&#37327;&#36716;&#31227;&#65292;&#25552;&#39640;&#20102;&#30446;&#26631;&#20219;&#21153;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#24182;&#28385;&#36275;&#26368;&#23567;-&#26368;&#22823;&#26368;&#20248;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2404.01153</link><description>&lt;p&gt;
TransFusion&#65306;&#29992;&#20110;&#39640;&#32500;&#22238;&#24402;&#30340;&#25239;&#21327;&#21464;&#37327;&#36716;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TransFusion: Covariate-Shift Robust Transfer Learning for High-Dimensional Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01153
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#34701;&#21512;&#27491;&#21017;&#21270;&#22120;&#30340;&#20004;&#27493;&#27861;&#26041;&#27861;&#65292;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#22238;&#24402;&#20013;&#30340;&#27169;&#22411;&#20559;&#31227;&#21644;&#21327;&#21464;&#37327;&#36716;&#31227;&#65292;&#25552;&#39640;&#20102;&#30446;&#26631;&#20219;&#21153;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#24182;&#28385;&#36275;&#26368;&#23567;-&#26368;&#22823;&#26368;&#20248;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#19982;&#36716;&#31227;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#20998;&#24067;&#20559;&#31227;&#65292;&#20307;&#29616;&#20026;&#28304;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#20559;&#31227;&#20197;&#21450;&#36793;&#38469;&#21327;&#21464;&#37327;&#20998;&#24067;&#20043;&#38388;&#30340;&#20559;&#31227;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#39640;&#32500;&#22238;&#24402;&#35774;&#32622;&#20013;&#22788;&#29702;&#23384;&#22312;&#21327;&#21464;&#37327;&#21464;&#21270;&#30340;&#27169;&#22411;&#21464;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#27861;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#34701;&#21512;&#27491;&#21017;&#21270;&#22120;&#65292;&#26377;&#25928;&#21033;&#29992;&#26469;&#33258;&#28304;&#20219;&#21153;&#30340;&#26679;&#26412;&#26469;&#25552;&#39640;&#22312;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;&#25552;&#20379;&#20102;&#30446;&#26631;&#27169;&#22411;&#20272;&#35745;&#35823;&#24046;&#30340;&#38750;&#28176;&#36817;&#30028;&#38480;&#65292;&#26174;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#23545;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#20272;&#35745;&#22120;&#26159;&#26368;&#23567;-&#26368;&#22823;&#26368;&#20248;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#20998;&#24067;&#24335;&#35774;&#32622;&#65292;&#20801;&#35768;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#20165;&#38656;&#19968;&#36718;&#36890;&#20449;&#21363;&#21487;&#20445;&#30041;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01153v1 Announce Type: cross  Abstract: The main challenge that sets transfer learning apart from traditional supervised learning is the distribution shift, reflected as the shift between the source and target models and that between the marginal covariate distributions. In this work, we tackle model shifts in the presence of covariate shifts in the high-dimensional regression setting. Specifically, we propose a two-step method with a novel fused-regularizer that effectively leverages samples from source tasks to improve the learning performance on a target task with limited samples. Nonasymptotic bound is provided for the estimation error of the target model, showing the robustness of the proposed method to covariate shifts. We further establish conditions under which the estimator is minimax-optimal. Additionally, we extend the method to a distributed setting, allowing for a pretraining-finetuning strategy, requiring just one round of communication while retaining the esti
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#27169;&#25311;&#22522;&#20110;&#20107;&#23454;&#38382;&#39064;&#30340;&#31038;&#20132;&#23186;&#20307;&#38382;&#39064;&#20013;&#20154;&#31867;&#31572;&#26696;&#26102;&#34920;&#29616;&#36739;&#22909;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2404.01147</link><description>&lt;p&gt;
LLM&#26159;&#21542;&#20250;&#23545;&#22522;&#20110;&#20107;&#23454;&#30340;&#38382;&#39064;&#30340;&#20154;&#31867;&#31572;&#26696;&#24863;&#21040;&#22256;&#24785;&#65311;&#20197;Reddit&#20026;&#20010;&#26696;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01147
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#27169;&#25311;&#22522;&#20110;&#20107;&#23454;&#38382;&#39064;&#30340;&#31038;&#20132;&#23186;&#20307;&#38382;&#39064;&#20013;&#20154;&#31867;&#31572;&#26696;&#26102;&#34920;&#29616;&#36739;&#22909;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#29087;&#32451;&#22320;&#27491;&#30830;&#22238;&#31572;&#22312;&#32447;&#35805;&#35821;&#29615;&#22659;&#19979;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;LLMs&#26469;&#27169;&#25311;&#20154;&#31867;&#23545;&#22522;&#20110;&#20107;&#23454;&#30340;&#31038;&#20132;&#23186;&#20307;&#38382;&#39064;&#30340;&#22238;&#31572;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#22914;&#20309;&#27169;&#25311;&#22312;&#20960;&#20010;&#19987;&#39064;&#24615;Reddit&#31038;&#21306;&#65288;&#25110;&#23376;&#31038;&#21306;&#65289;&#20013;&#25552;&#20986;&#30340;&#22522;&#20110;&#20107;&#23454;&#38382;&#39064;&#30340;&#21508;&#31181;&#20154;&#31867;&#31572;&#26696;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#20844;&#24320;&#20102;409&#20010;&#22522;&#20110;&#20107;&#23454;&#38382;&#39064;&#21644;&#26469;&#33258;15&#20010;r/Ask{Topic}&#31038;&#21306;&#30340;7,534&#20010;&#22810;&#26679;&#21270;&#30340;&#12289;&#32463;&#20154;&#31867;&#35780;&#20998;&#30340;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#31038;&#21306;&#35206;&#30422;&#20102;3&#20010;&#31867;&#21035;&#65306;&#32844;&#19994;&#12289;&#31038;&#20250;&#36523;&#20221;&#21644;&#22320;&#29702;&#20301;&#32622;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#27169;&#25311;&#39640;&#35780;&#20998;&#30340;&#20154;&#31867;&#31572;&#26696;&#26041;&#38754;&#35201;&#27604;&#27169;&#25311;&#20302;&#35780;&#20998;&#30340;&#20154;&#31867;&#31572;&#26696;&#25928;&#26524;&#26174;&#33879;&#12290;&#25105;&#20204;&#22522;&#20110;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01147v1 Announce Type: new  Abstract: Large language models (LLMs) have been shown to be proficient in correctly answering questions in the context of online discourse. However, the study of using LLMs to model human-like answers to fact-driven social media questions is still under-explored. In this work, we investigate how LLMs model the wide variety of human answers to fact-driven questions posed on several topic-specific Reddit communities, or subreddits. We collect and release a dataset of 409 fact-driven questions and 7,534 diverse, human-rated answers from 15 r/Ask{Topic} communities across 3 categories: profession, social identity, and geographic location. We find that LLMs are considerably better at modeling highly-rated human answers to such questions, as opposed to poorly-rated human answers. We present several directions for future research based on our initial findings.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26174;&#31034;&#36880;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#34987;&#24191;&#27867;&#29702;&#35299;&#20026;&#20248;&#21270;-&#31163;&#25955;&#21270;&#25110;&#31163;&#25955;&#21270;-&#20248;&#21270;&#26041;&#26696;&#65292;&#22312;&#31283;&#23450;&#24615;&#21644;&#35823;&#24046;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#26032;&#39062;&#32467;&#26524;&#65292;&#21516;&#26102;&#26377;&#21161;&#20110;&#24314;&#31435;&#26041;&#27861;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;</title><link>https://arxiv.org/abs/2404.01145</link><description>&lt;p&gt;
&#22312;&#35299;&#20915;&#26102;&#21464;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#36827;&#34892;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#30340;&#36880;&#26102;&#38388;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sequential-in-time training of nonlinear parametrizations for solving time-dependent partial differential equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01145
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26174;&#31034;&#36880;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#34987;&#24191;&#27867;&#29702;&#35299;&#20026;&#20248;&#21270;-&#31163;&#25955;&#21270;&#25110;&#31163;&#25955;&#21270;-&#20248;&#21270;&#26041;&#26696;&#65292;&#22312;&#31283;&#23450;&#24615;&#21644;&#35823;&#24046;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#26032;&#39062;&#32467;&#26524;&#65292;&#21516;&#26102;&#26377;&#21161;&#20110;&#24314;&#31435;&#26041;&#27861;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36880;&#26102;&#38388;&#39034;&#24207;&#26041;&#27861;&#35299;&#20915;&#19968;&#31995;&#21015;&#35757;&#32451;&#38382;&#39064;&#65292;&#20197;&#25311;&#21512;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#20197;&#36817;&#20284;&#35299;&#30340;&#36712;&#36857;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#36880;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#29702;&#35299;&#20026;&#20248;&#21270;-&#31163;&#25955;&#21270;&#65288;OtD&#65289;&#25110;&#31163;&#25955;&#21270;-&#20248;&#21270;&#65288;DtO&#65289;&#26041;&#26696;&#65292;&#36825;&#22312;&#25968;&#20540;&#20998;&#26512;&#20013;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#27010;&#24565;&#12290;&#32479;&#19968;&#30340;&#35266;&#28857;&#23548;&#33268;&#20102;&#25552;&#20986;&#26032;&#30340;&#31283;&#23450;&#24615;&#21644;&#21518;&#39564;&#35823;&#24046;&#20998;&#26512;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#23545;OtD&#25110;DtO&#26041;&#26696;&#26412;&#36136;&#30340;&#29702;&#35770;&#21644;&#25968;&#20540;&#26041;&#38754;&#27934;&#23519;&#21147;&#65292;&#20363;&#22914;&#27491;&#20999;&#31354;&#38388;&#22349;&#32553;&#29616;&#35937;&#65292;&#36825;&#26159;&#19968;&#31181;&#36807;&#25311;&#21512;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#32479;&#19968;&#30340;&#35266;&#28857;&#26377;&#21161;&#20110;&#24314;&#31435;&#36880;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#30340;&#21464;&#20307;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#36890;&#36807;&#30830;&#23450;&#23558;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#24212;&#29992;&#20026;OtD&#26041;&#26696;&#30340;&#33021;&#37327;&#27867;&#20989;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01145v1 Announce Type: cross  Abstract: Sequential-in-time methods solve a sequence of training problems to fit nonlinear parametrizations such as neural networks to approximate solution trajectories of partial differential equations over time. This work shows that sequential-in-time training methods can be understood broadly as either optimize-then-discretize (OtD) or discretize-then-optimize (DtO) schemes, which are well known concepts in numerical analysis. The unifying perspective leads to novel stability and a posteriori error analysis results that provide insights into theoretical and numerical aspects that are inherent to either OtD or DtO schemes such as the tangent space collapse phenomenon, which is a form of over-fitting. Additionally, the unified perspective facilitates establishing connections between variants of sequential-in-time training methods, which is demonstrated by identifying natural gradient descent methods on energy functionals as OtD schemes applied
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#39640;&#32500;&#25968;&#25454;&#20013;&#24046;&#20998;&#31169;&#26377;&#32447;&#24615;&#27169;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#23457;&#26597;&#65292;&#21457;&#29616;&#40065;&#26834;&#21644;&#20248;&#21270;&#30340;&#22352;&#26631;&#31639;&#27861;&#25928;&#26524;&#26368;&#22909;&#65292;&#21487;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2404.01141</link><description>&lt;p&gt;
SoK: &#39640;&#32500;&#25968;&#25454;&#20013;&#24046;&#20998;&#31169;&#26377;&#32447;&#24615;&#27169;&#22411;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
SoK: A Review of Differentially Private Linear Models For High-Dimensional Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#39640;&#32500;&#25968;&#25454;&#20013;&#24046;&#20998;&#31169;&#26377;&#32447;&#24615;&#27169;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#23457;&#26597;&#65292;&#21457;&#29616;&#40065;&#26834;&#21644;&#20248;&#21270;&#30340;&#22352;&#26631;&#31639;&#27861;&#25928;&#26524;&#26368;&#22909;&#65292;&#21487;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#27169;&#22411;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#38543;&#22788;&#21487;&#35265;&#65292;&#20294;&#22312;&#39640;&#32500;&#24230;&#20013;&#29305;&#21035;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#21644;&#25968;&#25454;&#35760;&#24518;&#12290;&#20026;&#20102;&#20445;&#35777;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#65292;&#21487;&#20197;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#12290;&#35768;&#22810;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#32500;&#24230;&#24046;&#20998;&#31169;&#26377;&#32447;&#24615;&#27169;&#22411;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#32570;&#20047;&#31995;&#32479;&#27604;&#36739;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#31169;&#26377;&#39640;&#32500;&#32447;&#24615;&#27169;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#23457;&#26597;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#23545;&#25152;&#26377;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#65292;&#40065;&#26834;&#21644;&#20248;&#21270;&#30340;&#22352;&#26631;&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#65292;&#36825;&#21487;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#21442;&#32771;&#12290;&#22312;&#32447;&#21457;&#24067;&#20102;&#25152;&#26377;&#26041;&#27861;&#30340;&#23454;&#29616;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01141v1 Announce Type: new  Abstract: Linear models are ubiquitous in data science, but are particularly prone to overfitting and data memorization in high dimensions. To guarantee the privacy of training data, differential privacy can be used. Many papers have proposed optimization techniques for high-dimensional differentially private linear models, but a systematic comparison between these methods does not exist. We close this gap by providing a comprehensive review of optimization methods for private high-dimensional linear models. Empirical tests on all methods demonstrate robust and coordinate-optimized algorithms perform best, which can inform future research. Code for implementing all methods is released online.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#29289;&#29702;&#20449;&#24687;&#30340;ConvLSTM2D&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#36739;&#32454;&#23610;&#24230;&#30340;&#38477;&#38632;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#38024;&#23545;&#23391;&#20080;&#22478;&#24066;&#30340;&#38477;&#27700;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2404.01122</link><description>&lt;p&gt;
&#23391;&#20080;&#38477;&#38632;&#39044;&#27979;&#30340;&#31934;&#30830;&#24230;&#22686;&#24378;&#65306;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#30340;ConvLSTM2D&#27169;&#22411;&#23454;&#29616;&#26356;&#31934;&#32454;&#30340;&#26102;&#31354;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Enhanced Precision in Rainfall Forecasting for Mumbai: Utilizing Physics Informed ConvLSTM2D Models for Finer Spatial and Temporal Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#29289;&#29702;&#20449;&#24687;&#30340;ConvLSTM2D&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#36739;&#32454;&#23610;&#24230;&#30340;&#38477;&#38632;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#38024;&#23545;&#23391;&#20080;&#22478;&#24066;&#30340;&#38477;&#27700;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28909;&#24102;&#22320;&#21306;&#39044;&#27979;&#38477;&#38632;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#30001;&#20110;&#22823;&#27668;&#34892;&#20026;&#22797;&#26434;&#12289;&#28287;&#24230;&#39640;&#65292;&#19988;&#23545;&#27969;&#24615;&#38477;&#38632;&#20107;&#20214;&#26222;&#36941;&#23384;&#22312;&#12290;&#22312;&#21360;&#24230;&#30340;&#32972;&#26223;&#19979;&#65292;&#38477;&#38632;&#39044;&#27979;&#30340;&#22256;&#38590;&#36827;&#19968;&#27493;&#21152;&#21095;&#65292;&#22240;&#20026;&#23395;&#39118;&#23395;&#33410;&#24615;&#25391;&#33633;&#20250;&#22312;&#30701;&#26399;&#20869;&#24341;&#20837;&#26174;&#33879;&#30340;&#38477;&#38632;&#27169;&#24335;&#21464;&#21270;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28145;&#24230;&#23398;&#20064;&#31354;&#38388;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#36739;&#32454;&#23610;&#24230;&#30340;&#38477;&#38632;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#20551;&#35774;&#34701;&#21512;&#29289;&#29702;&#29702;&#35299;&#21487;&#20197;&#25552;&#39640;&#20855;&#26377;&#36739;&#39640;&#31934;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#36739;&#32454;&#23610;&#24230;&#65288;&#22914;&#22478;&#24066;&#65289;&#30340;&#38477;&#27700;&#39044;&#27979;&#33021;&#21147;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#30340;ConvLSTM2D&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#23391;&#20080;&#22478;&#24066;&#26410;&#26469;6&#23567;&#26102;&#21644;12&#23567;&#26102;&#30340;&#38477;&#27700;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01122v1 Announce Type: new  Abstract: Forecasting rainfall in tropical areas is challenging due to complex atmospheric behaviour, elevated humidity levels, and the common presence of convective rain events. In the Indian context, the difficulty is further exacerbated because of the monsoon intra seasonal oscillations, which introduce significant variability in rainfall patterns over short periods. Earlier investigations into rainfall prediction leveraged numerical weather prediction methods, along with statistical and deep learning approaches. This study introduces deep learning spatial model aimed at enhancing rainfall prediction accuracy on a finer scale. In this study, we hypothesize that integrating physical understanding improves the precipitation prediction skill of deep learning models with high precision for finer spatial scales, such as cities. To test this hypothesis, we introduce a physics informed ConvLSTM2D model to predict precipitation 6hr and 12hr ahead for M
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25193;&#25955;&#24341;&#23548;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#36328;&#27169;&#24577;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2404.01102</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#38646;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#29992;&#20110;&#36328;&#27169;&#24577;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Diffusion based Zero-shot Medical Image-to-Image Translation for Cross Modality Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01102
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#24341;&#23548;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#36328;&#27169;&#24577;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#27169;&#24577;&#22270;&#20687;&#20998;&#21106;&#26088;&#22312;&#20351;&#29992;&#22312;&#28304;&#27169;&#24577;&#20013;&#35774;&#35745;&#30340;&#26041;&#27861;&#23545;&#30446;&#26631;&#27169;&#24577;&#36827;&#34892;&#20998;&#21106;&#12290;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#23558;&#30446;&#26631;&#27169;&#24577;&#22270;&#20687;&#36716;&#25442;&#20026;&#28304;&#27169;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#36328;&#27169;&#24577;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#37327;&#20132;&#21449;&#27169;&#24577;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#20381;&#36182;&#20110;&#30417;&#30563;&#23398;&#20064;&#12290;&#26412;&#24037;&#20316;&#26088;&#22312;&#35299;&#20915;&#22522;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#22270;&#20687;&#32763;&#35793;&#20219;&#21153;&#30340;&#25361;&#25112;&#65288;&#26497;&#31471;&#24773;&#20917;&#19979;&#30446;&#26631;&#27169;&#24577;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#30693;&#65289;&#12290;&#20026;&#20102;&#21033;&#29992;&#29983;&#25104;&#23398;&#20064;&#36827;&#34892;&#38646;&#26679;&#26412;&#36328;&#27169;&#24577;&#22270;&#20687;&#20998;&#21106;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#22266;&#26377;&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#36827;&#34892;&#25193;&#25955;&#24341;&#23548;&#65292;&#23398;&#20064;&#23558;&#26410;&#30693;&#28304;&#22270;&#20687;&#36716;&#25442;&#20026;&#30446;&#26631;&#27169;&#24577;&#20197;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25429;&#25417;&#20102;&#30456;&#21516;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01102v1 Announce Type: cross  Abstract: Cross-modality image segmentation aims to segment the target modalities using a method designed in the source modality. Deep generative models can translate the target modality images into the source modality, thus enabling cross-modality segmentation. However, a vast body of existing cross-modality image translation methods relies on supervised learning. In this work, we aim to address the challenge of zero-shot learning-based image translation tasks (extreme scenarios in the target modality is unseen in the training phase). To leverage generative learning for zero-shot cross-modality image segmentation, we propose a novel unsupervised image translation method. The framework learns to translate the unseen source image to the target modality for image segmentation by leveraging the inherent statistical consistency between different modalities for diffusion guidance. Our framework captures identical cross-modality features in the statis
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#29992;&#20110;&#36755;&#20837;&#32423;&#21518;&#38376;&#26816;&#27979;&#65292;&#24357;&#34917;&#20102;&#35813;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#24182;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#30340;&#30333;&#30418;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2404.01101</link><description>&lt;p&gt;
UFID: &#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#19978;&#30340;&#36755;&#20837;&#32423;&#21518;&#38376;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01101
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#29992;&#20110;&#36755;&#20837;&#32423;&#21518;&#38376;&#26816;&#27979;&#65292;&#24357;&#34917;&#20102;&#35813;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#24182;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#30340;&#30333;&#30418;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#21363;&#24694;&#24847;&#25915;&#20987;&#32773;&#22312;&#35757;&#32451;&#38454;&#27573;&#36890;&#36807;&#23545;&#37096;&#20998;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#27602;&#21270;&#26469;&#27880;&#20837;&#21518;&#38376;&#12290;&#20026;&#20102;&#20943;&#36731;&#21518;&#38376;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#23545;&#21518;&#38376;&#26816;&#27979;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#20154;&#20026;&#25193;&#25955;&#27169;&#22411;&#35774;&#35745;&#20102;&#19987;&#38376;&#30340;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#24471;&#36825;&#19968;&#39046;&#22495;&#36739;&#23569;&#34987;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#24456;&#38590;&#36731;&#26494;&#22320;&#23558;&#20854;&#36866;&#24212;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#21518;&#38376;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#21644;&#26550;&#26500;&#30340;&#30333;&#30418;&#35775;&#38382;&#65292;&#25110;&#27010;&#29575;logits&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#65292;&#36825;&#24182;&#19981;&#24635;&#26159;&#20999;&#23454;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01101v1 Announce Type: cross  Abstract: Diffusion Models are vulnerable to backdoor attacks, where malicious attackers inject backdoors by poisoning some parts of the training samples during the training stage. This poses a serious threat to the downstream users, who query the diffusion models through the API or directly download them from the internet. To mitigate the threat of backdoor attacks, there have been a plethora of investigations on backdoor detections. However, none of them designed a specialized backdoor detection method for diffusion models, rendering the area much under-explored. Moreover, these prior methods mainly focus on the traditional neural networks in the classification task, which cannot be adapted to the backdoor detections on the generative task easily. Additionally, most of the prior methods require white-box access to model weights and architectures, or the probability logits as additional information, which are not always practical. In this paper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#36827;&#34892;&#38750;&#21442;&#25968;&#39057;&#22495;&#31995;&#32479;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;Empirical Transfer Function Estimate&#65288;ETFE&#65289;&#22312;&#29305;&#23450;&#39057;&#29575;&#22788;&#20934;&#30830;&#20272;&#35745;&#39057;&#29575;&#21709;&#24212;&#65292;&#24182;&#35777;&#26126;&#22312;&#27425;&#39640;&#26031;&#24425;&#33394;&#22122;&#22768;&#21644;&#31283;&#23450;&#24615;&#20551;&#35774;&#19979;&#65292;ETFE&#20272;&#35745;&#20540;&#20934;&#30830;&#21487;&#38752;&#12290;</title><link>https://arxiv.org/abs/2404.01100</link><description>&lt;p&gt;
&#26377;&#38480;&#26679;&#26412;&#39057;&#22495;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Finite Sample Frequency Domain Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#36827;&#34892;&#38750;&#21442;&#25968;&#39057;&#22495;&#31995;&#32479;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;Empirical Transfer Function Estimate&#65288;ETFE&#65289;&#22312;&#29305;&#23450;&#39057;&#29575;&#22788;&#20934;&#30830;&#20272;&#35745;&#39057;&#29575;&#21709;&#24212;&#65292;&#24182;&#35777;&#26126;&#22312;&#27425;&#39640;&#26031;&#24425;&#33394;&#22122;&#22768;&#21644;&#31283;&#23450;&#24615;&#20551;&#35774;&#19979;&#65292;ETFE&#20272;&#35745;&#20540;&#20934;&#30830;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#39057;&#22495;&#31995;&#32479;&#35782;&#21035;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#24320;&#29615;&#24773;&#20917;&#19979;&#65292;&#28608;&#21169;&#36755;&#20837;&#26159;&#21608;&#26399;&#24615;&#30340;&#65292;&#24182;&#32771;&#34385;&#32463;&#39564;&#20256;&#36882;&#20989;&#25968;&#20272;&#35745;&#65288;ETFE&#65289;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#36755;&#20837;-&#36755;&#20986;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#22312;&#26576;&#20123;&#25152;&#38656;&#65288;&#22343;&#21248;&#38388;&#38548;&#30340;&#65289;&#39057;&#29575;&#22788;&#20272;&#35745;&#39057;&#29575;&#21709;&#24212;&#12290;&#25105;&#20204;&#34920;&#26126;&#22312;&#27425;&#39640;&#26031;&#24425;&#33394;&#22122;&#22768;&#65288;&#22312;&#26102;&#22495;&#65289;&#21644;&#31283;&#23450;&#24615;&#20551;&#35774;&#19979;&#65292;ETFE&#20272;&#35745;&#20540;&#38598;&#20013;&#22312;&#30495;&#23454;&#20540;&#21608;&#22260;&#12290;&#35823;&#24046;&#29575;&#20026;$\mathcal{O}((d_{\mathrm{u}}+\sqrt{d_{\mathrm{u}}d_{\mathrm{y}}})\sqrt{M/N_{\mathrm{tot}}})$&#65292;&#20854;&#20013;$N_{\mathrm{tot}}$&#26159;&#26679;&#26412;&#30340;&#24635;&#25968;&#65292;$M$&#26159;&#25152;&#38656;&#39057;&#29575;&#30340;&#25968;&#37327;&#65292;$d_{\mathrm{u}},\,d_{\mathrm{y}}$&#20998;&#21035;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#20449;&#21495;&#30340;&#32500;&#25968;&#12290;&#36825;&#20010;&#36895;&#29575;&#23545;&#20110;&#19968;&#33324;&#30340;&#38750;&#29702;&#24615;&#20256;&#36882;&#20989;&#25968;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26377;&#38480;&#38454;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01100v1 Announce Type: cross  Abstract: We study non-parametric frequency-domain system identification from a finite-sample perspective. We assume an open loop scenario where the excitation input is periodic and consider the Empirical Transfer Function Estimate (ETFE), where the goal is to estimate the frequency response at certain desired (evenly-spaced) frequencies, given input-output samples. We show that under sub-Gaussian colored noise (in time-domain) and stability assumptions, the ETFE estimates are concentrated around the true values. The error rate is of the order of $\mathcal{O}((d_{\mathrm{u}}+\sqrt{d_{\mathrm{u}}d_{\mathrm{y}}})\sqrt{M/N_{\mathrm{tot}}})$, where $N_{\mathrm{tot}}$ is the total number of samples, $M$ is the number of desired frequencies, and $d_{\mathrm{u}},\,d_{\mathrm{y}}$ are the dimensions of the input and output signals respectively. This rate remains valid for general irrational transfer functions and does not require a finite order state-sp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35782;&#21035;&#37027;&#20123;&#22312;&#24494;&#35843;&#21518;&#26356;&#21487;&#33021;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#26377;&#23475;&#35831;&#27714;&#30340;&#21709;&#24212;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.01099</link><description>&lt;p&gt;
&#20320;&#30340;&#8220;&#23433;&#20840;&#8221;&#25968;&#25454;&#20013;&#26377;&#20160;&#20040;&#65311;&#65306;&#35782;&#21035;&#30772;&#22351;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01099
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35782;&#21035;&#37027;&#20123;&#22312;&#24494;&#35843;&#21518;&#26356;&#21487;&#33021;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#26377;&#23475;&#35831;&#27714;&#30340;&#21709;&#24212;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21363;&#20351;&#32463;&#36807;&#35843;&#25972;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#20063;&#23481;&#26131;&#34987;&#36234;&#29425;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#21482;&#26159;&#36827;&#19968;&#27493;&#20351;&#29992;&#33391;&#24615;&#25968;&#25454;&#65288;&#21363;&#27809;&#26377;&#26377;&#23475;&#20869;&#23481;&#30340;&#25968;&#25454;&#65289;&#23545;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20250;&#23548;&#33268;&#23433;&#20840;&#24615;&#22823;&#24133;&#19979;&#38477;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#33391;&#24615;&#24494;&#35843;&#19981;&#32463;&#24847;&#38388;&#23548;&#33268;&#36234;&#29425;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#35270;&#35282;&#34920;&#24449;&#24494;&#35843;&#25968;&#25454;&#65306;&#34920;&#31034;&#21644;&#26799;&#24230;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#38752;&#36817;&#26377;&#23475;&#31034;&#20363;&#24182;&#36828;&#31163;&#33391;&#24615;&#31034;&#20363;&#30340;&#25968;&#25454;&#28857;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#26356;&#26377;&#21487;&#33021;&#22312;&#24494;&#35843;&#21518;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#12290;&#20165;&#20165;&#35757;&#32451;100&#20010;&#36825;&#20123;&#30475;&#20284;&#33391;&#24615;&#30340;&#25968;&#25454;&#28857;&#65292;&#23601;&#21487;&#20197;&#20351;&#24494;&#35843;&#27169;&#22411;&#32943;&#23450;&#22320;&#22238;&#24212;&#36229;&#36807;70&#65285;&#30340;&#34987;&#27979;&#35797;&#30340;&#26377;&#23475;&#35831;&#27714;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01099v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to &gt; 70% of tested harmful requests, compared to &lt;
&lt;/p&gt;</description></item><item><title>EmSHAP&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;Shapley&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;GRU&#26469;&#28040;&#38500;&#36755;&#20837;&#29305;&#24449;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#26377;&#25928;&#36817;&#20284;&#20219;&#24847;&#29305;&#24449;&#23376;&#38598;&#19979;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;Shapley&#20540;&#36129;&#29486;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2404.01078</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#20934;&#30830;Shapley&#20540;&#20272;&#35745;&#29992;&#20110;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Energy Model-based Accurate Shapley Value Estimation for Interpretable Deep Learning Predictive Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01078
&lt;/p&gt;
&lt;p&gt;
EmSHAP&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;Shapley&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;GRU&#26469;&#28040;&#38500;&#36755;&#20837;&#29305;&#24449;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#26377;&#25928;&#36817;&#20284;&#20219;&#24847;&#29305;&#24449;&#23376;&#38598;&#19979;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;Shapley&#20540;&#36129;&#29486;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26377;&#21033;&#24037;&#20855;&#65292;Shapley&#20540;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#36127;&#36733;&#38543;&#30528;&#36755;&#20837;&#29305;&#24449;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#20272;&#35745;Shapley&#20540;&#26159;&#19968;&#39033;&#22256;&#38590;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21152;&#36895;Shapley&#20540;&#20272;&#35745;&#26041;&#27861;&#24517;&#39035;&#22312;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#20570;&#20986;&#22949;&#21327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EmSHAP&#65288;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;Shapley&#20540;&#20272;&#35745;&#65289;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#39044;&#26399;Shapley&#36129;&#29486;&#20989;&#25968;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20219;&#24847;&#29305;&#24449;&#23376;&#38598;&#19979;&#32473;&#20986;&#20854;&#20313;&#29305;&#24449;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#30830;&#23450;&#33021;&#37327;&#27169;&#22411;&#20013;&#30340;&#25552;&#35758;&#26465;&#20214;&#20998;&#24067;&#65292;&#24341;&#20837;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#29305;&#24449;&#26144;&#23556;&#21040;&#38544;&#34255;&#31354;&#38388;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#36755;&#20837;&#29305;&#24449;&#39034;&#24207;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#21160;&#24577;&#25513;&#34109;&#26041;&#26696;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01078v1 Announce Type: new  Abstract: As a favorable tool for explainable artificial intelligence (XAI), Shapley value has been widely used to interpret deep learning based predictive models. However, accurate and efficient estimation of Shapley value is a difficult task since the computation load grows exponentially with the increase of input features. Most existing accelerated Shapley value estimation methods have to compromise on estimation accuracy with efficiency. In this article, we present EmSHAP(Energy model-based Shapley value estimation), which can effectively approximate the expectation of Shapley contribution function/deep learning model under arbitrary subset of features given the rest. In order to determine the proposal conditional distribution in the energy model, a gated recurrent unit(GRU) is introduced by mapping the input features onto a hidden space, so that the impact of input feature orderings can be eliminated. In addition, a dynamic masking scheme is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#25552;&#31034;&#23398;&#20064;&#24341;&#20837;&#21040;&#38754;&#21521;&#23545;&#35937;&#26816;&#27979;&#22120;&#65288;P2Det&#65289;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#21270;&#21644;&#25552;&#31034;&#20301;&#32622;&#26469;&#25552;&#21319;&#39640;&#20998;&#36776;&#29575;SAR&#22270;&#20687;&#20013;&#36755;&#30005;&#22612;&#30340;&#26816;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2404.01074</link><description>&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;SAR&#22270;&#20687;&#20013;&#26041;&#21521;&#24615;&#36755;&#30005;&#22612;&#26816;&#27979;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt Learning for Oriented Power Transmission Tower Detection in High-Resolution SAR Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25552;&#31034;&#23398;&#20064;&#24341;&#20837;&#21040;&#38754;&#21521;&#23545;&#35937;&#26816;&#27979;&#22120;&#65288;P2Det&#65289;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#21270;&#21644;&#25552;&#31034;&#20301;&#32622;&#26469;&#25552;&#21319;&#39640;&#20998;&#36776;&#29575;SAR&#22270;&#20687;&#20013;&#36755;&#30005;&#22612;&#30340;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#22270;&#20687;&#20013;&#26816;&#27979;&#36755;&#30005;&#22612;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#30001;&#20110;&#36755;&#30005;&#22612;&#30340;&#30456;&#23545;&#36739;&#23567;&#23610;&#23544;&#21644;&#20391;&#35270;&#20960;&#20309;&#24418;&#29366;&#65292;&#32972;&#26223;&#26434;&#27874;&#24178;&#25200;&#32463;&#24120;&#38459;&#30861;&#20102;&#22612;&#30340;&#35782;&#21035;&#12290;&#22823;&#37327;&#30340;&#24178;&#25200;&#20449;&#21495;&#21472;&#21152;&#22312;&#22612;&#30340;&#22238;&#27874;&#20449;&#21495;&#19978;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23450;&#20301;&#25110;&#25552;&#31034;&#36755;&#30005;&#22612;&#30340;&#20301;&#32622;&#26377;&#21033;&#20110;&#35299;&#20915;&#36825;&#19968;&#38556;&#30861;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#25991;&#23558;&#25552;&#31034;&#23398;&#20064;&#24341;&#20837;&#21040;&#38754;&#21521;&#23545;&#35937;&#26816;&#27979;&#22120;&#65288;P2Det&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#20449;&#24687;&#23398;&#20064;&#12290;P2Det&#21253;&#21547;&#20102;&#31232;&#30095;&#25552;&#31034;&#32534;&#30721;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25552;&#20986;&#20102;&#31232;&#30095;&#25552;&#31034;&#32534;&#30721;&#22120;&#65288;SPE&#65289;&#26469;&#34920;&#31034;&#28857;&#20301;&#32622;&#65292;&#23558;&#25552;&#31034;&#36716;&#25442;&#20026;&#31232;&#30095;&#23884;&#20837;&#12290;&#22270;&#20687;&#23884;&#20837;&#26159;&#36890;&#36807;Transformer&#23618;&#29983;&#25104;&#30340;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#21521;&#34701;&#21512;&#27169;&#22359;&#65288;TWFM&#65289;&#26469;&#35745;&#31639;&#20132;&#21449;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01074v1 Announce Type: cross  Abstract: Detecting transmission towers from synthetic aperture radar (SAR) images remains a challenging task due to the comparatively small size and side-looking geometry, with background clutter interference frequently hindering tower identification. A large number of interfering signals superimposes the return signal from the tower. We found that localizing or prompting positions of power transmission towers is beneficial to address this obstacle. Based on this revelation, this paper introduces prompt learning into the oriented object detector (P2Det) for multimodal information learning. P2Det contains the sparse prompt coding and cross-attention between the multimodal data. Specifically, the sparse prompt encoder (SPE) is proposed to represent point locations, converting prompts into sparse embeddings. The image embeddings are generated through the Transformer layers. Then a two-way fusion module (TWFM) is proposed to calculate the cross-att
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#28909;&#21147;&#23398;&#21407;&#29702;&#21040;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#26469;&#22686;&#21152;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;</title><link>https://arxiv.org/abs/2404.01060</link><description>&lt;p&gt;
&#28909;&#21147;&#23398;&#25351;&#23548;&#31070;&#32463;&#32593;&#32476;&#30340;&#21333;&#21457;&#30005;&#26426;&#21644;&#21452;&#21457;&#30005;&#26426;&#24418;&#24335;&#20027;&#20041;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A comparison of Single- and Double-generator formalisms for Thermodynamics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01060
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#28909;&#21147;&#23398;&#21407;&#29702;&#21040;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#26469;&#22686;&#21152;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#24341;&#20837;&#24402;&#32435;&#20559;&#35265;&#26159;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#27979;&#29289;&#29702;&#29616;&#35937;&#26102;&#12290;&#36825;&#20123;&#20559;&#35265;&#26174;&#33879;&#22686;&#21152;&#20102;&#39044;&#27979;&#30340;&#30830;&#23450;&#24615;&#65292;&#38477;&#20302;&#20102;&#35823;&#24046;&#65292;&#24182;&#20801;&#35768;&#20351;&#29992;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#12290;&#25991;&#29486;&#20013;&#26377;&#35768;&#22810;&#26041;&#27861;&#21487;&#20197;&#24320;&#21457;&#36825;&#20123;&#20559;&#35265;&#12290;&#22312;&#22788;&#29702;&#29289;&#29702;&#29616;&#35937;&#26102;&#65292;&#20854;&#20013;&#19968;&#31181;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#23558;&#34987;&#35748;&#21487;&#30340;&#29289;&#29702;&#21407;&#21017;&#24341;&#20837;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01060v1 Announce Type: new  Abstract: The development of inductive biases has been shown to be a very effective way to increase the accuracy and robustness of neural networks, particularly when they are used to predict physical phenomena. These biases significantly increase the certainty of predictions, decrease the error made and allow considerably smaller datasets to be used.   There are a multitude of methods in the literature to develop these biases. One of the most effective ways, when dealing with physical phenomena, is to introduce physical principles of recognised validity into the network architecture.   The problem becomes more complex without knowledge of the physical principles governing the phenomena under study. A very interesting possibility then is to turn to the principles of thermodynamics, which are universally valid, regardless of the level of abstraction of the description sought for the phenomenon under study.   To ensure compliance with the principles 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#39057;&#34920;&#31034;&#24418;&#24335;&#65292;&#20294;&#21457;&#29616;&#20854;&#22312;&#38899;&#20048;&#27969;&#27966;&#35782;&#21035;&#20219;&#21153;&#20013;&#24182;&#19981;&#27604;&#20256;&#32479;&#30340;Mel&#39057;&#35889;&#22270;&#26356;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2404.01058</link><description>&lt;p&gt;
&#38899;&#20048;&#27969;&#27966;&#35782;&#21035;&#20013;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A Novel Audio Representation for Music Genre Identification in MIR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#39057;&#34920;&#31034;&#24418;&#24335;&#65292;&#20294;&#21457;&#29616;&#20854;&#22312;&#38899;&#20048;&#27969;&#27966;&#35782;&#21035;&#20219;&#21153;&#20013;&#24182;&#19981;&#27604;&#20256;&#32479;&#30340;Mel&#39057;&#35889;&#22270;&#26356;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#26368;&#24120;&#35265;&#30340;&#38899;&#39057;&#34920;&#31034;&#24418;&#24335;&#26159;&#22522;&#20110;&#26102;&#39057;&#30340;&#34920;&#31034;&#65292;&#20363;&#22914;Mel&#39057;&#35889;&#22270;&#12290;&#20026;&#20102;&#35782;&#21035;&#38899;&#20048;&#27969;&#27966;&#65292;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#38899;&#39057;&#34920;&#31034;&#65292;&#29992;&#20110;&#20854;&#20013;&#26368;&#24120;&#35265;&#30340;MIR&#19979;&#28216;&#20219;&#21153;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20351;&#29992;&#28145;&#24230;&#21521;&#37327;&#37327;&#21270;&#31163;&#25955;&#22320;&#32534;&#30721;&#38899;&#20048;&#65307;&#20026;&#21019;&#26032;&#30340;&#29983;&#25104;&#24335;&#38899;&#20048;&#27169;&#22411;Jukebox&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#36890;&#36807;&#20351;&#29992;&#20960;&#20046;&#31561;&#21516;&#20110;&#19994;&#30028;&#39046;&#20808;&#27700;&#24179;(SOTA)&#21644;&#20960;&#20046;&#30456;&#21516;&#30340;&#21464;&#21387;&#22120;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#26469;&#27604;&#36739;Jukebox&#30340;&#38899;&#39057;&#34920;&#31034;&#19982;Mel&#39057;&#35889;&#22270;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#26263;&#31034;&#65292;&#33267;&#23569;&#24403;&#21464;&#21387;&#22120;&#20351;&#29992;&#38750;&#24120;&#36866;&#24230;&#30340;&#25968;&#25454;&#38598;&#65288;20k&#39318;&#38899;&#36712;&#65289;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;Jukebox&#30340;&#38899;&#39057;&#34920;&#31034;&#24182;&#19981;&#20248;&#20110;Mel&#39057;&#35889;&#22270;&#12290;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;Jukebox&#30340;&#38899;&#39057;&#34920;&#31034;&#24182;&#26410;&#20805;&#20998;&#32771;&#34385;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01058v1 Announce Type: cross  Abstract: For Music Information Retrieval downstream tasks, the most common audio representation is time-frequency-based, such as Mel spectrograms. In order to identify musical genres, this study explores the possibilities of a new form of audio representation one of the most usual MIR downstream tasks. Therefore, to discretely encoding music using deep vector quantization; a novel audio representation was created for the innovative generative music model i.e. Jukebox. The effectiveness of Jukebox's audio representation is compared to Mel spectrograms using a dataset that is almost equivalent to State-of-the-Art (SOTA) and an almost same transformer design. The results of this study imply that, at least when the transformers are pretrained using a very modest dataset of 20k tracks, Jukebox's audio representation is not superior to Mel spectrograms. This could be explained by the fact that Jukebox's audio representation does not sufficiently take
&lt;/p&gt;</description></item><item><title>DragNoise&#36890;&#36807;&#21033;&#29992;U-Net&#39044;&#27979;&#30340;&#22122;&#22768;&#36755;&#20986;&#20316;&#20026;&#35821;&#20041;&#32534;&#36753;&#22120;&#65292;&#22312;&#19981;&#37325;&#26032;&#36861;&#36394;&#28508;&#22312;&#22320;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#19988;&#21152;&#36895;&#30340;&#32534;&#36753;&#12290;</title><link>https://arxiv.org/abs/2404.01050</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#35821;&#20041;&#20256;&#25773;&#23454;&#29616;&#20132;&#20114;&#24335;&#22522;&#20110;&#28857;&#30340;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01050
&lt;/p&gt;
&lt;p&gt;
DragNoise&#36890;&#36807;&#21033;&#29992;U-Net&#39044;&#27979;&#30340;&#22122;&#22768;&#36755;&#20986;&#20316;&#20026;&#35821;&#20041;&#32534;&#36753;&#22120;&#65292;&#22312;&#19981;&#37325;&#26032;&#36861;&#36394;&#28508;&#22312;&#22320;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#19988;&#21152;&#36895;&#30340;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#24418;&#24335;&#30340;&#20132;&#20114;&#24335;&#32534;&#36753;&#20316;&#20026;&#19968;&#31181;&#37325;&#35201;&#24037;&#20855;&#65292;&#29992;&#20110;&#34917;&#20805;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DragNoise&#65292;&#36890;&#36807;&#21033;&#29992;&#27599;&#20010;U-Net&#30340;&#39044;&#27979;&#22122;&#22768;&#36755;&#20986;&#20316;&#20026;&#35821;&#20041;&#32534;&#36753;&#22120;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#19988;&#21152;&#36895;&#30340;&#32534;&#36753;&#65292;&#32780;&#26080;&#38656;&#37325;&#36861;&#36394;&#28508;&#22312;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01050v1 Announce Type: cross  Abstract: Point-based interactive editing serves as an essential tool to complement the controllability of existing generative models. A concurrent work, DragDiffusion, updates the diffusion latent map in response to user inputs, causing global latent map alterations. This results in imprecise preservation of the original content and unsuccessful editing due to gradient vanishing. In contrast, we present DragNoise, offering robust and accelerated editing without retracing the latent map. The core rationale of DragNoise lies in utilizing the predicted noise output of each U-Net as a semantic editor. This approach is grounded in two critical observations: firstly, the bottleneck features of U-Net inherently possess semantically rich features ideal for interactive editing; secondly, high-level semantics, established early in the denoising process, show minimal variation in subsequent stages. Leveraging these insights, DragNoise edits diffusion sema
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#30340;&#26032;&#22411;&#26143;&#31995;&#20998;&#31867;&#31639;&#27861;&#65292;&#21033;&#29992;SDSS&#25968;&#25454;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#26143;&#31995;&#20998;&#31867;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.01049</link><description>&lt;p&gt;
&#19968;&#31181;&#20248;&#21270;&#30340;&#26143;&#31995;&#20998;&#31867;&#30340;&#26032;&#22411;&#22522;&#20110;&#21306;&#22359;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Sector-Based Algorithm for an Optimized Star-Galaxy Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01049
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#30340;&#26032;&#22411;&#26143;&#31995;&#20998;&#31867;&#31639;&#27861;&#65292;&#21033;&#29992;SDSS&#25968;&#25454;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#26143;&#31995;&#20998;&#31867;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21306;&#22359;&#30340;&#26143;&#31995;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;Sloan&#25968;&#23383;&#22825;&#25991;&#25968;&#25454;&#24211;&#25968;&#25454;&#65288;SDSS-DR18&#65289;&#12290;&#36890;&#36807;&#23558;&#22825;&#31354;&#25112;&#30053;&#24615;&#22320;&#20998;&#21106;&#25104;&#19982;SDSS&#35266;&#27979;&#27169;&#24335;&#23545;&#40784;&#30340;&#21306;&#22359;&#65292;&#24182;&#21033;&#29992;&#19987;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26143;&#31995;&#20998;&#31867;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#23637;&#31034;&#20102;&#19968;&#26465;&#22312;&#23454;&#26102;&#35266;&#27979;&#35774;&#32622;&#20013;&#36827;&#34892;&#39640;&#25928;&#21644;&#31934;&#30830;&#22825;&#25991;&#20998;&#26512;&#30340;&#26377;&#21069;&#36884;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01049v1 Announce Type: cross  Abstract: This paper introduces a novel sector-based methodology for star-galaxy classification, leveraging the latest Sloan Digital Sky Survey data (SDSS-DR18). By strategically segmenting the sky into sectors aligned with SDSS observational patterns and employing a dedicated convolutional neural network (CNN), we achieve state-of-the-art performance for star galaxy classification. Our preliminary results demonstrate a promising pathway for efficient and precise astronomical analysis, especially in real-time observational settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#32423;&#32852;&#31995;&#32479;&#20013;&#36816;&#29992;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20943;&#23569;&#22312;&#26597;&#35810;&#36828;&#31243;&#27169;&#22411;&#26102;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#38544;&#31169;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.01041</link><description>&lt;p&gt;
LLM&#21487;&#20197;&#22312;&#19981;&#36879;&#38706;&#31169;&#20154;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20854;&#20182;LLM&#30340;&#24110;&#21161;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs get help from other LLMs without revealing private information?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#32423;&#32852;&#31995;&#32479;&#20013;&#36816;&#29992;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20943;&#23569;&#22312;&#26597;&#35810;&#36828;&#31243;&#27169;&#22411;&#26102;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#38544;&#31169;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32423;&#32852;&#26159;&#19968;&#31181;&#24120;&#35265;&#31867;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#20013;&#22914;&#26524;&#26412;&#22320;&#27169;&#22411;&#26080;&#27861;&#21333;&#29420;&#20934;&#30830;&#26631;&#35760;&#29992;&#25143;&#25968;&#25454;&#65292;&#21017;&#21487;&#20197;&#26597;&#35810;&#19968;&#20010;&#22823;&#22411;&#30340;&#36828;&#31243;&#27169;&#22411;&#12290;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#30001;&#20110;&#20854;&#22312;&#26174;&#33879;&#38477;&#20302;&#25512;&#26029;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#25345;&#20219;&#21153;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#26381;&#21153;&#22534;&#26632;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#32423;&#32852;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#22320;&#27169;&#22411;&#21487;&#20197;&#35775;&#38382;&#25935;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#32423;&#32852;&#31995;&#32479;&#26500;&#25104;&#29992;&#25143;&#30340;&#37325;&#22823;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#34987;&#36716;&#21457;&#21040;&#36828;&#31243;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#27492;&#31867;&#35774;&#32622;&#20013;&#24212;&#29992;&#32423;&#32852;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#65292;&#26041;&#27861;&#26159;&#20026;&#26412;&#22320;&#27169;&#22411;&#37197;&#22791;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#20174;&#32780;&#20943;&#23569;&#35775;&#38382;&#36828;&#31243;&#27169;&#22411;&#26102;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#37327;&#21270;&#27492;&#31867;&#35774;&#32622;&#20013;&#30340;&#20449;&#24687;&#27844;&#28431;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#38544;&#31169;&#24230;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26368;&#36817;&#24341;&#20837;&#30340;&#31038;&#20132;&#23398;&#20064;&#33539;&#24335;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01041v1 Announce Type: cross  Abstract: Cascades are a common type of machine learning systems in which a large, remote model can be queried if a local model is not able to accurately label a user's data by itself. Serving stacks for large language models (LLMs) increasingly use cascades due to their ability to preserve task performance while dramatically reducing inference costs. However, applying cascade systems in situations where the local model has access to sensitive data constitutes a significant privacy risk for users since such data could be forwarded to the remote model. In this work, we show the feasibility of applying cascade systems in such setups by equipping the local model with privacy-preserving techniques that reduce the risk of leaking private information when querying the remote model. To quantify information leakage in such setups, we introduce two privacy measures. We then propose a system that leverages the recently introduced social learning paradigm 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26159;&#31532;&#19968;&#20221;&#33268;&#21147;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#28145;&#20837;&#20171;&#32461;&#20102;HNN&#26550;&#26500;&#12289;&#35757;&#32451;&#31574;&#30053;&#21644;&#24212;&#29992;&#65292;&#20026;&#20174;&#20107;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2404.01039</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35843;&#26597;&#65306;&#28145;&#20837;&#21644;&#20998;&#27493;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step Guide
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26159;&#31532;&#19968;&#20221;&#33268;&#21147;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#28145;&#20837;&#20171;&#32461;&#20102;HNN&#26550;&#26500;&#12289;&#35757;&#32451;&#31574;&#30053;&#21644;&#24212;&#29992;&#65292;&#20026;&#20174;&#20107;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22797;&#26434;&#31995;&#32479;&#21644;&#24212;&#29992;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#22240;&#27492;&#65292;&#23545;&#20110;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#20026;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#19968;&#39033;&#37325;&#35201;&#35758;&#31243;&#12290;&#30001;&#20110;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#22312;&#25968;&#23398;&#19978;&#34987;&#34920;&#36798;&#20026;&#36229;&#22270;&#65292;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HNNs&#65289;&#24050;&#25104;&#20026;&#22312;&#36229;&#22270;&#19978;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#37492;&#20110;&#36825;&#19968;&#26032;&#36235;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20221;&#33268;&#21147;&#20110;HNNs&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#21547;&#28145;&#20837;&#21644;&#20998;&#27493;&#25351;&#21335;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26412;&#35843;&#26597;&#27010;&#36848;&#20102;HNN&#26550;&#26500;&#12289;&#35757;&#32451;&#31574;&#30053;&#21644;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;HNNs&#20998;&#35299;&#20026;&#22235;&#20010;&#35774;&#35745;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;i&#65289;&#36755;&#20837;&#29305;&#24449;&#65292;&#65288;ii&#65289;&#36755;&#20837;&#32467;&#26500;&#65292;&#65288;iii&#65289;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#21644;&#65288;iv&#65289;&#35757;&#32451;&#31574;&#30053;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#32771;&#23519;HNNs&#22914;&#20309;&#36890;&#36807;&#21508;&#33258;&#30340;&#32452;&#25104;&#37096;&#20998;&#22788;&#29702;&#21644;&#23398;&#20064;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;HNNs&#22312;&#25512;&#33616;&#12289;&#29983;&#29289;&#21644;&#21307;&#23398;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01039v1 Announce Type: new  Abstract: Higher-order interactions (HOIs) are ubiquitous in real-world complex systems and applications, and thus investigation of deep learning for HOIs has become a valuable agenda for the data mining and machine learning communities. As networks of HOIs are expressed mathematically as hypergraphs, hypergraph neural networks (HNNs) have emerged as a powerful tool for representation learning on hypergraphs. Given the emerging trend, we present the first survey dedicated to HNNs, with an in-depth and step-by-step guide. Broadly, the present survey overviews HNN architectures, training strategies, and applications. First, we break existing HNNs down into four design components: (i) input features, (ii) input structures, (iii) message-passing schemes, and (iv) training strategies. Second, we examine how HNNs address and learn HOIs with each of their components. Third, we overview the recent applications of HNNs in recommendation, biological and med
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#23545;&#39640;&#31561;&#25945;&#32946;&#35780;&#20272;&#23454;&#36341;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36825;&#20123;&#24037;&#20855;&#23637;&#31034;&#20102;&#23398;&#31185;&#30693;&#35782;&#12289;&#38382;&#39064;&#35299;&#20915;&#12289;&#20998;&#26512;&#33021;&#21147;&#31561;&#25216;&#33021;&#65292;&#20294;&#22312;&#19981;&#36947;&#24503;&#20351;&#29992;&#26102;&#21487;&#33021;&#38480;&#21046;&#23398;&#20064;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#37096;&#20998;&#23398;&#31185;&#35780;&#20272;&#20013;&#36825;&#20123;&#24037;&#20855;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01036</link><description>&lt;p&gt;
&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#26102;&#20195;&#30340;&#39640;&#31561;&#25945;&#32946;&#35780;&#20272;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Higher education assessment practice in the era of generative AI tools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#23545;&#39640;&#31561;&#25945;&#32946;&#35780;&#20272;&#23454;&#36341;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36825;&#20123;&#24037;&#20855;&#23637;&#31034;&#20102;&#23398;&#31185;&#30693;&#35782;&#12289;&#38382;&#39064;&#35299;&#20915;&#12289;&#20998;&#26512;&#33021;&#21147;&#31561;&#25216;&#33021;&#65292;&#20294;&#22312;&#19981;&#36947;&#24503;&#20351;&#29992;&#26102;&#21487;&#33021;&#38480;&#21046;&#23398;&#20064;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#37096;&#20998;&#23398;&#31185;&#35780;&#20272;&#20013;&#36825;&#20123;&#24037;&#20855;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#31561;&#25945;&#32946;&#65288;HE&#65289;&#37096;&#38376;&#23545;&#27599;&#20010;&#22269;&#23478;&#30340;&#32463;&#27982;&#21644;&#31038;&#20250;&#37117;&#26377;&#30410;&#22788;&#65292;&#20294;&#23427;&#20204;&#30340;&#36129;&#29486;&#21463;&#21040;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#31561;&#20808;&#36827;&#25216;&#26415;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;GenAI&#24037;&#20855;&#23545;&#35780;&#20272;&#21644;&#25945;&#23398;&#23454;&#36341;&#30340;&#24433;&#21709;&#65292;&#24182;&#38543;&#21518;&#35752;&#35770;&#20102;&#28508;&#22312;&#24433;&#21709;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#25968;&#25454;&#31185;&#23398;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#24314;&#31569;&#31649;&#29702;&#23398;&#31185;&#30340;&#19977;&#31181;&#35780;&#20272;&#24037;&#20855;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#32467;&#26524;&#26174;&#31034;GenAI&#24037;&#20855;&#23637;&#31034;&#20102;&#23398;&#31185;&#30693;&#35782;&#12289;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12289;&#20998;&#26512;&#33021;&#21147;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#19981;&#36947;&#24503;&#20351;&#29992;&#26102;&#21487;&#33021;&#20250;&#38480;&#21046;&#23398;&#20064;&#12290;&#20854;&#27425;&#65292;&#26576;&#20123;&#23398;&#31185;&#35780;&#20272;&#30340;&#35774;&#35745;&#25581;&#31034;&#20102;GenAI&#24037;&#20855;&#30340;&#23616;&#38480;&#24615;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#22914;&#20309;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36827;&#34892;&#25945;&#23398;&#19982;&#23398;&#20064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01036v1 Announce Type: cross  Abstract: The higher education (HE) sector benefits every nation's economy and society at large. However, their contributions are challenged by advanced technologies like generative artificial intelligence (GenAI) tools. In this paper, we provide a comprehensive assessment of GenAI tools towards assessment and pedagogic practice and, subsequently, discuss the potential impacts. This study experimented using three assessment instruments from data science, data analytics, and construction management disciplines. Our findings are two-fold: first, the findings revealed that GenAI tools exhibit subject knowledge, problem-solving, analytical, critical thinking, and presentation skills and thus can limit learning when used unethically. Secondly, the design of the assessment of certain disciplines revealed the limitations of the GenAI tools. Based on our findings, we made recommendations on how AI tools can be utilised for teaching and learning in HE.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01012</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#26597;&#35810;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Query Performance Prediction using Relevance Judgments Generated by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65288;QPP&#65289;&#26088;&#22312;&#20272;&#35745;&#25628;&#32034;&#31995;&#32479;&#23545;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#30456;&#20851;&#24615;&#21028;&#26029;&#12290;&#20808;&#21069;&#30340;QPP&#26041;&#27861;&#36890;&#24120;&#36820;&#22238;&#21333;&#20010;&#26631;&#37327;&#20540;&#65292;&#24182;&#19981;&#35201;&#27714;&#39044;&#27979;&#20540;&#25509;&#36817;&#29305;&#23450;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#35780;&#20272;&#25351;&#26631;&#65292;&#20174;&#32780;&#23548;&#33268;&#20197;&#19979;&#26576;&#20123;&#32570;&#28857;&#65306;&#65288;i&#65289;&#21333;&#20010;&#26631;&#37327;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#19981;&#21516;&#30340;IR&#35780;&#20272;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;&#24403;&#24230;&#37327;&#19981;&#39640;&#24230;&#30456;&#20851;&#26102;&#65292;&#65288;ii&#65289;&#21333;&#20010;&#26631;&#37327;&#38480;&#21046;&#20102;QPP&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#20165;&#20351;&#29992;&#26631;&#37327;&#26080;&#27861;&#35299;&#37322;QPP&#32467;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;QPP&#26694;&#26550;&#65288;QPP-GenRE&#65289;&#65292;&#23558;QPP&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#20219;&#21153;&#65292;&#21363;&#23545;&#25490;&#21517;&#21015;&#34920;&#20013;&#27599;&#20010;&#39033;&#30446;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#21028;&#26029;&#12290;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#20219;&#20309;IR&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01012v1 Announce Type: cross  Abstract: Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgment
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;C-Flat&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24179;&#22374;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#21487;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#31616;&#21270;&#20102;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.00986</link><description>&lt;p&gt;
&#36890;&#36807;C-Flat&#20351;&#25345;&#32493;&#23398;&#20064;&#26356;&#24378;&#22823;
&lt;/p&gt;
&lt;p&gt;
Make Continual Learning Stronger via C-Flat
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00986
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;C-Flat&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24179;&#22374;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#21487;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#31616;&#21270;&#20102;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#20013;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#22788;&#29702;&#36830;&#32493;&#21040;&#36798;&#20219;&#21153;&#30340;&#21160;&#24577;&#26356;&#26032;&#30693;&#35782;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20026;&#20102;&#35299;&#20915;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#25935;&#24863;&#24615;-&#31283;&#23450;&#24615;&#22256;&#22659;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26435;&#37325;&#25439;&#22833;&#26223;&#35266;&#30340;&#38497;&#23789;&#24230;&#65292;&#23547;&#25214;&#20301;&#20110;&#20855;&#26377;&#32479;&#19968;&#20302;&#25439;&#22833;&#25110;&#24179;&#31283;&#26799;&#24230;&#30340;&#37051;&#22495;&#20013;&#30340;&#24179;&#22374;&#26368;&#23567;&#20540;&#65292;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#35757;&#32451;&#26041;&#24335;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#20248;&#21270;&#22120;&#22914;SGD&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#20316;&#21697;&#35752;&#35770;&#20102;&#36825;&#31181;&#35757;&#32451;&#26041;&#24335;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#29305;&#23450;&#35774;&#35745;&#30340;&#38646;&#38454;&#38497;&#23789;&#24230;&#20248;&#21270;&#22120;&#21487;&#20197;&#25552;&#21319;&#25345;&#32493;&#23398;&#20064;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Continual Flatness&#65288;C-Flat&#65289;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20026;&#25345;&#32493;&#23398;&#20064;&#23450;&#21046;&#30340;&#26356;&#24179;&#22374;&#30340;&#25439;&#22833;&#26223;&#35266;&#12290;C-Flat&#21482;&#38656;&#19968;&#34892;&#20195;&#30721;&#21363;&#21487;&#36731;&#26494;&#35843;&#29992;&#65292;&#24182;&#21487;&#19982;&#20219;&#20309;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#25554;&#25773;&#12290;C-Flat&#24212;&#29992;&#20110;&#25152;&#26377;&#25345;&#32493;&#23398;&#20064;&#31867;&#21035;&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#24182;&#19982;&#25439;&#22833;&#26368;&#23567;&#21270;&#20248;&#21270;&#22120;&#36827;&#34892;&#20102;&#24443;&#24213;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00986v1 Announce Type: new  Abstract: Model generalization ability upon incrementally acquiring dynamically updating knowledge from sequentially arriving tasks is crucial to tackle the sensitivity-stability dilemma in Continual Learning (CL). Weight loss landscape sharpness minimization seeking for flat minima lying in neighborhoods with uniform low loss or smooth gradient is proven to be a strong training regime improving model generalization compared with loss minimization based optimizer like SGD. Yet only a few works have discussed this training regime for CL, proving that dedicated designed zeroth-order sharpness optimizer can improve CL performance. In this work, we propose a Continual Flatness (C-Flat) method featuring a flatter loss landscape tailored for CL. C-Flat could be easily called with only one line of code and is plug-and-play to any CL methods. A general framework of C-Flat applied to all CL categories and a thorough comparison with loss minima optimizer an
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#26234;&#24935;&#22478;&#24066;&#21457;&#23637;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20869;&#23481;&#28085;&#30422;&#20102;&#26041;&#27861;&#35770;&#20998;&#31867;&#12289;&#22810;&#31181;&#24212;&#29992;&#39046;&#22495;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2404.00983</link><description>&lt;p&gt;
&#26234;&#24935;&#22478;&#24066;&#30340;&#25345;&#32493;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Continual Learning for Smart City: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00983
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#26234;&#24935;&#22478;&#24066;&#21457;&#23637;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20869;&#23481;&#28085;&#30422;&#20102;&#26041;&#27861;&#35770;&#20998;&#31867;&#12289;&#22810;&#31181;&#24212;&#29992;&#39046;&#22495;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#22478;&#24066;&#25968;&#23383;&#21270;&#65292;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#21644;&#24378;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#20419;&#20351;&#26234;&#24935;&#22478;&#24066;&#20013;&#37096;&#32626;&#30340;&#26234;&#33021;&#27169;&#22411;&#36805;&#36895;&#26356;&#26032;&#12290;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#19981;&#26029;&#26356;&#26032;&#27169;&#22411;&#20197;&#36866;&#24212;&#21464;&#21270;&#30340;&#29615;&#22659;&#65292;&#20854;&#20013;&#23398;&#20064;&#20219;&#21153;&#12289;&#25968;&#25454;&#21644;&#20998;&#24067;&#21487;&#20197;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#20840;&#38754;&#23457;&#26597;&#20102;&#22312;&#26234;&#24935;&#22478;&#24066;&#21457;&#23637;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#20869;&#23481;&#20998;&#20026;&#19977;&#20010;&#37096;&#20998;&#65306;1&#65289;&#26041;&#27861;&#35770;&#12290;&#25105;&#20204;&#23558;&#22823;&#37327;&#22522;&#26412;&#30340;CL&#26041;&#27861;&#21644;&#32467;&#21512;&#20854;&#20182;&#23398;&#20064;&#33539;&#20363;&#30340;&#39640;&#32423;CL&#26694;&#26550;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#22270;&#23398;&#20064;&#12289;&#26102;&#31354;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#12290;2&#65289;&#24212;&#29992;&#26041;&#38754;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#28085;&#30422;&#20132;&#36890;&#12289;&#29615;&#22659;&#12289;&#20844;&#20849;&#21355;&#29983;&#12289;&#23433;&#20840;&#12289;&#32593;&#32476;&#20197;&#21450;&#19982;&#22478;&#24066;&#35745;&#31639;&#30456;&#20851;&#30340;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#20247;&#22810;CL&#24212;&#29992;&#12290;3&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00983v1 Announce Type: cross  Abstract: With the digitization of modern cities, large data volumes and powerful computational resources facilitate the rapid update of intelligent models deployed in smart cities. Continual learning (CL) is a novel machine learning paradigm that constantly updates models to adapt to changing environments, where the learning tasks, data, and distributions can vary over time. Our survey provides a comprehensive review of continual learning methods that are widely used in smart city development. The content consists of three parts: 1) Methodology-wise. We categorize a large number of basic CL methods and advanced CL frameworks in combination with other learning paradigms including graph learning, spatial-temporal learning, multi-modal learning, and federated learning. 2) Application-wise. We present numerous CL applications covering transportation, environment, public health, safety, networks, and associated datasets related to urban computing. 3
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861; GADM&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#25910;&#38598;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23558;&#29983;&#25104;&#27169;&#22411;&#36801;&#31227;&#21040;&#26032;&#39046;&#22495;&#65292;&#36890;&#36807;&#21033;&#29992;&#31561;&#21464;&#33945;&#26495;&#33258;&#32534;&#30721;&#22120;&#21644;&#21508;&#31181;&#25513;&#34109;&#31574;&#30053;&#26469;&#25429;&#33719;&#32467;&#26500;&#21464;&#21270;&#25152;&#24102;&#26469;&#30340;&#39046;&#22495;&#24046;&#24322;&#65292;&#24182;&#33021;&#22815;&#27867;&#21270;&#21040;&#30446;&#26631;&#39046;&#22495;&#20013;&#30475;&#19981;&#35265;&#30340;&#32467;&#26500;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2404.00962</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#29983;&#25104;3D&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Driven Domain Adaptation for Generating 3D Molecules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00962
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861; GADM&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#25910;&#38598;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23558;&#29983;&#25104;&#27169;&#22411;&#36801;&#31227;&#21040;&#26032;&#39046;&#22495;&#65292;&#36890;&#36807;&#21033;&#29992;&#31561;&#21464;&#33945;&#26495;&#33258;&#32534;&#30721;&#22120;&#21644;&#21508;&#31181;&#25513;&#34109;&#31574;&#30053;&#26469;&#25429;&#33719;&#32467;&#26500;&#21464;&#21270;&#25152;&#24102;&#26469;&#30340;&#39046;&#22495;&#24046;&#24322;&#65292;&#24182;&#33021;&#22815;&#27867;&#21270;&#21040;&#30446;&#26631;&#39046;&#22495;&#20013;&#30475;&#19981;&#35265;&#30340;&#32467;&#26500;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#35757;&#32451;&#19968;&#20010;&#20998;&#23376;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#26469;&#33258;&#26032;&#39046;&#22495;&#30340;3D&#20998;&#23376;&#65292;&#20174;&#32780;&#36991;&#20813;&#25910;&#38598;&#25968;&#25454;&#30340;&#38656;&#27714;&#65311;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#34987;&#35270;&#20026;&#39046;&#22495;&#33258;&#36866;&#24212;&#20998;&#23376;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#22522;&#20110;&#21407;&#21017;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;GADM&#65292;&#23427;&#20801;&#35768;&#23558;&#29983;&#25104;&#27169;&#22411;&#31227;&#33267;&#25152;&#38656;&#30340;&#26032;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#25910;&#38598;&#20219;&#20309;&#19968;&#20010;&#20998;&#23376;&#12290;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#36890;&#24120;&#30001;&#20998;&#23376;&#30340;&#32467;&#26500;&#21464;&#21270;&#24341;&#36215;&#65292;&#20363;&#22914;&#39592;&#26550;&#21464;&#21270;&#65292;&#25105;&#20204;&#21033;&#29992;&#25351;&#23450;&#30340;&#31561;&#21464;&#33945;&#26495;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#20197;&#21450;&#21508;&#31181;&#25513;&#34109;&#31574;&#30053;&#26469;&#25429;&#33719;&#39046;&#22495;&#20869;&#21464;&#20307;&#30340;&#32467;&#26500;&#31934;&#32454;&#34920;&#31034;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#19968;&#20010;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22359;&#65292;MAE&#21487;&#20197;&#27867;&#21270;&#21040;&#30446;&#26631;&#39046;&#22495;&#20013;&#30475;&#19981;&#35265;&#30340;&#32467;&#26500;&#21464;&#21270;&#12290;&#36825;&#20123;&#32467;&#26500;&#21464;&#21270;&#34987;&#32534;&#30721;&#20026;&#31561;&#21464;&#32534;&#30721;&#22120;&#65292;&#24182;&#34987;&#35270;&#20026;&#39046;&#22495;&#30417;&#30563;&#21592;&#26469;&#25511;&#21046;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00962v1 Announce Type: new  Abstract: Can we train a molecule generator that can generate 3D molecules from a new domain, circumventing the need to collect data? This problem can be cast as the problem of domain adaptive molecule generation. This work presents a novel and principled diffusion-based approach, called GADM, that allows shifting a generative model to desired new domains without the need to collect even a single molecule. As the domain shift is typically caused by the structure variations of molecules, e.g., scaffold variations, we leverage a designated equivariant masked autoencoder (MAE) along with various masking strategies to capture the structural-grained representations of the in-domain varieties. In particular, with an asymmetric encoder-decoder module, the MAE can generalize to unseen structure variations from the target domains. These structure variations are encoded with an equivariant encoder and treated as domain supervisors to control denoising. We s
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#21019;&#24314;&#35780;&#20272;&#27169;&#22411;&#20197;&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#24615;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#38477;&#20302;&#35780;&#20272;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.00942</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00942
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#21019;&#24314;&#35780;&#20272;&#27169;&#22411;&#20197;&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#24615;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#38477;&#20302;&#35780;&#20272;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#21147;&#12290;&#20107;&#23454;&#24615;&#38382;&#39064;&#23545;LLMs&#26469;&#35828;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#29983;&#25104;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GraphEval&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#27979;&#35797;&#25968;&#25454;&#38598;&#23545;LLM&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27979;&#35797;&#25968;&#25454;&#38598;&#26159;&#20174;&#25317;&#26377;&#36229;&#36807;1000&#19975;&#20010;&#20107;&#23454;&#30340;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#20013;&#26816;&#32034;&#32780;&#26469;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#20154;&#21147;&#25104;&#26412;&#12290;&#19982;&#22522;&#20110;&#29983;&#25104;&#21709;&#24212;&#35780;&#20272;LLMs&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;GraphEval&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#35780;&#20272;&#27169;&#22411;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;&#65292;&#29992;&#20110;&#20272;&#35745;LLM&#32473;&#20986;&#30340;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35780;&#20272;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#19982;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#23494;&#20999;&#30456;&#20851;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#35780;&#20272;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;LLM&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00942v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has significantly transformed the AI landscape, enhancing machine learning and AI capabilities. Factuality issue is a critical concern for LLMs, as they may generate factually incorrect responses. In this paper, we propose GraphEval to evaluate an LLM's performance using a substantially large test dataset. Specifically, the test dataset is retrieved from a large knowledge graph with more than 10 million facts without expensive human efforts. Unlike conventional methods that evaluate LLMs based on generated responses, GraphEval streamlines the evaluation process by creating a judge model to estimate the correctness of the answers given by the LLM. Our experiments demonstrate that the judge model's factuality assessment aligns closely with the correctness of the LLM's generated outputs, while also substantially reducing evaluation costs. Besides, our findings offer valuable insights into LLM per
&lt;/p&gt;</description></item><item><title>IGQ-ViT&#26159;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;Transformer&#30340;&#23454;&#20363;&#24863;&#30693;&#32452;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#23558;&#28608;&#27963;&#26144;&#23556;&#30340;&#36890;&#36947;&#20998;&#25104;&#22810;&#20010;&#32452;&#65292;&#20351;&#24471;&#27599;&#20010;&#36755;&#20837;&#23454;&#20363;&#20869;&#30340;&#28608;&#27963;&#20855;&#26377;&#30456;&#20284;&#32479;&#35745;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00928</link><description>&lt;p&gt;
&#35270;&#35273;Transformer&#30340;&#23454;&#20363;&#24863;&#30693;&#32452;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Instance-Aware Group Quantization for Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00928
&lt;/p&gt;
&lt;p&gt;
IGQ-ViT&#26159;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;Transformer&#30340;&#23454;&#20363;&#24863;&#30693;&#32452;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#23558;&#28608;&#27963;&#26144;&#23556;&#30340;&#36890;&#36947;&#20998;&#25104;&#22810;&#20010;&#32452;&#65292;&#20351;&#24471;&#27599;&#20010;&#36755;&#20837;&#23454;&#20363;&#20869;&#30340;&#28608;&#27963;&#20855;&#26377;&#30456;&#20284;&#32479;&#35745;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#20165;&#26377;&#23569;&#37327;&#26410;&#26631;&#35760;&#26679;&#26412;&#30340;&#26657;&#20934;&#38598;&#23545;&#39044;&#35757;&#32451;&#30340;&#20840;&#31934;&#24230;&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;PTQ&#26041;&#27861;&#25552;&#20379;&#20102;&#19982;&#20840;&#31934;&#24230;&#23545;&#24212;&#29289;&#21487;&#27604;&#30340;&#37327;&#21270;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#65292;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;CNNs&#21644;ViTs&#20043;&#38388;&#30340;&#26550;&#26500;&#24046;&#24322;&#12290;&#29305;&#21035;&#26159;&#65292;&#27599;&#20010;&#36890;&#36947;&#30340;&#28608;&#27963;&#20998;&#24067;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#22823;&#22823;&#21464;&#21270;&#65292;&#20351;&#24471;CNNs&#30340;PTQ&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;ViTs&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;ViTs&#30340;&#23454;&#20363;&#24863;&#30693;&#32452;&#37327;&#21270;&#65288;IGQ-ViT&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#28608;&#27963;&#26144;&#23556;&#30340;&#36890;&#36947;&#21160;&#24577;&#22320;&#20998;&#25104;&#22810;&#20010;&#32452;&#65292;&#20197;&#20415;&#20026;&#27599;&#20010;&#36755;&#20837;&#23454;&#20363;&#65292;&#20351;&#24471;&#27599;&#32452;&#20869;&#30340;&#28608;&#27963;&#20855;&#26377;&#30456;&#20284;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00928v1 Announce Type: cross  Abstract: Post-training quantization (PTQ) is an efficient model compression technique that quantizes a pretrained full-precision model using only a small calibration set of unlabeled samples without retraining. PTQ methods for convolutional neural networks (CNNs) provide quantization results comparable to full-precision counterparts. Directly applying them to vision transformers (ViTs), however, incurs severe performance degradation, mainly due to the differences in architectures between CNNs and ViTs. In particular, the distribution of activations for each channel vary drastically according to input instances, making PTQ methods for CNNs inappropriate for ViTs. To address this, we introduce instance-aware group quantization for ViTs (IGQ-ViT). To this end, we propose to split the channels of activation maps into multiple groups dynamically for each input instance, such that activations within each group share similar statistical properties. We
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Token-Efficient Leverage Learning&#65288;TELL&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#20854;&#38477;&#20302;&#20219;&#21153;&#25968;&#25454;&#38656;&#27714;&#12289;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20026;&#20302;&#36164;&#28304;&#20219;&#21153;&#24102;&#26469;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00914</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#20196;&#29260;&#21033;&#29992;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Token-Efficient Leverage Learning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00914
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Token-Efficient Leverage Learning&#65288;TELL&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23637;&#31034;&#20102;&#20854;&#38477;&#20302;&#20219;&#21153;&#25968;&#25454;&#38656;&#27714;&#12289;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20026;&#20302;&#36164;&#28304;&#20219;&#21153;&#24102;&#26469;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#39640;&#36164;&#28304;&#22330;&#26223;&#20013;&#34920;&#29616;&#26356;&#20339;&#65292;&#36825;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;&#25968;&#25454;&#31232;&#32570;&#21644;LLMs&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#22266;&#26377;&#30340;&#22256;&#38590;&#21152;&#21095;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#22823;&#38590;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;\textbf{Leverage Learning}&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#31616;&#21270;&#23454;&#29616;&#65292;&#31216;&#20026;Token-Efficient Leverage Learning (TELL)&#12290;TELL&#23637;&#31034;&#20102;Leverage Learning&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#23427;&#22312;&#21508;&#31181;LLMs&#21644;&#20302;&#36164;&#28304;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;$10^4$&#21040;$10^6$&#20010;&#20196;&#29260;&#19981;&#31561;&#12290;&#19982;&#20256;&#32479;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#30456;&#27604;&#65292;&#23427;&#23558;&#20219;&#21153;&#25968;&#25454;&#38656;&#27714;&#38477;&#20302;&#20102;&#36817;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#21516;&#26102;&#25552;&#20379;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#22312;&#30456;&#21516;&#37327;&#30340;&#20219;&#21153;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;TELL&#22312;&#25913;&#21892;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#39046;&#20808;&#20110;SFT&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;Leverage Learning&#30340;&#26426;&#21046;&#65292;&#26263;&#31034;&#20854;&#31526;&#21512;&#37327;&#21270;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00914v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have excelled in various tasks but perform better in high-resource scenarios, which presents challenges in low-resource scenarios. Data scarcity and the inherent difficulty of adapting LLMs to specific tasks compound the challenge. To address the twin hurdles, we introduce \textbf{Leverage Learning}. We present a streamlined implement of this methodology called Token-Efficient Leverage Learning (TELL). TELL showcases the potential of Leverage Learning, demonstrating effectiveness across various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$ tokens. It reduces task data requirements by up to nearly an order of magnitude compared to conventional Supervised Fine-Tuning (SFT) while delivering competitive performance. With the same amount of task data, TELL leads in improving task performance compared to SFT. We discuss the mechanism of Leverage Learning, suggesting it aligns with quantization hypoth
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31867;&#21035;&#20381;&#36182;&#24615;&#33258;&#21160;&#33258;&#36866;&#24212;&#31574;&#30053;&#65288;CAAP&#65289;&#30340;&#26032;&#39062;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#20013;&#23384;&#22312;&#30340;&#31867;&#21035;&#20381;&#36182;&#24615;&#20559;&#24046;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#24515;&#30005;&#22270;&#31561;&#37325;&#35201;&#20449;&#21495;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.00898</link><description>&lt;p&gt;
CAAP&#65306;&#22522;&#20110;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#31867;&#21035;&#20381;&#36182;&#24615;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
CAAP: Class-Dependent Automatic Data Augmentation Based On Adaptive Policies For Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00898
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31867;&#21035;&#20381;&#36182;&#24615;&#33258;&#21160;&#33258;&#36866;&#24212;&#31574;&#30053;&#65288;CAAP&#65289;&#30340;&#26032;&#39062;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#20013;&#23384;&#22312;&#30340;&#31867;&#21035;&#20381;&#36182;&#24615;&#20559;&#24046;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#24515;&#30005;&#22270;&#31561;&#37325;&#35201;&#20449;&#21495;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#31181;&#24120;&#35265;&#25216;&#26415;&#65292;&#36890;&#36807;&#25193;&#23637;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#25552;&#21319;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#65288;ADA&#65289;&#26041;&#27861;&#22240;&#33021;&#22815;&#29983;&#25104;&#36866;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#31574;&#30053;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;ADA&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#25972;&#20307;&#24615;&#33021;&#25913;&#36827;&#65292;&#24573;&#35270;&#20102;&#23548;&#33268;&#29305;&#23450;&#31867;&#21035;&#24615;&#33021;&#19979;&#38477;&#30340;&#31867;&#21035;&#20381;&#36182;&#24615;&#20559;&#24046;&#38382;&#39064;&#12290;&#36825;&#31181;&#20559;&#24046;&#22312;&#23558;&#27169;&#22411;&#37096;&#32626;&#21040;&#23454;&#38469;&#24212;&#29992;&#20013;&#26102;&#24102;&#26469;&#37325;&#22823;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;ADA&#20173;&#26159;&#19968;&#20010;&#26410;&#28145;&#20837;&#25506;&#35752;&#30340;&#39046;&#22495;&#65292;&#31361;&#26174;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#38656;&#27714;&#12290;&#29305;&#21035;&#26159;&#23558;ADA&#25216;&#26415;&#24212;&#29992;&#20110;&#24515;&#30005;&#22270;&#31561;&#37325;&#35201;&#20449;&#21495;&#26159;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#20363;&#23376;&#65292;&#30001;&#20110;&#20854;&#22312;&#24515;&#33039;&#30149;&#35786;&#26029;&#31561;&#21307;&#30103;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00898v1 Announce Type: new  Abstract: Data Augmentation is a common technique used to enhance the performance of deep learning models by expanding the training dataset. Automatic Data Augmentation (ADA) methods are getting popular because of their capacity to generate policies for various datasets. However, existing ADA methods primarily focused on overall performance improvement, neglecting the problem of class-dependent bias that leads to performance reduction in specific classes. This bias poses significant challenges when deploying models in real-world applications. Furthermore, ADA for time series remains an underexplored domain, highlighting the need for advancements in this field. In particular, applying ADA techniques to vital signals like an electrocardiogram (ECG) is a compelling example due to its potential in medical domains such as heart disease diagnostics.   We propose a novel deep learning-based approach called Class-dependent Automatic Adaptive Policies (CAA
&lt;/p&gt;</description></item><item><title>&#35813;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#31283;&#20581;&#24615;&#30340;&#37325;&#35201;&#27010;&#24565;&#21450;&#20851;&#38190;&#30340;&#25216;&#26415;&#21644;&#22240;&#32032;&#65292;&#20197;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00897</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#40065;&#26834;&#24615;&#65306;&#20837;&#38376;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Robustness: A Primer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00897
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#31283;&#20581;&#24615;&#30340;&#37325;&#35201;&#27010;&#24565;&#21450;&#20851;&#38190;&#30340;&#25216;&#26415;&#21644;&#22240;&#32032;&#65292;&#20197;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#31283;&#20581;&#24615;&#30340;&#22522;&#26412;&#27010;&#24565;&#21450;&#20854;&#22312;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#35752;&#35770;&#22987;&#20110;&#31283;&#20581;&#24615;&#30340;&#35814;&#32454;&#23450;&#20041;&#65292;&#23558;&#20854;&#25551;&#36848;&#20026;ML&#27169;&#22411;&#22312;&#21508;&#31181;&#19981;&#21516;&#21644;&#24847;&#22806;&#30340;&#29615;&#22659;&#26465;&#20214;&#19979;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;ML&#40065;&#26834;&#24615;&#36890;&#36807;&#22810;&#20010;&#35270;&#35282;&#36827;&#34892;&#20102;&#21078;&#26512;&#65306;&#20854;&#19982;&#27867;&#21270;&#33021;&#21147;&#30340;&#20114;&#34917;&#24615;&#65307;&#20854;&#20316;&#20026;&#21487;&#20449;AI&#30340;&#35201;&#27714;&#65307;&#20854;&#23545;&#25239;&#24615;&#19982;&#38750;&#23545;&#25239;&#24615;&#26041;&#38754;&#65307;&#20854;&#25968;&#37327;&#21270;&#25351;&#26631;&#65307;&#20197;&#21450;&#20854;&#21487;&#22797;&#29616;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#25351;&#26631;&#12290;&#31456;&#33410;&#28145;&#20837;&#25506;&#35752;&#20102;&#24433;&#21709;&#40065;&#26834;&#24615;&#30340;&#22240;&#32032;&#65292;&#22914;&#25968;&#25454;&#20559;&#24046;&#12289;&#27169;&#22411;&#22797;&#26434;&#24615;&#20197;&#21450;ML&#27969;&#31243;&#19981;&#26126;&#30830;&#30340;&#39118;&#38505;&#12290;&#23427;&#20174;&#24191;&#27867;&#30340;&#35270;&#35282;&#35843;&#26597;&#20102;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#21253;&#25324;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21253;&#25324;&#25968;&#23383;&#21644;&#29289;&#29702;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00897v1 Announce Type: cross  Abstract: This chapter explores the foundational concept of robustness in Machine Learning (ML) and its integral role in establishing trustworthiness in Artificial Intelligence (AI) systems. The discussion begins with a detailed definition of robustness, portraying it as the ability of ML models to maintain stable performance across varied and unexpected environmental conditions. ML robustness is dissected through several lenses: its complementarity with generalizability; its status as a requirement for trustworthy AI; its adversarial vs non-adversarial aspects; its quantitative metrics; and its indicators such as reproducibility and explainability. The chapter delves into the factors that impede robustness, such as data bias, model complexity, and the pitfalls of underspecified ML pipelines. It surveys key techniques for robustness assessment from a broad perspective, including adversarial attacks, encompassing both digital and physical realms.
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#24341;&#20837;&#21453;&#39304;&#26426;&#21046;&#65292;&#23558;&#19968;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#20316;&#20026;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#38544;&#34255;&#29305;&#24449;&#65292;&#20351;&#38745;&#24577;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#36716;&#21464;&#20026;&#21160;&#24577;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.00885</link><description>&lt;p&gt;
&#29992;&#21453;&#39304;&#26426;&#21046;&#24314;&#27169;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#36755;&#20986;&#32423;&#20219;&#21153;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Modeling Output-Level Task Relatedness in Multi-Task Learning with Feedback Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00885
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#24341;&#20837;&#21453;&#39304;&#26426;&#21046;&#65292;&#23558;&#19968;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#20316;&#20026;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#38544;&#34255;&#29305;&#24449;&#65292;&#20351;&#38745;&#24577;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#36716;&#21464;&#20026;&#21160;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#19981;&#21516;&#23618;&#27425;&#20849;&#20139;&#20449;&#24687;&#26469;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#30340;&#33539;&#24335;&#65292;&#22686;&#24378;&#27599;&#20010;&#21333;&#29420;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;&#36755;&#20986;&#32423;&#20219;&#21153;&#30456;&#20851;&#24615;&#65292;&#36890;&#36807;&#23558;&#21518;&#39564;&#20449;&#24687;&#24341;&#20837;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#19981;&#21516;&#20219;&#21153;&#21487;&#33021;&#20135;&#29983;&#30456;&#20851;&#30340;&#30456;&#20114;&#24433;&#21709;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00885v1 Announce Type: new  Abstract: Multi-task learning (MTL) is a paradigm that simultaneously learns multiple tasks by sharing information at different levels, enhancing the performance of each individual task. While previous research has primarily focused on feature-level or parameter-level task relatedness, and proposed various model architectures and learning algorithms to improve learning performance, we aim to explore output-level task relatedness. This approach introduces a posteriori information into the model, considering that different tasks may produce correlated outputs with mutual influences. We achieve this by incorporating a feedback mechanism into MTL models, where the output of one task serves as a hidden feature for another task, thereby transforming a static MTL model into a dynamic one. To ensure the training process converges, we introduce a convergence loss that measures the trend of a task's outputs during each iteration. Additionally, we propose a 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#38750;&#36127;&#24352;&#37327;&#22240;&#23376;&#20998;&#35299;&#35299;&#20915;&#20102;&#22522;&#20110;&#38170;&#22270;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#32570;&#20047;&#32858;&#31867;&#21487;&#35299;&#37322;&#24615;&#21644;&#24573;&#35270;&#35270;&#22270;&#38388;&#20449;&#24687;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2404.00883</link><description>&lt;p&gt;
&#22522;&#20110;&#38170;&#22270;&#24352;&#37327;&#20998;&#35299;&#30340;&#21487;&#35299;&#37322;&#22810;&#35270;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Interpretable Multi-View Clustering Based on Anchor Graph Tensor Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00883
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38750;&#36127;&#24352;&#37327;&#22240;&#23376;&#20998;&#35299;&#35299;&#20915;&#20102;&#22522;&#20110;&#38170;&#22270;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#32570;&#20047;&#32858;&#31867;&#21487;&#35299;&#37322;&#24615;&#21644;&#24573;&#35270;&#35270;&#22270;&#38388;&#20449;&#24687;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38170;&#22270;&#30340;&#32858;&#31867;&#26041;&#27861;&#22240;&#20854;&#20986;&#33394;&#30340;&#32858;&#31867;&#24615;&#33021;&#21644;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23398;&#20064;&#20855;&#26377;K&#20010;&#36830;&#25509;&#32452;&#20214;&#30340;&#20108;&#37096;&#22270;&#65292;&#26377;&#21161;&#20110;&#36991;&#20813;&#21518;&#22788;&#29702;&#30340;&#38656;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;&#38170;&#22270;&#22240;&#23376;&#21270;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#32570;&#20047;&#23545;&#20998;&#35299;&#30697;&#38453;&#30340;&#20805;&#20998;&#32858;&#31867;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#32463;&#24120;&#24573;&#35270;&#35270;&#22270;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#38750;&#36127;&#24352;&#37327;&#22240;&#23376;&#20998;&#35299;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#20197;&#20998;&#35299;&#32467;&#21512;&#20102;&#22810;&#35270;&#22270;&#38170;&#22270;&#30340;&#38170;&#22270;&#24352;&#37327;&#12290;&#36825;&#19968;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#33719;&#24471;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00883v1 Announce Type: new  Abstract: The clustering method based on the anchor graph has gained significant attention due to its exceptional clustering performance and ability to process large-scale data. One common approach is to learn bipartite graphs with K-connected components, helping avoid the need for post-processing. However, this method has strict parameter requirements and may not always get K-connected components. To address this issue, an alternative approach is to directly obtain the cluster label matrix by performing non-negative matrix factorization (NMF) on the anchor graph. Nevertheless, existing multi-view clustering methods based on anchor graph factorization lack adequate cluster interpretability for the decomposed matrix and often overlook the inter-view information. We address this limitation by using non-negative tensor factorization to decompose an anchor graph tensor that combines anchor graphs from multiple views. This approach allows us to conside
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20248;&#21270;&#31639;&#23376;&#20998;&#35010;&#31639;&#27861;&#30340;&#24230;&#37327;&#31354;&#38388;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20854;&#25910;&#25947;&#36895;&#24230;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00882</link><description>&lt;p&gt;
&#24230;&#37327;&#23398;&#20064;&#20197;&#21152;&#36895;&#21487;&#24494;&#21442;&#25968;&#35268;&#21010;&#30340;&#31639;&#23376;&#20998;&#35010;&#26041;&#27861;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Metric Learning to Accelerate Convergence of Operator Splitting Methods for Differentiable Parametric Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00882
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20248;&#21270;&#31639;&#23376;&#20998;&#35010;&#31639;&#27861;&#30340;&#24230;&#37327;&#31354;&#38388;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20854;&#25910;&#25947;&#36895;&#24230;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#29992;&#20110;&#21152;&#36895;&#35299;&#20915;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#22810;&#31181;&#26041;&#27861;&#12290;&#22312;&#35832;&#22914;&#20154;&#24037;&#26234;&#33021;&#21644;&#26368;&#20248;&#25511;&#21046;&#31561;&#24212;&#29992;&#20013;&#23545;&#23454;&#26102;&#20915;&#31574;&#33021;&#21147;&#30340;&#19981;&#26029;&#22686;&#24378;&#65292;&#24050;&#23548;&#33268;&#22522;&#20110;&#19981;&#21516;&#31574;&#30053;&#30340;&#21508;&#31181;&#26041;&#27861;&#30340;&#20986;&#29616;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20248;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#23398;&#20064;&#20102;&#19968;&#20010;&#22522;&#20110;&#36817;&#31471;&#31639;&#23376;&#20998;&#35010;&#31639;&#27861;&#30340;&#24230;&#37327;&#31354;&#38388;&#65292;&#20197;&#26368;&#22823;&#21270;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#20248;&#21270;&#29702;&#35770;&#30740;&#31350;&#24471;&#20986;&#20102;&#19968;&#20123;&#29305;&#23450;&#38382;&#39064;&#31867;&#30340;&#26368;&#20248;&#24230;&#37327;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#24182;&#19981;&#33021;&#25512;&#24191;&#21040;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#24418;&#24335;&#65292;&#21253;&#25324;&#19968;&#33324;&#30340;&#20108;&#27425;&#35268;&#21010;&#65288;QP&#65289;&#38382;&#39064;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#21487;&#24494;&#20248;&#21270;&#22914;&#20309;&#23454;&#29616;&#31471;&#21040;&#31471;&#23398;&#20064;&#36817;&#31471;&#24230;&#37327;&#65292;&#25552;&#21319;&#36817;&#31471;&#31639;&#27861;&#22312;QP&#38382;&#39064;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#36229;&#36234;&#20102;&#22522;&#20110;&#24050;&#30693;&#29702;&#35770;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00882v1 Announce Type: new  Abstract: Recent work has shown a variety of ways in which machine learning can be used to accelerate the solution of constrained optimization problems. Increasing demand for real-time decision-making capabilities in applications such as artificial intelligence and optimal control has led to a variety of approaches, based on distinct strategies. This work proposes a novel approach to learning optimization, in which the underlying metric space of a proximal operator splitting algorithm is learned so as to maximize its convergence rate. While prior works in optimization theory have derived optimal metrics for limited classes of problems, the results do not extend to many practical problem forms including general Quadratic Programming (QP). This paper shows how differentiable optimization can enable the end-to-end learning of proximal metrics, enhancing the convergence of proximal algorithms for QP problems beyond what is possible based on known theo
&lt;/p&gt;</description></item><item><title>&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#23384;&#22312;&#26356;&#32039;&#23494;&#30340;&#20851;&#31995;&#65292;&#36825;&#31181;&#20851;&#31995;&#27604;&#25105;&#20204;&#36890;&#24120;&#35748;&#20026;&#30340;&#26356;&#23454;&#38469;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#28145;&#23618;&#27425;&#30340;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2404.00880</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#38750;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;&#31232;&#30095;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Relationship between Recurrent and Non-Recurrent Neural Networks: A Study in Sparsity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00880
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#23384;&#22312;&#26356;&#32039;&#23494;&#30340;&#20851;&#31995;&#65292;&#36825;&#31181;&#20851;&#31995;&#27604;&#25105;&#20204;&#36890;&#24120;&#35748;&#20026;&#30340;&#26356;&#23454;&#38469;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#28145;&#23618;&#27425;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21487;&#20197;&#20998;&#20026;&#20004;&#22823;&#31867;&#65292;&#21363;&#24490;&#29615;&#21644;&#38750;&#24490;&#29615;&#12290;&#36825;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#31867;&#22411;&#37117;&#24456;&#21463;&#27426;&#36814;&#24182;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#36890;&#24120;&#23558;&#23427;&#20204;&#35270;&#20026;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#19981;&#21516;&#23478;&#26063;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#20182;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#23384;&#22312;&#27604;&#36890;&#24120;&#35748;&#35782;&#26356;&#32039;&#23494;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#24120;&#35265;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12289;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#29978;&#33267;&#28145;&#24230;&#22810;&#23618;&#21464;&#21387;&#22120;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#36845;&#20195;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00880v1 Announce Type: new  Abstract: Neural networks (NN) can be divided into two broad categories, recurrent and non-recurrent. Both types of neural networks are popular and extensively studied, but they are often treated as distinct families of machine learning algorithms. In this position paper, we argue that there is a closer relationship between these two types of neural networks than is normally appreciated. We show that many common neural network models, such as Recurrent Neural Networks (RNN), Multi-Layer Perceptrons (MLP), and even deep multi-layer transformers, can all be represented as iterative maps.   The close relationship between RNNs and other types of NNs should not be surprising. In particular, RNNs are known to be Turing complete, and therefore capable of representing any computable function (such as any other types of NNs), but herein we argue that the relationship runs deeper and is more practical than this. For example, RNNs are often thought to be mor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Lipsum-FT &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35821;&#35328;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#22312;&#20998;&#24067;&#36716;&#31227;&#22330;&#26223;&#19979;&#23545;&#38646;&#26679;&#26412;&#27169;&#22411;&#36827;&#34892;&#31283;&#20581;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00860</link><description>&lt;p&gt;
Lipsum-FT: &#20351;&#29992;&#38543;&#26426;&#25991;&#26412;&#24341;&#23548;&#36827;&#34892;&#38646;&#26679;&#26412;&#27169;&#22411;&#30340;&#31283;&#20581;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Lipsum-FT &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35821;&#35328;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#22312;&#20998;&#24067;&#36716;&#31227;&#22330;&#26223;&#19979;&#23545;&#38646;&#26679;&#26412;&#27169;&#22411;&#36827;&#34892;&#31283;&#20581;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#23545;&#27604;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#38646;&#26679;&#26412;&#27169;&#22411;&#25552;&#20379;&#20102;&#22312;&#19968;&#31995;&#21015;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#31454;&#20105;&#24615;&#34920;&#29616;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#22312;&#19979;&#28216;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#20174;&#30740;&#31350;&#38656;&#35201;&#36798;&#21040;&#31283;&#20581;&#24494;&#35843;&#30446;&#26631;&#30340;&#26465;&#20214;&#24320;&#22987;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#29305;&#24449;&#22833;&#30495;&#29702;&#35770;&#21644;&#32852;&#21512;&#33021;&#37327;&#27169;&#22411;&#30340;&#25551;&#36848;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31283;&#20581;&#24494;&#35843;&#31639;&#27861; Lipsum-FT&#65292;&#26377;&#25928;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#12290;&#22312; DomainNet &#21644; ImageNet &#19978;&#36827;&#34892;&#30340;&#20998;&#24067;&#36716;&#31227;&#22330;&#26223;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340; Lipsum-FT &#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00860v1 Announce Type: new  Abstract: Large-scale contrastive vision-language pre-trained models provide the zero-shot model achieving competitive performance across a range of image classification tasks without requiring training on downstream data. Recent works have confirmed that while additional fine-tuning of the zero-shot model on the reference data results in enhanced downstream performance, it compromises the model's robustness against distribution shifts. Our investigation begins by examining the conditions required to achieve the goals of robust fine-tuning, employing descriptions based on feature distortion theory and joint energy-based models. Subsequently, we propose a novel robust fine-tuning algorithm, Lipsum-FT, that effectively utilizes the language modeling aspect of the vision-language pre-trained models. Extensive experiments conducted on distribution shift scenarios in DomainNet and ImageNet confirm the superiority of our proposed Lipsum-FT approach over
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20250;&#25552;&#21069;&#20934;&#22791;&#26410;&#26469;&#26631;&#35760;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#26159;&#36890;&#36807;&#39044;&#32531;&#23384;&#25110;&#38754;&#21253;&#23633;&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00859</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#25552;&#21069;&#20026;&#26410;&#26469;&#26631;&#35760;&#36827;&#34892;&#35268;&#21010;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do language models plan ahead for future tokens?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00859
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20250;&#25552;&#21069;&#20934;&#22791;&#26410;&#26469;&#26631;&#35760;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#26159;&#36890;&#36807;&#39044;&#32531;&#23384;&#25110;&#38754;&#21253;&#23633;&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00859v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22312;&#32473;&#23450;&#20301;&#32622;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#21464;&#21387;&#22120;&#26159;&#21542;&#20250;&#8220;&#25552;&#21069;&#24605;&#32771;&#8221;&#65311;&#24050;&#30693;&#21464;&#21387;&#22120;&#22312;$t$&#30340;&#21069;&#21521;&#20256;&#36882;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#20934;&#22791;&#20449;&#24687;&#65292;&#28982;&#21518;&#22312;&#26410;&#26469;&#30340;&#21069;&#21521;&#20256;&#36882;$t+\tau$&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#37322;&#36825;&#31181;&#29616;&#35937;&#30340;&#21487;&#33021;&#24615;&#65306;&#39044;&#32531;&#23384;&#65292;&#21363;&#35757;&#32451;&#20013;&#23384;&#22312;&#30340;&#38750;&#23545;&#35282;&#26799;&#24230;&#39033;&#23548;&#33268;&#27169;&#22411;&#22312;$t$&#35745;&#31639;&#19982;&#24403;&#21069;&#25512;&#29702;&#20219;&#21153;&#26080;&#20851;&#20294;&#23545;&#26410;&#26469;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#38754;&#21253;&#23633;&#65292;&#21363;&#19982;&#26102;&#38388;&#27493;&#38271;$t$&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#24050;&#32463;&#19982;&#37027;&#20123;&#23558;&#26368;&#26377;&#21033;&#20110;&#26102;&#38388;&#27493;&#38271;$t+\tau$&#30340;&#29305;&#24449;&#30456;&#21516;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19981;&#23558;&#26799;&#24230;&#20256;&#25773;&#21040;&#36807;&#21435;&#26102;&#38388;&#27493;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#27979;&#35797;&#36825;&#20123;&#20551;&#35774;&#65292;&#36825;&#31181;&#26041;&#26696;&#25105;&#20204;&#27491;&#24335;&#31216;&#20026;&#30701;&#35270;&#35757;&#32451;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#39044;&#32531;&#23384;&#30340;&#26126;&#30830;&#35777;&#25454;&#12290;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#26356;&#22810;&#22320;&#25903;&#25345;&#20102;&#38754;&#21253;&#23633;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00859v1 Announce Type: cross  Abstract: Do transformers "think ahead" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at $t$ that is then used in future forward passes $t+\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present in training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis.
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#36234;&#21335;&#22330;&#26223;&#25991;&#23383;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00852</link><description>&lt;p&gt;
&#38024;&#23545;&#36234;&#21335;&#22478;&#24066;&#29615;&#22659;&#30340;&#38598;&#25104;&#23398;&#20064;&#29992;&#20110;&#22330;&#26223;&#25991;&#23383;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Ensemble Learning for Vietnamese Scene Text Spotting in Urban Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00852
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#36234;&#21335;&#22330;&#26223;&#25991;&#23383;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36234;&#21335;&#22330;&#26223;&#25991;&#23383;&#35782;&#21035;&#12290;&#21033;&#29992;&#38598;&#25104;&#23398;&#20064;&#30340;&#24378;&#22823;&#20248;&#21183;&#65292;&#32467;&#21512;&#22810;&#20010;&#27169;&#22411;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#26174;&#33879;&#25552;&#21319;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#22330;&#26223;&#25991;&#23383;&#35782;&#21035;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;VinText&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#30456;&#36739;&#29616;&#26377;&#26041;&#27861;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#36798;&#21040;&#20102;&#24778;&#20154;&#30340;5%&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#26080;&#30097;&#23637;&#31034;&#20102;&#38598;&#25104;&#23398;&#20064;&#22312;&#36234;&#21335;&#22478;&#24066;&#29615;&#22659;&#19979;&#22330;&#26223;&#25991;&#23383;&#35782;&#21035;&#20013;&#30340;&#21151;&#25928;&#65292;&#31361;&#26174;&#20854;&#22312;&#22478;&#24066;&#26631;&#35782;&#12289;&#24191;&#21578;&#20197;&#21450;&#21508;&#31181;&#23500;&#21547;&#25991;&#23383;&#30340;&#22478;&#24066;&#22330;&#26223;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00852v1 Announce Type: cross  Abstract: This paper presents a simple yet efficient ensemble learning framework for Vietnamese scene text spotting. Leveraging the power of ensemble learning, which combines multiple models to yield more accurate predictions, our approach aims to significantly enhance the performance of scene text spotting in challenging urban settings. Through experimental evaluations on the VinText dataset, our proposed method achieves a significant improvement in accuracy compared to existing methods with an impressive accuracy of 5%. These results unequivocally demonstrate the efficacy of ensemble learning in the context of Vietnamese scene text spotting in urban environments, highlighting its potential for real world applications, such as text detection and recognition in urban signage, advertisements, and various text-rich urban scenes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#29616;&#20195;&#35782;&#21035;&#26041;&#27861;&#27604;&#36739;&#20915;&#31574;&#25919;&#31574;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20851;&#38190;&#22312;&#20110;&#21487;&#20197;&#23433;&#20840;&#24573;&#30053;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2404.00848</link><description>&lt;p&gt;
&#20915;&#31574;&#25919;&#31574;&#22312;&#28151;&#26434;&#24773;&#20917;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Predictive Performance Comparison of Decision Policies Under Confounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00848
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#29616;&#20195;&#35782;&#21035;&#26041;&#27861;&#27604;&#36739;&#20915;&#31574;&#25919;&#31574;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20851;&#38190;&#22312;&#20110;&#21487;&#20197;&#23433;&#20840;&#24573;&#30053;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#34987;&#24341;&#20837;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#20854;&#22522;&#26412;&#29702;&#24565;&#26159;&#23427;&#20204;&#21487;&#20197;&#25552;&#21319;&#20915;&#31574;&#25919;&#31574;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#36890;&#24120;&#23384;&#22312;&#20110;&#26410;&#26126;&#30830;&#35268;&#23450;&#21644;&#20381;&#36182;&#19981;&#21487;&#35266;&#27979;&#22240;&#32032;&#30340;&#29616;&#26377;&#20915;&#31574;&#25919;&#31574;&#30456;&#27604;&#36739;&#39044;&#27979;&#24615;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#36890;&#24120;&#22312;&#23454;&#36341;&#20013;&#34987;&#36890;&#36807;&#23545;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#36827;&#34892;&#24378;&#20551;&#35774;&#26469;&#22788;&#29702;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26469;&#27604;&#36739;&#20915;&#31574;&#25919;&#31574;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#26681;&#25454;&#22240;&#26524;&#25512;&#26029;&#21644;&#31163;&#32447;&#35780;&#20272;&#25991;&#29486;&#20013;&#30340;&#21508;&#31181;&#29616;&#20195;&#35782;&#21035;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65288;&#20363;&#22914;&#65292;&#24037;&#20855;&#21464;&#37327;&#65292;&#36793;&#38469;&#25935;&#24863;&#24615;&#27169;&#22411;&#65292;&#36817;&#31471;&#21464;&#37327;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#25105;&#20204;&#21487;&#20197;&#23433;&#20840;&#22320;&#24573;&#30053;&#25919;&#31574;&#27604;&#36739;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#38480;&#26679;&#26412;&#20272;&#35745;&#36951;&#25022;&#21306;&#38388;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00848v1 Announce Type: new  Abstract: Predictive models are often introduced to decision-making tasks under the rationale that they improve performance over an existing decision-making policy. However, it is challenging to compare predictive performance against an existing decision-making policy that is generally under-specified and dependent on unobservable factors. These sources of uncertainty are often addressed in practice by making strong assumptions about the data-generating mechanism. In this work, we propose a method to compare the predictive performance of decision policies under a variety of modern identification approaches from the causal inference and off-policy evaluation literatures (e.g., instrumental variable, marginal sensitivity model, proximal variable). Key to our method is the insight that there are regions of uncertainty that we can safely ignore in the policy comparison. We develop a practical approach for finite-sample estimation of regret intervals u
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#28857;&#21464;&#25442;&#22120;&#30340;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#22312;&#22788;&#29702;&#28857;&#20113;&#25968;&#25454;&#20998;&#31867;&#31561;&#20219;&#21153;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36801;&#31227;&#23398;&#20064;&#25928;&#26524;&#24182;&#19981;&#24635;&#26159;&#20248;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#36739;&#22823;&#12290;</title><link>https://arxiv.org/abs/2404.00846</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#21464;&#25442;&#22120;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning with Point Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00846
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28857;&#21464;&#25442;&#22120;&#30340;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#22312;&#22788;&#29702;&#28857;&#20113;&#25968;&#25454;&#20998;&#31867;&#31561;&#20219;&#21153;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36801;&#31227;&#23398;&#20064;&#25928;&#26524;&#24182;&#19981;&#24635;&#26159;&#20248;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#21464;&#25442;&#22120;&#26159;&#36817;&#20046;&#22788;&#20110;&#26368;&#20808;&#36827;&#27700;&#24179;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#28857;&#20113;&#25968;&#25454;&#19978;&#30340;&#20998;&#31867;&#12289;&#20998;&#21106;&#21644;&#26816;&#27979;&#20219;&#21153;&#12290;&#23427;&#20204;&#21033;&#29992;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#27169;&#25311;&#22810;&#20010;&#28857;&#38598;&#20043;&#38388;&#30340;&#22823;&#33539;&#22260;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20004;&#20214;&#20107;&#65306;&#36825;&#20123;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32593;&#32476;&#22312;ModelNet10&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#26469;&#23545;3D MNIST&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#31867;&#24494;&#35843;&#12290;&#25105;&#20204;&#36824;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#22312;3D MNIST&#25968;&#25454;&#38598;&#19978;&#65292;&#20197;&#27604;&#36739;&#24494;&#35843;&#21644;&#20174;&#22836;&#24320;&#22987;&#27169;&#22411;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30001;&#20110;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#22312;&#20998;&#24067;&#31243;&#24230;&#19978;&#23384;&#22312;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24182;&#27809;&#26377;&#36229;&#36234;&#20174;&#22836;&#24320;&#22987;&#27169;&#22411;&#12290;&#23613;&#31649;&#25105;&#20204;&#39044;&#26399;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#20250;&#25910;&#25947;&#26356;&#24555;&#65292;&#22240;&#20026;&#23427;&#20204;&#24050;&#32463;&#20174;ModelNet10&#25968;&#25454;&#38598;&#20013;&#30693;&#36947;&#20102;&#36793;&#32536;&#12289;&#35282;&#31561;&#24213;&#23618;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00846v1 Announce Type: cross  Abstract: Point Transformers are near state-of-the-art models for classification, segmentation, and detection tasks on Point Cloud data. They utilize a self attention based mechanism to model large range spatial dependencies between multiple point sets. In this project we explore two things: classification performance of these attention based networks on ModelNet10 dataset and then, we use the trained model to classify 3D MNIST dataset after finetuning. We also train the model from scratch on 3D MNIST dataset to compare the performance of finetuned and from-scratch model on the MNIST dataset. We observe that since the two datasets have a large difference in the degree of the distributions, transfer learned models do not outperform the from-scratch models in this case. Although we do expect transfer learned models to converge faster since they already know the lower level edges, corners, etc features from the ModelNet10 dataset.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#37329;&#23383;&#22612;&#37319;&#26679;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#22312;&#20083;&#33146;&#30284;&#32452;&#32455;&#22270;&#20687;&#20013;&#23545;HER2&#29366;&#24577;&#30340;&#33258;&#21160;&#20998;&#31867;&#65292;&#26377;&#25928;&#31649;&#29702;&#35745;&#31639;&#36127;&#33655;&#24182;&#26377;&#25928;&#22320;&#20998;&#26512;&#32454;&#32990;&#21644;&#26356;&#22823;&#33539;&#22260;&#30340;&#32452;&#32455;&#32454;&#33410;&#12290;</title><link>https://arxiv.org/abs/2404.00837</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#37329;&#23383;&#22612;&#37319;&#26679;&#22312;&#20083;&#33146;&#30284;&#22270;&#20687;&#20013;&#33258;&#21160;&#36827;&#34892;HER2&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Automated HER2 Scoring in Breast Cancer Images Using Deep Learning and Pyramid Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00837
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#37329;&#23383;&#22612;&#37319;&#26679;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#22312;&#20083;&#33146;&#30284;&#32452;&#32455;&#22270;&#20687;&#20013;&#23545;HER2&#29366;&#24577;&#30340;&#33258;&#21160;&#20998;&#31867;&#65292;&#26377;&#25928;&#31649;&#29702;&#35745;&#31639;&#36127;&#33655;&#24182;&#26377;&#25928;&#22320;&#20998;&#26512;&#32454;&#32990;&#21644;&#26356;&#22823;&#33539;&#22260;&#30340;&#32452;&#32455;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#34920;&#30382;&#29983;&#38271;&#22240;&#23376;&#21463;&#20307;2&#65288;HER2&#65289;&#26159;&#30284;&#32454;&#32990;&#22686;&#38271;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#34507;&#30333;&#36136;&#65292;&#26631;&#24535;&#30528;&#20083;&#33146;&#30284;&#65288;BC&#65289;&#30340;&#20405;&#30053;&#24615;&#24182;&#26377;&#21161;&#20110;&#39044;&#27979;&#20854;&#39044;&#21518;&#12290;&#23545;&#20110;HER2&#22312;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#20013;&#34920;&#36798;&#27700;&#24179;&#30340;&#20934;&#30830;&#35780;&#20272;&#23545;&#20110;&#27835;&#30103;&#25351;&#23548;&#21644;&#20102;&#35299;&#30284;&#30151;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#30001;&#33891;&#20107;&#20250;&#35748;&#35777;&#30340;&#30149;&#29702;&#23398;&#23478;&#25163;&#21160;&#26816;&#26597;&#30340;&#24037;&#20316;&#27969;&#31243;&#38754;&#20020;&#25361;&#25112;&#65292;&#21253;&#25324;&#35266;&#23519;&#32773;&#38388;&#21644;&#35266;&#23519;&#32773;&#20869;&#30340;&#19981;&#19968;&#33268;&#24615;&#20197;&#21450;&#24310;&#38271;&#30340;&#21608;&#36716;&#26102;&#38388;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#37329;&#23383;&#22612;&#37319;&#26679;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;IHC&#26579;&#33394;&#30340;BC&#32452;&#32455;&#22270;&#20687;&#20013;&#30340;HER2&#29366;&#24577;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#26512;&#19981;&#21516;&#31354;&#38388;&#23610;&#24230;&#19979;&#30340;&#24418;&#24577;&#29305;&#24449;&#65292;&#26377;&#25928;&#31649;&#29702;&#35745;&#31639;&#36127;&#33655;&#65292;&#24182;&#20419;&#36827;&#23545;&#32454;&#32990;&#21644;&#26356;&#22823;&#33539;&#22260;&#32452;&#32455;&#27700;&#24179;&#32454;&#33410;&#30340;&#35814;&#32454;&#26816;&#26597;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00837v1 Announce Type: cross  Abstract: Human epidermal growth factor receptor 2 (HER2) is a critical protein in cancer cell growth that signifies the aggressiveness of breast cancer (BC) and helps predict its prognosis. Accurate assessment of immunohistochemically (IHC) stained tissue slides for HER2 expression levels is essential for both treatment guidance and understanding of cancer mechanisms. Nevertheless, the traditional workflow of manual examination by board-certified pathologists encounters challenges, including inter- and intra-observer inconsistency and extended turnaround times. Here, we introduce a deep learning-based approach utilizing pyramid sampling for the automated classification of HER2 status in IHC-stained BC tissue images. Our approach analyzes morphological features at various spatial scales, efficiently managing the computational load and facilitating a detailed examination of cellular and larger-scale tissue-level details. This method addresses the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#32536;&#23398;&#20064;&#20013;&#32852;&#21512;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#35774;&#35745;&#33539;&#24335;&#65292;&#36890;&#36807;&#20998;&#26512;&#31995;&#32479;&#21442;&#25968;&#23545;&#25910;&#25947;&#36895;&#29575;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#31649;&#29702;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.00836</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#23398;&#20064;&#20013;&#37325;&#26032;&#24605;&#32771;&#36164;&#28304;&#31649;&#29702;&#65306;&#19968;&#31181;&#32852;&#21512;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#35774;&#35745;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Rethinking Resource Management in Edge Learning: A Joint Pre-training and Fine-tuning Design Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00836
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36793;&#32536;&#23398;&#20064;&#20013;&#32852;&#21512;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#35774;&#35745;&#33539;&#24335;&#65292;&#36890;&#36807;&#20998;&#26512;&#31995;&#32479;&#21442;&#25968;&#23545;&#25910;&#25947;&#36895;&#29575;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#31649;&#29702;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#65292;&#36793;&#32536;&#23398;&#20064;&#27491;&#32463;&#21382;&#30528;&#20174;&#20256;&#32479;&#30340;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#36716;&#21521;&#32479;&#19968;&#39044;&#35757;&#32451;&#21644;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#30340;&#26032;&#20004;&#38454;&#27573;&#23398;&#20064;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#20004;&#38454;&#27573;&#36793;&#32536;&#23398;&#20064;&#31995;&#32479;&#20013;&#36827;&#34892;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#31649;&#29702;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#31995;&#32479;&#20013;&#65292;&#27169;&#22411;&#39044;&#35757;&#32451;&#39318;&#20808;&#36890;&#36807;&#36793;&#32536;&#26381;&#21153;&#22120;&#22312;&#26412;&#22320;&#39044;&#23384;&#30340;&#36890;&#29992;&#25968;&#25454;&#19978;&#36827;&#34892;&#38598;&#20013;&#24335;&#23398;&#20064;&#65292;&#28982;&#21518;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#36807;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#12290;&#23545;&#20110;&#20004;&#38454;&#27573;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#25910;&#25947;&#34892;&#20026;&#65288;&#20197;&#24179;&#22343;&#24179;&#26041;&#26799;&#24230;&#33539;&#25968;&#30028;&#24418;&#24335;&#65289;&#65292;&#36825;&#34920;&#24449;&#20102;&#21508;&#31181;&#31995;&#32479;&#21442;&#25968;&#65288;&#22914;&#20004;&#38454;&#27573;&#20013;&#23398;&#20064;&#36718;&#25968;&#21644;&#25209;&#27425;&#22823;&#23567;&#65289;&#23545;&#25910;&#25947;&#36895;&#29575;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00836v1 Announce Type: cross  Abstract: In some applications, edge learning is experiencing a shift in focusing from conventional learning from scratch to new two-stage learning unifying pre-training and task-specific fine-tuning. This paper considers the problem of joint communication and computation resource management in a two-stage edge learning system. In this system, model pre-training is first conducted at an edge server via centralized learning on local pre-stored general data, and then task-specific fine-tuning is performed at edge devices based on the pre-trained model via federated edge learning. For the two-stage learning model, we first analyze the convergence behavior (in terms of the average squared gradient norm bound), which characterizes the impacts of various system parameters such as the number of learning rounds and batch sizes in the two stages on the convergence rate. Based on our analytical results, we then propose a joint communication and computatio
&lt;/p&gt;</description></item><item><title>HeteroMILE&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#23884;&#20837;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#24403;&#20195;&#22270;&#23884;&#20837;&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#24322;&#26500;&#22270;&#65292;&#24182;&#36890;&#36807;&#21453;&#22797;&#31895;&#21270;&#21644;&#32454;&#21270;&#30340;&#26041;&#24335;&#26377;&#25928;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.00816</link><description>&lt;p&gt;
HeteroMILE: &#29992;&#20110;&#24322;&#26500;&#22270;&#30340;&#22810;&#23618;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HeteroMILE: a Multi-Level Graph Representation Learning Framework for Heterogeneous Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00816
&lt;/p&gt;
&lt;p&gt;
HeteroMILE&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#23884;&#20837;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#24403;&#20195;&#22270;&#23884;&#20837;&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#24322;&#26500;&#22270;&#65292;&#24182;&#36890;&#36807;&#21453;&#22797;&#31895;&#21270;&#21644;&#32454;&#21270;&#30340;&#26041;&#24335;&#26377;&#25928;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#34920;&#31034;&#19981;&#21516;&#31867;&#22411;&#23454;&#20307;&#20043;&#38388;&#30340;&#21508;&#31181;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#36825;&#31181;&#22270;&#20013;&#30340;&#23884;&#20837;&#26159;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#30001;&#20110;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#32780;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#24322;&#26500;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33410;&#28857;&#22312;&#24322;&#26500;&#22270;&#19978;&#30340;&#22810;&#32423;&#23884;&#20837;&#26694;&#26550;&#65288;HeteroMILE&#65289;-&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#24403;&#20195;&#22270;&#23884;&#20837;&#26041;&#27861;&#25193;&#23637;&#21040;&#22823;&#22270;&#12290;HeteroMILE&#23558;&#22823;&#22270;&#21453;&#22797;&#31895;&#21270;&#20026;&#36739;&#23567;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#30041;&#22270;&#30340;&#20027;&#24178;&#32467;&#26500;&#65292;&#28982;&#21518;&#23558;&#20854;&#23884;&#20837;&#65292;&#36890;&#36807;&#36991;&#20813;&#32791;&#26102;&#30340;&#22788;&#29702;&#25805;&#20316;&#26377;&#25928;&#22320;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#24322;&#26500;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23558;&#31895;&#21270;&#30340;&#23884;&#20837;&#20248;&#21270;&#21040;&#21407;&#22987;&#22270;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00816v1 Announce Type: cross  Abstract: Heterogeneous graphs are ubiquitous in real-world applications because they can represent various relationships between different types of entities. Therefore, learning embeddings in such graphs is a critical problem in graph machine learning. However, existing solutions for this problem fail to scale to large heterogeneous graphs due to their high computational complexity. To address this issue, we propose a Multi-Level Embedding framework of nodes on a heterogeneous graph (HeteroMILE) - a generic methodology that allows contemporary graph embedding methods to scale to large graphs. HeteroMILE repeatedly coarsens the large sized graph into a smaller size while preserving the backbone structure of the graph before embedding it, effectively reducing the computational cost by avoiding time-consuming processing operations. It then refines the coarsened embedding to the original graph using a heterogeneous graph convolution neural network.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36807;&#28388;&#36755;&#20837;&#20449;&#21495;&#26469;&#20248;&#21270;&#19982;&#20869;&#23384;&#36890;&#20449;&#65292;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20256;&#32479;&#24605;&#32500;&#12290;</title><link>https://arxiv.org/abs/2404.00798</link><description>&lt;p&gt;
&#22312;&#36890;&#36807;&#20849;&#20139;&#20869;&#23384;&#36827;&#34892;&#27880;&#24847;&#21147;&#22240;&#23376;&#20998;&#35299;&#30340;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
On Difficulties of Attention Factorization through Shared Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00798
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36807;&#28388;&#36755;&#20837;&#20449;&#21495;&#26469;&#20248;&#21270;&#19982;&#20869;&#23384;&#36890;&#20449;&#65292;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20256;&#32479;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#38899;&#39057;&#22788;&#29702;&#22312;&#20869;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#38761;&#21629;&#20102;&#28145;&#24230;&#23398;&#20064;&#12290;&#23427;&#20204;&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20801;&#35768;&#21457;&#29616;&#22797;&#26434;&#30340;&#36755;&#20837;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26426;&#21046;&#30340;&#20108;&#27425;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24615;&#20026;&#26356;&#22823;&#30340;&#36755;&#20837;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#30740;&#31350;&#20154;&#21592;&#29616;&#22312;&#27491;&#22312;&#30740;&#31350;&#35832;&#22914;&#32447;&#24615;&#32479;&#19968;&#23884;&#22871;&#27880;&#24847;&#21147;&#65288;Luna&#65289;&#25110;Memory Augmented Transformer&#31561;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#22806;&#37096;&#21487;&#23398;&#20064;&#20869;&#23384;&#65292;&#23558;&#27880;&#24847;&#21147;&#35745;&#31639;&#22797;&#26434;&#24615;&#38477;&#20302;&#21040;&#32447;&#24615;&#65292;&#25110;&#32773;&#22312;&#20197;&#22359;&#20026;&#21333;&#20301;&#30340;&#22788;&#29702;&#20013;&#22312;&#22359;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25361;&#25112;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#20256;&#32479;&#24605;&#32500;&#65292;&#25581;&#31034;&#20102;&#36890;&#36807;&#21508;&#31181;&#25805;&#20316;&#30452;&#25509;&#19982;&#20869;&#23384;&#25509;&#21475;&#30340;&#26041;&#27861;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#19982;&#20869;&#23384;&#36890;&#20449;&#20043;&#21069;&#36807;&#28388;&#36755;&#20837;&#20449;&#21495;&#65292;&#24615;&#33021;&#21487;&#33021;&#20250;&#24471;&#21040;&#26174;&#30528;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00798v1 Announce Type: new  Abstract: Transformers have revolutionized deep learning in numerous fields, including natural language processing, computer vision, and audio processing. Their strength lies in their attention mechanism, which allows for the discovering of complex input relationships. However, this mechanism's quadratic time and memory complexity pose challenges for larger inputs. Researchers are now investigating models like Linear Unified Nested Attention (Luna) or Memory Augmented Transformer, which leverage external learnable memory to either reduce the attention computation complexity down to linear, or to propagate information between chunks in chunk-wise processing. Our findings challenge the conventional thinking on these models, revealing that interfacing with the memory directly through an attention operation is suboptimal, and that the performance may be considerably improved by filtering the input signal before communicating with memory.
&lt;/p&gt;</description></item><item><title>Metarobotics&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#26080;&#32447;&#36890;&#20449;&#12289;&#22810;&#24863;&#23448;&#27785;&#28024;&#21644;&#38598;&#20307;&#26234;&#33021;&#65292;&#20026;&#36828;&#31243;&#26426;&#22120;&#20154;&#24212;&#29992;&#25552;&#20379;&#26222;&#36941;&#12289;&#27969;&#21160;&#21644;&#38750;&#20405;&#20837;&#24335;&#30340;&#35775;&#38382;&#21644;&#20114;&#21160;&#65292;&#26377;&#26395;&#20026;&#24037;&#19994;&#21644;&#31038;&#20250;&#24102;&#26469;&#35832;&#22810;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2404.00797</link><description>&lt;p&gt;
&#38754;&#21521;&#24037;&#19994;&#21644;&#31038;&#20250;&#30340;&#20803;&#26426;&#22120;&#20154;&#65306;&#24895;&#26223;&#12289;&#25216;&#26415;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Metarobotics for Industry and Society: Vision, Technologies, and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00797
&lt;/p&gt;
&lt;p&gt;
Metarobotics&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#26080;&#32447;&#36890;&#20449;&#12289;&#22810;&#24863;&#23448;&#27785;&#28024;&#21644;&#38598;&#20307;&#26234;&#33021;&#65292;&#20026;&#36828;&#31243;&#26426;&#22120;&#20154;&#24212;&#29992;&#25552;&#20379;&#26222;&#36941;&#12289;&#27969;&#21160;&#21644;&#38750;&#20405;&#20837;&#24335;&#30340;&#35775;&#38382;&#21644;&#20114;&#21160;&#65292;&#26377;&#26395;&#20026;&#24037;&#19994;&#21644;&#31038;&#20250;&#24102;&#26469;&#35832;&#22810;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Metarobotics&#26088;&#22312;&#23558;&#19979;&#19968;&#20195;&#26080;&#32447;&#36890;&#20449;&#65292;&#22810;&#24863;&#23448;&#27785;&#28024;&#21644;&#38598;&#20307;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#23545;&#36828;&#31243;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#26222;&#36941;&#12289;&#27969;&#21160;&#21644;&#38750;&#20405;&#20837;&#24335;&#35775;&#38382;&#21644;&#20114;&#21160;&#12290;&#24037;&#19994;&#21644;&#31038;&#20250;&#26377;&#26395;&#20174;&#36825;&#20123;&#21151;&#33021;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;Metarobotics&#22312;&#31038;&#20250;&#12289;&#24037;&#19994;&#21644;&#20004;&#32773;&#20043;&#38388;&#30340;&#30446;&#26631;&#12290;&#23427;&#30830;&#23450;&#24182;&#35843;&#26597;&#20102;&#21487;&#33021;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26550;&#26500;&#26469;&#25512;&#36827;Metarobotics&#20851;&#38190;&#32452;&#20214;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00797v1 Announce Type: cross  Abstract: Metarobotics aims to combine next generation wireless communication, multi-sense immersion, and collective intelligence to provide a pervasive, itinerant, and non-invasive access and interaction with distant robotized applications. Industry and society are expected to benefit from these functionalities. For instance, robot programmers will no longer travel worldwide to plan and test robot motions, even collaboratively. Instead, they will have a personalized access to robots and their environments from anywhere, thus spending more time with family and friends. Students enrolled in robotics courses will be taught under authentic industrial conditions in real-time. This paper describes objectives of Metarobotics in society, industry, and in-between. It identifies and surveys technologies likely to enable their completion and provides an architecture to put forward the interplay of key components of Metarobotics. Potentials for self-determ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25490;&#32451;&#30340;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#24335;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25345;&#32493;&#21521;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26032;&#27169;&#22359;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#27169;&#22359;&#32452;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#24182;&#26377;&#25928;&#25512;&#21160;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2404.00790</link><description>&lt;p&gt;
&#26080;&#38656;&#25490;&#32451;&#30340;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#24335;&#25345;&#32493;&#23398;&#20064;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Rehearsal-Free Modular and Compositional Continual Learning for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00790
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25490;&#32451;&#30340;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#24335;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25345;&#32493;&#21521;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26032;&#27169;&#22359;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#27169;&#22359;&#32452;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#24182;&#26377;&#25928;&#25512;&#21160;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#22312;&#19981;&#36951;&#24536;&#29616;&#26377;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36880;&#27493;&#33719;&#21462;&#26032;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#26041;&#27861;&#35201;&#20040;&#22522;&#20110;&#25490;&#32451;&#65292;&#21363;&#23384;&#20648;&#26469;&#33258;&#20808;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#31034;&#20363;&#20197;&#36827;&#34892;&#25968;&#25454;&#37325;&#25773;&#65292;&#35201;&#20040;&#23558;&#21442;&#25968;&#38548;&#31163;&#20998;&#37197;&#32473;&#27599;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25490;&#32451;&#30340;&#26041;&#27861;&#20250;&#24341;&#21457;&#38544;&#31169;&#21644;&#20869;&#23384;&#38382;&#39064;&#65292;&#21442;&#25968;&#38548;&#31163;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#19981;&#32771;&#34385;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#38459;&#30861;&#30693;&#35782;&#36716;&#31227;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MoCL&#65292;&#19968;&#20010;&#26080;&#38656;&#25490;&#32451;&#30340;&#27169;&#22359;&#21270;&#21644;&#32452;&#21512;&#24335;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#19981;&#26029;&#21521;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26032;&#30340;&#27169;&#22359;&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#27169;&#22359;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MoCL&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#26377;&#25928;&#20419;&#36827;&#20102;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00790v1 Announce Type: cross  Abstract: Continual learning aims at incrementally acquiring new knowledge while not forgetting existing knowledge. To overcome catastrophic forgetting, methods are either rehearsal-based, i.e., store data examples from previous tasks for data replay, or isolate parameters dedicated to each task. However, rehearsal-based methods raise privacy and memory issues, and parameter-isolation continual learning does not consider interaction between tasks, thus hindering knowledge transfer. In this work, we propose MoCL, a rehearsal-free Modular and Compositional Continual Learning framework which continually adds new modules to language models and composes them with existing modules. Experiments on various benchmarks show that MoCL outperforms state of the art and effectively facilitates knowledge transfer.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#35299;&#24320;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20013;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#30340;&#20851;&#38190;&#28508;&#21464;&#37327;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#22312;&#35299;&#24320;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00785</link><description>&lt;p&gt;
&#35299;&#24320;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#20043;&#35868;&#65306;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30740;&#31350;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;
&lt;/p&gt;
&lt;p&gt;
Disentangling Hippocampal Shape Variations: A Study of Neurological Disorders Using Graph Variational Autoencoder with Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#35299;&#24320;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20013;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#30340;&#20851;&#38190;&#28508;&#21464;&#37327;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#22312;&#35299;&#24320;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#32508;&#21512;&#30740;&#31350;&#65292;&#19987;&#27880;&#20110;&#22312;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#32972;&#26223;&#19979;&#20174;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65288;DTI&#65289;&#25968;&#25454;&#38598;&#20013;&#35299;&#24320;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#12290;&#20511;&#21161;&#22686;&#24378;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#21306;&#20998;&#20195;&#34920;&#24180;&#40836;&#21644;&#26159;&#21542;&#24739;&#30149;&#30340;&#20004;&#20010;&#19981;&#21516;&#28508;&#21464;&#37327;&#26469;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31995;&#21015;VAE&#26550;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22686;&#24378;&#30340;&#35299;&#24320;&#33021;&#21147;&#12290;&#36825;&#20010;&#35780;&#20272;&#20351;&#29992;&#20102;&#26469;&#33258;DTI&#28023;&#39532;&#25968;&#25454;&#38598;&#30340;&#21512;&#25104;3D&#29615;&#24418;&#32593;&#26684;&#25968;&#25454;&#21644;&#30495;&#23454;&#30340;3D&#28023;&#39532;&#32593;&#26684;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30417;&#30563;&#35299;&#24320;&#27169;&#22411;&#22312;&#35299;&#24320;&#20998;&#25968;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22914;&#23646;&#24615;&#21644;&#24341;&#23548;VAE&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#24180;&#40836;&#32452;&#21644;&#30142;&#30149;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00785v1 Announce Type: cross  Abstract: This paper presents a comprehensive study focused on disentangling hippocampal shape variations from diffusion tensor imaging (DTI) datasets within the context of neurological disorders. Leveraging a Graph Variational Autoencoder (VAE) enhanced with Supervised Contrastive Learning, our approach aims to improve interpretability by disentangling two distinct latent variables corresponding to age and the presence of diseases. In our ablation study, we investigate a range of VAE architectures and contrastive loss functions, showcasing the enhanced disentanglement capabilities of our approach. This evaluation uses synthetic 3D torus mesh data and real 3D hippocampal mesh datasets derived from the DTI hippocampal dataset. Our supervised disentanglement model outperforms several state-of-the-art (SOTA) methods like attribute and guided VAEs in terms of disentanglement scores. Our model distinguishes between age groups and disease status in pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#65292;&#36890;&#36807;&#22312;&#26799;&#24230;&#26356;&#26032;&#20013;&#24212;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25200;&#21160;&#65292;&#20445;&#25252;&#26377;&#29992;&#21333;&#20803;&#20197;&#38450;&#36951;&#24536;&#65292;&#21516;&#26102;&#24674;&#22797;&#19981;&#22826;&#26377;&#29992;&#21333;&#20803;&#30340;&#21487;&#22609;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00781</link><description>&lt;p&gt;
&#22788;&#29702;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#65292;&#36890;&#36807;&#22312;&#26799;&#24230;&#26356;&#26032;&#20013;&#24212;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25200;&#21160;&#65292;&#20445;&#25252;&#26377;&#29992;&#21333;&#20803;&#20197;&#38450;&#36951;&#24536;&#65292;&#21516;&#26102;&#24674;&#22797;&#19981;&#22826;&#26377;&#29992;&#21333;&#20803;&#30340;&#21487;&#22609;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#23384;&#22312;&#22256;&#38590;&#65292;&#26082;&#36973;&#21463;&#26377;&#29992;&#21333;&#20803;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21448;&#22240;&#20725;&#21270;&#21644;&#26080;&#29992;&#21333;&#20803;&#23548;&#33268;&#21487;&#22609;&#24615;&#20002;&#22833;&#12290;&#34429;&#28982;&#35768;&#22810;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#33021;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#25928;&#29992;&#30340;&#25200;&#21160;&#26799;&#24230;&#19979;&#38477;&#65288;UPGD&#65289;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#34920;&#31034;&#25345;&#32493;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;UPGD&#32467;&#21512;&#20102;&#26799;&#24230;&#26356;&#26032;&#21644;&#25200;&#21160;&#65292;&#23427;&#23545;&#26356;&#26377;&#29992;&#30340;&#21333;&#20803;&#24212;&#29992;&#36739;&#23567;&#30340;&#20462;&#25913;&#65292;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#36951;&#24536;&#65292;&#23545;&#19981;&#22826;&#26377;&#29992;&#30340;&#21333;&#20803;&#24212;&#29992;&#36739;&#22823;&#30340;&#20462;&#25913;&#65292;&#24674;&#22797;&#23427;&#20204;&#30340;&#21487;&#22609;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00781v1 Announce Type: cross  Abstract: Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#20214;&#32423;&#20154;&#33080;&#21435;&#26631;&#35782;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20809;&#23398;&#32534;&#30721;&#22120;&#21644;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#20154;&#33080;&#28909;&#22270;&#65292;&#21516;&#26102;&#38544;&#34255;&#20154;&#33080;&#36523;&#20221;&#65292;&#35299;&#20915;&#20102;&#20154;&#33080;&#21435;&#26631;&#35782;&#21270;&#36719;&#20214;&#23384;&#22312;&#30340;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2404.00777</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#20809;&#23398;&#25216;&#26415;&#22686;&#24378;&#20154;&#33080;&#21435;&#26631;&#35782;&#21270;&#30340;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving Optics for Enhancing Protection in Face De-identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#20214;&#32423;&#20154;&#33080;&#21435;&#26631;&#35782;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20809;&#23398;&#32534;&#30721;&#22120;&#21644;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#20154;&#33080;&#28909;&#22270;&#65292;&#21516;&#26102;&#38544;&#34255;&#20154;&#33080;&#36523;&#20221;&#65292;&#35299;&#20915;&#20102;&#20154;&#33080;&#21435;&#26631;&#35782;&#21270;&#36719;&#20214;&#23384;&#22312;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#26426;&#30340;&#29616;&#20195;&#28608;&#22686;&#65292;&#20197;&#21450;&#24191;&#27867;&#24212;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#24341;&#21457;&#20102;&#37325;&#22823;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#25285;&#24551;&#12290;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#24110;&#21161;&#35782;&#21035;&#30456;&#20851;&#20107;&#20214;&#65292;&#24182;&#22312;&#23478;&#24237;&#12289;&#21150;&#20844;&#23460;&#12289;&#21307;&#38498;&#31561;&#39046;&#22495;&#30340;&#26085;&#24120;&#20219;&#21153;&#20013;&#25552;&#20379;&#25903;&#25345;&#12290;&#20026;&#20102;&#36825;&#20123;&#30446;&#30340;&#32780;&#35775;&#38382;&#25110;&#22788;&#29702;&#20010;&#20154;&#20449;&#24687;&#24341;&#21457;&#20102;&#38544;&#31169;&#25285;&#24551;&#12290;&#34429;&#28982;&#36719;&#20214;&#32423;&#35299;&#20915;&#26041;&#26696;&#22914;&#20154;&#33080;&#21435;&#26631;&#35782;&#21270;&#25552;&#20379;&#20102;&#24456;&#22909;&#30340;&#38544;&#31169;/&#25928;&#29992;&#26435;&#34913;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#21957;&#25506;&#25915;&#20987;&#30340;&#28431;&#27934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#20214;&#32423;&#20154;&#33080;&#21435;&#26631;&#35782;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#28431;&#27934;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#23398;&#20064;&#20809;&#23398;&#32534;&#30721;&#22120;&#20197;&#21450;&#22238;&#24402;&#27169;&#22411;&#65292;&#33719;&#21462;&#20154;&#33080;&#28909;&#22270;&#65292;&#21516;&#26102;&#38544;&#34255;&#28304;&#22270;&#20687;&#20013;&#30340;&#20154;&#33080;&#36523;&#20221;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21311;&#21517;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#22270;&#20687;&#12289;&#20154;&#33080;&#28909;&#22270;&#21644;&#21442;&#32771;&#20154;&#33080;&#29983;&#25104;&#19968;&#20010;&#26032;&#30340;&#20154;&#33080;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00777v1 Announce Type: cross  Abstract: The modern surge in camera usage alongside widespread computer vision technology applications poses significant privacy and security concerns. Current artificial intelligence (AI) technologies aid in recognizing relevant events and assisting in daily tasks in homes, offices, hospitals, etc. The need to access or process personal information for these purposes raises privacy concerns. While software-level solutions like face de-identification provide a good privacy/utility trade-off, they present vulnerabilities to sniffing attacks. In this paper, we propose a hardware-level face de-identification method to solve this vulnerability. Specifically, our approach first learns an optical encoder along with a regression model to obtain a face heatmap while hiding the face identity from the source image. We also propose an anonymization framework that generates a new face using the privacy-preserving image, face heatmap, and a reference face i
&lt;/p&gt;</description></item><item><title>PyTorch Frame&#26159;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#30340;PyTorch&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#25968;&#25454;&#32467;&#26500;&#12289;&#27169;&#22411;&#25277;&#35937;&#21644;&#22806;&#37096;&#22522;&#30784;&#27169;&#22411;&#25972;&#21512;&#31561;&#21151;&#33021;&#65292;&#23454;&#29616;&#20102;&#27169;&#22359;&#21270;&#30340;&#34920;&#26684;&#27169;&#22411;&#23454;&#29616;&#65292;&#24182;&#25104;&#21151;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2404.00776</link><description>&lt;p&gt;
PyTorch Frame: &#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#34920;&#26684;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00776
&lt;/p&gt;
&lt;p&gt;
PyTorch Frame&#26159;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#30340;PyTorch&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#25968;&#25454;&#32467;&#26500;&#12289;&#27169;&#22411;&#25277;&#35937;&#21644;&#22806;&#37096;&#22522;&#30784;&#27169;&#22411;&#25972;&#21512;&#31561;&#21151;&#33021;&#65292;&#23454;&#29616;&#20102;&#27169;&#22359;&#21270;&#30340;&#34920;&#26684;&#27169;&#22411;&#23454;&#29616;&#65292;&#24182;&#25104;&#21151;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PyTorch Frame&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;PyTorch Frame&#36890;&#36807;&#25552;&#20379;&#22522;&#20110;PyTorch&#30340;&#25968;&#25454;&#32467;&#26500;&#26469;&#22788;&#29702;&#22797;&#26434;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#24341;&#20837;&#27169;&#22411;&#25277;&#35937;&#20197;&#23454;&#29616;&#34920;&#26684;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#23454;&#29616;&#65292;&#24182;&#20801;&#35768;&#25972;&#21512;&#22806;&#37096;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#22797;&#26434;&#21015;&#65288;&#20363;&#22914;&#65292;&#29992;&#20110;&#25991;&#26412;&#21015;&#30340;LLMs&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#27169;&#22359;&#21270;&#26041;&#24335;&#23454;&#29616;&#22810;&#26679;&#30340;&#34920;&#26684;&#27169;&#22411;&#65292;&#25104;&#21151;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;PyTorch Geometric&#38598;&#25104;&#65292;PyTorch Geometric&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;PyTorch&#24211;&#65292;&#20197;&#23454;&#29616;&#23545;&#20851;&#31995;&#25968;&#25454;&#24211;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00776v1 Announce Type: new  Abstract: We present PyTorch Frame, a PyTorch-based framework for deep learning over multi-modal tabular data. PyTorch Frame makes tabular deep learning easy by providing a PyTorch-based data structure to handle complex tabular data, introducing a model abstraction to enable modular implementation of tabular models, and allowing external foundation models to be incorporated to handle complex columns (e.g., LLMs for text columns). We demonstrate the usefulness of PyTorch Frame by implementing diverse tabular models in a modular way, successfully applying these models to complex multi-modal tabular data, and integrating our framework with PyTorch Geometric, a PyTorch library for Graph Neural Networks (GNNs), to perform end-to-end learning over relational databases.
&lt;/p&gt;</description></item><item><title>SOAR&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#26032;&#25968;&#25454;&#32034;&#24341;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#22686;&#24378;&#27491;&#20132;&#27531;&#24046;&#25439;&#22833;&#26469;&#20248;&#21270;&#27599;&#20010;&#34920;&#31034;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#32034;&#24341;&#36136;&#37327;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00774</link><description>&lt;p&gt;
SOAR: &#25913;&#36827;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#32034;&#24341;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SOAR: Improved Indexing for Approximate Nearest Neighbor Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00774
&lt;/p&gt;
&lt;p&gt;
SOAR&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#26032;&#25968;&#25454;&#32034;&#24341;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#22686;&#24378;&#27491;&#20132;&#27531;&#24046;&#25439;&#22833;&#26469;&#20248;&#21270;&#27599;&#20010;&#34920;&#31034;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#32034;&#24341;&#36136;&#37327;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SOAR&#65306;&#22686;&#24378;&#27491;&#20132;&#27531;&#24046;&#30340;&#27844;&#28431;&#32034;&#24341;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#36817;&#20284;&#26368;&#36817;&#37051;&#65288;ANN&#65289;&#25628;&#32034;&#30340;&#26032;&#22411;&#25968;&#25454;&#32034;&#24341;&#25216;&#26415;&#12290;SOAR&#22312;&#20998;&#21306;&#25968;&#25454;&#26102;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;ANN&#25628;&#32034;&#26041;&#27861;&#65292;&#22914;&#27844;&#28431;&#26641;&#65292;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#22810;&#20010;&#20887;&#20313;&#34920;&#31034;&#26469;&#38477;&#20302;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#38169;&#36807;&#26368;&#36817;&#37051;&#30340;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#19982;&#29420;&#31435;&#35757;&#32451;&#21644;&#35745;&#31639;&#36825;&#20123;&#20887;&#20313;&#34920;&#31034;&#19981;&#21516;&#65292;SOAR&#20351;&#29992;&#20102;&#19968;&#31181;&#22686;&#24378;&#27491;&#20132;&#27531;&#24046;&#25439;&#22833;&#65292;&#20248;&#21270;&#27599;&#20010;&#34920;&#31034;&#20197;&#34917;&#20607;&#20854;&#20182;&#34920;&#31034;&#24615;&#33021;&#19981;&#20339;&#30340;&#24773;&#20917;&#12290;&#36825;&#22823;&#22823;&#25552;&#39640;&#20102;&#25972;&#20307;&#32034;&#24341;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;ANN&#22522;&#20934;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#24555;&#36895;&#32034;&#24341;&#26102;&#38388;&#21644;&#20302;&#20869;&#23384;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00774v1 Announce Type: new  Abstract: This paper introduces SOAR: Spilling with Orthogonality-Amplified Residuals, a novel data indexing technique for approximate nearest neighbor (ANN) search. SOAR extends upon previous approaches to ANN search, such as spill trees, that utilize multiple redundant representations while partitioning the data to reduce the probability of missing a nearest neighbor during search. Rather than training and computing these redundant representations independently, however, SOAR uses an orthogonality-amplified residual loss, which optimizes each representation to compensate for cases where other representations perform poorly. This drastically improves the overall index quality, resulting in state-of-the-art ANN benchmark performance while maintaining fast indexing times and low memory consumption.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21517;&#20026;Recover&#65292;&#29992;&#20110;&#22312;&#32447;&#25925;&#38556;&#35782;&#21035;&#21644;&#24674;&#22797;&#65292;&#36890;&#36807;&#38598;&#25104;&#31526;&#21495;&#20449;&#24687;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24674;&#22797;&#35745;&#21010;&#30340;&#33021;&#21147;&#65292;&#38477;&#20302;&#30456;&#20851;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.00756</link><description>&lt;p&gt;
Recover&#65306;&#29992;&#20110;&#25925;&#38556;&#26816;&#27979;&#21644;&#24674;&#22797;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21517;&#20026;Recover&#65292;&#29992;&#20110;&#22312;&#32447;&#25925;&#38556;&#35782;&#21035;&#21644;&#24674;&#22797;&#65292;&#36890;&#36807;&#38598;&#25104;&#31526;&#21495;&#20449;&#24687;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24674;&#22797;&#35745;&#21010;&#30340;&#33021;&#21147;&#65292;&#38477;&#20302;&#30456;&#20851;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;Recover&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#32447;&#25925;&#38556;&#35782;&#21035;&#21644;&#24674;&#22797;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#12290;Recover&#36890;&#36807;&#38598;&#25104;&#26412;&#20307;&#35770;&#12289;&#36923;&#36753;&#35268;&#21017;&#21644;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#22120;&#65292;&#21033;&#29992;&#31526;&#21495;&#20449;&#24687;&#22686;&#24378;LLM&#29983;&#25104;&#24674;&#22797;&#35745;&#21010;&#30340;&#33021;&#21147;&#65292;&#24182;&#38477;&#20302;&#30456;&#20851;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00756v1 Announce Type: new  Abstract: Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor's logical ru
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;C-XGBoost&#30340;&#26641;&#25552;&#21319;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#39044;&#27979;&#28508;&#22312;&#32467;&#26524;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#32487;&#25215;&#20102;XGBoost&#27169;&#22411;&#30340;&#39640;&#25928;&#22788;&#29702;&#32570;&#22833;&#20540;&#29305;&#24449;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2404.00751</link><description>&lt;p&gt;
C-XGBoost&#65306;&#19968;&#31181;&#29992;&#20110;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#26641;&#25552;&#21319;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
C-XGBoost: A tree boosting model for causal effect estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00751
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;C-XGBoost&#30340;&#26641;&#25552;&#21319;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#39044;&#27979;&#28508;&#22312;&#32467;&#26524;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#32487;&#25215;&#20102;XGBoost&#27169;&#22411;&#30340;&#39640;&#25928;&#22788;&#29702;&#32570;&#22833;&#20540;&#29305;&#24449;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26088;&#22312;&#20272;&#35745;&#22788;&#29702;&#23545;&#32467;&#26524;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20197;&#21450;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65292;&#36825;&#20123;&#30693;&#35782;&#22312;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#36890;&#24120;&#38656;&#35201;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25552;&#21462;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#65292;&#21517;&#20026;C-XGBoost&#65292;&#29992;&#20110;&#39044;&#27979;&#28508;&#22312;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21160;&#26426;&#22312;&#20110;&#21033;&#29992;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#30340;&#20248;&#36234;&#24615;&#65292;&#20197;&#21450;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#23398;&#20064;&#26377;&#29992;&#20110;&#20272;&#35745;&#22788;&#29702;&#21644;&#38750;&#22788;&#29702;&#26696;&#20363;&#30340;&#32467;&#26524;&#30340;&#34920;&#31034;&#30340;&#26174;&#33879;&#29305;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36824;&#32487;&#25215;&#20102;XGBoost&#27169;&#22411;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#22914;&#39640;&#25928;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#20540;&#29305;&#24449;&#30340;&#29305;&#24615;&#65292;&#38656;&#35201;&#26368;&#23569;&#30340;&#39044;&#22788;&#29702;&#24037;&#20316;&#65292;&#21516;&#26102;&#20855;&#22791;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00751v1 Announce Type: cross  Abstract: Causal effect estimation aims at estimating the Average Treatment Effect as well as the Conditional Average Treatment Effect of a treatment to an outcome from the available data. This knowledge is important in many safety-critical domains, where it often needs to be extracted from observational data. In this work, we propose a new causal inference model, named C-XGBoost, for the prediction of potential outcomes. The motivation of our approach is to exploit the superiority of tree-based models for handling tabular data together with the notable property of causal inference neural network-based models to learn representations that are useful for estimating the outcome for both the treatment and non-treatment cases. The proposed model also inherits the considerable advantages of XGBoost model such as efficiently handling features with missing values requiring minimum preprocessing effort, as well as it is equipped with regularization tech
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#29992;&#20110;&#32771;&#34385;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#30340;&#20998;&#24067;&#24335;&#21487;&#20877;&#29983;&#21457;&#30005;&#36755;&#20986;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#36890;&#36807;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#24314;&#27169;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#32467;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#36807;&#31243;&#65292;&#25104;&#21151;&#23637;&#31034;&#20102;&#22312;&#22788;&#29702;&#32570;&#22833;&#20540;&#24773;&#20917;&#19979;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00729</link><description>&lt;p&gt;
&#32771;&#34385;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#30340;&#20998;&#24067;&#24335;&#21457;&#30005;&#36755;&#20986;&#38750;&#21442;&#25968;&#31471;&#21040;&#31471;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Nonparametric End-to-End Probabilistic Forecasting of Distributed Generation Outputs Considering Missing Data Imputation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#29992;&#20110;&#32771;&#34385;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#30340;&#20998;&#24067;&#24335;&#21487;&#20877;&#29983;&#21457;&#30005;&#36755;&#20986;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#36890;&#36807;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#24314;&#27169;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#32467;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#36807;&#31243;&#65292;&#25104;&#21151;&#23637;&#31034;&#20102;&#22312;&#22788;&#29702;&#32570;&#22833;&#20540;&#24773;&#20917;&#19979;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#29992;&#20110;&#32771;&#34385;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#30340;&#20998;&#24067;&#24335;&#21487;&#20877;&#29983;&#21457;&#30005;&#36755;&#20986;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#26500;&#24314;&#38750;&#21442;&#25968;&#21270;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;&#65292;&#23545;&#20998;&#24067;&#24335;&#21487;&#20877;&#29983;&#21457;&#30005;&#36755;&#20986;&#30340;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#35757;&#32451;&#36807;&#31243;&#65292;&#36890;&#36807;&#36845;&#20195;&#25554;&#34917;&#21644;&#36845;&#20195;&#22522;&#20110;&#25439;&#22833;&#30340;&#35757;&#32451;&#36807;&#31243;&#26469;&#21253;&#25324;&#32570;&#22833;&#25968;&#25454;&#30340;&#25554;&#34917;&#12290;&#36825;&#31181;&#20004;&#27493;&#24314;&#27169;&#26041;&#27861;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#38750;&#21442;&#25968;&#26041;&#27861;&#21644;&#31471;&#21040;&#31471;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#20998;&#24067;&#24335;&#21487;&#20877;&#29983;&#21457;&#30005;&#36755;&#20986;&#30340;&#27010;&#29575;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#26377;&#25928;&#22788;&#29702;&#32570;&#22833;&#20540;&#12290;&#20223;&#30495;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00729v1 Announce Type: cross  Abstract: In this paper, we introduce a nonparametric end-to-end method for probabilistic forecasting of distributed renewable generation outputs while including missing data imputation. Firstly, we employ a nonparametric probabilistic forecast model utilizing the long short-term memory (LSTM) network to model the probability distributions of distributed renewable generations' outputs. Secondly, we design an end-to-end training process that includes missing data imputation through iterative imputation and iterative loss-based training procedures. This two-step modeling approach effectively combines the strengths of the nonparametric method with the end-to-end approach. Consequently, our approach demonstrates exceptional capabilities in probabilistic forecasting for the outputs of distributed renewable generations while effectively handling missing values. Simulation results confirm the superior performance of our approach compared to existing al
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MugenNet&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32467;&#32928;&#24687;&#32905;&#22270;&#20687;&#20998;&#21106;&#20013;CNN&#35757;&#32451;&#26102;&#38388;&#38271;&#21644;Transformer&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2404.00726</link><description>&lt;p&gt;
MugenNet&#65306;&#19968;&#31181;&#26032;&#22411;&#32467;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#32593;&#32476;&#30340;&#26041;&#27861;&#21450;&#20854;&#22312;&#32467;&#32928;&#24687;&#32905;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MugenNet: A Novel Combined Convolution Neural Network and Transformer Network with its Application for Colonic Polyp Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00726
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MugenNet&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32467;&#32928;&#24687;&#32905;&#22270;&#20687;&#20998;&#21106;&#20013;CNN&#35757;&#32451;&#26102;&#38388;&#38271;&#21644;Transformer&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26159;&#30142;&#30149;&#35786;&#26029;&#20013;&#38750;&#24120;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#12290;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#26089;&#26399;&#26816;&#27979;&#24687;&#32905;&#26159;&#36890;&#36807;&#32467;&#32928;&#38236;&#26816;&#26597;&#21644;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#26469;&#36827;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#32467;&#32928;&#38236;&#26816;&#26597;&#20013;&#20934;&#30830;&#30340;&#24687;&#32905;&#22270;&#20687;&#20998;&#21106;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#25253;&#21578;&#30340;&#30740;&#31350;&#22522;&#20110;&#33879;&#21517;&#30340;&#28151;&#21512;&#21407;&#21017;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;CNN&#35757;&#32451;&#26102;&#38388;&#38271;&#21644;Transformer&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00726v1 Announce Type: cross  Abstract: Biomedical image segmentation is a very important part in disease diagnosis. The term "colonic polyps" refers to polypoid lesions that occur on the surface of the colonic mucosa within the intestinal lumen. In clinical practice, early detection of polyps is conducted through colonoscopy examinations and biomedical image processing. Therefore, the accurate polyp image segmentation is of great significance in colonoscopy examinations. Convolutional Neural Network (CNN) is a common automatic segmentation method, but its main disadvantage is the long training time. Transformer utilizes a self-attention mechanism, which essentially assigns different importance weights to each piece of information, thus achieving high computational efficiency during segmentation. However, a potential drawback is the risk of information loss. In the study reported in this paper, based on the well-known hybridization principle, we proposed a method to combine 
&lt;/p&gt;</description></item><item><title>&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#21516;&#39044;&#31639;&#19979;&#20135;&#29983;&#21487;&#38752;&#30340;&#25913;&#36827;&#65292;&#20294;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#36873;&#25321;&#25490;&#21517;&#27425;&#20110;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2404.00725</link><description>&lt;p&gt;
&#36234;&#22823;&#36234;&#22909;&#21527;&#65311;&#36890;&#36807;&#39044;&#31639;&#37325;&#26032;&#20998;&#37197;&#25913;&#36827;LLM&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
The Larger the Better? Improved LLM Code-Generation via Budget Reallocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00725
&lt;/p&gt;
&lt;p&gt;
&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#21516;&#39044;&#31639;&#19979;&#20135;&#29983;&#21487;&#38752;&#30340;&#25913;&#36827;&#65292;&#20294;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#36873;&#25321;&#25490;&#21517;&#27425;&#20110;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27604;&#36739;&#23567;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20063;&#38656;&#35201;&#26356;&#22810;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#23601;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#20004;&#20010;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;&#39044;&#31639;&#19979;&#36816;&#34892;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#65288;&#20363;&#22914;&#65292;&#35745;&#31639;&#36164;&#28304;&#65292;&#36816;&#34892;&#26102;&#38388;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21508;&#31181;&#22823;&#23567;&#30340;&#20195;&#30721;&#29983;&#25104;LLMs&#65292;&#24182;&#36827;&#34892;&#27604;&#36739;&#65292;&#20363;&#22914;&#36816;&#34892;&#19968;&#20010;70B&#27169;&#22411;&#19968;&#27425;&#19982;&#20174;13B&#27169;&#22411;&#29983;&#25104;&#20116;&#20010;&#36755;&#20986;&#24182;&#36873;&#25321;&#19968;&#20010;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26631;&#20934;&#21333;&#20803;&#27979;&#35797;&#35774;&#32622;&#20013;&#65292;&#21453;&#22797;&#20351;&#29992;&#36739;&#23567;&#30340;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#22312;&#20116;&#20010;&#20219;&#21153;&#20013;&#26368;&#39640;&#21487;&#36798;15%&#30340;&#22686;&#30410;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#26080;&#27861;&#36827;&#34892;&#21333;&#20803;&#27979;&#35797;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#36739;&#23567;&#27169;&#22411;&#20013;&#22522;&#20110;&#25490;&#21517;&#30340;&#20505;&#36873;&#36873;&#25321;&#34920;&#29616;&#19981;&#21450;&#26469;&#33258;&#36739;&#22823;&#27169;&#22411;&#30340;&#21333;&#20010;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#20351;&#29992;&#36739;&#23567;&#27169;&#22411;&#32780;&#38750;&#36739;&#22823;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00725v1 Announce Type: cross  Abstract: It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model and selecting one. Our findings reveal that, in a standard unit-test setup, the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#32508;&#36848;&#20102;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#65292;&#37325;&#28857;&#35299;&#26512;&#20854;&#27979;&#35797;&#38382;&#39064;&#36873;&#25321;&#31639;&#27861;&#21644;&#22914;&#20309;&#20248;&#21270;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#12289;&#39064;&#24211;&#26500;&#24314;&#21644;&#27979;&#35797;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.00712</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#32508;&#36848;&#65306;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Survey of Computerized Adaptive Testing: A Machine Learning Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#32508;&#36848;&#20102;&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#65292;&#37325;&#28857;&#35299;&#26512;&#20854;&#27979;&#35797;&#38382;&#39064;&#36873;&#25321;&#31639;&#27861;&#21644;&#22914;&#20309;&#20248;&#21270;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#12289;&#39064;&#24211;&#26500;&#24314;&#21644;&#27979;&#35797;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#33258;&#36866;&#24212;&#27979;&#35797;&#65288;CAT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#37327;&#36523;&#23450;&#21046;&#30340;&#35780;&#20272;&#32771;&#29983;&#29087;&#32451;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#20182;&#20204;&#30340;&#34920;&#29616;&#21160;&#24577;&#35843;&#25972;&#27979;&#35797;&#38382;&#39064;&#12290;CAT&#24191;&#27867;&#24212;&#29992;&#20110;&#25945;&#32946;&#12289;&#21307;&#30103;&#12289;&#20307;&#32946;&#21644;&#31038;&#20250;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#27979;&#35797;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#35268;&#27169;&#27979;&#35797;&#30340;&#22686;&#21152;&#22797;&#26434;&#24615;&#65292;CAT&#24050;&#32463;&#34701;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#20197;&#26426;&#22120;&#23398;&#20064;&#20026;&#37325;&#28857;&#30340;CAT&#32508;&#36848;&#65292;&#20174;&#26032;&#30340;&#35282;&#24230;&#35299;&#35835;&#36825;&#31181;&#33258;&#36866;&#24212;&#27979;&#35797;&#26041;&#27861;&#12290;&#36890;&#36807;&#30740;&#31350;CAT&#36866;&#24212;&#24615;&#26680;&#24515;&#30340;&#27979;&#35797;&#38382;&#39064;&#36873;&#25321;&#31639;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20854;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#12289;&#39064;&#24211;&#26500;&#24314;&#21644;CAT&#20013;&#30340;&#27979;&#35797;&#25511;&#21046;&#65292;&#25506;&#32034;&#20102;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#20248;&#21270;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#36890;&#36807;&#23545;&#24403;&#21069;&#24773;&#20917;&#30340;&#20998;&#26512;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00712v1 Announce Type: cross  Abstract: Computerized Adaptive Testing (CAT) provides an efficient and tailored method for assessing the proficiency of examinees, by dynamically adjusting test questions based on their performance. Widely adopted across diverse fields like education, healthcare, sports, and sociology, CAT has revolutionized testing practices. While traditional methods rely on psychometrics and statistics, the increasing complexity of large-scale testing has spurred the integration of machine learning techniques. This paper aims to provide a machine learning-focused survey on CAT, presenting a fresh perspective on this adaptive testing method. By examining the test question selection algorithm at the heart of CAT's adaptivity, we shed light on its functionality. Furthermore, we delve into cognitive diagnosis models, question bank construction, and test control within CAT, exploring how machine learning can optimize these components. Through an analysis of curre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#34920;&#26684;&#21512;&#25104;&#25968;&#25454;&#26102;&#21487;&#33021;&#20135;&#29983;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#20998;&#26512;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20877;&#35782;&#21035;&#25915;&#20987;&#30340;&#24433;&#21709;&#20197;&#21450;&#20449;&#24687;&#23545;&#21457;&#21160;&#26356;&#25104;&#21151;&#30340;&#20877;&#35782;&#21035;&#25915;&#20987;&#30340;&#28508;&#22312;&#29992;&#22788;&#12290;</title><link>https://arxiv.org/abs/2404.00696</link><description>&lt;p&gt;
&#38024;&#23545;&#34920;&#26684;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#38544;&#31169;&#20877;&#35782;&#21035;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Privacy Re-identification Attacks on Tabular GANs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#34920;&#26684;&#21512;&#25104;&#25968;&#25454;&#26102;&#21487;&#33021;&#20135;&#29983;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#20998;&#26512;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20877;&#35782;&#21035;&#25915;&#20987;&#30340;&#24433;&#21709;&#20197;&#21450;&#20449;&#24687;&#23545;&#21457;&#21160;&#26356;&#25104;&#21151;&#30340;&#20877;&#35782;&#21035;&#25915;&#20987;&#30340;&#28508;&#22312;&#29992;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#65292;&#22240;&#27492;&#21487;&#33021;&#28508;&#22312;&#22320;&#27844;&#38706;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#34920;&#26684;&#21512;&#25104;&#25968;&#25454;&#26102;&#21487;&#33021;&#20135;&#29983;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20877;&#35782;&#21035;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#25915;&#20987;&#26088;&#22312;&#36873;&#25321;&#34987;&#39044;&#27979;&#19982;&#22522;&#20110;&#26368;&#25509;&#36817;&#21512;&#25104;&#35760;&#24405;&#30340;&#35760;&#24518;&#35757;&#32451;&#26679;&#26412;&#23545;&#24212;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#31181;&#19981;&#21516;&#25915;&#20987;&#32773;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#35775;&#38382;&#32423;&#21035;&#25110;&#23545;&#29983;&#25104;&#27169;&#22411;&#21644;&#39044;&#27979;&#30340;&#30693;&#35782;&#65292;&#24182;&#35780;&#20272;&#21738;&#20123;&#20449;&#24687;&#21487;&#33021;&#26368;&#26377;&#21161;&#20110;&#21457;&#21160;&#26356;&#25104;&#21151;&#30340;&#20877;&#35782;&#21035;&#25915;&#20987;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#23558;&#20877;&#35782;&#21035;&#25915;&#20987;&#21046;&#23450;&#20026;&#37325;&#24314;&#25915;&#20987;&#30340;&#24773;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00696v1 Announce Type: cross  Abstract: Generative models are subject to overfitting and thus may potentially leak sensitive information from the training data. In this work. we investigate the privacy risks that can potentially arise from the use of generative adversarial networks (GANs) for creating tabular synthetic datasets. For the purpose, we analyse the effects of re-identification attacks on synthetic data, i.e., attacks which aim at selecting samples that are predicted to correspond to memorised training samples based on their proximity to the nearest synthetic records. We thus consider multiple settings where different attackers might have different access levels or knowledge of the generative model and predictive, and assess which information is potentially most useful for launching more successful re-identification attacks. In doing so we also consider the situation for which re-identification attacks are formulated as reconstruction attacks, i.e., the situation 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#20302;&#32500;&#20223;&#23556;&#23376;&#31354;&#38388;&#38598;&#20013;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20004;&#31181;&#31574;&#30053;&#35299;&#20915;&#20102;&#22312;&#22810;&#20010;&#29615;&#22659;&#38543;&#26426;&#33218;&#19978;&#20219;&#21153;&#20013;&#20943;&#23569;&#39044;&#26399;&#36951;&#25022;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00688</link><description>&lt;p&gt;
&#20849;&#20139;&#20223;&#23556;&#23376;&#31354;&#38388;&#20013;&#30340;&#33218;&#19978;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta Learning in Bandits within Shared Affine Subspaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00688
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20302;&#32500;&#20223;&#23556;&#23376;&#31354;&#38388;&#38598;&#20013;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20004;&#31181;&#31574;&#30053;&#35299;&#20915;&#20102;&#22312;&#22810;&#20010;&#29615;&#22659;&#38543;&#26426;&#33218;&#19978;&#20219;&#21153;&#20013;&#20943;&#23569;&#39044;&#26399;&#36951;&#25022;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#29615;&#22659;&#38543;&#26426;&#33218;&#19978;&#20219;&#21153;&#22312;&#20302;&#32500;&#20223;&#23556;&#23376;&#31354;&#38388;&#21608;&#22260;&#30340;&#38598;&#20013;&#24615;&#65292;&#36890;&#36807;&#22312;&#32447;&#20027;&#25104;&#20998;&#20998;&#26512;&#26469;&#20943;&#23569;&#22312;&#36935;&#21040;&#30340;&#33218;&#19978;&#20219;&#21153;&#20013;&#30340;&#39044;&#26399;&#36951;&#25022;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#29702;&#35770;&#20998;&#26512;&#20102;&#20004;&#31181;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#31574;&#30053;&#65306;&#19968;&#31181;&#22522;&#20110;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#20048;&#35266;&#21407;&#21017;&#65292;&#21478;&#19968;&#31181;&#36890;&#36807;&#27748;&#26222;&#26862;&#21462;&#26679;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#21253;&#25324;&#20808;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#20316;&#20026;&#29305;&#20363;&#12290;&#27492;&#22806;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#20960;&#20010;&#33218;&#19978;&#20219;&#21153;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00688v1 Announce Type: new  Abstract: We study the problem of meta-learning several contextual stochastic bandits tasks by leveraging their concentration around a low-dimensional affine subspace, which we learn via online principal component analysis to reduce the expected regret over the encountered bandits. We propose and theoretically analyze two strategies that solve the problem: One based on the principle of optimism in the face of uncertainty and the other via Thompson sampling. Our framework is generic and includes previously proposed approaches as special cases. Besides, the empirical results show that our methods significantly reduce the regret on several bandit tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;Q&#23398;&#20064;&#65288;MMD-QL&#65289;&#26469;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#20215;&#20540;&#20989;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#20256;&#25773;&#65292;&#36890;&#36807;&#20351;&#29992;MMD&#37325;&#24515;&#65292;&#23454;&#29616;&#20102;&#27604;Wasserstein&#36317;&#31163;&#26356;&#32039;&#30340;&#27010;&#29575;&#24230;&#37327;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#24182;&#32467;&#21512;&#28145;&#24230;&#32593;&#32476;&#21019;&#36896;&#20102;MMD Q&#32593;&#32476;&#65288;MMD-QN&#65289;&#12290;</title><link>https://arxiv.org/abs/2404.00686</link><description>&lt;p&gt;
&#21033;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#37325;&#24515;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20256;&#25773;&#20215;&#20540;&#20989;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Utilizing Maximum Mean Discrepancy Barycenter for Propagating the Uncertainty of Value Functions in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00686
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;Q&#23398;&#20064;&#65288;MMD-QL&#65289;&#26469;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#20215;&#20540;&#20989;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#20256;&#25773;&#65292;&#36890;&#36807;&#20351;&#29992;MMD&#37325;&#24515;&#65292;&#23454;&#29616;&#20102;&#27604;Wasserstein&#36317;&#31163;&#26356;&#32039;&#30340;&#27010;&#29575;&#24230;&#37327;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#24182;&#32467;&#21512;&#28145;&#24230;&#32593;&#32476;&#21019;&#36896;&#20102;MMD Q&#32593;&#32476;&#65288;MMD-QN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#20215;&#20540;&#20989;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#20419;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;Q&#23398;&#20064;&#65288;MMD-QL&#65289;&#65292;&#20197;&#25913;&#36827;Wasserstein Q&#23398;&#20064;&#65288;WQL&#65289;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#24046;&#20998;&#65288;TD&#65289;&#26356;&#26032;&#26399;&#38388;&#20256;&#25773;&#19981;&#30830;&#23450;&#24615;&#12290;MMD-QL&#20351;&#29992;MMD&#37325;&#24515;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#22240;&#20026;MMD&#25552;&#20379;&#20102;&#27604;Wasserstein&#36317;&#31163;&#26356;&#32039;&#30340;&#27010;&#29575;&#24230;&#37327;&#20043;&#38388;&#30340;&#25509;&#36817;&#24230;&#20272;&#35745;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24179;&#22343;&#25439;&#22833;&#24230;&#37327;&#19979;&#65292;MMD-QL&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#26159;&#8220;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#8221;&#30340;&#12290;&#22312;&#32771;&#34385;&#21040;&#32047;&#31215;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#34920;&#26684;&#29615;&#22659;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MMD-QL&#20248;&#20110;WQL&#21644;&#20854;&#20182;&#31639;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#32593;&#32476;&#32435;&#20837;MMD-QL&#20013;&#65292;&#21019;&#24314;MMD Q&#32593;&#32476;&#65288;MMD-QN&#65289;&#12290;&#36890;&#36807;&#21512;&#29702;&#20551;&#35774;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;MMD-QN&#22312;&#20989;&#25968;&#36924;&#36817;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;Atari&#28216;&#25103;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;MMD-QN&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00686v1 Announce Type: new  Abstract: Accounting for the uncertainty of value functions boosts exploration in Reinforcement Learning (RL). Our work introduces Maximum Mean Discrepancy Q-Learning (MMD-QL) to improve Wasserstein Q-Learning (WQL) for uncertainty propagation during Temporal Difference (TD) updates. MMD-QL uses the MMD barycenter for this purpose, as MMD provides a tighter estimate of closeness between probability measures than the Wasserstein distance. Firstly, we establish that MMD-QL is Probably Approximately Correct in MDP (PAC-MDP) under the average loss metric. Concerning the accumulated rewards, experiments on tabular environments show that MMD-QL outperforms WQL and other algorithms. Secondly, we incorporate deep networks into MMD-QL to create MMD Q-Network (MMD-QN). Making reasonable assumptions, we analyze the convergence rates of MMD-QN using function approximation. Empirical results on challenging Atari games demonstrate that MMD-QN performs well comp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#35843;&#26597;&#27169;&#22411;&#35299;&#37322;&#20013;&#38544;&#31169;&#25915;&#20987;&#21450;&#20854;&#23545;&#25239;&#25514;&#26045;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#20998;&#31867;&#38544;&#31169;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;&#65292;&#21021;&#27493;&#25506;&#35752;&#20102;&#38544;&#31169;&#27844;&#28431;&#21407;&#22240;&#65292;&#25552;&#20986;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2404.00673</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#22411;&#27169;&#22411;&#35299;&#37322;&#30740;&#31350;&#32508;&#36848;&#65306;&#38544;&#31169;&#39118;&#38505;&#12289;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
A Survey of Privacy-Preserving Model Explanations: Privacy Risks, Attacks, and Countermeasures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#35843;&#26597;&#27169;&#22411;&#35299;&#37322;&#20013;&#38544;&#31169;&#25915;&#20987;&#21450;&#20854;&#23545;&#25239;&#25514;&#26045;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#20998;&#31867;&#38544;&#31169;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;&#65292;&#21021;&#27493;&#25506;&#35752;&#20102;&#38544;&#31169;&#27844;&#28431;&#21407;&#22240;&#65292;&#25552;&#20986;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#37319;&#29992;&#19981;&#26029;&#25193;&#22823;&#65292;&#35299;&#20915;&#20854;&#38544;&#31169;&#24433;&#21709;&#30340;&#32039;&#36843;&#24615;&#21464;&#24471;&#26356;&#21152;&#36843;&#20999;&#12290;&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#38544;&#31169;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#38544;&#31169;&#20445;&#25252;&#22411;&#27169;&#22411;&#35299;&#37322;&#21364;&#40092;&#26377;&#20851;&#27880;&#12290;&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#27169;&#22411;&#35299;&#37322;&#30340;&#38544;&#31169;&#25915;&#20987;&#21450;&#20854;&#23545;&#25239;&#25514;&#26045;&#12290;&#25105;&#20204;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36129;&#29486;&#21253;&#25324;&#23545;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#20998;&#31867;&#27861;&#65292;&#20415;&#20110;&#26681;&#25454;&#30446;&#26631;&#35299;&#37322;&#23545;&#38544;&#31169;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#30740;&#31350;&#36824;&#23545;&#38544;&#31169;&#27844;&#28431;&#21407;&#22240;&#36827;&#34892;&#20102;&#21021;&#27493;&#35843;&#26597;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#20998;&#26512;&#20013;&#21457;&#29616;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#35813;&#35843;&#26597;&#26088;&#22312;&#25104;&#20026;&#30740;&#31350;&#30028;&#30340;&#23453;&#36149;&#36164;&#28304;&#65292;&#24182;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#26032;&#25163;&#25552;&#20379;&#26126;&#30830;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00673v1 Announce Type: cross  Abstract: As the adoption of explainable AI (XAI) continues to expand, the urgency to address its privacy implications intensifies. Despite a growing corpus of research in AI privacy and explainability, there is little attention on privacy-preserving model explanations. This article presents the first thorough survey about privacy attacks on model explanations and their countermeasures. Our contribution to this field comprises a thorough analysis of research papers with a connected taxonomy that facilitates the categorisation of privacy attacks and countermeasures based on the targeted explanations. This work also includes an initial investigation into the causes of privacy leaks. Finally, we discuss unresolved issues and prospective research directions uncovered in our analysis. This survey aims to be a valuable resource for the research community and offers clear insights for those new to this domain. To support ongoing research, we have estab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToE&#30340;&#20196;&#29260;&#29983;&#38271;&#26041;&#26696;&#65292;&#26088;&#22312;&#36890;&#36807;&#21152;&#36895;&#19968;&#33268;&#24615;&#35757;&#32451;&#26469;&#25913;&#21892;ViT&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00672</link><description>&lt;p&gt;
&#36890;&#36807;&#20196;&#29260;&#25193;&#23637;&#23454;&#29616;Transformer&#30340;&#19968;&#33324;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
A General and Efficient Training for Transformer via Token Expansion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToE&#30340;&#20196;&#29260;&#29983;&#38271;&#26041;&#26696;&#65292;&#26088;&#22312;&#36890;&#36807;&#21152;&#36895;&#19968;&#33268;&#24615;&#35757;&#32451;&#26469;&#25913;&#21892;ViT&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformer&#65288;ViT&#65289;&#36890;&#24120;&#38656;&#35201;&#26497;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#25165;&#33021;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20196;&#29260;&#29983;&#38271;&#26041;&#26696;Token Expansion (ToE)&#65292;&#20197;&#23454;&#29616;ViT&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#21152;&#36895;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#8220;&#21021;&#22987;&#21270;-&#25193;&#23637;-&#21512;&#24182;&#8221;&#31649;&#36947;&#65292;&#20197;&#20445;&#25345;&#21407;&#22987;Transformer&#30340;&#20013;&#38388;&#29305;&#24449;&#20998;&#24067;&#30340;&#23436;&#25972;&#24615;&#65292;&#38450;&#27490;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20002;&#22833;&#20851;&#38190;&#30340;&#21487;&#23398;&#20064;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00672v1 Announce Type: cross  Abstract: The remarkable performance of Vision Transformers (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original transformers, including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different Transformer networks. In this paper, we propose a novel token growth scheme Token Expansion (termed ToE) to achieve consistent training acceleration for ViTs. We introduce an "initialization-expansion-merging" pipeline to maintain the integrity of the intermediate feature distribution of original transformers, preventing the loss of crucial learnable information in the training process. ToE can not only be seamlessly integrated into the training and fine-tuning process of transformers (e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#26080;&#21442;&#25968;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20808;&#39564;&#20102;&#35299;&#38382;&#39064;&#21442;&#25968;&#65292;&#22312;&#24179;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#24182;&#34920;&#29616;&#24378;&#22823;&#12290;</title><link>https://arxiv.org/abs/2404.00666</link><description>&lt;p&gt;
&#21152;&#36895;&#30340;&#26080;&#21442;&#25968;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accelerated Parameter-Free Stochastic Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00666
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#26080;&#21442;&#25968;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20808;&#39564;&#20102;&#35299;&#38382;&#39064;&#21442;&#25968;&#65292;&#22312;&#24179;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#24182;&#34920;&#29616;&#24378;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#24179;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#30340;&#36817;&#20046;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#19988;&#22522;&#26412;&#19978;&#19981;&#38656;&#35201;&#20808;&#39564;&#20102;&#35299;&#38382;&#39064;&#21442;&#25968;&#12290;&#36825;&#25913;&#36827;&#20102;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#38656;&#35201;&#33267;&#23569;&#30693;&#36947;&#21040;&#26368;&#20248;&#35299;&#30340;&#21021;&#22987;&#36317;&#31163; d0&#12290;&#25105;&#20204;&#30340;&#26041;&#27861; U-DoG &#23558; UniXGrad (Kavis &#31561;&#20154;&#65292;2019) &#21644; DoG (Ivgi &#31561;&#20154;&#65292;2023) &#19982;&#26032;&#39062;&#30340;&#36845;&#20195;&#31283;&#23450;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#23427;&#20165;&#38656;&#35201;&#23545; d0 &#21644;&#22122;&#22768;&#24133;&#24230;&#26377;&#26494;&#25955;&#30340;&#30028;&#38480;&#65292;&#22312;&#27425;&#39640;&#26031;&#22122;&#22768;&#19979;&#25552;&#20379;&#39640;&#27010;&#29575;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#38750;&#24179;&#28369;&#24773;&#20917;&#19979;&#20063;&#25509;&#36817;&#26368;&#20339;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20984;&#38382;&#39064;&#19978;&#33021;&#22815;&#31283;&#23450;&#22320;&#34920;&#29616;&#24378;&#22823;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#19978;&#26377;&#30528;&#19981;&#21516;&#30340;&#25104;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00666v1 Announce Type: new  Abstract: We propose a method that achieves near-optimal rates for smooth stochastic convex optimization and requires essentially no prior knowledge of problem parameters. This improves on prior work which requires knowing at least the initial distance to optimality d0. Our method, U-DoG, combines UniXGrad (Kavis et al., 2019) and DoG (Ivgi et al., 2023) with novel iterate stabilization techniques. It requires only loose bounds on d0 and the noise magnitude, provides high probability guarantees under sub-Gaussian noise, and is also near-optimal in the non-smooth case. Our experiments show consistent, strong performance on convex problems and mixed results on neural network training.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22238;&#39038;&#20102;&#23545;&#25216;&#26415;&#25991;&#26723;RAG&#31995;&#32479;&#26500;&#24314;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26368;&#20339;&#23454;&#36341;&#21644;&#28508;&#22312;&#25361;&#25112;</title><link>https://arxiv.org/abs/2404.00657</link><description>&lt;p&gt;
&#20851;&#20110;&#20026;&#25216;&#26415;&#25991;&#26723;&#26500;&#24314;RAG&#31995;&#32479;&#30340;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Observations on Building RAG Systems for Technical Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00657
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22238;&#39038;&#20102;&#23545;&#25216;&#26415;&#25991;&#26723;RAG&#31995;&#32479;&#26500;&#24314;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26368;&#20339;&#23454;&#36341;&#21644;&#28508;&#22312;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#29992;&#20110;&#25216;&#26415;&#25991;&#26723;&#26102;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23884;&#20837;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#39046;&#22495;&#20449;&#24687;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#24433;&#21709;RAG&#30340;&#37325;&#35201;&#22240;&#32032;&#30340;&#20808;&#21069;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#20197;&#31361;&#20986;&#26500;&#24314;&#25216;&#26415;&#25991;&#26723;RAG&#31995;&#32479;&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#28508;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00657v1 Announce Type: cross  Abstract: Retrieval augmented generation (RAG) for technical documents creates challenges as embeddings do not often capture domain information. We review prior art for important factors affecting RAG and perform experiments to highlight best practices and potential challenges to build RAG systems for technical documents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#39044;&#27979;&#27169;&#22411;&#21644;&#31163;&#32447;&#23398;&#20064;&#20803;&#32032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20869;&#22312;&#22870;&#21169;&#19982;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#32852;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26679;&#26412;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2404.00651</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#20869;&#22312;&#21160;&#26426;&#23398;&#20064;&#31163;&#32447;&#31574;&#30053;&#20197;&#23454;&#29616;&#20027;&#21160;&#22312;&#32447;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#39044;&#27979;&#27169;&#22411;&#21644;&#31163;&#32447;&#23398;&#20064;&#20803;&#32032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20869;&#22312;&#22870;&#21169;&#19982;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#32852;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26679;&#26412;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#21644;&#26080;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#26679;&#26412;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#39044;&#27979;&#27169;&#22411;&#21644;&#31163;&#32447;&#23398;&#20064;&#20803;&#32032;&#65292;&#20854;&#20013;&#22312;&#32447;&#35268;&#21010;&#22120;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#24863;&#30693;&#30340;&#32456;&#31471;&#20215;&#20540;&#20989;&#25968;&#29992;&#20110;&#26679;&#26412;&#25910;&#38598;&#12290;&#21033;&#29992;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#21069;&#21521;&#39044;&#27979;&#38169;&#35823;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#20869;&#22312;&#22870;&#21169;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#21442;&#25968;&#24320;&#38144;&#12290;&#35813;&#22870;&#21169;&#24314;&#31435;&#20102;&#19982;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#29282;&#22266;&#32852;&#31995;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26377;&#25928;&#22320;&#20811;&#26381;&#28176;&#36817;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00651v1 Announce Type: cross  Abstract: Recent advancements in deep reinforcement learning (RL) have demonstrated notable progress in sample efficiency, spanning both model-based and model-free paradigms. Despite the identification and mitigation of specific bottlenecks in prior works, the agent's exploration ability remains under-emphasized in the realm of sample-efficient RL. This paper investigates how to achieve sample-efficient exploration in continuous control tasks. We introduce an RL algorithm that incorporates a predictive model and off-policy learning elements, where an online planner enhanced by a novelty-aware terminal value function is employed for sample collection. Leveraging the forward predictive error within a latent state space, we derive an intrinsic reward without incurring parameters overhead. This reward establishes a solid connection to model uncertainty, allowing the agent to effectively overcome the asymptotic performance gap. Through extensive expe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#65292;&#24110;&#21161;&#32456;&#31471;&#29992;&#25143;&#36890;&#36807;&#26085;&#24535;&#23450;&#20301;&#37197;&#32622;&#38169;&#35823;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#24037;&#20855;LogConfigLocalizer&#65292;&#20197;&#24110;&#21161;&#32456;&#31471;&#29992;&#25143;&#36890;&#36807;&#26085;&#24535;&#20998;&#26512;&#35299;&#20915;&#37197;&#32622;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2404.00640</link><description>&lt;p&gt;
&#12298;&#38754;&#23545;&#23427;&#20204;&#33258;&#24049;&#65306;&#22522;&#20110;LLM&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#36890;&#36807;&#26085;&#24535;&#23450;&#20301;&#37197;&#32622;&#38169;&#35823;&#12299;
&lt;/p&gt;
&lt;p&gt;
Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00640
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#65292;&#24110;&#21161;&#32456;&#31471;&#29992;&#25143;&#36890;&#36807;&#26085;&#24535;&#23450;&#20301;&#37197;&#32622;&#38169;&#35823;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#24037;&#20855;LogConfigLocalizer&#65292;&#20197;&#24110;&#21161;&#32456;&#31471;&#29992;&#25143;&#36890;&#36807;&#26085;&#24535;&#20998;&#26512;&#35299;&#20915;&#37197;&#32622;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#23481;&#26131;&#20986;&#29616;&#37197;&#32622;&#38169;&#35823;&#65292;&#32473;&#20844;&#21496;&#24102;&#26469;&#37325;&#22823;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#37197;&#32622;&#31354;&#38388;&#65292;&#35786;&#26029;&#36825;&#20123;&#38169;&#35823;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#20123;&#38169;&#35823;&#23545;&#26377;&#32463;&#39564;&#30340;&#32500;&#25252;&#32773;&#21644;&#27809;&#26377;&#36719;&#20214;&#31995;&#32479;&#28304;&#20195;&#30721;&#35775;&#38382;&#26435;&#38480;&#30340;&#26032;&#32456;&#31471;&#29992;&#25143;&#37117;&#26500;&#25104;&#37325;&#22823;&#25361;&#25112;&#12290;&#37492;&#20110;&#26085;&#24535;&#23545;&#22823;&#22810;&#25968;&#32456;&#31471;&#29992;&#25143;&#26469;&#35828;&#26131;&#20110;&#35775;&#38382;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#27010;&#36848;&#20102;&#21033;&#29992;&#26085;&#24535;&#23450;&#20301;&#37197;&#32622;&#38169;&#35823;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26681;&#25454;&#21021;&#27493;&#30740;&#31350;&#25152;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#65292;&#24110;&#21161;&#32456;&#31471;&#29992;&#25143;&#36890;&#36807;&#26085;&#24535;&#23450;&#20301;&#37197;&#32622;&#38169;&#35823;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#19968;&#20010;&#24037;&#20855;LogConfigLocalizer&#65292;&#31526;&#21512;&#19978;&#36848;&#31574;&#30053;&#30340;&#35774;&#35745;&#65292;&#24076;&#26395;&#36890;&#36807;&#26085;&#24535;&#20998;&#26512;&#24110;&#21161;&#32456;&#31471;&#29992;&#25143;&#24212;&#23545;&#37197;&#32622;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00640v1 Announce Type: cross  Abstract: Configurable software systems are prone to configuration errors, resulting in significant losses to companies. However, diagnosing these errors is challenging due to the vast and complex configuration space. These errors pose significant challenges for both experienced maintainers and new end-users, particularly those without access to the source code of the software systems. Given that logs are easily accessible to most end-users, we conduct a preliminary study to outline the challenges and opportunities of utilizing logs in localizing configuration errors. Based on the insights gained from the preliminary study, we propose an LLM-based two-stage strategy for end-users to localize the root-cause configuration properties based on logs. We further implement a tool, LogConfigLocalizer, aligned with the design of the aforementioned strategy, hoping to assist end-users in coping with configuration errors through log analysis.   To the best
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20056;&#27861;&#22120;&#35774;&#35745;&#20248;&#21270;&#26694;&#26550;RL-MUL&#65292;&#21033;&#29992;&#30697;&#38453;&#21644;&#24352;&#37327;&#34920;&#31034;&#20056;&#27861;&#22120;&#30340;&#21387;&#32553;&#26641;&#65292;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22870;&#21169;&#23454;&#29616;&#21306;&#22495;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21516;&#26102;&#25193;&#23637;&#21040;&#20248;&#21270;&#34701;&#21512;&#20056;-&#32047;&#21152;&#65288;MAC&#65289;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2404.00639</link><description>&lt;p&gt;
RL-MUL&#65306;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20056;&#27861;&#22120;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
RL-MUL: Multiplier Design Optimization with Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00639
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20056;&#27861;&#22120;&#35774;&#35745;&#20248;&#21270;&#26694;&#26550;RL-MUL&#65292;&#21033;&#29992;&#30697;&#38453;&#21644;&#24352;&#37327;&#34920;&#31034;&#20056;&#27861;&#22120;&#30340;&#21387;&#32553;&#26641;&#65292;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22870;&#21169;&#23454;&#29616;&#21306;&#22495;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21516;&#26102;&#25193;&#23637;&#21040;&#20248;&#21270;&#34701;&#21512;&#20056;-&#32047;&#21152;&#65288;MAC&#65289;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20056;&#27861;&#26159;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#20056;&#27861;&#22120;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#30005;&#36335;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#35745;&#31354;&#38388;&#24040;&#22823;&#65292;&#20248;&#21270;&#20056;&#27861;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#38750;&#24179;&#20961;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL-MUL&#65292;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20056;&#27861;&#22120;&#35774;&#35745;&#20248;&#21270;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#30697;&#38453;&#21644;&#24352;&#37327;&#34920;&#31034;&#20056;&#27861;&#22120;&#30340;&#21387;&#32553;&#26641;&#65292;&#22522;&#20110;&#36825;&#19968;&#34920;&#31034;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#20026;&#20195;&#29702;&#32593;&#32476;&#12290;&#20195;&#29702;&#21487;&#20197;&#23398;&#20064;&#26681;&#25454;&#23450;&#21046;&#21270;&#30340;&#21487;&#23481;&#24525;&#21306;&#22495;&#19982;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#26469;&#20248;&#21270;&#20056;&#27861;&#22120;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;RL-MUL&#30340;&#33021;&#21147;&#34987;&#25193;&#23637;&#21040;&#20248;&#21270;&#34701;&#21512;&#20056;-&#32047;&#21152;&#65288;MAC&#65289;&#35774;&#35745;&#12290;&#23454;&#39564;&#22312;&#19981;&#21516;&#20301;&#23485;&#30340;&#20056;&#27861;&#22120;&#19978;&#36827;&#34892;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;RL-MUL&#29983;&#25104;&#30340;&#20056;&#27861;&#22120;&#33021;&#22815;&#36229;&#36234;&#25152;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00639v1 Announce Type: cross  Abstract: Multiplication is a fundamental operation in many applications, and multipliers are widely adopted in various circuits. However, optimizing multipliers is challenging and non-trivial due to the huge design space. In this paper, we propose RL-MUL, a multiplier design optimization framework based on reinforcement learning. Specifically, we utilize matrix and tensor representations for the compressor tree of a multiplier, based on which the convolutional neural networks can be seamlessly incorporated as the agent network. The agent can learn to optimize the multiplier structure based on a Pareto-driven reward which is customized to accommodate the trade-off between area and delay. Additionally, the capability of RL-MUL is extended to optimize the fused multiply-accumulator (MAC) designs. Experiments are conducted on different bit widths of multipliers. The results demonstrate that the multipliers produced by RL-MUL can dominate all baseli
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#36229;&#36793;&#22635;&#20805;&#30340;&#20219;&#21153;&#26377;&#25928;&#25429;&#33719;&#22797;&#26434;&#30340;&#36229;&#22270;&#25299;&#25169;&#32467;&#26500;</title><link>https://arxiv.org/abs/2404.00638</link><description>&lt;p&gt;
HypeBoy: &#36229;&#22270;&#19978;&#30340;&#29983;&#25104;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HypeBoy: Generative Self-Supervised Representation Learning on Hypergraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00638
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#36229;&#36793;&#22635;&#20805;&#30340;&#20219;&#21153;&#26377;&#25928;&#25429;&#33719;&#22797;&#26434;&#30340;&#36229;&#22270;&#25299;&#25169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#20197;&#22797;&#26434;&#30340;&#25299;&#25169;&#32467;&#26500;&#20026;&#29305;&#24449;&#65292;&#34920;&#36798;&#20102;&#22810;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#39640;&#38454;&#20132;&#20114;&#65292;&#24182;&#19988;&#26356;&#22909;&#22320;&#25429;&#25417;&#25299;&#25169;&#32467;&#26500;&#23545;&#20110;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#22312;&#29983;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#20174;&#29983;&#25104;&#33258;&#30417;&#30563;&#20013;&#23398;&#20064;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#26377;&#28508;&#21147;&#26377;&#25928;&#22320;&#32534;&#30721;&#22797;&#26434;&#30340;&#36229;&#22270;&#25299;&#25169;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#20026;&#36229;&#22270;&#35774;&#35745;&#29983;&#25104;&#24335;SSL&#31574;&#30053;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20107;&#24773;&#12290;&#20851;&#20110;&#20854;&#29983;&#25104;&#24335;SSL&#20219;&#21153;&#12289;&#19982;&#19979;&#28216;&#20219;&#21153;&#30340;&#36830;&#25509;&#20197;&#21450;&#25152;&#23398;&#34920;&#31034;&#30340;&#23454;&#39564;&#24615;&#36136;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#12290;&#37492;&#20110;&#36825;&#20123;&#26399;&#26395;&#21644;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36229;&#22270;&#29983;&#25104;&#24335;SSL&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#36229;&#22270;&#19978;&#21046;&#23450;&#20102;&#19968;&#20010;&#29983;&#25104;&#24335;SSL&#20219;&#21153;&#65306;&#36229;&#36793;&#22635;&#20805;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#19982;&#33410;&#28857;&#20998;&#31867;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;&#22522;&#20110;&#36825;&#20010;&#29983;&#25104;&#24335;SSL&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36229;&#22270;&#30340;&#29983;&#25104;&#24335;SSL
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00638v1 Announce Type: new  Abstract: Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple nodes with hyperedges, and better capturing the topology is essential for effective representation learning. Recent advances in generative self-supervised learning (SSL) suggest that hypergraph neural networks learned from generative self supervision have the potential to effectively encode the complex hypergraph topology. Designing a generative SSL strategy for hypergraphs, however, is not straightforward. Questions remain with regard to its generative SSL task, connection to downstream tasks, and empirical properties of learned representations. In light of the promises and challenges, we propose a novel generative SSL strategy for hypergraphs. We first formulate a generative SSL task on hypergraphs, hyperedge filling, and highlight its theoretical connection to node classification. Based on the generative SSL task, we propose a hypergraph SS
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30896;&#25758;&#22238;&#36991;&#20013;&#20351;&#29992;&#22806;&#37096;&#24863;&#30693;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#20197;&#33719;&#21462;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.00623</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30896;&#25758;&#22238;&#36991;&#20013;&#30340;&#22806;&#37096;&#24863;&#30693;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Variational Autoencoders for exteroceptive perception in reinforcement learning-based collision avoidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00623
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30896;&#25758;&#22238;&#36991;&#20013;&#20351;&#29992;&#22806;&#37096;&#24863;&#30693;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#20197;&#33719;&#21462;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25511;&#21046;&#31995;&#32479;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#22686;&#24378;&#20854;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25511;&#21046;&#26694;&#26550;&#65292;&#29305;&#21035;&#22312;&#28023;&#27915;&#36816;&#36755;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#22312;&#33719;&#21462;&#39640;&#20445;&#30495;&#24230;&#27979;&#36317;&#20256;&#24863;&#22120;&#30340;&#27867;&#21270;&#20302;&#32500;&#28508;&#22312;&#32534;&#30721;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#36825;&#23558;&#20316;&#20026;DRL&#20195;&#29702;&#30340;&#22806;&#37096;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00623v1 Announce Type: new  Abstract: Modern control systems are increasingly turning to machine learning algorithms to augment their performance and adaptability. Within this context, Deep Reinforcement Learning (DRL) has emerged as a promising control framework, particularly in the domain of marine transportation. Its potential for autonomous marine applications lies in its ability to seamlessly combine path-following and collision avoidance with an arbitrary number of obstacles. However, current DRL algorithms require disproportionally large computational resources to find near-optimal policies compared to the posed control problem when the searchable parameter space becomes large. To combat this, our work delves into the application of Variational AutoEncoders (VAEs) to acquire a generalized, low-dimensional latent encoding of a high-fidelity range-finding sensor, which serves as the exteroceptive input to a DRL agent. The agent's performance, encompassing path-following
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#25903;&#24452;&#21521;&#22522;&#32593;&#32476;&#26041;&#27861;&#26469;&#25104;&#21151;&#39044;&#27979;&#22797;&#26434;&#28151;&#27788;&#34892;&#20026;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24182;&#24341;&#20837;&#20102;&#27880;&#24847;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#38416;&#26126;&#20013;&#30340;&#28508;&#21147;</title><link>https://arxiv.org/abs/2404.00618</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#20998;&#25903;&#24452;&#21521;&#22522;&#32593;&#32476;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#22797;&#26434;&#28151;&#27788;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
A Multi-Branched Radial Basis Network Approach to Predicting Complex Chaotic Behaviours
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00618
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#25903;&#24452;&#21521;&#22522;&#32593;&#32476;&#26041;&#27861;&#26469;&#25104;&#21151;&#39044;&#27979;&#22797;&#26434;&#28151;&#27788;&#34892;&#20026;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24182;&#24341;&#20837;&#20102;&#27880;&#24847;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#38416;&#26126;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20998;&#25903;&#32593;&#32476;&#26041;&#27861;&#26469;&#39044;&#27979;&#30001;&#38169;&#32508;&#22797;&#26434;&#21644;&#28151;&#27788;&#34892;&#20026;&#29305;&#24449;&#30340;&#29289;&#29702;&#21560;&#24341;&#23376;&#30340;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#30001;&#24452;&#21521;&#22522;&#20989;&#25968;&#65288;RBF&#65289;&#23618;&#21644;&#26088;&#22312;&#26377;&#25928;&#25429;&#25417;&#21560;&#24341;&#23376;&#26102;&#38388;&#28436;&#21464;&#20013;&#38750;&#32447;&#24615;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#30340;&#27880;&#24847;&#26426;&#21046;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#21253;&#21547;&#22823;&#32422;28&#20998;&#38047;&#27963;&#21160;&#30340;&#29616;&#23454;&#25968;&#25454;&#38598;&#30340;36,700&#20010;&#26102;&#38388;&#24207;&#21015;&#35266;&#27979;&#65292;&#25105;&#20204;&#25104;&#21151;&#39044;&#27979;&#20102;&#21560;&#24341;&#23376;&#30340;&#36712;&#36857;&#30340;100&#27425;&#39044;&#27979;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35828;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#21487;&#35270;&#21270;&#65292;&#23637;&#31034;&#20102;&#21560;&#24341;&#23376;&#30340;&#21407;&#22987;&#21644;&#39044;&#27979;&#34892;&#20026;&#65292;&#24182;&#23558;&#35266;&#23519;&#21040;&#30340;&#19982;&#20272;&#35745;&#32467;&#26524;&#36827;&#34892;&#20102;&#23450;&#37327;&#27604;&#36739;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#38416;&#26126;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00618v1 Announce Type: new  Abstract: In this study, we propose a multi branched network approach to predict the dynamics of a physics attractor characterized by intricate and chaotic behavior. We introduce a unique neural network architecture comprised of Radial Basis Function (RBF) layers combined with an attention mechanism designed to effectively capture nonlinear inter-dependencies inherent in the attractor's temporal evolution. Our results demonstrate successful prediction of the attractor's trajectory across 100 predictions made using a real-world dataset of 36,700 time-series observations encompassing approximately 28 minutes of activity. To further illustrate the performance of our proposed technique, we provide comprehensive visualizations depicting the attractor's original and predicted behaviors alongside quantitative measures comparing observed versus estimated outcomes. Overall, this work showcases the potential of advanced machine learning algorithms in elucid
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24191;&#27867;&#33258;&#23545;&#27604;&#29983;&#25104;&#36127;&#20363;&#30340;&#26080;&#38656;&#21453;&#39304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00604</link><description>&lt;p&gt;
&#24191;&#27867;&#30340;&#33258;&#23545;&#27604;&#20351;&#24471;&#26080;&#38656;&#21453;&#39304;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Extensive Self-Contrast Enables Feedback-Free Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24191;&#27867;&#33258;&#23545;&#27604;&#29983;&#25104;&#36127;&#20363;&#30340;&#26080;&#38656;&#21453;&#39304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#19968;&#30452;&#26159;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#40784;&#30340;&#26680;&#24515;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20854;&#20005;&#37325;&#20381;&#36182;&#26114;&#36149;&#30340;&#20154;&#31867;&#25110;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#30340;&#20559;&#22909;&#21453;&#39304;&#21487;&#33021;&#20250;&#38459;&#30861;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Self-Contrast&#65292;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#24191;&#27867;&#33258;&#21160;&#29983;&#25104;&#30340;&#36127;&#20363;&#26469;&#36827;&#34892;&#26080;&#38656;&#21453;&#39304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#12290;&#20165;&#36890;&#36807;&#30417;&#30563;&#30340;&#24494;&#35843;&#65288;SFT&#65289;&#30446;&#26631;&#65292;Self-Contrast&#21033;&#29992;LLM&#26412;&#36523;&#29983;&#25104;&#22823;&#37327;&#22810;&#26679;&#30340;&#20505;&#36873;&#39033;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#26681;&#25454;&#25991;&#26412;&#30456;&#20284;&#24615;&#36807;&#28388;&#22810;&#20010;&#36127;&#20363;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#20165;&#20165;&#25193;&#22823;&#36127;&#38754;&#22238;&#24212;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#20855;&#26377;&#26356;&#24179;&#34913;&#30340;&#27491;&#38754;&#21644;&#36127;&#38754;&#20559;&#22909;&#27880;&#37322;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Self-Contrast&#33021;&#22815;&#22987;&#32456;&#20248;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00604v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been a central technique for recent large language model (LLM) alignment. However, its heavy dependence on costly human or LLM-as-Judge preference feedback could stymie its wider applications. In this work, we introduce Self-Contrast, a feedback-free large language model alignment method via exploiting extensive self-generated negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast leverages the LLM itself to generate massive diverse candidates, and harnesses a pre-trained embedding model to filter multiple negatives according to text similarity. Theoretically, we illustrate that in this setting, merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations. Our experiments with direct preference optimization (DPO) on three datasets show that, Self-Contrast could consistently outperform
&lt;/p&gt;</description></item><item><title>LAESI&#25968;&#25454;&#38598;&#21253;&#21547;10&#19975;&#24352;&#21512;&#25104;&#21494;&#29255;&#22270;&#20687;&#65292;&#29992;&#20110;&#21494;&#24418;&#24577;&#20998;&#26512;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#21518;&#21487;&#39044;&#27979;&#21494;&#29255;&#34920;&#38754;&#31215;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;3D&#31243;&#24207;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;AI&#30340;&#39640;&#25928;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.00593</link><description>&lt;p&gt;
LAESI: &#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#36827;&#34892;&#21494;&#29255;&#38754;&#31215;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
LAESI: Leaf Area Estimation with Synthetic Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00593
&lt;/p&gt;
&lt;p&gt;
LAESI&#25968;&#25454;&#38598;&#21253;&#21547;10&#19975;&#24352;&#21512;&#25104;&#21494;&#29255;&#22270;&#20687;&#65292;&#29992;&#20110;&#21494;&#24418;&#24577;&#20998;&#26512;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#21518;&#21487;&#39044;&#27979;&#21494;&#29255;&#34920;&#38754;&#31215;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;3D&#31243;&#24207;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;AI&#30340;&#39640;&#25928;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;LAESI&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#20102;10&#19975;&#24352;&#21512;&#25104;&#21494;&#29255;&#22270;&#20687;&#30340;&#21512;&#25104;&#21494;&#29255;&#25968;&#25454;&#38598;&#65292;&#27599;&#24352;&#22270;&#20687;&#37117;&#26377;&#35821;&#20041;&#20998;&#21106;&#25513;&#27169;&#21644;&#34920;&#38754;&#31215;&#26631;&#27880;&#65292;&#22270;&#20687;&#32972;&#26223;&#26159;&#27627;&#31859;&#32440;&#12290;&#35813;&#25968;&#25454;&#38598;&#20027;&#35201;&#29992;&#20110;&#23545;&#23665;&#27611;&#27017;&#21644;&#27233;&#26641;&#21494;&#36827;&#34892;&#24418;&#24577;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21494;&#29255;&#34920;&#38754;&#31215;&#39044;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#36866;&#29992;&#24615;&#65292;&#20351;&#29992;&#30495;&#23454;&#22270;&#20687;&#36827;&#34892;&#39564;&#35777;&#12290;&#39564;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#35757;&#32451;&#20986;&#39044;&#27979;&#21494;&#38754;&#31215;&#30340;&#33021;&#21147;&#65292;&#30456;&#23545;&#35823;&#24046;&#19981;&#20250;&#36229;&#36807;&#24179;&#22343;&#20154;&#31867;&#26631;&#27880;&#21592;&#30340;&#35823;&#24046;&#12290;LAESI&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;3D&#31243;&#24207;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#12289;&#21487;&#25511;&#30340;&#25968;&#25454;&#29983;&#25104;&#65292;&#22312;&#20892;&#19994;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32435;&#20837;&#25105;&#20204;&#30340;&#31243;&#24207;&#21270;&#25968;&#25454;&#29983;&#25104;&#27969;&#31243;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;&#27880;&#37322;&#19968;&#33268;&#24615;&#30340;&#25968;&#25454;&#36807;&#28388;&#22914;&#20309;&#23548;&#33268;&#25968;&#25454;&#38598;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00593v1 Announce Type: cross  Abstract: We introduce LAESI, a Synthetic Leaf Dataset of 100,000 synthetic leaf images on millimeter paper, each with semantic masks and surface area labels. This dataset provides a resource for leaf morphology analysis primarily aimed at beech and oak leaves. We evaluate the applicability of the dataset by training machine learning models for leaf surface area prediction and semantic segmentation, using real images for validation. Our validation shows that these models can be trained to predict leaf surface area with a relative error not greater than an average human annotator. LAESI also provides an efficient framework based on 3D procedural models and generative AI for the large-scale, controllable generation of data with potential further applications in agriculture and biology. We evaluate the inclusion of generative AI in our procedural data generation pipeline and show how data filtering based on annotation consistency results in dataset
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#22270;&#25968;&#25454;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22359;&#22686;&#24378;&#65292;&#25552;&#20379;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#23436;&#25104;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00589</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#22270;&#25968;&#25454;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00589
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#22270;&#25968;&#25454;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22359;&#22686;&#24378;&#65292;&#25552;&#20379;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#23436;&#25104;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#22270;&#25968;&#25454;&#26159;&#19968;&#39033;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#25216;&#26415;&#65292;&#20363;&#22914;&#22522;&#20110;&#20960;&#20309;&#21644;&#30697;&#38453;&#20998;&#35299;&#30340;&#25216;&#26415;&#65292;&#20381;&#36182;&#20110;&#23545;&#25968;&#25454;&#20851;&#31995;&#30340;&#20551;&#35774;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#22270;&#25968;&#25454;&#26102;&#21464;&#24471;&#19981;&#36275;&#22815;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23637;&#31034;&#20102;&#22788;&#29702;&#22823;&#22411;&#22270;&#25968;&#25454;&#30340;&#33391;&#22909;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#20351;&#22270;&#22788;&#29702;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#22686;&#24378;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22359;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21147;&#37327;&#65292;&#20197;&#25552;&#20379;&#29983;&#25104;&#31572;&#26696;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22270;&#22788;&#29702;&#20219;&#21153;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#65306;&#23569;&#26679;&#26412;&#30693;&#35782;&#22270;&#23436;&#25104;&#21644;&#22270;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;LLM&#22312;&#21508;&#20010;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00589v1 Announce Type: cross  Abstract: Handling graph data is one of the most difficult tasks. Traditional techniques, such as those based on geometry and matrix factorization, rely on assumptions about the data relations that become inadequate when handling large and complex graph data. On the other hand, deep learning approaches demonstrate promising results in handling large graph data, but they often fall short of providing interpretable explanations. To equip the graph processing with both high accuracy and explainability, we introduce a novel approach that harnesses the power of a large language model (LLM), enhanced by an uncertainty-aware module to provide a confidence score on the generated answer. We experiment with our approach on two graph processing tasks: few-shot knowledge graph completion and graph classification. Our results demonstrate that through parameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms by a substantial margin across
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#33258;&#21160;&#21452;&#21521;&#21152;&#26435;&#38598;&#25104;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#33041;&#32959;&#30244;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#25928;&#26524;</title><link>https://arxiv.org/abs/2404.00576</link><description>&lt;p&gt;
&#33258;&#21160;&#21452;&#21521;&#21152;&#26435;&#38598;&#25104;&#31639;&#27861;&#21450;&#20854;&#22312;&#33041;&#32959;&#30244;&#26816;&#27979;&#21644;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Automated Bi-Fold Weighted Ensemble Algorithms and its Application to Brain Tumor Detection and Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00576
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#33258;&#21160;&#21452;&#21521;&#21152;&#26435;&#38598;&#25104;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#33041;&#32959;&#30244;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#32454;&#32990;&#30340;&#19981;&#21463;&#25511;&#21046;&#21644;&#26080;&#32467;&#26500;&#29983;&#38271;&#34987;&#31216;&#20026;&#33041;&#32959;&#30244;&#65292;&#23427;&#22312;&#21508;&#31181;&#30284;&#30151;&#20013;&#25317;&#26377;&#26368;&#39640;&#30340;&#27515;&#20129;&#29575;&#20043;&#19968;&#12290;&#30001;&#20110;&#26377;&#38480;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#31532;&#19977;&#19990;&#30028;&#22269;&#23478;&#65292;&#33041;&#32959;&#30244;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26089;&#26399;&#35786;&#26029;&#22312;&#26377;&#25928;&#31649;&#29702;&#33041;&#32959;&#30244;&#21644;&#20943;&#23569;&#27515;&#20129;&#29575;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#25104;&#26412;&#21644;&#38271;&#26102;&#38388;&#30340;&#32467;&#26524;&#33719;&#21462;&#26102;&#38388;&#31561;&#21508;&#31181;&#38480;&#21046;&#65292;&#35786;&#26029;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#21463;&#21040;&#38459;&#30861;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#30142;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21069;&#27839;&#30340;&#21452;&#21521;&#21152;&#26435;&#25237;&#31080;&#38598;&#25104;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#21152;&#26435;&#38598;&#25104;&#26041;&#27861;&#30340;&#25928;&#21147;&#12290;&#36825;&#20004;&#31181;&#25552;&#20986;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#26469;&#33258;&#22810;&#20010;&#20998;&#31867;&#22120;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#31532;&#19968;&#31181;&#26041;&#27861;&#20013;&#20855;&#26377;&#26368;&#39640;&#27010;&#29575;&#30340;&#32467;&#26524;&#65292;&#20197;&#21450;&#26368;&#39640;&#21152;&#26435; p
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00576v1 Announce Type: cross  Abstract: The uncontrolled and unstructured growth of brain cells is known as brain tumor, which has one of the highest mortality rates among diseases from all types of cancers. Due to limited diagnostic and treatment capabilities, they pose significant challenges, especially in third-world countries. Early diagnosis plays a vital role in effectively managing brain tumors and reducing mortality rates. However, the availability of diagnostic methods is hindered by various limitations, including high costs and lengthy result acquisition times, impeding early detection of the disease. In this study, we present two cutting-edge bi-fold weighted voting ensemble models that aim to boost the effectiveness of weighted ensemble methods. These two proposed methods combine the classification outcomes from multiple classifiers and determine the optimal result by selecting the one with the highest probability in the first approach, and the highest weighted p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;ADs&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21516;&#26102;&#36873;&#25321;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#30410;&#30340;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#28857;&#24182;&#20943;&#36731;&#25152;&#26377;&#36873;&#23450;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#30830;&#20445;&#22810;&#21488;&#26426;&#22120;&#20043;&#38388;&#20849;&#20139;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.00572</link><description>&lt;p&gt;
ADs: &#27963;&#36291;&#25968;&#25454;&#20849;&#20139;&#29992;&#20110;&#20808;&#36827;&#21046;&#36896;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#36136;&#37327;&#20445;&#38556;
&lt;/p&gt;
&lt;p&gt;
ADs: Active Data-sharing for Data Quality Assurance in Advanced Manufacturing Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00572
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;ADs&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21516;&#26102;&#36873;&#25321;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#30410;&#30340;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#28857;&#24182;&#20943;&#36731;&#25152;&#26377;&#36873;&#23450;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#30830;&#20445;&#22810;&#21488;&#26426;&#22120;&#20043;&#38388;&#20849;&#20139;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#25910;&#38598;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#25104;&#26412;&#21644;&#23545;&#21046;&#36896;&#31995;&#32479;&#30340;&#25237;&#36164;&#65292;&#24120;&#24120;&#23384;&#22312;&#25968;&#25454;&#31232;&#32570;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#20855;&#26377;&#31867;&#20284;&#21151;&#33021;&#30340;&#22810;&#21488;&#26426;&#22120;&#20043;&#38388;&#24191;&#27867;&#24320;&#21551;&#25968;&#25454;&#20849;&#20139;&#65292;&#20197;&#22686;&#21152;&#29992;&#20110;&#26500;&#24314;ML&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#30340;&#24037;&#20316;&#26465;&#20214;&#65292;&#23427;&#20204;&#30340;&#25968;&#25454;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#32780;ML&#26041;&#27861;&#34987;&#20551;&#35774;&#26159;&#22312;&#36981;&#24490;&#30456;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#19978;&#26500;&#24314;&#21644;&#27979;&#35797;&#30340;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#25968;&#25454;&#20849;&#20139;&#65288;ADs&#65289;&#26694;&#26550;&#65292;&#20197;&#30830;&#20445;&#22810;&#21488;&#26426;&#22120;&#20043;&#38388;&#20849;&#20139;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#23427;&#26088;&#22312;&#21516;&#26102;&#36873;&#25321;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#30410;&#30340;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#20943;&#36731;&#25152;&#26377;&#36873;&#23450;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00572v1 Announce Type: new  Abstract: Machine learning (ML) methods are widely used in industrial applications, which usually require a large amount of training data. However, data collection needs extensive time costs and investments in the manufacturing system, and data scarcity commonly exists. Therefore, data-sharing is widely enabled among multiple machines with similar functionality to augment the dataset for building ML methods. However, distribution mismatch inevitably exists in their data due to different working conditions, while the ML methods are assumed to be built and tested on the dataset following the same distribution. Thus, an Active Data-sharing (ADs) framework is proposed to ensure the quality of the shared data among multiple machines. It is designed to simultaneously select the most informative data points benefiting the downstream tasks and mitigate the distribution mismatch among all selected data points. The proposed method is validated on anomaly de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32447;&#24615;&#25554;&#20540;&#30340;CNFs&#22312;&#20174;&#26377;&#38480;&#38543;&#26426;&#26679;&#26412;&#20013;&#23398;&#20064;&#27010;&#29575;&#20998;&#24067;&#26102;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#35823;&#24046;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#20998;&#26512;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.00551</link><description>&lt;p&gt;
&#36830;&#32493;&#27491;&#35268;&#21270;&#27969;&#22312;&#23398;&#20064;&#27010;&#29575;&#20998;&#24067;&#20013;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of Continuous Normalizing Flows for Learning Probability Distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32447;&#24615;&#25554;&#20540;&#30340;CNFs&#22312;&#20174;&#26377;&#38480;&#38543;&#26426;&#26679;&#26412;&#20013;&#23398;&#20064;&#27010;&#29575;&#20998;&#24067;&#26102;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#35823;&#24046;&#30028;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#20998;&#26512;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#27491;&#35268;&#21270;&#27969;&#65288;CNFs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#23398;&#20064;&#27010;&#29575;&#20998;&#24067;&#30340;&#29983;&#25104;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#32463;&#39564;&#25104;&#21151;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#22270;&#20687;&#21512;&#25104;&#12289;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#21644;&#20998;&#23376;&#29983;&#25104;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32447;&#24615;&#25554;&#20540;&#30340;CNFs&#22312;&#20174;&#26377;&#38480;&#38543;&#26426;&#26679;&#26412;&#20013;&#23398;&#20064;&#27010;&#29575;&#20998;&#24067;&#26102;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#20351;&#29992;&#20102;&#27969;&#21305;&#37197;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;CNFs&#30340;&#20998;&#24067;&#20272;&#35745;&#22120;&#30340;&#38750;&#28176;&#36817;&#35823;&#24046;&#30028;&#65292;&#20197;Wasserstein-2&#36317;&#31163;&#34920;&#31034;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#20551;&#35774;&#26159;&#30446;&#26631;&#20998;&#24067;&#28385;&#36275;&#20197;&#19979;&#19977;&#20010;&#26465;&#20214;&#20043;&#19968;&#65306;&#35201;&#20040;&#20855;&#26377;&#26377;&#30028;&#25903;&#25345;&#65292;&#35201;&#20040;&#26159;&#24378;&#23545;&#25968;&#20985;&#30340;&#65292;&#35201;&#20040;&#26159;&#26377;&#38480;&#25110;&#26080;&#38480;&#28151;&#21512;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20102;&#35823;&#24046;&#25910;&#25947;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00551v1 Announce Type: cross  Abstract: Continuous normalizing flows (CNFs) are a generative method for learning probability distributions, which is based on ordinary differential equations. This method has shown remarkable empirical success across various applications, including large-scale image synthesis, protein structure prediction, and molecule generation. In this work, we study the theoretical properties of CNFs with linear interpolation in learning probability distributions from a finite random sample, using a flow matching objective function. We establish non-asymptotic error bounds for the distribution estimator based on CNFs, in terms of the Wasserstein-2 distance. The key assumption in our analysis is that the target distribution satisfies one of the following three conditions: it either has a bounded support, is strongly log-concave, or is a finite or infinite mixture of Gaussian distributions. We present a convergence analysis framework that encompasses the err
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31070;&#32463;&#27169;&#25311;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#24555;&#36895;&#22788;&#29702;&#25968;&#21315;&#20010;&#33258;&#30001;&#24230;&#30340;&#25955;&#23556;&#27169;&#25311;&#65292;&#24182;&#36890;&#36807;&#20013;&#38388;&#39044;&#27979;&#33021;&#21147;&#25552;&#20379;&#20102;&#20934;&#30830;&#24615;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2404.00545</link><description>&lt;p&gt;
&#32479;&#19968;&#12289;&#21487;&#39564;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;&#29992;&#20110;&#30005;&#30913;&#27874;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Unified, Verifiable Neural Simulators for Electromagnetic Wave Inverse Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00545
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31070;&#32463;&#27169;&#25311;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#24555;&#36895;&#22788;&#29702;&#25968;&#21315;&#20010;&#33258;&#30001;&#24230;&#30340;&#25955;&#23556;&#27169;&#25311;&#65292;&#24182;&#36890;&#36807;&#20013;&#38388;&#39044;&#27979;&#33021;&#21147;&#25552;&#20379;&#20102;&#20934;&#30830;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#25311;&#22120;&#20026;&#30005;&#30913;&#27874;&#27169;&#25311;&#25552;&#20379;&#20102;&#19968;&#26465;&#36895;&#24230;&#24555;&#25968;&#20010;&#25968;&#37327;&#32423;&#30340;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#20165;&#38024;&#23545;&#29421;&#31364;&#30340;&#38382;&#39064;&#31867;&#21035;&#65292;&#24182;&#19988;&#20165;&#33021;&#25193;&#23637;&#21040;&#20960;&#21313;&#20010;&#33258;&#30001;&#24230;&#65288;DoFs&#65289;&#30340;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#21333;&#19968;&#12289;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#25968;&#21315;&#20010;DoFs&#30340;&#25955;&#23556;&#27169;&#25311;&#65292;&#20219;&#24847;&#27874;&#38271;&#12289;&#20219;&#24847;&#29031;&#26126;&#27874;&#21069;&#21644;&#33258;&#30001;&#24418;&#24335;&#30340;&#26448;&#26009;&#65292;&#19988;&#22312;&#24191;&#27867;&#30340;&#21487;&#37197;&#32622;&#33539;&#22260;&#20869;&#12290;&#22522;&#20110;&#27880;&#24847;&#21147;&#22810;&#26465;&#20214;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#20801;&#35768;&#38750;&#36882;&#24402;&#30417;&#30563;&#21644;&#39044;&#27979;&#20013;&#38388;&#29289;&#29702;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#29983;&#25104;&#25104;&#26412;&#12290;&#21033;&#29992;&#36825;&#31181;O(1)&#26102;&#38388;&#30340;&#20013;&#38388;&#39044;&#27979;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#19968;&#31181;&#20005;&#26684;&#19988;&#39640;&#25928;&#35745;&#31639;&#30340;&#39044;&#27979;&#35823;&#24046;&#19978;&#30028;&#65292;&#20801;&#35768;&#25512;&#26029;&#26102;&#23545;&#25152;&#26377;&#39044;&#27979;&#36827;&#34892;&#20934;&#30830;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00545v1 Announce Type: cross  Abstract: Simulators based on neural networks offer a path to orders-of-magnitude faster electromagnetic wave simulations. Existing models, however, only address narrowly tailored classes of problems and only scale to systems of a few dozen degrees of freedom (DoFs). Here, we demonstrate a single, unified model capable of addressing scattering simulations with thousands of DoFs, of any wavelength, any illumination wavefront, and freeform materials, within broad configurable bounds. Based on an attentional multi-conditioning strategy, our method also allows non-recurrent supervision on and prediction of intermediate physical states, which provides improved generalization with no additional data-generation cost. Using this O(1)-time intermediate prediction capability, we propose and prove a rigorous, efficiently computable upper bound on prediction error, allowing accuracy guarantees at inference time for all predictions. After training solely on 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20004;&#38454;&#27573;&#22270;&#25351;&#38024;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#35299;&#20915;QAP&#38382;&#39064;&#30340;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#21322;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2404.00539</link><description>&lt;p&gt;
&#36890;&#36807;&#20004;&#38454;&#27573;&#22270;&#25351;&#38024;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;QAP&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving the QAP by Two-Stage Graph Pointer Networks and Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00539
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20004;&#38454;&#27573;&#22270;&#25351;&#38024;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#35299;&#20915;QAP&#38382;&#39064;&#30340;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#21322;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#65288;QAP&#65289;&#26159;&#19968;&#20010;&#23454;&#38469;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#22810;&#24180;&#26469;&#19968;&#30452;&#21463;&#21040;&#30740;&#31350;&#12290;&#30001;&#20110;&#23427;&#26159;NP&#22256;&#38590;&#30340;&#65292;&#35299;&#20915;QAP&#30340;&#22823;&#38382;&#39064;&#23454;&#20363;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#23613;&#31649;&#21551;&#21457;&#24335;&#26041;&#27861;&#21487;&#20197;&#25214;&#21040;&#21322;&#26368;&#20248;&#35299;&#65292;&#20294;&#38543;&#30528;&#38382;&#39064;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#25191;&#34892;&#26102;&#38388;&#26174;&#30528;&#22686;&#21152;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#27604;&#21551;&#21457;&#24335;&#31639;&#27861;&#26356;&#24555;&#30340;&#35299;&#20915;&#22120;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#20294;&#21363;&#20351;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65292;&#35299;&#20915;&#22823;&#22411;QAP&#38382;&#39064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31216;&#20026;&#20004;&#38454;&#27573;&#22270;&#25351;&#38024;&#32593;&#32476;&#65288;GPN&#65289;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;QAP&#12290;&#20004;&#38454;&#27573;GPN&#20381;&#36182;&#20110;GPN&#65292;&#35813;&#27169;&#22411;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#27431;&#20960;&#37324;&#24471;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25193;&#23637;GPN&#20197;&#29992;&#20110;&#19968;&#33324;TSP&#65292;&#28982;&#21518;&#25105;&#20204;&#21521;&#35813;&#27169;&#22411;&#28155;&#21152;&#26032;&#31639;&#27861;&#20197;&#35299;&#20915;QAP&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20004;&#38454;&#27573;GPN&#25552;&#20379;&#20102;&#21322;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00539v1 Announce Type: new  Abstract: Quadratic Assignment Problem (QAP) is a practical combinatorial optimization problems that has been studied for several years. Since it is NP-hard, solving large problem instances of QAP is challenging. Although heuristics can find semi-optimal solutions, the execution time significantly increases as the problem size increases. Recently, solving combinatorial optimization problems by deep learning has been attracting attention as a faster solver than heuristics. Even with deep learning, however, solving large QAP is still challenging. In this paper, we propose the deep reinforcement learning model called the two-stage graph pointer network (GPN) for solving QAP. Two-stage GPN relies on GPN, which has been proposed for Euclidean Traveling Salesman Problem (TSP). First, we extend GPN for general TSP, and then we add new algorithms to that model for solving QAP. Our experimental results show that our two-stage GPN provides semi-optimal solu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#22909;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;DOVE&#21327;&#35758;&#23545;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.00530</link><description>&lt;p&gt;
&#23558;&#22351;&#33529;&#26524;&#19982;&#22909;&#27224;&#23376;&#36827;&#34892;&#27604;&#36739;&#65306;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#20559;&#22909;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#22909;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;DOVE&#21327;&#35758;&#23545;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#24120;&#35265;&#30340;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25216;&#26415;&#20381;&#36182;&#20110;&#36890;&#36807;&#27604;&#36739;&#22312;&#22266;&#23450;&#19978;&#19979;&#25991;&#20013;&#26465;&#20214;&#29983;&#25104;&#30340;&#22810;&#20010;&#29983;&#25104;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#29983;&#25104;&#25918;&#32622;&#22312;&#30456;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#26102;&#65292;&#36825;&#20165;&#21033;&#29992;&#20102;&#25104;&#23545;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26465;&#20214;&#25490;&#21517;&#36890;&#24120;&#26080;&#27861;&#25429;&#33719;&#20154;&#31867;&#20559;&#22909;&#30340;&#22797;&#26434;&#21644;&#22810;&#32500;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20559;&#22909;&#33719;&#21462;&#30340;&#20256;&#32479;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22312;&#25351;&#20196;-&#21709;&#24212;&#23545;&#19978;&#32852;&#21512;&#24341;&#21457;&#20559;&#22909;&#30340;&#26032;&#36724;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#20559;&#22909;&#20248;&#21270;&#26159;&#38024;&#23545;&#26465;&#20214;&#25490;&#21517;&#21327;&#35758;&#65288;&#20363;&#22914;&#65292;DPO&#65289;&#35774;&#35745;&#30340;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;&#20559;&#22909;&#33719;&#21462;&#21327;&#35758;&#24341;&#20837;&#20102;DOVE&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20559;&#22909;&#20248;&#21270;&#30446;&#26631;&#65292;&#36890;&#36807;&#25552;&#21319;&#25152;&#36873;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#26469;&#38477;&#20302;&#25152;&#25298;&#32477;&#25351;&#20196;-&#21709;&#24212;&#23545;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00530v1 Announce Type: cross  Abstract: A common technique for aligning large language models (LLMs) relies on acquiring human preferences by comparing multiple generations conditioned on a fixed context. This only leverages the pairwise comparisons when the generations are placed in an identical context. However, such conditional rankings often fail to capture the complex and multidimensional aspects of human preferences. In this work, we revisit the traditional paradigm of preference acquisition and propose a new axis that is based on eliciting preferences jointly over the instruction-response pairs. While prior preference optimizations are designed for conditional ranking protocols (e.g., DPO), our proposed preference acquisition protocol introduces DOVE, a new preference optimization objective that upweights the joint probability of the chosen instruction-response pair over the rejected instruction-response pair. Interestingly, we find that the LLM trained with joint ins
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#24378;&#27745;&#26579;&#27169;&#22411;&#19979;&#40065;&#26834;&#23398;&#20064;&#20302;&#27425;&#22810;&#39033;&#24335;&#38408;&#20989;&#25968;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;PAC&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#39640;&#26031;&#20998;&#24067;&#19979;&#20855;&#26377;&#35823;&#24046;&#20445;&#35777;$O_{d, c}(\text{opt}^{1-c})$&#12290;</title><link>https://arxiv.org/abs/2404.00529</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#30340;&#36229;&#38750;&#22855;&#24322;&#20998;&#35299;&#21450;&#20854;&#22312;&#40065;&#26834;&#23398;&#20064;&#20302;&#27425;&#22810;&#39033;&#24335;&#38408;&#20989;&#25968;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Super Non-singular Decompositions of Polynomials and their Application to Robustly Learning Low-degree PTFs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00529
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#24378;&#27745;&#26579;&#27169;&#22411;&#19979;&#40065;&#26834;&#23398;&#20064;&#20302;&#27425;&#22810;&#39033;&#24335;&#38408;&#20989;&#25968;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;PAC&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#39640;&#26031;&#20998;&#24067;&#19979;&#20855;&#26377;&#35823;&#24046;&#20445;&#35777;$O_{d, c}(\text{opt}^{1-c})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#24694;&#24847;&#30772;&#22351;&#30340;&#24773;&#20917;&#19979;&#65292;&#20302;&#27425;&#22810;&#39033;&#24335;&#38408;&#20540;&#20989;&#25968;&#65288;PTFs&#65289;&#30340;&#26377;&#25928;&#21487;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#31639;&#27861;&#32467;&#26524;&#26159;&#22312;&#24378;&#27745;&#26579;&#27169;&#22411;&#19979;&#23545;&#36825;&#20010;&#27010;&#24565;&#31867;&#21035;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#65292;&#38024;&#23545;&#39640;&#26031;&#20998;&#24067;&#65292;&#20445;&#35777;&#35823;&#24046;&#20026;$O_{d, c}(\text{opt}^{1-c})$&#65292;&#20854;&#20013;$c&gt;0$&#26159;&#20219;&#24847;&#24120;&#25968;&#65292;$\text{opt}$&#26159;&#30772;&#22351;&#30340;&#27604;&#20363;&#12290;&#22312;&#24378;&#27745;&#26579;&#27169;&#22411;&#20013;&#65292;&#19968;&#20010;&#26080;&#25152;&#19981;&#30693;&#30340;&#23545;&#25163;&#21487;&#20197;&#20219;&#24847;&#30772;&#22351;$\text{opt}$&#27604;&#20363;&#30340;&#25968;&#25454;&#28857;&#21450;&#20854;&#26631;&#31614;&#12290;&#35813;&#27169;&#22411;&#27867;&#21270;&#20102;&#24694;&#24847;&#22122;&#22768;&#27169;&#22411;&#21644;&#23545;&#25239;&#24615;&#26631;&#31614;&#22122;&#22768;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20043;&#21069;&#65292;&#24050;&#30693;&#22312;&#36825;&#31181;&#30772;&#22351;&#27169;&#22411;&#65288;&#29978;&#33267;&#22312;&#36739;&#24369;&#30340;&#23545;&#25239;&#24615;&#26631;&#31614;&#22122;&#22768;&#27169;&#22411;&#65289;&#20013;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#30340;&#38169;&#35823;&#29575;&#20026;$\tilde{O}_d(\text{opt}^{1/(d+1)})$&#65292;&#36825;&#38543;&#30528;&#24230;&#25968;$d$&#30340;&#22686;&#21152;&#32780;&#26174;&#33879;&#24694;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00529v1 Announce Type: cross  Abstract: We study the efficient learnability of low-degree polynomial threshold functions (PTFs) in the presence of a constant fraction of adversarial corruptions. Our main algorithmic result is a polynomial-time PAC learning algorithm for this concept class in the strong contamination model under the Gaussian distribution with error guarantee $O_{d, c}(\text{opt}^{1-c})$, for any desired constant $c&gt;0$, where $\text{opt}$ is the fraction of corruptions. In the strong contamination model, an omniscient adversary can arbitrarily corrupt an $\text{opt}$-fraction of the data points and their labels. This model generalizes the malicious noise model and the adversarial label noise model. Prior to our work, known polynomial-time algorithms in this corruption model (or even in the weaker adversarial label noise model) achieved error $\tilde{O}_d(\text{opt}^{1/(d+1)})$, which deteriorates significantly as a function of the degree $d$.   Our algorithm e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26500;&#24314;&#29983;&#25104;&#24335;&#27169;&#22411;&#29992;&#20110;&#38271;&#26399;&#22825;&#27668;&#39044;&#25253;&#65292;&#20174;&#32780;&#25913;&#36827;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#23567;&#40614;&#12289;&#22823;&#40614;&#21644;&#27833;&#33756;&#30340;&#29983;&#20135;&#20013;&#65292;&#20197;&#21450;&#36825;&#20123;&#20316;&#29289;&#36718;&#20316;&#19977;&#24180;&#29983;&#20135;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00528</link><description>&lt;p&gt;
&#29992;&#20110;&#25913;&#36827;&#20316;&#29289;&#27169;&#25311;&#30340;&#29983;&#25104;&#24335;&#22825;&#27668;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative weather for improved crop model simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00528
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26500;&#24314;&#29983;&#25104;&#24335;&#27169;&#22411;&#29992;&#20110;&#38271;&#26399;&#22825;&#27668;&#39044;&#25253;&#65292;&#20174;&#32780;&#25913;&#36827;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#23567;&#40614;&#12289;&#22823;&#40614;&#21644;&#27833;&#33756;&#30340;&#29983;&#20135;&#20013;&#65292;&#20197;&#21450;&#36825;&#20123;&#20316;&#29289;&#36718;&#20316;&#19977;&#24180;&#29983;&#20135;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#21644;&#31934;&#20934;&#30340;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#23545;&#20110;&#20892;&#22330;&#27700;&#24179;&#21644;&#21306;&#22495;&#27700;&#24179;&#30340;&#20915;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#36827;&#34892;&#20135;&#37327;&#39044;&#27979;&#65292;&#20316;&#29289;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#20854;&#27169;&#25311;&#20551;&#35774;&#24773;&#26223;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#20135;&#37327;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#27169;&#25311;&#20013;&#30340;&#22825;&#27668;&#36755;&#20837;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#23545;&#20934;&#22791;&#22825;&#27668;&#36755;&#20837;&#30340;&#20851;&#27880;&#29978;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#38271;&#26399;&#22825;&#27668;&#39044;&#25253;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#32456;&#25913;&#36827;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;--&#23567;&#40614;&#12289;&#22823;&#40614;&#21644;&#27833;&#33756;&#30340;&#21333;&#24180;&#29983;&#20135;&#20197;&#21450;&#21033;&#29992;&#36825;&#20123;&#20316;&#29289;&#36718;&#20316;&#30340;&#19977;&#24180;&#29983;&#20135;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#36739;&#20256;&#32479;&#26041;&#27861;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#39044;&#27979;&#35823;&#24046;&#30340;&#24179;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31532;&#19968;&#22330;&#26223;&#30340;18&#20010;&#25351;&#26631;&#20013;&#27599;&#19968;&#20010;&#37117;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00528v1 Announce Type: new  Abstract: Accurate and precise crop yield prediction is invaluable for decision making at both farm levels and regional levels. To make yield prediction, crop models are widely used for their capability to simulate hypothetical scenarios. While accuracy and precision of yield prediction critically depend on weather inputs to simulations, surprisingly little attention has been paid to preparing weather inputs. We propose a new method to construct generative models for long-term weather forecasts and ultimately improve crop yield prediction. We demonstrate use of the method in two representative scenarios -- single-year production of wheat, barley and canola and three-year production using rotations of these crops. Results show significant improvement from the conventional method, measured in terms of mean and standard deviation of prediction errors. Our method outperformed the conventional method in every one of 18 metrics for the first scenario an
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#33021;&#28304;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#26399;&#24180;&#24230;&#28040;&#36153;&#37197;&#32622;&#25991;&#20214;&#24182;&#21033;&#29992;&#20803;&#25968;&#25454;&#20135;&#29983;&#36830;&#36143;&#30340;&#25968;&#25454;</title><link>https://arxiv.org/abs/2404.00525</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#21644;&#24314;&#31569;&#20803;&#25968;&#25454;&#21019;&#24314;&#21512;&#25104;&#33021;&#28304;&#34920;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Creating synthetic energy meter data using conditional diffusion and building metadata
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00525
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#33021;&#28304;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#26399;&#24180;&#24230;&#28040;&#36153;&#37197;&#32622;&#25991;&#20214;&#24182;&#21033;&#29992;&#20803;&#25968;&#25454;&#20135;&#29983;&#36830;&#36143;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#27493;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#24378;&#25512;&#21160;&#20102;&#33021;&#28304;&#30456;&#20851;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#26469;&#33258;&#24314;&#31569;&#29289;&#30340;&#31169;&#20154;&#33021;&#28304;&#25968;&#25454;&#30340;&#26377;&#38480;&#35775;&#38382;&#38480;&#21046;&#20102;&#20381;&#36182;&#20110;&#21382;&#21490;&#25968;&#25454;&#30340;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;&#23613;&#31649;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#30701;&#26399;&#29983;&#25104;&#21608;&#26399;&#65288;&#20363;&#22914;&#65292;&#26085;&#24120;&#37197;&#32622;&#25991;&#20214;&#65289;&#21644;&#26377;&#38480;&#25968;&#37327;&#30340;&#34920;&#35745;&#19978;&#12290;&#22240;&#27492;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#20803;&#25968;&#25454;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#33021;&#28304;&#25968;&#25454;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#12290;&#20351;&#29992;&#21253;&#21547;&#26469;&#33258;&#19981;&#21516;&#24314;&#31569;&#29289;&#21644;&#22269;&#23478;&#30340;1,828&#20010;&#21151;&#29575;&#34920;&#30340;&#25968;&#25454;&#38598;&#65292;&#23558;&#35813;&#27169;&#22411;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGAN&#65289;&#21644;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23427;&#26126;&#30830;&#22788;&#29702;&#38271;&#26399;&#24180;&#24230;&#28040;&#36153;&#37197;&#32622;&#25991;&#20214;&#65292;&#21033;&#29992;&#20301;&#32622;&#12289;&#22825;&#27668;&#12289;&#24314;&#31569;&#29289;&#21644;&#34920;&#31867;&#22411;&#31561;&#20803;&#25968;&#25454;&#20135;&#29983;&#36830;&#36143;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00525v1 Announce Type: new  Abstract: Advances in machine learning and increased computational power have driven progress in energy-related research. However, limited access to private energy data from buildings hinders traditional regression models relying on historical data. While generative models offer a solution, previous studies have primarily focused on short-term generation periods (e.g., daily profiles) and a limited number of meters. Thus, the study proposes a conditional diffusion model for generating high-quality synthetic energy data using relevant metadata. Using a dataset comprising 1,828 power meters from various buildings and countries, this model is compared with traditional methods like Conditional Generative Adversarial Networks (CGAN) and Conditional Variational Auto-Encoders (CVAE). It explicitly handles long-term annual consumption profiles, harnessing metadata such as location, weather, building, and meter type to produce coherent synthetic data that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#35774;&#32622;&#19979;&#65292;&#33391;&#24615;&#36807;&#25311;&#21512;&#32447;&#24615;&#25554;&#20540;&#22120;&#30340;&#38750;&#28176;&#36817;&#36229;&#39069;&#39118;&#38505;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00522</link><description>&lt;p&gt;
&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Minimum-Norm Interpolation Under Covariate Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#35774;&#32622;&#19979;&#65292;&#33391;&#24615;&#36807;&#25311;&#21512;&#32447;&#24615;&#25554;&#20540;&#22120;&#30340;&#38750;&#28176;&#36817;&#36229;&#39069;&#39118;&#38505;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#26159;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#37096;&#32626;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#39564;&#30740;&#31350;&#20013;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#31616;&#21333;&#35774;&#32622;&#20013;&#65292;&#22312;&#23545;&#36716;&#31227;&#23398;&#20064;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#22312;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#30340;&#20998;&#24067;&#30740;&#31350;&#20013;&#65292;&#24050;&#32463;&#21457;&#29616;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#8221;&#29616;&#35937;&#30340;&#29616;&#35937;&#65292;&#21363;&#32447;&#24615;&#25554;&#20540;&#22120;&#20250;&#23545;&#22122;&#22768;&#35757;&#32451;&#26631;&#31614;&#36807;&#25311;&#21512;&#65292;&#20294;&#20173;&#28982;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#36825;&#31181;&#34892;&#20026;&#21457;&#29983;&#22312;&#28304;&#21327;&#26041;&#24046;&#30697;&#38453;&#21644;&#36755;&#20837;&#25968;&#25454;&#32500;&#24230;&#19978;&#30340;&#29305;&#23450;&#26465;&#20214;&#19979;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#24819;&#30693;&#36947;&#36825;&#26679;&#30340;&#39640;&#32500;&#32447;&#24615;&#27169;&#22411;&#22312;&#36716;&#31227;&#23398;&#20064;&#19979;&#22914;&#20309;&#34892;&#20026;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#35774;&#32622;&#20013;&#33391;&#24615;&#36807;&#25311;&#21512;&#32447;&#24615;&#25554;&#20540;&#22120;&#30340;&#31532;&#19968;&#20010;&#38750;&#28176;&#36817;&#36229;&#39069;&#39118;&#38505;&#30028;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;\textit {b&#36827;&#34892;&#20998;&#31867;}}&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00522v1 Announce Type: new  Abstract: Transfer learning is a critical part of real-world machine learning deployments and has been extensively studied in experimental works with overparameterized neural networks. However, even in the simplest setting of linear regression a notable gap still exists in the theoretical understanding of transfer learning. In-distribution research on high-dimensional linear regression has led to the identification of a phenomenon known as \textit{benign overfitting}, in which linear interpolators overfit to noisy training labels and yet still generalize well. This behavior occurs under specific conditions on the source covariance matrix and input data dimension. Therefore, it is natural to wonder how such high-dimensional linear models behave under transfer learning. We prove the first non-asymptotic excess risk bounds for benignly-overfit linear interpolators in the transfer learning setting. From our analysis, we propose a taxonomy of \textit{b
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;CHAIN&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;GANs&#20013;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00521</link><description>&lt;p&gt;
CHAIN&#65306;&#36890;&#36807;&#21463;&#38480;&#21807;&#19968;&#24615;&#36830;&#32493;&#24615;&#35268;&#33539;&#21270;&#22686;&#24378;&#25968;&#25454;&#39640;&#25928;GANs&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;CHAIN&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;GANs&#20013;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26174;&#30528;&#25512;&#21160;&#20102;&#22270;&#20687;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;GANs&#32463;&#24120;&#38754;&#20020;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#35782;&#21035;Batch Normalization&#65288;BN&#65289;&#20013;&#30340;&#20851;&#38190;&#32570;&#38519;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65306;&#22312;&#20013;&#24515;&#21270;&#21644;&#32553;&#25918;&#27493;&#39588;&#20013;&#26799;&#24230;&#29190;&#28856;&#30340;&#20542;&#21521;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CHAIN&#65288;&#21463;&#38480;&#21807;&#19968;&#24615;&#36830;&#32493;&#24615;&#35268;&#33539;&#21270;&#65289;&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#20013;&#24515;&#21270;&#27493;&#39588;&#26367;&#25442;&#20026;&#38646;&#22343;&#20540;&#27491;&#21017;&#21270;&#65292;&#24182;&#22312;&#32553;&#25918;&#27493;&#39588;&#20013;&#38598;&#25104;&#20102;Lipschitz&#36830;&#32493;&#24615;&#32422;&#26463;&#12290;CHAIN&#36890;&#36807;&#33258;&#36866;&#24212;&#25554;&#20540;&#24402;&#19968;&#21270;&#21644;&#38750;&#24402;&#19968;&#21270;&#29305;&#24449;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;GANs&#30340;&#35757;&#32451;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00521v1 Announce Type: new  Abstract: Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#24471;&#33021;&#22815;&#22312;&#30701;&#30701;18&#23567;&#26102;&#20869;&#20351;&#29992;&#21333;&#21488;&#26426;&#22120;&#35757;&#32451;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#39640;&#36798;5&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2404.00509</link><description>&lt;p&gt;
DailyMAE&#65306;&#26397;&#30528;&#19968;&#22825;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
DailyMAE: Towards Pretraining Masked Autoencoders in One Day
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00509
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#24471;&#33021;&#22815;&#22312;&#30701;&#30701;18&#23567;&#26102;&#20869;&#20351;&#29992;&#21333;&#21488;&#26426;&#22120;&#35757;&#32451;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#39640;&#36798;5&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#20316;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#65292;&#22240;&#20854;&#22312;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#32780;&#24341;&#36215;&#20851;&#27880;&#12290;&#35768;&#22810;&#30740;&#31350;&#24378;&#35843;&#20102;MIM&#30340;&#20248;&#21183;&#65292;&#31361;&#26174;&#20986;&#22312;&#24191;&#27867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22914;&#20309;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#23398;&#26415;&#29615;&#22659;&#20013;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;SSL&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;MIM&#30340;&#39640;&#25928;&#35757;&#32451;&#37197;&#26041;&#30340;&#24314;&#35758;&#65292;&#37325;&#28857;&#26159;&#20943;&#36731;&#25968;&#25454;&#21152;&#36733;&#29942;&#39048;&#65292;&#24182;&#37319;&#29992;&#28176;&#36827;&#35757;&#32451;&#25216;&#26415;&#21644;&#20854;&#20182;&#25216;&#24039;&#20197;&#32039;&#23494;&#32500;&#25345;&#39044;&#35757;&#32451;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24211;&#20351;&#24471;&#22312;&#20165;18&#23567;&#26102;&#20869;&#36890;&#36807;&#21333;&#21488;&#37197;&#22791;8&#39063;A100 GPU&#30340;&#26426;&#22120;&#35757;&#32451;MAE-Base/16&#27169;&#22411;&#22312;ImageNet 1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;800&#20010;epochs&#25104;&#20026;&#21487;&#33021;&#12290;&#36890;&#36807;&#33719;&#24471;&#39640;&#36798;5&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00509v1 Announce Type: new  Abstract: Recently, masked image modeling (MIM), an important self-supervised learning (SSL) method, has drawn attention for its effectiveness in learning data representation from unlabeled data. Numerous studies underscore the advantages of MIM, highlighting how models pretrained on extensive datasets can enhance the performance of downstream tasks. However, the high computational demands of pretraining pose significant challenges, particularly within academic environments, thereby impeding the SSL research progress. In this study, we propose efficient training recipes for MIM based SSL that focuses on mitigating data loading bottlenecks and employing progressive training techniques and other tricks to closely maintain pretraining performance. Our library enables the training of a MAE-Base/16 model on the ImageNet 1K dataset for 800 epochs within just 18 hours, using a single machine equipped with 8 A100 GPUs. By achieving speed gains of up to 5.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26631;&#31614;&#30340;&#26080;&#30417;&#30563;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#26041;&#27861;&#24182;&#21033;&#29992;&#34920;&#31034;&#20998;&#24067;&#30340;&#36817;&#20284;&#65292;&#23454;&#29616;&#20102;&#22312;&#28145;&#24230;&#27169;&#22411;&#20013;&#28040;&#38500;&#24050;&#36951;&#24536;&#25968;&#25454;&#20449;&#24687;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2404.00506</link><description>&lt;p&gt;
&#19982;&#26631;&#31614;&#26080;&#20851;&#30340;&#36951;&#24536;:&#28145;&#24230;&#27169;&#22411;&#20013;&#26080;&#30417;&#30563;&#30340;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00506
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26631;&#31614;&#30340;&#26080;&#30417;&#30563;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#26041;&#27861;&#24182;&#21033;&#29992;&#34920;&#31034;&#20998;&#24067;&#30340;&#36817;&#20284;&#65292;&#23454;&#29616;&#20102;&#22312;&#28145;&#24230;&#27169;&#22411;&#20013;&#28040;&#38500;&#24050;&#36951;&#24536;&#25968;&#25454;&#20449;&#24687;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21435;&#23398;&#20064;&#26088;&#22312;&#20174;&#24050;&#36951;&#24536;&#25968;&#25454;&#20013;&#21024;&#38500;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#35757;&#32451;&#33391;&#22909;&#30340;&#27169;&#22411;&#20013;&#21097;&#20313;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25972;&#20010;&#21435;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#23436;&#20840;&#30417;&#30563;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#26631;&#27880;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#25152;&#38656;&#30340;&#24040;&#22823;&#25104;&#26412;&#65292;&#33719;&#24471;&#36825;&#31181;&#30417;&#30563;&#21487;&#33021;&#23454;&#38469;&#19978;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#36825;&#20010;&#25361;&#25112;&#20419;&#20351;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22312;&#21435;&#23398;&#20064;&#36807;&#31243;&#20013;&#26080;&#38656;&#26631;&#31614;&#30340;&#26080;&#30417;&#30563;&#21435;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#21464;&#20998;&#26041;&#27861;&#26469;&#36817;&#20284;&#21097;&#20313;&#25968;&#25454;&#30340;&#34920;&#31034;&#20998;&#24067;&#12290;&#21033;&#29992;&#36825;&#31181;&#36817;&#20284;&#65292;&#25105;&#20204;&#35843;&#25972;&#21407;&#22987;&#27169;&#22411;&#20197;&#22312;&#34920;&#31034;&#32423;&#21035;&#28040;&#38500;&#24050;&#36951;&#24536;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00506v1 Announce Type: new  Abstract: Machine unlearning aims to remove information derived from forgotten data while preserving that of the remaining dataset in a well-trained model. With the increasing emphasis on data privacy, several approaches to machine unlearning have emerged. However, these methods typically rely on complete supervision throughout the unlearning process. Unfortunately, obtaining such supervision, whether for the forgetting or remaining data, can be impractical due to the substantial cost associated with annotating real-world datasets. This challenge prompts us to propose a supervision-free unlearning approach that operates without the need for labels during the unlearning process. Specifically, we introduce a variational approach to approximate the distribution of representations for the remaining data. Leveraging this approximation, we adapt the original model to eliminate information from the forgotten data at the representation level. To further a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#37325;&#24314;&#38454;&#27573;&#21644;&#37325;&#24314;&#25439;&#22833;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#21644;&#29305;&#24449;&#34920;&#31034;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20849;&#21516;&#20449;&#24687;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35299;&#20915;&#30456;&#20851;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2404.00505</link><description>&lt;p&gt;
&#20855;&#26377;&#37325;&#24314;&#25439;&#22833;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning with Reconstruction Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#37325;&#24314;&#38454;&#27573;&#21644;&#37325;&#24314;&#25439;&#22833;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#21644;&#29305;&#24449;&#34920;&#31034;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20849;&#21516;&#20449;&#24687;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35299;&#20915;&#30456;&#20851;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25968;&#23398;&#20248;&#21270;&#30340;&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#20026;&#27599;&#20010;&#29305;&#23450;&#20248;&#21270;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#19987;&#29992;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#65292;&#21516;&#19968;&#32452;&#38382;&#39064;&#36755;&#20837;&#19978;&#32463;&#24120;&#38656;&#35201;&#20248;&#21270;&#20960;&#20010;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#30446;&#26631;&#25110;&#20219;&#21153;&#12290;&#19982;&#20026;&#27599;&#20010;&#38382;&#39064;&#21333;&#29420;&#35757;&#32451;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#36825;&#20123;&#30446;&#26631;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#21644;&#29305;&#24449;&#34920;&#31034;&#35757;&#32451;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26412;&#25991;&#39318;&#20808;&#24314;&#31435;&#20102;&#20849;&#21516;&#20449;&#24687;&#30340;&#27010;&#24565;&#65306;&#35299;&#20915;&#30456;&#20851;&#20219;&#21153;&#25152;&#38656;&#30340;&#20849;&#20139;&#30693;&#35782;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#37325;&#24314;&#38454;&#27573;&#20197;&#21450;&#30456;&#20851;&#30340;&#26032;&#37325;&#24314;&#25439;&#22833;&#12290;&#35813;&#25439;&#22833;&#29992;&#20110;&#20174;&#36873;&#25321;&#30340;&#38544;&#34255;&#29366;&#24577;&#24320;&#22987;&#37325;&#26032;&#26500;&#24314;&#20849;&#21516;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00505v1 Announce Type: cross  Abstract: In most applications of utilizing neural networks for mathematical optimization, a dedicated model is trained for each specific optimization objective. However, in many scenarios, several distinct yet correlated objectives or tasks often need to be optimized on the same set of problem inputs. Instead of independently training a different neural network for each problem separately, it would be more efficient to exploit the correlations between these objectives and to train multiple neural network models with shared model parameters and feature representations. To achieve this, this paper first establishes the concept of common information: the shared knowledge required for solving the correlated tasks, then proposes a novel approach for model training by adding into the model an additional reconstruction stage associated with a new reconstruction loss. This loss is for reconstructing the common information starting from a selected hidde
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26465;&#20214;&#20266;&#21487;&#36870;&#26631;&#20934;&#21270;&#27969;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#26500;&#24314;&#20195;&#29702;&#27169;&#22411;&#65292;&#26080;&#38656;&#20107;&#20808;&#20102;&#35299;&#22122;&#22768;&#21644;&#20989;&#25968;&#65292;&#30452;&#25509;&#23398;&#20064;&#24182;&#39640;&#25928;&#29983;&#25104;&#26679;&#26412;&#65292;&#29992;&#20110;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#20013;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#12290;</title><link>https://arxiv.org/abs/2404.00502</link><description>&lt;p&gt;
&#26465;&#20214;&#20266;&#21487;&#36870;&#26631;&#20934;&#21270;&#27969;&#22312;&#37327;&#21270;&#19981;&#30830;&#23450;&#20256;&#25773;&#20013;&#30340;&#20195;&#29702;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conditional Pseudo-Reversible Normalizing Flow for Surrogate Modeling in Quantifying Uncertainty Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00502
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26465;&#20214;&#20266;&#21487;&#36870;&#26631;&#20934;&#21270;&#27969;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#26500;&#24314;&#20195;&#29702;&#27169;&#22411;&#65292;&#26080;&#38656;&#20107;&#20808;&#20102;&#35299;&#22122;&#22768;&#21644;&#20989;&#25968;&#65292;&#30452;&#25509;&#23398;&#20064;&#24182;&#39640;&#25928;&#29983;&#25104;&#26679;&#26412;&#65292;&#29992;&#20110;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#20013;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26465;&#20214;&#20266;&#21487;&#36870;&#26631;&#20934;&#21270;&#27969;&#29992;&#20110;&#26500;&#24314;&#29289;&#29702;&#27169;&#22411;&#21463;&#38468;&#21152;&#22122;&#22768;&#27745;&#26579;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#26377;&#25928;&#37327;&#21270;&#27491;&#21521;&#21644;&#21453;&#21521;&#19981;&#30830;&#23450;&#20256;&#25773;&#12290;&#29616;&#26377;&#30340;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#36890;&#24120;&#19987;&#27880;&#20110;&#36817;&#20284;&#29289;&#29702;&#27169;&#22411;&#30340;&#30830;&#23450;&#24615;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#38656;&#35201;&#23545;&#22122;&#22768;&#26377;&#25152;&#20102;&#35299;&#65292;&#24182;&#19988;&#20511;&#21161;&#36741;&#21161;&#37319;&#26679;&#26041;&#27861;&#26469;&#37327;&#21270;&#21453;&#21521;&#19981;&#30830;&#23450;&#20256;&#25773;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26465;&#20214;&#20266;&#21487;&#36870;&#26631;&#20934;&#21270;&#27969;&#27169;&#22411;&#65292;&#20197;&#30452;&#25509;&#23398;&#20064;&#21644;&#39640;&#25928;&#22320;&#29983;&#25104;&#26679;&#26412;&#20174;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;&#35757;&#32451;&#36807;&#31243;&#21033;&#29992;&#30001;&#36755;&#20837;-&#36755;&#20986;&#23545;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#26080;&#38656;&#20107;&#20808;&#20102;&#35299;&#22122;&#22768;&#21644;&#20989;&#25968;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20174;&#20219;&#20309;&#39640;&#27010;&#29575;&#21306;&#22495;&#34987;&#35757;&#32451;&#35206;&#30422;&#30340;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20013;&#29983;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00502v1 Announce Type: new  Abstract: We introduce a conditional pseudo-reversible normalizing flow for constructing surrogate models of a physical model polluted by additive noise to efficiently quantify forward and inverse uncertainty propagation. Existing surrogate modeling approaches usually focus on approximating the deterministic component of physical model. However, this strategy necessitates knowledge of noise and resorts to auxiliary sampling methods for quantifying inverse uncertainty propagation. In this work, we develop the conditional pseudo-reversible normalizing flow model to directly learn and efficiently generate samples from the conditional probability density functions. The training process utilizes dataset consisting of input-output pairs without requiring prior knowledge about the noise and the function. Our model, once trained, can generate samples from any conditional probability density functions whose high probability regions are covered by the train
&lt;/p&gt;</description></item><item><title>&#22312;&#21333;&#20010;GPU&#19978;&#20197;3.29&#31186;&#23454;&#29616;CIFAR-10&#25968;&#25454;&#38598;94%&#20934;&#30830;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27700;&#24179;&#32763;&#36716;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00498</link><description>&lt;p&gt;
&#22312;&#21333;&#20010;GPU&#19978;&#20197;3.29&#31186;&#23454;&#29616;CIFAR-10&#25968;&#25454;&#38598;94%&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
94% on CIFAR-10 in 3.29 Seconds on a Single GPU
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00498
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21333;&#20010;GPU&#19978;&#20197;3.29&#31186;&#23454;&#29616;CIFAR-10&#25968;&#25454;&#38598;94%&#20934;&#30830;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27700;&#24179;&#32763;&#36716;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CIFAR-10&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#20043;&#19968;&#65292;&#27599;&#24180;&#26377;&#25968;&#21315;&#20010;&#30740;&#31350;&#39033;&#30446;&#20197;&#27492;&#20026;&#22522;&#30784;&#12290;&#20026;&#21152;&#36895;&#30740;&#31350;&#24182;&#38477;&#20302;&#23454;&#39564;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;CIFAR-10&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#20010;NVIDIA A100 GPU&#26102;&#65292;&#21487;&#20197;&#22312;3.29&#31186;&#20869;&#36798;&#21040;94%&#20934;&#30830;&#29575;&#65292;10.4&#31186;&#20869;&#36798;&#21040;95%&#65292;46.3&#31186;&#20869;&#36798;&#21040;96%&#12290;&#20316;&#20026;&#21152;&#36895;&#35757;&#32451;&#36895;&#24230;&#30340;&#19968;&#20010;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#38543;&#26426;&#21270;&#30340;&#27700;&#24179;&#32763;&#36716;&#22686;&#24378;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#22343;&#20248;&#20110;&#26631;&#20934;&#26041;&#27861;&#65292;&#21363;&#22312;&#36866;&#21512;&#27700;&#24179;&#32763;&#36716;&#30340;&#25152;&#26377;&#24773;&#20917;&#19979;&#22343;&#26377;&#25152;&#25913;&#21892;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#22312;https://github.com/KellerJordan/cifar10-airbench&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00498v1 Announce Type: new  Abstract: CIFAR-10 is among the most widely used datasets in machine learning, facilitating thousands of research projects per year. To accelerate research and reduce the cost of experiments, we introduce training methods for CIFAR-10 which reach 94% accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3 seconds, when run on a single NVIDIA A100 GPU. As one factor contributing to these training speeds, we propose a derandomized variant of horizontal flipping augmentation, which we show improves over the standard method in every case where flipping is beneficial over no flipping at all. Our code is released at https://github.com/KellerJordan/cifar10-airbench.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TEMPLE-MQA&#65292;&#36890;&#36807;&#26500;&#24314;&#26102;&#38388;&#24863;&#30693;&#22270;&#21644;&#25512;&#29702;&#36335;&#24452;&#12289;&#32467;&#26500;&#26816;&#32034;&#21644;&#32852;&#21512;&#25512;&#29702;&#38454;&#27573;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#26102;&#38388;&#32972;&#26223;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#36129;&#29486;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;TKEMQA&#12290;</title><link>https://arxiv.org/abs/2404.00492</link><description>&lt;p&gt;
&#26102;&#24577;&#30693;&#35782;&#32534;&#36753;&#19979;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Multi-hop Question Answering under Temporal Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00492
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TEMPLE-MQA&#65292;&#36890;&#36807;&#26500;&#24314;&#26102;&#38388;&#24863;&#30693;&#22270;&#21644;&#25512;&#29702;&#36335;&#24452;&#12289;&#32467;&#26500;&#26816;&#32034;&#21644;&#32852;&#21512;&#25512;&#29702;&#38454;&#27573;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#26102;&#38388;&#32972;&#26223;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#36129;&#29486;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;TKEMQA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572; (MQA) &#22312;&#30693;&#35782;&#32534;&#36753; (KE) &#19979;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MQA&#22312;&#22788;&#29702;&#21253;&#21547;&#26174;&#24335;&#26102;&#38388;&#32972;&#26223;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#26102;&#24577;&#30693;&#35782;&#22686;&#24378;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572; (TEMPLE-MQA)&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#65292;TEMPLE-MQA&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#26102;&#38388;&#24863;&#30693;&#22270; (TAG)&#65292;&#20197;&#32467;&#26500;&#21270;&#26041;&#24335;&#23384;&#20648;&#32534;&#36753;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#25512;&#29702;&#36335;&#24452;&#12289;&#32467;&#26500;&#26816;&#32034;&#21644;&#32852;&#21512;&#25512;&#29702;&#38454;&#27573;&#65292;TEMPLE-MQA&#26377;&#25928;&#22320;&#35782;&#21035;&#38382;&#39064;&#26597;&#35810;&#20013;&#30340;&#26102;&#38388;&#32972;&#26223;&#12290;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TEMPLE-MQA&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TKEMQA&#65292;&#19987;&#38376;&#20026;&#24102;&#26377;&#26102;&#38388;&#32422;&#26463;&#30340;MQA&#37327;&#36523;&#23450;&#21046;&#65292;&#20316;&#20026;&#39318;&#20010;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00492v1 Announce Type: cross  Abstract: Multi-hop question answering (MQA) under knowledge editing (KE) has garnered significant attention in the era of large language models. However, existing models for MQA under KE exhibit poor performance when dealing with questions containing explicit temporal contexts. To address this limitation, we propose a novel framework, namely TEMPoral knowLEdge augmented Multi-hop Question Answering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a time-aware graph (TAG) to store edit knowledge in a structured manner. Then, through our proposed inference path, structural retrieval, and joint reasoning stages, TEMPLE-MQA effectively discerns temporal contexts within the question query. Experiments on benchmark datasets demonstrate that TEMPLE-MQA significantly outperforms baseline models. Additionally, we contribute a new dataset, namely TKEMQA, which serves as the inaugural benchmark tailored specifically for MQA with temporal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PROMPT-SAW&#27169;&#22411;&#65292;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#26469;&#23454;&#29616;&#25991;&#26412;&#25552;&#31034;&#30340;&#21387;&#32553;&#65292;&#25552;&#39640;&#20102;&#25552;&#31034;&#30340;&#21487;&#35835;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00489</link><description>&lt;p&gt;
PROMPT-SAW&#65306;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#36827;&#34892;&#25991;&#26412;&#25552;&#31034;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PROMPT-SAW&#27169;&#22411;&#65292;&#21033;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#26469;&#23454;&#29616;&#25991;&#26412;&#25552;&#31034;&#30340;&#21387;&#32553;&#65292;&#25552;&#39640;&#20102;&#25552;&#31034;&#30340;&#21487;&#35835;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22810;&#31181;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#25552;&#31034;&#26159;LLM&#25512;&#29702;&#20013;&#30340;&#22522;&#26412;&#24037;&#20855;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36229;&#38271;&#25552;&#31034;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#25104;&#26412;&#12290;&#29616;&#26377;&#30340;&#21387;&#32553;&#38271;&#25552;&#31034;&#30340;&#23581;&#35797;&#23548;&#33268;&#21387;&#32553;&#25552;&#31034;&#22312;&#21487;&#35835;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#23545;&#25552;&#31034;&#25928;&#29992;&#20135;&#29983;&#26377;&#23475;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PROMPT-SAW&#65306;&#36890;&#36807;&#20851;&#31995;&#24863;&#30693;&#22270;&#36827;&#34892;&#25552;&#31034;&#21387;&#32553;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#24863;&#30693;&#25552;&#31034;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;PROMPT-SAW&#20351;&#29992;&#25552;&#31034;&#30340;&#25991;&#26412;&#20449;&#24687;&#26500;&#24314;&#22270;&#24418;&#65292;&#22312;&#22270;&#24418;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#20803;&#32032;&#65292;&#20174;&#32780;&#24471;&#20986;&#21387;&#32553;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;GSM8K-AUG&#65292;&#21363;&#29616;&#26377;GSM8k&#22522;&#20934;&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#29992;&#20110;&#20219;&#21153;&#19981;&#21487;&#30693;&#25552;&#31034;&#65292;&#20197;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00489v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown exceptional abilities for multiple different natural language processing tasks. While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead to sub-standard results in terms of readability and interpretability of the compressed prompt, with a detrimental impact on prompt utility. To address this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#20351;&#29992;&#24369;&#26631;&#35760;&#25991;&#26723;&#35757;&#32451;&#25552;&#21462;&#22120;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#22312;&#20225;&#19994;&#22330;&#26223;&#20013;&#35757;&#32451;&#22823;&#37327;&#19981;&#21516;&#25991;&#26723;&#31867;&#22411;&#30340;&#33258;&#23450;&#20041;&#25552;&#21462;&#22120;&#25152;&#38656;&#30340;&#26114;&#36149;&#20154;&#24037;&#26631;&#35760;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.00488</link><description>&lt;p&gt;
&#22122;&#22768;&#24863;&#30693;&#30340;&#24067;&#23616;&#24863;&#30693;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Noise-Aware Training of Layout-Aware Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#20351;&#29992;&#24369;&#26631;&#35760;&#25991;&#26723;&#35757;&#32451;&#25552;&#21462;&#22120;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#22312;&#20225;&#19994;&#22330;&#26223;&#20013;&#35757;&#32451;&#22823;&#37327;&#19981;&#21516;&#25991;&#26723;&#31867;&#22411;&#30340;&#33258;&#23450;&#20041;&#25552;&#21462;&#22120;&#25152;&#38656;&#30340;&#26114;&#36149;&#20154;&#24037;&#26631;&#35760;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#65288;VRD&#65289;&#21033;&#29992;&#35270;&#35273;&#29305;&#24449;&#21644;&#35821;&#35328;&#32447;&#32034;&#20256;&#25773;&#20449;&#24687;&#12290;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#20174;&#25991;&#26723;&#20013;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#33258;&#23450;&#20041;&#25552;&#21462;&#22120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#20026;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#30446;&#26631;&#25991;&#26723;&#23454;&#20363;&#12290;&#22312;&#20225;&#19994;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#20026;&#25104;&#21315;&#19978;&#19975;&#31181;&#19981;&#21516;&#25991;&#26723;&#31867;&#22411;&#35757;&#32451;&#33258;&#23450;&#20041;&#25552;&#21462;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#29942;&#39048;&#12290;&#22312;&#26410;&#26631;&#35760;&#30446;&#26631;&#25991;&#26723;&#23454;&#20363;&#19978;&#39044;&#35757;&#32451;&#25552;&#21462;&#22120;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#20154;&#24037;&#26631;&#35760;&#23454;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#26159;&#34892;&#19981;&#36890;&#30340;&#65292;&#22240;&#20026;&#23427;&#36229;&#20986;&#20102;&#20026;&#25552;&#21462;&#22120;&#20998;&#37197;&#30340;&#26368;&#22823;&#20801;&#35768;&#35757;&#32451;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65288;NAT&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#22330;&#26223;&#12290;NAT&#21033;&#29992;&#24369;&#26631;&#35760;&#25991;&#26723;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#35757;&#32451;&#25552;&#21462;&#22120;&#65292;&#32780;&#19981;&#26159;&#33719;&#21462;&#26114;&#36149;&#30340;&#20154;&#24037;&#26631;&#35760;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00488v1 Announce Type: cross  Abstract: A visually rich document (VRD) utilizes visual features along with linguistic cues to disseminate information. Training a custom extractor that identifies named entities from a document requires a large number of instances of the target document type annotated at textual and visual modalities. This is an expensive bottleneck in enterprise scenarios, where we want to train custom extractors for thousands of different document types in a scalable way. Pre-training an extractor model on unlabeled instances of the target document type, followed by a fine-tuning step on human-labeled instances does not work in these scenarios, as it surpasses the maximum allowable training time allocated for the extractor. We address this scenario by proposing a Noise-Aware Training method or NAT in this paper. Instead of acquiring expensive human-labeled documents, NAT utilizes weakly labeled documents to train an extractor in a scalable way. To avoid degr
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#29992;&#20110;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#30340;&#35782;&#21035;&#12289;&#20998;&#31867;&#12289;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;</title><link>https://arxiv.org/abs/2404.00482</link><description>&lt;p&gt;
&#29992;&#20110;&#26031;&#25289;&#22827;&#35821;&#30340;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Named Entity Corpus for Slavic Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00482
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#29992;&#20110;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#30340;&#35782;&#21035;&#12289;&#20998;&#31867;&#12289;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#21547;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#65288;&#20445;&#21152;&#21033;&#20122;&#35821;&#12289;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#12289;&#26031;&#27931;&#25991;&#23612;&#20122;&#35821;&#12289;&#20420;&#35821;&#21644;&#20044;&#20811;&#20848;&#35821;&#65289;&#21629;&#21517;&#23454;&#20307;&#30340;&#35821;&#26009;&#24211;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;2017-2023&#24180;&#38388;&#26031;&#25289;&#22827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#35752;&#20250;&#30340;&#19968;&#31995;&#21015;&#20849;&#20139;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#20102;5017&#20221;&#28085;&#30422;&#19971;&#20010;&#20027;&#39064;&#30340;&#25991;&#26723;&#65292;&#25991;&#26723;&#26631;&#26377;&#20116;&#31867;&#21629;&#21517;&#23454;&#20307;&#65292;&#27599;&#20010;&#23454;&#20307;&#30001;&#31867;&#21035;&#12289;&#24341;&#29992;&#35789;&#21644;&#21807;&#19968;&#36328;&#35821;&#35328;&#26631;&#35782;&#31526;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#35843;&#25972;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998; - &#21333;&#20010;&#20027;&#39064;&#21010;&#20998;&#21644;&#36328;&#20027;&#39064;&#21010;&#20998;&#12290;&#23545;&#20110;&#27599;&#20010;&#21010;&#20998;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#32622;&#20102;&#22522;&#20934;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;XLM-RoBERTa-large&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#25552;&#21450;&#35782;&#21035;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;mT5-large&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00482v1 Announce Type: cross  Abstract: This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19981;&#31561;&#26465;&#20214;&#30340;&#39069;&#22806;&#20107;&#20214;&#65292;&#23558;&#26465;&#20214;&#27010;&#29575;&#36716;&#21270;&#20026;&#29305;&#27530;&#31215;&#20998;&#24418;&#24335;&#65292;&#25512;&#24191;&#20026;&#21367;&#31215;&#24418;&#24335;&#30340;&#26032;&#28388;&#27874;&#26694;&#26550;&#65292;&#31216;&#20026;&#21367;&#31215;&#36125;&#21494;&#26031;&#28388;&#27874;&#12290;</title><link>https://arxiv.org/abs/2404.00481</link><description>&lt;p&gt;
&#21367;&#31215;&#36125;&#21494;&#26031;&#28388;&#27874;
&lt;/p&gt;
&lt;p&gt;
Convolutional Bayesian Filtering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00481
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19981;&#31561;&#26465;&#20214;&#30340;&#39069;&#22806;&#20107;&#20214;&#65292;&#23558;&#26465;&#20214;&#27010;&#29575;&#36716;&#21270;&#20026;&#29305;&#27530;&#31215;&#20998;&#24418;&#24335;&#65292;&#25512;&#24191;&#20026;&#21367;&#31215;&#24418;&#24335;&#30340;&#26032;&#28388;&#27874;&#26694;&#26550;&#65292;&#31216;&#20026;&#21367;&#31215;&#36125;&#21494;&#26031;&#28388;&#27874;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#28388;&#27874;&#26159;&#21160;&#24577;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#30340;&#20027;&#35201;&#26694;&#26550;&#12290;&#26631;&#20934;&#29256;&#26412;&#21033;&#29992;&#20840;&#27010;&#29575;&#35268;&#21017;&#21644;&#36125;&#21494;&#26031;&#23450;&#29702;&#20132;&#26367;&#20351;&#29992;&#65292;&#32780;&#23450;&#20041;&#21644;&#35745;&#31639;&#26465;&#20214;&#27010;&#29575;&#30340;&#26041;&#24335;&#23545;&#29366;&#24577;&#20998;&#24067;&#25512;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#21069;&#65292;&#26465;&#20214;&#27010;&#29575;&#34987;&#20551;&#23450;&#20026;&#31934;&#30830;&#24050;&#30693;&#65292;&#20195;&#34920;&#19968;&#20010;&#20107;&#20214;&#21457;&#29983;&#27010;&#29575;&#32473;&#23450;&#31532;&#20108;&#20010;&#20107;&#20214;&#30340;&#24230;&#37327;&#12290;&#26412;&#25991;&#21457;&#29616;&#36890;&#36807;&#28155;&#21152;&#19968;&#20010;&#35268;&#23450;&#19981;&#31561;&#26465;&#20214;&#30340;&#39069;&#22806;&#20107;&#20214;&#65292;&#21487;&#20197;&#23558;&#26465;&#20214;&#27010;&#29575;&#36716;&#21270;&#20026;&#31867;&#20284;&#20110;&#21367;&#31215;&#30340;&#29305;&#27530;&#31215;&#20998;&#24418;&#24335;&#12290;&#22522;&#20110;&#36825;&#19968;&#36716;&#21270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36807;&#28193;&#27010;&#29575;&#21644;&#36755;&#20986;&#27010;&#29575;&#22343;&#21487;&#20197;&#25512;&#24191;&#20026;&#21367;&#31215;&#24418;&#24335;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#28388;&#27874;&#26694;&#26550;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21367;&#31215;&#36125;&#21494;&#26031;&#28388;&#27874;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00481v1 Announce Type: cross  Abstract: Bayesian filtering serves as the mainstream framework of state estimation in dynamic systems. Its standard version utilizes total probability rule and Bayes' law alternatively, where how to define and compute conditional probability is critical to state distribution inference. Previously, the conditional probability is assumed to be exactly known, which represents a measure of the occurrence probability of one event, given the second event. In this paper, we find that by adding an additional event that stipulates an inequality condition, we can transform the conditional probability into a special integration that is analogous to convolution. Based on this transformation, we show that both transition probability and output probability can be generalized to convolutional forms, resulting in a more general filtering framework that we call convolutional Bayesian filtering. This new framework encompasses standard Bayesian filtering as a spe
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#24072;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;DE-HNN&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;&#20110;&#30005;&#36335;&#32593;&#34920;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#20248;&#21270;&#24037;&#20855;&#36816;&#34892;&#26102;&#38388;&#38271;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00477</link><description>&lt;p&gt;
DE-HNN: &#19968;&#31181;&#29992;&#20110;&#30005;&#36335;&#32593;&#34920;&#34920;&#31034;&#30340;&#26377;&#25928;&#31070;&#32463;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DE-HNN: An effective neural model for Circuit Netlist representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00477
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#24072;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;DE-HNN&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;&#20110;&#30005;&#36335;&#32593;&#34920;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#20248;&#21270;&#24037;&#20855;&#36816;&#34892;&#26102;&#38388;&#38271;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#24037;&#20855;&#30340;&#36816;&#34892;&#26102;&#38388;&#38543;&#30528;&#35774;&#35745;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#32780;&#22686;&#38271;&#65292;&#21040;&#20102;&#21487;&#20197;&#33457;&#36153;&#25968;&#22825;&#26469;&#23436;&#25104;&#19968;&#20010;&#35774;&#35745;&#21608;&#26399;&#30340;&#22320;&#27493;&#65292;&#36825;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#29942;&#39048;&#12290;&#35774;&#35745;&#24072;&#20204;&#24076;&#26395;&#33021;&#22815;&#24555;&#36895;&#33719;&#24471;&#35774;&#35745;&#21453;&#39304;&#30340;&#24037;&#20855;&#12290;&#36890;&#36807;&#20351;&#29992;&#36807;&#21435;&#35774;&#35745;&#30340;&#24037;&#20855;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#25968;&#25454;&#65292;&#21487;&#20197;&#23581;&#35797;&#26500;&#24314;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#26174;&#33879;&#36739;&#30701;&#30340;&#26102;&#38388;&#39044;&#27979;&#35774;&#35745;&#32467;&#26524;&#65292;&#36825;&#27604;&#36816;&#34892;&#24037;&#20855;&#35201;&#24555;&#24471;&#22810;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21463;&#21040;&#35774;&#35745;&#25968;&#25454;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#26159;&#25551;&#36848;&#25968;&#23383;&#30005;&#36335;&#20803;&#32032;&#21450;&#20854;&#36830;&#25509;&#26041;&#24335;&#30340;&#32593;&#34920;&#12290;&#32593;&#34920;&#30340;&#22270;&#34920;&#31034;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#30740;&#31350;&#29992;&#20110;&#36825;&#31181;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33410;&#28857;&#25968;&#37327;&#20247;&#22810;&#21644;&#36828;&#31243;&#36830;&#25509;&#30340;&#37325;&#35201;&#24615;&#65292;&#32593;&#34920;&#30340;&#29305;&#24615;&#32473;&#29616;&#26377;&#22270;&#23398;&#20064;&#26694;&#26550;&#24102;&#26469;&#20102;&#20960;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00477v1 Announce Type: new  Abstract: The run-time for optimization tools used in chip design has grown with the complexity of designs to the point where it can take several days to go through one design cycle which has become a bottleneck. Designers want fast tools that can quickly give feedback on a design. Using the input and output data of the tools from past designs, one can attempt to build a machine learning model that predicts the outcome of a design in significantly shorter time than running the tool. The accuracy of such models is affected by the representation of the design data, which is usually a netlist that describes the elements of the digital circuit and how they are connected. Graph representations for the netlist together with graph neural networks have been investigated for such models. However, the characteristics of netlists pose several challenges for existing graph learning frameworks, due to the large number of nodes and the importance of long-range 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#26469;&#23454;&#29616;&#35821;&#35328;&#26657;&#20934;&#65292;&#21487;&#20197;&#20351;&#29992;&#25143;&#20570;&#20986;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00474</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Linguistic Calibration of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#26469;&#23454;&#29616;&#35821;&#35328;&#26657;&#20934;&#65292;&#21487;&#20197;&#20351;&#29992;&#25143;&#20570;&#20986;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#22312;&#33258;&#20449;&#24187;&#35273;&#26102;&#23548;&#33268;&#29992;&#25143;&#20570;&#20986;&#27425;&#20248;&#21270;&#30340;&#19979;&#28216;&#20915;&#31574;&#12290;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#21475;&#22836;&#20256;&#36798;&#20854;&#20027;&#24352;&#27491;&#30830;&#27010;&#29575;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#29983;&#25104;&#20855;&#26377;&#26657;&#20934;&#32622;&#20449;&#24230;&#22768;&#26126;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#20915;&#31574;&#35282;&#24230;&#65292;&#20026;&#38271;&#31687;&#29983;&#25104;&#24418;&#24335;&#30340;&#35821;&#35328;&#26657;&#20934;&#24418;&#24335;&#21270;&#23450;&#20041;&#65306;&#22914;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#20351;&#20854;&#29992;&#25143;&#33021;&#22815;&#20570;&#20986;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#65292;&#21017;&#35813;&#27169;&#22411;&#26159;&#35821;&#35328;&#19978;&#26657;&#20934;&#30340;&#12290;&#36825;&#20010;&#23450;&#20041;&#20351;&#24471;&#19968;&#20010;&#35757;&#32451;&#26694;&#26550;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;&#19968;&#20010;&#30417;&#30563;&#24494;&#35843;&#27493;&#39588;&#24341;&#23548;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#21457;&#20986;&#24102;&#26377;&#32622;&#20449;&#24230;&#22768;&#26126;&#30340;&#38271;&#31687;&#29983;&#25104;&#65292;&#35832;&#22914;&#8220;&#25105;&#20272;&#35745;&#26377;30%&#30340;&#26426;&#20250;&#8230;&#8221;&#25110;&#8220;&#25105;&#30830;&#20449;&#8230;&#8221;&#65292;&#28982;&#21518;&#26159;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#27493;&#39588;&#65292;&#22870;&#21169;&#20351;&#29992;&#25143;&#33021;&#22815;&#23545;&#30456;&#20851;&#38382;&#39064;&#25552;&#20379;&#26657;&#20934;&#31572;&#26696;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#23545;Llama 2 7B &#36827;&#34892;&#35821;&#35328;&#26657;&#20934;&#65292;&#24182;&#21457;&#29616;&#22312;&#33258;&#21160;&#21270;&#21644;&#20154;&#31867;&#27979;&#35797;&#20013;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00474v1 Announce Type: cross  Abstract: Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as "I estimate a 30% chance of..." or "I am certain that...", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and huma
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#36973;&#21040;&#31713;&#25913;&#21518;&#65292;&#21487;&#20197;&#26500;&#24314;&#38544;&#31169;&#21518;&#38376;&#65292;&#23436;&#20840;&#25439;&#23475;&#24494;&#35843;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#65292;&#36827;&#32780;&#23545;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#20005;&#26684;&#30340;&#38544;&#31169;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2404.00473</link><description>&lt;p&gt;
&#38544;&#31169;&#21518;&#38376;&#65306;&#20351;&#29992;&#25439;&#22351;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#31363;&#21462;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Privacy Backdoors: Stealing Data with Corrupted Pretrained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00473
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#36973;&#21040;&#31713;&#25913;&#21518;&#65292;&#21487;&#20197;&#26500;&#24314;&#38544;&#31169;&#21518;&#38376;&#65292;&#23436;&#20840;&#25439;&#23475;&#24494;&#35843;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#65292;&#36827;&#32780;&#23545;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#20005;&#26684;&#30340;&#38544;&#31169;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#32773;&#36890;&#24120;&#20174;&#24320;&#25918;&#20179;&#24211;&#19979;&#36733;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#24494;&#35843;&#20197;&#36866;&#24212;&#29305;&#23450;&#24212;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#23454;&#36341;&#24341;&#20837;&#20102;&#26032;&#30340;&#38544;&#31169;&#21518;&#38376;&#39118;&#38505;&#12290;&#36890;&#36807;&#31713;&#25913;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23436;&#20840;&#25439;&#23475;&#24494;&#35843;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20026;&#21508;&#31181;&#27169;&#22411;&#26500;&#24314;&#38544;&#31169;&#21518;&#38376;&#65292;&#21253;&#25324;transformers&#65292;&#20351;&#25915;&#20987;&#32773;&#33021;&#22815;&#37325;&#26500;&#21333;&#20010;&#24494;&#35843;&#26679;&#26412;&#65292;&#19988;&#25104;&#21151;&#25285;&#20445;&#65281;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#24102;&#26377;&#21518;&#38376;&#30340;&#27169;&#22411;&#20801;&#35768;&#23545;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#20005;&#26684;&#30340;&#38544;&#31169;&#25915;&#20987;&#12290;&#22914;&#26524;&#27169;&#22411;&#19981;&#21463;&#20449;&#20219;&#65292;&#21017;&#20856;&#22411;&#30340;&#29992;&#23485;&#26494;&#38544;&#31169;&#20445;&#35777;&#35757;&#32451;DP&#27169;&#22411;&#30340;&#20048;&#35266;&#20570;&#27861;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#38544;&#31169;&#30340;&#19968;&#31181;&#20851;&#38190;&#19988;&#34987;&#24573;&#35270;&#30340;&#20379;&#24212;&#38142;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00473v1 Announce Type: cross  Abstract: Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#20809;&#22768; tomography &#22270;&#20687;&#37325;&#24314;&#20013;&#30001;&#20110;&#26377;&#38480;&#20256;&#24863;&#22120;&#35206;&#30422;&#25110;&#25442;&#33021;&#22120;&#23494;&#24230;&#19981;&#36275;&#32780;&#23548;&#33268;&#30340;&#36870;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25442;&#33021;&#22120;&#31232;&#30095;&#26465;&#20214;&#19979;&#20445;&#25345;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00471</link><description>&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#20809;&#22768; tomography &#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Score-Based Diffusion Models for Photoacoustic Tomography Image Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#20809;&#22768; tomography &#22270;&#20687;&#37325;&#24314;&#20013;&#30001;&#20110;&#26377;&#38480;&#20256;&#24863;&#22120;&#35206;&#30422;&#25110;&#25442;&#33021;&#22120;&#23494;&#24230;&#19981;&#36275;&#32780;&#23548;&#33268;&#30340;&#36870;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25442;&#33021;&#22120;&#31232;&#30095;&#26465;&#20214;&#19979;&#20445;&#25345;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#22768;&#26029;&#23618;&#25195;&#25551;&#65288;PAT&#65289;&#26159;&#19968;&#31181;&#36805;&#36895;&#21457;&#23637;&#30340;&#21307;&#23398;&#25104;&#20687;&#25216;&#26415;&#65292;&#23427;&#23558;&#20809;&#23398;&#21560;&#25910;&#23545;&#27604;&#19982;&#36229;&#22768;&#25104;&#20687;&#28145;&#24230;&#30456;&#32467;&#21512;&#12290;PAT &#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#30001;&#20110;&#20256;&#24863;&#22120;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#25110;&#25442;&#33021;&#22120;&#38453;&#21015;&#30340;&#23494;&#24230;&#19981;&#36275;&#32780;&#23548;&#33268;&#22768;&#23398;&#20449;&#21495;&#19981;&#36275;&#65292;&#36825;&#31181;&#24773;&#20917;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#36870;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35780;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#20174;&#26377;&#38480;&#30340;PAT&#27979;&#37327;&#20013;&#37325;&#24314;&#22270;&#20687;&#30340;&#36870;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#22312;&#23545;&#27169;&#25311;&#34880;&#31649;&#32467;&#26500;&#36827;&#34892;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#25442;&#33021;&#22120;&#31232;&#30095;&#26465;&#20214;&#19979;&#20173;&#28982;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00471v1 Announce Type: cross  Abstract: Photoacoustic tomography (PAT) is a rapidly-evolving medical imaging modality that combines optical absorption contrast with ultrasound imaging depth. One challenge in PAT is image reconstruction with inadequate acoustic signals due to limited sensor coverage or due to the density of the transducer array. Such cases call for solving an ill-posed inverse reconstruction problem. In this work, we use score-based diffusion models to solve the inverse problem of reconstructing an image from limited PAT measurements. The proposed approach allows us to incorporate an expressive prior learned by a diffusion model on simulated vessel structures while still being robust to varying transducer sparsity conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21464;&#21387;&#22120;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#20799;&#31461;&#24515;&#38899;&#36827;&#34892;&#30701;&#27573;&#20998;&#31867;&#65292;&#35843;&#26597;&#20102;&#33258;&#21160;&#20998;&#31867;&#25152;&#38656;&#30340;&#26368;&#23567;&#20449;&#21495;&#25345;&#32493;&#26102;&#38388;&#65292;&#24182;&#30830;&#23450;&#20102;&#36866;&#21512;RMSSD&#21644;ZCR&#25351;&#26631;&#30340;&#29702;&#24819;&#38408;&#20540;&#12290;</title><link>https://arxiv.org/abs/2404.00470</link><description>&lt;p&gt;
&#22522;&#20110;&#21464;&#21387;&#22120;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20799;&#31461;&#24515;&#38899;&#30701;&#27573;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Short Segment Pediatric Heart Sounds Based on a Transformer-Based Convolutional Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21464;&#21387;&#22120;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#20799;&#31461;&#24515;&#38899;&#36827;&#34892;&#30701;&#27573;&#20998;&#31867;&#65292;&#35843;&#26597;&#20102;&#33258;&#21160;&#20998;&#31867;&#25152;&#38656;&#30340;&#26368;&#23567;&#20449;&#21495;&#25345;&#32493;&#26102;&#38388;&#65292;&#24182;&#30830;&#23450;&#20102;&#36866;&#21512;RMSSD&#21644;ZCR&#25351;&#26631;&#30340;&#29702;&#24819;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#22825;&#24615;&#24515;&#33039;&#30149;&#25110;CHD&#26159;&#30001;&#24515;&#33039;&#21644;&#22823;&#34880;&#31649;&#32467;&#26500;&#32570;&#38519;&#23548;&#33268;&#30340;&#20808;&#22825;&#24615;&#24322;&#24120;&#12290;&#24515;&#33039;&#38899;&#39057;&#20449;&#21495;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#24515;&#33039;&#26426;&#26800;&#20256;&#23548;&#31995;&#32479;&#30340;&#37325;&#35201;&#32454;&#33410;&#65292;&#24182;&#25351;&#20986;&#19982;&#19981;&#21516;&#31867;&#22411;CHD&#30456;&#20851;&#32852;&#30340;&#29305;&#23450;&#27169;&#24335;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#33258;&#21160;&#20998;&#31867;&#24515;&#38899;&#25152;&#38656;&#30340;&#26368;&#23567;&#20449;&#21495;&#25345;&#32493;&#26102;&#38388;&#12290;&#35813;&#30740;&#31350;&#36824;&#35843;&#26597;&#20102;&#26368;&#20339;&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#65288;&#36830;&#32493;&#24046;&#30340;&#22343;&#26041;&#26681;&#20540;&#65289;RMSSD&#21644;&#65288;&#36807;&#38646;&#29575;&#65289;ZCR&#20540;&#12290;&#22522;&#20110;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#65288;MFCCs&#65289;&#30340;&#29305;&#24449;&#34987;&#29992;&#20316;&#36755;&#20837;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27531;&#20313;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20998;&#31867;&#24515;&#38899;&#12290;&#30740;&#31350;&#34920;&#26126;0.4&#26159;&#33719;&#21462;&#36866;&#21512;RMSSD&#21644;ZCR&#25351;&#26631;&#30340;&#20449;&#21495;&#30340;&#29702;&#24819;&#38408;&#20540;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22312;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#24515;&#38899;&#20998;&#31867;&#32467;&#26524;&#65292;&#30740;&#31350;&#30830;&#23450;&#20102;&#26368;&#20339;&#20449;&#21495;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00470v1 Announce Type: cross  Abstract: Congenital anomalies arising as a result of a defect in the structure of the heart and great vessels are known as congenital heart diseases or CHDs. A PCG can provide essential details about the mechanical conduction system of the heart and point out specific patterns linked to different kinds of CHD. This study aims to investigate the minimum signal duration required for the automatic classification of heart sounds. This study also investigated the optimum signal quality assessment indicator (Root Mean Square of Successive Differences) RMSSD and (Zero Crossings Rate) ZCR value. Mel-frequency cepstral coefficients (MFCCs) based feature is used as an input to build a Transformer-Based residual one-dimensional convolutional neural network, which is then used for classifying the heart sound. The study showed that 0.4 is the ideal threshold for getting suitable signals for the RMSSD and ZCR indicators. Moreover, a minimum signal length of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#36731;&#37327;&#32423;&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;&#65288;LVFL&#65289;&#30340;&#27010;&#24565;&#65292;&#38024;&#23545;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#37319;&#29992;&#20998;&#31163;&#30340;&#36731;&#37327;&#21270;&#31574;&#30053;&#65292;&#24314;&#31435;&#20102;&#25910;&#25947;&#30028;&#38480;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;</title><link>https://arxiv.org/abs/2404.00466</link><description>&lt;p&gt;
&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#36731;&#37327;&#32423;&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Computation and Communication Efficient Lightweighting Vertical Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00466
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#36731;&#37327;&#32423;&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;&#65288;LVFL&#65289;&#30340;&#27010;&#24565;&#65292;&#38024;&#23545;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#37319;&#29992;&#20998;&#31163;&#30340;&#36731;&#37327;&#21270;&#31574;&#30053;&#65292;&#24314;&#31435;&#20102;&#25910;&#25947;&#30028;&#38480;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#25506;&#32034;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#24050;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#21644;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21162;&#21147;&#37117;&#38598;&#20013;&#22312;&#25552;&#39640;&#36825;&#20123;&#25928;&#29575;&#65292;&#20294;&#30001;&#20110;&#22402;&#30452;FL&#30340;&#19981;&#21516;&#36807;&#31243;&#21644;&#27169;&#22411;&#32467;&#26500;&#65292;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#22522;&#20110;&#27700;&#24179;FL&#30340;&#25216;&#26415;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36731;&#37327;&#32423;&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;&#65288;LVFL&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#25552;&#39640;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#38024;&#23545;&#29305;&#24449;&#27169;&#22411;&#30340;&#21333;&#29420;&#36731;&#37327;&#21270;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#38024;&#23545;&#29305;&#24449;&#23884;&#20837;&#36827;&#34892;&#36731;&#37327;&#21270;&#65292;&#20197;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;LVFL&#31639;&#27861;&#24314;&#31435;&#20102;&#25910;&#25947;&#30028;&#38480;&#65292;&#32771;&#34385;&#20102;&#36890;&#20449;&#21644;&#35745;&#31639;&#36731;&#37327;&#21270;&#27604;&#29575;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23545;&#35813;&#31639;&#27861;&#36827;&#34892;&#35780;&#20272;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LVFL&#26174;&#33879;&#20943;&#36731;&#20102;c
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00466v1 Announce Type: new  Abstract: The exploration of computational and communication efficiency within Federated Learning (FL) has emerged as a prominent and crucial field of study. While most existing efforts to enhance these efficiencies have focused on Horizontal FL, the distinct processes and model structures of Vertical FL preclude the direct application of Horizontal FL-based techniques. In response, we introduce the concept of Lightweight Vertical Federated Learning (LVFL), targeting both computational and communication efficiencies. This approach involves separate lightweighting strategies for the feature model, to improve computational efficiency, and for feature embedding, to enhance communication efficiency. Moreover, we establish a convergence bound for our LVFL algorithm, which accounts for both communication and computational lightweighting ratios. Our evaluation of the algorithm on a image classification dataset reveals that LVFL significantly alleviates c
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#22522;&#20110;Transformer&#30340;&#23884;&#20837;&#65292;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21450;&#30456;&#20851;&#30196;&#21574;&#24739;&#32773;&#36827;&#34892;&#29305;&#24449;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#30142;&#30149;&#20154;&#32676;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#20122;&#22411;&#30340;&#20020;&#24202;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2404.00464</link><description>&lt;p&gt;
&#21033;&#29992;&#26469;&#33258;&#30005;&#23376;&#30149;&#21382;&#30340;&#39044;&#35757;&#32451;&#21644;&#22522;&#20110;Transformer&#30340;&#23884;&#20837;&#26469;&#34920;&#24449;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21450;&#30456;&#20851;&#30196;&#21574;&#30340;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Pre-trained and Transformer-derived Embeddings from EHRs to Characterize Heterogeneity Across Alzheimer's Disease and Related Dementias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00464
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#22522;&#20110;Transformer&#30340;&#23884;&#20837;&#65292;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21450;&#30456;&#20851;&#30196;&#21574;&#24739;&#32773;&#36827;&#34892;&#29305;&#24449;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#30142;&#30149;&#20154;&#32676;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#20122;&#22411;&#30340;&#20020;&#24202;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#31181;&#36827;&#34892;&#24615;&#30340;&#12289;&#20351;&#20154;&#34928;&#24369;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#20840;&#29699;&#26377;5000&#19975;&#20154;&#21463;&#24433;&#21709;&#12290;&#23613;&#31649;&#36825;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#20581;&#24247;&#36127;&#25285;&#65292;&#20294;&#30446;&#21069;&#27835;&#30103;&#25163;&#27573;&#26377;&#38480;&#65292;&#20854;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#23384;&#22312;&#20020;&#24202;&#24847;&#20041;&#30340;&#20122;&#22411;&#65292;&#21487;&#33021;&#23545;&#24212;&#20110;&#19981;&#21516;&#30340;&#30149;&#22240;&#12289;&#30142;&#30149;&#36827;&#31243;&#65292;&#20197;&#21450;&#26368;&#32456;&#30340;&#36866;&#24403;&#27835;&#30103;&#12290;&#26412;&#25991;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#23545;&#19968;&#32452;&#35760;&#24518;&#38556;&#30861;&#24739;&#32773;&#30340;&#30005;&#23376;&#30149;&#21382;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#34920;&#24449;&#35813;&#30142;&#30149;&#20154;&#32676;&#30340;&#24322;&#36136;&#24615;&#12290;&#21033;&#29992;&#21307;&#23398;&#20195;&#30721;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#20197;&#21450;&#22522;&#20110;Transformer&#30340;&#20020;&#24202;BERT&#23884;&#20837;&#26469;&#23545;&#24739;&#32773;&#30340;&#30005;&#23376;&#30149;&#21382;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#22522;&#20110;&#21512;&#24182;&#30151;&#21644;&#20849;&#20139;&#25991;&#26412;&#29305;&#24449;&#30340;&#23384;&#22312;&#65292;&#30830;&#23450;&#20102;&#20122;&#32676;&#20307;&#30340;&#23384;&#22312;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20020;&#24202;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00464v1 Announce Type: new  Abstract: Alzheimer's disease is a progressive, debilitating neurodegenerative disease that affects 50 million people globally. Despite this substantial health burden, available treatments for the disease are limited and its fundamental causes remain poorly understood. Previous work has suggested the existence of clinically-meaningful sub-types, which it is suggested may correspond to distinct etiologies, disease courses, and ultimately appropriate treatments. Here, we use unsupervised learning techniques on electronic health records (EHRs) from a cohort of memory disorder patients to characterise heterogeneity in this disease population. Pre-trained embeddings for medical codes as well as transformer-derived Clinical BERT embeddings of free text are used to encode patient EHRs. We identify the existence of sub-populations on the basis of comorbidities and shared textual features, and discuss their clinical significance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;NLP&#27169;&#22411;&#20013;&#21516;&#26102;&#22788;&#29702;&#32479;&#35745;&#21644;&#22240;&#26524;&#24615;&#21035;&#20844;&#24179;&#24615;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#32467;&#21512;&#32479;&#35745;&#21644;&#22240;&#26524;&#24615;&#21435;&#20559;&#32622;&#25216;&#26415;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2404.00463</link><description>&lt;p&gt;
&#22312;NLP&#27169;&#22411;&#20013;&#35299;&#20915;&#32479;&#35745;&#21644;&#22240;&#26524;&#24615;&#21035;&#20844;&#24179;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Both Statistical and Causal Gender Fairness in NLP Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;NLP&#27169;&#22411;&#20013;&#21516;&#26102;&#22788;&#29702;&#32479;&#35745;&#21644;&#22240;&#26524;&#24615;&#21035;&#20844;&#24179;&#24615;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#32467;&#21512;&#32479;&#35745;&#21644;&#22240;&#26524;&#24615;&#21435;&#20559;&#32622;&#25216;&#26415;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#20844;&#24179;&#24615;&#35268;&#23450;&#23545;&#27599;&#20010;&#21463;&#20445;&#25252;&#32676;&#20307;&#26377;&#30456;&#21516;&#30340;&#32467;&#26524;&#65292;&#32780;&#22240;&#26524;&#20844;&#24179;&#24615;&#35201;&#27714;&#27169;&#22411;&#23545;&#20010;&#20307;&#30340;&#39044;&#27979;&#19981;&#21463;&#20854;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65288;CDA&#65289;&#23545;&#20110;&#20943;&#23569;NLP&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#26159;&#26377;&#25928;&#30340;&#65292;&#28982;&#32780;&#20351;&#29992;CDA&#35757;&#32451;&#30340;&#27169;&#22411;&#36890;&#24120;&#21482;&#22522;&#20110;&#19982;&#22240;&#26524;&#20844;&#24179;&#24615;&#27010;&#24565;&#23494;&#20999;&#30456;&#20851;&#30340;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65307;&#21516;&#26679;&#65292;&#20026;&#20419;&#36827;&#32479;&#35745;&#20844;&#24179;&#24615;&#32780;&#35774;&#35745;&#30340;&#22522;&#20110;&#25277;&#26679;&#30340;&#26041;&#27861;&#24456;&#23569;&#21463;&#21040;&#22240;&#26524;&#20844;&#24179;&#24615;&#30340;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;NLP&#27169;&#22411;&#20013;&#22788;&#29702;&#24615;&#21035;&#20559;&#35265;&#30340;&#32479;&#35745;&#24615;&#21644;&#22240;&#26524;&#24615;&#21435;&#20559;&#32622;&#26041;&#27861;&#65292;&#21457;&#29616;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#38477;&#20302;&#20559;&#35265;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#20250;&#25913;&#21892;&#20854;&#20182;&#20559;&#35265;&#25351;&#26631;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#32479;&#35745;&#24615;&#21644;&#22240;&#26524;&#24615;&#21435;&#20559;&#32622;&#25216;&#26415;&#30340;&#32452;&#21512;&#33021;&#22815;&#20943;&#23569;&#36890;&#36807;&#36825;&#20004;&#31181;&#31867;&#22411;&#25351;&#26631;&#34913;&#37327;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00463v1 Announce Type: new  Abstract: Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#22240;&#26524;&#26410;&#26469;&#29366;&#24577;&#30340;&#26041;&#27861;&#65292;&#22312;&#23433;&#20840;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#26631;&#20934;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#19988;&#24615;&#33021;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#65292;&#23613;&#31649;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2404.00462</link><description>&lt;p&gt;
&#20855;&#26377;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#30340;&#26080;&#20154;&#39550;&#39542;&#27773;&#36710;&#38646;&#23556;&#20987;&#23433;&#20840;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Safety Prediction for Autonomous Robots with Foundation World Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00462
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#22240;&#26524;&#26410;&#26469;&#29366;&#24577;&#30340;&#26041;&#27861;&#65292;&#22312;&#23433;&#20840;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#26631;&#20934;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#19988;&#24615;&#33021;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#65292;&#23613;&#31649;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#21019;&#36896;&#19968;&#20010;&#20195;&#29702;&#19990;&#30028;&#26469;&#35757;&#32451;&#25511;&#21046;&#22120;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#30340;&#20869;&#37096;&#21160;&#24577;&#27169;&#22411;&#26469;&#39044;&#27979;&#23433;&#20840;&#36829;&#35268;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#19990;&#30028;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#32479;&#35745;&#23398;&#20064;&#22914;&#20309;&#35266;&#23519;&#38543;&#30528;&#34892;&#21160;&#32780;&#21464;&#21270;&#65292;&#32570;&#20047;&#23545;&#20195;&#29702;&#21160;&#24577;&#20934;&#30830;&#24615;&#30340;&#31934;&#30830;&#37327;&#21270;&#65292;&#36825;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#26500;&#25104;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#35266;&#23519;&#23884;&#20837;&#21040;&#26377;&#24847;&#20041;&#19988;&#22240;&#26524;&#28508;&#22312;&#30340;&#34920;&#31034;&#20013;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#20195;&#29702;&#21160;&#24577;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#22240;&#26524;&#26410;&#26469;&#29366;&#24577;&#12290;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#36825;&#31181;&#26032;&#39062;&#27169;&#22411;&#22312;&#23433;&#20840;&#39044;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#26631;&#20934;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#19988;&#24615;&#33021;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#65292;&#23613;&#31649;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#26356;&#19987;&#19994;&#21644;&#31995;&#32479;&#30456;&#20851;&#30340;&#25351;&#26631;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00462v1 Announce Type: new  Abstract: A world model creates a surrogate world to train a controller and predict safety violations by learning the internal dynamic model of systems. However, the existing world models rely solely on statistical learning of how observations change in response to actions, lacking precise quantification of how accurate the surrogate dynamics are, which poses a significant challenge in safety-critical systems. To address this challenge, we propose foundation world models that embed observations into meaningful and causally latent representations. This enables the surrogate dynamics to directly predict causal future states by leveraging a training-free large language model. In two common benchmarks, this novel model outperforms standard world models in the safety prediction task and has a performance comparable to supervised learning despite not using any data. We evaluate its performance with a more specialized and system-relevant metric by compar
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20013;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#36890;&#36807;&#29305;&#23450;&#25552;&#31034;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#23454;&#29616;&#25104;&#21151;&#30340;&#21516;&#26102;&#30830;&#20445;&#27491;&#30830;&#26631;&#35760;&#26377;&#27602;&#26679;&#26412;&#65292;&#20294;&#20173;&#38754;&#20020;&#35823;&#28608;&#27963;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#39640;&#27604;&#20363;&#30340;&#27602;&#23475;&#12290;</title><link>https://arxiv.org/abs/2404.00461</link><description>&lt;p&gt;
&#20174;&#23545;&#27604;&#20013;&#20986;&#29616;&#30340;&#24555;&#36895;&#26041;&#27861;&#65306;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#21644;&#38544;&#34109;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00461
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20013;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#36890;&#36807;&#29305;&#23450;&#25552;&#31034;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#23454;&#29616;&#25104;&#21151;&#30340;&#21516;&#26102;&#30830;&#20445;&#27491;&#30830;&#26631;&#35760;&#26377;&#27602;&#26679;&#26412;&#65292;&#20294;&#20173;&#38754;&#20020;&#35823;&#28608;&#27963;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#39640;&#27604;&#20363;&#30340;&#27602;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt-based learning&#33539;&#24335;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25928;&#21147;&#65292;&#21487;&#20197;&#25552;&#21319;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23398;&#20064;&#33539;&#24335;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#24403;&#21069;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#65292;&#21033;&#29992;&#29305;&#23450;&#25552;&#31034;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#65292;&#24182;&#30830;&#20445;&#23545;&#26377;&#27602;&#26679;&#26412;&#30340;&#27491;&#30830;&#26631;&#35760;&#65292;&#30456;&#27604;&#26377;&#27602;&#26631;&#31614;&#25915;&#20987;&#26356;&#20855;&#38544;&#34109;&#24615;&#65292;&#20294;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#38754;&#20020;&#30528;&#20005;&#37325;&#30340;&#35823;&#28608;&#27963;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#39640;&#27604;&#20363;&#30340;&#27602;&#23475;&#12290;&#36890;&#36807;&#20256;&#32479;&#30340;&#36127;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#24178;&#20928;&#26631;&#31614;&#35774;&#32622;&#20013;&#22312;&#25928;&#21147;&#21644;&#38544;&#34109;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#21463;&#21040;&#21518;&#38376;&#20805;&#24403;&#24555;&#25463;&#26041;&#24335;&#30340;&#35266;&#24565;&#30340;&#21551;&#21457;&#65292;&#24182;&#20551;&#35774;&#36825;&#19968;&#24555;&#25463;&#26041;&#24335;&#28304;&#20110;t&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00461v1 Announce Type: cross  Abstract: Prompt-based learning paradigm has demonstrated remarkable efficacy in enhancing the adaptability of pretrained language models (PLMs), particularly in few-shot scenarios. However, this learning paradigm has been shown to be vulnerable to backdoor attacks. The current clean-label attack, employing a specific prompt as a trigger, can achieve success without the need for external triggers and ensure correct labeling of poisoned samples, which is more stealthy compared to the poisoned-label attack, but on the other hand, it faces significant issues with false activations and poses greater challenges, necessitating a higher rate of poisoning. Using conventional negative data augmentation methods, we discovered that it is challenging to trade off between effectiveness and stealthiness in a clean-label setting. In addressing this issue, we are inspired by the notion that a backdoor acts as a shortcut and posit that this shortcut stems from t
&lt;/p&gt;</description></item><item><title>QuaRot&#26159;&#19968;&#31181;&#22522;&#20110;&#26059;&#36716;&#30340;&#26032;&#37327;&#21270;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;LLMs&#20013;&#36827;&#34892;&#26080;&#24322;&#24120;&#20540;&#30340;4&#20301;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#37327;&#21270;&#65292;&#24182;&#20445;&#25345;&#20102;99%&#30340;&#38646;-shot&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00456</link><description>&lt;p&gt;
QuaRot&#65306;&#26059;&#36716;LLMs&#20013;&#26080;&#24322;&#24120;&#20540;&#30340;4&#20301;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00456
&lt;/p&gt;
&lt;p&gt;
QuaRot&#26159;&#19968;&#31181;&#22522;&#20110;&#26059;&#36716;&#30340;&#26032;&#37327;&#21270;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;LLMs&#20013;&#36827;&#34892;&#26080;&#24322;&#24120;&#20540;&#30340;4&#20301;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#37327;&#21270;&#65292;&#24182;&#20445;&#25345;&#20102;99%&#30340;&#38646;-shot&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;QuaRot&#65292;&#19968;&#31181;&#22522;&#20110;&#26059;&#36716;&#30340;&#26032;&#37327;&#21270;&#26041;&#26696;&#65292;&#33021;&#22815;&#31471;&#23545;&#31471;&#22320;&#23558;LLMs&#20013;&#30340;&#25152;&#26377;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#37327;&#21270;&#20026;4&#20301;&#12290;QuaRot&#20197;&#19968;&#31181;&#33021;&#22815;&#21435;&#38500;&#38544;&#34255;&#29366;&#24577;&#20013;&#24322;&#24120;&#20540;&#20294;&#19981;&#25913;&#21464;&#36755;&#20986;&#30340;&#26041;&#24335;&#23545;LLMs&#36827;&#34892;&#26059;&#36716;&#65292;&#20351;&#24471;&#37327;&#21270;&#21464;&#24471;&#26356;&#31616;&#21333;&#12290;&#36825;&#31181;&#35745;&#31639;&#19981;&#21464;&#24615;&#34987;&#24212;&#29992;&#20110;LLM&#30340;&#38544;&#34255;&#29366;&#24577;&#65288;&#27531;&#24046;&#65289;&#65292;&#20197;&#21450;&#21069;&#39304;&#32452;&#20214;&#30340;&#28608;&#27963;&#12289;&#27880;&#24847;&#26426;&#21046;&#30340;&#37096;&#20998;&#20869;&#23481;&#21644;KV&#32531;&#23384;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#65292;&#20854;&#20013;&#25152;&#26377;&#30697;&#38453;&#20056;&#27861;&#37117;&#20197;4&#20301;&#36827;&#34892;&#65292;&#26080;&#38656;&#35782;&#21035;&#38656;&#35201;&#20197;&#26356;&#39640;&#31934;&#24230;&#20445;&#30041;&#30340;&#36890;&#36947;&#12290;&#25105;&#20204;&#30340;&#37327;&#21270;LLaMa2-70B&#27169;&#22411;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#20165;&#25439;&#22833;0.29&#30340;WikiText-2&#22256;&#24785;&#24230;&#65292;&#24182;&#20445;&#30041;&#20102;99%&#30340;&#38646;-shot&#34920;&#29616;&#12290;&#20195;&#30721;&#21487;&#22312;&#27492;&#22788;&#33719;&#24471;&#65306;https://github.com/spcl/QuaRot&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00456v1 Announce Type: new  Abstract: We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4-bits, without any channels identified for retention in higher precision. Our quantized LLaMa2-70B model has losses of at most 0.29 WikiText-2 perplexity and retains 99% of the zero-shot performance. Code is available at: https://github.com/spcl/QuaRot.
&lt;/p&gt;</description></item><item><title>&#20998;&#24067;&#24335;&#29422;&#23376;&#26159;&#23545; Lion &#36827;&#34892;&#20102;&#21019;&#26032;&#24615;&#25913;&#36827;&#65292;&#21033;&#29992;&#31526;&#21495;&#25805;&#20316;&#31526;&#38477;&#20302;&#20102;&#36890;&#20449;&#25104;&#26412;&#65292;&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934; Lion &#25110; AdamW &#20248;&#21270;&#22120;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#36890;&#20449;&#24102;&#23485;&#12290;</title><link>https://arxiv.org/abs/2404.00438</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;&#29422;&#23376;&#36827;&#34892;&#39640;&#25928;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Communication Efficient Distributed Training with Distributed Lion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00438
&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#29422;&#23376;&#26159;&#23545; Lion &#36827;&#34892;&#20102;&#21019;&#26032;&#24615;&#25913;&#36827;&#65292;&#21033;&#29992;&#31526;&#21495;&#25805;&#20316;&#31526;&#38477;&#20302;&#20102;&#36890;&#20449;&#25104;&#26412;&#65292;&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934; Lion &#25110; AdamW &#20248;&#21270;&#22120;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#36890;&#20449;&#24102;&#23485;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lion&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#19982;AdamW&#26377;&#19968;&#23450;&#31454;&#20105;&#21147;&#65292;&#20855;&#26377;&#22312;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#26679;&#26412;&#25928;&#29575;&#19978;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#29422;&#23376;&#65292;&#36825;&#26159;&#29422;&#23376;&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#29615;&#22659;&#20013;&#30340;&#21019;&#26032;&#24615;&#25913;&#36827;&#12290;&#21033;&#29992;&#29422;&#23376;&#20013;&#30340;&#31526;&#21495;&#25805;&#20316;&#31526;&#65292;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#29422;&#23376;&#21482;&#38656;&#35201;&#22312;&#24037;&#20316;&#33410;&#28857;&#21644;&#20013;&#24515;&#26381;&#21153;&#22120;&#20043;&#38388;&#20256;&#36882;&#20108;&#36827;&#21046;&#25110;&#20302;&#31934;&#24230;&#21521;&#37327;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#35777;&#23454;&#20102;&#20998;&#24067;&#24335;&#29422;&#23376;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#22312;&#22810;&#31181;&#20219;&#21153;&#12289;&#24037;&#20316;&#32773;&#25968;&#37327;&#21644;&#25209;&#37327;&#22823;&#23567;&#19978;&#34920;&#29616;&#31283;&#20581;&#65292;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20998;&#24067;&#24335;&#29422;&#23376;&#22312;&#32858;&#21512;&#26799;&#24230;&#19978;&#36798;&#21040;&#20102;&#19982;&#26631;&#20934;&#29422;&#23376;&#25110;AdamW&#20248;&#21270;&#22120;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#20449;&#24102;&#23485;&#26174;&#33879;&#20943;&#23569;&#12290;&#36825;&#20010;&#29305;&#24615;&#23545;&#20110;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#23588;&#20026;&#26377;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00438v1 Announce Type: cross  Abstract: The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages on memory, computation, and sample efficiency. In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments. Leveraging the sign operator in Lion, our Distributed Lion only requires communicating binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost. Our theoretical analysis confirms Distributed Lion's convergence properties. Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems. Notably, Distributed Lion attains comparable performance to standard Lion or AdamW optimizers applied on aggregated gradients, but with significantly reduced communication bandwidth. This feature is particularly advantageous for training large model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27861;&#24459;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#65292;&#20351;&#27169;&#22411;&#30340;&#20915;&#31574;&#21464;&#24471;&#21487;&#29702;&#35299;&#32473;&#26368;&#32456;&#29992;&#25143;</title><link>https://arxiv.org/abs/2404.00437</link><description>&lt;p&gt;
&#21033;&#29992;&#26641;&#20272;&#35745;&#22120;&#33258;&#21160;&#35299;&#37322;&#35199;&#29677;&#29273;&#27861;&#24459;&#21028;&#20915;&#22312;&#20381;&#36182;&#21496;&#27861;&#31649;&#36758;&#30340;&#27861;&#24459;&#31867;&#21035;&#20013;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Automatic explanation of the classification of Spanish legal judgments in jurisdiction-dependent law categories with tree estimators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27861;&#24459;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#65292;&#20351;&#27169;&#22411;&#30340;&#20915;&#31574;&#21464;&#24471;&#21487;&#29702;&#35299;&#32473;&#26368;&#32456;&#29992;&#25143;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27861;&#24459;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#24050;&#32463;&#34987;&#25552;&#20986;&#22312;&#25991;&#29486;&#20013;&#65292;&#20197;&#35299;&#20915;&#30693;&#35782;&#20174;&#21028;&#20915;&#20013;&#25552;&#21462;&#24182;&#26816;&#27979;&#20854;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23427;&#20204;&#30340;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#31995;&#32479;&#37117;&#26159;&#40657;&#30418;&#30340;&#12290;&#36825;&#21487;&#33021;&#24341;&#21457;&#23545;&#23427;&#20204;&#21487;&#20449;&#24230;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#23545;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20915;&#31574;&#20013;&#28041;&#21450;&#30340;&#29305;&#24449;&#21644;&#26641;&#32467;&#26500;&#30340;&#20915;&#31574;&#36335;&#24452;&#30340;&#38408;&#20540;&#20998;&#21449;&#20540;&#65292;&#24182;&#20197;&#33258;&#28982;&#35821;&#35328;&#30340;&#26041;&#24335;&#21521;&#29992;&#25143;&#21576;&#29616;&#36825;&#20123;&#20449;&#24687;&#12290;&#36825;&#26159;&#31532;&#19968;&#39033;&#20851;&#20110;&#33258;&#21160;&#20998;&#26512;&#27861;&#24459;&#25991;&#26412;&#30340;&#24037;&#20316;&#65292;&#32467;&#21512;NLP&#21644;ML&#20197;&#21450;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#20197;&#20351;&#27169;&#22411;&#30340;&#20915;&#31574;&#33258;&#21160;&#21464;&#24471;&#21487;&#29702;&#35299;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;&#27492;&#22806;&#65292;&#27861;&#24459;&#19987;&#23478;&#24050;&#32463;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#30693;&#35782;&#20063;&#24050;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00437v1 Announce Type: cross  Abstract: Automatic legal text classification systems have been proposed in the literature to address knowledge extraction from judgments and detect their aspects. However, most of these systems are black boxes even when their models are interpretable. This may raise concerns about their trustworthiness. Accordingly, this work contributes with a system combining Natural Language Processing (NLP) with Machine Learning (ML) to classify legal texts in an explainable manner. We analyze the features involved in the decision and the threshold bifurcation values of the decision paths of tree structures and present this information to the users in natural language. This is the first work on automatic analysis of legal texts combining NLP and ML along with Explainable Artificial Intelligence techniques to automatically make the models' decisions understandable to end users. Furthermore, legal experts have validated our solution, and this knowledge has al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;AI&#25216;&#26415;&#25552;&#20986;&#20102;&#21033;&#29992;&#35821;&#20041;&#28508;&#22312;&#21521;&#37327;&#37327;&#21270;&#35270;&#35273;&#22806;&#35266;&#29305;&#24449;&#65292;&#35745;&#31639;&#34903;&#26223;&#22270;&#20687;&#30456;&#20284;&#24615;&#24182;&#21457;&#29616;&#31354;&#38388;&#22270;&#20687;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#39550;&#39542;&#36335;&#32447;&#35268;&#21010;&#22120;&#20013;&#65292;&#26368;&#32456;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#21407;&#22411;VivaRoutes&#65292;&#24110;&#21161;&#29992;&#25143;&#26377;&#25928;&#35268;&#21010;&#39550;&#39542;&#36335;&#32447;&#12290;</title><link>https://arxiv.org/abs/2404.00431</link><description>&lt;p&gt;
&#21033;&#29992;AI&#21457;&#29616;&#30340;&#34903;&#26223;&#27169;&#24335;&#21487;&#35270;&#21270;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Visualizing Routes with AI-Discovered Street-View Patterns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;AI&#25216;&#26415;&#25552;&#20986;&#20102;&#21033;&#29992;&#35821;&#20041;&#28508;&#22312;&#21521;&#37327;&#37327;&#21270;&#35270;&#35273;&#22806;&#35266;&#29305;&#24449;&#65292;&#35745;&#31639;&#34903;&#26223;&#22270;&#20687;&#30456;&#20284;&#24615;&#24182;&#21457;&#29616;&#31354;&#38388;&#22270;&#20687;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#39550;&#39542;&#36335;&#32447;&#35268;&#21010;&#22120;&#20013;&#65292;&#26368;&#32456;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#21407;&#22411;VivaRoutes&#65292;&#24110;&#21161;&#29992;&#25143;&#26377;&#25928;&#35268;&#21010;&#39550;&#39542;&#36335;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34903;&#36947;&#32423;&#21035;&#30340;&#35270;&#35273;&#22806;&#35266;&#22312;&#30740;&#31350;&#31038;&#20250;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20363;&#22914;&#29702;&#35299;&#24314;&#31569;&#29615;&#22659;&#12289;&#39550;&#39542;&#36335;&#24452;&#20197;&#21450;&#30456;&#20851;&#31038;&#20250;&#21644;&#32463;&#27982;&#22240;&#32032;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#19968;&#26032;&#30340;&#21487;&#35270;&#21270;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#39033;&#26032;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#19968;&#31995;&#21015;AI&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#35821;&#20041;&#28508;&#22312;&#21521;&#37327;&#26469;&#37327;&#21270;&#35270;&#35273;&#22806;&#35266;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#22823;&#37327;&#34903;&#26223;&#22270;&#20687;&#20013;&#35745;&#31639;&#22270;&#20687;&#30456;&#20284;&#24615;&#65292;&#28982;&#21518;&#21457;&#29616;&#31354;&#38388;&#22270;&#20687;&#27169;&#24335;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#21457;&#29616;&#30340;&#27169;&#24335;&#19982;&#26032;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#25972;&#21512;&#21040;&#39550;&#39542;&#36335;&#32447;&#35268;&#21010;&#22120;&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#21407;&#22411;VivaRoutes&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#30340;&#27169;&#24335;&#26469;&#24110;&#21161;&#29992;&#25143;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00431v1 Announce Type: cross  Abstract: Street-level visual appearances play an important role in studying social systems, such as understanding the built environment, driving routes, and associated social and economic factors. It has not been integrated into a typical geographical visualization interface (e.g., map services) for planning driving routes. In this paper, we study this new visualization task with several new contributions. First, we experiment with a set of AI techniques and propose a solution of using semantic latent vectors for quantifying visual appearance features. Second, we calculate image similarities among a large set of street-view images and then discover spatial imagery patterns. Third, we integrate these discovered patterns into driving route planners with new visualization techniques. Finally, we present VivaRoutes, an interactive visualization prototype, to show how visualizations leveraged with these discovered patterns can help users effectively
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24037;&#20316;&#27969;&#26469;&#28304;&#23398;&#20064;&#26381;&#21153;&#34920;&#31034;&#21644;&#26381;&#21153;&#36873;&#25321;&#20915;&#31574;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#33616;&#31185;&#23398;&#24037;&#20316;&#27969;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#19979;&#19968;&#27493;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2404.00420</link><description>&lt;p&gt;
&#23398;&#20064;&#31185;&#23398;&#24037;&#20316;&#27969;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#26381;&#21153;&#36873;&#25321;&#20915;&#31574;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning Service Selection Decision Making Behaviors During Scientific Workflow Development
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24037;&#20316;&#27969;&#26469;&#28304;&#23398;&#20064;&#26381;&#21153;&#34920;&#31034;&#21644;&#26381;&#21153;&#36873;&#25321;&#20915;&#31574;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#33616;&#31185;&#23398;&#24037;&#20316;&#27969;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#19979;&#19968;&#27493;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#36719;&#20214;&#26381;&#21153;&#34987;&#21457;&#24067;&#21040;&#20114;&#32852;&#32593;&#19978;&#65292;&#36825;&#32473;&#31185;&#23398;&#24037;&#20316;&#27969;&#26500;&#24314;&#36807;&#31243;&#20013;&#30340;&#26381;&#21153;&#25512;&#33616;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#24037;&#20316;&#27969;&#26469;&#28304;&#20013;&#23398;&#20064;&#26381;&#21153;&#34920;&#31034;&#21644;&#26381;&#21153;&#36873;&#25321;&#20915;&#31574;&#34892;&#20026;&#65292;&#26469;&#25512;&#33616;&#19979;&#19968;&#20010;&#26381;&#21153;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#29983;&#25104;&#30340;&#21551;&#21457;&#65292;&#23558;&#31185;&#23398;&#24037;&#20316;&#27969;&#30340;&#32452;&#21512;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#22312;&#24037;&#20316;&#27969;&#30446;&#26631;&#32972;&#26223;&#19979;&#30340;&#36880;&#27493;&#36807;&#31243;&#65292;&#19979;&#19968;&#20010;&#26381;&#21153;&#25512;&#33616;&#30340;&#38382;&#39064;&#34987;&#26144;&#23556;&#20026;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#12290;&#39318;&#20808;&#20174;&#31185;&#23398;&#24037;&#20316;&#27969;&#26469;&#28304;&#20013;&#25552;&#21462;&#21382;&#21490;&#26381;&#21153;&#20381;&#36182;&#20851;&#31995;&#20197;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#28982;&#21518;&#22522;&#20110;&#19981;&#21516;&#30340;&#32452;&#21512;&#36335;&#24452;&#29983;&#25104;&#31574;&#30053;&#29983;&#25104;&#26381;&#21153;&#24207;&#21015;&#65292;&#26368;&#21518;&#26681;&#25454;&#29983;&#25104;&#30340;&#32452;&#21512;&#36335;&#24452;&#35821;&#26009;&#24211;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00420v1 Announce Type: cross  Abstract: Increasingly, more software services have been published onto the Internet, making it a big challenge to recommend services in the process of a scientific workflow composition. In this paper, a novel context-aware approach is proposed to recommending next services in a workflow development process, through learning service representation and service selection decision making behaviors from workflow provenance. Inspired by natural language sentence generation, the composition process of a scientific workflow is formalized as a step-wise procedure within the context of the goal of workflow, and the problem of next service recommendation is mapped to next word prediction. Historical service dependencies are first extracted from scientific workflow provenance to build a knowledge graph. Service sequences are then generated based on diverse composition path generation strategies. Afterwards, the generated corpus of composition paths are lev
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Multi-level Online Sequential Experts (MOSE)&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#23618;&#30417;&#30563;&#21644;&#21453;&#21521;&#33258;&#33976;&#39311;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#26032;&#26087;&#35757;&#32451;&#26679;&#26412;&#23398;&#20064;&#19981;&#36275;&#21644;&#37325;&#22797;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00417</link><description>&lt;p&gt;
&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#25512;&#36827;&#28508;&#22312;&#19987;&#19994;&#30693;&#35782;&#30340;&#32534;&#25490;&#65306;&#22810;&#23618;&#30417;&#30563;&#21644;&#21453;&#21521;&#33258;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00417
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Multi-level Online Sequential Experts (MOSE)&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#23618;&#30417;&#30563;&#21644;&#21453;&#21521;&#33258;&#33976;&#39311;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#26032;&#26087;&#35757;&#32451;&#26679;&#26412;&#23398;&#20064;&#19981;&#36275;&#21644;&#37325;&#22797;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36866;&#24212;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#21464;&#21270;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38656;&#35201;&#20197;&#22312;&#32447;&#26041;&#24335;&#22788;&#29702;&#36830;&#32493;&#21040;&#36798;&#30340;&#20869;&#23481;&#12290;&#22312;&#27491;&#24120;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#35797;&#22270;&#36890;&#36807;&#31163;&#32447;&#35757;&#32451;&#27599;&#20010;&#20219;&#21153;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#20043;&#22806;&#65292;&#22312;&#19968;&#27425;&#25968;&#25454;&#27969;&#20013;&#25191;&#34892;CL&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#65288;OCL&#65289;&#26159;&#19968;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#20294;&#26356;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;&#24403;&#21069;&#30340;OCL&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26087;&#35757;&#32451;&#26679;&#26412;&#30340;&#20869;&#23384;&#37325;&#25918;&#12290;&#28982;&#32780;&#65292;&#20174;CL&#21040;OCL&#30340;&#19968;&#20010;&#26174;&#30528;&#24046;&#36317;&#28304;&#20110;&#19982;&#37325;&#28436;&#32531;&#20914;&#21306;&#30340;&#20351;&#29992;&#30456;&#20851;&#30340;&#36807;&#24230;&#25311;&#21512;-&#27424;&#25311;&#21512;&#22256;&#22659;&#65306;&#23545;&#26032;&#35757;&#32451;&#26679;&#26412;&#30340;&#23398;&#20064;&#19981;&#36275;&#65288;&#27424;&#25311;&#21512;&#65289;&#20197;&#21450;&#23545;&#23569;&#37327;&#26087;&#35757;&#32451;&#26679;&#26412;&#30340;&#37325;&#22797;&#23398;&#20064;&#65288;&#36807;&#25311;&#21512;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22810;&#32423;&#22312;&#32447;&#39034;&#24207;&#19987;&#23478;&#65288;MOSE&#65289;&#65292;&#23427;&#23558;&#27169;&#22411;&#20316;&#20026;&#22534;&#21472;&#30340;&#23376;&#19987;&#23478;&#36827;&#34892;&#22521;&#32946;&#65292;&#24182;&#25972;&#21512;&#20102;&#22810;&#32423;&#30417;&#30563;&#21644;&#21453;&#21521;&#33258;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00417v1 Announce Type: cross  Abstract: To accommodate real-world dynamics, artificial intelligence systems need to cope with sequentially arriving content in an online manner. Beyond regular Continual Learning (CL) attempting to address catastrophic forgetting with offline training of each task, Online Continual Learning (OCL) is a more challenging yet realistic setting that performs CL in a one-pass data stream. Current OCL methods primarily rely on memory replay of old training samples. However, a notable gap from CL to OCL stems from the additional overfitting-underfitting dilemma associated with the use of rehearsal buffers: the inadequate learning of new training samples (underfitting) and the repeated learning of a few old training samples (overfitting). To this end, we introduce a novel approach, Multi-level Online Sequential Experts (MOSE), which cultivates the model as stacked sub-experts, integrating multi-level supervision and reverse self-distillation. Supervisi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24212;&#29992;&#20110;&#31354;&#38388;&#23548;&#33322;&#21644;&#25511;&#21046;&#39046;&#22495;&#65292;&#36890;&#36807;&#24320;&#21457;&#32431;LLM&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312; Kerbal &#22826;&#31354;&#35745;&#21010;&#24046;&#20998;&#28216;&#25103;&#25361;&#25112;&#20013;&#21462;&#24471;&#31532;&#20108;&#21517;&#65292;&#39318;&#27425;&#23558;LLM&#20195;&#29702;&#38598;&#25104;&#21040;&#31354;&#38388;&#36164;&#28304;&#20013;&#12290;</title><link>https://arxiv.org/abs/2404.00413</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#33322;&#22825;&#22120;&#25805;&#20316;&#21592;
&lt;/p&gt;
&lt;p&gt;
Language Models are Spacecraft Operators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24212;&#29992;&#20110;&#31354;&#38388;&#23548;&#33322;&#21644;&#25511;&#21046;&#39046;&#22495;&#65292;&#36890;&#36807;&#24320;&#21457;&#32431;LLM&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312; Kerbal &#22826;&#31354;&#35745;&#21010;&#24046;&#20998;&#28216;&#25103;&#25361;&#25112;&#20013;&#21462;&#24471;&#31532;&#20108;&#21517;&#65292;&#39318;&#27425;&#23558;LLM&#20195;&#29702;&#38598;&#25104;&#21040;&#31354;&#38388;&#36164;&#28304;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#20010;&#36235;&#21183;&#65292;&#21363;&#24191;&#27867;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#26681;&#25454;&#29992;&#25143;&#25991;&#26412;&#25552;&#31034;&#20869;&#23481;&#37319;&#21462;&#34892;&#21160;&#30340;&#33258;&#20027;&#20195;&#29702;&#12290;&#25105;&#20204;&#25171;&#31639;&#23558;&#36825;&#20123;&#27010;&#24565;&#24212;&#29992;&#20110;&#31354;&#38388;&#23548;&#33322;&#21644;&#25511;&#21046;&#39046;&#22495;&#65292;&#20351;LLMs&#22312;&#33258;&#20027;&#21355;&#26143;&#25805;&#20316;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#20316;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#20026; Kerbal &#22826;&#31354;&#35745;&#21010;&#24046;&#20998;&#28216;&#25103;(KSPDG)&#25361;&#25112;&#24320;&#21457;&#20102;&#19968;&#31181;&#32431;LLM&#26041;&#26696;&#65292;&#36825;&#26159;&#19968;&#20010;&#20844;&#24320;&#30340;&#36719;&#20214;&#35774;&#35745;&#31454;&#36187;&#65292;&#21442;&#19982;&#32773;&#20026;&#21355;&#26143;&#25805;&#32437;&#21019;&#24314;&#33258;&#20027;&#20195;&#29702;&#65292;&#22312;KSP&#28216;&#25103;&#24341;&#25806;&#19978;&#36816;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#24494;&#35843;&#25216;&#26415;&#21019;&#24314;&#20102;&#19968;&#20010;&#25928;&#26524;&#33391;&#22909;&#30340;LLM&#20195;&#29702;&#65292;&#22312;&#31454;&#36187;&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#24320;&#21019;&#20102;&#23558;LLM&#20195;&#29702;&#25972;&#21512;&#21040;&#31354;&#38388;&#36164;&#28304;&#20013;&#30340;&#20808;&#27827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00413v1 Announce Type: cross  Abstract: Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Guidance, Navigation, and Control in space, enabling LLMs to have a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space res
&lt;/p&gt;</description></item><item><title>SVGCraft&#24341;&#20837;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#25551;&#32472;&#25972;&#20010;&#22330;&#26223;&#30340;&#30690;&#37327;&#22270;&#65292;&#20854;&#20013;&#21253;&#25324;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#24067;&#23616;&#29983;&#25104;&#12289;&#20135;&#29983;&#36974;&#32617;&#28508;&#21464;&#37327;&#20197;&#36827;&#34892;&#20934;&#30830;&#23545;&#35937;&#25918;&#32622;&#12289;&#34701;&#21512;&#27880;&#24847;&#21147;&#22270;&#20197;&#21450;&#20351;&#29992;&#25193;&#25955;U-Net&#36827;&#34892;&#21512;&#25104;&#65292;&#21516;&#26102;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#21644;LPIPS&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2404.00412</link><description>&lt;p&gt;
SVGCraft:&#36229;&#36234;&#21333;&#20010;&#30446;&#26631;&#25991;&#23383;&#21040;SVG&#32508;&#21512;&#30011;&#24067;&#24067;&#23616;
&lt;/p&gt;
&lt;p&gt;
SVGCraft: Beyond Single Object Text-to-SVG Synthesis with Comprehensive Canvas Layout
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00412
&lt;/p&gt;
&lt;p&gt;
SVGCraft&#24341;&#20837;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#25551;&#32472;&#25972;&#20010;&#22330;&#26223;&#30340;&#30690;&#37327;&#22270;&#65292;&#20854;&#20013;&#21253;&#25324;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#24067;&#23616;&#29983;&#25104;&#12289;&#20135;&#29983;&#36974;&#32617;&#28508;&#21464;&#37327;&#20197;&#36827;&#34892;&#20934;&#30830;&#23545;&#35937;&#25918;&#32622;&#12289;&#34701;&#21512;&#27880;&#24847;&#21147;&#22270;&#20197;&#21450;&#20351;&#29992;&#25193;&#25955;U-Net&#36827;&#34892;&#21512;&#25104;&#65292;&#21516;&#26102;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#21644;LPIPS&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20174;&#25991;&#26412;&#25552;&#31034;&#21040;&#30690;&#37327;&#22270;&#30340;VectorArt&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#20219;&#21153;&#65292;&#38656;&#35201;&#23545;&#24050;&#30693;&#21644;&#26410;&#30693;&#23454;&#20307;&#36827;&#34892;&#22810;&#26679;&#21270;&#32780;&#30495;&#23454;&#30340;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#23616;&#38480;&#20110;&#29983;&#25104;&#21333;&#20010;&#23545;&#35937;&#65292;&#32780;&#19981;&#26159;&#30001;&#22810;&#20010;&#20803;&#32032;&#32452;&#25104;&#30340;&#22330;&#26223;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;SVGCraft&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#25551;&#32472;&#25972;&#20010;&#22330;&#26223;&#30340;&#30690;&#37327;&#22270;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#24067;&#23616;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#29983;&#20135;&#29305;&#23450;&#36793;&#30028;&#26694;&#20013;&#30340;&#25513;&#33180;&#28508;&#21464;&#37327;&#23454;&#29616;&#20934;&#30830;&#30340;&#23545;&#35937;&#25918;&#32622;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#34701;&#21512;&#26426;&#21046;&#65292;&#29992;&#20110;&#38598;&#25104;&#27880;&#24847;&#21147;&#22270;&#65292;&#24182;&#20351;&#29992;&#25193;&#25955;U-Net&#36827;&#34892;&#36830;&#36143;&#30340;&#21512;&#25104;&#65292;&#21152;&#24555;&#32472;&#22270;&#36807;&#31243;&#12290;&#29983;&#25104;&#30340;SVG&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#21644;LPIPS&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#36879;&#26126;&#24230;&#35843;&#21046;&#26469;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00412v1 Announce Type: cross  Abstract: Generating VectorArt from text prompts is a challenging vision task, requiring diverse yet realistic depictions of the seen as well as unseen entities. However, existing research has been mostly limited to the generation of single objects, rather than comprehensive scenes comprising multiple elements. In response, this work introduces SVGCraft, a novel end-to-end framework for the creation of vector graphics depicting entire scenes from textual descriptions. Utilizing a pre-trained LLM for layout generation from text prompts, this framework introduces a technique for producing masked latents in specified bounding boxes for accurate object placement. It introduces a fusion mechanism for integrating attention maps and employs a diffusion U-Net for coherent composition, speeding up the drawing process. The resulting SVG is optimized using a pre-trained encoder and LPIPS loss with opacity modulation to maximize similarity. Additionally, th
&lt;/p&gt;</description></item><item><title>Aardvark Weather&#26159;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#25253;&#31995;&#32479;&#65292;&#33021;&#22815;&#21462;&#20195;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#25552;&#20379;&#20840;&#29699;&#21644;&#26412;&#22320;&#31934;&#20934;&#39044;&#25253;&#12290;</title><link>https://arxiv.org/abs/2404.00411</link><description>&lt;p&gt;
Aardvark Weather:&#31471;&#23545;&#31471;&#25968;&#25454;&#39537;&#21160;&#30340;&#22825;&#27668;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Aardvark Weather: end-to-end data-driven weather forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00411
&lt;/p&gt;
&lt;p&gt;
Aardvark Weather&#26159;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#25253;&#31995;&#32479;&#65292;&#33021;&#22815;&#21462;&#20195;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#25552;&#20379;&#20840;&#29699;&#21644;&#26412;&#22320;&#31934;&#20934;&#39044;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#20013;&#31243;&#22825;&#27668;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23427;&#20165;&#34987;&#24212;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#31649;&#36947;&#30340;&#29305;&#23450;&#21644;&#21333;&#20010;&#32452;&#20214;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26080;&#27861;&#22312;&#27809;&#26377;&#26469;&#33258;&#20256;&#32479;&#25805;&#20316;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#30340;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#37096;&#32626;&#65292;&#36825;&#26159;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#19988;&#19981;&#25903;&#25345;&#31471;&#21040;&#31471;&#20248;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26681;&#26412;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21462;&#20195;&#25972;&#20010;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#31649;&#36947;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Aardvark Weather&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#25253;&#31995;&#32479;&#65292;&#23427;&#25509;&#21463;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#25552;&#20379;&#20840;&#29699;&#21644;&#26412;&#22320;&#39044;&#25253;&#12290;&#36825;&#20123;&#20840;&#29699;&#39044;&#25253;&#20197;&#19968;&#24230;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;24&#23567;&#26102;&#26102;&#38388;&#20998;&#36776;&#29575;&#20026;&#22810;&#20010;&#21387;&#21147;&#27700;&#24179;24&#20010;&#21464;&#37327;&#20135;&#29983;&#65292;&#24182;&#22312;&#20116;&#21040;&#19971;&#22825;&#30340;&#21069;&#26399;&#39046;&#20808;&#26102;&#27573;&#23545;&#27599;&#23567;&#26102;&#27668;&#20505;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#22320;&#39044;&#25253;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00411v1 Announce Type: cross  Abstract: Machine learning is revolutionising medium-range weather prediction. However it has only been applied to specific and individual components of the weather prediction pipeline. Consequently these data-driven approaches are unable to be deployed without input from conventional operational numerical weather prediction (NWP) systems, which is computationally costly and does not support end-to-end optimisation. In this work, we take a radically different approach and replace the entire NWP pipeline with a machine learning model. We present Aardvark Weather, the first end-to-end data-driven forecasting system which takes raw observations as input and provides both global and local forecasts. These global forecasts are produced for 24 variables at multiple pressure levels at one-degree spatial resolution and 24 hour temporal resolution, and are skillful with respect to hourly climatology at five to seven day lead times. Local forecasts are pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38236;&#22836;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#32479;&#19968;&#21508;&#31181;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#21644;&#31163;&#25955;&#39046;&#22495;&#37117;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2404.00408</link><description>&lt;p&gt;
&#20351;&#29992;&#21442;&#25968;&#21270;&#38236;&#22836;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Learning with Parametric Lenses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00408
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38236;&#22836;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#32479;&#19968;&#21508;&#31181;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#21644;&#31163;&#25955;&#39046;&#22495;&#37117;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38236;&#22836;&#12289;&#21442;&#25968;&#26144;&#23556;&#21644;&#21453;&#21521;&#23548;&#25968;&#33539;&#30068;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#33539;&#30068;&#35821;&#20041;&#12290;&#36825;&#19968;&#22522;&#30784;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#35299;&#37322;&#24615;&#21644;&#32479;&#19968;&#24615;&#26694;&#26550;&#65306;&#23427;&#21253;&#25324;&#21508;&#31181;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22914;ADAM&#12289;AdaGrad&#21644;Nesterov&#21160;&#37327;&#65292;&#20197;&#21450;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#65292;&#22914;&#22343;&#26041;&#35823;&#24046;&#21644;Softmax &#20132;&#21449;&#29109;&#65292;&#24182;&#28085;&#30422;&#19981;&#21516;&#30340;&#26550;&#26500;&#65292;&#25581;&#31034;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#36229;&#36234;&#29087;&#24713;&#36830;&#32493;&#22495;&#65288;&#22312;&#20809;&#28369;&#26144;&#23556;&#33539;&#30068;&#20013;&#24314;&#27169;&#65289;&#30340;&#27867;&#21270;&#31034;&#20363;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#24067;&#23572;&#21644;&#22810;&#39033;&#24335;&#30005;&#36335;&#30340;&#31163;&#25955;&#35774;&#32622;&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;Python&#20013;&#23454;&#29616;&#26469;&#28436;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#23454;&#38469;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00408v1 Announce Type: new  Abstract: We propose a categorical semantics for machine learning algorithms in terms of lenses, parametric maps, and reverse derivative categories. This foundation provides a powerful explanatory and unifying framework: it encompasses a variety of gradient descent algorithms such as ADAM, AdaGrad, and Nesterov momentum, as well as a variety of loss functions such as MSE and Softmax cross-entropy, and different architectures, shedding new light on their similarities and differences. Furthermore, our approach to learning has examples generalising beyond the familiar continuous domains (modelled in categories of smooth maps) and can be realised in the discrete setting of Boolean and polynomial circuits. We demonstrate the practical significance of our framework with an implementation in Python.
&lt;/p&gt;</description></item><item><title>Aurora-M &#26159;&#31532;&#19968;&#20010;&#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#65292;&#19981;&#26029;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807; 2 &#19975;&#20159;&#20010;</title><link>https://arxiv.org/abs/2404.00399</link><description>&lt;p&gt;
Aurora-M: &#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#65292;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00399
&lt;/p&gt;
&lt;p&gt;
Aurora-M &#26159;&#31532;&#19968;&#20010;&#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#65292;&#19981;&#26029;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807; 2 &#19975;&#20159;&#20010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#22810;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35757;&#32451;&#26102;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#21487;&#35775;&#38382;&#24615;&#12290;BLOOM &#21644; StarCoder &#31561;&#20513;&#35758;&#26088;&#22312;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#20110;&#21327;&#20316;&#31038;&#21306;&#24320;&#21457;&#26356;&#20855;&#27665;&#20027;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23384;&#22312;&#30340;&#27169;&#22411;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65306;&#22810;&#35821;&#35328;&#33021;&#21147;&#26377;&#38480;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#21448;&#20855;&#26377;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#38656;&#35201;&#36981;&#23432;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#21644;&#21457;&#23637;&#27861;&#24459;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; Aurora-M&#65292;&#19968;&#20010;&#21253;&#21547; 15B &#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#24320;&#28304;&#27169;&#22411;&#65292;&#35757;&#32451;&#35821;&#35328;&#21253;&#25324;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#12290;Aurora-M &#19981;&#26029;&#20174; StarCoderPlus &#19978;&#39044;&#35757;&#32451;&#65292;&#39069;&#22806;&#35757;&#32451;&#20102; 4350 &#20159;&#20010; token&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807;&#20102; 2 &#19975;&#20159;&#20010;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#22312;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#24320;&#21457;&#19982;&#20256;&#32479;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00399v1 Announce Type: cross  Abstract: Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#23450;&#20041;&#30340;&#24809;&#32602;&#25439;&#22833;&#23398;&#20064;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;FBF&#31639;&#27861;&#25552;&#20379;&#25910;&#25947;&#20445;&#35777;&#65292;&#20197;&#35299;&#20915;&#38750;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00390</link><description>&lt;p&gt;
&#23398;&#20064;&#30495;&#27491;&#21333;&#35843;&#31639;&#23376;&#21450;&#20854;&#22312;&#38750;&#32447;&#24615;&#36870;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning truly monotone operators with applications to nonlinear inverse problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00390
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#23450;&#20041;&#30340;&#24809;&#32602;&#25439;&#22833;&#23398;&#20064;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;FBF&#31639;&#27861;&#25552;&#20379;&#25910;&#25947;&#20445;&#35777;&#65292;&#20197;&#35299;&#20915;&#38750;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26032;&#23450;&#20041;&#30340;&#24809;&#32602;&#25439;&#22833;&#26469;&#23398;&#20064;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#19968;&#31867;&#21464;&#20998;&#38382;&#39064;&#20013;&#29305;&#21035;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#20013;&#24120;&#36935;&#21040;&#30340;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#12290;&#37319;&#29992;&#21069;-&#21518;-&#21069;&#65288;FBF&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;FBF&#31639;&#27861;&#22312;&#23398;&#20064;&#31639;&#23376;&#21333;&#35843;&#30340;&#26465;&#20214;&#19979;&#25552;&#20379;&#25910;&#25947;&#20445;&#35777;&#12290;&#20511;&#37492;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#36825;&#20123;&#26032;&#23398;&#20064;&#30340;&#31639;&#23376;&#24212;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#21464;&#20998;&#21253;&#21547;&#38382;&#39064;&#65292;&#38543;&#21518;&#35757;&#32451;&#19968;&#20010;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#19968;&#20010;&#26412;&#36136;&#19978;&#21487;&#33021;&#19981;&#26159;&#21333;&#35843;&#30340;&#31639;&#23376;&#12290;&#21033;&#29992;FBF&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00390v1 Announce Type: cross  Abstract: This article introduces a novel approach to learning monotone neural networks through a newly defined penalization loss. The proposed method is particularly effective in solving classes of variational problems, specifically monotone inclusion problems, commonly encountered in image processing tasks. The Forward-Backward-Forward (FBF) algorithm is employed to address these problems, offering a solution even when the Lipschitz constant of the neural network is unknown. Notably, the FBF algorithm provides convergence guarantees under the condition that the learned operator is monotone. Building on plug-and-play methodologies, our objective is to apply these newly learned operators to solving non-linear inverse problems. To achieve this, we initially formulate the problem as a variational inclusion problem. Subsequently, we train a monotone neural network to approximate an operator that may not inherently be monotone. Leveraging the FBF al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38754;&#21521;&#23545;&#35937;&#30340;&#21463;&#32422;&#26463;&#24067;&#23616;&#29983;&#25104;&#65292;&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#22797;&#26434;&#20132;&#20114;&#20851;&#31995;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2404.00385</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#23376;&#22270;&#30340;&#21463;&#32422;&#26463;&#24067;&#23616;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Constrained Layout Generation with Factor Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38754;&#21521;&#23545;&#35937;&#30340;&#21463;&#32422;&#26463;&#24067;&#23616;&#29983;&#25104;&#65292;&#21487;&#20197;&#20934;&#30830;&#25429;&#25417;&#22797;&#26434;&#20132;&#20114;&#20851;&#31995;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#21253;&#25324;&#24179;&#38754;&#35774;&#35745;&#36807;&#31243;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#20013;&#20986;&#29616;&#30340;&#38754;&#21521;&#23545;&#35937;&#30340;&#21463;&#32422;&#26463;&#24067;&#23616;&#29983;&#25104;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20316;&#21697;&#36890;&#24120;&#23558;&#23545;&#35937;&#34920;&#31034;&#20026;&#21333;&#20010;&#33410;&#28857;&#65292;&#32570;&#20047;&#32454;&#31890;&#24230;&#26469;&#20934;&#30830;&#24314;&#27169;&#23545;&#35937;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#23376;&#22270;&#30340;&#26041;&#27861;&#65292;&#23545;&#27599;&#20010;&#25151;&#38388;&#24341;&#20837;&#22235;&#20010;&#28508;&#21464;&#37327;&#33410;&#28857;&#21644;&#27599;&#20010;&#32422;&#26463;&#24341;&#20837;&#19968;&#20010;&#22240;&#23376;&#33410;&#28857;&#12290;&#22240;&#23376;&#33410;&#28857;&#34920;&#31034;&#19982;&#20854;&#36830;&#25509;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#26377;&#25928;&#25429;&#25417;&#21487;&#33021;&#26159;&#39640;&#38454;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00385v1 Announce Type: cross  Abstract: This paper addresses the challenge of object-centric layout generation under spatial constraints, seen in multiple domains including floorplan design process. The design process typically involves specifying a set of spatial constraints that include object attributes like size and inter-object relations such as relative positioning. Existing works, which typically represent objects as single nodes, lack the granularity to accurately model complex interactions between objects. For instance, often only certain parts of an object, like a room's right wall, interact with adjacent objects. To address this gap, we introduce a factor graph based approach with four latent variable nodes for each room, and a factor node for each constraint. The factor nodes represent dependencies among the variables to which they are connected, effectively capturing constraints that are potentially of a higher order. We then develop message-passing on the bipar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#38381;&#29615;&#27169;&#22411;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#30446;&#26631;&#23548;&#21521;&#23458;&#25143;&#36873;&#25321;&#65292;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31995;&#32479;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.00371</link><description>&lt;p&gt;
&#20174;&#23398;&#20064;&#21040;&#20998;&#26512;&#65306;&#36890;&#36807;&#30446;&#26631;&#23548;&#21521;&#30340;&#23458;&#25143;&#36873;&#25321;&#25552;&#39640;&#27169;&#22411;&#25928;&#33021;
&lt;/p&gt;
&lt;p&gt;
From Learning to Analytics: Improving Model Efficacy with Goal-Directed Client Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00371
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#38381;&#29615;&#27169;&#22411;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#30446;&#26631;&#23548;&#21521;&#23458;&#25143;&#36873;&#25321;&#65292;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31995;&#32479;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#20043;&#38388;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#29615;&#27169;&#22411;&#20998;&#26512;&#26694;&#26550;&#65292;&#20801;&#35768;&#21033;&#29992;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#26377;&#25928;&#35780;&#20272;&#35757;&#32451;&#22909;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;FL&#36807;&#31243;&#20013;&#31995;&#32479;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#20998;&#26512;&#26694;&#26550;&#30340;&#30446;&#26631;&#23548;&#21521;&#23458;&#25143;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#36873;&#25321;&#19968;&#37096;&#20998;&#23458;&#25143;&#31471;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;SMAB&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#21021;&#22987;&#19978;&#32622;&#20449;&#30028;&#65288;Quick-Init UCB&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;SMAB&#38382;&#39064;&#65292;&#21363;&#22312;&#32852;&#37030;&#20998;&#26512;&#65288;FA&#65289;&#26694;&#26550;&#19979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20449;&#24565;&#20256;&#25773;&#30340;UCB&#65288;BP-U
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00371v1 Announce Type: new  Abstract: Federated learning (FL) is an appealing paradigm for learning a global model among distributed clients while preserving data privacy. Driven by the demand for high-quality user experiences, evaluating the well-trained global model after the FL process is crucial. In this paper, we propose a closed-loop model analytics framework that allows for effective evaluation of the trained global model using clients' local data. To address the challenges posed by system and data heterogeneities in the FL process, we study a goal-directed client selection problem based on the model analytics framework by selecting a subset of clients for the model training. This problem is formulated as a stochastic multi-armed bandit (SMAB) problem. We first put forth a quick initial upper confidence bound (Quick-Init UCB) algorithm to solve this SMAB problem under the federated analytics (FA) framework. Then, we further propose a belief propagation-based UCB (BP-U
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#38543;&#26426;&#26435;&#37325;&#25200;&#21160;&#26041;&#27861;&#29992;&#20110;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#20004;&#26041;&#38754;&#30340;&#25913;&#36827;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00357</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38543;&#26426;&#26435;&#37325;&#25200;&#21160;&#20197;&#26377;&#25928;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Revisiting Random Weight Perturbation for Efficiently Improving Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00357
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38543;&#26426;&#26435;&#37325;&#25200;&#21160;&#26041;&#27861;&#29992;&#20110;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#20004;&#26041;&#38754;&#30340;&#25913;&#36827;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#20004;&#31181;&#26041;&#27861;&#30340;&#20998;&#25903;&#24050;&#34987;&#25552;&#20986;&#20197;&#23547;&#27714;&#24179;&#22374;&#26368;&#23567;&#20540;&#24182;&#25913;&#21892;&#27867;&#21270;&#65306;&#19968;&#31181;&#26159;&#30001;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#39046;&#23548;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#26435;&#37325;&#25200;&#21160;&#65288;AWP&#65289;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#37051;&#22495;&#25439;&#22833;&#65307;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#38543;&#26426;&#26435;&#37325;&#25200;&#21160;&#65288;RWP&#65289;&#26368;&#23567;&#21270;&#26399;&#26395;&#36125;&#21494;&#26031;&#30446;&#26631;&#12290;&#23613;&#31649;RWP&#22312;&#35745;&#31639;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#25968;&#23398;&#22522;&#30784;&#19978;&#19982;AWP&#26377;&#23494;&#20999;&#32852;&#31995;&#65292;&#20294;&#20854;&#23454;&#35777;&#24615;&#33021;&#19968;&#30452;&#33853;&#21518;&#20110;AWP&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;RWP&#25913;&#21892;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#20174;&#20004;&#20010;&#35282;&#24230;&#25552;&#20986;&#20102;&#25913;&#36827;: i&#65289;&#27867;&#21270;&#21644;&#25910;&#25947;&#20043;&#38388;&#30340;&#26435;&#34913;&#20197;&#21450;ii&#65289;&#38543;&#26426;&#25200;&#21160;&#29983;&#25104;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22686;&#24378;&#30340;RWP&#26041;&#27861;&#22312;&#25552;&#39640;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00357v1 Announce Type: new  Abstract: Improving the generalization ability of modern deep neural networks (DNNs) is a fundamental challenge in machine learning. Two branches of methods have been proposed to seek flat minima and improve generalization: one led by sharpness-aware minimization (SAM) minimizes the worst-case neighborhood loss through adversarial weight perturbation (AWP), and the other minimizes the expected Bayes objective with random weight perturbation (RWP). While RWP offers advantages in computation and is closely linked to AWP on a mathematical basis, its empirical performance has consistently lagged behind that of AWP. In this paper, we revisit the use of RWP for improving generalization and propose improvements from two perspectives: i) the trade-off between generalization and convergence and ii) the random perturbation generation. Through extensive experimental evaluations, we demonstrate that our enhanced RWP methods achieve greater efficiency in enhan
&lt;/p&gt;</description></item><item><title>YNetr&#27169;&#22411;&#22312;Plain Scan Liver Tumors&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;62.63%&#30340;Dice&#31995;&#25968;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#32925;&#32959;&#30244;&#26222;&#36890;&#25195;&#25551;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#31639;&#27861;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2404.00327</link><description>&lt;p&gt;
YNetr&#65306;&#22312;Plain Scan Liver Tumors (PSLT)&#19978;&#30340;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
YNetr: Dual-Encoder architecture on Plain Scan Liver Tumors (PSLT)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00327
&lt;/p&gt;
&lt;p&gt;
YNetr&#27169;&#22411;&#22312;Plain Scan Liver Tumors&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;62.63%&#30340;Dice&#31995;&#25968;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#32925;&#32959;&#30244;&#26222;&#36890;&#25195;&#25551;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#31639;&#27861;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32925;&#32959;&#30244;&#26159;&#32925;&#33039;&#20013;&#19981;&#27491;&#24120;&#30340;&#29983;&#38271;&#65292;&#21487;&#33021;&#26159;&#33391;&#24615;&#25110;&#24694;&#24615;&#65292;&#32925;&#30284;&#26159;&#20840;&#29699;&#37325;&#35201;&#30340;&#20581;&#24247;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#29992;&#20110;&#32925;&#32959;&#30244;&#26222;&#36890;&#25195;&#25551;&#20998;&#21106;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#27809;&#26377;&#30456;&#20851;&#31639;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Plain Scan Liver Tumors(PSLT)&#21644;YNetr&#12290;&#20351;&#29992;40&#20010;&#32925;&#32959;&#30244;&#26222;&#36890;&#25195;&#25551;&#20998;&#21106;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32452;&#35013;&#21644;&#27880;&#37322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;Dice&#31995;&#25968;&#20316;&#20026;&#35780;&#20272;YNetr&#20135;&#29983;&#30340;&#20998;&#21106;&#32467;&#26524;&#30340;&#25351;&#26631;&#65292;&#26377;&#21033;&#20110;&#25429;&#33719;&#19981;&#21516;&#39057;&#29575;&#20449;&#24687;&#12290;YNetr&#27169;&#22411;&#22312;PSLT&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;62.63%&#30340;Dice&#31995;&#25968;&#65292;&#36229;&#36807;&#20854;&#20182;&#20844;&#24320;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#33539;&#22260;1.22%&#12290;&#36827;&#34892;&#20102;&#19982;&#21253;&#25324; UNet 3+&#12289;XNet&#12289;UNetr&#12289;Swin UNetr&#12289;Trans-BTS&#12289;COTr&#12289;nnUNetv2 (2D)&#12289;nnUNetv2 (3D fullres)&#12289;MedNext &#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#30340;&#27604;&#36739;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00327v1 Announce Type: cross  Abstract: Background: Liver tumors are abnormal growths in the liver that can be either benign or malignant, with liver cancer being a significant health concern worldwide. However, there is no dataset for plain scan segmentation of liver tumors, nor any related algorithms. To fill this gap, we propose Plain Scan Liver Tumors(PSLT) and YNetr. Methods: A collection of 40 liver tumor plain scan segmentation datasets was assembled and annotated. Concurrently, we utilized Dice coefficient as the metric for assessing the segmentation outcomes produced by YNetr, having advantage of capturing different frequency information. Results: The YNetr model achieved a Dice coefficient of 62.63% on the PSLT dataset, surpassing the other publicly available model by an accuracy margin of 1.22%. Comparative evaluations were conducted against a range of models including UNet 3+, XNet, UNetr, Swin UNetr, Trans-BTS, COTr, nnUNetv2 (2D), nnUNetv2 (3D fullres), MedNext
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#31163;&#32676;&#20540;&#21512;&#25104;&#26041;&#27861;&#65288;CLIP-OS&#65289;&#65292;&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;OOD&#26816;&#27979;&#20013;&#35299;&#20915;&#32570;&#20047;&#21487;&#38752;OOD&#30417;&#30563;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;ID&#30456;&#20851;&#20449;&#24687;&#21644;ID&#19981;&#30456;&#20851;&#20449;&#24687;&#30340;&#20998;&#31163;&#65292;&#36890;&#36807;&#28151;&#21512;ID&#30456;&#20851;&#29305;&#24449;&#21512;&#25104;&#21487;&#38752;&#30340;OOD&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2404.00323</link><description>&lt;p&gt;
&#22522;&#20110;CLIP&#30340;&#31163;&#32676;&#20540;&#21512;&#25104;&#29992;&#20110;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CLIP-driven Outliers Synthesis for few-shot OOD detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00323
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#31163;&#32676;&#20540;&#21512;&#25104;&#26041;&#27861;&#65288;CLIP-OS&#65289;&#65292;&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;OOD&#26816;&#27979;&#20013;&#35299;&#20915;&#32570;&#20047;&#21487;&#38752;OOD&#30417;&#30563;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;ID&#30456;&#20851;&#20449;&#24687;&#21644;ID&#19981;&#30456;&#20851;&#20449;&#24687;&#30340;&#20998;&#31163;&#65292;&#36890;&#36807;&#28151;&#21512;ID&#30456;&#20851;&#29305;&#24449;&#21512;&#25104;&#21487;&#38752;&#30340;OOD&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;OOD&#26816;&#27979;&#30528;&#37325;&#20110;&#35782;&#21035;&#37027;&#20123;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#30340;&#12289;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;ID&#22270;&#20687;&#30340;out-of-distribution (OOD) &#22270;&#20687;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20027;&#27969;&#30340;&#31574;&#30053;&#26159;&#22522;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;CLIP&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#35270;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#32570;&#20047;&#21487;&#38752;&#30340;OOD&#30417;&#30563;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;ID&#21644;OOD&#20043;&#38388;&#20135;&#29983;&#20559;&#35265;&#30340;&#36793;&#30028;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;CLIP&#30340;&#31163;&#32676;&#20540;&#21512;&#25104;&#65288;CLIP-OS&#65289;&#12290;&#39318;&#20808;&#65292;CLIP-OS&#36890;&#36807;&#26032;&#25552;&#20986;&#30340;patch uniform convolution&#22686;&#24378;&#20102;patch&#32423;&#29305;&#24449;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;CLIP-surgery-discrepancy&#33258;&#36866;&#24212;&#22320;&#33719;&#21462;ID&#30456;&#20851;&#20449;&#24687;&#30340;&#27604;&#20363;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;ID&#30456;&#20851;&#20449;&#24687;&#21644;ID&#19981;&#30456;&#20851;&#20449;&#24687;&#20043;&#38388;&#30340;&#20998;&#31163;&#12290;&#25509;&#19979;&#26469;&#65292;CLIP-OS&#36890;&#36807;&#28151;&#21512;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;ID&#30456;&#20851;&#29305;&#24449;&#26469;&#21512;&#25104;&#21487;&#38752;&#30340;OOD&#25968;&#25454;&#65292;&#20197;&#25552;&#20379;OOD&#30417;&#30563;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00323v1 Announce Type: cross  Abstract: Few-shot OOD detection focuses on recognizing out-of-distribution (OOD) images that belong to classes unseen during training, with the use of only a small number of labeled in-distribution (ID) images. Up to now, a mainstream strategy is based on large-scale vision-language models, such as CLIP. However, these methods overlook a crucial issue: the lack of reliable OOD supervision information, which can lead to biased boundaries between in-distribution (ID) and OOD. To tackle this problem, we propose CLIP-driven Outliers Synthesis~(CLIP-OS). Firstly, CLIP-OS enhances patch-level features' perception by newly proposed patch uniform convolution, and adaptively obtains the proportion of ID-relevant information by employing CLIP-surgery-discrepancy, thus achieving separation between ID-relevant and ID-irrelevant. Next, CLIP-OS synthesizes reliable OOD data by mixing up ID-relevant features from different classes to provide OOD supervision i
&lt;/p&gt;</description></item><item><title>TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2404.00297</link><description>&lt;p&gt;
TRABSA&#65306;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;BiLSTM&#21644;Twitter-RoBERTa&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00297
&lt;/p&gt;
&lt;p&gt;
TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#20844;&#20247;&#33286;&#35770;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#27169;&#22411;&#38754;&#20020;&#30528;&#35821;&#35328;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TRABSA&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#21033;&#29992;&#22312;124M&#26465;&#25512;&#25991;&#19978;&#35757;&#32451;&#30340;RoBERTa&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24046;&#36317;&#65292;&#30830;&#20445;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23558;&#26469;&#33258;32&#20010;&#22269;&#23478;&#21644;&#32654;&#22269;&#21508;&#24030;&#30340;&#25512;&#25991;&#19982;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20845;&#31181;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#19977;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26631;&#27880;&#25216;&#26415;&#65292;&#24182;&#36873;&#25321;&#20102;&#26368;&#20339;&#25216;&#26415;&#20197;&#23454;&#29616;&#26368;&#20339;&#24773;&#24863;&#20998;&#26512;&#25928;&#26524;&#12290;TRABSA&#20197;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#22686;&#30410;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#20102;&#19968;&#33268;&#30340;&#20248;&#36234;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;SHAP&#21644;LIME&#20998;&#26512;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22686;&#24378;&#20102;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00297v1 Announce Type: new  Abstract: Sentiment analysis is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating transformer-based architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six word-embedding techniques and three lexicon-based labeling techniques, selecting the best for optimal sentiment analysis. TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence i
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2404.00282</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#26597;:&#27010;&#24565;&#12289;&#20998;&#31867;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00282
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25317;&#26377;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#39640;&#32423;&#36890;&#29992;&#33021;&#21147;&#65292;&#23427;&#20204;&#22312;&#22686;&#24378;&#23398;&#20064;&#26041;&#38754;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#20219;&#21153;&#35268;&#21010;&#31561;&#26041;&#38754;&#23637;&#29616;&#20986;&#28508;&#21147;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#29616;&#26377;$\textit{LLM&#22686;&#24378;RL}$&#25991;&#29486;&#65292;&#24635;&#32467;&#20102;&#20854;&#19982;&#20256;&#32479;RL&#26041;&#27861;&#30340;&#29305;&#24449;&#65292;&#26088;&#22312;&#28548;&#28165;&#30740;&#31350;&#33539;&#22260;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#21033;&#29992;&#32463;&#20856;&#30340;Agent-&#29615;&#22659;&#20132;&#20114;&#33539;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#27861;&#65292;&#31995;&#32479;&#22320;&#23558;LLMs&#22312;RL&#20013;&#30340;&#21151;&#33021;&#20998;&#31867;&#65292;&#21253;&#25324;&#22235;&#31181;&#35282;&#33394;&#65306;&#20449;&#24687;&#22788;&#29702;&#22120;&#12289;&#22870;&#21169;&#35774;&#35745;&#32773;&#12289;&#20915;&#31574;&#32773;&#21644;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#27599;&#20010;&#35282;&#33394;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#26041;&#27861;&#35770;&#65292;&#20998;&#26512;&#20102;&#32531;&#35299;&#30340;&#29305;&#23450;RL&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#28508;&#22312;&#24212;&#29992;&#12289;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00282v1 Announce Type: cross  Abstract: With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospecti
&lt;/p&gt;</description></item><item><title>TG-NAS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#36890;&#29992;&#20195;&#29702;&#65292;&#21033;&#29992;Transformer&#30340;&#36816;&#31639;&#31526;&#23884;&#20837;&#29983;&#25104;&#22120;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#26550;&#26500;&#24615;&#33021;&#65292;&#25351;&#23548;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#12290;</title><link>https://arxiv.org/abs/2404.00271</link><description>&lt;p&gt;
TG-NAS&#65306;&#21033;&#29992;Transformer&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#19982;&#38646;&#25104;&#26412;&#20195;&#29702;&#36827;&#34892;&#39640;&#25928;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00271
&lt;/p&gt;
&lt;p&gt;
TG-NAS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#36890;&#29992;&#20195;&#29702;&#65292;&#21033;&#29992;Transformer&#30340;&#36816;&#31639;&#31526;&#23884;&#20837;&#29983;&#25104;&#22120;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#26550;&#26500;&#24615;&#33021;&#65292;&#25351;&#23548;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;(NAS)&#26159;&#19968;&#31181;&#21457;&#29616;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26550;&#26500;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#32791;&#26102;&#30340;&#35757;&#32451;&#25110;&#23494;&#38598;&#30340;&#37319;&#26679;&#21644;&#35780;&#20272;&#12290;&#38646;&#25104;&#26412;NAS&#26088;&#22312;&#20026;&#26550;&#26500;&#24615;&#33021;&#39044;&#27979;&#21019;&#24314;&#20813;&#35757;&#32451;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20195;&#29702;&#24615;&#33021;&#20122;&#20248;&#65292;&#24182;&#19988;&#24120;&#24120;&#34987;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#25110;&#28014;&#28857;&#36816;&#31639;&#27425;&#25968;&#31561;&#31616;&#21333;&#25351;&#26631;&#25152;&#36229;&#36234;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#26080;&#27861;&#23558;&#27867;&#21270;&#21040;&#26032;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#20854;&#20013;&#20855;&#26377;&#26410;&#35265;&#26032;&#31867;&#22411;&#36816;&#31639;&#31526;&#19988;&#19981;&#24102;&#26377;&#40644;&#37329;&#20934;&#30830;&#24230;&#12290;&#19968;&#20010;&#26222;&#36941;&#26368;&#20248;&#30340;&#20195;&#29702;&#20173;&#28982;&#38590;&#20197;&#25214;&#21040;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TG-NAS&#65292;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#36816;&#31639;&#31526;&#23884;&#20837;&#29983;&#25104;&#22120;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#26469;&#39044;&#27979;&#26550;&#26500;&#24615;&#33021;&#30340;&#26032;&#22411;&#27169;&#22411;&#36890;&#29992;&#20195;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#25351;&#23548;&#30528;&#22312;&#20219;&#20309;&#32473;&#23450;&#25628;&#32034;&#31354;&#38388;&#20869;&#36827;&#34892;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00271v1 Announce Type: cross  Abstract: Neural architecture search (NAS) is an effective method for discovering new convolutional neural network (CNN) architectures. However, existing approaches often require time-consuming training or intensive sampling and evaluations. Zero-shot NAS aims to create training-free proxies for architecture performance prediction. However, existing proxies have suboptimal performance, and are often outperformed by simple metrics such as model parameter counts or the number of floating-point operations. Besides, existing model-based proxies cannot be generalized to new search spaces with unseen new types of operators without golden accuracy truth. A universally optimal proxy remains elusive. We introduce TG-NAS, a novel model-based universal proxy that leverages a transformer-based operator embedding generator and a graph convolution network (GCN) to predict architecture performance. This approach guides neural architecture search across any giv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiLM&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#20316;&#20026;&#21512;&#25104;&#35757;&#32451;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#23884;&#20837;&#32423;&#21035;&#33976;&#39311;&#25968;&#25454;&#38598;&#26080;&#27861;&#29992;&#20110;&#35757;&#32451;&#20854;&#20182;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00264</link><description>&lt;p&gt;
&#23558;&#25968;&#25454;&#38598;&#25552;&#28860;&#20026;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25991;&#26412;&#32423;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00264
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiLM&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#20316;&#20026;&#21512;&#25104;&#35757;&#32451;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#23884;&#20837;&#32423;&#21035;&#33976;&#39311;&#25968;&#25454;&#38598;&#26080;&#27861;&#29992;&#20110;&#35757;&#32451;&#20854;&#20182;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#23569;&#37327;&#20449;&#24687;&#20016;&#23500;&#30340;&#21512;&#25104;&#26679;&#26412;&#26469;&#21387;&#32553;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20351;&#24471;&#22312;&#20854;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#33021;&#22815;&#19982;&#22312;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32593;&#32476;&#19968;&#26679;&#22909;&#12290;&#24403;&#21069;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#23558;&#27599;&#20010;&#21512;&#25104;&#26679;&#26412;&#21019;&#24314;&#20026;&#35789;&#23884;&#20837;&#24207;&#21015;&#32780;&#19981;&#26159;&#25991;&#26412;&#65292;&#20197;&#24212;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65307;&#28982;&#32780;&#65292;&#36825;&#31181;&#23884;&#20837;&#32423;&#21035;&#30340;&#33976;&#39311;&#25968;&#25454;&#38598;&#26080;&#27861;&#29992;&#20110;&#35757;&#32451;&#20854;&#20182;&#27169;&#22411;&#65292;&#20854;&#35789;&#23884;&#20837;&#26435;&#37325;&#19981;&#21516;&#20110;&#29992;&#20110;&#33976;&#39311;&#30340;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#31216;&#20026;Distilling dataset into Language Model&#65288;DiLM&#65289;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#25968;&#25454;&#20316;&#20026;&#21512;&#25104;&#35757;&#32451;&#26679;&#26412;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#20248;&#21270;&#21512;&#25104;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;DiLM&#65292;&#24182;&#23637;&#31034;&#20102;&#20174;DiLM &#20013;&#33976;&#39311;&#24471;&#21040;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#20248;&#31168;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00264v1 Announce Type: new  Abstract: Dataset distillation aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset. Current text dataset distillation methods create each synthetic sample as a sequence of word embeddings instead of a text to apply gradient-based optimization; however, such embedding-level distilled datasets cannot be used for training other models whose word embedding weights are different from the model used for distillation. To address this issue, we propose a novel text dataset distillation approach, called Distilling dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as text data, instead of directly optimizing synthetic samples. We evaluated DiLM on various text classification datasets and showed that distilled synthetic datasets from DiLM outp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;YOLO&#26550;&#26500;&#30340;YOLOOC&#26816;&#27979;&#22120;&#65292;&#38024;&#23545;&#24320;&#25918;&#31867;&#21035;&#35774;&#32622;&#24341;&#20837;&#20102;&#26631;&#31614;&#24179;&#28369;&#65292;&#26377;&#25928;&#24212;&#23545;&#26032;&#31867;&#21035;&#26816;&#27979;&#21644;&#22686;&#37327;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.00257</link><description>&lt;p&gt;
YOLOOC: &#22522;&#20110;YOLO&#30340;&#24320;&#25918;&#31867;&#21035;&#22686;&#37327;&#30446;&#26631;&#26816;&#27979;&#19982;&#26032;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00257
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;YOLO&#26550;&#26500;&#30340;YOLOOC&#26816;&#27979;&#22120;&#65292;&#38024;&#23545;&#24320;&#25918;&#31867;&#21035;&#35774;&#32622;&#24341;&#20837;&#20102;&#26631;&#31614;&#24179;&#28369;&#65292;&#26377;&#25928;&#24212;&#23545;&#26032;&#31867;&#21035;&#26816;&#27979;&#21644;&#22686;&#37327;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#36816;&#29992;&#65292;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#65288;OWOD&#65289;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#25361;&#25112;&#22312;&#20110;&#27169;&#22411;&#22914;&#20309;&#26816;&#27979;&#26032;&#31867;&#21035;&#65292;&#28982;&#21518;&#22686;&#37327;&#23398;&#20064;&#23427;&#20204;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#24050;&#30693;&#30340;&#31867;&#21035;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#24378;&#30417;&#30563;&#25110;&#24369;&#30417;&#30563;&#30340;&#26032;&#31867;&#21035;&#25968;&#25454;&#29992;&#20110;&#26032;&#31867;&#21035;&#26816;&#27979;&#65292;&#36825;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#26032;&#31867;&#21035;&#21482;&#22312;&#25512;&#26029;&#38454;&#27573;&#36935;&#21040;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLO&#26550;&#26500;&#30340;&#26032;OWOD&#26816;&#27979;&#22120;YOLOOC&#65292;&#19987;&#38376;&#38024;&#23545;&#24320;&#25918;&#31867;&#21035;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#31614;&#24179;&#28369;&#20197;&#38450;&#27490;&#26816;&#27979;&#22120;&#36807;&#20110;&#33258;&#20449;&#22320;&#23558;&#26032;&#31867;&#21035;&#26144;&#23556;&#21040;&#24050;&#30693;&#31867;&#21035;&#24182;&#21457;&#29616;&#26032;&#31867;&#21035;&#12290;&#22312;&#25105;&#20204;&#26356;&#21152;&#29616;&#23454;&#30340;&#35774;&#32622;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25105;&#20204;&#30340;&#26032;&#22522;&#20934;&#19979;&#21457;&#29616;&#26032;&#31867;&#21035;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00257v1 Announce Type: cross  Abstract: Because of its use in practice, open-world object detection (OWOD) has gotten a lot of attention recently. The challenge is how can a model detect novel classes and then incrementally learn them without forgetting previously known classes. Previous approaches hinge on strongly-supervised or weakly-supervised novel-class data for novel-class detection, which may not apply to real applications. We construct a new benchmark that novel classes are only encountered at the inference stage. And we propose a new OWOD detector YOLOOC, based on the YOLO architecture yet for the Open-Class setup. We introduce label smoothing to prevent the detector from over-confidently mapping novel classes to known classes and to discover novel classes. Extensive experiments conducted on our more realistic setup demonstrate the effectiveness of our method for discovering novel classes in our new benchmark.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32858;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#34507;&#30333;&#36136;&#30340;&#19968;&#32423;&#21644;&#19977;&#32423;&#32467;&#26500;&#20449;&#24687;&#65292;&#33258;&#21160;&#21457;&#29616;&#34507;&#30333;&#36136;&#30340;&#20851;&#38190;&#32452;&#20998;&#65292;&#24182;&#21033;&#29992;&#36845;&#20195;&#32858;&#31867;&#31574;&#30053;&#21019;&#24314;&#23618;&#27425;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#34507;&#30333;&#36136;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.00254</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#34920;&#31034;&#23398;&#20064;&#30340;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Clustering for Protein Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00254
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32858;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#34507;&#30333;&#36136;&#30340;&#19968;&#32423;&#21644;&#19977;&#32423;&#32467;&#26500;&#20449;&#24687;&#65292;&#33258;&#21160;&#21457;&#29616;&#34507;&#30333;&#36136;&#30340;&#20851;&#38190;&#32452;&#20998;&#65292;&#24182;&#21033;&#29992;&#36845;&#20195;&#32858;&#31867;&#31574;&#30053;&#21019;&#24314;&#23618;&#27425;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#34507;&#30333;&#36136;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#34920;&#31034;&#23398;&#20064;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#27688;&#22522;&#37240;&#24207;&#21015;&#20013;&#25429;&#33719;&#34507;&#30333;&#36136;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#24182;&#38750;&#25152;&#26377;&#27688;&#22522;&#37240;&#23545;&#20110;&#34507;&#30333;&#36136;&#30340;&#25240;&#21472;&#21644;&#27963;&#24615;&#37117;&#21516;&#26679;&#37325;&#35201;&#30340;&#20107;&#23454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32858;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#34507;&#30333;&#36136;&#30340;&#19968;&#32423;&#21644;&#19977;&#32423;&#32467;&#26500;&#20449;&#24687;&#65292;&#21487;&#20197;&#33258;&#21160;&#21457;&#29616;&#34507;&#30333;&#36136;&#30340;&#20851;&#38190;&#32452;&#20998;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#34507;&#30333;&#36136;&#35270;&#20026;&#19968;&#20010;&#22270;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#20195;&#34920;&#19968;&#20010;&#27688;&#22522;&#37240;&#65292;&#27599;&#26465;&#36793;&#20195;&#34920;&#27688;&#22522;&#37240;&#20043;&#38388;&#30340;&#31354;&#38388;&#25110;&#24207;&#21015;&#36830;&#25509;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#36845;&#20195;&#32858;&#31867;&#31574;&#30053;&#23558;&#33410;&#28857;&#26681;&#25454;&#23427;&#20204;&#30340;&#19968;&#32500;&#21644;&#19977;&#32500;&#20301;&#32622;&#20998;&#32452;&#65292;&#24182;&#20026;&#27599;&#20010;&#32676;&#20998;&#37197;&#20998;&#25968;&#12290;&#25105;&#20204;&#36873;&#25321;&#24471;&#20998;&#26368;&#39640;&#30340;&#32676;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#30340;&#20013;&#24515;&#33410;&#28857;&#36827;&#34892;&#19979;&#19968;&#36718;&#30340;&#32858;&#31867;&#65292;&#30452;&#21040;&#33719;&#24471;&#23618;&#27425;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00254v1 Announce Type: new  Abstract: Protein representation learning is a challenging task that aims to capture the structure and function of proteins from their amino acid sequences. Previous methods largely ignored the fact that not all amino acids are equally important for protein folding and activity. In this article, we propose a neural clustering framework that can automatically discover the critical components of a protein by considering both its primary and tertiary structure information. Our framework treats a protein as a graph, where each node represents an amino acid and each edge represents a spatial or sequential connection between amino acids. We then apply an iterative clustering strategy to group the nodes into clusters based on their 1D and 3D positions and assign scores to each cluster. We select the highest-scoring clusters and use their medoid nodes for the next iteration of clustering, until we obtain a hierarchical and informative representation of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#36801;&#31227;&#23398;&#20064;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20026;&#36807;&#31243;&#25511;&#21046;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00247</link><description>&lt;p&gt;
&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#20419;&#36827;&#36807;&#31243;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Facilitating Reinforcement Learning for Process Control Using Transfer Learning: Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36801;&#31227;&#23398;&#20064;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20026;&#36807;&#31243;&#25511;&#21046;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36801;&#31227;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#20026;&#36807;&#31243;&#25511;&#21046;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#25552;&#20379;&#20102;&#28145;&#20837;&#35265;&#35299;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#36807;&#31243;&#24037;&#19994;&#39046;&#22495;&#24212;&#29992;DRL&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#24341;&#20837;&#36801;&#31227;&#23398;&#20064;&#30340;&#24517;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#24314;&#35758;&#21644;&#23637;&#26395;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#36801;&#31227;&#23398;&#20064;&#19982;DRL&#32467;&#21512;&#36215;&#26469;&#21152;&#24378;&#36807;&#31243;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00247v1 Announce Type: cross  Abstract: This paper provides insights into deep reinforcement learning (DRL) for process control from the perspective of transfer learning. We analyze the challenges of applying DRL in the field of process industries and the necessity of introducing transfer learning. Furthermore, recommendations and prospects are provided for future research directions on how transfer learning can be integrated with DRL to empower process control.
&lt;/p&gt;</description></item><item><title>&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#20449;&#24687;&#23433;&#20840;&#19982;&#38544;&#31169;&#38754;&#20020;&#26032;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#24378;&#22823;&#21644;&#26356;&#20855;&#38887;&#24615;&#30340;&#23433;&#20840;&#26041;&#26696;&#65292;&#36825;&#26412;&#20070;&#21576;&#29616;&#20102;&#23494;&#30721;&#23398;&#21644;&#35745;&#31639;&#19982;&#36890;&#20449;&#23433;&#20840;&#39046;&#22495;&#30340;&#26368;&#21069;&#27839;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2404.00235</link><description>&lt;p&gt;
&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#20449;&#24687;&#23433;&#20840;&#19982;&#38544;&#31169;&#65306;&#19968;&#20123;&#31934;&#36873;&#20027;&#39064;
&lt;/p&gt;
&lt;p&gt;
Information Security and Privacy in the Digital World: Some Selected Topics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00235
&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#20449;&#24687;&#23433;&#20840;&#19982;&#38544;&#31169;&#38754;&#20020;&#26032;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#24378;&#22823;&#21644;&#26356;&#20855;&#38887;&#24615;&#30340;&#23433;&#20840;&#26041;&#26696;&#65292;&#36825;&#26412;&#20070;&#21576;&#29616;&#20102;&#23494;&#30721;&#23398;&#21644;&#35745;&#31639;&#19982;&#36890;&#20449;&#23433;&#20840;&#39046;&#22495;&#30340;&#26368;&#21069;&#27839;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#29289;&#32852;&#32593;&#26102;&#20195;&#65292;&#38543;&#30528;&#25968;&#25454;&#37327;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#22788;&#29702;&#12289;&#20998;&#26512;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#35782;&#21035;&#34394;&#20551;&#20449;&#24687;&#12289;&#20445;&#25252;&#25935;&#24863;&#25968;&#25454;&#38544;&#31169;&#31561;&#22810;&#39033;&#26032;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#23545;&#20110;&#26356;&#24378;&#22823;&#21644;&#26356;&#20855;&#38887;&#24615;&#30340;&#36523;&#20221;&#39564;&#35777;&#12289;&#23436;&#25972;&#24615;&#20445;&#25252;&#12289;&#21152;&#23494;&#12289;&#19981;&#21487;&#21542;&#35748;&#20197;&#21450;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#26041;&#26696;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#26412;&#20070;&#30340;&#31456;&#33410;&#21576;&#29616;&#20102;&#23494;&#30721;&#23398;&#21644;&#35745;&#31639;&#19982;&#36890;&#20449;&#23433;&#20840;&#39046;&#22495;&#19968;&#20123;&#26368;&#21069;&#27839;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00235v1 Announce Type: cross  Abstract: In the era of generative artificial intelligence and the Internet of Things, while there is explosive growth in the volume of data and the associated need for processing, analysis, and storage, several new challenges are faced in identifying spurious and fake information and protecting the privacy of sensitive data. This has led to an increasing demand for more robust and resilient schemes for authentication, integrity protection, encryption, non-repudiation, and privacy-preservation of data. The chapters in this book present some of the state-of-the-art research works in the field of cryptography and security in computing and communications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;Portfolio&#26469;&#25913;&#36827;AutoMPC&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#21270;BO&#30340;&#21021;&#22987;&#35774;&#35745;&#65292;&#24182;&#36890;&#36807;&#22266;&#23450;&#21021;&#22987;&#37197;&#32622;&#26469;&#31283;&#23450;&#35843;&#33410;&#36807;&#31243;&#65292;&#22312;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#20869;&#36229;&#36234;&#20102;&#32431;BO&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00232</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#39640;&#25928;&#33258;&#21160;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
Efficient Automatic Tuning for Data-driven Model Predictive Control via Meta-Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;Portfolio&#26469;&#25913;&#36827;AutoMPC&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#21270;BO&#30340;&#21021;&#22987;&#35774;&#35745;&#65292;&#24182;&#36890;&#36807;&#22266;&#23450;&#21021;&#22987;&#37197;&#32622;&#26469;&#31283;&#23450;&#35843;&#33410;&#36807;&#31243;&#65292;&#22312;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#20869;&#36229;&#36234;&#20102;&#32431;BO&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AutoMPC&#26159;&#19968;&#20010;&#33258;&#21160;&#20248;&#21270;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;Python&#36719;&#20214;&#21253;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#32431;&#36125;&#21494;&#26031;&#20248;&#21270;&#25506;&#32034;&#22823;&#25628;&#32034;&#31354;&#38388;&#26102;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#35745;&#31639;&#36127;&#25285;&#36807;&#37325;&#21644;&#19981;&#31283;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Portfolio&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28909;&#21551;&#21160;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#25552;&#39640;AutoMPC&#30340;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;Portfolio&#21033;&#29992;&#21069;&#19968;&#20219;&#21153;&#30340;&#22810;&#26679;&#37197;&#32622;&#38598;&#20248;&#21270;BO&#30340;&#21021;&#22987;&#35774;&#35745;&#65292;&#24182;&#36890;&#36807;&#22266;&#23450;&#21021;&#22987;&#37197;&#32622;&#32780;&#19981;&#26159;&#38543;&#26426;&#36873;&#25321;&#26469;&#31283;&#23450;&#35843;&#33410;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;11&#20010;&#38750;&#32447;&#24615;&#25511;&#21046;&#20223;&#30495;&#22522;&#20934;&#21644;1&#20010;&#29289;&#29702;&#27700;&#19979;&#36719;&#20307;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#19978;&#65292;Portfolio&#22312;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#20869;&#20248;&#20110;&#32431;BO&#23547;&#25214;AutoMPC&#30340;&#29702;&#24819;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00232v1 Announce Type: cross  Abstract: AutoMPC is a Python package that automates and optimizes data-driven model predictive control. However, it can be computationally expensive and unstable when exploring large search spaces using pure Bayesian Optimization (BO). To address these issues, this paper proposes to employ a meta-learning approach called Portfolio that improves AutoMPC's efficiency and stability by warmstarting BO. Portfolio optimizes initial designs for BO using a diverse set of configurations from previous tasks and stabilizes the tuning process by fixing initial configurations instead of selecting them randomly. Experimental results demonstrate that Portfolio outperforms the pure BO in finding desirable solutions for AutoMPC within limited computational resources on 11 nonlinear control simulation benchmarks and 1 physical underwater soft robot dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;TransDeformer&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#33136;&#26894;&#36718;&#24275;&#30340;&#39640;&#31354;&#38388;&#20934;&#30830;&#24615;&#37325;&#24314;&#65292;&#24182;&#36328;&#24739;&#32773;&#23454;&#29616;&#20102;&#32593;&#26684;&#23545;&#24212;&#65292;&#20026;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#25552;&#20379;&#20102;&#21487;&#38752;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#21464;&#20307;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2404.00231</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24418;&#29366;&#21464;&#24418;&#32593;&#32476;&#29992;&#20110;&#26080;&#20266;&#24433;&#20960;&#20309;&#37325;&#26500;&#39592;&#30406;&#33136;&#26894;MR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00231
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;TransDeformer&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#33136;&#26894;&#36718;&#24275;&#30340;&#39640;&#31354;&#38388;&#20934;&#30830;&#24615;&#37325;&#24314;&#65292;&#24182;&#36328;&#24739;&#32773;&#23454;&#29616;&#20102;&#32593;&#26684;&#23545;&#24212;&#65292;&#20026;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#25552;&#20379;&#20102;&#21487;&#38752;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#21464;&#20307;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33136;&#26894;&#26894;&#38388;&#30424;&#36864;&#21464;&#65292;&#26159;&#33136;&#26894;&#38388;&#30424;&#28176;&#36827;&#24615;&#32467;&#26500;&#24615;&#30952;&#25439;&#65292;&#34987;&#35748;&#20026;&#22312;&#33136;&#37096;&#30140;&#30171;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20840;&#29699;&#20581;&#24247;&#20851;&#27880;&#28966;&#28857;&#12290;&#20174;MR&#22270;&#20687;&#20013;&#33258;&#21160;&#37325;&#24314;&#33136;&#26894;&#20960;&#20309;&#24418;&#29366;&#65292;&#23558;&#20351;&#21307;&#23398;&#21442;&#25968;&#30340;&#24555;&#36895;&#27979;&#37327;&#25104;&#20026;&#21487;&#33021;&#65292;&#20197;&#35780;&#20272;&#33136;&#26894;&#29366;&#24577;&#65292;&#20174;&#32780;&#30830;&#23450;&#21512;&#36866;&#30340;&#27835;&#30103;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#25216;&#26415;&#36890;&#24120;&#20250;&#29983;&#25104;&#38169;&#35823;&#30340;&#20998;&#21106;&#25110;&#19981;&#36866;&#21512;&#21307;&#23398;&#21442;&#25968;&#27979;&#37327;&#30340;&#26080;&#32467;&#26500;&#28857;&#20113;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransDeformer&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#39640;&#31354;&#38388;&#20934;&#30830;&#24230;&#21644;&#24739;&#32773;&#38388;&#32593;&#26684;&#23545;&#24212;&#30340;&#26041;&#24335;&#37325;&#24314;&#33136;&#26894;&#36718;&#24275;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;TransDeformer&#30340;&#21464;&#31181;&#29992;&#20110;&#38169;&#35823;&#20272;&#35745;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#26032;&#30340;&#27880;&#24847;&#21147;&#20844;&#24335;&#65292;&#23558;&#22270;&#20687;&#29305;&#24449;&#21644;&#26631;&#35760;&#21270;&#30340;&#36718;&#24275;&#29305;&#24449;&#38598;&#25104;&#36215;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00231v1 Announce Type: cross  Abstract: Lumbar disc degeneration, a progressive structural wear and tear of lumbar intervertebral disc, is regarded as an essential role on low back pain, a significant global health concern. Automated lumbar spine geometry reconstruction from MR images will enable fast measurement of medical parameters to evaluate the lumbar status, in order to determine a suitable treatment. Existing image segmentation-based techniques often generate erroneous segments or unstructured point clouds, unsuitable for medical parameter measurement. In this work, we present TransDeformer: a novel attention-based deep learning approach that reconstructs the contours of the lumbar spine with high spatial accuracy and mesh correspondence across patients, and we also present a variant of TransDeformer for error estimation. Specially, we devise new attention modules with a new attention formula, which integrates image features and tokenized contour features to predict 
&lt;/p&gt;</description></item><item><title>InfLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#21517;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#26088;&#22312;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2404.00228</link><description>&lt;p&gt;
InfLoRA&#65306;&#26080;&#24178;&#25200;&#30340;&#20302;&#31209;&#33258;&#36866;&#24212;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00228
&lt;/p&gt;
&lt;p&gt;
InfLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#21517;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#26088;&#22312;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#35201;&#27714;&#27169;&#22411;&#20381;&#27425;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#12290;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#24212;&#20855;&#22791;&#22312;&#26087;&#20219;&#21153;&#19978;&#32500;&#25345;&#24615;&#33021;&#65288;&#31283;&#23450;&#24615;&#65289;&#21644;&#19981;&#26029;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#65288;&#21487;&#22609;&#24615;&#65289;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#29616;&#26377;&#22522;&#20110;PEFT&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#38750;PEFT&#26041;&#27861;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#24182;&#26410;&#32771;&#34385;&#22914;&#20309;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00228v1 Announce Type: cross  Abstract: Continual learning requires the model to learn multiple tasks sequentially. In continual learning, the model should possess the ability to maintain its performance on old tasks (stability) and the ability to adapt to new tasks continuously (plasticity). Recently, parameter-efficient fine-tuning (PEFT), which involves freezing a pre-trained model and injecting a small number of learnable parameters to adapt to downstream tasks, has gained increasing popularity in continual learning. Although existing continual learning methods based on PEFT have demonstrated superior performance compared to those not based on PEFT, most of them do not consider how to eliminate the interference of the new task on the old tasks, which inhibits the model from making a good trade-off between stability and plasticity. In this work, we propose a new PEFT method, called interference-free low-rank adaptation (InfLoRA), for continual learning. InfLoRA injects a 
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#20026;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#35843;&#26597;&#23545;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#24322;&#26500;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2404.00225</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#27169;&#22411;&#21450;&#20854;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Contrastive Learning for Foundation Models and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00225
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#20026;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#35843;&#26597;&#23545;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#24322;&#26500;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#21033;&#29992;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#24314;&#27169;&#22823;&#35268;&#27169;&#24322;&#26500;&#25968;&#25454;&#30340;&#26032;&#20852;&#33539;&#24335;&#21463;&#21040;&#20851;&#27880;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#32039;&#20945;&#12289;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#24418;&#24335;&#32780;&#21463;&#30410;&#20110;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20219;&#20309;&#26631;&#31614;&#20449;&#24687;&#12290;&#26412;&#35843;&#26597;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24322;&#26500;&#23545;&#27604;&#23398;&#20064;&#30340;&#24403;&#21069;&#24773;&#20917;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#39640;&#32423;&#26041;&#27861;&#22914;&#20309;&#22788;&#29702;&#35270;&#22270;&#30340;&#24322;&#26500;&#24615;&#65292;&#20197;&#21450;&#23545;&#27604;&#23398;&#20064;&#22312;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00225v1 Announce Type: new  Abstract: In the era of big data and Artificial Intelligence, an emerging paradigm is to utilize contrastive self-supervised learning to model large-scale heterogeneous data. Many existing foundation models benefit from the generalization capability of contrastive self-supervised learning by learning compact and high-quality representations without relying on any label information. Amidst the explosive advancements in foundation models across multiple domains, including natural language processing and computer vision, a thorough survey on heterogeneous contrastive learning for the foundation model is urgently needed. In response, this survey critically evaluates the current landscape of heterogeneous contrastive learning for foundation models, highlighting the open challenges and future trends of contrastive learning. In particular, we first present how the recent advanced contrastive learning-based methods deal with view heterogeneity and how con
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#37096;&#20998;&#35266;&#27979;&#22810;&#20256;&#24863;&#22120;&#24207;&#36143;&#21464;&#28857;&#26816;&#27979;&#30340;&#33258;&#36866;&#24212;&#19978;&#32622;&#20449;&#21306;&#38388;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;AUCRSS&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#23454;&#29616;&#39640;&#25928;&#30340;&#21464;&#28857;&#26816;&#27979;&#21644;&#23450;&#20301;&#12290;</title><link>https://arxiv.org/abs/2404.00220</link><description>&lt;p&gt;
&#21487;&#37096;&#20998;&#35266;&#27979;&#24207;&#36143;&#33258;&#30456;&#20851;&#25968;&#25454;&#30340;&#21464;&#28857;&#26816;&#27979;&#65306;&#19978;&#32622;&#20449;&#21306;&#38388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Partially-Observable Sequential Change-Point Detection for Autocorrelated Data via Upper Confidence Region
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00220
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#37096;&#20998;&#35266;&#27979;&#22810;&#20256;&#24863;&#22120;&#24207;&#36143;&#21464;&#28857;&#26816;&#27979;&#30340;&#33258;&#36866;&#24212;&#19978;&#32622;&#20449;&#21306;&#38388;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;AUCRSS&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#23454;&#29616;&#39640;&#25928;&#30340;&#21464;&#28857;&#26816;&#27979;&#21644;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00220v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#22810;&#21464;&#37327;&#33258;&#30456;&#20851;&#25968;&#25454;&#30340;&#24207;&#36143;&#21464;&#28857;&#26816;&#27979;&#26159;&#23454;&#36341;&#20013;&#19968;&#20010;&#38750;&#24120;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#24863;&#30693;&#36164;&#28304;&#26377;&#38480;&#26102;&#65292;&#27599;&#27425;&#24863;&#30693;&#26102;&#38388;&#28857;&#21482;&#33021;&#35266;&#27979;&#22810;&#21464;&#37327;&#31995;&#32479;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#36825;&#23601;&#25552;&#20986;&#20102;&#21487;&#37096;&#20998;&#35266;&#27979;&#22810;&#20256;&#24863;&#22120;&#24207;&#36143;&#21464;&#28857;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#36866;&#24212;&#19978;&#32622;&#20449;&#21306;&#38388;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(AUCRSS)&#30340;&#26816;&#27979;&#26041;&#26696;&#12290;&#23427;&#36890;&#36807;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSM)&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#36827;&#34892;&#39640;&#25928;&#30340;&#21464;&#28857;&#26816;&#27979;&#21644;&#23450;&#20301;&#12290;&#23545;&#22312;&#32447;&#25512;&#26029;SSM&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#21345;&#23572;&#26364;&#28388;&#27874;&#31639;&#27861;&#36827;&#34892;&#24320;&#21457;&#65292;&#24182;&#30456;&#24212;&#22320;&#65292;&#22522;&#20110;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#21464;&#28857;&#26816;&#27979;&#26041;&#26696;&#34987;&#24320;&#21457;&#12290;&#20998;&#26512;&#20102;&#20854;&#26816;&#27979;&#33021;&#21147;&#19982;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#30340;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#26816;&#27979;&#33021;&#21147;&#35270;&#20026;&#19968;&#31181;&#20877;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00220v1 Announce Type: cross  Abstract: Sequential change point detection for multivariate autocorrelated data is a very common problem in practice. However, when the sensing resources are limited, only a subset of variables from the multivariate system can be observed at each sensing time point. This raises the problem of partially observable multi-sensor sequential change point detection. For it, we propose a detection scheme called adaptive upper confidence region with state space model (AUCRSS). It models multivariate time series via a state space model (SSM), and uses an adaptive sampling policy for efficient change point detection and localization. A partially-observable Kalman filter algorithm is developed for online inference of SSM, and accordingly, a change point detection scheme based on a generalized likelihood ratio test is developed. How its detection power relates to the adaptive sampling strategy is analyzed. Meanwhile, by treating the detection power as a re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21151;&#33021;&#36793;&#32536;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#36793;&#35270;&#20026;&#21151;&#33021;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#32500;&#24230;&#26469;&#34920;&#31034;&#20989;&#25968;&#65292;&#20351;&#29992;Tucker&#21151;&#33021;&#20998;&#35299;&#22788;&#29702;&#21151;&#33021;&#37051;&#25509;&#24352;&#37327;&#65292;&#36827;&#34892;&#27169;&#22411;&#25512;&#26029;&#20197;&#35299;&#20915;&#19981;&#35268;&#21017;&#35266;&#27979;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#20351;&#22522;&#30784;&#30697;&#38453;&#23545;&#31216;&#21270;&#65292;&#26368;&#32456;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00218</link><description>&lt;p&gt;
&#21151;&#33021;&#36793;&#32536;&#32593;&#32476;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Functional-Edged Network Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21151;&#33021;&#36793;&#32536;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#36793;&#35270;&#20026;&#21151;&#33021;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#32500;&#24230;&#26469;&#34920;&#31034;&#20989;&#25968;&#65292;&#20351;&#29992;Tucker&#21151;&#33021;&#20998;&#35299;&#22788;&#29702;&#21151;&#33021;&#37051;&#25509;&#24352;&#37327;&#65292;&#36827;&#34892;&#27169;&#22411;&#25512;&#26029;&#20197;&#35299;&#20915;&#19981;&#35268;&#21017;&#35266;&#27979;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#20351;&#22522;&#30784;&#30697;&#38453;&#23545;&#31216;&#21270;&#65292;&#26368;&#32456;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#29616;&#26377;&#20316;&#21697;&#24418;&#25104;&#23545;&#27604;&#65292;&#29616;&#26377;&#20316;&#21697;&#37117;&#23558;&#33410;&#28857;&#35270;&#20026;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#36793;&#26469;&#34920;&#31034;&#19981;&#21516;&#20989;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#32593;&#32476;&#24314;&#27169;&#65292;&#20854;&#20013;&#36793;&#26159;&#21151;&#33021;&#25968;&#25454;&#65292;&#24182;&#23558;&#37051;&#25509;&#30697;&#38453;&#36716;&#25442;&#20026;&#21151;&#33021;&#37051;&#25509;&#24352;&#37327;&#65292;&#24341;&#20837;&#19968;&#20010;&#39069;&#22806;&#30340;&#32500;&#24230;&#19987;&#38376;&#29992;&#20110;&#20989;&#25968;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;Tucker&#21151;&#33021;&#20998;&#35299;&#26469;&#22788;&#29702;&#21151;&#33021;&#37051;&#25509;&#24352;&#37327;&#65292;&#20026;&#36827;&#19968;&#27493;&#32771;&#34385;&#33410;&#28857;&#20043;&#38388;&#30340;&#31038;&#21306;&#65292;&#23545;&#22522;&#30784;&#30697;&#38453;&#36827;&#34892;&#27491;&#21017;&#21270;&#20351;&#20854;&#23545;&#31216;&#21270;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22788;&#29702;&#21151;&#33021;&#36793;&#30340;&#19981;&#35268;&#21017;&#35266;&#27979;&#65292;&#25105;&#20204;&#36827;&#34892;&#27169;&#22411;&#25512;&#26029;&#20197;&#35299;&#20915;&#24352;&#37327;&#23436;&#25104;&#38382;&#39064;&#65292;&#36890;&#36807;Riemann&#20849;&#36717;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20960;&#20010;&#23450;&#29702;&#26469;&#23637;&#31034;&#21151;&#33021;&#36793;&#32536;&#32593;&#32476;&#27169;&#22411;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#22320;&#38081;&#31995;&#32479;&#25968;&#25454;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00218v1 Announce Type: cross  Abstract: Contrasts with existing works which all consider nodes as functions and use edges to represent the relationships between different functions. We target at network modeling whose edges are functional data and transform the adjacency matrix into a functional adjacency tensor, introducing an additional dimension dedicated to function representation. Tucker functional decomposition is used for the functional adjacency tensor, and to further consider the community between nodes, we regularize the basis matrices to be symmetrical. Furthermore, to deal with irregular observations of the functional edges, we conduct model inference to solve a tensor completion problem. It is optimized by a Riemann conjugate gradient descent method. Besides these, we also derive several theorems to show the desirable properties of the functional edged network model. Finally, we evaluate the efficacy of our proposed model using simulation data and real metro sys
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;&#25512;&#29702;&#26041;&#27861;(MCRank)&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26465;&#20214;&#25490;&#24207;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00211</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#30340;&#22810;&#26465;&#20214;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Multi-Conditional Ranking with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;&#25512;&#29702;&#26041;&#27861;(MCRank)&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26465;&#20214;&#25490;&#24207;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#19968;&#32452;&#39033;&#30446;&#36827;&#34892;&#25490;&#24207;&#24050;&#25104;&#20026;&#25512;&#33616;&#21644;&#26816;&#32034;&#31995;&#32479;&#20013;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#24182;&#25506;&#35752;&#20102;&#22810;&#26465;&#20214;&#25490;&#24207;&#30340;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;MCRank&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#36328;&#19981;&#21516;&#39033;&#30446;&#31867;&#22411;&#21644;&#26465;&#20214;&#36827;&#34892;&#22810;&#26465;&#20214;&#25490;&#24207;&#12290;&#25105;&#20204;&#20351;&#29992;MCRank&#23545;LLMs&#36827;&#34892;&#20998;&#26512;&#34920;&#26126;&#65292;&#38543;&#30528;&#39033;&#30446;&#21644;&#26465;&#20214;&#25968;&#37327;&#20197;&#21450;&#22797;&#26434;&#24615;&#30340;&#22686;&#38271;&#65292;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;&#25512;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#25552;&#21462;&#21644;&#25490;&#24207;&#26465;&#20214;&#65292;&#28982;&#21518;&#36845;&#20195;&#22320;&#23545;&#26465;&#20214;&#36827;&#34892;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00211v1 Announce Type: new  Abstract: Utilizing large language models (LLMs) to rank a set of items has become a common approach in recommendation and retrieval systems. Typically, these systems focus on ordering a substantial number of documents in a monotonic order based on a given query. However, real-world scenarios often present a different challenge: ranking a comparatively smaller set of items, but according to a variety of diverse and occasionally conflicting conditions. In this paper, we define and explore the task of multi-conditional ranking by introducing MCRank, a benchmark tailored for assessing multi-conditional ranking across various item types and conditions. Our analysis of LLMs using MCRank indicates a significant decrease in performance as the number and complexity of items and conditions grow. To overcome this limitation, we propose a novel decomposed reasoning method, consisting of EXtracting and Sorting the conditions, and then Iterativly Ranking the i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#21160;&#24577;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#20272;&#35745;Incremental Stylistic Effect&#26469;&#35299;&#37322;&#22914;&#20309;&#25913;&#21464;&#21512;&#20316;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00207</link><description>&lt;p&gt;
&#20154;&#31867;-&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal Inference for Human-Language Model Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#20316;&#21160;&#24577;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#20272;&#35745;Incremental Stylistic Effect&#26469;&#35299;&#37322;&#22914;&#20309;&#25913;&#21464;&#21512;&#20316;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20043;&#38388;&#30340;&#21327;&#20316;&#21160;&#24577;&#65292;&#36825;&#20123;&#20114;&#21160;&#36890;&#24120;&#28041;&#21450;LMs&#25552;&#20986;&#25991;&#26412;&#27573;&#33853;&#65292;&#32780;&#20154;&#31867;&#32534;&#36753;&#25110;&#22238;&#24212;&#36825;&#20123;&#24314;&#35758;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19982;LMs&#36827;&#34892;&#26377;&#25928;&#30340;&#20114;&#21160;&#35201;&#27714;&#20154;&#31867;&#36776;&#21035;&#20986;&#26377;&#25928;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20114;&#21160;&#31574;&#30053;&#65292;&#20363;&#22914;&#32534;&#36753;&#21644;&#22238;&#24212;&#26679;&#24335;&#65292;&#20174;&#21382;&#21490;&#20154;&#31867;-LM&#20114;&#21160;&#20013;&#12290;&#36825;&#20010;&#30446;&#26631;&#26412;&#36136;&#19978;&#26159;&#22240;&#26524;&#20851;&#31995;&#65292;&#21463;&#21040;&#21453;&#20107;&#23454;&#8220;&#22914;&#26524;&#8221;&#38382;&#39064;&#30340;&#39537;&#21160;:&#22914;&#26524;&#20154;&#31867;&#37319;&#29992;&#19981;&#21516;&#30340;&#25991;&#26412;&#32534;&#36753;/&#31934;&#28860;&#31574;&#30053;&#65292;&#21327;&#20316;&#30340;&#32467;&#26524;&#20250;&#22914;&#20309;&#25913;&#21464;&#65311;&#22238;&#31572;&#36825;&#20010;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#21046;&#23450;&#19968;&#20010;&#36866;&#24403;&#30340;&#22240;&#26524;&#20272;&#35745;:&#20256;&#32479;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#20272;&#35745;&#30001;&#20110;&#25991;&#26412;&#30340;&#39640;&#32500;&#24230;&#32780;&#19981;&#36866;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22240;&#26524;&#20272;&#35745; - &#22686;&#37327;&#39118;&#26684;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00207v1 Announce Type: cross  Abstract: In this paper, we examine the collaborative dynamics between humans and language models (LMs), where the interactions typically involve LMs proposing text segments and humans editing or responding to these proposals. Productive engagement with LMs in such scenarios necessitates that humans discern effective text-based interaction strategies, such as editing and response styles, from historical human-LM interactions. This objective is inherently causal, driven by the counterfactual `what-if' question: how would the outcome of collaboration change if humans employed a different text editing/refinement strategy? A key challenge in answering this causal inference question is formulating an appropriate causal estimand: the conventional average treatment effect (ATE) estimand is inapplicable to text-based treatments due to their high dimensionality. To address this concern, we introduce a new causal estimand -- Incremental Stylistic Effect (
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00204</link><description>&lt;p&gt;
&#22522;&#20110;PPO&#30340;DRL&#33258;&#35843;PID&#38750;&#32447;&#24615;&#26080;&#20154;&#26426;&#25511;&#21046;&#22120;&#29992;&#20110;&#31283;&#20581;&#33258;&#20027;&#39134;&#34892;
&lt;/p&gt;
&lt;p&gt;
A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#20316;&#20026;&#20256;&#32479;&#32447;&#24615;&#27604;&#20363;&#31215;&#20998;&#24494;&#20998;&#65288;PID&#65289;&#25511;&#21046;&#22120;&#30340;&#26367;&#20195;&#21697;&#65292;&#20174;&#32780;&#24443;&#24213;&#25913;&#21464;&#26080;&#20154;&#26426;&#39134;&#34892;&#25511;&#21046;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#25163;&#21160;&#21644;&#33258;&#20027;&#27169;&#24335;&#20043;&#38388;&#23454;&#29616;&#26080;&#32541;&#36807;&#28193;&#65292;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;Gazebo&#27169;&#25311;&#22120;&#20013;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;DRL&#20195;&#29702;&#12290;&#28155;&#21152;20000&#32654;&#20803;&#30340;&#23460;&#20869;Vicon&#36319;&#36394;&#31995;&#32479;&#25552;&#20379;&lt;1mm&#30340;&#23450;&#20301;&#31934;&#24230;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;&#20026;&#20102;&#22312;&#26368;&#30701;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#20013;&#23548;&#33322;&#26080;&#20154;&#26426;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#19977;&#32500;A*&#36335;&#24452;&#35268;&#21010;&#22120;&#24182;&#25104;&#21151;&#22320;&#23558;&#20854;&#23454;&#26045;&#21040;&#23454;&#38469;&#39134;&#34892;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00204v1 Announce Type: cross  Abstract: This project aims to revolutionize drone flight control by implementing a nonlinear Deep Reinforcement Learning (DRL) agent as a replacement for traditional linear Proportional Integral Derivative (PID) controllers. The primary objective is to seamlessly transition drones between manual and autonomous modes, enhancing responsiveness and stability. We utilize the Proximal Policy Optimization (PPO) reinforcement learning strategy within the Gazebo simulator to train the DRL agent. Adding a $20,000 indoor Vicon tracking system offers &lt;1mm positioning accuracy, which significantly improves autonomous flight precision. To navigate the drone in the shortest collision-free trajectory, we also build a 3 dimensional A* path planner and implement it into the real flight successfully.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; $\mathrm{CAESAR}$ &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#19968;&#20010;&#36817;&#20284;&#30340;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#21516;&#26102;&#20272;&#35745;&#22810;&#20010;&#31574;&#30053;&#30340;&#20215;&#20540;&#65292;&#20197;&#35299;&#20915;&#22810;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00195</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#20272;&#35745;&#36827;&#34892;&#22810;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multiple-policy Evaluation via Density Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; $\mathrm{CAESAR}$ &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#19968;&#20010;&#36817;&#20284;&#30340;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#21516;&#26102;&#20272;&#35745;&#22810;&#20010;&#31574;&#30053;&#30340;&#20215;&#20540;&#65292;&#20197;&#35299;&#20915;&#22810;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22810;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#32473;&#23450;&#19968;&#32452; $K$ &#20010;&#30446;&#26631;&#31574;&#30053;&#65292;&#30446;&#26631;&#26159;&#20197;&#33267;&#23569; $1-\delta$ &#30340;&#27010;&#29575;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#65288;&#26399;&#26395;&#24635;&#22870;&#21169;&#65289;&#36798;&#21040;&#31934;&#24230; $\epsilon$&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; $\mathrm{CAESAR}$ &#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#35745;&#31639;&#19968;&#20010;&#36817;&#20284;&#30340;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#20174;&#20013;&#37319;&#26679;&#30340;&#25968;&#25454;&#26469;&#21516;&#26102;&#20272;&#35745;&#31574;&#30053;&#20215;&#20540;&#12290;$\mathrm{CAESAR}$ &#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#20197;&#38543;&#30528; $\tilde{O}(\frac{1}{\epsilon})$ &#32553;&#25918;&#30340;&#20302;&#35746;&#21333;&#37319;&#26679;&#22797;&#26434;&#24615;&#29575;&#20135;&#29983;&#30446;&#26631;&#31574;&#30053;&#30340;&#35775;&#38382;&#20998;&#24067;&#30340;&#31895;&#30053;&#20272;&#35745;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36817;&#20284;&#26368;&#20248;&#31163;&#32447;&#37319;&#26679;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#36880;&#27493;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#26469;&#35745;&#31639;&#25152;&#26377;&#30446;&#26631;&#31574;&#30053;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00195v1 Announce Type: cross  Abstract: In this work, we focus on the multiple-policy evaluation problem where we are given a set of $K$ target policies and the goal is to evaluate their performance (the expected total rewards) to an accuracy $\epsilon$ with probability at least $1-\delta$. We propose an algorithm named $\mathrm{CAESAR}$ to address this problem. Our approach is based on computing an approximate optimal offline sampling distribution and using the data sampled from it to perform the simultaneous estimation of the policy values. $\mathrm{CAESAR}$ consists of two phases. In the first one we produce coarse estimates of the vistation distributions of the target policies at a low order sample complexity rate that scales with $\tilde{O}(\frac{1}{\epsilon})$. In the second phase, we approximate the optimal offline sampling distribution and compute the importance weighting ratios for all target policies by minimizing a step-wise quadratic loss function inspired by the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#21306;&#22495;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21355;&#26143;&#22270;&#20687;&#20013;&#20998;&#21106;&#20892;&#30000;&#36793;&#30028;&#65292;&#35299;&#20915;&#20102;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00179</link><description>&lt;p&gt;
&#22810;&#21306;&#22495;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#22312;&#21355;&#26143;&#22270;&#20687;&#20013;&#20998;&#21106;&#20892;&#30000;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Multi-Region Transfer Learning for Segmentation of Crop Field Boundaries in Satellite Images with Limited Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00179
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#21306;&#22495;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21355;&#26143;&#22270;&#20687;&#20013;&#20998;&#21106;&#20892;&#30000;&#36793;&#30028;&#65292;&#35299;&#20915;&#20102;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30000;&#30028;&#21246;&#30011;&#30340;&#30446;&#26631;&#26159;&#22312;&#39640;&#31354;&#24863;&#24212;&#22270;&#20687;&#65288;&#20363;&#22914;&#26469;&#33258;&#21355;&#26143;&#25110;&#26080;&#20154;&#26426;&#30340;&#22270;&#20687;&#65289;&#20013;&#39044;&#27979;&#21333;&#20010;&#20892;&#30000;&#30340;&#22810;&#36793;&#24418;&#36793;&#30028;&#21644;&#20869;&#37096;&#12290;&#33258;&#21160;&#21246;&#30011;&#30000;&#30028;&#26159;&#35768;&#22810;&#20892;&#19994;&#23454;&#38469;&#29992;&#20363;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#20272;&#35745;&#19968;&#20010;&#22320;&#21306;&#30340;&#32789;&#22320;&#38754;&#31215;&#25110;&#39044;&#27979;&#19968;&#20010;&#30000;&#22320;&#30340;&#23395;&#26411;&#20135;&#37327;&#12290;&#30000;&#30028;&#21246;&#30011;&#21487;&#20197;&#34987;&#24402;&#31867;&#20026;&#19968;&#20010;&#23454;&#20363;&#20998;&#21106;&#38382;&#39064;&#65292;&#20294;&#19982;&#20256;&#32479;&#29992;&#20110;&#23454;&#20363;&#20998;&#21106;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#23427;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#20197;&#21069;&#20316;&#21697;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#20063;&#21463;&#38480;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#22312;&#30000;&#30028;&#21246;&#30011;&#27169;&#22411;&#23558;&#34987;&#24212;&#29992;&#30340;&#22320;&#26041;&#26377;&#36275;&#22815;&#22823;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#21487;&#29992;&#65292;&#36825;&#23545;&#22823;&#22810;&#25968;&#22320;&#21306;&#26469;&#35828;&#24182;&#38750;&#29616;&#23454;&#65288;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#22320;&#21306;&#65292;&#22914;&#25746;&#21704;&#25289;&#20197;&#21335;&#30340;&#38750;&#27954;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20892;&#30000;&#36793;&#30028;&#20998;&#21106;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00179v1 Announce Type: cross  Abstract: The goal of field boundary delineation is to predict the polygonal boundaries and interiors of individual crop fields in overhead remotely sensed images (e.g., from satellites or drones). Automatic delineation of field boundaries is a necessary task for many real-world use cases in agriculture, such as estimating cultivated area in a region or predicting end-of-season yield in a field. Field boundary delineation can be framed as an instance segmentation problem, but presents unique research challenges compared to traditional computer vision datasets used for instance segmentation. The practical applicability of previous work is also limited by the assumption that a sufficiently-large labeled dataset is available where field boundary delineation models will be applied, which is not the reality for most regions (especially under-resourced regions such as Sub-Saharan Africa). We present an approach for segmentation of crop field boundarie
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#27979;&#24615;&#21644;&#22788;&#26041;&#24615;&#20998;&#26512;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#20026;&#32467;&#26463;&#29616;&#26377;&#20307;&#32946;&#36187;&#23395;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#31867;&#20284;&#23436;&#25972;&#36187;&#23395;&#32467;&#26524;&#30340;&#38431;&#20237;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2404.00178</link><description>&lt;p&gt;
&#36229;&#36234;&#20572;&#36187;&#65306;&#32467;&#26463;&#20307;&#32946;&#32852;&#30431;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
Beyond Suspension: A Two-phase Methodology for Concluding Sports Leagues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00178
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#27979;&#24615;&#21644;&#22788;&#26041;&#24615;&#20998;&#26512;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#20026;&#32467;&#26463;&#29616;&#26377;&#20307;&#32946;&#36187;&#23395;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#31867;&#20284;&#23436;&#25972;&#36187;&#23395;&#32467;&#26524;&#30340;&#38431;&#20237;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#23450;&#20041;&#65306;&#19987;&#19994;&#20307;&#32946;&#32852;&#30431;&#21487;&#33021;&#22240;&#21508;&#31181;&#21407;&#22240;&#26242;&#20572;&#65292;&#27604;&#22914;&#26368;&#36817;&#30340;COVID-19&#22823;&#27969;&#34892;&#12290;&#37325;&#35201;&#38382;&#39064;&#26159;&#32852;&#30431;&#22312;&#37325;&#26032;&#24320;&#36187;&#26102;&#22914;&#20309;&#36866;&#24403;&#22320;&#36873;&#25321;&#20854;&#20313;&#27604;&#36187;&#30340;&#23376;&#38598;&#65292;&#20197;&#22312;&#32553;&#30701;&#30340;&#26102;&#38388;&#20869;&#32467;&#26463;&#36187;&#23395;&#12290;&#23398;&#26415;/&#23454;&#36341;&#24847;&#20041;&#65306;&#23613;&#31649;&#26377;&#20016;&#23500;&#30340;&#25991;&#29486;&#20851;&#20110;&#20174;&#38646;&#24320;&#22987;&#23433;&#25490;&#25972;&#20010;&#36187;&#23395;&#65292;&#32467;&#26463;&#29616;&#26377;&#36187;&#23395;&#21364;&#25130;&#28982;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#23454;&#29616;&#22242;&#38431;&#25490;&#21517;&#65292;&#20351;&#20854;&#19982;&#23436;&#25972;&#36187;&#23395;&#27604;&#36187;&#30340;&#32467;&#26524;&#31867;&#20284;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#21033;&#29992;&#39044;&#27979;&#24615;&#21644;&#22788;&#26041;&#24615;&#20998;&#26512;&#65292;&#20026;&#21097;&#20313;&#36187;&#23395;&#21046;&#23450;&#19968;&#20010;&#30001;&#21407;&#23450;&#23433;&#25490;&#27604;&#36187;&#32452;&#25104;&#30340;&#36187;&#31243;&#23376;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38543;&#26426;&#20248;&#21270;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#22522;&#20110;&#26032;&#39062;&#25490;&#21517;&#30446;&#26631;&#65292;&#20854;&#21442;&#25968;&#39318;&#20808;&#20351;&#29992;&#39044;&#27979;&#24615;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00178v1 Announce Type: cross  Abstract: Problem definition: Professional sports leagues may be suspended due to various reasons such as the recent COVID-19 pandemic. A critical question the league must address when re-opening is how to appropriately select a subset of the remaining games to conclude the season in a shortened time frame. Academic/practical relevance: Despite the rich literature on scheduling an entire season starting from a blank slate, concluding an existing season is quite different. Our approach attempts to achieve team rankings similar to that which would have resulted had the season been played out in full. Methodology: We propose a data-driven model which exploits predictive and prescriptive analytics to produce a schedule for the remainder of the season comprised of a subset of originally-scheduled games. Our model introduces novel rankings-based objectives within a stochastic optimization model, whose parameters are first estimated using a predictive 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#36229;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#26377;&#26426;&#22826;&#38451;&#33021;&#30005;&#27744;&#25928;&#29575;&#36864;&#21270;&#65292;&#20934;&#30830;&#24230;&#39640;&#19988;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2404.00173</link><description>&lt;p&gt;
&#27604;&#36739;&#36229;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#39044;&#27979;&#26377;&#26426;&#22826;&#38451;&#33021;&#30005;&#27744;&#25928;&#29575;&#36864;&#21270;
&lt;/p&gt;
&lt;p&gt;
Comparing Hyper-optimized Machine Learning Models for Predicting Efficiency Degradation in Organic Solar Cells
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00173
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#36229;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#26377;&#26426;&#22826;&#38451;&#33021;&#30005;&#27744;&#25928;&#29575;&#36864;&#21270;&#65292;&#20934;&#30830;&#24230;&#39640;&#19988;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#26368;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#26469;&#34920;&#31034;&#22810;&#23618;&#32467;&#26500;ITO/PEDOT:PSS/P3HT:PCBM/Al&#32858;&#21512;&#29289;&#26377;&#26426;&#22826;&#38451;&#33021;&#30005;&#27744;&#65288;OSCs&#65289;&#30340;&#21151;&#29575;&#36716;&#25442;&#25928;&#29575;&#65288;PCE&#65289;&#25152;&#36973;&#21463;&#30340;&#26102;&#38388;&#36864;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;996&#26465;&#25968;&#25454;&#30340;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#20851;&#20110;&#21046;&#36896;&#36807;&#31243;&#21644;&#29615;&#22659;&#26465;&#20214;&#30340;7&#20010;&#21464;&#37327;&#65292;&#36229;&#36807;180&#22825;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20381;&#38752;&#19968;&#20010;&#36719;&#20214;&#26694;&#26550;&#65292;&#27719;&#38598;&#20102;&#19968;&#31995;&#21015;&#33258;&#21160;&#21270;ML&#21327;&#35758;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#21629;&#20196;&#34892;&#30028;&#38754;&#39034;&#24207;&#22320;&#38024;&#23545;&#25105;&#20204;&#30340;&#25968;&#25454;&#24211;&#25191;&#34892;&#65292;&#20174;&#32780;&#36731;&#26494;&#22320;&#36890;&#36807;&#35814;&#23613;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#36229;&#20248;&#21270;&#21644;&#38543;&#26426;&#21270;ML&#27169;&#22411;&#30340;&#31181;&#23376;&#65292;&#20197;&#33719;&#24471;&#26368;&#20339;&#27169;&#22411;&#12290;&#25152;&#36798;&#21040;&#30340;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;&#24191;&#27867;&#36229;&#36807;0.90&#30340;&#31995;&#25968;&#30830;&#23450;&#20540;&#65288;R2&#65289;&#65292;&#32780;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#12289;&#24179;&#26041;&#35823;&#24046;&#65288;SSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00173v1 Announce Type: new  Abstract: This work presents a set of optimal machine learning (ML) models to represent the temporal degradation suffered by the power conversion efficiency (PCE) of polymeric organic solar cells (OSCs) with a multilayer structure ITO/PEDOT:PSS/P3HT:PCBM/Al. To that aim, we generated a database with 996 entries, which includes up to 7 variables regarding both the manufacturing process and environmental conditions for more than 180 days. Then, we relied on a software framework that brings together a conglomeration of automated ML protocols that execute sequentially against our database by simply command-line interface. This easily permits hyper-optimizing and randomizing seeds of the ML models through exhaustive benchmarking so that optimal models are obtained. The accuracy achieved reaches values of the coefficient determination (R2) widely exceeding 0.90, whereas the root mean squared error (RMSE), sum of squared error (SSE), and mean absolute er
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#25968;&#25454;&#21644;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#36827;&#34892;&#36890;&#29992;&#29275;&#26631;&#35782;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#29289;&#31181;&#29305;&#23450;&#30340;&#22806;&#22871;&#22270;&#26696;&#25110;&#29305;&#20889;&#21475;&#21563;&#21360;&#35760;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;CNN&#21644;MLP&#22522;&#30784;&#23454;&#29616;&#23398;&#20064;&#21040;&#33391;&#22909;&#27867;&#21270;&#23884;&#20837;&#31354;&#38388;&#65292;&#20174;&#20307;&#24418;&#21306;&#20998;&#20010;&#20307;&#12290;</title><link>https://arxiv.org/abs/2404.00172</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#25968;&#25454;&#21644;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#23454;&#29616;&#30340;&#36890;&#29992;&#29275;&#26631;&#35782;
&lt;/p&gt;
&lt;p&gt;
Universal Bovine Identification via Depth Data and Deep Metric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00172
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#25968;&#25454;&#21644;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#36827;&#34892;&#36890;&#29992;&#29275;&#26631;&#35782;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#29289;&#31181;&#29305;&#23450;&#30340;&#22806;&#22871;&#22270;&#26696;&#25110;&#29305;&#20889;&#21475;&#21563;&#21360;&#35760;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;CNN&#21644;MLP&#22522;&#30784;&#23454;&#29616;&#23398;&#20064;&#21040;&#33391;&#22909;&#27867;&#21270;&#23884;&#20837;&#31354;&#38388;&#65292;&#20174;&#20307;&#24418;&#21306;&#20998;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#39030;&#37096;&#65288;&#32972;&#37096;&#35270;&#22270;&#65289;&#28145;&#24230;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#20010;&#20307;&#29275;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#26435;&#37325;&#65292;&#20197;&#20415;&#31435;&#21363;&#22797;&#29616;&#12290;&#30044;&#32676;&#35268;&#27169;&#22686;&#21152;&#23548;&#33268;&#29275;&#19982;&#20154;&#30340;&#27604;&#20363;&#22833;&#34913;&#65292;&#20351;&#24471;&#23545;&#20010;&#20307;&#36827;&#34892;&#25163;&#21160;&#30417;&#27979;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#23454;&#26102;&#29275;&#26631;&#35782;&#23545;&#20892;&#22330;&#33267;&#20851;&#37325;&#35201;&#65292;&#26159;&#31934;&#20934;&#30044;&#29287;&#19994;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00172v1 Announce Type: cross  Abstract: This paper proposes and evaluates, for the first time, a top-down (dorsal view), depth-only deep learning system for accurately identifying individual cattle and provides associated code, datasets, and training weights for immediate reproducibility. An increase in herd size skews the cow-to-human ratio at the farm and makes the manual monitoring of individuals more challenging. Therefore, real-time cattle identification is essential for the farms and a crucial step towards precision livestock farming. Underpinned by our previous work, this paper introduces a deep-metric learning method for cattle identification using depth data from an off-the-shelf 3D camera. The method relies on CNN and MLP backbones that learn well-generalised embedding spaces from the body shape to differentiate individuals -- requiring neither species-specific coat patterns nor close-up muzzle prints for operation. The network embeddings are clustered using a simp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20154;&#23545;&#20307;&#39564;&#30340;&#24320;&#25918;&#24615;&#36825;&#19968;&#20154;&#26684;&#32500;&#24230;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20010;&#20307;&#30340;&#35895;&#27468;&#25628;&#32034;&#21382;&#21490;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20284;&#24615;&#29305;&#24449;&#22522;&#20110;&#20010;&#20154;&#25991;&#26412;&#35821;&#26009;&#24211;&#26469;&#35299;&#37322;35%&#30340;&#24320;&#25918;&#24615;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2404.00165</link><description>&lt;p&gt;
&#20010;&#20154;&#25991;&#26412;&#35821;&#26009;&#24211;&#39044;&#27979;&#24320;&#25918;&#24615;&#12289;&#20852;&#36259;&#12289;&#30693;&#35782;&#21644;&#25945;&#32946;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Individual Text Corpora Predict Openness, Interests, Knowledge and Level of Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20154;&#23545;&#20307;&#39564;&#30340;&#24320;&#25918;&#24615;&#36825;&#19968;&#20154;&#26684;&#32500;&#24230;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20010;&#20307;&#30340;&#35895;&#27468;&#25628;&#32034;&#21382;&#21490;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20284;&#24615;&#29305;&#24449;&#22522;&#20110;&#20010;&#20154;&#25991;&#26412;&#35821;&#26009;&#24211;&#26469;&#35299;&#37322;35%&#30340;&#24320;&#25918;&#24615;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20010;&#20307;&#23545;&#20307;&#39564;&#30340;&#24320;&#25918;&#24615;&#36825;&#19968;&#20154;&#26684;&#32500;&#24230;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20010;&#20307;&#30340;&#35895;&#27468;&#25628;&#32034;&#21382;&#21490;&#36827;&#34892;&#39044;&#27979;&#12290;&#36890;&#36807;&#32593;&#32476;&#25235;&#21462;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#26469;&#33258;214&#21517;&#21442;&#19982;&#32773;&#30340;&#20010;&#20154;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#24179;&#22343;&#35789;&#27719;&#37327;&#20026;500&#19975;&#20010;&#35789;&#20803;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;word2vec&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#27599;&#20010;&#20010;&#20154;&#25991;&#26412;&#35821;&#26009;&#24211;&#19982;&#26631;&#35760;&#21333;&#35789;&#30340;&#30456;&#20284;&#24615;&#65292;&#36825;&#20123;&#26631;&#35760;&#21333;&#35789;&#26469;&#33258;&#20110;&#20154;&#26684;&#30340;&#35789;&#27719;&#26041;&#27861;&#12290;&#36825;&#20123;&#20010;&#20154;&#25991;&#26412;&#35821;&#26009;&#24211;-&#26631;&#35760;&#21333;&#35789;&#30340;&#30456;&#20284;&#24615;&#34987;&#29992;&#20316;&#31070;&#32463;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#25105;&#20204;&#20381;&#36182;179&#21517;&#21442;&#19982;&#32773;&#65292;&#24182;&#20445;&#30041;&#20102;35&#21517;&#21442;&#19982;&#32773;&#30340;&#27979;&#35797;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#24102;&#26377;&#19981;&#21516;&#25968;&#37327;&#39044;&#27979;&#29305;&#24449;&#12289;&#38544;&#34255;&#21333;&#20803;&#21644;&#22686;&#37327;&#22240;&#23376;&#30340;&#32593;&#26684;&#25628;&#32034;&#12290;&#20316;&#20026;&#27169;&#22411;&#36873;&#25321;&#26631;&#20934;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22312;&#39564;&#35777;&#26679;&#26412;&#20013;&#30001;&#32477;&#23545;R2&#24046;&#24322;&#24809;&#32602;&#30340;R2&#12290;&#36873;&#25321;&#30340;&#31070;&#32463;&#27169;&#22411;&#22312;&#27979;&#35797;&#26679;&#26412;&#20013;&#35299;&#37322;&#20102;35%&#30340;&#24320;&#25918;&#24615;&#26041;&#24046;&#65292;&#32780;&#38598;&#25104;&#27169;&#22411;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00165v1 Announce Type: new  Abstract: Here we examine whether the personality dimension of openness to experience can be predicted from the individual google search history. By web scraping, individual text corpora (ICs) were generated from 214 participants with a mean number of 5 million word tokens. We trained word2vec models and used the similarities of each IC to label words, which were derived from a lexical approach of personality. These IC-label-word similarities were utilized as predictive features in neural models. For training and validation, we relied on 179 participants and held out a test sample of 35 participants. A grid search with varying number of predictive features, hidden units and boost factor was performed. As model selection criterion, we used R2 in the validation samples penalized by the absolute R2 difference between training and validation. The selected neural model explained 35% of the openness variance in the test sample, while an ensemble model w
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#25163;&#26426;&#21644;&#20247;&#21253;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#20272;&#31639;&#22823;&#35268;&#27169;&#21306;&#22495;&#32593;&#32476;&#20013;&#30340;&#27493;&#34892;&#21644;&#39569;&#34892;&#37327;&#65292;&#35752;&#35770;&#20102;&#22312;&#27169;&#22411;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#25512;&#26029;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.00162</link><description>&lt;p&gt;
&#24314;&#27169;&#22823;&#35268;&#27169;&#27493;&#34892;&#21644;&#39569;&#34892;&#32593;&#32476;&#65306;&#20351;&#29992;&#25163;&#26426;&#21644;&#20247;&#21253;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modeling Large-Scale Walking and Cycling Networks: A Machine Learning Approach Using Mobile Phone and Crowdsourced Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#25163;&#26426;&#21644;&#20247;&#21253;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#20272;&#31639;&#22823;&#35268;&#27169;&#21306;&#22495;&#32593;&#32476;&#20013;&#30340;&#27493;&#34892;&#21644;&#39569;&#34892;&#37327;&#65292;&#35752;&#35770;&#20102;&#22312;&#27169;&#22411;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#25512;&#26029;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27493;&#34892;&#21644;&#39569;&#34892;&#34987;&#35748;&#20026;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#20581;&#24247;&#12289;&#29615;&#22659;&#21644;&#32463;&#27982;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#35777;&#25454;&#30340;&#31215;&#26497;&#20132;&#36890;&#35268;&#21010;&#21644;&#25919;&#31574;&#30340;&#21457;&#23637;&#21463;&#21040;&#25968;&#25454;&#38480;&#21046;&#30340;&#38459;&#30861;&#65292;&#20363;&#22914;&#20247;&#21253;&#25968;&#25454;&#30340;&#20559;&#35265;&#21644;&#25163;&#26426;&#25968;&#25454;&#30340;&#20195;&#34920;&#24615;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#24212;&#29992;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#31639;&#28595;&#22823;&#21033;&#20122;&#26032;&#21335;&#23041;&#23572;&#22763;&#24030;&#19968;&#20010;&#21253;&#21547;188,999&#20010;&#27493;&#34892;&#38142;&#25509;&#21644;114,885&#20010;&#39569;&#34892;&#38142;&#25509;&#30340;&#22823;&#35268;&#27169;&#22320;&#21306;&#32593;&#32476;&#30340;&#26085;&#24120;&#27493;&#34892;&#21644;&#39569;&#34892;&#37327;&#12290;&#24314;&#27169;&#26041;&#27861;&#21033;&#29992;&#20102;&#20247;&#21253;&#21644;&#25163;&#26426;&#25968;&#25454;&#20197;&#21450;&#20154;&#21475;&#12289;&#22303;&#22320;&#21033;&#29992;&#12289;&#22320;&#24418;&#12289;&#27668;&#20505;&#31561;&#19968;&#31995;&#21015;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;&#35813;&#30740;&#31350;&#35752;&#35770;&#20102;&#19982;&#27169;&#22411;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#25512;&#26029;&#30340;&#19977;&#20010;&#26041;&#38754;&#30456;&#20851;&#30340;&#29420;&#29305;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#32771;&#34385;&#21040;&#24314;&#27169;&#32593;&#32476;&#30340;&#22823;&#22320;&#29702;&#33539;&#22260;&#21644;&#30456;&#23545;&#31232;&#32570;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00162v1 Announce Type: new  Abstract: Walking and cycling are known to bring substantial health, environmental, and economic advantages. However, the development of evidence-based active transportation planning and policies has been impeded by significant data limitations, such as biases in crowdsourced data and representativeness issues of mobile phone data. In this study, we develop and apply a machine learning based modeling approach for estimating daily walking and cycling volumes across a large-scale regional network in New South Wales, Australia that includes 188,999 walking links and 114,885 cycling links. The modeling methodology leverages crowdsourced and mobile phone data as well as a range of other datasets on population, land use, topography, climate, etc. The study discusses the unique challenges and limitations related to all three aspects of model training, testing, and inference given the large geographical extent of the modeled networks and relative scarcity
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39640;&#26031;&#24179;&#28369;&#20272;&#35745;&#20989;&#25968;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20559;&#23548;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#20840;&#38646;&#38454;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#38750;&#28176;&#36817;&#25910;&#25947;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2404.00158</link><description>&lt;p&gt;
&#23436;&#20840;&#38646;&#38454;&#21452;&#23618;&#35268;&#21010;&#36890;&#36807;&#39640;&#26031;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Fully Zeroth-Order Bilevel Programming via Gaussian Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00158
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#24179;&#28369;&#20272;&#35745;&#20989;&#25968;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20559;&#23548;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#20840;&#38646;&#38454;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#38750;&#28176;&#36817;&#25910;&#25947;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#24182;&#20998;&#26512;&#20102;&#38646;&#38454;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21452;&#23618;&#38382;&#39064;&#65292;&#21363;&#22312;&#19978;/&#19979;&#30446;&#26631;&#20540;&#25110;&#20854;&#26080;&#20559;&#26799;&#24230;&#20272;&#35745;&#22343;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#12290;&#29305;&#21035;&#22320;&#65292;&#21033;&#29992;Stein's identity&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#39640;&#26031;&#24179;&#28369;&#26469;&#20272;&#35745;&#20855;&#26377;&#20004;&#20010;&#29420;&#31435;&#21464;&#37327;&#22359;&#30340;&#20989;&#25968;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20559;&#23548;&#25968;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#20272;&#35745;&#20540;&#24212;&#29992;&#20110;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20854;&#38750;&#28176;&#36817;&#25910;&#25947;&#20998;&#26512;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#20026;&#23436;&#20840;&#38543;&#26426;&#38646;&#38454;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00158v1 Announce Type: cross  Abstract: In this paper, we study and analyze zeroth-order stochastic approximation algorithms for solving bilvel problems, when neither the upper/lower objective values, nor their unbiased gradient estimates are available. In particular, exploiting Stein's identity, we first use Gaussian smoothing to estimate first- and second-order partial derivatives of functions with two independent block of variables. We then used these estimates in the framework of a stochastic approximation algorithm for solving bilevel optimization problems and establish its non-asymptotic convergence analysis. To the best of our knowledge, this is the first time that sample complexity bounds are established for a fully stochastic zeroth-order bilevel optimization algorithm.
&lt;/p&gt;</description></item><item><title>&#22312;&#27491;-&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20102;&#39564;&#35777;&#23436;&#20840;&#38543;&#26426;&#36873;&#25321;&#20551;&#35774;&#65288;SCAR&#65289;&#21644;&#26356;&#20026;&#29616;&#23454;&#30340;&#38543;&#26426;&#36873;&#25321;&#20551;&#35774;&#65288;SAR&#65289;&#23545;&#31639;&#27861;&#22797;&#26434;&#24615;&#21644;&#36895;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.00145</link><description>&lt;p&gt;
&#22312;&#27491;-&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#39564;&#35777;&#23436;&#20840;&#38543;&#26426;&#36873;&#25321;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Verifying the Selected Completely at Random Assumption in Positive-Unlabeled Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00145
&lt;/p&gt;
&lt;p&gt;
&#22312;&#27491;-&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20102;&#39564;&#35777;&#23436;&#20840;&#38543;&#26426;&#36873;&#25321;&#20551;&#35774;&#65288;SCAR&#65289;&#21644;&#26356;&#20026;&#29616;&#23454;&#30340;&#38543;&#26426;&#36873;&#25321;&#20551;&#35774;&#65288;SAR&#65289;&#23545;&#31639;&#27861;&#22797;&#26434;&#24615;&#21644;&#36895;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;-&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#22312;&#21253;&#21547;&#27491;&#20363;&#21644;&#26410;&#26631;&#35760;&#23454;&#20363;&#30340;&#35757;&#32451;&#25968;&#25454;&#22522;&#30784;&#19978;&#35757;&#32451;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#26410;&#26631;&#35760;&#35266;&#27979;&#21487;&#20197;&#23646;&#20110;&#27491;&#31867;&#25110;&#36127;&#31867;&#12290;&#24314;&#27169;&#27491;-&#26080;&#30417;&#30563;&#25968;&#25454;&#38656;&#35201;&#20851;&#20110;&#26631;&#31614;&#26426;&#21046;&#30340;&#19968;&#20123;&#20551;&#35774;&#65292;&#25551;&#36848;&#21738;&#20123;&#27491;&#20363;&#34987;&#20998;&#37197;&#26631;&#31614;&#12290;&#26089;&#26399;&#30740;&#31350;&#20013;&#32771;&#34385;&#30340;&#26368;&#31616;&#21333;&#20551;&#35774;&#26159;SCAR&#65288;&#23436;&#20840;&#38543;&#26426;&#36873;&#25321;&#20551;&#35774;&#65289;&#65292;&#20854;&#27010;&#29575;&#20998;&#25968;&#20989;&#25968;&#23450;&#20041;&#20026;&#32473;&#27491;&#20363;&#20998;&#37197;&#26631;&#31614;&#30340;&#27010;&#29575;&#26159;&#24120;&#25968;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19968;&#20010;&#26356;&#20026;&#29616;&#23454;&#30340;&#20551;&#35774;&#26159;SAR&#65288;&#38543;&#26426;&#36873;&#25321;&#65289;&#65292;&#23427;&#34920;&#26126;&#27010;&#29575;&#20989;&#25968;&#20165;&#20381;&#36182;&#20110;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#22522;&#20110;SCAR&#30340;&#31639;&#27861;&#27604;&#22522;&#20110;SAR&#30340;&#31639;&#27861;&#31616;&#21333;&#24471;&#22810;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#19978;&#26356;&#24555;&#65292;&#21518;&#32773;&#36890;&#24120;&#38656;&#35201;&#25361;&#25112;&#24615;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00145v1 Announce Type: cross  Abstract: The goal of positive-unlabeled (PU) learning is to train a binary classifier on the basis of training data containing positive and unlabeled instances, where unlabeled observations can belong either to the positive class or to the negative class. Modeling PU data requires certain assumptions on the labeling mechanism that describes which positive observations are assigned a label. The simplest assumption, considered in early works, is SCAR (Selected Completely at Random Assumption), according to which the propensity score function, defined as the probability of assigning a label to a positive observation, is constant. On the other hand, a much more realistic assumption is SAR (Selected at Random), which states that the propensity function solely depends on the observed feature vector. SCAR-based algorithms are much simpler and computationally much faster compared to SAR-based algorithms, which usually require challenging estimation of 
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#25200;&#21160;&#26041;&#27861;Shapley&#20540;&#21644;LIME&#21487;&#23454;&#29616;&#26356;&#39640;&#30340;&#20449;&#23454;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24314;&#35758;&#20248;&#21270;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#21452;&#37325;&#30446;&#26631;</title><link>https://arxiv.org/abs/2404.00140</link><description>&lt;p&gt;
&#20449;&#23454;&#24615;&#19982;&#21487;&#20449;&#24230;&#26159;&#21542;&#23384;&#22312;&#20914;&#31361;&#65311;&#19968;&#39033;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Does Faithfulness Conflict with Plausibility? An Empirical Study in Explainable AI across NLP Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00140
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25200;&#21160;&#26041;&#27861;Shapley&#20540;&#21644;LIME&#21487;&#23454;&#29616;&#26356;&#39640;&#30340;&#20449;&#23454;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24314;&#35758;&#20248;&#21270;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#21452;&#37325;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#35299;&#37322;&#20915;&#31574;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#36890;&#24120;&#35201;&#24179;&#34913;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;1&#65289;\textit{&#20449;&#23454;&#24615;}&#65292;&#21363;&#35299;&#37322;&#24517;&#39035;&#20934;&#30830;&#21453;&#26144;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65307;2&#65289;\textit{&#21487;&#20449;&#24230;}&#65292;&#21363;&#35299;&#37322;&#24517;&#39035;&#19982;&#39046;&#22495;&#19987;&#23478;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#20449;&#23454;&#24615;&#21644;&#21487;&#20449;&#24230;&#26159;&#21542;&#20174;&#26681;&#26412;&#19978;&#23384;&#22312;&#20914;&#31361;&#65311;&#36890;&#36807;&#22312;&#24773;&#24863;&#20998;&#26512;&#12289;&#24847;&#22270;&#26816;&#27979;&#21644;&#20027;&#39064;&#26631;&#27880;&#19977;&#20010;NLP&#20219;&#21153;&#20013;&#36873;&#23450;&#30340;&#35299;&#37322;&#26041;&#27861;&#19982;&#19987;&#23478;&#32423;&#35299;&#37322;&#20043;&#38388;&#30340;&#20840;&#38754;&#37327;&#21270;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20256;&#32479;&#30340;&#25200;&#21160;&#26041;&#27861;Shapley&#20540;&#21644;LIME&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#20449;&#23454;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#23547;&#27714;&#36890;&#36807;&#21452;&#37325;&#30446;&#26631;&#20248;&#21270;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#26469;&#23454;&#29616;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00140v1 Announce Type: new  Abstract: Explainability algorithms aimed at interpreting decision-making AI systems usually consider balancing two critical dimensions: 1) \textit{faithfulness}, where explanations accurately reflect the model's inference process. 2) \textit{plausibility}, where explanations are consistent with domain experts. However, the question arises: do faithfulness and plausibility inherently conflict? In this study, through a comprehensive quantitative comparison between the explanations from the selected explainability methods and expert-level interpretations across three NLP tasks: sentiment analysis, intent detection, and topic labeling, we demonstrate that traditional perturbation-based methods Shapley value and LIME could attain greater faithfulness and plausibility. Our findings suggest that rather than optimizing for one dimension at the expense of the other, we could seek to optimize explainability algorithms with dual objectives to achieve high l
&lt;/p&gt;</description></item><item><title>&#23558;&#20256;&#32479;&#25104;&#26412;&#21333;&#20301;&#35270;&#20026;&#21464;&#37327;&#65292;&#36890;&#36807;&#35843;&#25972;&#20540;&#21487;&#20197;&#33719;&#24471;&#27604;&#40664;&#35748;&#26597;&#35810;&#35745;&#21010;&#26356;&#20248;&#30340;&#26597;&#35810;&#35745;&#21010;</title><link>https://arxiv.org/abs/2404.00137</link><description>&lt;p&gt;
&#39044;&#31639;&#24863;&#30693;&#26597;&#35810;&#20248;&#21270;&#65306;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Budget-aware Query Tuning: An AutoML Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00137
&lt;/p&gt;
&lt;p&gt;
&#23558;&#20256;&#32479;&#25104;&#26412;&#21333;&#20301;&#35270;&#20026;&#21464;&#37327;&#65292;&#36890;&#36807;&#35843;&#25972;&#20540;&#21487;&#20197;&#33719;&#24471;&#27604;&#40664;&#35748;&#26597;&#35810;&#35745;&#21010;&#26356;&#20248;&#30340;&#26597;&#35810;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25968;&#25454;&#24211;&#31995;&#32479;&#20381;&#36182;&#22522;&#20110;&#25104;&#26412;&#30340;&#26597;&#35810;&#20248;&#21270;&#22120;&#20026;&#36755;&#20837;&#26597;&#35810;&#25552;&#20379;&#33391;&#22909;&#30340;&#25191;&#34892;&#35745;&#21010;&#12290;&#36825;&#31181;&#26597;&#35810;&#20248;&#21270;&#22120;&#20381;&#36182;&#25104;&#26412;&#27169;&#22411;&#26469;&#20272;&#31639;&#20505;&#36873;&#26597;&#35810;&#25191;&#34892;&#35745;&#21010;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#65292;&#23558;&#36825;&#20123;&#25104;&#26412;&#21333;&#20301;&#35270;&#20026;&#21464;&#37327;&#65292;&#34920;&#26126;&#36890;&#36807;&#35843;&#25972;&#25104;&#26412;&#21333;&#20301;&#20540;&#65292;&#21487;&#20197;&#33719;&#24471;&#26126;&#26174;&#20248;&#20110;&#23558;&#25104;&#26412;&#21333;&#20301;&#35270;&#20026;&#24120;&#25968;&#26102;&#26597;&#35810;&#20248;&#21270;&#22120;&#36820;&#22238;&#30340;&#40664;&#35748;&#26597;&#35810;&#35745;&#21010;&#30340;&#26597;&#35810;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00137v1 Announce Type: cross  Abstract: Modern database systems rely on cost-based query optimizers to come up with good execution plans for input queries. Such query optimizers rely on cost models to estimate the costs of candidate query execution plans. A cost model represents a function from a set of cost units to query execution cost, where each cost unit specifies the unit cost of executing a certain type of query processing operation (such as table scan or join). These cost units are traditionally viewed as constants, whose values only depend on the platform configuration where the database system runs on top of but are invariant for queries processed by the database system. In this paper, we challenge this classic view by thinking of these cost units as variables instead. We show that, by varying the cost-unit values one can obtain query plans that significantly outperform the default query plans returned by the query optimizer when viewing the cost units as constants
&lt;/p&gt;</description></item><item><title>FISBe&#25552;&#20379;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38271;&#31243;&#32454;&#19997;&#29366;&#32467;&#26500;&#30340;&#23545;&#35937;&#20998;&#21106;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#31070;&#32463;&#31185;&#23398;&#20013;&#23545;&#31070;&#32463;&#20803;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.00130</link><description>&lt;p&gt;
FISBe: &#19968;&#31181;&#29992;&#20110;&#38271;&#31243;&#32454;&#19997;&#29376;&#32467;&#26500;&#23545;&#35937;&#20998;&#21106;&#30340;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FISBe: A real-world benchmark dataset for instance segmentation of long-range thin filamentous structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00130
&lt;/p&gt;
&lt;p&gt;
FISBe&#25552;&#20379;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38271;&#31243;&#32454;&#19997;&#29366;&#32467;&#26500;&#30340;&#23545;&#35937;&#20998;&#21106;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#31070;&#32463;&#31185;&#23398;&#20013;&#23545;&#31070;&#32463;&#20803;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#31995;&#32479;&#20307;&#31215;&#20809;&#23398;&#26174;&#24494;&#25104;&#20687;&#20013;&#23545;&#31070;&#32463;&#20803;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#65292;&#21487;&#20197;&#36890;&#36807;&#20419;&#36827;&#31070;&#32463;&#22238;&#36335;&#30340;&#32852;&#21512;&#21151;&#33021;&#21644;&#24418;&#24577;&#20998;&#26512;&#65292;&#20026;&#31070;&#32463;&#31185;&#23398;&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#25552;&#20379;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22810;&#31070;&#32463;&#20803;&#20809;&#23398;&#26174;&#24494;&#25968;&#25454;&#20855;&#26377;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#29305;&#28857;&#65306;&#21333;&#20010;&#31070;&#32463;&#20803;&#20855;&#26377;&#38271;&#31243;&#12289;&#32454;&#19997;&#29376;&#21644;&#24191;&#27867;&#20998;&#25903;&#30340;&#24418;&#24577;&#65292;&#22810;&#20010;&#31070;&#32463;&#20803;&#32039;&#23494;&#20132;&#32455;&#65292;&#37096;&#20998;&#20307;&#31215;&#25928;&#24212;&#12289;&#19981;&#22343;&#21248;&#29031;&#26126;&#21644;&#20809;&#23398;&#26174;&#24494;&#22266;&#26377;&#30340;&#22122;&#38899;&#20005;&#37325;&#38459;&#30861;&#20102;&#20010;&#20307;&#31070;&#32463;&#20803;&#30340;&#23616;&#37096;&#20998;&#31163;&#21644;&#38271;&#31243;&#36861;&#36394;&#12290;&#36825;&#20123;&#23646;&#24615;&#21453;&#26144;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#24403;&#21069;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#22914;&#20309;&#26377;&#25928;&#22320;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#30456;&#20851;&#26041;&#27861;&#30740;&#31350;&#21313;&#20998;&#27963;&#36291;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#26041;&#27861;&#20027;&#35201;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00130v1 Announce Type: cross  Abstract: Instance segmentation of neurons in volumetric light microscopy images of nervous systems enables groundbreaking research in neuroscience by facilitating joint functional and morphological analyses of neural circuits at cellular resolution. Yet said multi-neuron light microscopy data exhibits extremely challenging properties for the task of instance segmentation: Individual neurons have long-ranging, thin filamentous and widely branching morphologies, multiple neurons are tightly inter-weaved, and partial volume effects, uneven illumination and noise inherent to light microscopy severely impede local disentangling as well as long-range tracing of individual neurons. These properties reflect a current key challenge in machine learning research, namely to effectively capture long-range dependencies in the data. While respective methodological research is buzzing, to date methods are typically benchmarked on synthetic datasets. To address
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;ACEv2&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#19982;&#37327;&#21270;&#27169;&#22411;&#30340;&#25512;&#29702;&#25104;&#26412;&#21644;&#22312;ML&#30828;&#20214;&#19978;&#30340;&#33021;&#32791;&#26356;&#21305;&#37197;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;PikeLPN&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#37327;&#21270;&#24212;&#29992;&#20110;&#36880;&#20803;&#32032;&#25805;&#20316;&#21644;&#20056;&#32047;&#31215;&#25805;&#20316;&#65292;&#35299;&#20915;&#20102;&#20302;&#31934;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34987;&#24573;&#35270;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00103</link><description>&lt;p&gt;
PikeLPN: &#32531;&#35299;&#20302;&#31934;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#20302;&#25928;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
PikeLPN: Mitigating Overlooked Inefficiencies of Low-Precision Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;ACEv2&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#19982;&#37327;&#21270;&#27169;&#22411;&#30340;&#25512;&#29702;&#25104;&#26412;&#21644;&#22312;ML&#30828;&#20214;&#19978;&#30340;&#33021;&#32791;&#26356;&#21305;&#37197;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;PikeLPN&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#37327;&#21270;&#24212;&#29992;&#20110;&#36880;&#20803;&#32032;&#25805;&#20316;&#21644;&#20056;&#32047;&#31215;&#25805;&#20316;&#65292;&#35299;&#20915;&#20102;&#20302;&#31934;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34987;&#24573;&#35270;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31934;&#24230;&#37327;&#21270;&#20197;&#20854;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21151;&#25928;&#32780;&#38395;&#21517;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#38750;&#37327;&#21270;&#30340;&#36880;&#20803;&#32032;&#25805;&#20316;&#65292;&#22312;&#35832;&#22914;&#21442;&#25968;&#21270;&#28608;&#27963;&#20989;&#25968;&#12289;&#25209;&#37327;&#24402;&#19968;&#21270;&#21644;&#37327;&#21270;&#32553;&#25918;&#31561;&#23618;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#20027;&#23548;&#20102;&#20302;&#31934;&#24230;&#27169;&#22411;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#36825;&#20123;&#38750;&#37327;&#21270;&#30340;&#36880;&#20803;&#32032;&#25805;&#20316;&#36890;&#24120;&#34987;&#24573;&#35270;&#20110;&#22522;&#20110;&#31639;&#26415;&#35745;&#31639;&#24037;&#20316;&#37327;&#65288;ACE&#65289;&#31561;&#26368;&#20808;&#36827;&#30340;&#25928;&#29575;&#24230;&#37327;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACEv2 - &#19968;&#20010;ACE&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#33021;&#26356;&#22909;&#22320;&#19982;&#37327;&#21270;&#27169;&#22411;&#30340;&#25512;&#29702;&#25104;&#26412;&#20197;&#21450;&#23427;&#20204;&#22312;ML&#30828;&#20214;&#19978;&#30340;&#33021;&#32791;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PikeLPN&#65292;&#19968;&#20010;&#36890;&#36807;&#23558;&#37327;&#21270;&#24212;&#29992;&#20110;&#36880;&#20803;&#32032;&#25805;&#20316;&#21644;&#20056;&#32047;&#31215;&#25805;&#20316;&#26469;&#35299;&#20915;&#36825;&#20123;&#25928;&#29575;&#38382;&#39064;&#30340;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38024;&#23545;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#30340;&#37327;&#21270;&#25216;&#26415;&#65292;&#21517;&#20026;QuantNorm&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#25209;&#37327;&#24402;&#19968;&#21270;&#25805;&#20316;&#30340;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00103v1 Announce Type: new  Abstract: Low-precision quantization is recognized for its efficacy in neural network optimization. Our analysis reveals that non-quantized elementwise operations which are prevalent in layers such as parameterized activation functions, batch normalization, and quantization scaling dominate the inference cost of low-precision models. These non-quantized elementwise operations are commonly overlooked in SOTA efficiency metrics such as Arithmetic Computation Effort (ACE). In this paper, we propose ACEv2 - an extended version of ACE which offers a better alignment with the inference cost of quantized models and their energy consumption on ML hardware. Moreover, we introduce PikeLPN, a model that addresses these efficiency issues by applying quantization to both elementwise operations and multiply-accumulate operations. In particular, we present a novel quantization technique for batch normalization layers named QuantNorm which allows for quantizing t
&lt;/p&gt;</description></item><item><title>&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#27169;&#22411;&#20026;&#32479;&#35745;&#27169;&#22411;&#36873;&#25321;&#25552;&#20379;&#20102;&#28789;&#27963;&#32780;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#30340;&#22810;&#25165;&#22810;&#33402;&#21644;&#39640;&#25928;&#24615;&#65292;&#20026;&#22312;&#21508;&#20010;&#23398;&#31185;&#39046;&#22495;&#20013;&#24212;&#23545;&#22797;&#26434;&#25361;&#25112;&#25552;&#20379;&#20102;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.00085</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65306;&#28145;&#24230;&#23398;&#20064;&#30340;&#26367;&#20195;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Bayesian Nonparametrics: An Alternative to Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00085
&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#27169;&#22411;&#20026;&#32479;&#35745;&#27169;&#22411;&#36873;&#25321;&#25552;&#20379;&#20102;&#28789;&#27963;&#32780;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#30340;&#22810;&#25165;&#22810;&#33402;&#21644;&#39640;&#25928;&#24615;&#65292;&#20026;&#22312;&#21508;&#20010;&#23398;&#31185;&#39046;&#22495;&#20013;&#24212;&#23545;&#22797;&#26434;&#25361;&#25112;&#25552;&#20379;&#20102;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#27169;&#22411;&#20026;&#32479;&#35745;&#27169;&#22411;&#36873;&#25321;&#25552;&#20379;&#20102;&#28789;&#27963;&#32780;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#27169;&#22411;&#22797;&#26434;&#24230;&#36866;&#24212;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#28145;&#20837;&#25506;&#35752;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#20915;&#32479;&#35745;&#23398;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#30005;&#27668;&#24037;&#31243;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#22797;&#26434;&#25361;&#25112;&#26041;&#38754;&#12290;&#36890;&#36807;&#38416;&#26126;&#36825;&#20123;&#38750;&#21442;&#25968;&#27169;&#22411;&#30340;&#22522;&#26412;&#29305;&#24615;&#21644;&#29702;&#35770;&#22522;&#30784;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#25552;&#20379;&#23545;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#21450;&#20854;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#30340;&#30456;&#20851;&#24615;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#30446;&#26631;&#36319;&#36394;&#39046;&#22495;&#12290;&#36890;&#36807;&#36825;&#31181;&#25506;&#32034;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#30340;&#22810;&#25165;&#22810;&#33402;&#21644;&#39640;&#25928;&#24615;&#65292;&#20026;&#22312;&#21508;&#20010;&#23398;&#31185;&#39046;&#22495;&#20013;&#24212;&#23545;&#22797;&#26434;&#25361;&#25112;&#25552;&#20379;&#20102;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00085v1 Announce Type: new  Abstract: Bayesian nonparametric models offer a flexible and powerful framework for statistical model selection, enabling the adaptation of model complexity to the intricacies of diverse datasets. This survey intends to delve into the significance of Bayesian nonparametrics, particularly in addressing complex challenges across various domains such as statistics, computer science, and electrical engineering. By elucidating the basic properties and theoretical foundations of these nonparametric models, this survey aims to provide a comprehensive understanding of Bayesian nonparametrics and their relevance in addressing complex problems, particularly in the domain of multi-object tracking. Through this exploration, we uncover the versatility and efficacy of Bayesian nonparametric methodologies, paving the way for innovative solutions to intricate challenges across diverse disciplines.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#23454;&#29616;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#30340;&#21442;&#25968;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#22768;&#23398;&#29305;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2404.00082</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#21644;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#30340;&#25968;&#25454;&#39537;&#21160;&#23460;&#20869;&#22768;&#23398;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay Networks With Learnable Delay Lines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#23454;&#29616;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#30340;&#21442;&#25968;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#22768;&#23398;&#29305;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#35774;&#35745;&#20154;&#24037;&#28151;&#21709;&#31639;&#27861;&#65292;&#26088;&#22312;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#30340;&#23460;&#20869;&#22768;&#23398;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24310;&#36831;&#32593;&#32476;&#27169;&#22411;&#30340;&#33258;&#21160;&#21442;&#25968;&#35843;&#25972;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#65288;FDN&#65289;&#30340;&#21442;&#25968;&#65292;&#20351;&#20854;&#36755;&#20986;&#21576;&#29616;&#20986;&#25152;&#27979;&#24471;&#30340;&#23460;&#20869;&#33033;&#20914;&#21709;&#24212;&#30340;&#24863;&#30693;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00082v1 Announce Type: cross  Abstract: Over the past few decades, extensive research has been devoted to the design of artificial reverberation algorithms aimed at emulating the room acoustics of physical environments. Despite significant advancements, automatic parameter tuning of delay-network models remains an open challenge. We introduce a novel method for finding the parameters of a Feedback Delay Network (FDN) such that its output renders the perceptual qualities of a measured room impulse response. The proposed approach involves the implementation of a differentiable FDN with trainable delay lines, which, for the first time, allows us to simultaneously learn each and every delay-network parameter via backpropagation. The iterative optimization process seeks to minimize a time-domain loss function incorporating differentiable terms accounting for energy decay and echo density. Through experimental validation, we show that the proposed method yields time-invariant freq
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#30340;GAN&#65292;&#21363;InstGAN&#65292;&#20197;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#29983;&#25104;&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#21270;&#20449;&#24687;&#29109;&#26469;&#32531;&#35299;&#27169;&#24335;&#23849;&#28291;&#12290;</title><link>https://arxiv.org/abs/2404.00081</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Molecular Generative Adversarial Network with Multi-Property Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#30340;GAN&#65292;&#21363;InstGAN&#65292;&#20197;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#29983;&#25104;&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#21270;&#20449;&#24687;&#29109;&#26469;&#32531;&#35299;&#27169;&#24335;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#65292;&#24050;&#34987;&#24212;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#20013;$de~novo$&#20998;&#23376;&#29983;&#25104;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#65292;&#26469;&#22788;&#29702;GANs&#20013;&#20998;&#23376;&#34920;&#31034;&#30340;&#31163;&#25955;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;GANs&#21644;RL&#27169;&#22411;&#30340;&#22266;&#26377;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#20197;&#21450;&#19982;MCTS&#37319;&#26679;&#30456;&#20851;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;MCTS RL-based GANs&#38590;&#20197;&#25193;&#23637;&#21040;&#22823;&#22411;&#21270;&#23398;&#25968;&#25454;&#24211;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24102;&#21363;&#26102;&#21644;&#20840;&#23616;&#22870;&#21169;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;RL&#30340;&#26032;&#22411;GAN&#65292;&#31216;&#20026;InstGAN&#65292;&#20197;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#29983;&#25104;&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#12290;&#27492;&#22806;&#65292;&#26368;&#22823;&#21270;&#20449;&#24687;&#29109;&#34987;&#21033;&#29992;&#26469;&#32531;&#35299;&#27169;&#24335;&#23849;&#28291;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;InstGAN&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#65292;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00081v1 Announce Type: cross  Abstract: Deep generative models, such as generative adversarial networks (GANs), have been employed for $de~novo$ molecular generation in drug discovery. Most prior studies have utilized reinforcement learning (RL) algorithms, particularly Monte Carlo tree search (MCTS), to handle the discrete nature of molecular representations in GANs. However, due to the inherent instability in training GANs and RL models, along with the high computational cost associated with MCTS sampling, MCTS RL-based GANs struggle to scale to large chemical databases. To tackle these challenges, this study introduces a novel GAN based on actor-critic RL with instant and global rewards, called InstGAN, to generate molecules at the token-level with multi-property optimization. Furthermore, maximized information entropy is leveraged to alleviate the mode collapse. The experimental results demonstrate that InstGAN outperforms other baselines, achieves comparable performance
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#65292;&#21033;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#22312;&#36873;&#23450;&#30340;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;</title><link>https://arxiv.org/abs/2404.00076</link><description>&lt;p&gt;
&#20351;&#29992;&#20498;&#32622;&#26631;&#31614;&#30340;&#21518;&#38376;&#26041;&#27861;&#65306;&#33039;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#65292;&#21033;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#22312;&#36873;&#23450;&#30340;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22768;&#38899;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#32463;&#24120;&#20351;&#29992;&#20844;&#20849;&#25110;&#31532;&#19977;&#26041;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;&#36825;&#20351;&#24471;&#35757;&#32451;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#28508;&#22312;&#30340;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#12290;&#22312;&#36825;&#31181;&#25915;&#20987;&#31867;&#22411;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#27602;&#21270;&#25968;&#25454;&#26469;&#35757;&#32451;DNN&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#38477;&#20302;&#20854;&#24615;&#33021;&#12290;&#21478;&#19968;&#31181;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#38750;&#24120;&#30456;&#20851;&#30340;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#31867;&#22411;&#26159;&#26631;&#31614;&#32763;&#36716;&#65292;&#25915;&#20987;&#32773;&#22312;&#20854;&#20013;&#25805;&#32437;&#25968;&#25454;&#23376;&#38598;&#30340;&#26631;&#31614;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#21363;&#20351;&#26159;&#33021;&#21147;&#26377;&#38480;&#30340;&#25915;&#20987;&#32773;&#65292;&#36825;&#20123;&#25915;&#20987;&#20063;&#21487;&#33021;&#26497;&#22823;&#22320;&#38477;&#20302;&#31995;&#32479;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#65292;&#8220;&#26631;&#31614;&#23545;&#26631;&#31614;&#8221;&#65292;&#22312;&#19982;&#30446;&#26631;&#31867;&#21035;&#30456;&#20851;&#30340;&#36873;&#23450;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65288;&#25293;&#25163;&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00076v1 Announce Type: cross  Abstract: Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CO$_2$&#23553;&#23384;&#39033;&#30446;&#20013;&#37319;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#30340;&#20248;&#21270;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#27969;&#20307;&#27969;&#21160;&#27714;&#35299;&#22120;&#21644;&#29983;&#25104;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#30830;&#20445;&#22312;&#26377;&#38480;&#39044;&#31639;&#20869;&#24067;&#32622;&#26368;&#20339;&#30340;&#30417;&#27979;&#20117;&#20301;&#12290;</title><link>https://arxiv.org/abs/2404.00075</link><description>&lt;p&gt;
BEACON: &#20855;&#26377;&#26465;&#20214;&#27491;&#24577;&#27969;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#21152;&#36895; - &#20108;&#27687;&#21270;&#30899;&#23553;&#23384;&#20013;&#26368;&#20339;&#30417;&#27979;&#20117;&#24067;&#32622;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
BEACON: Bayesian Experimental design Acceleration with Conditional Normalizing flows $-$ a case study in optimal monitor well placement for CO$_2$ sequestration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CO$_2$&#23553;&#23384;&#39033;&#30446;&#20013;&#37319;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#30340;&#20248;&#21270;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#27969;&#20307;&#27969;&#21160;&#27714;&#35299;&#22120;&#21644;&#29983;&#25104;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#30830;&#20445;&#22312;&#26377;&#38480;&#39044;&#31639;&#20869;&#24067;&#32622;&#26368;&#20339;&#30340;&#30417;&#27979;&#20117;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CO$_2$&#23553;&#23384;&#26159;&#32531;&#35299;&#27668;&#20505;&#21464;&#21270;&#30340;&#20851;&#38190;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20648;&#23618;&#23646;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38656;&#35201;&#23545;CO$_2$&#22320;&#19979;&#27969;&#30340;&#20005;&#26684;&#30417;&#27979;&#65292;&#20197;&#38450;&#27490;&#39118;&#38505;&#65292;&#22914;&#27844;&#28431;&#12289;&#35825;&#21457;&#22320;&#38663;&#25110;&#31361;&#30772;&#35768;&#21487;&#36793;&#30028;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#39033;&#30446;&#32463;&#29702;&#20351;&#29992;&#38075;&#23380;&#20117;&#22312;&#29305;&#23450;&#20301;&#32622;&#30452;&#25509;&#30417;&#27979;CO$_2$&#21644;&#21387;&#21147;&#12290;&#37492;&#20110;&#38075;&#20117;&#30340;&#39640;&#25104;&#26412;&#65292;&#20851;&#38190;&#26159;&#25112;&#30053;&#22320;&#24067;&#32622;&#26377;&#38480;&#25968;&#37327;&#30340;&#20117;&#65292;&#20197;&#30830;&#20445;&#22312;&#39044;&#31639;&#38480;&#21046;&#20869;&#36827;&#34892;&#26368;&#26377;&#25928;&#30340;&#30417;&#27979;&#12290;&#25105;&#20204;&#36873;&#25321;&#20117;&#20301;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#29992;&#20110;&#39044;&#27979;&#22320;&#19979;&#27969;&#36712;&#36857;&#30340;&#27969;&#20307;&#27969;&#21160;&#27714;&#35299;&#22120;&#21644;&#29992;&#20110;&#25512;&#26029;&#22320;&#19979;&#27969;&#19981;&#30830;&#23450;&#24615;&#30340;&#29983;&#25104;&#24335;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#32500;&#39046;&#22495;&#20013;&#26159;&#21487;&#25193;&#23637;&#30340;&#65292;&#24182;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#20869;&#24320;&#21457;&#65292;&#20197;&#23454;&#29616;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#65292;&#20445;&#35777;&#21487;&#25193;&#23637;&#24615;&#21644;&#25968;&#23398;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00075v1 Announce Type: new  Abstract: CO$_2$ sequestration is a crucial engineering solution for mitigating climate change. However, the uncertain nature of reservoir properties, necessitates rigorous monitoring of CO$_2$ plumes to prevent risks such as leakage, induced seismicity, or breaching licensed boundaries. To address this, project managers use borehole wells for direct CO$_2$ and pressure monitoring at specific locations. Given the high costs associated with drilling, it is crucial to strategically place a limited number of wells to ensure maximally effective monitoring within budgetary constraints. Our approach for selecting well locations integrates fluid-flow solvers for forecasting plume trajectories with generative neural networks for plume inference uncertainty. Our methodology is extensible to three-dimensional domains and is developed within a Bayesian framework for optimal experimental design, ensuring scalability and mathematical optimality. We use a reali
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26377;&#38480;&#31639;&#23376;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23398;&#20064;&#21442;&#25968;&#21270;&#35299;&#20915;&#20102;&#24494;&#32467;&#26500;&#24377;&#24615;&#23646;&#24615;&#26144;&#23556;&#21040;&#26426;&#26800;&#21464;&#24418;&#30340;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#33021;&#22788;&#29702;&#20855;&#26377;&#26126;&#26174;&#19981;&#36830;&#32493;&#24615;&#30340;&#35299;&#12290;</title><link>https://arxiv.org/abs/2404.00074</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23558;&#24494;&#32467;&#26500;&#30340;&#24377;&#24615;&#23646;&#24615;&#26144;&#23556;&#21040;&#20854;&#26426;&#26800;&#21464;&#24418;&#30340;&#26377;&#38480;&#31639;&#23376;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A finite operator learning technique for mapping the elastic properties of microstructures to their mechanical deformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00074
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26377;&#38480;&#31639;&#23376;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23398;&#20064;&#21442;&#25968;&#21270;&#35299;&#20915;&#20102;&#24494;&#32467;&#26500;&#24377;&#24615;&#23646;&#24615;&#26144;&#23556;&#21040;&#26426;&#26800;&#21464;&#24418;&#30340;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#33021;&#22788;&#29702;&#20855;&#26377;&#26126;&#26174;&#19981;&#36830;&#32493;&#24615;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24320;&#21457;&#22266;&#20307;&#21147;&#23398;&#20013;&#25511;&#21046;&#29289;&#29702;&#26041;&#31243;&#30340;&#26356;&#24555;&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#23398;&#20064;&#26426;&#26800;&#24179;&#34913;&#35299;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#23558;&#26631;&#20934;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#27867;&#21270;&#21644;&#22686;&#24378;&#65292;&#20197;&#23398;&#20064;&#19968;&#20010;&#24102;&#26377;&#30456;&#24403;&#26126;&#26174;&#19981;&#36830;&#32493;&#24615;&#30340;&#21442;&#25968;&#35299;&#12290;&#25105;&#20204;&#20197;&#24494;&#35266;&#21147;&#23398;&#20026;&#20363;&#65292;&#20854;&#20013;&#23545;&#20110;&#32473;&#23450;&#30340;&#24322;&#36136;&#24494;&#32467;&#26500;&#26469;&#35828;&#65292;&#24494;&#35266;&#21147;&#23398;&#35299;&#65288;&#21363;&#21464;&#24418;&#21644;&#24212;&#21147;&#22330;&#65289;&#30340;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#38024;&#23545;&#30340;&#21442;&#25968;&#26159;&#24322;&#36136;&#22266;&#20307;&#31995;&#32479;&#20869;&#30340;&#26472;&#27663;&#27169;&#37327;&#20998;&#24067;&#12290;&#21463;&#31639;&#23376;&#23398;&#20064;&#21644;&#26377;&#38480;&#20803;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#26080;&#38656;&#20381;&#36182;&#20854;&#20182;&#25968;&#20540;&#27714;&#35299;&#22120;&#25968;&#25454;&#23601;&#33021;&#36827;&#34892;&#35757;&#32451;&#30340;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26377;&#38480;&#20803;&#26041;&#27861;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00074v1 Announce Type: new  Abstract: To develop faster solvers for governing physical equations in solid mechanics, we introduce a method that parametrically learns the solution to mechanical equilibrium. The introduced method outperforms traditional ones in terms of computational cost while acceptably maintaining accuracy. Moreover, it generalizes and enhances the standard physics-informed neural networks to learn a parametric solution with rather sharp discontinuities. We focus on micromechanics as an example, where the knowledge of the micro-mechanical solution, i.e., deformation and stress fields for a given heterogeneous microstructure, is crucial. The parameter under investigation is the Young modulus distribution within the heterogeneous solid system. Our method, inspired by operator learning and the finite element method, demonstrates the ability to train without relying on data from other numerical solvers. Instead, we leverage ideas from the finite element approac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20004;&#38454;&#27573;&#65288;&#31895;&#30053;&#21484;&#22238;&#21644;&#31934;&#32454;&#36873;&#25321;&#65289;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65292;&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#36873;&#25321;&#31283;&#20581;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#24615;&#33021;&#65292;&#32858;&#31867;&#23637;&#31034;&#30456;&#20284;&#35757;&#32451;&#24615;&#33021;&#30340;&#27169;&#22411;</title><link>https://arxiv.org/abs/2404.00069</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#24555;&#36895;&#27169;&#22411;&#36873;&#25321;&#30340;&#20004;&#38454;&#27573;&#21484;&#22238;&#21644;&#36873;&#25321;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Two-Phase Recall-and-Select Framework for Fast Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00069
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20004;&#38454;&#27573;&#65288;&#31895;&#30053;&#21484;&#22238;&#21644;&#31934;&#32454;&#36873;&#25321;&#65289;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65292;&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#36873;&#25321;&#31283;&#20581;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#24615;&#33021;&#65292;&#32858;&#31867;&#23637;&#31034;&#30456;&#20284;&#35757;&#32451;&#24615;&#33021;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#26222;&#21450;&#65292;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22823;&#37327;&#35757;&#32451;&#21644;&#22312;&#20844;&#20849;&#27169;&#22411;&#23384;&#20648;&#24211;&#20013;&#20849;&#20139;&#24050;&#32463;&#26159;&#21496;&#31354;&#35265;&#24815;&#12290;&#22312;&#38024;&#23545;&#24615;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#21033;&#29992;&#36866;&#24403;&#30340;&#28304;&#27169;&#22411;&#20316;&#20026;&#36215;&#28857;&#36890;&#24120;&#20248;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#20808;&#21069;&#24037;&#20316;&#20013;&#23545;&#35768;&#22810;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#24320;&#21457;&#65292;&#20294;&#35813;&#36807;&#31243;&#20173;&#28982;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#27169;&#22411;&#23384;&#20648;&#24211;&#35268;&#27169;&#19981;&#26029;&#22686;&#21152;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#65288;&#31895;&#30053;&#21484;&#22238;&#21644;&#31934;&#32454;&#36873;&#25321;&#65289;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#24615;&#33021;&#26469;&#25552;&#39640;&#36873;&#25321;&#31283;&#20581;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#31895;&#30053;&#21484;&#22238;&#38454;&#27573;&#23558;&#23637;&#31034;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24615;&#33021;&#30456;&#20284;&#30340;&#27169;&#22411;&#36827;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00069v1 Announce Type: new  Abstract: As the ubiquity of deep learning in various machine learning applications has amplified, a proliferation of neural network models has been trained and shared on public model repositories. In the context of a targeted machine learning assignment, utilizing an apt source model as a starting point typically outperforms the strategy of training from scratch, particularly with limited training data. Despite the investigation and development of numerous model selection strategies in prior work, the process remains time-consuming, especially given the ever-increasing scale of model repositories. In this paper, we propose a two-phase (coarse-recall and fine-selection) model selection framework, aiming to enhance the efficiency of selecting a robust model by leveraging the models' training performances on benchmark datasets. Specifically, the coarse-recall phase clusters models showcasing similar training performances on benchmark datasets in an 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#26469;&#39044;&#27979;&#21487;&#33021;&#25104;&#20026;&#32593;&#32476;&#25915;&#20987;&#21463;&#23475;&#32773;&#30340;&#20010;&#20154;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#35813;&#27169;&#22411;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00068</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20851;&#38190;&#39118;&#38505;&#22240;&#32032;&#23545;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Data-Driven Predictive Analysis on Cyber Security Threats with Key Risk Factors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00068
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#26469;&#39044;&#27979;&#21487;&#33021;&#25104;&#20026;&#32593;&#32476;&#25915;&#20987;&#21463;&#23475;&#32773;&#30340;&#20010;&#20154;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#35813;&#27169;&#22411;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#39118;&#38505;&#25351;&#30340;&#26159;&#30772;&#22351;&#22768;&#35465;&#12289;&#36135;&#24065;&#25439;&#22833;&#25110;&#32452;&#32455;&#25110;&#20010;&#20154;&#30340;&#24178;&#25200;&#39118;&#38505;&#65292;&#36825;&#31181;&#24773;&#20917;&#36890;&#24120;&#26159;&#30001;&#20110;&#23545;&#32593;&#32476;&#31995;&#32479;&#30340;&#26080;&#24847;&#35782;&#20351;&#29992;&#32780;&#23548;&#33268;&#30340;&#12290;&#32593;&#32476;&#39118;&#38505;&#27599;&#22825;&#37117;&#22312;&#36880;&#28176;&#22686;&#21152;&#65292;&#30446;&#21069;&#24050;&#25104;&#20026;&#20840;&#29699;&#24615;&#23041;&#32961;&#12290;&#20687;&#23391;&#21152;&#25289;&#22269;&#36825;&#26679;&#30340;&#21457;&#23637;&#20013;&#22269;&#23478;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#32593;&#32476;&#39118;&#38505;&#25361;&#25112;&#12290;&#20840;&#29699;&#33539;&#22260;&#20869;&#19981;&#26029;&#22686;&#38271;&#30340;&#32593;&#32476;&#23041;&#32961;&#24378;&#35843;&#20102;&#23545;&#26377;&#25928;&#24314;&#27169;&#20197;&#39044;&#27979;&#21644;&#31649;&#29702;&#30456;&#20851;&#39118;&#38505;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#26469;&#39044;&#27979;&#21487;&#33021;&#25104;&#20026;&#32593;&#32476;&#25915;&#20987;&#21463;&#23475;&#32773;&#30340;&#20010;&#20154;&#12290;&#25105;&#20204;&#20174;&#32593;&#32476;&#25915;&#20987;&#21463;&#23475;&#32773;&#21644;&#38750;&#21463;&#23475;&#32773;&#20013;&#25910;&#38598;&#20102;&#22522;&#20110;&#31038;&#20250;&#20154;&#21475;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#30740;&#31350;&#28041;&#21450;&#24320;&#21457;&#38382;&#21367;&#20197;&#25910;&#38598;&#25968;&#25454;&#65292;&#28982;&#21518;&#29992;&#20110;&#34913;&#37327;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#65292;&#25968;&#25454;&#38598;&#25193;&#23637;&#21040;3286&#26465;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00068v1 Announce Type: cross  Abstract: Cyber risk refers to the risk of defacing reputation, monetary losses, or disruption of an organization or individuals, and this situation usually occurs by the unconscious use of cyber systems. The cyber risk is unhurriedly increasing day by day and it is right now a global threat. Developing countries like Bangladesh face major cyber risk challenges. The growing cyber threat worldwide focuses on the need for effective modeling to predict and manage the associated risk. This paper exhibits a Machine Learning(ML) based model for predicting individuals who may be victims of cyber attacks by analyzing socioeconomic factors. We collected the dataset from victims and non-victims of cyberattacks based on socio-demographic features. The study involved the development of a questionnaire to gather data, which was then used to measure the significance of features. Through data augmentation, the dataset was expanded to encompass 3286 entries, se
&lt;/p&gt;</description></item><item><title>&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#22312;&#37329;&#34701;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#36866;&#24212;&#20102;&#29616;&#20195;&#37329;&#34701;&#31995;&#32479;&#21160;&#24577;&#21644;&#22797;&#26434;&#30340;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00060</link><description>&lt;p&gt;
&#37329;&#34701;&#32593;&#32476;&#20013;&#30340;&#26102;&#38388;&#22270;&#32593;&#32476;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Temporal Graph Networks for Graph Anomaly Detection in Financial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00060
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#22312;&#37329;&#34701;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#36866;&#24212;&#20102;&#29616;&#20195;&#37329;&#34701;&#31995;&#32479;&#21160;&#24577;&#21644;&#22797;&#26434;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#36827;&#34892;&#37329;&#34701;&#24322;&#24120;&#26816;&#27979;&#65292;&#22312;&#37329;&#34701;&#31185;&#25216;&#21644;&#25968;&#23383;&#21270;&#37329;&#34701;&#20132;&#26131;&#26102;&#20195;&#36825;&#26159;&#19968;&#20010;&#36843;&#20999;&#38656;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;TGN&#25429;&#25417;&#37329;&#34701;&#32593;&#32476;&#20013;&#36793;&#30340;&#21160;&#24577;&#21464;&#21270;&#65292;&#29992;&#20110;&#27450;&#35784;&#26816;&#27979;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;TGN&#19982;&#38745;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22522;&#32447;&#20197;&#21450;&#20351;&#29992;DGraph&#25968;&#25454;&#38598;&#36827;&#34892;&#29616;&#23454;&#37329;&#34701;&#22330;&#26223;&#19979;&#30340;&#21069;&#27839;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TGN&#22312;AUC&#25351;&#26631;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#36825;&#31181;&#20248;&#36234;&#24615;&#33021;&#31361;&#26174;&#20102;TGN&#20316;&#20026;&#26816;&#27979;&#37329;&#34701;&#27450;&#35784;&#30340;&#26377;&#25928;&#24037;&#20855;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#29616;&#20195;&#37329;&#34701;&#31995;&#32479;&#21160;&#24577;&#21644;&#22797;&#26434;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#22312;TGN&#26694;&#26550;&#20869;&#23581;&#35797;&#20102;&#21508;&#31181;&#22270;&#23884;&#20837;&#27169;&#22359;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00060v1 Announce Type: cross  Abstract: This paper explores the utilization of Temporal Graph Networks (TGN) for financial anomaly detection, a pressing need in the era of fintech and digitized financial transactions. We present a comprehensive framework that leverages TGN, capable of capturing dynamic changes in edges within financial networks, for fraud detection. Our study compares TGN's performance against static Graph Neural Network (GNN) baselines, as well as cutting-edge hypergraph neural network baselines using DGraph dataset for a realistic financial context. Our results demonstrate that TGN significantly outperforms other models in terms of AUC metrics. This superior performance underlines TGN's potential as an effective tool for detecting financial fraud, showcasing its ability to adapt to the dynamic and complex nature of modern financial systems. We also experimented with various graph embedding modules within the TGN framework and compared the effectiveness of 
&lt;/p&gt;</description></item><item><title>&#36816;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;HTTP&#21709;&#24212;&#22836;&#30340;&#32534;&#30721;&#35782;&#21035;Web&#26381;&#21153;&#22120;&#65292;&#21487;&#20197;&#22312;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#29305;&#24322;&#24615;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#23545;&#26131;&#21463;&#25915;&#20987;&#30340;Web&#26381;&#21153;&#22120;&#29256;&#26412;&#30340;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2404.00056</link><description>&lt;p&gt;
&#36890;&#36807;Transformer&#32534;&#30721;&#30340;HTTP&#21709;&#24212;&#22836;&#25351;&#32441;&#35782;&#21035;Web&#26381;&#21153;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fingerprinting web servers through Transformer-encoded HTTP response headers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00056
&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;HTTP&#21709;&#24212;&#22836;&#30340;&#32534;&#30721;&#35782;&#21035;Web&#26381;&#21153;&#22120;&#65292;&#21487;&#20197;&#22312;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#29305;&#24322;&#24615;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#23545;&#26131;&#21463;&#25915;&#20987;&#30340;Web&#26381;&#21153;&#22120;&#29256;&#26412;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#12289;&#22823;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#22686;&#24378;&#23545;&#26131;&#21463;&#25915;&#20987;&#30340;Web&#26381;&#21153;&#22120;&#29256;&#26412;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#21521;477&#19975;&#20010;&#22495;&#21457;&#36865;&#21508;&#31181;&#27169;&#31946;&#21644;&#38750;&#26631;&#20934;&#30340;HTTP&#35831;&#27714;&#65292;&#24182;&#25429;&#33719;HTTP&#21709;&#24212;&#29366;&#24577;&#34892;&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;BPE&#20998;&#35789;&#22120;&#21644;RoBERTa&#32534;&#30721;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#26469;&#34920;&#31034;&#36825;&#20123;&#29366;&#24577;&#34892;&#12290;&#28982;&#21518;&#23545;&#32534;&#30721;&#30340;&#21709;&#24212;&#34892;&#36827;&#34892;&#38477;&#32500;&#24182;&#36830;&#25509;&#20197;&#34920;&#31034;&#27599;&#20010;&#22495;&#30340;Web&#26381;&#21153;&#22120;&#12290;&#19968;&#20010;&#38543;&#26426;&#26862;&#26519;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#23545;&#36825;&#20123;Web&#26381;&#21153;&#22120;&#36827;&#34892;&#20998;&#31867;&#65292;&#22312;&#26816;&#27979;&#21040;&#20116;&#31181;&#26368;&#27969;&#34892;&#30340;&#21021;&#22987;Web&#26381;&#21153;&#22120;&#26102;&#20998;&#21035;&#36798;&#21040;0.94&#21644;0.96&#30340;&#23439;F1&#20998;&#25968;&#12290;MLP&#22312;&#20998;&#31867;347&#20010;&#20027;&#35201;&#31867;&#22411;&#21644;&#27425;&#35201;&#29256;&#26412;&#23545;&#26102;&#23454;&#29616;&#20102;0.55&#30340;&#21152;&#26435;F1&#20998;&#25968;&#12290;&#20998;&#26512;&#34920;&#26126;&#25105;&#20204;&#30340;&#27979;&#35797;&#29992;&#20363;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00056v1 Announce Type: cross  Abstract: We explored leveraging state-of-the-art deep learning, big data, and natural language processing to enhance the detection of vulnerable web server versions. Focusing on improving accuracy and specificity over rule-based systems, we conducted experiments by sending various ambiguous and non-standard HTTP requests to 4.77 million domains and capturing HTTP response status lines. We represented these status lines through training a BPE tokenizer and RoBERTa encoder for unsupervised masked language modeling. We then dimensionality reduced and concatenated encoded response lines to represent each domain's web server. A Random Forest and multilayer perceptron (MLP) classified these web servers, and achieved 0.94 and 0.96 macro F1-score, respectively, on detecting the five most popular origin web servers. The MLP achieved a weighted F1-score of 0.55 on classifying 347 major type and minor version pairs. Analysis indicates that our test cases 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#25551;&#36848;&#30340;&#35774;&#35745;&#24037;&#20855;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#21487;&#35270;&#21270;&#33402;&#26415;&#36816;&#21160;&#65292;&#20854;&#20013;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#24490;&#29615;&#23646;&#24615;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21644;&#29983;&#25104;&#36924;&#30495;&#30340;3D&#20154;&#20307;&#21160;&#20316;&#65292;&#20998;&#21035;&#23398;&#20064;&#20498;&#19979;&#21160;&#20316;&#30340;&#19981;&#21516;&#38454;&#27573;&#12290;</title><link>https://arxiv.org/abs/2404.00054</link><description>&lt;p&gt;
&#22312;&#25968;&#23383;&#30011;&#24067;&#19978;&#32534;&#33310;&#65306;&#19968;&#31181;&#33402;&#26415;&#34920;&#29616;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Choreographing the Digital Canvas: A Machine Learning Approach to Artistic Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00054
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#25551;&#36848;&#30340;&#35774;&#35745;&#24037;&#20855;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#21487;&#35270;&#21270;&#33402;&#26415;&#36816;&#21160;&#65292;&#20854;&#20013;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#24490;&#29615;&#23646;&#24615;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21644;&#29983;&#25104;&#36924;&#30495;&#30340;3D&#20154;&#20307;&#21160;&#20316;&#65292;&#20998;&#21035;&#23398;&#20064;&#20498;&#19979;&#21160;&#20316;&#30340;&#19981;&#21516;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#25551;&#36848;&#30340;&#33402;&#26415;&#34920;&#28436;&#35774;&#35745;&#24037;&#20855;&#30340;&#27010;&#24565;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#20498;&#19979;&#21160;&#20316;&#34920;&#28436;&#12290;&#35813;&#24179;&#21488;&#38598;&#25104;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#21644;&#20132;&#20114;&#24335;&#30028;&#38754;&#65292;&#20197;&#29983;&#25104;&#21644;&#21487;&#35270;&#21270;&#33402;&#26415;&#36816;&#21160;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#24490;&#29615;&#23646;&#24615;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;AC-VAE&#65289;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#21160;&#20316;&#25429;&#25417;&#65288;MoCap&#65289;&#25968;&#25454;&#20013;&#25429;&#25417;&#21644;&#29983;&#25104;&#36924;&#30495;&#30340;3D&#20154;&#20307;&#21160;&#20316;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#27880;&#20110;&#20498;&#19979;&#21160;&#20316;&#21160;&#21147;&#23398;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#65292;&#20854;&#29305;&#28857;&#26159;&#23558;&#36816;&#21160;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#38454;&#27573;&#65306;&#20914;&#20987;&#12289;&#25925;&#38556;&#21644;&#19979;&#38477;&#12290;&#35813;ML&#27169;&#22411;&#30340;&#21019;&#26032;&#22312;&#20110;&#20854;&#33021;&#22815;&#20998;&#21035;&#23398;&#20064;&#36825;&#20123;&#38454;&#27573;&#12290;&#36890;&#36807;&#24212;&#29992;&#20840;&#38754;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#21021;&#22987;&#23039;&#21183;&#25439;&#22833;&#20989;&#25968;&#65292;&#26469;&#29983;&#25104;&#33258;&#28982;&#21644;&#21512;&#29702;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00054v1 Announce Type: cross  Abstract: This paper introduces the concept of a design tool for artistic performances based on attribute descriptions. To do so, we used a specific performance of falling actions. The platform integrates a novel machine-learning (ML) model with an interactive interface to generate and visualize artistic movements. Our approach's core is a cyclic Attribute-Conditioned Variational Autoencoder (AC-VAE) model developed to address the challenge of capturing and generating realistic 3D human body motions from motion capture (MoCap) data. We created a unique dataset focused on the dynamics of falling movements, characterized by a new ontology that divides motion into three distinct phases: Impact, Glitch, and Fall. The ML model's innovation lies in its ability to learn these phases separately. It is achieved by applying comprehensive data augmentation techniques and an initial pose loss function to generate natural and plausible motion. Our web-based 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ChapTER&#65292;&#19968;&#20010;&#23545;&#27604;&#21382;&#21490;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#21069;&#32512;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#19982;&#26102;&#38388;&#30340;&#24179;&#34913;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2404.00051</link><description>&lt;p&gt;
Deja vu: &#20351;&#29992;&#21069;&#32512;&#35843;&#25972;&#36827;&#34892;&#23545;&#27604;&#21382;&#21490;&#24314;&#27169;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00051
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ChapTER&#65292;&#19968;&#20010;&#23545;&#27604;&#21382;&#21490;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#21069;&#32512;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#19982;&#26102;&#38388;&#30340;&#24179;&#34913;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;&#65288;TKGR&#65289;&#26159;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#20026;&#19981;&#23436;&#25972;&#30340;TKG&#25512;&#26029;&#32570;&#22833;&#20107;&#23454;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#20256;&#23548;&#21644;&#24402;&#32435;&#35774;&#32622;&#65289;&#65292;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#20943;&#23569;TKG&#20013;&#32467;&#26500;&#36830;&#25509;&#30340;&#20381;&#36182;&#24615;&#65292;&#24050;&#24320;&#21457;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23454;&#20307;&#25551;&#36848;&#20013;&#20016;&#23500;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#21442;&#25968;&#21644;&#19981;&#28789;&#27963;&#24615;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#26114;&#36149;&#19988;&#30446;&#30340;&#24314;&#31435;&#30340;&#35757;&#32451;&#31574;&#30053;&#19978;&#24456;&#38590;&#24179;&#34913;&#25991;&#26412;&#30693;&#35782;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;&#20026;&#20102;&#21457;&#25496;&#25991;&#26412;&#27169;&#22411;&#22312;&#21508;&#31181;&#22797;&#26434;&#22330;&#26223;&#20013;&#29992;&#20110;TKGR&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChapTER&#65292;&#19968;&#20010;&#20855;&#26377;&#21069;&#32512;&#35843;&#25972;&#23545;&#27604;&#21382;&#21490;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00051v1 Announce Type: new  Abstract: Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive settings), which has been gaining increasing attention. Recently, to mitigate dependence on structured connections in TKGs, text-based methods have been developed to utilize rich linguistic information from entity descriptions. However, suffering from the enormous parameters and inflexibility of pre-trained language models, existing text-based methods struggle to balance the textual knowledge and temporal information with computationally expensive purpose-built training strategies. To tap the potential of text-based models for TKGR in various complex scenarios, we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via cont
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#21147;&#23398;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;Grappa&#65292;&#36890;&#36807;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#21644;transformer&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#65292;&#33021;&#22312;&#29616;&#26377;MD&#24341;&#25806;&#20013;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2404.00050</link><description>&lt;p&gt;
Grappa--&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#23376;&#21147;&#23398;&#21183;&#22330;
&lt;/p&gt;
&lt;p&gt;
Grappa -- A Machine Learned Molecular Mechanics Force Field
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00050
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#21147;&#23398;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;Grappa&#65292;&#36890;&#36807;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#21644;transformer&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#65292;&#33021;&#22312;&#29616;&#26377;MD&#24341;&#25806;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#22823;&#20998;&#23376;&#31995;&#32479;&#38656;&#35201;&#26082;&#20934;&#30830;&#21448;&#39640;&#25928;&#30340;&#21147;&#22330;&#12290;&#36817;&#24180;&#26469;&#65292;E(3)&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#21147;&#22330;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24352;&#21147;&#65292;&#20294;&#20854;&#20195;&#20215;&#20173;&#27604;&#20256;&#32479;&#30340;&#20998;&#23376;&#21147;&#23398;&#65288;MM&#65289;&#21147;&#22330;&#26114;&#36149;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#21644;&#20855;&#26377;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#20301;&#32622;&#32534;&#30721;&#30340;transformer&#26469;&#39044;&#27979;&#20998;&#23376;&#22270;&#20013;&#30340;MM&#21442;&#25968;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#21147;&#22330;Grappa&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#24050;&#24314;&#31435;&#30340;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#30340;MM&#21147;&#22330;&#65292;&#24182;&#19988;&#33021;&#20197;&#30456;&#21516;&#30340;&#35745;&#31639;&#25928;&#29575;&#22312;&#29616;&#26377;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#24341;&#25806;&#22914;GROMACS&#21644;OpenMM&#20013;&#20351;&#29992;&#12290;&#23427;&#21487;&#20197;&#39044;&#27979;&#23567;&#20998;&#23376;&#12289;&#32957;&#12289;RNA&#30340;&#33021;&#37327;&#21644;&#21147;--&#23637;&#31034;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21040;&#26410;&#30693;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00050v1 Announce Type: cross  Abstract: Simulating large molecular systems over long timescales requires force fields that are both accurate and efficient. In recent years, E(3) equivariant neural networks have lifted the tension between computational efficiency and accuracy of force fields, but they are still several orders of magnitude more expensive than classical molecular mechanics (MM) force fields.   Here, we propose a novel machine learning architecture to predict MM parameters from the molecular graph, employing a graph attentional neural network and a transformer with symmetry-preserving positional encoding. The resulting force field, Grappa, outperforms established and other machine-learned MM force fields in terms of accuracy at the same computational efficiency and can be used in existing Molecular Dynamics (MD) engines like GROMACS and OpenMM. It predicts energies and forces of small molecules, peptides, RNA and - showcasing its extensibility to uncharted regio
&lt;/p&gt;</description></item><item><title>SLIMBRAIN&#26159;&#19968;&#31181;&#23454;&#26102;&#33719;&#21462;&#21644;&#22788;&#29702;AR&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#20307;&#20869;&#25163;&#26415;&#36807;&#31243;&#20013;&#20174;&#39640;&#20809;&#35889;&#20449;&#24687;&#20013;&#20998;&#31867;&#21644;&#26174;&#31034;&#33041;&#32959;&#30244;&#32452;&#32455;&#65292;&#24182;&#32467;&#21512;RGB&#28857;&#20113;&#36827;&#34892;AR&#21487;&#35270;&#21270;&#12290;</title><link>https://arxiv.org/abs/2404.00048</link><description>&lt;p&gt;
SLIMBRAIN&#65306;&#22686;&#24378;&#29616;&#23454;&#23454;&#26102;&#33719;&#21462;&#19982;&#22788;&#29702;&#31995;&#32479;&#65292;&#29992;&#20110;&#24102;&#26377;&#28145;&#24230;&#20449;&#24687;&#30340;&#39640;&#20809;&#35889;&#20998;&#31867;&#26144;&#23556;&#22312;&#20307;&#20869;&#25163;&#26415;&#31243;&#24207;&#20013;
&lt;/p&gt;
&lt;p&gt;
SLIMBRAIN: Augmented Reality Real-Time Acquisition and Processing System For Hyperspectral Classification Mapping with Depth Information for In-Vivo Surgical Procedures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00048
&lt;/p&gt;
&lt;p&gt;
SLIMBRAIN&#26159;&#19968;&#31181;&#23454;&#26102;&#33719;&#21462;&#21644;&#22788;&#29702;AR&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#20307;&#20869;&#25163;&#26415;&#36807;&#31243;&#20013;&#20174;&#39640;&#20809;&#35889;&#20449;&#24687;&#20013;&#20998;&#31867;&#21644;&#26174;&#31034;&#33041;&#32959;&#30244;&#32452;&#32455;&#65292;&#24182;&#32467;&#21512;RGB&#28857;&#20113;&#36827;&#34892;AR&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#23548;&#33268;&#20102;&#21508;&#31181;&#31038;&#20250;&#21644;&#25216;&#26415;&#24212;&#29992;&#39046;&#22495;&#30340;&#26032;&#30028;&#38754;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#20854;&#20013;&#20043;&#19968;&#39046;&#22495;&#26159;&#21307;&#23398;&#65292;&#23588;&#20854;&#26159;&#22312;&#25163;&#26415;&#26041;&#38754;&#65292;&#36825;&#20123;&#21487;&#35270;&#21270;&#25216;&#26415;&#26377;&#21161;&#20110;&#25552;&#39640;&#26415;&#21069;&#21644;&#26415;&#20013;&#31243;&#24207;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SLIMBRAIN&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#26102;&#33719;&#21462;&#21644;&#22788;&#29702;AR&#31995;&#32479;&#65292;&#36866;&#29992;&#20110;&#20174;&#39640;&#20809;&#35889;&#65288;HS&#65289;&#20449;&#24687;&#20013;&#20998;&#31867;&#21644;&#26174;&#31034;&#33041;&#32959;&#30244;&#32452;&#32455;&#12290;&#35813;&#31995;&#32479;&#22312;&#32959;&#30244;&#20999;&#38500;&#25163;&#26415;&#36807;&#31243;&#20013;&#20197;&#27599;&#31186;14&#24103;&#30340;&#36895;&#24230;&#25429;&#33719;&#24182;&#22788;&#29702;HS&#22270;&#20687;&#65292;&#20197;&#20415;&#22312;&#31070;&#32463;&#22806;&#31185;&#21307;&#29983;&#36827;&#34892;&#25163;&#26415;&#26102;&#21516;&#26102;&#26816;&#27979;&#21644;&#30028;&#23450;&#30284;&#32452;&#32455;&#12290;&#32467;&#26524;&#21576;&#29616;&#22312;AR&#21487;&#35270;&#21270;&#20013;&#65292;&#20998;&#31867;&#32467;&#26524;&#19982;LiDAR&#30456;&#26426;&#25429;&#33719;&#30340;RGB&#28857;&#20113;&#37325;&#21472;&#12290;&#36825;&#31181;&#34920;&#31034;&#20801;&#35768;&#23545;&#22330;&#26223;&#36827;&#34892;&#33258;&#28982;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00048v1 Announce Type: cross  Abstract: Over the last two decades, augmented reality (AR) has led to the rapid development of new interfaces in various fields of social and technological application domains. One such domain is medicine, and to a higher extent surgery, where these visualization techniques help to improve the effectiveness of preoperative and intraoperative procedures. Following this trend, this paper presents SLIMBRAIN, a real-time acquisition and processing AR system suitable to classify and display brain tumor tissue from hyperspectral (HS) information. This system captures and processes HS images at 14 frames per second (FPS) during the course of a tumor resection operation to detect and delimit cancer tissue at the same time the neurosurgeon operates. The result is represented in an AR visualization where the classification results are overlapped with the RGB point cloud captured by a LiDAR camera. This representation allows natural navigation of the scen
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#23545;&#19968;&#33324;&#21644;&#24635; $N$-agent &#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#20135;&#29983;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;NE&#31526;&#21512;&#32447;&#24615;&#39640;&#26031;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#25919;&#31574;&#20248;&#21270;&#31639;&#27861;&#20197;&#21450;&#22686;&#24378;&#25216;&#26415;&#26469;&#25214;&#21040;&#28216;&#25103;&#20869;&#30340;NE&#12290;</title><link>https://arxiv.org/abs/2404.00045</link><description>&lt;p&gt;
&#25919;&#31574;&#20248;&#21270;&#22312;&#27491;&#21017;&#21270;&#24191;&#20041;&#21644;&#24635; LQ &#28216;&#25103;&#20013;&#25214;&#21040;&#32435;&#20160;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization finds Nash Equilibrium in Regularized General-Sum LQ Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00045
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#23545;&#19968;&#33324;&#21644;&#24635; $N$-agent &#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#20135;&#29983;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;NE&#31526;&#21512;&#32447;&#24615;&#39640;&#26031;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#25919;&#31574;&#20248;&#21270;&#31639;&#27861;&#20197;&#21450;&#22686;&#24378;&#25216;&#26415;&#26469;&#25214;&#21040;&#28216;&#25103;&#20869;&#30340;NE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#20837;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#23545;&#19968;&#33324;&#21644;&#24635; $N$-agent &#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913; (NE) &#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#36825;&#31867;&#28216;&#25103;&#30340;NE&#31526;&#21512;&#32447;&#24615;&#39640;&#26031;&#31574;&#30053;&#30340;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#23427;&#25551;&#32472;&#20102;&#22312;&#29109;&#27491;&#21017;&#21270;&#30340;&#36866;&#24403;&#24615;&#26041;&#38754;&#65292;&#23545;&#28216;&#25103;&#20869;NE&#29420;&#29305;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#30001;&#20110;&#25919;&#31574;&#20248;&#21270;&#26159;&#24378;&#21270;&#23398;&#20064; (RL) &#25216;&#26415;&#30340;&#22522;&#30784;&#26041;&#27861;&#65292;&#26088;&#22312;&#25214;&#21040; NE&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#25919;&#31574;&#20248;&#21270;&#31639;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#35813;&#31639;&#27861; (&#22312;&#29109;&#27491;&#21017;&#21270;&#30340;&#36866;&#24403;&#24615;&#19979;) &#33021;&#22815;&#26126;&#26174;&#22320;&#23454;&#29616; NE&#12290;&#27492;&#22806;&#65292;&#22312;&#29109;&#27491;&#21017;&#21270;&#35777;&#26126;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010; $\delta$-&#22686;&#24378;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;&#28216;&#25103;&#20869;&#30340; $\epsilon$-NE&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00045v1 Announce Type: cross  Abstract: In this paper, we investigate the impact of introducing relative entropy regularization on the Nash Equilibria (NE) of General-Sum $N$-agent games, revealing the fact that the NE of such games conform to linear Gaussian policies. Moreover, it delineates sufficient conditions, contingent upon the adequacy of entropy regularization, for the uniqueness of the NE within the game. As Policy Optimization serves as a foundational approach for Reinforcement Learning (RL) techniques aimed at finding the NE, in this work we prove the linear convergence of a policy optimization algorithm which (subject to the adequacy of entropy regularization) is capable of provably attaining the NE. Furthermore, in scenarios where the entropy regularization proves insufficient, we present a $\delta$-augmentation technique, which facilitates the achievement of an $\epsilon$-NE within the game.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UAlign&#65292;&#19968;&#31181;&#26080;&#27169;&#26495;&#21270;&#30340;&#22270;&#21040;&#24207;&#21015;&#30340;&#36870;&#21512;&#25104;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65292;&#21033;&#29992;&#20998;&#23376;&#30340;&#22266;&#26377;&#22270;&#32467;&#26500;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;SMILES&#23545;&#40784;&#25216;&#26415;&#26469;&#20419;&#36827;&#26410;&#25913;&#21464;&#32467;&#26500;&#30340;&#22797;&#29992;&#12290;</title><link>https://arxiv.org/abs/2404.00044</link><description>&lt;p&gt;
UAlign: &#26080;&#27169;&#26495;&#21270;&#30340;&#38750;&#30417;&#30563;&#24335;SMILES&#23545;&#40784;&#25512;&#21160;&#26080;&#27169;&#26495;&#21270;&#36870;&#21512;&#25104;&#39044;&#27979;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
UAlign: Pushing the Limit of Template-free Retrosynthesis Prediction with Unsupervised SMILES Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UAlign&#65292;&#19968;&#31181;&#26080;&#27169;&#26495;&#21270;&#30340;&#22270;&#21040;&#24207;&#21015;&#30340;&#36870;&#21512;&#25104;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65292;&#21033;&#29992;&#20998;&#23376;&#30340;&#22266;&#26377;&#22270;&#32467;&#26500;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;SMILES&#23545;&#40784;&#25216;&#26415;&#26469;&#20419;&#36827;&#26410;&#25913;&#21464;&#32467;&#26500;&#30340;&#22797;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21512;&#25104;&#35268;&#21010;&#22312;&#26377;&#26426;&#21270;&#24037;&#34892;&#19994;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#21046;&#33647;&#39046;&#22495;&#65292;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#12290;&#21333;&#27493;&#36870;&#21512;&#25104;&#39044;&#27979;&#26159;&#35268;&#21010;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#27493;&#65292;&#36817;&#24180;&#26469;&#30001;&#20110;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#65292;&#36825;&#19968;&#27493;&#39588;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#36817;&#24180;&#26469;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19981;&#21516;&#31243;&#24230;&#30340;&#39069;&#22806;&#21270;&#23398;&#30693;&#35782;&#20381;&#36182;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;UAlign&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#21040;&#24207;&#21015;&#30340;&#26080;&#27169;&#26495;&#21270;&#36870;&#21512;&#25104;&#39044;&#27979;&#31649;&#32447;&#12290;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20998;&#23376;&#30340;&#22266;&#26377;&#22270;&#32467;&#26500;&#12290;&#22522;&#20110;&#20998;&#23376;&#32467;&#26500;&#22312;&#21270;&#23398;&#21453;&#24212;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;SMILES&#23545;&#40784;&#25216;&#26415;&#65292;&#20197;&#20419;&#36827;&#26410;&#25913;&#21464;&#32467;&#26500;&#30340;&#22797;&#29992;&#20197;&#29983;&#25104;&#21453;&#24212;&#29289;&#12290;&#22823;&#37327;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00044v1 Announce Type: cross  Abstract: Retrosynthesis planning poses a formidable challenge in the organic chemical industry, particularly in pharmaceuticals. Single-step retrosynthesis prediction, a crucial step in the planning process, has witnessed a surge in interest in recent years due to advancements in AI for science. Various deep learning-based methods have been proposed for this task in recent years, incorporating diverse levels of additional chemical knowledge dependency. This paper introduces UAlign, a template-free graph-to-sequence pipeline for retrosynthesis prediction. By combining graph neural networks and Transformers, our method can more effectively leverage the inherent graph structure of molecules. Based on the fact that the majority of molecule structures remain unchanged during a chemical reaction, we propose a simple yet effective SMILES alignment technique to facilitate the reuse of unchanged structures for reactant generation. Extensive experiments 
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#24320;&#21457;&#31227;&#21160;&#24212;&#29992;&#65292;&#24110;&#21161;&#30450;&#20154;&#36890;&#36807;&#38899;&#39057;&#21644;&#35302;&#35273;&#21453;&#39304;&#23454;&#26102;&#23450;&#20301;&#21608;&#22260;&#29615;&#22659;&#65292;&#20855;&#26377;&#25195;&#25551;&#25991;&#26412;&#21644;&#26391;&#35835;&#12289;&#26816;&#27979;&#29289;&#20307;&#31561;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00043</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25552;&#39640;&#20302;&#35270;&#21147;&#21644;&#30450;&#20154;&#30340;&#21487;&#35775;&#38382;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improve accessibility for Low Vision and Blind people using Machine Learning and Computer Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#24320;&#21457;&#31227;&#21160;&#24212;&#29992;&#65292;&#24110;&#21161;&#30450;&#20154;&#36890;&#36807;&#38899;&#39057;&#21644;&#35302;&#35273;&#21453;&#39304;&#23454;&#26102;&#23450;&#20301;&#21608;&#22260;&#29615;&#22659;&#65292;&#20855;&#26377;&#25195;&#25551;&#25991;&#26412;&#21644;&#26391;&#35835;&#12289;&#26816;&#27979;&#29289;&#20307;&#31561;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#31227;&#21160;&#25216;&#26415;&#30340;&#19981;&#26029;&#25193;&#23637;&#65292;&#27531;&#38556;&#20154;&#22763;&#24613;&#38656;&#24471;&#21040;&#30456;&#24212;&#30340;&#24110;&#21161;&#12290;&#35813;&#39033;&#30446;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26469;&#25552;&#39640;&#35270;&#35273;&#38556;&#30861;&#20154;&#22763;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#36807;&#21435;&#30340;&#23581;&#35797;&#24320;&#21457;&#20102;&#35768;&#22810;&#36719;&#20214;&#65292;&#26088;&#22312;&#25913;&#21892;&#30450;&#20154;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#28982;&#32780;&#65292;&#24066;&#22330;&#19978;&#30340;&#24212;&#29992;&#20934;&#30830;&#24230;&#36739;&#20302;&#65292;&#24182;&#19988;&#20165;&#25552;&#20379;&#38899;&#39057;&#21453;&#39304;&#12290;&#35813;&#39033;&#30446;&#23558;&#19987;&#27880;&#20110;&#24314;&#31435;&#19968;&#20010;&#31227;&#21160;&#24212;&#29992;&#65292;&#36890;&#36807;&#23454;&#26102;&#25509;&#25910;&#29992;&#25143;&#21608;&#22260;&#30340;&#38899;&#39057;&#21644;&#35302;&#35273;&#21453;&#39304;&#65288;&#22914;&#25391;&#21160;&#65289;&#65292;&#26469;&#24110;&#21161;&#30450;&#20154;&#22312;&#31354;&#38388;&#20013;&#23450;&#20301;&#12290;&#31227;&#21160;&#24212;&#29992;&#23558;&#20855;&#26377;&#19977;&#20010;&#20027;&#35201;&#21151;&#33021;&#12290;&#39318;&#35201;&#21151;&#33021;&#26159;&#20174;&#30456;&#26426;&#25195;&#25551;&#25991;&#26412;&#24182;&#26391;&#35835;&#32473;&#29992;&#25143;&#21548;&#12290;&#36825;&#19968;&#21151;&#33021;&#21487;&#29992;&#20110;&#32440;&#24352;&#19978;&#30340;&#25991;&#26412;&#12289;&#29615;&#22659;&#20013;&#20197;&#21450;&#36947;&#36335;&#26631;&#35782;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00043v1 Announce Type: cross  Abstract: With the ever-growing expansion of mobile technology worldwide, there is an increasing need for accommodation for those who are disabled. This project explores how machine learning and computer vision could be utilized to improve accessibility for people with visual impairments. There have been many attempts to develop various software that would improve accessibility in the day-to-day lives of blind people. However, applications on the market have low accuracy and only provide audio feedback. This project will concentrate on building a mobile application that helps blind people to orient in space by receiving audio and haptic feedback, e.g. vibrations, about their surroundings in real-time. The mobile application will have 3 main features. The initial feature is scanning text from the camera and reading it to a user. This feature can be used on paper with text, in the environment, and on road signs. The second feature is detecting obj
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20984;&#32422;&#26463;&#30340;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#28176;&#36817;&#20445;&#35777;&#30340;VRPG&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#21463;&#21040;&#35299;&#20197;&#21450;&#24102;&#20984;&#32422;&#26463;&#35299;&#20915;&#30340;&#38382;&#39064;&#30340;&#32553;&#25918;&#36317;&#31163;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.00042</link><description>&lt;p&gt;
&#20855;&#26377;&#32422;&#26463;&#30340;&#38543;&#26426;&#20248;&#21270;&#65306;&#38750;&#28176;&#36817;&#23454;&#20363;&#30456;&#20851;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Stochastic Optimization with Constraints: A Non-asymptotic Instance-Dependent Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20984;&#32422;&#26463;&#30340;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#28176;&#36817;&#20445;&#35777;&#30340;VRPG&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#21463;&#21040;&#35299;&#20197;&#21450;&#24102;&#20984;&#32422;&#26463;&#35299;&#20915;&#30340;&#38382;&#39064;&#30340;&#32553;&#25918;&#36317;&#31163;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#38543;&#26426;&#20984;&#20248;&#21270;&#22312;&#20984;&#32422;&#26463;&#19979;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36825;&#20010;&#38382;&#39064;&#30340;&#33258;&#28982;&#26041;&#24046;&#20943;&#23569;&#30340;&#36817;&#31471;&#26799;&#24230;&#65288;VRPG&#65289;&#31639;&#27861;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;VRPG&#31639;&#27861;&#30340;&#38750;&#28176;&#36817;&#20445;&#35777;&#12290;&#19982;&#26497;&#23567;&#20540;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#22522;&#20110;&#23454;&#20363;&#30340;&#12290;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#20445;&#35777;&#25429;&#25417;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#65292;&#22122;&#22768;&#30340;&#21464;&#24322;&#24615;&#21644;&#32422;&#26463;&#38598;&#30340;&#20960;&#20309;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;VRPG&#31639;&#27861;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#21463;&#32473;&#23450;&#38382;&#39064;&#30340;&#35299;&#21644;&#32473;&#23450;&#20984;&#32422;&#26463;&#19979;&#35299;&#20915;&#30340;&#29305;&#23450;&#23567;&#25200;&#21160;&#38382;&#39064;&#30340;&#35299;&#20043;&#38388;&#30340;&#32553;&#25918;&#36317;&#31163;&#65288;&#30001;$\sqrt{N}$&#32553;&#25918;&#65289;&#30340;&#25511;&#21046;&#65292;&#36825;&#37324;&#65292;$N$&#34920;&#31034;&#26679;&#26412;&#25968;&#12290;&#21033;&#29992;&#23616;&#37096;&#26497;&#23567;&#20540;&#19979;&#30028;&#21644;&#25200;&#21160;&#38382;&#39064;&#35299;&#20043;&#38388;&#30340;&#19968;&#31181;&#25104;&#29087;&#32852;&#31995;&#65292;&#25105;&#20204;&#34920;&#26126;&#24403;$N \rightarrow +\infty$&#26102;&#65292;&#26497;&#23567;&#20540;&#23384;&#22312;&#24182;&#19988;&#21463;&#25351;&#23450;&#20984;&#32422;&#26463;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00042v1 Announce Type: cross  Abstract: We consider the problem of stochastic convex optimization under convex constraints. We analyze the behavior of a natural variance reduced proximal gradient (VRPG) algorithm for this problem. Our main result is a non-asymptotic guarantee for VRPG algorithm. Contrary to minimax worst case guarantees, our result is instance-dependent in nature. This means that our guarantee captures the complexity of the loss function, the variability of the noise, and the geometry of the constraint set. We show that the non-asymptotic performance of the VRPG algorithm is governed by the scaled distance (scaled by $\sqrt{N}$) between the solutions of the given problem and that of a certain small perturbation of the given problem -- both solved under the given convex constraints; here, $N$ denotes the number of samples. Leveraging a well-established connection between local minimax lower bounds and solutions to perturbed problems, we show that as $N \right
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MicroHD&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;TinyML&#31995;&#32479;&#30340;&#31934;&#24230;&#39537;&#21160;&#36229;&#39640;&#32500;&#35745;&#31639;&#20248;&#21270;&#26041;&#27861;</title><link>https://arxiv.org/abs/2404.00039</link><description>&lt;p&gt;
MicroHD&#65306;&#38754;&#21521;TinyML&#31995;&#32479;&#30340;&#39640;&#32500;&#36229;&#35745;&#31639;&#31639;&#27861;&#30340;&#31934;&#24230;&#39537;&#21160;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
MicroHD: An Accuracy-Driven Optimization of Hyperdimensional Computing Algorithms for TinyML systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00039
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MicroHD&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;TinyML&#31995;&#32479;&#30340;&#31934;&#24230;&#39537;&#21160;&#36229;&#39640;&#32500;&#35745;&#31639;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#20316;&#20026;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#23450;&#20301;&#21040;TinyML&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#27491;&#22312;&#20852;&#36215;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#20854;&#36731;&#37327;&#32423;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#22312;HDC&#30340;&#20808;&#21069;&#30740;&#31350;&#20013;&#65292;&#35777;&#26126;&#23558;&#26631;&#20934;&#30340;10k&#32500;&#36229;&#32500;&#31354;&#38388;&#38480;&#21046;&#22312;&#26356;&#20302;&#30340;&#20540;&#26159;&#21487;&#34892;&#30340;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;HDC&#30340;&#36164;&#28304;&#38656;&#27714;&#12290;&#31867;&#20284;&#22320;&#65292;&#20854;&#20182;&#30740;&#31350;&#34920;&#26126;&#20108;&#36827;&#21046;&#20540;&#21487;&#29992;&#20316;&#29983;&#25104;&#30340;&#36229;&#30690;&#37327;&#30340;&#20803;&#32032;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#65292;&#20195;&#20215;&#26159;&#26576;&#31181;&#31243;&#24230;&#30340;&#31934;&#24230;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20248;&#21270;&#23581;&#35797;&#27809;&#26377;&#21516;&#26102;&#21327;&#21516;&#20248;&#21270;HDC&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#19988;&#31934;&#24230;&#38477;&#20302;&#19981;&#33021;&#30452;&#25509;&#25511;&#21046;&#65292;&#23548;&#33268;HDC&#27169;&#22411;&#19981;&#22815;&#20248;&#21270;&#65292;&#25552;&#20379;&#20102;&#20960;&#20010;&#36136;&#37327;&#19981;&#21487;&#25509;&#21463;&#30340;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;MicroHD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31934;&#24230;&#39537;&#21160;HDC&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#35843;&#25972;HDC&#30340;&#36229;&#21442;&#25968;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00039v1 Announce Type: cross  Abstract: Hyperdimensional computing (HDC) is emerging as a promising AI approach that can effectively target TinyML applications thanks to its lightweight computing and memory requirements. Previous works on HDC showed that limiting the standard 10k dimensions of the hyperdimensional space to much lower values is possible, reducing even more HDC resource requirements. Similarly, other studies demonstrated that binary values can be used as elements of the generated hypervectors, leading to significant efficiency gains at the cost of some degree of accuracy degradation. Nevertheless, current optimization attempts do not concurrently co-optimize HDC hyper-parameters, and accuracy degradation is not directly controlled, resulting in sub-optimal HDC models providing several applications with unacceptable output qualities. In this work, we propose MicroHD, a novel accuracy-driven HDC optimization approach that iteratively tunes HDC hyper-parameters, 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#30740;&#31350;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#21327;&#35758;&#25552;&#20379;&#30340;&#26381;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25104;&#21151;&#23558;&#36825;&#20123;&#26381;&#21153;&#20998;&#32452;&#20026;&#20855;&#26377;&#30456;&#20284;&#21151;&#33021;&#30340;&#38598;&#32676;&#12290;</title><link>https://arxiv.org/abs/2404.00034</link><description>&lt;p&gt;
&#25506;&#31350;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#65288;DeFi&#65289;&#26381;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating Similarities Across Decentralized Financial (DeFi) Services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#30740;&#31350;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#21327;&#35758;&#25552;&#20379;&#30340;&#26381;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25104;&#21151;&#23558;&#36825;&#20123;&#26381;&#21153;&#20998;&#32452;&#20026;&#20855;&#26377;&#30456;&#20284;&#21151;&#33021;&#30340;&#38598;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#37319;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#31639;&#27861;&#26469;&#30740;&#31350;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#65288;DeFi&#65289;&#21327;&#35758;&#25552;&#20379;&#30340;&#26381;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20197;&#22826;&#22346;&#20132;&#26131;&#25968;&#25454;&#26469;&#35782;&#21035;DeFi&#26500;&#24314;&#27169;&#22359;&#65292;&#36825;&#20123;&#26159;&#21327;&#35758;&#29305;&#23450;&#30340;&#26234;&#33021;&#21512;&#32422;&#38598;&#65292;&#23427;&#20204;&#22312;&#21333;&#20010;&#20132;&#26131;&#20013;&#20197;&#32452;&#21512;&#26041;&#24335;&#20351;&#29992;&#65292;&#24182;&#23553;&#35013;&#20102;&#25191;&#34892;&#29305;&#23450;&#37329;&#34701;&#26381;&#21153;&#65288;&#22914;&#21152;&#23494;&#36164;&#20135;&#20132;&#25442;&#25110;&#20511;&#36151;&#65289;&#30340;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#21512;&#32422;&#23646;&#24615;&#21644;&#26234;&#33021;&#21512;&#32422;&#35843;&#29992;&#30340;&#22270;&#32467;&#26500;&#23558;&#36825;&#20123;&#27169;&#22359;&#20998;&#31867;&#36827;&#38598;&#32676;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;GRL&#20174;&#26500;&#24314;&#27169;&#22359;&#21019;&#24314;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#20957;&#32858;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#32858;&#31867;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#26159;&#21542;&#26377;&#25928;&#22320;&#20998;&#32452;&#20026;&#20855;&#26377;&#30456;&#20284;&#21151;&#33021;&#30340;&#38598;&#32676;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#20843;&#20010;&#37329;&#34701;&#21151;&#33021;&#31867;&#21035;&#20851;&#32852;&#65292;&#24182;&#23558;&#27492;&#20449;&#24687;&#29992;&#20316;&#30446;&#26631;l
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00034v1 Announce Type: cross  Abstract: We explore the adoption of graph representation learning (GRL) algorithms to investigate similarities across services offered by Decentralized Finance (DeFi) protocols. Following existing literature, we use Ethereum transaction data to identify the DeFi building blocks. These are sets of protocol-specific smart contracts that are utilized in combination within single transactions and encapsulate the logic to conduct specific financial services such as swapping or lending cryptoassets. We propose a method to categorize these blocks into clusters based on their smart contract attributes and the graph structure of their smart contract calls. We employ GRL to create embedding vectors from building blocks and agglomerative models for clustering them. To evaluate whether they are effectively grouped in clusters of similar functionalities, we associate them with eight financial functionality categories and use this information as the target l
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35797;&#39564;&#24615;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#26397;&#21521;&#26080;&#38656;&#20957;&#35270;&#30340;&#33041;&#26426;&#25509;&#21475;&#25340;&#20889;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#27880;&#24847;&#21147;&#32780;&#38750;&#30524;&#29699;&#31227;&#21160;&#26469;&#35299;&#30721;&#35270;&#35273;&#21050;&#28608;&#65292;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.00031</link><description>&lt;p&gt;
&#26397;&#21521;&#26080;&#38656;&#20957;&#35270;&#30340;c-VEP&#33041;&#26426;&#25509;&#21475;&#65306;&#19968;&#39033;&#35797;&#39564;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards gaze-independent c-VEP BCI: A pilot study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00031
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35797;&#39564;&#24615;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#26397;&#21521;&#26080;&#38656;&#20957;&#35270;&#30340;&#33041;&#26426;&#25509;&#21475;&#25340;&#20889;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#27880;&#24847;&#21147;&#32780;&#38750;&#30524;&#29699;&#31227;&#21160;&#26469;&#35299;&#30721;&#35270;&#35273;&#21050;&#28608;&#65292;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#25340;&#20889;&#22120;&#30340;&#19968;&#20010;&#23616;&#38480;&#24615;&#22312;&#20110;&#23427;&#20204;&#35201;&#27714;&#29992;&#25143;&#33021;&#22815;&#31227;&#21160;&#30524;&#30555;&#27880;&#35270;&#30446;&#26631;&#12290;&#23545;&#20110;&#26080;&#27861;&#33258;&#24895;&#25511;&#21046;&#30524;&#30555;&#31227;&#21160;&#30340;&#29992;&#25143;&#65288;&#20363;&#22914;&#24739;&#26377;&#26202;&#26399;&#32908;&#33806;&#32553;&#20391;&#32034;&#30828;&#21270;&#30151;&#65288;ALS&#65289;&#30340;&#20154;&#32676;&#65289;&#65292;&#36825;&#20250;&#36896;&#25104;&#38382;&#39064;&#12290;&#36825;&#39033;&#35797;&#39564;&#24615;&#30740;&#31350;&#39318;&#27425;&#36808;&#20986;&#20102;&#26397;&#21521;&#22522;&#20110;&#20195;&#30721;&#35843;&#21046;&#35270;&#35273;&#35825;&#21457;&#30005;&#20301;&#65288;c-VEP&#65289;&#30340;&#26080;&#38656;&#20957;&#35270;&#30340;&#25340;&#20889;&#22120;&#30340;&#31532;&#19968;&#27493;&#12290;&#21442;&#19982;&#32773;&#34987;&#21576;&#29616;&#20004;&#20010;&#21452;&#20391;&#20301;&#32622;&#30340;&#21050;&#28608;&#65292;&#20854;&#20013;&#19968;&#20010;&#22312;&#38378;&#28865;&#65292;&#24182;&#34987;&#35201;&#27714;&#19987;&#27880;&#20110;&#36825;&#20123;&#21050;&#28608;&#20013;&#30340;&#19968;&#20010;&#65292;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#30475;&#30528;&#21050;&#28608;&#65288;&#26126;&#26174;&#26465;&#20214;&#65289;&#25110;&#20351;&#29992;&#31354;&#38388;&#27880;&#24847;&#21147;&#26469;&#23436;&#25104;&#65292;&#28040;&#38500;&#20102;&#30524;&#29699;&#31227;&#21160;&#30340;&#38656;&#35201;&#65288;&#38544;&#34109;&#26465;&#20214;&#65289;&#12290;&#34987;&#19987;&#27880;&#30340;&#21050;&#28608;&#20174;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20013;&#35299;&#30721;&#65292;&#38544;&#34109;&#21644;&#26126;&#26174;&#26465;&#20214;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;88%&#21644;100%&#12290;&#36825;&#20123;&#22522;&#30784;&#24615;&#35265;&#35299;&#23637;&#31034;&#20102;&#36825;&#19968;&#25216;&#26415;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00031v1 Announce Type: cross  Abstract: A limitation of brain-computer interface (BCI) spellers is that they require the user to be able to move the eyes to fixate on targets. This poses an issue for users who cannot voluntarily control their eye movements, for instance, people living with late-stage amyotrophic lateral sclerosis (ALS). This pilot study makes the first step towards a gaze-independent speller based on the code-modulated visual evoked potential (c-VEP). Participants were presented with two bi-laterally located stimuli, one of which was flashing, and were tasked to attend to one of these stimuli either by directly looking at the stimuli (overt condition) or by using spatial attention, eliminating the need for eye movement (covert condition). The attended stimuli were decoded from electroencephalography (EEG) and classification accuracies of 88% and 100% were obtained for the covert and overt conditions, respectively. These fundamental insights show the promisin
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26495;&#29699;&#30701;&#25991;&#26412;&#35780;&#35770;&#25968;&#25454;&#36827;&#34892;&#21487;&#35270;&#21270;&#65292;&#21253;&#25324;&#26500;&#24314;&#29699;&#21592;&#30340;&#23454;&#21147;&#35268;&#21017;&#21644;&#24369;&#28857;&#35268;&#21017;&#65292;&#24182;&#23637;&#31034;&#20855;&#26377;&#31867;&#20284;&#35268;&#21017;&#30340;&#29699;&#21592;&#12290;</title><link>https://arxiv.org/abs/2404.00030</link><description>&lt;p&gt;
&#38750;&#32467;&#26500;&#21270;&#20307;&#32946;&#25968;&#25454;&#30340;&#21487;&#35270;&#21270; - &#20197;&#26495;&#29699;&#30701;&#25991;&#26412;&#35780;&#35770;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Visualization of Unstructured Sports Data -- An Example of Cricket Short Text Commentary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00030
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26495;&#29699;&#30701;&#25991;&#26412;&#35780;&#35770;&#25968;&#25454;&#36827;&#34892;&#21487;&#35270;&#21270;&#65292;&#21253;&#25324;&#26500;&#24314;&#29699;&#21592;&#30340;&#23454;&#21147;&#35268;&#21017;&#21644;&#24369;&#28857;&#35268;&#21017;&#65292;&#24182;&#23637;&#31034;&#20855;&#26377;&#31867;&#20284;&#35268;&#21017;&#30340;&#29699;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00030v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#20307;&#32946;&#21487;&#35270;&#21270;&#20851;&#27880;&#20110;&#20351;&#29992;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#22914;&#27604;&#36187;&#25968;&#25454;&#21644;&#36319;&#36394;&#25968;&#25454;&#12290;&#19982;&#20307;&#32946;&#30456;&#20851;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#26469;&#28304;&#21253;&#25324;&#21338;&#23458;&#12289;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21644;&#22312;&#32447;&#26032;&#38395;&#25991;&#31456;&#31561;&#12290;&#20307;&#32946;&#21487;&#35270;&#21270;&#26041;&#27861;&#35201;&#20040;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#26469;&#28304;&#20013;&#23384;&#22312;&#30340;&#20449;&#24687;&#65292;&#35201;&#20040;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#26469;&#28304;&#25552;&#20986;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#27809;&#26377;&#22686;&#24378;&#20307;&#32946;&#21487;&#35270;&#21270;&#26041;&#27861;&#30340;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#21363;&#26495;&#29699;&#30701;&#25991;&#26412;&#35780;&#35770;&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#30701;&#25991;&#26412;&#35780;&#35770;&#25968;&#25454;&#29992;&#20110;&#26500;&#24314;&#20010;&#20154;&#29699;&#21592;&#30340;&#23454;&#21147;&#35268;&#21017;&#21644;&#24369;&#28857;&#35268;&#21017;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#19978;&#21487;&#34892;&#30340;&#29699;&#21592;&#23454;&#21147;&#35268;&#21017;&#21644;&#24369;&#28857;&#35268;&#21017;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26500;&#24314;&#35268;&#21017;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35745;&#31639;&#24182;&#21487;&#35270;&#21270;&#20855;&#26377;&#31867;&#20284;&#23454;&#21147;&#35268;&#21017;&#25110;&#24369;&#28857;&#35268;&#21017;&#30340;&#29699;&#21592;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#21487;&#35270;&#21270;&#36825;&#20123;&#35268;&#21017;&#21644;&#29699;&#21592;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00030v1 Announce Type: cross  Abstract: Sports visualization focuses on the use of structured data, such as box-score data and tracking data. Unstructured data sources pertaining to sports are available in various places such as blogs, social media posts, and online news articles. Sports visualization methods either not fully exploited the information present in these sources or the proposed visualizations through the use of these sources did not augment to the body of sports visualization methods. We propose the use of unstructured data, namely cricket short text commentary for visualization. The short text commentary data is used for constructing individual player's strength rules and weakness rules. A computationally feasible definition for player's strength rule and weakness rule is proposed. A visualization method for the constructed rules is presented. In addition, players having similar strength rules or weakness rules is computed and visualized. We demonstrate the us
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;</title><link>https://arxiv.org/abs/2404.00027</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#65306;&#25506;&#35752;&#25152;&#26377;&#26435;&#24863;&#21644;&#25512;&#29702;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00027
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#24605;&#24819;&#12289;&#26102;&#38388;&#21644;&#36129;&#29486;&#30340;&#25237;&#20837;&#65292;&#23548;&#33268;&#23545;&#20135;&#20986;&#29289;&#30340;&#20381;&#24651;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20889;&#20316;&#21161;&#25163;&#24341;&#20837;&#20102;&#19968;&#31181;&#24515;&#29702;&#22256;&#22659;&#65292;&#22240;&#20026;&#19968;&#20123;&#20869;&#23481;&#24182;&#38750;&#30452;&#25509;&#25105;&#20204;&#30340;&#21019;&#20316;&#12290;&#25105;&#20204;&#24448;&#24448;&#26356;&#20542;&#21521;&#20110;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#20013;&#26356;&#22810;&#22320;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23613;&#31649;&#23427;&#20204;&#23545;&#25152;&#26377;&#20219;&#21153;&#37117;&#26159;&#24179;&#31561;&#30340;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25105;&#20204;&#21487;&#33021;&#19981;&#20250;&#23436;&#20840;&#22768;&#31216;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#25317;&#26377;&#25152;&#26377;&#26435;&#65292;&#20294;&#21364;&#33258;&#30001;&#22320;&#22768;&#31216;&#20316;&#32773;&#36523;&#20221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31616;&#30701;&#35843;&#26597;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20102;&#35299;&#28508;&#22312;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20154;&#26426;&#20132;&#20114;&#22312;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#24182;&#25913;&#36827;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#29616;&#26377;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#35299;&#37322;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26410;&#26469;&#30740;&#31350;&#36335;&#32447;&#22270;&#65292;&#37325;&#28857;&#25918;&#22312;&#20102;&#20102;&#35299;&#20132;&#27969;&#23545;&#35937;&#21644;&#29983;&#25104;&#21450;&#26102;&#35299;&#37322;&#19978;&#12290;</title><link>https://arxiv.org/abs/2404.00019</link><description>&lt;p&gt;
&#25512;&#36827;&#21487;&#35299;&#37322;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#31995;&#32479;&#65306;&#32508;&#36848;&#19982;&#30740;&#31350;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Advancing Explainable Autonomous Vehicle Systems: A Comprehensive Review and Research Roadmap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#29616;&#26377;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#35299;&#37322;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26410;&#26469;&#30740;&#31350;&#36335;&#32447;&#22270;&#65292;&#37325;&#28857;&#25918;&#22312;&#20102;&#20102;&#35299;&#20132;&#27969;&#23545;&#35937;&#21644;&#29983;&#25104;&#21450;&#26102;&#35299;&#37322;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29616;&#26377;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#35299;&#37322;&#26041;&#27861;&#22914;&#20309;&#28385;&#36275;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#22810;&#26679;&#38656;&#27714;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#65292;&#24517;&#39035;&#36827;&#34892;&#28145;&#20837;&#35843;&#26597;&#20197;&#30830;&#23450;&#38656;&#35201;&#35299;&#37322;&#30340;&#24773;&#22659;&#21644;&#36866;&#24403;&#30340;&#20114;&#21160;&#31574;&#30053;&#12290;&#19968;&#39033;&#20840;&#38754;&#30340;&#32508;&#36848;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#35780;&#20272;&#24403;&#21069;&#26041;&#27861;&#19982;AV&#29983;&#24577;&#31995;&#32479;&#20869;&#19981;&#21516;&#21033;&#30410;&#21644;&#26399;&#26395;&#30340;&#19968;&#33268;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#39033;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#29983;&#25104;&#21644;&#21576;&#29616;&#35299;&#37322;&#25152;&#28041;&#21450;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#20419;&#36827;&#24320;&#21457;&#26356;&#21152;&#26377;&#25928;&#21644;&#21253;&#23481;&#30340;&#21487;&#35299;&#37322;AV&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#23558;&#29616;&#26377;&#25991;&#29486;&#20998;&#31867;&#20026;&#19977;&#20010;&#20027;&#35201;&#20027;&#39064;&#65306;&#35299;&#37322;&#20219;&#21153;&#12289;&#35299;&#37322;&#20449;&#24687;&#21644;&#35299;&#37322;&#20449;&#24687;&#20256;&#36798;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26410;&#26469;&#30740;&#31350;&#30340;&#20840;&#38754;&#36335;&#32447;&#22270;&#65292;&#38598;&#20013;&#22312;&#65288;i&#65289;&#20102;&#35299;&#20132;&#27969;&#23545;&#35937;&#65292;&#65288;ii&#65289;&#29983;&#25104;&#21450;&#26102;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00019v1 Announce Type: cross  Abstract: Given the uncertainty surrounding how existing explainability methods for autonomous vehicles (AVs) meet the diverse needs of stakeholders, a thorough investigation is imperative to determine the contexts requiring explanations and suitable interaction strategies. A comprehensive review becomes crucial to assess the alignment of current approaches with the varied interests and expectations within the AV ecosystem. This study presents a review to discuss the complexities associated with explanation generation and presentation to facilitate the development of more effective and inclusive explainable AV systems. Our investigation led to categorising existing literature into three primary topics: explanatory tasks, explanatory information, and explanatory information communication. Drawing upon our insights, we have proposed a comprehensive roadmap for future research centred on (i) knowing the interlocutor, (ii) generating timely explanat
&lt;/p&gt;</description></item><item><title>SOMson&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#38899;&#39057;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#22686;&#24378;Kohonen&#22320;&#22270;&#19979;&#25968;&#25454;&#30340;&#20449;&#24687;&#37327;&#65292;&#35299;&#20915;SOM&#22312;&#25552;&#20379;&#25972;&#20307;&#22270;&#29255;&#26102;&#30340;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2404.00016</link><description>&lt;p&gt;
SOMson -- &#22312;Kohonen&#22320;&#22270;&#20013;&#23545;&#22810;&#32500;&#25968;&#25454;&#36827;&#34892;&#38899;&#39057;&#21270;
&lt;/p&gt;
&lt;p&gt;
SOMson -- Sonification of Multidimensional Data in Kohonen Maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00016
&lt;/p&gt;
&lt;p&gt;
SOMson&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#38899;&#39057;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#22686;&#24378;Kohonen&#22320;&#22270;&#19979;&#25968;&#25454;&#30340;&#20449;&#24687;&#37327;&#65292;&#35299;&#20915;SOM&#22312;&#25552;&#20379;&#25972;&#20307;&#22270;&#29255;&#26102;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kohonen Maps&#65292;&#21448;&#31216;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;SOMs&#65289;&#65292;&#26159;&#19968;&#31181;&#21487;&#20197;&#23558;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#21487;&#35270;&#21270;&#21040;&#20302;&#32500;&#22320;&#22270;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#34429;&#28982;SOMs&#26159;&#25968;&#25454;&#23457;&#26597;&#21644;&#25506;&#32034;&#30340;&#32477;&#20339;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#22266;&#26377;&#22320;&#20250;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#12290;&#22320;&#22270;&#19979;&#30340;&#25968;&#25454;&#21487;&#35270;&#21270;&#24182;&#19981;&#23436;&#20840;&#25972;&#21512;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#20379;&#20840;&#23616;&#22270;&#29255;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;SOMson&#65292;&#19968;&#31181;&#23545;&#25968;&#25454;&#36827;&#34892;&#20132;&#20114;&#38899;&#39057;&#21270;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#38899;&#39057;&#21270;&#22686;&#21152;&#20102;SOM&#21516;&#26102;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#27809;&#26377;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#65292;&#32780;&#26159;&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#22312;&#32447;&#31034;&#20363;&#65292;&#35753;&#35835;&#32773;&#21487;&#20197;&#33258;&#34892;&#25506;&#32034;SOMson&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20854;&#20248;&#21183;&#12289;&#21155;&#21183;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00016v1 Announce Type: cross  Abstract: Kohonen Maps, aka. Self-organizing maps (SOMs) are neural networks that visualize a high-dimensional feature space on a low-dimensional map. While SOMs are an excellent tool for data examination and exploration, they inherently cause a loss of detail. Visualizations of the underlying data do not integrate well and, therefore, fail to provide an overall picture. Consequently, we suggest SOMson, an interactive sonification of the underlying data, as a data augmentation technique. The sonification increases the amount of information provided simultaneously by the SOM. Instead of a user study, we present an interactive online example, so readers can explore SOMson themselves. Its strengths, weaknesses, and prospects are discussed.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Systemic Quantum Score (SQS)&#30340;&#26032;&#26041;&#27861;&#65292;&#23637;&#31034;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#20135;&#32423;&#24212;&#29992;&#26696;&#20363;&#20013;&#30456;&#27604;&#32431;&#32463;&#20856;&#27169;&#22411;&#26356;&#26377;&#20248;&#21183;&#65292;&#33021;&#22815;&#20174;&#36739;&#23569;&#25968;&#25454;&#28857;&#20013;&#25552;&#21462;&#27169;&#24335;&#24182;&#34920;&#29616;&#20986;&#26356;&#22909;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00015</link><description>&lt;p&gt;
&#21033;&#29992;&#37327;&#23376;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#36171;&#33021;&#20449;&#29992;&#35780;&#20998;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00015
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Systemic Quantum Score (SQS)&#30340;&#26032;&#26041;&#27861;&#65292;&#23637;&#31034;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#20135;&#32423;&#24212;&#29992;&#26696;&#20363;&#20013;&#30456;&#27604;&#32431;&#32463;&#20856;&#27169;&#22411;&#26356;&#26377;&#20248;&#21183;&#65292;&#33021;&#22815;&#20174;&#36739;&#23569;&#25968;&#25454;&#28857;&#20013;&#25552;&#21462;&#27169;&#24335;&#24182;&#34920;&#29616;&#20986;&#26356;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Quantum Kernels&#34987;&#35748;&#20026;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26089;&#26399;&#38454;&#27573;&#25552;&#20379;&#20102;&#26377;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#21033;&#29992;&#24222;&#22823;&#25968;&#25454;&#38598;&#26102;&#65292;&#39640;&#24230;&#22797;&#26434;&#30340;&#32463;&#20856;&#27169;&#22411;&#24456;&#38590;&#36229;&#36234;&#65292;&#29305;&#21035;&#26159;&#22312;&#29702;&#35299;&#21147;&#26041;&#38754;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#19968;&#26086;&#25968;&#25454;&#31232;&#32570;&#19988;&#20542;&#26012;&#65292;&#32463;&#20856;&#27169;&#22411;&#23601;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#37327;&#23376;&#29305;&#24449;&#31354;&#38388;&#34987;&#39044;&#35745;&#22312;&#36825;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#26223;&#20013;&#33021;&#22815;&#25214;&#21040;&#26356;&#22909;&#30340;&#25968;&#25454;&#29305;&#24449;&#21644;&#30446;&#26631;&#31867;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Systemic Quantum Score (SQS)&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#32467;&#26524;&#65292;&#34920;&#26126;&#22312;&#37329;&#34701;&#34892;&#19994;&#29983;&#20135;&#32423;&#24212;&#29992;&#26696;&#20363;&#20013;&#65292;SQS&#21487;&#33021;&#27604;&#32431;&#32463;&#20856;&#27169;&#22411;&#20855;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#20855;&#20307;&#30740;&#31350;&#34920;&#26126;&#65292;SQS&#33021;&#22815;&#20174;&#36739;&#23569;&#30340;&#25968;&#25454;&#28857;&#20013;&#25552;&#21462;&#20986;&#27169;&#24335;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#38656;&#27714;&#37327;&#22823;&#30340;&#31639;&#27861;&#65288;&#22914;XGBoost&#65289;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24102;&#26469;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00015v1 Announce Type: cross  Abstract: Quantum Kernels are projected to provide early-stage usefulness for quantum machine learning. However, highly sophisticated classical models are hard to surpass without losing interpretability, particularly when vast datasets can be exploited. Nonetheless, classical models struggle once data is scarce and skewed. Quantum feature spaces are projected to find better links between data features and the target class to be predicted even in such challenging scenarios and most importantly, enhanced generalization capabilities. In this work, we propose a novel approach called Systemic Quantum Score (SQS) and provide preliminary results indicating potential advantage over purely classical models in a production grade use case for the Finance sector. SQS shows in our specific study an increased capacity to extract patterns out of fewer data points as well as improved performance over data-hungry algorithms such as XGBoost, providing advantage i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#31934;&#30830;&#35821;&#20041;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31890;&#31354;&#38388;&#20013;&#21033;&#29992;&#29305;&#24449;&#35821;&#20041;&#21644;&#21487;&#38752;&#35266;&#27979;&#26469;&#39044;&#27979;&#32570;&#22833;&#20540;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#30772;&#20135;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.00013</link><description>&lt;p&gt;
&#20855;&#26377;&#31934;&#30830;&#35821;&#20041;&#21644;AI&#39537;&#21160;&#27969;&#31243;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#21450;&#30772;&#20135;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Missing Data Imputation With Granular Semantics and AI-driven Pipeline for Bankruptcy Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#31934;&#30830;&#35821;&#20041;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31890;&#31354;&#38388;&#20013;&#21033;&#29992;&#29305;&#24449;&#35821;&#20041;&#21644;&#21487;&#38752;&#35266;&#27979;&#26469;&#39044;&#27979;&#32570;&#22833;&#20540;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#30772;&#20135;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#30772;&#20135;&#30340;&#27969;&#31243;&#12290;&#32570;&#22833;&#20540;&#12289;&#39640;&#32500;&#25968;&#25454;&#20197;&#21450;&#39640;&#24230;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#24211;&#26159;&#35813;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#31934;&#30830;&#35821;&#20041;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#26032;&#26041;&#27861;&#12290;&#25506;&#35752;&#20102;&#31890;&#35745;&#31639;&#30340;&#20248;&#28857;&#20197;&#23450;&#20041;&#27492;&#26041;&#27861;&#12290;&#21033;&#29992;&#29305;&#24449;&#35821;&#20041;&#21644;&#21487;&#38752;&#35266;&#27979;&#22312;&#20302;&#32500;&#31354;&#38388;&#12289;&#31890;&#31354;&#38388;&#20013;&#39044;&#27979;&#32570;&#22833;&#20540;&#12290;&#22260;&#32469;&#27599;&#20010;&#32570;&#22833;&#26465;&#30446;&#24418;&#25104;&#31890;&#23376;&#65292;&#32771;&#34385;&#21040;&#19968;&#20123;&#39640;&#24230;&#30456;&#20851;&#30340;&#29305;&#24449;&#21644;&#26368;&#21487;&#38752;&#30340;&#26368;&#36817;&#35266;&#27979;&#20197;&#20445;&#25345;&#25968;&#25454;&#24211;&#23545;&#32570;&#22833;&#26465;&#30446;&#30340;&#30456;&#20851;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#21518;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#31890;&#23376;&#20013;&#36827;&#34892;&#36328;&#31890;&#23376;&#39044;&#27979;&#36827;&#34892;&#25554;&#34917;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#19978;&#19979;&#25991;&#31890;&#23376;&#20351;&#24471;&#22312;&#24040;&#22823;&#25968;&#25454;&#24211;&#20013;&#36827;&#34892;&#19968;&#23567;&#37096;&#20998;&#30456;&#20851;&#30340;&#25554;&#34917;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00013v1 Announce Type: cross  Abstract: This work focuses on designing a pipeline for the prediction of bankruptcy. The presence of missing values, high dimensional data, and highly class-imbalance databases are the major challenges in the said task. A new method for missing data imputation with granular semantics has been introduced here. The merits of granular computing have been explored here to define this method. The missing values have been predicted using the feature semantics and reliable observations in a low-dimensional space, in the granular space. The granules are formed around every missing entry, considering a few of the highly correlated features and most reliable closest observations to preserve the relevance and reliability, the context, of the database against the missing entries. An intergranular prediction is then carried out for the imputation within those contextual granules. That is, the contextual granules enable a small relevant fraction of the huge 
&lt;/p&gt;</description></item><item><title>FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.20261</link><description>&lt;p&gt;
FABind+: &#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#22686;&#24378;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20261
&lt;/p&gt;
&lt;p&gt;
FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23545;&#25509;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#36807;&#31243;&#12290;&#20256;&#32479;&#25216;&#26415;&#20381;&#36182;&#20110;&#21463;&#29289;&#29702;&#21407;&#29702;&#25903;&#37197;&#30340;&#24191;&#27867;&#37319;&#26679;&#21644;&#27169;&#25311;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#36895;&#24230;&#24930;&#19988;&#26114;&#36149;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#21069;&#26223;&#65292;&#25552;&#20379;&#20102;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#22686;&#38271;&#12290;&#24314;&#31435;&#22312;FABind&#30340;&#22522;&#30784;&#24037;&#20316;&#20043;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind+&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22823;&#25552;&#21319;&#20854;&#21069;&#36523;&#24615;&#33021;&#30340;&#22686;&#24378;&#29256;&#12290;&#25105;&#20204;&#30830;&#23450;&#21475;&#34955;&#39044;&#27979;&#26159;&#20998;&#23376;&#23545;&#25509;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#33879;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#23545;&#25509;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#23545;&#25509;&#27169;&#22359;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#22686;&#24378;&#20854;&#23039;&#24577;&#29983;&#25104;&#33021;&#21147;&#12290;&#20026;&#20102;&#32553;&#23567;&#19982;&#20256;&#32479;&#37319;&#26679;/&#29983;&#25104;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20261v1 Announce Type: cross  Abstract: Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective s
&lt;/p&gt;</description></item><item><title>DiJiang&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#22495;&#26680;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#30340;&#22522;&#26412;Transformer&#27169;&#22411;&#36716;&#21270;&#20026;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#65292;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#36924;&#36817;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.19928</link><description>&lt;p&gt;
DiJiang&#65306;&#36890;&#36807;&#32039;&#20945;&#30340;&#26680;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiJiang: Efficient Large Language Models through Compact Kernelization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19928
&lt;/p&gt;
&lt;p&gt;
DiJiang&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#22495;&#26680;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#30340;&#22522;&#26412;Transformer&#27169;&#22411;&#36716;&#21270;&#20026;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#65292;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#36924;&#36817;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#23569;Transformers&#30340;&#35745;&#31639;&#36127;&#33655;&#65292;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#30740;&#31350;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#27880;&#24847;&#26426;&#21046;&#30340;&#25913;&#36827;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#32463;&#36807;&#22823;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#22312;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DiJiang&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#22495;&#26680;&#26041;&#27861;&#65292;&#21487;&#23558;&#39044;&#35757;&#32451;&#30340;&#22522;&#26412;Transformer&#36716;&#21270;&#20026;&#20855;&#26377;&#36739;&#23567;&#35757;&#32451;&#25104;&#26412;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#27169;&#22411;&#12290;&#36890;&#36807;&#37319;&#29992;&#21152;&#26435;&#25311;&#38543;&#26426;&#37319;&#26679;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#36924;&#36817;&#25928;&#29575;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#38477;&#20302;&#35757;&#32451;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#30340;&#26680;&#26041;&#27861;&#22522;&#20110;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#65288;DCT&#65289;&#25805;&#20316;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#19982;&#21407;&#22987;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#35757;&#32451;&#26102;&#38388;&#22823;&#22823;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19928v1 Announce Type: new  Abstract: In an effort to reduce the computational load of Transformers, research on linear attention has gained significant momentum. However, the improvement strategies for attention mechanisms typically necessitate extensive retraining, which is impractical for large language models with a vast array of parameters. In this paper, we present DiJiang, a novel Frequency Domain Kernelization approach that enables the transformation of a pre-trained vanilla Transformer into a linear complexity model with little training costs. By employing a weighted Quasi-Monte Carlo method for sampling, the proposed approach theoretically offers superior approximation efficiency. To further reduce the training computational complexity, our kernelization is based on Discrete Cosine Transform (DCT) operations. Extensive experiments demonstrate that the proposed method achieves comparable performance to the original Transformer, but with significantly reduced trainin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;TRAC-2024&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#26631;&#27880;&#30340;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#21361;&#23475;&#21487;&#33021;&#24615;&#24182;&#35782;&#21035;&#30446;&#26631;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#36187;&#36947;&#20013;&#21462;&#24471;&#31532;&#20108;&#21517;&#30340;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2403.19713</link><description>&lt;p&gt;
NJUST-KMG&#21442;&#21152;TRAC-2024&#20219;&#21153;1&#21644;&#20219;&#21153;2&#65306;&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
NJUST-KMG at TRAC-2024 Tasks 1 and 2: Offline Harm Potential Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;TRAC-2024&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#26631;&#27880;&#30340;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#21361;&#23475;&#21487;&#33021;&#24615;&#24182;&#35782;&#21035;&#30446;&#26631;&#30340;&#31639;&#27861;&#65292;&#22312;&#20004;&#20010;&#36187;&#36947;&#20013;&#21462;&#24471;&#31532;&#20108;&#21517;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;TRAC-2024&#31163;&#32447;&#21361;&#23475;&#28508;&#22312;&#24615;&#35782;&#21035;&#20013;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#35813;&#27604;&#36187;&#21253;&#21547;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;&#30740;&#31350;&#21033;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#21360;&#24230;&#35821;&#35328;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#30340;&#20016;&#23500;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#35780;&#20998;&#26631;&#27880;&#65292;&#20197;&#25429;&#25417;&#31163;&#32447;&#29615;&#22659;&#21361;&#23475;&#30340;&#24494;&#22937;&#21547;&#20041;&#12290;&#21442;&#19982;&#32773;&#30340;&#20219;&#21153;&#26159;&#35774;&#35745;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#29305;&#23450;&#24773;&#20917;&#19979;&#21361;&#23475;&#21487;&#33021;&#24615;&#24182;&#35782;&#21035;&#31163;&#32447;&#21361;&#23475;&#26368;&#21487;&#33021;&#30340;&#30446;&#26631;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#36187;&#36947;&#20013;&#25490;&#21517;&#31532;&#20108;&#65292;F1&#20540;&#20998;&#21035;&#20026;0.73&#21644;0.96&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#36873;&#25321;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#25972;&#21512;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#26041;&#27861;&#24212;&#29992;&#20110;&#27979;&#35797;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19713v1 Announce Type: new  Abstract: This report provide a detailed description of the method that we proposed in the TRAC-2024 Offline Harm Potential dentification which encloses two sub-tasks. The investigation utilized a rich dataset comprised of social media comments in several Indian languages, annotated with precision by expert judges to capture the nuanced implications for offline context harm. The objective assigned to the participants was to design algorithms capable of accurately assessing the likelihood of harm in given situations and identifying the most likely target(s) of offline harm. Our approach ranked second in two separate tracks, with F1 values of 0.73 and 0.96 respectively. Our method principally involved selecting pretrained models for finetuning, incorporating contrastive learning techniques, and culminating in an ensemble approach for the test set.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19647</link><description>&lt;p&gt;
&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65306;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#21457;&#29616;&#21644;&#24212;&#29992;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#30005;&#36335;&#26159;&#20154;&#31867;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#22240;&#26524;&#30456;&#20851;&#23376;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#12290; &#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#30830;&#23450;&#30340;&#30005;&#36335;&#30001;&#22810;&#20041;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#21333;&#20803;&#32452;&#25104;&#65292;&#20363;&#22914;&#27880;&#24847;&#21147;&#22836;&#25110;&#31070;&#32463;&#20803;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23454;&#29616;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#12290; &#30001;&#20110;&#23427;&#20204;&#22522;&#20110;&#32454;&#31890;&#24230;&#21333;&#20803;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23545;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65306;&#25105;&#20204; introduc&#20102;SHIFT&#65292;&#36890;&#36807;&#20999;&#38500;&#20154;&#31867;&#21028;&#26029;&#20026;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21457;&#29616;&#25104;&#21315;&#19978;&#19975;&#20010;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#26469;&#23637;&#31034;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#19988;&#21487;&#25193;&#23637;&#30340;&#21487;&#35299;&#37322;&#24615;&#31649;&#32447;&#65292;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19647v1 Announce Type: cross  Abstract: We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#35774;&#35745;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.19561</link><description>&lt;p&gt;
&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;&#29992;&#20110;&#21487;&#25193;&#23637;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-Improved Learning for Scalable Neural Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#35774;&#35745;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
end-to-end&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;(NCO)&#26041;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#19987;&#23478;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#38382;&#39064;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#25105;&#25913;&#36827;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#19968;&#31181;&#21019;&#26032;&#30340;&#23616;&#37096;&#37325;&#26500;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#33258;&#36523;&#36845;&#20195;&#29983;&#25104;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#20197;&#25351;&#23548;&#26377;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19561v1 Announce Type: cross  Abstract: The end-to-end neural combinatorial optimization (NCO) method shows promising performance in solving complex combinatorial optimization problems without the need for expert design. However, existing methods struggle with large-scale problems, hindering their practical applicability. To overcome this limitation, this work proposes a novel Self-Improved Learning (SIL) method for better scalability of neural combinatorial optimization. Specifically, we develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without any labeled data. Powered by an innovative local reconstruction approach, this method can iteratively generate better solutions by itself as pseudo-labels to guide efficient model training. In addition, we design a linear complexity attention mechanism for the model to efficiently handle large-scale combinatorial problem instances with low computation overhead. Comprehens
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#23545;&#25152;&#26377;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#22343;&#31561;&#37325;&#35201;&#24615;&#30340;&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#35780;&#20272;&#31639;&#27861;&#20844;&#24179;&#24615;&#65292;&#30830;&#20445;&#22312;&#20943;&#23569;&#20559;&#35265;&#21644;&#20840;&#23616;&#20998;&#31867;&#38169;&#35823;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.19165</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Evaluating Fair Feature Selection in Machine Learning for Healthcare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19165
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#23545;&#25152;&#26377;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#22343;&#31561;&#37325;&#35201;&#24615;&#30340;&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#35780;&#20272;&#31639;&#27861;&#20844;&#24179;&#24615;&#65292;&#30830;&#20445;&#22312;&#20943;&#23569;&#20559;&#35265;&#21644;&#20840;&#23616;&#20998;&#31867;&#38169;&#35823;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#33258;&#21160;&#21270;&#31038;&#20250;&#20559;&#35265;&#36827;&#19968;&#27493;&#21152;&#21095;&#20581;&#24247;&#24046;&#36317;&#30340;&#28508;&#21147;&#26500;&#25104;&#20102;&#37325;&#22823;&#39118;&#38505;&#12290;&#25105;&#20204;&#20174;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#25506;&#35752;&#31639;&#27861;&#20844;&#24179;&#24615;&#12290;&#20256;&#32479;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#36890;&#36807;&#21435;&#38500;&#36164;&#28304;&#23494;&#38598;&#12289;&#30456;&#20851;&#25110;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#26469;&#35782;&#21035;&#29992;&#20110;&#26356;&#22909;&#20915;&#31574;&#30340;&#29305;&#24449;&#65292;&#20294;&#24573;&#30053;&#20102;&#36825;&#20123;&#22240;&#32032;&#22312;&#19981;&#21516;&#23376;&#32676;&#20307;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31181;&#32771;&#34385;&#23545;&#25152;&#26377;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#22343;&#31561;&#37325;&#35201;&#24615;&#30340;&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20013;&#21516;&#26102;&#32771;&#34385;&#20102;&#20844;&#24179;&#24615;&#24230;&#37327;&#21644;&#38169;&#35823;&#24230;&#37327;&#65292;&#20197;&#30830;&#20445;&#22312;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#20559;&#35265;&#21644;&#20840;&#23616;&#20998;&#31867;&#38169;&#35823;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#25152;&#26377;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20844;&#24179;&#24615;&#25351;&#26631;&#24471;&#21040;&#25913;&#21892;&#65292;&#21516;&#26102;&#20998;&#31867;&#38169;&#35823;&#20165;&#26377;&#36731;&#24494;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19165v1 Announce Type: new  Abstract: With the universal adoption of machine learning in healthcare, the potential for the automation of societal biases to further exacerbate health disparities poses a significant risk. We explore algorithmic fairness from the perspective of feature selection. Traditional feature selection methods identify features for better decision making by removing resource-intensive, correlated, or non-relevant features but overlook how these factors may differ across subgroups. To counter these issues, we evaluate a fair feature selection method that considers equal importance to all demographic groups. We jointly considered a fairness metric and an error metric within the feature selection process to ensure a balance between minimizing both bias and global classification error. We tested our approach on three publicly available healthcare datasets. On all three datasets, we observed improvements in fairness metrics coupled with a minimal degradation 
&lt;/p&gt;</description></item><item><title>TinyML&#26159;&#19968;&#31181;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#21040;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24494;&#25511;&#21046;&#22120;&#20013;&#23454;&#29616;&#26080;&#22788;&#19981;&#22312;&#26234;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#38656;&#35201;&#20849;&#21516;&#35774;&#35745;&#31639;&#27861;&#21644;&#31995;&#32479;&#22534;&#26632;&#20197;&#20811;&#26381;&#30828;&#20214;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.19076</link><description>&lt;p&gt;
&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#65306;&#36827;&#23637;&#19982;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Tiny Machine Learning: Progress and Futures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19076
&lt;/p&gt;
&lt;p&gt;
TinyML&#26159;&#19968;&#31181;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#21040;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24494;&#25511;&#21046;&#22120;&#20013;&#23454;&#29616;&#26080;&#22788;&#19981;&#22312;&#26234;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#38656;&#35201;&#20849;&#21516;&#35774;&#35745;&#31639;&#27861;&#21644;&#31995;&#32479;&#22534;&#26632;&#20197;&#20811;&#26381;&#30828;&#20214;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tiny Machine Learning&#65288;TinyML&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#26032;&#39046;&#22495;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#21040;&#25968;&#21313;&#20159;&#20010;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24494;&#25511;&#21046;&#22120;&#65288;MCUs&#65289;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#33539;&#22260;&#65292;&#23454;&#29616;&#20102;&#26080;&#22788;&#19981;&#22312;&#30340;&#26234;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30828;&#20214;&#38480;&#21046;&#65292;TinyML&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#26377;&#38480;&#30340;&#20869;&#23384;&#36164;&#28304;&#20351;&#24471;&#38590;&#20197;&#23481;&#32435;&#20026;&#20113;&#21644;&#31227;&#21160;&#24179;&#21488;&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#23545;&#20110;&#35064;&#26426;&#35774;&#22791;&#65292;&#32534;&#35793;&#22120;&#21644;&#25512;&#26029;&#24341;&#25806;&#25903;&#25345;&#20063;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#20849;&#21516;&#35774;&#35745;&#31639;&#27861;&#21644;&#31995;&#32479;&#22534;&#26632;&#20197;&#23454;&#29616;TinyML&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19076v1 Announce Type: cross  Abstract: Tiny Machine Learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of AI applications and enable ubiquitous intelligence. However, TinyML is challenging due to hardware constraints: the tiny memory resource makes it difficult to hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. Therefore, we need to co-design the algorithm and system stack to enable TinyML. In this review, we will first discuss the definition, challenges, and applications of TinyML. We then survey the recent progress in TinyML and deep learning on MCUs. Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design. We will further extend the solution from inference to training and in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.18998</link><description>&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#36328;&#31995;&#32479;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18998
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#65288;MSS&#65289;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#21160;&#24577;&#30340;&#29305;&#24615;&#21487;&#33021;&#22312;&#21508;&#31181;&#25925;&#38556;&#31867;&#21035;&#20013;&#20986;&#29616;&#25925;&#38556;&#12290;&#20026;&#20102;&#26377;&#25928;&#22788;&#29702;&#25925;&#38556;&#65292;AIOps&#24037;&#20855;&#21033;&#29992;&#22522;&#20110;&#36319;&#36394;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#65288;2&#65289;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;MSS&#65292;Trainticket&#21644;OnlineBoutique&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20351;&#29992;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35843;&#25972;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20197;&#23545;&#26032;&#30340;&#12289;&#26410;&#35265;&#30340;&#26032;&#39062;&#25925;&#38556;&#31867;&#21035;&#30340;&#24322;&#24120;&#36319;&#36394;&#36827;&#34892;&#20998;&#31867;&#65292;&#26080;&#35770;&#26159;&#22312;&#26368;&#21021;&#35757;&#32451;&#30340;&#21516;&#19968;&#31995;&#32479;&#20869;&#65292;&#36824;&#26159;&#22312;&#20854;&#20182;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18998v1 Announce Type: cross  Abstract: Microservice-based systems (MSS) may experience failures in various fault categories due to their complex and dynamic nature. To effectively handle failures, AIOps tools utilize trace-based anomaly detection and root cause analysis. In this paper, we propose a novel framework for few-shot abnormal trace classification for MSS. Our framework comprises two main components: (1) Multi-Head Attention Autoencoder for constructing system-specific trace representations, which enables (2) Transformer Encoder-based Model-Agnostic Meta-Learning to perform effective and efficient few-shot learning for abnormal trace classification. The proposed framework is evaluated on two representative MSS, Trainticket and OnlineBoutique, with open datasets. The results show that our framework can adapt the learned knowledge to classify new, unseen abnormal traces of novel fault categories both within the same system it was initially trained on and even in the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#20803;&#32990;&#33258;&#21160;&#26426;&#27169;&#22411;&#21644;CNN-LSTM&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25104;&#21151;&#39044;&#27979;&#20132;&#36890;&#27969;&#65292;&#20805;&#20998;&#21033;&#29992;&#20132;&#36890;&#27969;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.18710</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20803;&#32990;&#33258;&#21160;&#26426;&#27169;&#22411;&#21644;CNN-LSTM&#26550;&#26500;&#36827;&#34892;&#20132;&#36890;&#27969;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Traffic Flow Prediction using Cellular Automata-based Model and CNN-LSTM architecture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#20803;&#32990;&#33258;&#21160;&#26426;&#27169;&#22411;&#21644;CNN-LSTM&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25104;&#21151;&#39044;&#27979;&#20132;&#36890;&#27969;&#65292;&#20805;&#20998;&#21033;&#29992;&#20132;&#36890;&#27969;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#39044;&#27979;&#20132;&#36890;&#27969;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20294;&#21462;&#24471;&#20102;&#19981;&#21516;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#23545;&#20110;&#20132;&#36890;&#27969;&#31995;&#32479;&#26469;&#35828;&#36825;&#26679;&#30340;&#25968;&#25454;&#30446;&#21069;&#24182;&#19981;&#23481;&#26131;&#33719;&#24471;&#12290;&#20854;&#27425;&#65292;&#21363;&#20351;&#26377;&#25968;&#25454;&#65292;&#31070;&#32463;&#32593;&#32476;&#20063;&#38656;&#35201;&#35775;&#38382;&#28085;&#30422;&#22823;&#22810;&#25968;&#21487;&#33021;&#30340;&#20132;&#36890;&#27969;&#21160;&#24577;&#30340;&#21382;&#21490;&#25968;&#25454;&#25165;&#33021;&#25104;&#21151;&#39044;&#27979;&#26410;&#26469;&#30340;&#20132;&#36890;&#29366;&#24577;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20851;&#20110;&#20132;&#36890;&#27969;&#21160;&#24577;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#23613;&#31649;&#24050;&#32463;&#26377;&#30456;&#24403;&#22810;&#30340;&#29616;&#26377;&#30693;&#35782;&#24211;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#25104;&#21151;&#39044;&#27979;&#20132;&#36890;&#27969;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#20803;&#32990;&#33258;&#21160;&#26426;&#30340;&#20132;&#36890;&#27969;&#32479;&#35745;&#21147;&#23398;&#27169;&#22411;&#29983;&#25104;&#20132;&#36890;&#27969;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18710v1 Announce Type: new  Abstract: Recent works have attempted to use deep learning to predict future states of traffic flow, but have met with mixed results. These approaches face two key challenges. First, training deep learning neural networks requires large amounts of training data which are not yet easily available for traffic flow systems. Second, even when data is available, the neural networks require access to historical data that covers most possible traffic flow dynamics to successfully predict future traffic states. Specifically, these deep learning approaches do not fully leverage domain-knowledge about traffic flow dynamics, despite a significant existing knowledge-base. In this work, we propose to solve both issues using a Convolutional Neural Network (CNNs) with Long Short Term Memory (LSTM) deep learning architecture to successfully predict traffic flow, while leveraging a cellular automata-based statistical mechanics model of traffic flow to generate tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#31283;&#20581;RL&#39046;&#22495;&#30340;&#25506;&#32034;&#65292;&#35782;&#21035;&#24182;&#36827;&#19968;&#27493;&#29702;&#35299;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#37096;&#32626;RL&#31995;&#32479;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#31639;&#27861;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#36848;&#20197;&#22686;&#24378;RL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18539</link><description>&lt;p&gt;
&#23433;&#20840;&#31283;&#20581;&#30340;&#24378;&#21270;&#23398;&#20064;: &#21407;&#21017;&#19982;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Safe and Robust Reinforcement-Learning: Principles and Practice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#23433;&#20840;&#21644;&#31283;&#20581;RL&#39046;&#22495;&#30340;&#25506;&#32034;&#65292;&#35782;&#21035;&#24182;&#36827;&#19968;&#27493;&#29702;&#35299;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#37096;&#32626;RL&#31995;&#32479;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#31639;&#27861;&#26041;&#27861;&#30340;&#32508;&#21512;&#35780;&#36848;&#20197;&#22686;&#24378;RL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#35299;&#20915;&#30456;&#23545;&#22797;&#26434;&#30340;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#28982;&#32780;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#37096;&#32626;RL&#31995;&#32479;&#38754;&#20020;&#19982;&#23433;&#20840;&#21644;&#31283;&#20581;&#24615;&#26377;&#20851;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#23433;&#20840;&#21644;&#31283;&#20581;RL&#39046;&#22495;&#30340;&#20027;&#35201;&#32500;&#24230;&#65292;&#28085;&#30422;&#31639;&#27861;&#12289;&#20262;&#29702;&#21644;&#23454;&#38469;&#32771;&#34385;&#22240;&#32032;&#65292;&#35782;&#21035;&#24182;&#36827;&#19968;&#27493;&#29702;&#35299;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#36817;&#24180;&#26469;&#33268;&#21147;&#20110;&#35299;&#20915;&#19982;RL&#24212;&#29992;&#30456;&#20851;&#22266;&#26377;&#39118;&#38505;&#30340;&#26041;&#27861;&#21644;&#24320;&#25918;&#38382;&#39064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#12290;&#22312;&#35752;&#35770;&#24182;&#25552;&#20986;&#23433;&#20840;&#21644;&#31283;&#20581;RL&#30340;&#23450;&#20041;&#21518;&#65292;&#26412;&#25991;&#23558;&#29616;&#26377;&#30740;&#31350;&#24037;&#20316;&#24402;&#31867;&#20026;&#19981;&#21516;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;RL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#35832;&#22914;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#20248;&#21270;&#26041;&#27861;&#23398;&#12289;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#20197;&#21450;&#23545;&#25239;&#24615;&#31561;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18539v1 Announce Type: new  Abstract: Reinforcement Learning (RL) has shown remarkable success in solving relatively complex tasks, yet the deployment of RL systems in real-world scenarios poses significant challenges related to safety and robustness. This paper aims to identify and further understand those challenges thorough the exploration of the main dimensions of the safe and robust RL landscape, encompassing algorithmic, ethical, and practical considerations. We conduct a comprehensive review of methodologies and open problems that summarizes the efforts in recent years to address the inherent risks associated with RL applications.   After discussing and proposing definitions for both safe and robust RL, the paper categorizes existing research works into different algorithmic approaches that enhance the safety and robustness of RL agents. We examine techniques such as uncertainty estimation, optimisation methodologies, exploration-exploitation trade-offs, and adversari
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;BCM&#65289;&#65292;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#23454;&#29616;&#27839;&#30528;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#21069;&#21521;&#21644;&#21518;&#21521;&#36941;&#21382;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32479;&#19968;&#20102;&#29983;&#25104;&#21644;&#32534;&#36753;&#22270;&#20687;&#31561;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.18035</link><description>&lt;p&gt;
&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Consistency Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18035
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;BCM&#65289;&#65292;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#23454;&#29616;&#27839;&#30528;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#21069;&#21521;&#21644;&#21518;&#21521;&#36941;&#21382;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32479;&#19968;&#20102;&#29983;&#25104;&#21644;&#32534;&#36753;&#22270;&#20687;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#36890;&#36807;&#36845;&#20195;&#21435;&#22122;&#19968;&#20010;&#38543;&#26426;&#21521;&#37327;&#33021;&#22815;&#29983;&#25104;&#38750;&#24120;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#36825;&#20010;&#36807;&#31243;&#23545;&#24212;&#20110;&#27839;&#30528;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;PF ODE&#65289;&#31227;&#21160;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;DMs&#36824;&#21487;&#20197;&#36890;&#36807;&#27839;&#30528;PF ODE&#21521;&#21518;&#31227;&#21160;&#23558;&#36755;&#20837;&#22270;&#20687;&#36716;&#25442;&#20026;&#22122;&#22768;&#65292;&#36825;&#26159;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#25554;&#20540;&#21644;&#22270;&#20687;&#32534;&#36753;&#65289;&#30340;&#20851;&#38190;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#30340;&#36845;&#20195;&#24615;&#36136;&#38480;&#21046;&#20102;&#20854;&#36895;&#24230;&#65292;&#38459;&#30861;&#20102;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CMs&#65289;&#24050;&#32463;&#20986;&#29616;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#36817;&#20284;PF ODE&#30340;&#31215;&#20998;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#38656;&#35201;&#36845;&#20195;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#26174;&#24335;ODE&#27714;&#35299;&#22120;&#20351;&#24471;&#21453;&#28436;&#36807;&#31243;&#22797;&#26434;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;BCM&#65289;&#65292;&#23398;&#20064;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#27839;&#30528;PF ODE&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#36941;&#21382;&#65292;&#26377;&#25928;&#22320;&#32479;&#19968;&#29983;&#25104;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18035v1 Announce Type: new  Abstract: Diffusion models (DMs) are capable of generating remarkably high-quality samples by iteratively denoising a random vector, a process that corresponds to moving along the probability flow ordinary differential equation (PF ODE). Interestingly, DMs can also invert an input image to noise by moving backward along the PF ODE, a key operation for downstream tasks such as interpolation and image editing. However, the iterative nature of this process restricts its speed, hindering its broader application. Recently, Consistency Models (CMs) have emerged to address this challenge by approximating the integral of the PF ODE, thereby bypassing the need to iterate. Yet, the absence of an explicit ODE solver complicates the inversion process. To resolve this, we introduce the Bidirectional Consistency Model (BCM), which learns a single neural network that enables both forward and backward traversal along the PF ODE, efficiently unifying generation an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Triplet Mamba-UNet&#65292;&#21033;&#29992;&#27531;&#20313;VSS&#22359;&#25552;&#21462;&#23494;&#38598;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;Triplet SSM&#34701;&#21512;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.17701</link><description>&lt;p&gt;
&#26059;&#36716;&#25195;&#25551;&#65306;&#24102;&#26377;&#19977;&#20803;SSM&#27169;&#22359;&#30340;UNet-like Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Triplet Mamba-UNet&#65292;&#21033;&#29992;&#27531;&#20313;VSS&#22359;&#25552;&#21462;&#23494;&#38598;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;Triplet SSM&#34701;&#21512;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#21106;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#21344;&#25454;&#37325;&#35201;&#20301;&#32622;&#12290;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Transformer&#27169;&#22411;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#30001;&#20110;&#26377;&#38480;&#24863;&#21463;&#37326;&#25110;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#65292;&#29305;&#21035;&#26159;Mamba&#21450;&#20854;&#21464;&#20307;&#65292;&#22312;&#35270;&#35273;&#39046;&#22495;&#34920;&#29616;&#20986;&#26174;&#33879;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21487;&#33021;&#19981;&#22815;&#26377;&#25928;&#65292;&#20445;&#30041;&#20102;&#19968;&#20123;&#20887;&#20313;&#32467;&#26500;&#65292;&#30041;&#19979;&#20102;&#21442;&#25968;&#20943;&#23569;&#30340;&#31354;&#38388;&#12290;&#21463;&#20808;&#21069;&#30340;&#31354;&#38388;&#21644;&#36890;&#36947;&#27880;&#24847;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Triplet Mamba-UNet&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#27531;&#20313;VSS&#22359;&#26469;&#25552;&#21462;&#23494;&#38598;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#21516;&#26102;&#21033;&#29992;Triplet SSM&#26469;&#34701;&#21512;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;ISIC17&#12289;ISIC18&#12289;CVC-300&#12289;CVC-ClinicDB&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17701v1 Announce Type: cross  Abstract: Image segmentation holds a vital position in the realms of diagnosis and treatment within the medical domain. Traditional convolutional neural networks (CNNs) and Transformer models have made significant advancements in this realm, but they still encounter challenges because of limited receptive field or high computing complexity. Recently, State Space Models (SSMs), particularly Mamba and its variants, have demonstrated notable performance in the field of vision. However, their feature extraction methods may not be sufficiently effective and retain some redundant structures, leaving room for parameter reduction. Motivated by previous spatial and channel attention methods, we propose Triplet Mamba-UNet. The method leverages residual VSS Blocks to extract intensive contextual features, while Triplet SSM is employed to fuse features across spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18, CVC-300, CVC-ClinicDB, 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#26159;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#30340;&#26368;&#26032;&#21457;&#23637;</title><link>https://arxiv.org/abs/2403.17561</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21450;&#20854;&#26368;&#26032;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning and State-of-the-arts Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17561
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#30340;&#26368;&#26032;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;, &#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#23618;&#20114;&#36830;&#21333;&#20803;&#65288;&#31070;&#32463;&#20803;&#65289;&#20174;&#21407;&#22987;&#36755;&#20837;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#21644;&#34920;&#31034;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#21463;&#21040;&#36825;&#31181;&#23398;&#20064;&#33021;&#21147;&#30340;&#36171;&#33021;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26159;&#35768;&#22810;&#31361;&#30772;&#24615;&#25216;&#26415;&#21644;&#21019;&#26032;&#30340;&#26680;&#24515;&#39537;&#21160;&#21147;&#12290;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#29616;&#23454;&#38382;&#39064;&#30340;&#21160;&#24577;&#24615;&#12290;&#26377;&#20960;&#39033;&#30740;&#31350;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#27010;&#24565;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31867;&#22411;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26368;&#26032;&#21457;&#23637;&#30340;&#35206;&#30422;&#38754;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#21463;&#21040;&#36825;&#20123;&#38480;&#21046;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;th
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17561v1 Announce Type: new  Abstract: Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data. Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations. Building a deep learning model is a challenging task due to the algorithm`s complexity and the dynamic nature of real-world problems. Several studies have reviewed deep learning concepts and applications. However, the studies mostly focused on the types of deep learning models and convolutional neural network architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains. Therefore, motivated by the limitations, this study aims to comprehensively review th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.17407</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;
&lt;/p&gt;
&lt;p&gt;
Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23391;&#21152;&#25289;&#25991;&#26412;&#21040;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#30340;&#20934;&#30830;&#36716;&#24405;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35821;&#35328;&#30340;&#22797;&#26434;&#38899;&#38901;&#23398;&#21644;&#35821;&#22659;&#30456;&#20851;&#30340;&#38899;&#21464;&#12290;&#23545;&#20110;&#21306;&#22495;&#23391;&#21152;&#25289;&#26041;&#35328;&#26469;&#35828;&#65292;&#30001;&#20110;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#26041;&#35328;&#30340;&#26631;&#20934;&#25340;&#20889;&#32422;&#23450;&#12289;&#24403;&#22320;&#21644;&#22806;&#35821;&#22312;&#36825;&#20123;&#22320;&#21306;&#20013;&#27969;&#34892;&#30340;&#35789;&#27719;&#20197;&#21450;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#38899;&#38901;&#22810;&#26679;&#24615;&#65292;&#36825;&#19968;&#25361;&#25112;&#29978;&#33267;&#26356;&#20026;&#20005;&#23803;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#35206;&#30422;&#23391;&#21152;&#25289;&#22269;&#20845;&#20010;&#22320;&#21306;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#24341;&#20837;&#8220;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#8221;&#65288;DGT&#65289;&#25216;&#26415;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#29983;&#25104;IPA&#36716;&#24405;&#20043;&#21069;&#21521;&#27169;&#22411;&#25552;&#20379;&#26377;&#20851;&#36755;&#20837;&#25991;&#26412;&#30340;&#21306;&#22495;&#26041;&#35328;&#25110;&#8220;&#22320;&#21306;&#8221;&#30340;&#26126;&#30830;&#20449;&#24687;&#12290;&#36825;&#36890;&#36807;&#22312;&#36755;&#20837;&#24207;&#21015;&#21069;&#28155;&#21152;&#19968;&#20010;&#22320;&#21306;&#26631;&#35760;&#26469;&#23454;&#29616;&#65292;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17407v1 Announce Type: cross  Abstract: Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or "district" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each 
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#29420;&#29305;&#30340;Transformer&#27169;&#22411;&#22312;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#20026;&#21487;&#38752;&#21644;&#21487;&#25345;&#32493;&#30340;&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.16108</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#30340;Transformer&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Transformer approach for Electricity Price Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16108
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#29420;&#29305;&#30340;Transformer&#27169;&#22411;&#22312;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#20026;&#21487;&#38752;&#21644;&#21487;&#25345;&#32493;&#30340;&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;Transformer&#27169;&#22411;&#36827;&#34892;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#65288;EPF&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#27809;&#26377;&#20351;&#29992;&#20854;&#20182;&#36882;&#24402;&#32593;&#32476;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22240;&#27492;&#65292;&#34920;&#26126;&#27880;&#24847;&#21147;&#23618;&#36275;&#20197;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#12290;&#35813;&#35770;&#25991;&#36824;&#36890;&#36807;&#20351;&#29992;&#24320;&#28304;EPF&#24037;&#20855;&#36827;&#34892;&#20102;&#23545;&#27169;&#22411;&#30340;&#20844;&#24179;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#20195;&#30721;&#20197;&#22686;&#24378;EPF&#30740;&#31350;&#30340;&#21487;&#20877;&#29616;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20026;&#21487;&#38752;&#21644;&#21487;&#25345;&#32493;&#30340;&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16108v1 Announce Type: cross  Abstract: This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15796</link><description>&lt;p&gt;
&#20174;&#25439;&#22833;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding Emergent Abilities of Language Models from the Loss Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#36136;&#30097;&#20102;&#20256;&#32479;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#20165;&#23384;&#22312;&#20110;&#22823;&#27169;&#22411;&#20013;&#30340;&#35266;&#28857;&#12290;&#36825;&#31181;&#24576;&#30097;&#28304;&#33258;&#20004;&#28857;&#35266;&#23519;&#65306;1&#65289;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#23637;&#29616;&#20986;&#23545;&#31361;&#29616;&#33021;&#21147;&#30340;&#39640;&#24615;&#33021;&#65307;2&#65289;&#36136;&#30097;&#29992;&#20110;&#27979;&#37327;&#36825;&#20123;&#33021;&#21147;&#30340;&#19981;&#36830;&#32493;&#24615;&#25351;&#26631;&#12290;&#26412;&#25991;&#25552;&#35758;&#20174;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#35282;&#24230;&#30740;&#31350;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#38750;&#27169;&#22411;&#22823;&#23567;&#25110;&#35757;&#32451;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#20294;&#19981;&#21516;&#27169;&#22411;&#21644;&#25968;&#25454;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#21516;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24403;&#26576;&#19968;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#19981;&#35770;&#25351;&#26631;&#30340;&#36830;&#32493;&#24615;&#22914;&#20309;&#65307;&#32780;&#22312;&#36798;&#21040;&#35813;&#38408;&#20540;&#20043;&#21069;&#65292;&#20854;&#24615;&#33021;&#20173;&#20445;&#25345;&#22312;&#38543;&#26426;&#29468;&#27979;&#27700;&#24179;&#12290;&#36825;&#21551;&#21457;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#31361;&#29616;&#33021;&#21147;&#20026;&#37027;&#20123;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15796v1 Announce Type: cross  Abstract: Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Private Mixing of Ensemble Distributions (PMixED)&#65306;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#30340;&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#24182;&#37319;&#26679;&#24179;&#22343;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#20197;&#26356;&#36731;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.15638</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24046;&#20998;&#31169;&#26377;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Next-Token Prediction of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15638
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Private Mixing of Ensemble Distributions (PMixED)&#65306;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#20998;&#24067;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#30340;&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#24182;&#37319;&#26679;&#24179;&#22343;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#20197;&#26356;&#36731;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38544;&#31169;&#26085;&#30410;&#37325;&#35201;&#12290;DP-SGD&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26368;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#23427;&#20197;&#19968;&#31181;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DP-SGD&#38656;&#35201;&#27604;SGD&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#26356;&#22823;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#36807;&#39640;&#20272;&#35745;&#23545;&#25163;&#20855;&#26377;&#30333;&#30418;&#35775;&#38382;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26356;&#29616;&#23454;&#30340;&#22330;&#26223;&#20551;&#35774;&#21482;&#26377;&#23545;&#38544;&#31169;&#25935;&#24863;&#30340;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#12290;&#22312;&#36825;&#20123;&#35266;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31169;&#26377;&#28151;&#21512;&#38598;&#21512;&#20998;&#24067;&#65288;PMixED&#65289;&#65306;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#27599;&#20010;&#36755;&#20986;&#20998;&#24067;&#20174;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;LLM&#38598;&#21512;&#25237;&#24433;&#21040;&#20844;&#20849;LLM&#36755;&#20986;&#20998;&#24067;&#21608;&#22260;&#30340;&#38598;&#21512;&#19978;&#65292;&#28982;&#21518;&#23545;&#25237;&#24433;&#20998;&#24067;&#36827;&#34892;&#24179;&#22343;&#24182;&#20174;&#20013;&#25277;&#26679;&#26469;&#23454;&#29616;&#23454;&#38469;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#31169;&#26377;&#39044;&#27979;&#21327;&#35758;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;DP-SGD&#26356;&#36731;&#37327;&#21270;&#65292;&#22240;&#20026;&#23427;&#19982;&#27169;&#22411;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15638v1 Announce Type: cross  Abstract: Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly important. The most widely adopted technique to accomplish this is DP-SGD, which trains a model in such a way that guarantees Differential Privacy (DP). However, DP-SGD requires longer training times and larger memory requirements than SGD, while overestimating an adversary's capabilities in having white box access to the model. A more realistic scenario assumes only black-box access to a privacy-sensitive LLM. Motivated by these observations, we present Private Mixing of Ensemble Distributions (PMixED): a private prediction protocol that achieves practical next-token prediction by projecting each of the model's output distribution from an ensemble of fine-tuned LLMs onto a set around a public LLM's output distribution, then averaging the projected distributions and sampling from it. Our approach is more lightweight than DP-SGD in that it is model agnostic, i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;PET&#25195;&#25551;&#22270;&#20687;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26102;&#20351;&#29992;&#20102;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15443</link><description>&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;PET&#25195;&#25551;&#22270;&#20687;&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;
&lt;/p&gt;
&lt;p&gt;
Introducing an ensemble method for the early detection of Alzheimer's disease through the analysis of PET scan images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15443
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;PET&#25195;&#25551;&#22270;&#20687;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26102;&#20351;&#29992;&#20102;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#31181;&#36880;&#28176;&#24694;&#21270;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#20027;&#35201;&#24433;&#21709;&#35760;&#24518;&#12289;&#24605;&#32500;&#21644;&#34892;&#20026;&#31561;&#35748;&#30693;&#21151;&#33021;&#12290;&#26412;&#30149;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38454;&#27573;&#65292;&#21363;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65292;&#38750;&#24120;&#37325;&#35201;&#23613;&#26089;&#35786;&#26029;&#65292;&#22240;&#20026;&#19968;&#20123;&#36880;&#28176;&#21457;&#23637;&#20026;&#30149;&#30151;&#30340;MCI&#24739;&#32773;&#20250;&#21457;&#23637;&#20026;&#36825;&#31181;&#30142;&#30149;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#20026;&#22235;&#20010;&#19981;&#21516;&#32452;&#65306;&#25511;&#21046;&#27491;&#24120;&#65288;CN&#65289;&#12289;&#36880;&#28176;&#21457;&#23637;&#30340;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;pMCI&#65289;&#12289;&#31283;&#23450;&#30340;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;sMCI&#65289;&#21644;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#20998;&#31867;&#26159;&#22522;&#20110;&#23545;&#20174;ADNI&#25968;&#25454;&#38598;&#33719;&#24471;&#30340;PET&#25195;&#25551;&#22270;&#20687;&#30340;&#24443;&#24213;&#26816;&#26597;&#65292;&#36825;&#25552;&#20379;&#20102;&#23545;&#30142;&#30149;&#36827;&#23637;&#30340;&#24443;&#24213;&#29702;&#35299;&#12290;&#24050;&#32463;&#20351;&#29992;&#20102;&#20960;&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20351;&#29992;&#20102;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;VGG16&#12289;AlexNet&#21644;&#33258;&#23450;&#20041;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15443v1 Announce Type: cross  Abstract: Alzheimer's disease is a progressive neurodegenerative disorder that primarily affects cognitive functions such as memory, thinking, and behavior. In this disease, there is a critical phase, mild cognitive impairment, that is really important to be diagnosed early since some patients with progressive MCI will develop the disease. This study delves into the challenging task of classifying Alzheimer's disease into four distinct groups: control normal (CN), progressive mild cognitive impairment (pMCI), stable mild cognitive impairment (sMCI), and Alzheimer's disease (AD). This classification is based on a thorough examination of PET scan images obtained from the ADNI dataset, which provides a thorough understanding of the disease's progression. Several deep-learning and traditional machine-learning models have been used to detect Alzheimer's disease. In this paper, three deep-learning models, namely VGG16 and AlexNet, and a custom Convolu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22478;&#24066;&#35774;&#35745;&#22312;&#20844;&#20849;&#20132;&#36890;&#25928;&#29575;&#21644;&#20943;&#23569;&#30899;&#25490;&#25918;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25351;&#20986;&#22478;&#24066;&#24067;&#23616;&#23545;&#20844;&#20849;&#20132;&#36890;&#32467;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#22478;&#24066;&#35774;&#35745;&#20803;&#32032;&#30340;&#23450;&#21046;&#31574;&#30053;&#23545;&#20110;&#27668;&#20505;&#36866;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.14671</link><description>&lt;p&gt;
&#20102;&#35299;&#36807;&#22659;&#32570;&#21475;&#65306;&#21335;&#21345;&#32599;&#26469;&#32435;&#24030;&#22799;&#27931;&#29305;&#24066;&#21335;&#31471;&#21644;&#30000;&#32435;&#35199;&#24030;&#26597;&#22612;&#21162;&#21152;&#24066;&#38463;&#20911;&#20195;&#23572;&#30340;&#21363;&#38656;&#20844;&#20849;&#27773;&#36710;&#26381;&#21153;&#21450;&#22478;&#24066;&#27668;&#20505;&#36866;&#24212;&#33021;&#21147;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Transit Gap: A Comparative Study of On-Demand Bus Services and Urban Climate Resilience in South End, Charlotte, NC and Avondale, Chattanooga, TN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22478;&#24066;&#35774;&#35745;&#22312;&#20844;&#20849;&#20132;&#36890;&#25928;&#29575;&#21644;&#20943;&#23569;&#30899;&#25490;&#25918;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25351;&#20986;&#22478;&#24066;&#24067;&#23616;&#23545;&#20844;&#20849;&#20132;&#36890;&#32467;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#22478;&#24066;&#35774;&#35745;&#20803;&#32032;&#30340;&#23450;&#21046;&#31574;&#30053;&#23545;&#20110;&#27668;&#20505;&#36866;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#35774;&#35745;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#20132;&#36890;&#25928;&#29575;&#21644;&#20943;&#23569;&#30899;&#25490;&#25918;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20855;&#26377;&#19981;&#21516;&#22478;&#24066;&#35774;&#35745;&#30340;&#20004;&#20010;&#31038;&#21306;&#65306;&#21271;&#21345;&#32599;&#26469;&#32435;&#24030;&#22799;&#27931;&#29305;&#24066;&#21335;&#31471;&#65292;&#20855;&#26377;&#21160;&#24577;&#28151;&#21512;&#29992;&#36884;&#30340;&#22478;&#24066;&#35774;&#35745;&#27169;&#24335;&#65307;&#21644;&#30000;&#32435;&#35199;&#24030;&#26597;&#22612;&#21162;&#21152;&#24066;&#38463;&#20911;&#20195;&#23572;&#65292;&#37319;&#29992;&#20303;&#23429;&#37066;&#21306;&#32593;&#26684;&#24067;&#23616;&#12290;&#36890;&#36807;&#20351;&#29992;TRANSIT-GYM&#24037;&#20855;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#36825;&#20123;&#19981;&#21516;&#22478;&#24066;&#29615;&#22659;&#20013;&#22686;&#21152;&#20844;&#20849;&#27773;&#36710;&#21033;&#29992;&#29575;&#23545;&#20132;&#36890;&#21644;CO2&#25490;&#25918;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#22478;&#24066;&#35774;&#35745;&#21644;&#35268;&#21010;&#22312;&#20132;&#36890;&#31995;&#32479;&#25928;&#29575;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#22799;&#27931;&#29305;&#24066;&#21335;&#31471;&#65292;&#28151;&#21512;&#29992;&#36884;&#35774;&#35745;&#23548;&#33268;&#26356;&#22810;&#30340;&#25490;&#25918;&#20943;&#23569;&#65292;&#34920;&#26126;&#22478;&#24066;&#24067;&#23616;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#20844;&#20849;&#20132;&#36890;&#32467;&#26524;&#12290;&#32771;&#34385;&#21040;&#29420;&#29305;&#30340;&#22478;&#24066;&#35774;&#35745;&#20803;&#32032;&#30340;&#37327;&#36523;&#23450;&#21046;&#31574;&#30053;&#23545;&#20110;&#27668;&#20505;&#36866;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#21335;&#31471;&#65292;&#20844;&#20849;&#27773;&#36710;&#21033;&#29992;&#29575;&#32763;&#20493;&#20351;&#26085;&#24120;&#25490;&#25918;&#20943;&#23569;&#20102;10.18&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14671v1 Announce Type: cross  Abstract: Urban design significantly impacts sustainability, particularly in the context of public transit efficiency and carbon emissions reduction. This study explores two neighborhoods with distinct urban designs: South End, Charlotte, NC, featuring a dynamic mixed-use urban design pattern, and Avondale, Chattanooga, TN, with a residential suburban grid layout. Using the TRANSIT-GYM tool, we assess the impact of increased bus utilization in these different urban settings on traffic and CO2 emissions. Our results highlight the critical role of urban design and planning in transit system efficiency. In South End, the mixed-use design led to more substantial emission reductions, indicating that urban layout can significantly influence public transit outcomes. Tailored strategies that consider the unique urban design elements are essential for climate resilience. Notably, doubling bus utilization decreased daily emissions by 10.18% in South End a
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#36890;&#36807;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20197;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#20943;&#23569;&#24341;&#20837;&#30340;&#38468;&#21152;&#21442;&#25968;&#25110;&#35745;&#31639;&#36164;&#28304;&#25968;&#37327;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.14608</link><description>&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14608
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#36890;&#36807;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20197;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#20943;&#23569;&#24341;&#20837;&#30340;&#38468;&#21152;&#21442;&#25968;&#25110;&#35745;&#31639;&#36164;&#28304;&#25968;&#37327;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20195;&#34920;&#20102;&#19968;&#39033;&#31361;&#30772;&#24615;&#30340;&#36827;&#23637;&#65292;&#20351;&#24471;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#31354;&#21069;&#30340;&#35268;&#27169;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#30001;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#32452;&#25104;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#25191;&#34892;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20026;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#23450;&#21046;&#22823;&#22411;&#27169;&#22411;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#21463;&#21040;&#35745;&#31639;&#33021;&#21147;&#38480;&#21046;&#30340;&#30828;&#20214;&#24179;&#21488;&#19978;&#65292;&#35268;&#27169;&#24222;&#22823;&#21644;&#35745;&#31639;&#35201;&#27714;&#24040;&#22823;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35843;&#25972;&#22823;&#22411;&#27169;&#22411;&#20197;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PEFT&#26159;&#25351;&#35843;&#25972;&#39044;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20351;&#20854;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;&#36807;&#31243;&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#24341;&#20837;&#30340;&#38468;&#21152;&#21442;&#25968;&#25110;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14608v1 Announce Type: new  Abstract: Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adapt the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large models to adapt it to a specific task while minimizing the number of additional parameters introduced or computational resources required. This approac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14589</link><description>&lt;p&gt;
ReAct&#36935;&#19978;ActRe&#65306;&#23545;&#27604;&#24615;&#33258;&#35757;&#32451;&#20013;&#30340;&#20195;&#29702;&#36712;&#36857;&#33258;&#21160;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25991;&#25688;&#65306;&#35821;&#35328;&#20195;&#29702;&#36890;&#36807;&#19982;&#22522;&#30784;&#27169;&#22411;&#25512;&#29702;&#23637;&#31034;&#20102;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#22810;&#27493;&#25512;&#29702;&#21644;&#34892;&#21160;&#36712;&#36857;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#35821;&#35328;&#20195;&#29702;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#36825;&#26679;&#30340;&#36712;&#36857;&#20173;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#20154;&#21147;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#36824;&#26159;&#23454;&#26045;&#22810;&#26679;&#21270;&#25552;&#31034;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;A$^3$T&#65292;&#19968;&#20010;&#20801;&#35768;&#20197;ReAct&#39118;&#26684;&#33258;&#20027;&#27880;&#37322;&#20195;&#29702;&#36712;&#36857;&#30340;&#26694;&#26550;&#12290;&#20854;&#20013;&#24515;&#26159;&#19968;&#20010;ActRe&#25552;&#31034;&#20195;&#29702;&#65292;&#23427;&#35299;&#37322;&#20219;&#24847;&#21160;&#20316;&#30340;&#21407;&#22240;&#12290;&#24403;&#38543;&#26426;&#25277;&#21462;&#22806;&#37096;&#21160;&#20316;&#26102;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#20197;&#26597;&#35810;ActRe&#20195;&#29702;&#20197;&#33719;&#21462;&#20854;&#25991;&#26412;&#29702;&#30001;&#12290;&#26032;&#39062;&#30340;&#36712;&#36857;&#28982;&#21518;&#36890;&#36807;&#23558;ActRe&#30340;&#21518;&#39564;&#25512;&#29702;&#21069;&#32622;&#21040;&#25277;&#26679;&#21160;&#20316;&#20013;&#36827;&#34892;&#32508;&#21512;&#21512;&#25104;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 Announce Type: new  Abstract: Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent exe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.14119</link><description>&lt;p&gt;
C-TPT&#65306;&#36890;&#36807;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#26657;&#20934;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#27979;&#35797;&#26102;&#36866;&#24212;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#20363;&#35777;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#31034;&#20027;&#35201;&#26159;&#20026;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#32780;&#24320;&#21457;&#30340;&#65292;&#24573;&#35270;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#8212;&#8212;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26657;&#20934;&#26041;&#27861;&#20381;&#36182;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#27979;&#35797;&#26102;&#22330;&#26223;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#25506;&#35752;&#26657;&#20934;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14119v1 Announce Type: cross  Abstract: In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13802</link><description>&lt;p&gt;
ZigMa&#65306;&#34623;&#34578;&#26364;&#24052;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ZigMa: Zigzag Mamba Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#21487;&#20280;&#32553;&#24615;&#21644;&#20108;&#27425;&#22797;&#26434;&#24615;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#32467;&#26500;&#20869;&#37096;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;&#26364;&#24052;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#65292;&#20197;&#25193;&#23637;&#20854;&#22312;&#35270;&#35273;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22823;&#22810;&#25968;&#24403;&#21069;&#22522;&#20110;&#26364;&#24052;&#30340;&#35270;&#35273;&#26041;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30095;&#24573;&#65292;&#21363;&#26364;&#24052;&#30340;&#25195;&#25551;&#26041;&#26696;&#20013;&#32570;&#20047;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#32771;&#34385;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#31616;&#21333;&#12289;&#21363;&#25554;&#21363;&#29992;&#12289;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#23427;&#20248;&#20110;&#22522;&#20110;&#26364;&#24052;&#30340;&#22522;&#32447;&#65292;&#24182;&#34920;&#29616;&#20986;&#27604;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22522;&#32447;&#26356;&#24555;&#36895;&#21644;&#26356;&#22909;&#30340;&#20869;&#23384;&#21033;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;Zigzag Mamba&#38598;&#25104;&#21040;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#20013;&#65292;&#20197;&#30740;&#31350;&#27169;&#22411;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;FacesHQ $1024\times 1024$&#21644;UCF101&#65292;MultiModal-CelebA-HQ&#65289;&#19978;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13802v1 Announce Type: cross  Abstract: The diffusion model has long been plagued by scalability and quadratic complexity issues, especially within transformer-based structures. In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation. Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba. Secondly, building upon this insight, we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to transformer-based baselines. Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#21253;&#25216;&#26415;&#35777;&#26126;&#21453;&#21521;&#25311;&#21512;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#26680;&#22810;&#37325;&#32593;&#26684;&#31639;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22686;&#24378;&#21453;&#21521;&#25311;&#21512;&#65292;&#36866;&#29992;&#20110;&#32467;&#26500;&#21270;&#21644;&#20998;&#25955;&#25968;&#25454;&#30340;&#21152;&#24615;GPs&#12290;</title><link>https://arxiv.org/abs/2403.13300</link><description>&lt;p&gt;
&#26680;&#22810;&#37325;&#32593;&#26684;&#65306;&#36890;&#36807;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#21152;&#36895;&#21453;&#21521;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Kernel Multigrid: Accelerate Back-fitting via Sparse Gaussian Process Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#21253;&#25216;&#26415;&#35777;&#26126;&#21453;&#21521;&#25311;&#21512;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#26680;&#22810;&#37325;&#32593;&#26684;&#31639;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22686;&#24378;&#21453;&#21521;&#25311;&#21512;&#65292;&#36866;&#29992;&#20110;&#32467;&#26500;&#21270;&#21644;&#20998;&#25955;&#25968;&#25454;&#30340;&#21152;&#24615;GPs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28155;&#21152;&#39640;&#26031;&#36807;&#31243;(GPs)&#26159;&#38750;&#21442;&#25968;&#29305;&#24449;&#36873;&#25321;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#24120;&#35265;&#35757;&#32451;&#26041;&#27861;&#26159;&#36125;&#21494;&#26031;&#21453;&#21521;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#21152;&#24615;GPs&#26102;&#65292;&#21453;&#21521;&#25311;&#21512;&#30340;&#25910;&#25947;&#36895;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;&#26680;&#21253;(KP)&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21453;&#21521;&#25311;&#21512;&#30340;&#25910;&#25947;&#36895;&#24230;&#19981;&#20250;&#27604;$(1-\mathcal{O}(\frac{1}{n}))^t$&#26356;&#24555;&#65292;&#20854;&#20013;$n$&#21644;$t$&#20998;&#21035;&#34920;&#31034;&#25968;&#25454;&#22823;&#23567;&#21644;&#36845;&#20195;&#27425;&#25968;&#12290;&#22240;&#27492;&#65292;&#21453;&#21521;&#25311;&#21512;&#38656;&#35201;&#26368;&#23569;$\mathcal{O}(n\log n)$&#27425;&#36845;&#20195;&#25165;&#33021;&#23454;&#29616;&#25910;&#25947;&#12290;&#22522;&#20110;KP&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26680;&#22810;&#37325;&#32593;&#26684;(KMG)&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;(GPR)&#32435;&#20837;&#27599;&#20010;&#21453;&#21521;&#25311;&#21512;&#36845;&#20195;&#20043;&#21518;&#22788;&#29702;&#27531;&#24046;&#26469;&#22686;&#24378;&#21453;&#21521;&#25311;&#21512;&#12290;&#23427;&#36866;&#29992;&#20110;&#20855;&#26377;&#32467;&#26500;&#21270;&#21644;&#20998;&#25955;&#25968;&#25454;&#30340;&#21152;&#24615;GPs&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;K
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13300v1 Announce Type: cross  Abstract: Additive Gaussian Processes (GPs) are popular approaches for nonparametric feature selection. The common training method for these models is Bayesian Back-fitting. However, the convergence rate of Back-fitting in training additive GPs is still an open problem. By utilizing a technique called Kernel Packets (KP), we prove that the convergence rate of Back-fitting is no faster than $(1-\mathcal{O}(\frac{1}{n}))^t$, where $n$ and $t$ denote the data size and the iteration number, respectively. Consequently, Back-fitting requires a minimum of $\mathcal{O}(n\log n)$ iterations to achieve convergence. Based on KPs, we further propose an algorithm called Kernel Multigrid (KMG). This algorithm enhances Back-fitting by incorporating a sparse Gaussian Process Regression (GPR) to process the residuals subsequent to each Back-fitting iteration. It is applicable to additive GPs with both structured and scattered data. Theoretically, we prove that K
&lt;/p&gt;</description></item><item><title>STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.12418</link><description>&lt;p&gt;
STG-Mamba: &#36890;&#36807;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12418
&lt;/p&gt;
&lt;p&gt;
STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Graph&#65288;STG&#65289;&#25968;&#25454;&#20855;&#26377;&#21160;&#24577;&#24615;&#12289;&#24322;&#36136;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#29305;&#28857;&#65292;&#23548;&#33268;&#26102;&#31354;&#22270;&#23398;&#20064;&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#38598;&#20013;&#20110;&#27169;&#25311;STG&#32593;&#32476;&#20013;&#33410;&#28857;&#20010;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#38543;&#26102;&#38388;&#23384;&#22312;&#30340;STG&#31995;&#32479;&#26412;&#36136;&#29305;&#24449;&#30340;&#24314;&#27169;&#37325;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#29616;&#20195;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSSMs&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#31934;&#24515;&#25506;&#32034;&#20102;STG&#31995;&#32479;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#21160;&#24577;&#29366;&#24577;&#28436;&#21464;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Spatial-Temporal Graph Mamba&#65288;STG-Mamba&#65289;&#65292;&#20316;&#20026;&#39318;&#20010;&#21033;&#29992;&#24378;&#22823;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;STG&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12418v1 Announce Type: cross  Abstract: Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of ST
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#26041;&#27861; Reprogrammer &#22312;&#25991;&#26412;&#22270;&#20687;&#32534;&#30721;&#22120;&#20013;&#30340;&#24212;&#29992;&#20248;&#20110;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#19979;&#28216;&#27169;&#22411;&#22312;&#20998;&#24067;&#20869;&#21644;&#20998;&#24067;&#22806;&#25968;&#25454;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.10800</link><description>&lt;p&gt;
&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#20248;&#20110;&#22312;&#25991;&#26412;&#22270;&#20687;&#32534;&#30721;&#22120;&#20013;&#38024;&#23545;&#20998;&#24067;&#22806;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10800
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#26041;&#27861; Reprogrammer &#22312;&#25991;&#26412;&#22270;&#20687;&#32534;&#30721;&#22120;&#20013;&#30340;&#24212;&#29992;&#20248;&#20110;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#19979;&#28216;&#27169;&#22411;&#22312;&#20998;&#24067;&#20869;&#21644;&#20998;&#24067;&#22806;&#25968;&#25454;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35780;&#20272;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26102;&#65292;&#19981;&#20165;&#38656;&#35201;&#35780;&#20272;&#19979;&#28216;&#27169;&#22411;&#30340;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#20934;&#30830;&#24615;&#65292;&#36824;&#38656;&#35201;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#24182;&#35782;&#21035;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26679;&#26412;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#20405;&#20837;&#24615;&#24494;&#35843;&#25216;&#26415;&#25152;&#24102;&#26469;&#30340;&#38544;&#34255;&#25104;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24120;&#29992;&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#20165;&#25197;&#26354;&#20102;&#29992;&#20110;&#27867;&#21270;&#21040;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;OOD&#26679;&#26412;&#65288;OOD&#27867;&#21270;&#65289;&#25152;&#38656;&#30340;&#34920;&#31034;&#65292;&#36824;&#25197;&#26354;&#20102;&#29992;&#20110;&#26816;&#27979;&#22312;&#35821;&#20041;&#19978;&#36716;&#31227;&#30340;OOD&#26679;&#26412;&#65288;OOD&#26816;&#27979;&#65289;&#25152;&#38656;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#26041;&#27861;&#29992;&#20110;&#24494;&#35843;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#37325;&#26032;&#32534;&#31243;&#22120;&#12290;&#37325;&#26032;&#32534;&#31243;&#22120;&#26088;&#22312;&#25552;&#39640;&#19979;&#28216;&#27169;&#22411;&#22312;ID&#12289;OOD&#27867;&#21270;&#21644;OOD&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35777;&#25454;&#26174;&#31034;&#37325;&#26032;&#32534;&#31243;&#22120;&#20248;&#20110;&#23545;&#25239;&#24615;&#24494;&#35843;&#19982;&#21407;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#26159;&#19968;&#31181;&#36890;&#29992;&#12289;&#26377;&#25928;&#19988;&#24378;&#22823;&#30340;&#24037;&#20855;&#26469;&#24212;&#23545;OOD&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10800v1 Announce Type: new  Abstract: When evaluating the performance of a pre-trained model transferred to a downstream task, it is imperative to assess not only the in-distribution (ID) accuracy of the downstream model but also its capacity to generalize and identify out-of-distribution (OOD) samples. In this paper, we unveil the hidden costs associated with intrusive fine-tuning techniques. Specifically, we demonstrate that commonly used fine-tuning methods not only distort the representations necessary for generalizing to covariate-shifted OOD samples (OOD generalization) but also distort the representations necessary for detecting semantically-shifted OOD samples (OOD detection). To address these challenges, we introduce a new model reprogramming approach for fine-tuning, which we name Reprogrammer. Reprogrammer aims to improve the holistic performance of the downstream model across ID, OOD generalization, and OOD detection tasks. Our empirical evidence reveals that Rep
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09863</link><description>&lt;p&gt;
&#19968;&#20010;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Conceptual Framework For White Box Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09863
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#23436;&#20840;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#12290;&#19968;&#20010;&#20805;&#20998;&#21160;&#26426;&#30340;MNIST&#30456;&#20851;&#23376;&#38382;&#39064;&#30340;&#27010;&#24565;&#39564;&#35777;&#27169;&#22411;&#21253;&#25324;4&#20010;&#36825;&#26679;&#30340;&#23618;&#65292;&#24635;&#20849;4800&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#26131;&#20110;&#35299;&#37322;&#65292;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#23545;&#25239;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23545;&#25239;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21333;&#20010;CPU&#19978;&#24555;&#36895;&#35757;&#32451;&#12290;&#35813;&#25216;&#26415;&#30340;&#36890;&#29992;&#24615;&#25215;&#35834;&#20026;&#24443;&#24213;&#27665;&#20027;&#21270;&#21644;&#30495;&#27491;&#36890;&#29992;&#30340;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/314-Foundation/white-box-nn&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09863v1 Announce Type: cross  Abstract: This paper introduces semantic features as a general conceptual framework for fully explainable neural network layers. A well-motivated proof of concept model for relevant subproblem of MNIST consists of 4 such layers with the total of 4.8K learnable parameters. The model is easily interpretable, achieves human-level adversarial test accuracy with no form of adversarial training, requires little hyperparameter tuning and can be quickly trained on a single CPU. The general nature of the technique bears promise for a paradigm shift towards radically democratised and truly generalizable white box neural networks. The code is available at https://github.com/314-Foundation/white-box-nn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#35299;&#24352;&#37327;&#36827;&#19968;&#27493;&#25277;&#35937;&#31232;&#30095;DNN&#21152;&#36895;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#25104;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07953</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#20998;&#35299;&#23545;&#31232;&#30095;DNN&#21152;&#36895;&#36827;&#34892;&#25277;&#35937;&#21270;
&lt;/p&gt;
&lt;p&gt;
Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#35299;&#24352;&#37327;&#36827;&#19968;&#27493;&#25277;&#35937;&#31232;&#30095;DNN&#21152;&#36895;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#25104;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#21033;&#29992;&#31232;&#30095;&#24615;&#24050;&#25104;&#20026;&#28385;&#36275;&#29616;&#20195;DNN&#26085;&#30410;&#22686;&#38271;&#30340;&#35745;&#31639;&#38656;&#27714;&#30340;&#19968;&#31181;&#20855;&#26377;&#21069;&#26223;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#31232;&#30095;DNN&#21152;&#36895;&#20173;&#28982;&#38754;&#20020;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#31232;&#30095;&#21152;&#36895;&#30340;&#24320;&#38144;&#65292;&#30828;&#20214;&#35774;&#35745;&#24072;&#26368;&#36817;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#31232;&#30095;&#30828;&#20214;&#25903;&#25345;&#65292;&#36825;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#28789;&#27963;&#24615;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#20026;&#26576;&#20123;&#32467;&#26500;&#21270;&#31232;&#30095;&#30828;&#20214;&#24494;&#35843;&#30340;&#20219;&#20309;&#31232;&#30095;&#27169;&#22411;&#26080;&#27861;&#34987;&#20854;&#20182;&#32467;&#26500;&#21270;&#30828;&#20214;&#21152;&#36895;&#12290;&#20026;&#20102;&#24357;&#21512;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#20998;&#35299;&#30340;&#24352;&#37327;&#36817;&#20284;&#65288;TASD&#65289;&#65292;&#21033;&#29992;&#20102;&#32447;&#24615;&#20195;&#25968;&#20013;&#30340;&#20998;&#37197;&#24615;&#36136;&#23558;&#20219;&#20309;&#31232;&#30095;&#24352;&#37327;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36719;&#20214;&#26694;&#26550;TASDER&#65292;&#36890;&#36807;&#25628;&#32034;&#36880;&#23618;&#39640;&#36136;&#37327;&#30340;&#32467;&#26500;&#21270;&#20998;&#35299;&#26469;&#21152;&#36895;DNNs&#30340;&#26435;&#37325;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07953v1 Announce Type: cross  Abstract: Exploiting sparsity in deep neural networks (DNNs) has been a promising area to meet the growing computation need of modern DNNs. However, in practice, sparse DNN acceleration still faces a key challenge. To minimize the overhead of sparse acceleration, hardware designers have proposed structured sparse hardware support recently, which provides limited flexibility and requires extra model fine-tuning. Moreover, any sparse model fine-tuned for certain structured sparse hardware cannot be accelerated by other structured hardware. To bridge the gap between sparse DNN models and hardware, this paper proposes tensor approximation via structured decomposition (TASD), which leverages the distributive property in linear algebra to turn any sparse tensor into a series of structured sparse tensors. Next, we develop a software framework, TASDER, to accelerate DNNs by searching layer-wise, high-quality structured decomposition for both weight and 
&lt;/p&gt;</description></item><item><title>SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.07378</link><description>&lt;p&gt;
SVD-LLM: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#30340;&#25130;&#26029;&#24863;&#30693;&#22855;&#24322;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07378
&lt;/p&gt;
&lt;p&gt;
SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#21463;&#21040;&#20854;&#24222;&#22823;&#23610;&#23544;&#30340;&#38480;&#21046;&#65292;&#36825;&#38656;&#35201;LLM&#21387;&#32553;&#26041;&#27861;&#20197;&#23454;&#29616;&#23454;&#38469;&#37096;&#32626;&#12290;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20026;LLM&#21387;&#32553;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#25130;&#26029;&#36739;&#23567;&#30340;&#22855;&#24322;&#20540;&#21487;&#33021;&#23548;&#33268;&#26356;&#39640;&#30340;&#21387;&#32553;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;SVD&#25130;&#26029;&#21518;&#21097;&#20313;&#27169;&#22411;&#21442;&#25968;&#30340;&#26356;&#26032;&#32570;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVD-LLM&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;SVD-LLM&#37319;&#29992;&#20102;&#19968;&#31181;&#25130;&#26029;&#24863;&#30693;&#30340;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;SVD-LLM&#37319;&#29992;&#19968;&#31181;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#20197;&#24357;&#34917;SVD&#25130;&#26029;&#24341;&#36215;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;&#24635;&#20849;11&#20010;&#25968;&#25454;&#38598;&#21644;&#19971;&#20010;m&#19978;&#35780;&#20272;&#20102;SVD-LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
&lt;/p&gt;</description></item><item><title>Mamba&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#20851;&#27880;&#39537;&#21160;&#30340;&#27169;&#22411;&#65292;&#36825;&#19982;&#21464;&#21387;&#22120;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#19988;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#21487;&#20197;&#28145;&#20837;&#20102;&#35299;&#20854;&#20869;&#37096;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.01590</link><description>&lt;p&gt;
Mamba&#27169;&#22411;&#30340;&#38544;&#34255;&#20851;&#27880;
&lt;/p&gt;
&lt;p&gt;
The Hidden Attention of Mamba Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01590
&lt;/p&gt;
&lt;p&gt;
Mamba&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#20851;&#27880;&#39537;&#21160;&#30340;&#27169;&#22411;&#65292;&#36825;&#19982;&#21464;&#21387;&#22120;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#19988;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#21487;&#20197;&#28145;&#20837;&#20102;&#35299;&#20854;&#20869;&#37096;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mamba&#23618;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSM)&#65292;&#22312;&#24314;&#27169;&#22810;&#20010;&#39046;&#22495;&#21253;&#25324;NLP&#12289;&#38271;&#36317;&#31163;&#24207;&#21015;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#36873;&#25321;&#24615;SSMs&#34987;&#35270;&#20026;&#21452;&#37325;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#36890;&#36807;IO-aware&#24182;&#34892;&#25195;&#25551;&#22312;&#25972;&#20010;&#24207;&#21015;&#19978;&#36827;&#34892;&#24182;&#34892;&#35757;&#32451;&#65292;&#24182;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#37096;&#32626;&#12290;&#25105;&#20204;&#28155;&#21152;&#20102;&#31532;&#19977;&#20010;&#35270;&#35282;&#65292;&#24182;&#23637;&#31034;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#20851;&#27880;&#39537;&#21160;&#30340;&#27169;&#22411;&#12290;&#36825;&#19968;&#26032;&#35270;&#35282;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#24213;&#23618;&#26426;&#21046;&#19982;&#21464;&#21387;&#22120;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35753;&#25105;&#20204;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#31397;&#25506;Mamba&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01590v1 Announce Type: new  Abstract: The Mamba layer offers an efficient selective state space model (SSM) that is highly effective in modeling multiple domains including NLP, long-range sequences processing, and computer vision. Selective SSMs are viewed as dual models, in which one trains in parallel on the entire sequence via IO-aware parallel scan, and deploys in an autoregressive manner. We add a third view and show that such models can be viewed as attention-driven models. This new perspective enables us to compare the underlying mechanisms to that of the self-attention layers in transformers and allows us to peer inside the inner workings of the Mamba model with explainability methods. Our code is publicly available.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#36866;&#24212;&#22810;&#26679;&#21270;&#25252;&#29702;&#25509;&#25910;&#32773;&#21709;&#24212;&#30340;&#40065;&#26834;&#25252;&#29702;&#20154;&#21592;&#25919;&#31574;&#65292;&#20197;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25919;&#31574;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00344</link><description>&lt;p&gt;
&#37319;&#29992;&#22810;&#26679;&#21270;&#30340;&#21512;&#20316;&#34892;&#20026;&#21644;&#23545;&#25239;&#26679;&#24335;&#37319;&#26679;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25919;&#31574;&#30340;&#40065;&#26834;&#24615;&#29992;&#20110;&#36741;&#21161;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Robustifying a Policy in Multi-Agent RL with Diverse Cooperative Behavior and Adversarial Style Sampling for Assistive Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00344
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#36866;&#24212;&#22810;&#26679;&#21270;&#25252;&#29702;&#25509;&#25910;&#32773;&#21709;&#24212;&#30340;&#40065;&#26834;&#25252;&#29702;&#20154;&#21592;&#25919;&#31574;&#65292;&#20197;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25919;&#31574;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20154;&#25658;&#24102;&#21160;&#20316;&#38556;&#30861;&#32773;&#30340;&#33258;&#20027;&#21327;&#21161;&#26159;&#33258;&#20027;&#26426;&#22120;&#20154;&#31995;&#32479;&#26368;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25253;&#36947;&#20102;&#22312;&#21307;&#30103;&#39046;&#22495;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36741;&#21161;&#20219;&#21153;&#21487;&#20197;&#34987;&#21046;&#23450;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#26377;&#20004;&#20010;&#26234;&#33021;&#20307;&#65306;&#25252;&#29702;&#20154;&#21592;&#21644;&#25252;&#29702;&#25509;&#25910;&#32773;&#12290;&#28982;&#32780;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#25919;&#31574;&#24448;&#24448;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#25919;&#31574;&#25935;&#24863;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#26377;&#32032;&#30340;&#25252;&#29702;&#20154;&#21592;&#25919;&#31574;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#25252;&#29702;&#25509;&#25910;&#32773;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35757;&#32451;&#36866;&#24212;&#22810;&#26679;&#21270;&#25252;&#29702;&#25509;&#25910;&#32773;&#21709;&#24212;&#30340;&#40065;&#26834;&#25252;&#29702;&#20154;&#21592;&#25919;&#31574;&#30340;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#22810;&#26679;&#21270;&#30340;&#25252;&#29702;&#25509;&#25910;&#32773;&#21709;&#24212;&#26159;&#36890;&#36807;&#35797;&#39564;&#21644;&#38169;&#35823;&#33258;&#20027;&#23398;&#20064;&#30340;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#25252;&#29702;&#32773;&#25919;&#31574;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25252;&#29702;&#25509;&#25910;&#32773;&#21709;&#24212;&#36827;&#34892;&#23545;&#25239;&#37319;&#26679;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00344v1 Announce Type: cross  Abstract: Autonomous assistance of people with motor impairments is one of the most promising applications of autonomous robotic systems. Recent studies have reported encouraging results using deep reinforcement learning (RL) in the healthcare domain. Previous studies showed that assistive tasks can be formulated as multi-agent RL, wherein there are two agents: a caregiver and a care-receiver. However, policies trained in multi-agent RL are often sensitive to the policies of other agents. In such a case, a trained caregiver's policy may not work for different care-receivers. To alleviate this issue, we propose a framework that learns a robust caregiver's policy by training it for diverse care-receiver responses. In our framework, diverse care-receiver responses are autonomously learned through trials and errors. In addition, to robustify the care-giver's policy, we propose a strategy for sampling a care-receiver's response in an adversarial mann
&lt;/p&gt;</description></item><item><title>Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16562</link><description>&lt;p&gt;
Q-FOX&#23398;&#20064;&#65306;&#39072;&#35206;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Q-FOX Learning: Breaking Tradition in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16562
&lt;/p&gt;
&lt;p&gt;
Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#23398;&#20064;&#26368;&#20339;&#21160;&#20316;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#25110;&#30452;&#25509;&#30417;&#30563;&#30340;&#20219;&#21153;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-FOX&#30340;&#26032;&#39062;&#33258;&#21160;&#35843;&#21442;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;FOX&#20248;&#21270;&#22120;&#21644;&#24120;&#29992;&#30340;&#26131;&#20110;&#23454;&#29616;&#30340;RL Q-learning&#31639;&#27861;&#35299;&#20915;&#20102;&#35843;&#21442;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#23558;&#22870;&#21169;&#25918;&#22312;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;&#23398;&#20064;&#26102;&#38388;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16562v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision. Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms. Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards. In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes' hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning. Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#30340;&#31616;&#21333;&#38543;&#26426;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#30697;&#38453;&#20056;&#27861;&#23454;&#29616;&#39640;&#36136;&#37327;&#36924;&#36817;&#20272;&#35745;&#27010;&#29575;&#21644;&#27169;&#22411;&#25972;&#20307;&#24046;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16326</link><description>&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#30340;&#21487;&#35777;&#23454;&#20934;&#30830;&#24615;&#38543;&#26426;&#25277;&#26679;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Provably Accurate Randomized Sampling Algorithm for Logistic Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16326
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#30340;&#31616;&#21333;&#38543;&#26426;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#30697;&#38453;&#20056;&#27861;&#23454;&#29616;&#39640;&#36136;&#37327;&#36924;&#36817;&#20272;&#35745;&#27010;&#29575;&#21644;&#27169;&#22411;&#25972;&#20307;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#36923;&#36753;&#22238;&#24402;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#20108;&#20998;&#31867;&#20219;&#21153;&#30340;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#12290;&#24403;&#35266;&#27979;&#25968;&#37327;&#36828;&#36828;&#36229;&#36807;&#39044;&#27979;&#21464;&#37327;&#25968;&#37327;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#38543;&#26426;&#25277;&#26679;&#30340;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#31639;&#27861;&#65292;&#20445;&#35777;&#39640;&#36136;&#37327;&#36924;&#36817;&#20272;&#35745;&#27010;&#29575;&#21644;&#27169;&#22411;&#25972;&#20307;&#24046;&#24322;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24314;&#31435;&#22312;&#20004;&#20010;&#31616;&#21333;&#30340;&#32467;&#26500;&#26465;&#20214;&#22522;&#30784;&#19978;&#65292;&#36825;&#20004;&#20010;&#26465;&#20214;&#21487;&#24402;&#32467;&#20026;&#38543;&#26426;&#30697;&#38453;&#20056;&#27861;&#65292;&#26159;&#38543;&#26426;&#21270;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#30340;&#22522;&#26412;&#19988;&#28145;&#20837;&#29702;&#35299;&#30340;&#22522;&#20803;&#12290;&#24403;&#21033;&#29992;&#26464;&#26438;&#20998;&#25968;&#23545;&#35266;&#27979;&#36827;&#34892;&#25277;&#26679;&#26102;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36923;&#36753;&#22238;&#24402;&#30340;&#20272;&#35745;&#27010;&#29575;&#23646;&#24615;&#65292;&#24182;&#35777;&#26126;&#20934;&#30830;&#36924;&#36817;&#21487;&#20197;&#36890;&#36807;&#36828;&#23567;&#20110;&#24635;&#35266;&#27979;&#25968;&#30340;&#26679;&#26412;&#23454;&#29616;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16326v1 Announce Type: cross  Abstract: In statistics and machine learning, logistic regression is a widely-used supervised learning technique primarily employed for binary classification tasks. When the number of observations greatly exceeds the number of predictor variables, we present a simple, randomized sampling-based algorithm for logistic regression problem that guarantees high-quality approximations to both the estimated probabilities and the overall discrepancy of the model. Our analysis builds upon two simple structural conditions that boil down to randomized matrix multiplication, a fundamental and well-understood primitive of randomized numerical linear algebra. We analyze the properties of estimated probabilities of logistic regression when leverage scores are used to sample observations, and prove that accurate approximations can be achieved with a sample whose size is much smaller than the total number of observations. To further validate our theoretical findi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;ArEEG_Chars&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.15733</link><description>&lt;p&gt;
ArEEG_Chars: &#29992;&#20110;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#35774;&#24819;&#35821;&#38899;&#35782;&#21035;&#30340;&#38463;&#25289;&#20271;&#23383;&#31526;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15733
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;ArEEG_Chars&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#26159;&#36817;&#24180;&#26469;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#21487;&#20197;&#24110;&#21161;&#30251;&#30186;&#24739;&#32773;&#25913;&#21892;&#29983;&#27963;&#12290;&#26377;&#20960;&#39033;&#30740;&#31350;&#33258;&#21160;&#23558;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#20998;&#31867;&#20026;&#33521;&#25991;&#23383;&#31526;&#21644;&#21333;&#35789;&#12290;&#38463;&#25289;&#20271;&#35821;&#26159;&#19990;&#30028;&#19978;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#35821;&#35328;&#20043;&#19968;&#12290;&#28982;&#32780;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#38024;&#23545;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;&#33041;&#30005;&#22270;&#20449;&#21495;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;&#65292;&#24182;&#21629;&#21517;&#20026;ArEEG_Chars&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;ArEEG_Chars&#36827;&#34892;&#20102;&#22810;&#39033;&#23454;&#39564;&#12290;&#22312;&#20351;&#29992;LSTM&#26102;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#12290;ArEEG_Chars&#25968;&#25454;&#38598;&#23558;&#23545;&#30740;&#31350;&#20154;&#21592;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15733v1 Announce Type: cross  Abstract: Brain-Computer-Interface (BCI) has been a hot research topic in the last few years that could help paralyzed people in their lives. Several researches were done to classify electroencephalography (EEG) signals automatically into English characters and words. Arabic language is one of the most used languages around the world. However, to the best of our knowledge, there is no dataset for Arabic characters EEG signals. In this paper, we have created an EEG dataset for Arabic characters and named it ArEEG_Chars. Moreover, several experiments were done on ArEEG_Chars using deep learning. Best results were achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be public for researchers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#40657;&#30418;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#30340;&#34892;&#20026;&#21644;&#24213;&#23618;&#25299;&#25169;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#19978;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#30340;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11686</link><description>&lt;p&gt;
&#23398;&#20064;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning the Topology and Behavior of Discrete Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#40657;&#30418;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#30340;&#34892;&#20026;&#21644;&#24213;&#23618;&#25299;&#25169;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#19978;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#30340;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#36890;&#24120;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;&#19978;&#20256;&#26579;&#30340;&#20256;&#25773;&#12290;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#22312;PAC&#26694;&#26550;&#19979;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#24213;&#23618;&#32593;&#32476;&#30340;&#26465;&#20214;&#19979;&#23398;&#20064;&#31995;&#32479;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#22659;&#65306;&#23398;&#20064;&#19968;&#20010;&#40657;&#30418;&#31995;&#32479;&#30340;&#34892;&#20026;&#21644;&#24213;&#23618;&#25299;&#25169;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#12290;&#22312;&#31215;&#26497;&#30340;&#19968;&#38754;&#65292;&#25105;&#20204;&#22312;PAC&#27169;&#22411;&#19979;&#38024;&#23545;&#26576;&#20123;&#31867;&#23646;&#20110;&#30340;&#21160;&#21147;&#31995;&#32479;&#22270;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#25918;&#26494;&#30340;&#24773;&#22659;&#65292;&#20854;&#20013;&#26410;&#30693;&#31995;&#32479;&#30340;&#25299;&#25169;&#32467;&#26500;&#37096;&#20998;&#21487;&#35266;&#27979;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;PAC&#23398;&#20064;&#22120;&#26469;&#25512;&#26029;&#31995;&#32479;&#24182;&#30830;&#23450;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#21160;&#21147;&#31995;&#32479;&#30340;&#20551;&#35774;&#31867;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#34892;&#20026;&#21644;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11686v1 Announce Type: new  Abstract: Discrete dynamical systems are commonly used to model the spread of contagions on real-world networks. Under the PAC framework, existing research has studied the problem of learning the behavior of a system, assuming that the underlying network is known. In this work, we focus on a more challenging setting: to learn both the behavior and the underlying topology of a black-box system. We show that, in general, this learning problem is computationally intractable. On the positive side, we present efficient learning methods under the PAC model when the underlying graph of the dynamical system belongs to some classes. Further, we examine a relaxed setting where the topology of an unknown system is partially observed. For this case, we develop an efficient PAC learner to infer the system and establish the sample complexity. Lastly, we present a formal analysis of the expressive power of the hypothesis class of dynamical systems where both the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RS-DPO&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65292;&#24182;&#20174;&#35813;&#27169;&#22411;&#20013;&#30452;&#25509;&#37319;&#26679;&#21709;&#24212;&#65292;RS-DPO&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#65292;RS-DPO&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;RLHF&#12290;</title><link>https://arxiv.org/abs/2402.10038</link><description>&lt;p&gt;
RS-DPO&#65306;&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28151;&#21512;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RS-DPO&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#25298;&#32477;&#37319;&#26679;&#21644;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65292;&#24182;&#20174;&#35813;&#27169;&#22411;&#20013;&#30452;&#25509;&#37319;&#26679;&#21709;&#24212;&#65292;RS-DPO&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#65292;RS-DPO&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;RLHF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;RLHF&#26377;&#26102;&#19981;&#31283;&#23450;&#65292;&#38656;&#35201;&#26174;&#33879;&#30340;&#36229;&#21442;&#25968;&#24494;&#35843;&#65292;&#24182;&#19988;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#65288;DPO&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;DPO&#20381;&#36182;&#20110;&#20174;&#20154;&#31867;&#26631;&#27880;&#32773;&#21644;&#26367;&#20195;LLM&#29983;&#25104;&#30340;&#23545;&#27604;&#22238;&#22797;&#65292;&#32780;&#19981;&#26159;&#31574;&#30053;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;RLHF&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#22320;&#32467;&#21512;&#25298;&#32477;&#37319;&#26679;&#65288;RS&#65289;&#21644;DPO&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;RS-DPO&#65292;&#39318;&#20808;&#24320;&#21457;&#20986;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#31574;&#30053;&#27169;&#22411;&#65288;SFT&#65289;&#12290;&#28982;&#21518;&#30452;&#25509;&#20174;SFT&#27169;&#22411;&#20013;&#37319;&#26679;&#27599;&#20010;&#25552;&#31034;&#30340;k&#20010;&#21709;&#24212;&#12290;RS-DPO&#22522;&#20110;&#20854;&#30456;&#20284;&#24230;&#35782;&#21035;&#23545;&#27604;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10038v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31215;&#20998;&#39033;&#34917;&#20607;&#20108;&#27425;&#22870;&#21169;&#20989;&#25968;&#31283;&#24577;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#38271;&#26399;&#22870;&#21169;&#30340;&#32771;&#34385;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#31995;&#32479;&#24615;&#33021;&#65288;&#22914;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#21644;&#21464;&#36947;&#27169;&#22411;&#65289;&#20013;&#30340;&#31283;&#24577;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09075</link><description>&lt;p&gt;
&#29992;&#20110;&#20855;&#26377;&#20108;&#27425;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#31283;&#24577;&#35823;&#24046;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
Steady-State Error Compensation for Reinforcement Learning with Quadratic Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31215;&#20998;&#39033;&#34917;&#20607;&#20108;&#27425;&#22870;&#21169;&#20989;&#25968;&#31283;&#24577;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#38271;&#26399;&#22870;&#21169;&#30340;&#32771;&#34385;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#31995;&#32479;&#24615;&#33021;&#65288;&#22914;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#21644;&#21464;&#36947;&#27169;&#22411;&#65289;&#20013;&#30340;&#31283;&#24577;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#36873;&#25321;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#24403;&#20351;&#29992;&#20108;&#27425;&#22870;&#21169;&#20989;&#25968;&#26102;&#65292;&#32463;&#24120;&#20250;&#20986;&#29616;&#31283;&#24577;&#35823;&#24046;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#26377;&#30340;&#20351;&#29992;&#32477;&#23545;&#20540;&#31867;&#22411;&#22870;&#21169;&#20989;&#25968;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#24448;&#24448;&#20250;&#22312;&#29305;&#23450;&#31995;&#32479;&#29366;&#24577;&#19979;&#24341;&#36215;&#36739;&#22823;&#27874;&#21160;&#65292;&#23548;&#33268;&#31361;&#28982;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#20837;&#31215;&#20998;&#39033;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#36825;&#20010;&#39033;&#31215;&#20998;&#36827;&#20108;&#27425;&#22870;&#21169;&#20989;&#25968;&#20013;&#65292;&#23545;RL&#31639;&#27861;&#36827;&#34892;&#31934;&#30830;&#35843;&#25972;&#65292;&#22686;&#24378;&#31995;&#32479;&#23545;&#38271;&#26399;&#22870;&#21169;&#30340;&#32771;&#34385;&#65292;&#20174;&#32780;&#32531;&#35299;&#31283;&#24577;&#35823;&#24046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#27169;&#22411;&#21644;&#21464;&#36947;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#21644;&#24615;&#33021;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#26377;&#25928;&#22320;&#20943;&#23567;&#20102;&#31283;&#24577;&#35823;&#24046;&#65292;&#36824;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09075v1 Announce Type: cross Abstract: The selection of a reward function in Reinforcement Learning (RL) has garnered significant attention because of its impact on system performance. Issues of steady-state error often manifest when quadratic reward functions are employed. Although existing solutions using absolute-value-type reward functions partially address this problem, they tend to induce substantial fluctuations in specific system states, leading to abrupt changes. In response to this challenge, this study proposes an approach that introduces an integral term. By integrating this term into quadratic-type reward functions, the RL algorithm is adeptly tuned, augmenting the system's consideration of long-term rewards and, consequently, alleviating concerns related to steady-state errors. Through experiments and performance evaluations on the Adaptive Cruise Control (ACC) model and lane change models, we validate that the proposed method not only effectively diminishes st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#65292;&#21033;&#29992;&#23884;&#22871;&#31890;&#23376;&#28388;&#27874;&#22120;&#21644;&#31435;&#20307;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#26469;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07868</link><description>&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#30340;&#23884;&#22871;&#31890;&#23376;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nesting Particle Filters for Experimental Design in Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#65292;&#21033;&#29992;&#23884;&#22871;&#31890;&#23376;&#28388;&#27874;&#22120;&#21644;&#31435;&#20307;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#26469;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#20132;&#25442;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20869;&#22806;SMC^2&#31639;&#27861;&#65292;&#20351;&#29992;&#23884;&#22871;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#65288;SMC&#65289;&#20272;&#35745;&#22120;&#26469;&#39044;&#27979;&#26399;&#26395;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21040;&#31890;&#23376;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;pMCMC&#65289;&#26694;&#26550;&#20013;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#19982;&#26368;&#36817;&#20381;&#36182;&#20110;&#20559;&#20272;&#35745;&#22120;&#26469;&#25674;&#38144;&#20808;&#21069;&#23398;&#20064;&#35774;&#35745;&#31574;&#30053;&#30340;&#25104;&#26412;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#19968;&#32452;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#20540;&#39564;&#35777;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach to Bayesian Experimental Design (BED) for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC^2 algorithm that uses a nested sequential Monte Carlo (SMC) estimator of the expected information gain and embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform gradient-based policy optimization. This is in contrast to recent approaches that rely on biased estimators of the expected information gain (EIG) to amortize the cost of experiments by learning a design policy in advance. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;3D&#28857;&#20113;&#19978;&#36827;&#34892;&#31232;&#30095;&#21367;&#31215;&#30340;GPU&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28857;&#20113;&#30340;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07710</link><description>&lt;p&gt;
&#22522;&#20110;CUDA&#30340;GPU&#20248;&#21270;&#31232;&#30095;&#21367;&#31215;&#22312;3D&#28857;&#20113;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;3D&#28857;&#20113;&#19978;&#36827;&#34892;&#31232;&#30095;&#21367;&#31215;&#30340;GPU&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28857;&#20113;&#30340;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24212;&#29992;&#26174;&#33879;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#23427;&#20204;&#24050;&#32463;&#25104;&#20026;&#28041;&#21450;&#32467;&#26500;&#21270;&#26684;&#32593;&#25968;&#25454;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#22914;&#22270;&#20687;&#20998;&#26512;&#21644;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LiDAR&#21644;3D&#20256;&#24863;&#22120;&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#20351;&#29992;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#23545;3D&#28857;&#20113;&#30340;&#20998;&#26512;&#38656;&#27714;&#20063;&#22686;&#21152;&#20102;&#12290;&#21033;&#29992;3D&#28857;&#20113;&#22312;&#21253;&#25324;&#29289;&#20307;&#35782;&#21035;&#21644;&#20998;&#21106;&#22312;&#20869;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19977;&#32500;&#29615;&#22659;&#20013;&#20107;&#29289;&#30340;&#31354;&#38388;&#25551;&#36848;&#12290;&#19982;&#29031;&#29255;&#19981;&#21516;&#65292;&#28857;&#20113;&#20855;&#26377;&#31232;&#30095;&#24615;&#21644;&#32570;&#20047;&#35268;&#21017;&#30340;&#26684;&#32593;&#65292;&#22240;&#27492;&#23384;&#22312;&#30528;&#29420;&#29305;&#30340;&#22788;&#29702;&#21644;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a significant increase in the utilization of deep learning methods, particularly convolutional neural networks (CNNs), which have emerged as the dominant approach in various domains that involve structured grid data, such as picture analysis and processing. Nevertheless, the exponential growth in the utilization of LiDAR and 3D sensors across many domains has resulted in an increased need for the analysis of 3D point clouds. The utilization of 3D point clouds is crucial in various applications, including object recognition and segmentation, as they offer a spatial depiction of things within a three-dimensional environment. In contrast to photos, point clouds exhibit sparsity and lack a regular grid, hence posing distinct processing and computational issues.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30456;&#23545;&#32489;&#25928;&#26631;&#20934;&#19979;&#30340;&#26368;&#20248;&#25237;&#36164;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21069;&#21521;-&#21518;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#32435;&#20160;&#22343;&#34913;&#29305;&#24449;&#21644;&#38543;&#26426;&#24494;&#20998;&#28216;&#25103;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25968;&#20540;&#23454;&#39564;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#37329;&#34701;&#27169;&#22411;&#19978;&#36827;&#34892;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#20132;&#20114;&#32467;&#26500;&#30340;&#22270;&#24418;&#22270;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.07365</link><description>&lt;p&gt;
&#30456;&#23545;&#32489;&#25928;&#26631;&#20934;&#19979;&#30340;&#24322;&#36136;&#20195;&#29702;&#20154;&#26368;&#20248;&#25237;&#36164;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Method for Optimal Investment Under Relative Performance Criteria Among Heterogeneous Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30456;&#23545;&#32489;&#25928;&#26631;&#20934;&#19979;&#30340;&#26368;&#20248;&#25237;&#36164;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21069;&#21521;-&#21518;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#32435;&#20160;&#22343;&#34913;&#29305;&#24449;&#21644;&#38543;&#26426;&#24494;&#20998;&#28216;&#25103;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25968;&#20540;&#23454;&#39564;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#37329;&#34701;&#27169;&#22411;&#19978;&#36827;&#34892;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#20132;&#20114;&#32467;&#26500;&#30340;&#22270;&#24418;&#22270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Graphon&#28216;&#25103;&#26469;&#30740;&#31350;&#36890;&#36807;&#19968;&#24352;&#20132;&#20114;&#30340;&#21152;&#26435;&#22270;&#30340;&#35768;&#22810;&#29609;&#23478;&#30340;&#28216;&#25103;&#12290;&#36890;&#36807;&#21462;&#26497;&#38480;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#20855;&#26377;&#36830;&#32493;&#29609;&#23478;&#30340;&#28216;&#25103;&#65292;&#20854;&#20013;&#30340;&#20132;&#20114;&#26159;&#36890;&#36807;&#19968;&#20010;&#22270;&#24418;&#22270;&#12290;&#26412;&#25991;&#20851;&#27880;&#30456;&#23545;&#32489;&#25928;&#26631;&#20934;&#19979;&#30340;&#22270;&#24418;&#28216;&#25103;&#30340;&#26368;&#20248;&#25237;&#36164;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#22312;&#20004;&#20010;&#20851;&#38190;&#35201;&#32032;&#30340;&#22522;&#30784;&#19978;&#65306;&#39318;&#20808;&#65292;&#36890;&#36807;&#21069;&#21521;-&#21518;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#23545;&#32435;&#20160;&#22343;&#34913;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#20854;&#27425;&#65292;&#21033;&#29992;&#38543;&#26426;&#24494;&#20998;&#28216;&#25103;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#19981;&#21516;&#30340;&#37329;&#34701;&#27169;&#22411;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#22312;&#27599;&#20010;&#27169;&#22411;&#20013;&#27604;&#36739;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#22270;&#24418;&#22270;&#23545;&#20132;&#20114;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphon games have been introduced to study games with many players who interact through a weighted graph of interaction. By passing to the limit, a game with a continuum of players is obtained, in which the interactions are through a graphon. In this paper, we focus on a graphon game for optimal investment under relative performance criteria, and we propose a deep learning method. The method builds upon two key ingredients: first, a characterization of Nash equilibria by forward-backward stochastic differential equations and, second, recent advances of machine learning algorithms for stochastic differential games. We provide numerical experiments on two different financial models. In each model, we compare the effect of several graphons, which correspond to different structures of interactions.
&lt;/p&gt;</description></item><item><title>X-LoRA&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#27169;&#22411;&#24182;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#39046;&#22495;&#24212;&#29992;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#21463;&#21040;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#12290;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#21363;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07148</link><description>&lt;p&gt;
X-LoRA: &#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07148
&lt;/p&gt;
&lt;p&gt;
X-LoRA&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#27169;&#22411;&#24182;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#39046;&#22495;&#24212;&#29992;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#21463;&#21040;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#12290;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#21363;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#23618;&#36880;&#23618;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#26032;&#39062;&#39044;&#35757;&#32451;&#36866;&#37197;&#22120;&#30340;&#28151;&#21512;&#19987;&#23478;&#31574;&#30053;&#65292;&#29992;&#20110;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#34255;&#29366;&#24577;&#21160;&#24577;&#28151;&#21512;&#32463;&#36807;&#36866;&#24212;&#30340;&#23618;&#30340;&#38376;&#25511;&#31574;&#30053;&#65292;&#20801;&#35768;&#24471;&#21040;&#30340;X-LoRA&#27169;&#22411;&#21033;&#29992;&#19981;&#21516;&#30340;&#33021;&#21147;&#24182;&#21019;&#24314;&#20197;&#21069;&#26410;&#20351;&#29992;&#30340;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#12290;&#35813;&#35774;&#35745;&#21463;&#21040;&#20102;&#29983;&#29289;&#26222;&#36941;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#22359;&#22312;&#19981;&#21516;&#30340;&#20998;&#23618;&#34920;&#31034;&#20013;&#34987;&#37325;&#22797;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;X-LoRA&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;X-LoRA&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21253;&#25324;&#21069;&#21521;/&#36870;&#21521;&#20998;&#26512;&#20219;&#21153;&#21644;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#22312;&#20869;&#30340;&#31185;&#23398;&#33021;&#21147;&#65292;&#37325;&#28857;&#26159;&#29983;&#29289;&#26448;&#26009;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose a gating strategy that uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations of adaptations are established to solve specific tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis,
&lt;/p&gt;</description></item><item><title>TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04616</link><description>&lt;p&gt;
TinyLLM: &#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#20010;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TinyLLM: Learning a Small Student from Multiple Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04616
&lt;/p&gt;
&lt;p&gt;
TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26356;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#31227;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#36739;&#23567;&#30340;LLMs&#26356;&#28789;&#27963;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;&#22312;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#30693;&#35782;&#33976;&#39311;&#22240;&#20854;&#20986;&#33394;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#20419;&#36827;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TinyLLM&#65292;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#25945;&#24072;LLMs&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;LLM&#30340;&#26032;&#22411;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#40723;&#21169;&#23398;&#29983;LLM&#19981;&#20165;&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#32780;&#19988;&#29702;&#35299;&#36825;&#20123;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;&#37492;&#20110;&#19981;&#21516;&#30340;LLMs&#20855;&#26377;&#19981;&#21516;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#21560;&#25910;&#26469;&#33258;&#22810;&#20010;&#25945;&#24072;LLMs&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#32769;&#24072;&#24378;&#21046;&#27169;&#22359;...
&lt;/p&gt;
&lt;p&gt;
Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)</title><link>https://arxiv.org/abs/2402.04154</link><description>&lt;p&gt;
&#12298;&#35835;&#29609;&#28216;&#25103;&#65288;R2-Play&#65289;: &#22810;&#27169;&#24577;&#28216;&#25103;&#25351;&#23548;&#19979;&#30340;&#20915;&#31574; Transformer&#12299;
&lt;/p&gt;
&lt;p&gt;
Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24320;&#21457;&#19968;&#27454;&#36890;&#29992;&#26234;&#33021;&#20307;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30446;&#26631;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#20219;&#21153;&#30340;&#22823;&#37327;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#25193;&#23637;&#21040;&#26032;&#20219;&#21153;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#25972;&#21512;&#21040;&#20915;&#31574;&#32593;&#32476;&#20013;&#65292;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#20165;&#20381;&#36182;&#20110;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#23545;&#20110;&#20934;&#30830;&#20256;&#36798;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#22686;&#24378;&#26234;&#33021;&#20307;&#20219;&#21153;&#25351;&#23548;&#30340;&#24418;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#65292;&#20174;&#32780;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#38271;&#26399;&#35270;&#35273;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;
Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#65292;&#24182;&#23558;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#22240;&#26524;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02277</link><description>&lt;p&gt;
&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#36890;&#36807;&#22806;&#28304;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Bayesian Optimization via Exogenous Distribution Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#65292;&#24182;&#23558;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#22240;&#26524;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#23558;&#30446;&#26631;&#21464;&#37327;&#26368;&#22823;&#21270;&#20316;&#20026;&#25805;&#20316;&#30446;&#26631;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CBO&#65289;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#25913;&#21464;&#22240;&#26524;&#32467;&#26500;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#30828;&#24178;&#39044;&#65292;&#35201;&#20040;&#24341;&#20837;&#21160;&#20316;&#33410;&#28857;&#21040;&#20869;&#29983;&#21464;&#37327;&#20013;&#65292;&#20197;&#35843;&#25972;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#20197;&#23454;&#29616;&#30446;&#26631;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#36825;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#36890;&#24120;&#34987;&#24573;&#30053;&#25110;&#36890;&#36807;&#26399;&#26395;&#36827;&#34892;&#36793;&#32536;&#21270;&#12290;&#22806;&#28304;&#20998;&#24067;&#23398;&#20064;&#25552;&#39640;&#20102;&#36890;&#24120;&#36890;&#36807;&#26377;&#38480;&#35266;&#27979;&#25968;&#25454;&#35757;&#32451;&#30340;&#20195;&#29702;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#21040;&#30340;&#22806;&#28304;&#20998;&#24067;&#23558;&#29616;&#26377;&#30340;CBO&#25193;&#23637;&#21040;&#36229;&#20986;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65288;ANM&#65289;&#30340;&#19968;&#33324;&#22240;&#26524;&#26041;&#26696;&#12290;&#24674;&#22797;&#22806;&#28304;&#21464;&#37327;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#22122;&#22768;&#25110;&#26410;&#35266;&#27979;&#21040;&#30340;&#38544;&#34255;&#21464;&#37327;&#20351;&#29992;&#26356;&#28789;&#27963;&#30340;&#20808;&#39564;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;CBO&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximizing a target variable as an operational objective in a structured causal model is an important problem. Existing Causal Bayesian Optimization (CBO) methods either rely on hard interventions that alter the causal structure to maximize the reward; or introduce action nodes to endogenous variables so that the data generation mechanisms are adjusted to achieve the objective. In this paper, a novel method is introduced to learn the distribution of exogenous variables, which is typically ignored or marginalized through expectation by existing methods.   Exogenous distribution learning improves the approximation accuracy of structured causal models in a surrogate model that is usually trained with limited observational data. Moreover, the learned exogenous distribution extends existing CBO to general causal schemes beyond Additive Noise Models (ANM). The recovery of exogenous variables allows us to use a more flexible prior for noise or unobserved hidden variables. A new CBO method is 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01858</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Explaining latent representations of generative models with large multimodal models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#29983;&#25104;&#28508;&#22312;&#22240;&#32032;&#34920;&#31034;&#26159;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#23427;&#21487;&#20197;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#20197;&#29983;&#25104;&#31572;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37327;&#21270;&#35780;&#20272;&#20102;&#25105;&#20204;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#22810;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20043;&#38388;&#35780;&#20272;&#20102;&#35299;&#37322;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#23450;&#24615;&#22320;&#21487;&#35270;&#21270;&#20102;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#23398;&#20064;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#23545;&#35299;&#37322;&#30340;&#35299;&#32544;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
&lt;/p&gt;</description></item><item><title>SPO&#31639;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;KL&#25955;&#24230;&#21098;&#20999;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;PPO&#30340;&#20027;&#27969;&#21464;&#20307;&#65292;&#22312;Atari 2600&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#12289;&#26497;&#20302;&#30340;KL&#25955;&#24230;&#21644;&#26356;&#39640;&#30340;&#31574;&#30053;&#29109;&#65292;&#19988;&#23545;&#32593;&#32476;&#28145;&#24230;&#25110;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.16025</link><description>&lt;p&gt;
&#31616;&#21333;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simple Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16025
&lt;/p&gt;
&lt;p&gt;
SPO&#31639;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;KL&#25955;&#24230;&#21098;&#20999;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;PPO&#30340;&#20027;&#27969;&#21464;&#20307;&#65292;&#22312;Atari 2600&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#12289;&#26497;&#20302;&#30340;KL&#25955;&#24230;&#21644;&#26356;&#39640;&#30340;&#31574;&#30053;&#29109;&#65292;&#19988;&#23545;&#32593;&#32476;&#28145;&#24230;&#25110;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PPO&#65288;Proximal Policy Optimization&#65289;&#31639;&#27861;&#22312;&#35768;&#22810;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#34987;&#35748;&#20026;&#26159;TRPO&#65288;Trust Region Policy Optimization&#65289;&#31639;&#27861;&#30340;&#31616;&#21270;&#29256;&#26412;&#12290;&#28982;&#32780;&#65292;PPO&#20013;&#30340;&#27604;&#29575;&#21098;&#20999;&#25805;&#20316;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#22320;&#24378;&#21046;&#25191;&#34892;&#20449;&#20219;&#21306;&#22495;&#32422;&#26463;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21098;&#20999;&#26041;&#27861;&#65292;&#21363;Simple Policy Optimization&#65288;SPO&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#26087;&#31574;&#30053;&#21644;&#24403;&#21069;&#31574;&#30053;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#22312;Atari 2600&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;PPO&#30340;&#20027;&#27969;&#21464;&#20307;&#30456;&#27604;&#65292;SPO&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#26497;&#20302;&#30340;KL&#25955;&#24230;&#21644;&#26356;&#39640;&#30340;&#31574;&#30053;&#29109;&#65292;&#24182;&#19988;&#23545;&#32593;&#32476;&#28145;&#24230;&#25110;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;SPO&#20445;&#25345;&#20102;&#26080;&#32422;&#26463;&#19968;&#38454;&#31639;&#27861;&#30340;&#31616;&#21333;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16025v2 Announce Type: replace  Abstract: PPO (Proximal Policy Optimization) algorithm has demonstrated excellent performance in many fields, and it is considered as a simple version of TRPO (Trust Region Policy Optimization) algorithm. However, the ratio clipping operation in PPO may not always effectively enforce the trust region constraints, this can be a potential factor affecting the stability of the algorithm. In this paper, we propose Simple Policy Optimization (SPO) algorithm, which introduces a novel clipping method for KL divergence between the old and current policies. Extensive experimental results in Atari 2600 environments indicate that, compared to the mainstream variants of PPO, SPO achieves better sample efficiency, extremely low KL divergence, and higher policy entropy, and is robust to the increase in network depth or complexity. More importantly, SPO maintains the simplicity of an unconstrained first-order algorithm. Code is available at https://github.co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#32972;&#26223;&#20559;&#35265;&#30340;&#26356;&#24555;ISNet&#65292;&#36890;&#36807;&#37325;&#26032;&#26500;&#24314;&#26550;&#26500;&#20197;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#24341;&#20837;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;LRP&#23454;&#29616;&#65292;&#25104;&#21151;&#38459;&#27490;&#20102;&#32972;&#26223;&#27880;&#24847;&#21147;&#21644;&#24555;&#25463;&#23398;&#20064;&#65292;&#20248;&#20110;&#22810;&#20010;&#26368;&#26032;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.08409</link><description>&lt;p&gt;
&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32972;&#26223;&#20559;&#24046;&#32531;&#35299;&#30340;&#26356;&#24555;ISNet
&lt;/p&gt;
&lt;p&gt;
Faster ISNet for Background Bias Mitigation on Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08409
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32531;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#32972;&#26223;&#20559;&#35265;&#30340;&#26356;&#24555;ISNet&#65292;&#36890;&#36807;&#37325;&#26032;&#26500;&#24314;&#26550;&#26500;&#20197;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#24341;&#20837;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;LRP&#23454;&#29616;&#65292;&#25104;&#21151;&#38459;&#27490;&#20102;&#32972;&#26223;&#27880;&#24847;&#21147;&#21644;&#24555;&#25463;&#23398;&#20064;&#65292;&#20248;&#20110;&#22810;&#20010;&#26368;&#26032;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#32972;&#26223;&#20013;&#30340;&#20559;&#35265;&#25110;&#20266;&#30456;&#20851;&#24615;&#21487;&#33021;&#20250;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#65292;&#23548;&#33268;&#24555;&#25463;&#23398;&#20064;&#65288;&#32874;&#26126;&#30340;&#27721;&#26031;&#25928;&#24212;&#65289;&#24182;&#38459;&#30861;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#27867;&#21270;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;ISNet&#26550;&#26500;&#25552;&#20986;&#20102;&#20248;&#21270;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65292;&#19968;&#31181;&#35299;&#37322;&#25216;&#26415;&#65289;&#28909;&#22270;&#65292;&#20197;&#20943;&#36731;&#32972;&#26223;&#23545;&#28145;&#24230;&#20998;&#31867;&#22120;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;ISNet&#30340;&#35757;&#32451;&#26102;&#38388;&#19982;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#31867;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32463;&#37325;&#26032;&#26500;&#24314;&#30340;&#26550;&#26500;&#65292;&#20854;&#35757;&#32451;&#26102;&#38388;&#19981;&#20877;&#20381;&#36182;&#20110;&#36825;&#20010;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#26126;&#19988;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;LRP&#23454;&#29616;&#12290;&#25105;&#20204;&#21033;&#29992;&#21512;&#25104;&#32972;&#26223;&#20559;&#35265;&#21644;&#33016;&#37096;X&#20809;&#20013;&#30340;COVID-19&#26816;&#27979;&#25361;&#25112;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#36890;&#24120;&#23384;&#22312;&#32972;&#26223;&#20559;&#35265;&#12290;&#36825;&#20123;&#32593;&#32476;&#38459;&#30861;&#20102;&#32972;&#26223;&#27880;&#24847;&#21147;&#21644;&#24555;&#25463;&#23398;&#20064;&#65292;&#36229;&#36234;&#20102;&#22810;&#20010;&#26368;&#26032;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08409v2 Announce Type: replace-cross  Abstract: Bias or spurious correlations in image backgrounds can impact neural networks, causing shortcut learning (Clever Hans Effect) and hampering generalization to real-world data. ISNet, a recently introduced architecture, proposed the optimization of Layer-Wise Relevance Propagation (LRP, an explanation technique) heatmaps, to mitigate the influence of backgrounds on deep classifiers. However, ISNet's training time scales linearly with the number of classes in an application. Here, we propose reformulated architectures whose training time becomes independent from this number. Additionally, we introduce a concise and model-agnostic LRP implementation. We challenge the proposed architectures using synthetic background bias, and COVID-19 detection in chest X-rays, an application that commonly presents background bias. The networks hindered background attention and shortcut learning, surpassing multiple state-of-the-art models on out-o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#29366;&#24577;&#31934;&#28860;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25581;&#31034;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24433;&#21709;&#26234;&#33021;&#20307;&#30446;&#26631;&#30340;&#22870;&#21169;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.00104</link><description>&lt;p&gt;
&#22240;&#26524;&#29366;&#24577;&#31934;&#28860;&#29992;&#20110;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal State Distillation for Explainable Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#29366;&#24577;&#31934;&#28860;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25581;&#31034;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24433;&#21709;&#26234;&#33021;&#20307;&#30446;&#26631;&#30340;&#22870;&#21169;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#35757;&#32451;&#26234;&#33021;&#20307;&#30340;&#24378;&#22823;&#25216;&#26415;&#65292;&#20294;&#29702;&#35299;&#36825;&#20123;&#26234;&#33021;&#20307;&#20026;&#20309;&#20570;&#20986;&#29305;&#23450;&#20915;&#31574;&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;RL&#27169;&#22411;&#30340;&#19981;&#36879;&#26126;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#25143;&#38590;&#20197;&#29702;&#35299;&#26234;&#33021;&#20307;&#34892;&#20026;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#29366;&#24577;&#31934;&#28860;&#65288;Causal State Distillation&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20811;&#26381;&#22870;&#21169;&#20998;&#35299;&#31561;&#20854;&#20182;&#26041;&#27861;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25581;&#31034;&#20986;&#23545;&#26234;&#33021;&#20307;&#30446;&#26631;&#20135;&#29983;&#24433;&#21709;&#30340;&#22870;&#21169;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00104v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a powerful technique for training intelligent agents, but understanding why these agents make specific decisions can be quite challenging. This lack of transparency in RL models has been a long-standing problem, making it difficult for users to grasp the reasons behind an agent's behaviour. Various approaches have been explored to address this problem, with one promising avenue being reward decomposition (RD). RD is appealing as it sidesteps some of the concerns associated with other methods that attempt to rationalize an agent's behaviour in a post-hoc manner. RD works by exposing various facets of the rewards that contribute to the agent's objectives during training. However, RD alone has limitations as it primarily offers insights based on sub-rewards and does not delve into the intricate cause-and-effect relationships that occur within an RL agent's neural model. In this paper, we present an e
&lt;/p&gt;</description></item><item><title>&#22312;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#26679;&#26412;&#25968;&#37327;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20849;&#20139;&#34920;&#31034;&#36827;&#34892;&#20803;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2312.13978</link><description>&lt;p&gt;
&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#26679;&#26412;&#25968;&#37327;&#24456;&#23569;&#30340;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Metalearning with Very Few Samples Per Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13978
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#26679;&#26412;&#25968;&#37327;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20849;&#20139;&#34920;&#31034;&#36827;&#34892;&#20803;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Metalearning&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26159;&#20004;&#31181;&#35299;&#20915;&#30456;&#20851;&#23398;&#20064;&#20219;&#21153;&#32452;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#27604;&#25105;&#20204;&#21333;&#29420;&#35299;&#20915;&#27599;&#20010;&#20219;&#21153;&#26356;&#39640;&#25928;&#12290; &#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#34987;&#32473;&#23450;&#19968;&#32452;&#30456;&#20851;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#38656;&#35201;&#20026;&#27599;&#20010;&#20219;&#21153;&#36755;&#20986;&#19968;&#20010;&#20934;&#30830;&#30340;&#27169;&#22411;&#65307;&#32780;&#22312;&#20803;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#34987;&#32473;&#23450;&#20174;&#20803;&#20998;&#24067;&#20013;&#29420;&#31435;&#21516;&#20998;&#24067;&#32472;&#21046;&#30340;&#20219;&#21153;&#65292;&#24182;&#38656;&#35201;&#36755;&#20986;&#19968;&#20123;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#19987;&#38376;&#29992;&#20110;&#20803;&#20998;&#24067;&#20013;&#26032;&#20219;&#21153;&#30340;&#20844;&#20849;&#20449;&#24687;&#12290; &#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#20108;&#20998;&#31867;&#35774;&#32622;&#65292;&#20219;&#21153;&#26159;&#30001;&#20849;&#20139;&#34920;&#31034;&#30456;&#20851;&#32852;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#27599;&#20010;&#20219;&#21153;$P$&#21487;&#20197;&#36890;&#36807;&#24418;&#24335;&#20026;$f_{P}\circ h$&#30340;&#20998;&#31867;&#22120;&#35299;&#20915;&#65292;&#20854;&#20013;$h \in H$&#26159;&#20174;&#29305;&#24449;&#21040;&#20849;&#20139;&#34920;&#31034;&#31354;&#38388;&#30340;&#26144;&#23556;&#65292;&#32780;$f_{P} \in F$&#26159;&#20174;&#34920;&#31034;&#31354;&#38388;&#21040;&#26631;&#31614;&#30340;&#20219;&#21153;&#29305;&#23450;&#20998;&#31867;&#22120;&#12290; &#25105;&#20204;&#20027;&#35201;&#25506;&#35752;&#30340;&#38382;&#39064;&#26159;&#25105;&#20204;&#38656;&#35201;&#22810;&#23569;&#25968;&#25454;&#26469;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.13978v2 Announce Type: replace  Abstract: Metalearning and multitask learning are two frameworks for solving a group of related learning tasks more efficiently than we could hope to solve each of the individual tasks on their own. In multitask learning, we are given a fixed set of related learning tasks and need to output one accurate model per task, whereas in metalearning we are given tasks that are drawn i.i.d. from a metadistribution and need to output some common information that can be easily specialized to new tasks from the metadistribution.   We consider a binary classification setting where tasks are related by a shared representation, that is, every task $P$ can be solved by a classifier of the form $f_{P} \circ h$ where $h \in H$ is a map from features to a representation space that is shared across tasks, and $f_{P} \in F$ is a task-specific classifier from the representation space to labels. The main question we ask is how much data do we need to metalearn a go
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#22788;&#29702;&#24322;&#24120;&#20540;&#30340;&#36817;&#20284;&#26368;&#20248;&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;&#65292;&#36890;&#36807;&#24658;&#23450;&#36817;&#20284;1&#20013;&#24515;&#21644;1&#22343;&#20540;&#32858;&#31867;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24230;&#37327;&#26631;&#20934;&#20248;&#21270;&#26041;&#27861;</title><link>https://arxiv.org/abs/2312.12835</link><description>&lt;p&gt;
&#20351;&#29992;1&#20013;&#24515;&#21644;1&#22343;&#20540;&#32858;&#31867;&#22788;&#29702;&#24322;&#24120;&#20540;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#36817;&#20284;&#26368;&#20248;&#30340;&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Resilient Aggregation Rules for Distributed Learning Using 1-Center and 1-Mean Clustering with Outliers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#22788;&#29702;&#24322;&#24120;&#20540;&#30340;&#36817;&#20284;&#26368;&#20248;&#40065;&#26834;&#32858;&#21512;&#35268;&#21017;&#65292;&#36890;&#36807;&#24658;&#23450;&#36817;&#20284;1&#20013;&#24515;&#21644;1&#22343;&#20540;&#32858;&#31867;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24230;&#37327;&#26631;&#20934;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25308;&#21344;&#24237;&#26426;&#22120;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#23398;&#20064;&#31995;&#32479;&#20013;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#19981;&#21487;&#39044;&#30693;&#30340;&#25925;&#38556;&#21487;&#33021;&#20250;&#23548;&#33268;&#31995;&#32479;&#23849;&#28291;&#12290;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#65292;&#23454;&#29616;&#23545;&#25239;&#25308;&#21344;&#24237;&#26426;&#22120;&#30340;&#23433;&#20840;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#22312;&#20110;&#40065;&#26834;&#32858;&#21512;&#26426;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#32858;&#31867;&#30340;&#36817;&#20284;&#26368;&#20248;&#32858;&#21512;&#35268;&#21017;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25239;&#24322;&#24120;&#20540;&#30340;&#32858;&#31867;&#26041;&#27861;&#21033;&#29992;&#20102;&#24037;&#20316;&#32773;&#25552;&#20379;&#30340;&#26356;&#26032;&#21521;&#37327;&#30340;&#20960;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20855;&#26377;&#24322;&#24120;&#20540;&#30340;1&#20013;&#24515;&#21644;1&#22343;&#20540;&#32858;&#31867;&#38382;&#39064;&#30340;&#24658;&#23450;&#36817;&#20284;&#35299;&#25552;&#20379;&#20102;&#22312;&#24230;&#37327;&#26631;&#20934;&#19978;&#36817;&#20284;&#26368;&#20248;&#30340;&#40065;&#26834;&#32858;&#21512;&#22120;&#65292;&#36825;&#22312;&#22343;&#21248;&#21644;&#38750;&#22343;&#21248;&#24773;&#20917;&#19979;&#37117;&#34987;&#35777;&#26126;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12835v2 Announce Type: replace  Abstract: Byzantine machine learning has garnered considerable attention in light of the unpredictable faults that can occur in large-scale distributed learning systems. The key to secure resilience against Byzantine machines in distributed learning is resilient aggregation mechanisms. Although abundant resilient aggregation rules have been proposed, they are designed in ad-hoc manners, imposing extra barriers on comparing, analyzing, and improving the rules across performance criteria. This paper studies near-optimal aggregation rules using clustering in the presence of outliers. Our outlier-robust clustering approach utilizes geometric properties of the update vectors provided by workers. Our analysis show that constant approximations to the 1-center and 1-mean clustering problems with outliers provide near-optimal resilient aggregators for metric-based criteria, which have been proven to be crucial in the homogeneous and heterogeneous cases
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#26041;&#27861;&#23545;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#30340;&#39118;&#38505;&#22240;&#32032;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25490;&#21517;&#65292;&#20197;&#25552;&#39640;&#23545;&#20854;&#29702;&#35299;&#12289;&#20998;&#31867;&#21644;&#20248;&#20808;&#32771;&#34385;&#39044;&#38450;&#21644;&#27835;&#30103;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.11517</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#39118;&#38505;&#22240;&#32032;&#20998;&#31867;&#19982;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
A Natural Language Processing-Based Classification and Mode-Based Ranking of Musculoskeletal Disorder Risk Factors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#26041;&#27861;&#23545;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#30340;&#39118;&#38505;&#22240;&#32032;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25490;&#21517;&#65292;&#20197;&#25552;&#39640;&#23545;&#20854;&#29702;&#35299;&#12289;&#20998;&#31867;&#21644;&#20248;&#20808;&#32771;&#34385;&#39044;&#38450;&#21644;&#27835;&#30103;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#65288;MSD&#65289;&#39118;&#38505;&#22240;&#32032;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#30456;&#32467;&#21512;&#12290;&#26088;&#22312;&#31934;&#32454;&#21270;&#29702;&#35299;&#12289;&#20998;&#31867;&#21644;&#20248;&#20808;&#32771;&#34385;&#38024;&#23545;&#24615;&#39044;&#38450;&#21644;&#27835;&#30103;&#12290;&#35780;&#20272;&#20102;&#20843;&#20010;NLP&#27169;&#22411;&#65292;&#32467;&#21512;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#12289;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#36317;&#31163;&#24230;&#37327;&#23558;&#22240;&#32032;&#20998;&#31867;&#20026;&#20010;&#20154;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#24037;&#20316;&#22330;&#25152;&#12289;&#24515;&#29702;&#21644;&#32452;&#32455;&#31561;&#31867;&#21035;&#12290;BERT&#19982;&#20313;&#24358;&#30456;&#20284;&#24230;&#36798;&#21040;28%&#30340;&#20934;&#30830;&#29575;&#65307;&#21477;&#23376;&#36716;&#25442;&#22120;&#19982;&#27431;&#27663;&#12289;&#24067;&#38647;&#26354;&#33922;&#26031;&#21644;&#38389;&#21487;&#22827;&#26031;&#22522;&#36317;&#31163;&#24471;&#20998;&#20026;100%&#12290;&#36890;&#36807;10&#20493;&#20132;&#21449;&#39564;&#35777;&#65292;&#32479;&#35745;&#26816;&#39564;&#30830;&#20445;&#40065;&#26834;&#32467;&#26524;&#12290;&#35843;&#26597;&#25968;&#25454;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#30830;&#23450;&#20102;&#20005;&#37325;&#24615;&#31561;&#32423;&#65292;&#19982;&#25991;&#29486;&#30456;&#19968;&#33268;&#12290;"&#24037;&#20316;&#23039;&#21183;"&#26159;&#26368;&#20005;&#37325;&#30340;&#65292;&#20984;&#26174;&#20102;&#23039;&#21183;&#30340;&#20316;&#29992;&#12290;&#35843;&#26597;&#32467;&#26524;&#24378;&#35843;&#20102;"&#24037;&#20316;&#19981;&#31283;&#23450;&#24615;"&#12289;"&#24037;&#20316;&#21162;&#21147;&#21644;&#22238;&#25253;&#19981;&#24179;&#34913;"&#21644;"&#21592;&#24037;&#35774;&#26045;&#24046;"&#31561;&#22240;&#32032;&#30340;&#26174;&#33879;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11517v3 Announce Type: replace  Abstract: This research delves into Musculoskeletal Disorder (MSD) risk factors, using a blend of Natural Language Processing (NLP) and mode-based ranking. The aim is to refine understanding, classification, and prioritization for focused prevention and treatment. Eight NLP models are evaluated, combining pre-trained transformers, cosine similarity, and distance metrics to categorize factors into personal, biomechanical, workplace, psychological, and organizational classes. BERT with cosine similarity achieves 28% accuracy; sentence transformer with Euclidean, Bray-Curtis, and Minkowski distances scores 100%. With 10-fold cross-validation, statistical tests ensure robust results. Survey data and mode-based ranking determine severity hierarchy, aligning with the literature. "Working posture" is the most severe, highlighting posture's role. Survey insights emphasize "Job insecurity," "Effort reward imbalance," and "Poor employee facility" as sig
&lt;/p&gt;</description></item><item><title>ComplexityNet&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;LLM&#25512;&#29702;&#25928;&#29575;&#65292;&#36890;&#36807;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#36755;&#20986;&#27010;&#29575;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;90%&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#24182;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#30830;&#23450;&#26041;&#38754;&#21462;&#24471;&#20102;79%&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.11511</link><description>&lt;p&gt;
ComplexityNet: &#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#22797;&#26434;&#24615;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11511
&lt;/p&gt;
&lt;p&gt;
ComplexityNet&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;LLM&#25512;&#29702;&#25928;&#29575;&#65292;&#36890;&#36807;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#36755;&#20986;&#27010;&#29575;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;90%&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#24182;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#30830;&#23450;&#26041;&#38754;&#21462;&#24471;&#20102;79%&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ComplexityNet&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#35780;&#20272;&#20219;&#21153;&#22797;&#26434;&#24615;&#32780;&#35774;&#35745;&#30340;&#31616;&#21270;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#19981;&#21516;&#33021;&#21147;&#30340;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#26469;&#39044;&#27979;&#20934;&#30830;&#36755;&#20986;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;Mostly Basic Python Problems&#65288;MBPP&#65289;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20102;ComplexityNet&#12290;&#25105;&#20204;&#24320;&#21019;&#24615;&#22320;&#21019;&#24314;&#20102;&#31532;&#19968;&#32452;&#26631;&#31614;&#26469;&#23450;&#20041;&#20219;&#21153;&#22797;&#26434;&#24615;&#12290;ComplexityNet&#22312;&#30830;&#23450;&#20219;&#21153;&#22797;&#26434;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;79%&#20934;&#30830;&#29575;&#65292;&#36739;&#21407;&#22987;&#12289;&#38750;&#24494;&#35843;&#27169;&#22411;&#30340;34%&#20934;&#30830;&#29575;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#19982;&#20351;&#29992;&#26368;&#39640;&#22797;&#26434;&#24615;&#27169;&#22411;&#30456;&#27604;&#65292;ComplexityNet&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;90%&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;86.7%&#30340;&#39640;&#20195;&#30721;&#29983;&#25104;&#20934;&#30830;&#29575;&#12290;&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#24494;&#35843;&#36739;&#23567;&#30340;&#27169;&#22411;&#26469;&#23545;&#20219;&#21153;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#26356;&#24179;&#34913;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11511v2 Announce Type: replace-cross  Abstract: We present ComplexityNet, a streamlined language model designed for assessing task complexity. This model predicts the likelihood of accurate output by various language models, each with different capabilities. Our initial application of ComplexityNet involves the Mostly Basic Python Problems (MBPP) dataset. We pioneered the creation of the first set of labels to define task complexity. ComplexityNet achieved a notable 79% accuracy in determining task complexity, a significant improvement over the 34% accuracy of the original, non fine-tuned model. Furthermore, ComplexityNet effectively reduces computational resource usage by 90% compared to using the highest complexity model, while maintaining a high code generation accuracy of 86.7%. This study demonstrates that fine-tuning smaller models to categorize tasks based on their complexity can lead to a more balanced trade-off between accuracy and efficiency in the use of Large Lan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#22522;&#30784;&#27169;&#22411;&#26694;&#26550;&#65288;CLOUDS&#65289;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#35821;&#20041;&#20998;&#21106;&#65292;&#38598;&#25104;&#20102;CLIP&#39592;&#24178;&#12289;&#29983;&#25104;&#27169;&#22411;&#21644;&#8220;Segment Anything Model&#8221;&#65292;&#20197;&#23454;&#29616;&#23545;&#21508;&#31181;&#30446;&#26631;&#20998;&#24067;&#27169;&#24335;&#30340;&#35206;&#30422;&#21644;&#39044;&#27979;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2312.09788</link><description>&lt;p&gt;
&#21327;&#20316;&#22522;&#30784;&#27169;&#22411;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Collaborating Foundation Models for Domain Generalized Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09788
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#22522;&#30784;&#27169;&#22411;&#26694;&#26550;&#65288;CLOUDS&#65289;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#35821;&#20041;&#20998;&#21106;&#65292;&#38598;&#25104;&#20102;CLIP&#39592;&#24178;&#12289;&#29983;&#25104;&#27169;&#22411;&#21644;&#8220;Segment Anything Model&#8221;&#65292;&#20197;&#23454;&#29616;&#23545;&#21508;&#31181;&#30446;&#26631;&#20998;&#24067;&#27169;&#24335;&#30340;&#35206;&#30422;&#21644;&#39044;&#27979;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#35821;&#20041;&#20998;&#21106;&#65288;DGSS&#65289;&#28041;&#21450;&#22312;&#26631;&#35760;&#30340;&#28304;&#22495;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;DGSS&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#22495;&#38543;&#26426;&#21270;&#65288;DR&#65289;&#23454;&#29616;&#24378;&#20581;&#29305;&#24449;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#21482;&#33021;&#32771;&#34385;&#26679;&#24335;&#22810;&#26679;&#24615;&#32780;&#19981;&#26159;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#27491;&#20132;&#30340;DGSS&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#19968;&#31995;&#21015;&#21327;&#20316;&#22522;&#30784;&#27169;&#22411;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#35821;&#20041;&#20998;&#21106;&#65288;CLOUDS&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CLOUDS&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#30340;&#26694;&#26550;&#65306;&#65288;i&#65289;CLIP&#39592;&#24178;&#29992;&#20110;&#20854;&#24378;&#20581;&#29305;&#24449;&#34920;&#31034;&#65292;&#65288;ii&#65289;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#20351;&#20869;&#23481;&#22810;&#26679;&#21270;&#65292;&#20174;&#32780;&#28085;&#30422;&#21487;&#33021;&#30446;&#26631;&#20998;&#24067;&#30340;&#21508;&#31181;&#27169;&#24335;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#20351;&#29992;&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#36845;&#20195;&#22320;&#25913;&#36827;&#20998;&#21106;&#27169;&#22411;&#39044;&#27979;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;CLOUDS&#34920;&#29616;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09788v2 Announce Type: replace-cross  Abstract: Domain Generalized Semantic Segmentation (DGSS) deals with training a model on a labeled source domain with the aim of generalizing to unseen domains during inference. Existing DGSS methods typically effectuate robust features by means of Domain Randomization (DR). Such an approach is often limited as it can only account for style diversification and not content. In this work, we take an orthogonal approach to DGSS and propose to use an assembly of CoLlaborative FOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In detail, CLOUDS is a framework that integrates FMs of various kinds: (i) CLIP backbone for its robust feature representation, (ii) generative models to diversify the content, thereby covering various modes of the possible target distribution, and (iii) Segment Anything Model (SAM) for iteratively refining the predictions of the segmentation model. Extensive experiments show that our CLOUDS excels
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Auto MC-Reward&#30340;&#20808;&#36827;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35774;&#35745;&#31264;&#23494;&#22870;&#21169;&#20989;&#25968;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;</title><link>https://arxiv.org/abs/2312.09238</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;Minecraft&#33258;&#21160;&#35774;&#35745;&#31264;&#23494;&#22870;&#21169;&#30340;Auto MC-Reward
&lt;/p&gt;
&lt;p&gt;
Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Auto MC-Reward&#30340;&#20808;&#36827;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35774;&#35745;&#31264;&#23494;&#22870;&#21169;&#20989;&#25968;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65288;&#20363;&#22914;Minecraft&#65289;&#20165;&#25552;&#20379;&#25351;&#31034;&#20219;&#21153;&#23436;&#25104;&#25110;&#22833;&#36133;&#30340;&#31232;&#30095;&#22870;&#21169;&#65292;&#36825;&#20123;&#22870;&#21169;&#20197;&#20108;&#36827;&#21046;&#20540;&#34920;&#31034;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#25506;&#32034;&#25928;&#29575;&#30340;&#25361;&#25112;&#20351;&#24471;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#29702;&#31243;&#24207;&#38590;&#20197;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Auto MC-Reward&#30340;&#20808;&#36827;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#35774;&#35745;&#31264;&#23494;&#22870;&#21169;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;Auto MC-Reward&#21253;&#25324;&#19977;&#20010;&#37325;&#35201;&#32452;&#20214;&#65306;&#22870;&#21169;&#35774;&#35745;&#32773;&#12289;&#22870;&#21169;&#35780;&#35770;&#23478;&#21644;&#36712;&#36857;&#20998;&#26512;&#22120;&#12290;&#32473;&#23450;&#29615;&#22659;&#20449;&#24687;&#21644;&#20219;&#21153;&#25551;&#36848;&#65292;&#22870;&#21169;&#35774;&#35745;&#32773;&#39318;&#20808;&#36890;&#36807;&#32534;&#20889;&#21487;&#25191;&#34892;&#30340;Python&#20989;&#25968;&#21644;&#39044;&#23450;&#20041;&#30340;&#35266;&#27979;&#36755;&#20837;&#26469;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#22870;&#21169;&#35780;&#35770;&#23478;&#23558;&#36127;&#36131;&#39564;&#35777;&#20195;&#30721;&#65292;&#26816;&#26597;&#20195;&#30721;&#26159;&#21542;&#33258;&#27965;&#19988;&#26080;&#35821;&#27861;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09238v2 Announce Type: replace  Abstract: Many reinforcement learning environments (e.g., Minecraft) provide only sparse rewards that indicate task completion or failure with binary values. The challenge in exploration efficiency in such environments makes it difficult for reinforcement-learning-based agents to learn complex tasks. To address this, this paper introduces an advanced learning system, named Auto MC-Reward, that leverages Large Language Models (LLMs) to automatically design dense reward functions, thereby enhancing the learning efficiency. Auto MC-Reward consists of three important components: Reward Designer, Reward Critic, and Trajectory Analyzer. Given the environment information and task descriptions, the Reward Designer first design the reward function by coding an executable Python function with predefined observation inputs. Then, our Reward Critic will be responsible for verifying the code, checking whether the code is self-consistent and free of syntax 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;OCTDL&#30340;&#24320;&#25918;&#33719;&#21462;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;2000&#24352;&#26631;&#35760;&#26377;&#30142;&#30149;&#32452;&#21644;&#35270;&#32593;&#33180;&#30149;&#29702;&#30340;OCT&#22270;&#20687;&#65292;&#26377;&#21161;&#20110;&#35786;&#26029;&#30524;&#37096;&#29366;&#20917;&#12290;</title><link>https://arxiv.org/abs/2312.08255</link><description>&lt;p&gt;
OCTDL&#65306;&#22522;&#20110;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08255
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;OCTDL&#30340;&#24320;&#25918;&#33719;&#21462;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;2000&#24352;&#26631;&#35760;&#26377;&#30142;&#30149;&#32452;&#21644;&#35270;&#32593;&#33180;&#30149;&#29702;&#30340;OCT&#22270;&#20687;&#65292;&#26377;&#21161;&#20110;&#35786;&#26029;&#30524;&#37096;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#25104;&#20687;&#25216;&#26415;&#65292;&#22312;&#30524;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;OCT&#21487;&#20197;&#21487;&#35270;&#21270;&#35270;&#32593;&#33180;&#23618;&#65292;&#23545;&#26089;&#26399;&#26816;&#27979;&#21644;&#30417;&#27979;&#35270;&#32593;&#33180;&#30142;&#30149;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#25918;&#33719;&#21462;&#30340;OCT&#25968;&#25454;&#38598;&#65288;OCTDL&#65289;&#65292;&#21253;&#25324;&#36229;&#36807;2000&#24352;&#26681;&#25454;&#30142;&#30149;&#32452;&#21644;&#35270;&#32593;&#33180;&#30149;&#29702;&#26631;&#35760;&#30340;OCT&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#24739;&#26377;&#32769;&#24180;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#12289;&#31958;&#23615;&#30149;&#40644;&#26001;&#27700;&#32959;&#65288;DME&#65289;&#12289;&#29627;&#29827;&#20307;&#35270;&#32593;&#33180;&#33180;&#65288;ERM&#65289;&#12289;&#35270;&#32593;&#33180;&#21160;&#33033;&#38381;&#22622;&#65288;RAO&#65289;&#12289;&#35270;&#32593;&#33180;&#38745;&#33033;&#38381;&#22622;&#65288;RVO&#65289;&#21644;&#29627;&#29827;&#20307;&#40644;&#26001;&#30028;&#38754;&#30142;&#30149;&#65288;VID&#65289;&#30340;&#24739;&#32773;&#30340;OCT&#35760;&#24405;&#12290;&#36825;&#20123;&#22270;&#20687;&#26159;&#20351;&#29992;Optovue Avanti RTVue XR&#37319;&#38598;&#30340;&#65292;&#37319;&#29992;&#20102;&#21160;&#24577;&#25195;&#25551;&#38271;&#24230;&#30340;&#20809;&#26629;&#25195;&#25551;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08255v2 Announce Type: replace-cross  Abstract: Optical coherence tomography (OCT) is a non-invasive imaging technique with extensive clinical applications in ophthalmology. OCT enables the visualization of the retinal layers, playing a vital role in the early detection and monitoring of retinal diseases. OCT uses the principle of light wave interference to create detailed images of the retinal microstructures, making it a valuable tool for diagnosing ocular conditions. This work presents an open-access OCT dataset (OCTDL) comprising over 2000 OCT images labeled according to disease group and retinal pathology. The dataset consists of OCT records of patients with Age-related Macular Degeneration (AMD), Diabetic Macular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO), Retinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). The images were acquired with an Optovue Avanti RTVue XR using raster scanning protocols with dynamic scan length a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#28789;&#27963;&#21448;&#22686;&#24378;&#23616;&#37096;&#24615;&#30340;&#26032;&#22411;&#25237;&#24433;&#20202;&#35774;&#35745;&#65292;&#26377;&#21161;&#20110;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#31354;&#38388;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2312.06742</link><description>&lt;p&gt;
&#34588;&#34562;&#65306;&#22810;&#27169;&#24577;LLM&#30340;&#22686;&#24378;&#23616;&#37096;&#25237;&#24433;&#20202;
&lt;/p&gt;
&lt;p&gt;
Honeybee: Locality-enhanced Projector for Multimodal LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#28789;&#27963;&#21448;&#22686;&#24378;&#23616;&#37096;&#24615;&#30340;&#26032;&#22411;&#25237;&#24433;&#20202;&#35774;&#35745;&#65292;&#26377;&#21161;&#20110;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#31354;&#38388;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20013;&#65292;&#35270;&#35273;&#25237;&#24433;&#20202;&#22312;&#36830;&#25509;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#19982;LLMs&#20043;&#38388;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23454;&#29616;&#28145;&#20837;&#30340;&#35270;&#35273;&#29702;&#35299;&#24182;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#23613;&#31649;&#35270;&#35273;&#25237;&#24433;&#20202;&#30340;&#37325;&#35201;&#24615;&#19981;&#35328;&#32780;&#21947;&#65292;&#20294;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#20004;&#20010;&#20851;&#38190;&#30340;&#25237;&#24433;&#20202;&#23646;&#24615;&#65306;&#65288;i&#65289;&#28789;&#27963;&#24615;&#20197;&#31649;&#29702;&#35270;&#35273;&#20195;&#24065;&#30340;&#25968;&#37327;&#65292;&#23545;&#20110;MLLMs&#30340;&#25972;&#20307;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65307;&#65288;ii&#65289;&#20445;&#30041;&#26469;&#33258;&#35270;&#35273;&#29305;&#24449;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#65292;&#23545;&#20110;&#31354;&#38388;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#28789;&#27963;&#21448;&#22686;&#24378;&#23616;&#37096;&#24615;&#30340;&#26032;&#22411;&#25237;&#24433;&#20202;&#35774;&#35745;&#65292;&#26377;&#25928;&#22320;&#28385;&#36275;&#20102;&#36825;&#20004;&#31181;&#29702;&#24819;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#22810;&#20010;&#21644;&#22810;&#26041;&#38754;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#26816;&#39564;&#20102;&#21508;&#31181;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06742v2 Announce Type: replace-cross  Abstract: In Multimodal Large Language Models (MLLMs), a visual projector plays a crucial role in bridging pre-trained vision encoders with LLMs, enabling profound visual understanding while harnessing the LLMs' robust capabilities. Despite the importance of the visual projector, it has been relatively less explored. In this study, we first identify two essential projector properties: (i) flexibility in managing the number of visual tokens, crucial for MLLMs' overall efficiency, and (ii) preservation of local context from visual features, vital for spatial understanding. Based on these findings, we propose a novel projector design that is both flexible and locality-enhanced, effectively satisfying the two desirable properties. Additionally, we present comprehensive strategies to effectively utilize multiple and multifaceted instruction datasets. Through extensive experiments, we examine the impact of individual design choices. Finally, o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31169;&#26377;&#35757;&#32451;&#21644;&#25512;&#29702;&#26694;&#26550;Delta&#65292;&#36890;&#36807;&#20004;&#20010;&#19981;&#23545;&#31216;&#25968;&#25454;&#27969;&#23454;&#29616;&#20102;&#20855;&#26377;&#21487;&#27604;&#36739;&#27169;&#22411;&#24615;&#33021;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2312.05264</link><description>&lt;p&gt;
&#25152;&#26377;&#30340;&#27827;&#27969;&#37117;&#27719;&#32858;&#21040;&#22823;&#28023;&#65306;&#20855;&#26377;&#19981;&#23545;&#31216;&#27969;&#37327;&#30340;&#31169;&#26377;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
All Rivers Run to the Sea: Private Learning with Asymmetric Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05264
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31169;&#26377;&#35757;&#32451;&#21644;&#25512;&#29702;&#26694;&#26550;Delta&#65292;&#36890;&#36807;&#20004;&#20010;&#19981;&#23545;&#31216;&#25968;&#25454;&#27969;&#23454;&#29616;&#20102;&#20855;&#26377;&#21487;&#27604;&#36739;&#27169;&#22411;&#24615;&#33021;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#22312;&#20113;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#24179;&#21488;&#20013;&#22791;&#21463;&#20851;&#27880;&#65292;&#24403;&#25935;&#24863;&#25968;&#25454;&#26292;&#38706;&#32473;&#26381;&#21153;&#25552;&#20379;&#21830;&#26102;&#12290;&#20026;&#20102;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#24615;&#33021;&#35745;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31169;&#26377;&#35757;&#32451;&#21644;&#25512;&#29702;&#26694;&#26550;Delta&#65292;&#20855;&#26377;&#19982;&#38750;&#31169;&#26377;&#38598;&#20013;&#35757;&#32451;&#30456;&#24403;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;Delta&#20855;&#26377;&#20004;&#20010;&#19981;&#23545;&#31216;&#30340;&#25968;&#25454;&#27969;&#65306;&#20027;&#35201;&#30340;&#20449;&#24687;&#25935;&#24863;&#27969;&#21644;&#27531;&#24046;&#27969;&#12290;&#20027;&#35201;&#37096;&#20998;&#27969;&#20837;&#19968;&#20010;&#23567;&#27169;&#22411;&#65292;&#32780;&#27531;&#20313;&#37096;&#20998;&#21017;&#34987;&#36716;&#31227;&#21040;&#19968;&#20010;&#22823;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Delta&#23558;&#20449;&#24687;&#25935;&#24863;&#34920;&#31034;&#23884;&#20837;&#21040;&#20302;&#32500;&#31354;&#38388;&#20013;&#65292;&#21516;&#26102;&#23558;&#20449;&#24687;&#19981;&#25935;&#24863;&#37096;&#20998;&#25512;&#20837;&#39640;&#32500;&#27531;&#24046;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05264v2 Announce Type: replace-cross  Abstract: Data privacy is of great concern in cloud machine-learning service platforms, when sensitive data are exposed to service providers. While private computing environments (e.g., secure enclaves), and cryptographic approaches (e.g., homomorphic encryption) provide strong privacy protection, their computing performance still falls short compared to cloud GPUs. To achieve privacy protection with high computing performance, we propose Delta, a new private training and inference framework, with comparable model performance as non-private centralized training. Delta features two asymmetric data flows: the main information-sensitive flow and the residual flow. The main part flows into a small model while the residuals are offloaded to a large model. Specifically, Delta embeds the information-sensitive representations into a low-dimensional space while pushing the information-insensitive part into high-dimension residuals. To ensure priv
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#26364;&#35895;&#22320;&#22522;&#28748;&#27880;&#26729;&#30340;&#36733;&#33655;&#21464;&#24418;&#39044;&#27979;&#65292;&#22312;&#32771;&#34385;&#22303;&#22756;&#21078;&#38754;&#21644;&#26729;&#29305;&#24449;&#30340;&#22522;&#30784;&#19978;&#65292;&#32467;&#21512;&#20043;&#21069;&#39034;&#24207;&#25968;&#25454;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21487;&#28385;&#24847;&#22320;&#39044;&#27979;&#36733;&#33655;-&#21464;&#24418;&#26354;&#32447;&#65292;&#24182;&#21487;&#29992;&#20110;&#19981;&#21516;&#26729;&#26465;&#20214;&#19979;&#30340;&#21442;&#25968;&#20998;&#26512;&#21644;&#35774;&#35745;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.03041</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26364;&#35895;&#22320;&#22522;&#20013;&#29992;&#20110;&#26729;&#36733;&#33655;&#21464;&#24418;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transformer-Based Deep Learning Model for Bored Pile Load-Deformation Prediction in Bangkok Subsoil
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03041
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#26364;&#35895;&#22320;&#22522;&#28748;&#27880;&#26729;&#30340;&#36733;&#33655;&#21464;&#24418;&#39044;&#27979;&#65292;&#22312;&#32771;&#34385;&#22303;&#22756;&#21078;&#38754;&#21644;&#26729;&#29305;&#24449;&#30340;&#22522;&#30784;&#19978;&#65292;&#32467;&#21512;&#20043;&#21069;&#39034;&#24207;&#25968;&#25454;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21487;&#28385;&#24847;&#22320;&#39044;&#27979;&#36733;&#33655;-&#21464;&#24418;&#26354;&#32447;&#65292;&#24182;&#21487;&#29992;&#20110;&#19981;&#21516;&#26729;&#26465;&#20214;&#19979;&#30340;&#21442;&#25968;&#20998;&#26512;&#21644;&#35774;&#35745;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#26364;&#35895;&#22320;&#22522;&#20013;&#22823;&#22411;&#28748;&#27880;&#26729;&#30340;&#36733;&#33655;&#21464;&#24418;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#23558;&#22303;&#22756;&#21078;&#38754;&#21644;&#26729;&#30340;&#29305;&#24449;&#32534;&#30721;&#20026;tokenization&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#36733;&#33655;&#21464;&#24418;&#26354;&#32447;&#20316;&#20026;&#36755;&#20986;&#12290;&#27169;&#22411;&#36824;&#23558;&#20043;&#21069;&#30340;&#36733;&#33655;-&#21464;&#24418;&#26354;&#32447;&#39034;&#24207;&#25968;&#25454;&#32534;&#30721;&#36827;&#35299;&#30721;&#22120;&#20013;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#35813;&#27169;&#22411;&#23637;&#29616;&#20102;&#23545;&#36733;&#33655;-&#21464;&#24418;&#26354;&#32447;&#39044;&#27979;&#30340;&#28385;&#24847;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;5.72%&#12290;&#35813;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#19981;&#21516;&#22303;&#22756;&#21644;&#26729;&#26465;&#20214;&#19979;&#65292;&#26729;&#27178;&#25130;&#38754;&#12289;&#26729;&#38271;&#24230;&#21644;&#26729;&#31867;&#22411;&#30340;&#21442;&#25968;&#20998;&#26512;&#21644;&#35774;&#35745;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03041v2 Announce Type: replace  Abstract: This paper presents a novel deep learning model based on the transformer architecture to predict the load-deformation behavior of large bored piles in Bangkok subsoil. The model encodes the soil profile and pile features as tokenization input, and generates the load-deformation curve as output. The model also incorporates the previous sequential data of load-deformation curve into the decoder to improve the prediction accuracy. The model also incorporates the previous sequential data of load-deformation curve into the decoder. The model shows a satisfactory accuracy and generalization ability for the load-deformation curve prediction, with a mean absolute error of 5.72% for the test data. The model could also be used for parametric analysis and design optimization of piles under different soil and pile conditions, pile cross section, pile length and type of pile.
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#22609;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#20316;&#25345;&#32493;&#23398;&#20064;&#65288;CCL&#65289;&#21644;&#33976;&#39311;&#38142;&#65288;DC&#65289;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2312.00600</link><description>&lt;p&gt;
&#36890;&#36807;&#21327;&#20316;&#23398;&#20064;&#25552;&#39640;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Plasticity in Online Continual Learning via Collaborative Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00600
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#22609;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#20316;&#25345;&#32493;&#23398;&#20064;&#65288;CCL&#65289;&#21644;&#33976;&#39311;&#38142;&#65288;DC&#65289;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#20102;&#20174;&#36830;&#32493;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#19981;&#26029;&#20986;&#29616;&#30340;&#26032;&#20998;&#31867;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#33021;&#21147;&#65288;&#21363;&#27169;&#22411;&#30340;&#21487;&#22609;&#24615;&#65289;&#26159;&#21478;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#20316;&#25345;&#32493;&#23398;&#20064;&#65288;CCL&#65289;&#65292;&#19968;&#20010;&#22522;&#20110;&#21327;&#20316;&#23398;&#20064;&#30340;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#33719;&#21462;&#26032;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33976;&#39311;&#38142;&#65288;DC&#65289;&#65292;&#19968;&#31181;&#21327;&#20316;&#23398;&#20064;&#26041;&#26696;&#65292;&#20197;&#25552;&#21319;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00600v2 Announce Type: replace  Abstract: Online Continual Learning (CL) solves the problem of learning the ever-emerging new classification tasks from a continuous data stream. Unlike its offline counterpart, in online CL, the training data can only be seen once. Most existing online CL research regards catastrophic forgetting (i.e., model stability) as almost the only challenge. In this paper, we argue that the model's capability to acquire new knowledge (i.e., model plasticity) is another challenge in online CL. While replay-based strategies have been shown to be effective in alleviating catastrophic forgetting, there is a notable gap in research attention toward improving model plasticity. To this end, we propose Collaborative Continual Learning (CCL), a collaborative learning based strategy to improve the model's capability in acquiring new concepts. Additionally, we introduce Distillation Chain (DC), a collaborative learning scheme to boost the training of the models. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#25345;&#32493;&#23398;&#20064;&#65288;ACL&#65289;&#26469;&#35757;&#32451;&#33258;&#24341;&#29992;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#20803;&#23398;&#20064;&#33258;&#36523;&#30340;&#19978;&#19979;&#25991;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#35299;&#20915;&#8220;&#19978;&#19979;&#25991;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2312.00276</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automating Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00276
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#25345;&#32493;&#23398;&#20064;&#65288;ACL&#65289;&#26469;&#35757;&#32451;&#33258;&#24341;&#29992;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#20803;&#23398;&#20064;&#33258;&#36523;&#30340;&#19978;&#19979;&#25991;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#35299;&#20915;&#8220;&#19978;&#19979;&#25991;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#29992;&#36884;&#30340;&#23398;&#20064;&#31995;&#32479;&#24212;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#20197;&#24320;&#25918;&#24335;&#26041;&#24335;&#19981;&#26029;&#25913;&#36827;&#33258;&#36523;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#20256;&#32479;&#23398;&#20064;&#31639;&#27861;&#24120;&#24120;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;-&#24403;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#20808;&#21069;&#33719;&#24471;&#30340;&#25216;&#33021;&#34987;&#36951;&#24536;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#25345;&#32493;&#23398;&#20064;&#65288;ACL&#65289;&#26469;&#35757;&#32451;&#33258;&#24341;&#29992;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#20803;&#23398;&#20064;&#33258;&#36523;&#30340;&#19978;&#19979;&#25991;&#25345;&#32493;(meta-)&#23398;&#20064;&#31639;&#27861;&#12290;ACL&#23558;&#25152;&#26377;&#26399;&#26395;&#34920;&#29616;&#33391;&#22909;&#20110;&#26032;&#26087;&#20219;&#21153;&#30340;&#35201;&#27714;&#32534;&#30721;&#21040;&#20854;&#20803;&#23398;&#20064;&#30446;&#26631;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;ACL&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#8220;&#19978;&#19979;&#25991;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;; &#25105;&#20204;&#36890;&#36807;ACL&#23398;&#21040;&#30340;&#31639;&#27861;&#22312;&#26080;&#37325;&#25918;&#35774;&#32622;&#19979;&#20248;&#20110;&#25163;&#24037;&#21046;&#23450;&#30340;&#31639;&#27861;&#65292;&#20363;&#22914;&#22312;Split-MNIST&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#24182;&#19988;&#21487;&#20197;&#25345;&#32493;&#23398;&#20064;&#30001;&#22810;&#20010;&#23569;&#37327;&#31034;&#20363;&#21644;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#32452;&#25104;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00276v2 Announce Type: replace  Abstract: General-purpose learning systems should improve themselves in open-ended fashion in ever-changing environments. Conventional learning algorithms for neural networks, however, suffer from catastrophic forgetting (CF) -- previously acquired skills are forgotten when a new task is learned. Instead of hand-crafting new algorithms for avoiding CF, we propose Automated Continual Learning (ACL) to train self-referential neural networks to meta-learn their own in-context continual (meta-)learning algorithms. ACL encodes all desiderata -- good performance on both old and new tasks -- into its meta-learning objectives. Our experiments demonstrate that ACL effectively solves "in-context catastrophic forgetting"; our ACL-learned algorithms outperform hand-crafted ones, e.g., on the Split-MNIST benchmark in the replay-free setting, and enables continual learning of diverse tasks consisting of multiple few-shot and standard image classification da
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Contrastive Denoising Score (CDS) &#30340;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#20462;&#25913;&#29256;&#26412;&#65292;&#29992;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; (LDM)&#65292;&#22312;DDS&#26694;&#26550;&#20869;&#20351;&#29992;&#20102;CUT&#25439;&#22833;&#26469;&#26356;&#22909;&#22320;&#20445;&#30041;&#21407;&#22987;&#22270;&#20687;&#30340;&#29305;&#23450;&#32467;&#26500;&#20803;&#32032;&#12290;</title><link>https://arxiv.org/abs/2311.18608</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#21435;&#22122;&#20998;&#25968;&#30340;&#25991;&#26412;&#24341;&#23548;&#28508;&#22312;&#25193;&#25955;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18608
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Contrastive Denoising Score (CDS) &#30340;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#20462;&#25913;&#29256;&#26412;&#65292;&#29992;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; (LDM)&#65292;&#22312;DDS&#26694;&#26550;&#20869;&#20351;&#29992;&#20102;CUT&#25439;&#22833;&#26469;&#26356;&#22909;&#22320;&#20445;&#30041;&#21407;&#22987;&#22270;&#20687;&#30340;&#29305;&#23450;&#32467;&#26500;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#26174;&#33879;&#20986;&#29616;&#65292;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#21464;&#24471;&#26356;&#21152;&#22810;&#26679;&#21270;&#24182;&#19981;&#26029;&#21457;&#23637;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#26368;&#36817;&#26041;&#27861;&#26159;Delta Denoising Score (DDS) - &#19968;&#31181;&#22522;&#20110;Score Distillation Sampling (SDS)&#26694;&#26550;&#30340;&#22270;&#20687;&#32534;&#36753;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#20016;&#23500;&#29983;&#25104;&#20808;&#39564;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#35780;&#20998;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#19981;&#36275;&#20197;&#20445;&#30041;&#21407;&#22987;&#22270;&#20687;&#30340;&#29305;&#23450;&#32467;&#26500;&#20803;&#32032;&#65292;&#36825;&#26159;&#22270;&#20687;&#32534;&#36753;&#30340;&#20851;&#38190;&#26041;&#38754;&#20043;&#19968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; DDS &#30340;&#19968;&#20010;&#23604;&#23596;&#31616;&#21333;&#20294;&#38750;&#24120;&#24378;&#22823;&#30340;&#20462;&#25913;&#29256;&#26412;&#65292;&#31216;&#20026;Contrastive Denoising Score (CDS)&#65292;&#29992;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; (LDM)&#12290;&#21463;&#21040; DDS &#21644;&#26080;&#37197;&#23545;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#30340;&#23545;&#27604;&#23398;&#20064; (CUT) &#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30452;&#25509;&#30340;&#26041;&#27861;&#65292;&#22312;DDS&#26694;&#26550;&#20869;&#20351;&#29992;CUT&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18608v2 Announce Type: replace-cross  Abstract: With the remarkable advent of text-to-image diffusion models, image editing methods have become more diverse and continue to evolve. A promising recent approach in this realm is Delta Denoising Score (DDS) - an image editing technique based on Score Distillation Sampling (SDS) framework that leverages the rich generative prior of text-to-image diffusion models. However, relying solely on the difference between scoring functions is insufficient for preserving specific structural elements from the original image, a crucial aspect of image editing. To address this, here we present an embarrassingly simple yet very powerful modification of DDS, called Contrastive Denoising Score (CDS), for latent diffusion models (LDM). Inspired by the similarities and differences between DDS and the contrastive learning for unpaired image-to-image translation(CUT), we introduce a straightforward approach using CUT loss within the DDS framework. Ra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#65292;&#20197;&#20811;&#26381;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.17076</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32452;&#21512;&#24335;&#24605;&#32500;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Compositional Chain-of-Thought Prompting for Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#65292;&#20197;&#20811;&#26381;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#30340;&#35270;&#35273;&#39592;&#24178;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#25512;&#29702;&#30340;&#32467;&#21512;&#24050;&#32463;&#23548;&#33268;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMM)&#25104;&#20026;&#24403;&#21069;&#24191;&#27867;&#35270;&#35273;&#21644;&#35821;&#35328;(VL)&#20219;&#21153;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LMM&#20173;&#28982;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#65292;&#27604;&#22914;&#23545;&#35937;&#20043;&#38388;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#22330;&#26223;&#22270;(SGs)&#8212;&#8212;&#23545;&#35937;&#21450;&#20854;&#20851;&#31995;&#21644;&#23646;&#24615;&#30340;&#24418;&#24335;&#21270;&#34920;&#36798;&#65292;&#23427;&#24050;&#34987;&#24191;&#27867;&#29992;&#20316;&#35270;&#35273;&#21644;&#25991;&#26412;&#39046;&#22495;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#28982;&#32780;&#65292;&#22330;&#26223;&#22270;&#25968;&#25454;&#38656;&#35201;&#22330;&#26223;&#22270;&#27880;&#37322;&#65292;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#38590;&#20197;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22330;&#26223;&#22270;&#25968;&#25454;&#24494;&#35843;LMM&#21487;&#33021;&#23548;&#33268;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#21463;&#21040;&#24605;&#32500;&#38142;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17076v2 Announce Type: replace-cross  Abstract: The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-sho
&lt;/p&gt;</description></item><item><title>MobileCLIP&#36890;&#36807;&#22810;&#27169;&#24577;&#24378;&#21270;&#35757;&#32451;&#23454;&#29616;&#20102;&#39640;&#25928;&#29575;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#65292;&#22312;&#38646;-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#24615;&#33021;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2311.17049</link><description>&lt;p&gt;
MobileCLIP: &#36890;&#36807;&#22810;&#27169;&#24577;&#24378;&#21270;&#35757;&#32451;&#23454;&#29616;&#24555;&#36895;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17049
&lt;/p&gt;
&lt;p&gt;
MobileCLIP&#36890;&#36807;&#22810;&#27169;&#24577;&#24378;&#21270;&#35757;&#32451;&#23454;&#29616;&#20102;&#39640;&#25928;&#29575;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#65292;&#22312;&#38646;-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#24615;&#33021;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#36827;&#34892;&#23545;&#27604;&#39044;&#35757;&#32451;&#34920;&#26126;&#65292;&#22312;&#24191;&#27867;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#38646;-shot&#24615;&#33021;&#21644;&#25913;&#21892;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#22823;&#22411;&#22522;&#20110;transformer&#30340;&#32534;&#30721;&#22120;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#20869;&#23384;&#21644;&#24310;&#36831;&#24320;&#38144;&#65292;&#36825;&#20250;&#32473;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#37096;&#32626;&#24102;&#26469;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MobileCLIP--&#19968;&#31181;&#20248;&#21270;&#20102;&#36816;&#34892;&#26102;&#24615;&#33021;&#30340;&#39640;&#25928;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#23478;&#26063;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#22810;&#27169;&#24577;&#24378;&#21270;&#35757;&#32451;&#12290;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#21033;&#29992;&#26469;&#33258;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#21644;&#24378;CLIP&#32534;&#30721;&#22120;&#38598;&#25104;&#30340;&#30693;&#35782;&#36716;&#31227;&#26469;&#25552;&#39640;&#39640;&#25928;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#39069;&#22806;&#30340;&#30693;&#35782;&#23384;&#20648;&#22312;&#24378;&#21270;&#25968;&#25454;&#38598;&#20013;&#36991;&#20813;&#20102;&#35757;&#32451;&#26102;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;MobileCLIP&#20026;&#38646;-shot&#20998;&#31867;&#21644;&#26816;&#32034;&#35774;&#32622;&#20102;&#19968;&#20010;&#26032;&#30340;&#24310;&#36831;-&#20934;&#30830;&#24615;&#26435;&#34913;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17049v2 Announce Type: replace-cross  Abstract: Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks. However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices. In this work, we introduce MobileCLIP -- a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training. The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient models. Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrie
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20855;&#26377;&#21487;&#23398;&#20064;&#24352;&#37327;&#26680;&#33539;&#25968;&#30340;&#26032;&#22411;&#24352;&#37327;&#24674;&#22797;&#27169;&#22411;&#65292;&#24341;&#20837;&#20132;&#26367;&#36817;&#31471;&#20056;&#23376;&#26041;&#27861;&#65288;APMM&#65289;&#20248;&#21270;&#31639;&#27861;&#65292;&#35299;&#20915;&#22788;&#29702;&#38750;&#20809;&#28369;&#21464;&#21270;&#30340;&#24352;&#37327;&#25968;&#25454;&#25361;&#25112;</title><link>https://arxiv.org/abs/2311.13958</link><description>&lt;p&gt;
&#22788;&#29702;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#20013;&#30340;&#38750;&#20809;&#28369;&#25361;&#25112;&#65306;&#22810;&#30446;&#26631;&#24352;&#37327;&#24674;&#22797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Handling The Non-Smooth Challenge in Tensor SVD: A Multi-Objective Tensor Recovery Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13958
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20855;&#26377;&#21487;&#23398;&#20064;&#24352;&#37327;&#26680;&#33539;&#25968;&#30340;&#26032;&#22411;&#24352;&#37327;&#24674;&#22797;&#27169;&#22411;&#65292;&#24341;&#20837;&#20132;&#26367;&#36817;&#31471;&#20056;&#23376;&#26041;&#27861;&#65288;APMM&#65289;&#20248;&#21270;&#31639;&#27861;&#65292;&#35299;&#20915;&#22788;&#29702;&#38750;&#20809;&#28369;&#21464;&#21270;&#30340;&#24352;&#37327;&#25968;&#25454;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#30340;&#24352;&#37327;&#24674;&#22797;&#26041;&#27861;&#22312;&#22788;&#29702;&#35270;&#35273;&#25968;&#25454;&#65288;&#22914;&#24425;&#33394;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#26174;&#31034;&#20986;&#38750;&#20809;&#28369;&#21464;&#21270;&#30340;&#24352;&#37327;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20250;&#36973;&#21463;&#20005;&#37325;&#30340;&#24615;&#33021;&#36864;&#21270;&#12290;&#34429;&#28982;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#32463;&#24120;&#35266;&#23519;&#21040;&#36825;&#31181;&#24773;&#20917;&#65292;&#20294;&#20256;&#32479;&#30340;&#22522;&#20110;t-SVD&#30340;&#26041;&#27861;&#21364;&#24573;&#35270;&#20102;&#36825;&#19968;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24352;&#37327;&#24674;&#22797;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#21487;&#23398;&#20064;&#30340;&#24352;&#37327;&#26680;&#33539;&#25968;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#26367;&#36817;&#31471;&#20056;&#23376;&#26041;&#27861;&#65288;APMM&#65289;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#36845;&#20195;&#22320;&#35299;&#20915;&#25552;&#20986;&#30340;&#24352;&#37327;&#34917;&#20840;&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;APMM&#25910;&#25947;&#21040;&#20248;&#21270;&#38382;&#39064;&#30340;Karush-Kuhn-Tucker&#65288;KKT&#65289;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;APMM&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#24352;&#37327;&#24674;&#22797;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#25506;&#32034;&#21327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13958v2 Announce Type: replace-cross  Abstract: Recently, numerous tensor singular value decomposition (t-SVD)-based tensor recovery methods have shown promise in processing visual data, such as color images and videos. However, these methods often suffer from severe performance degradation when confronted with tensor data exhibiting non-smooth changes. It has been commonly observed in real-world scenarios but ignored by the traditional t-SVD-based methods. In this work, we introduce a novel tensor recovery model with a learnable tensor nuclear norm to address such a challenge. We develop a new optimization algorithm named the Alternating Proximal Multiplier Method (APMM) to iteratively solve the proposed tensor completion model. Theoretical analysis demonstrates the convergence of the proposed APMM to the Karush-Kuhn-Tucker (KKT) point of the optimization problem. In addition, we propose a multi-objective tensor recovery framework based on APMM to efficiently explore the co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#24182;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#65292;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2311.12304</link><description>&lt;p&gt;
&#21457;&#29616;&#26377;&#25928;&#30340;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Discovering Effective Policies for Land-Use Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12304
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#24182;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#65292;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22303;&#22320;&#34987;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#29992;&#36884;&#65292;&#22914;&#26862;&#26519;&#12289;&#22478;&#24066;&#21306;&#22495;&#21644;&#20892;&#19994;&#65292;&#23545;&#38470;&#22320;&#30899;&#24179;&#34913;&#21644;&#27668;&#20505;&#21464;&#21270;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#22522;&#20110;&#21487;&#29992;&#30340;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#30340;&#21382;&#21490;&#25968;&#25454;&#21644;&#30456;&#20851;&#30340;&#30899;&#25490;&#25918;&#21644;&#21560;&#25910;&#30340;&#27169;&#25311;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#20174;&#32780;&#33021;&#22815;&#39640;&#25928;&#35780;&#20272;&#20915;&#31574;&#32773;&#21487;&#36873;&#25321;&#30340;&#19981;&#21516;&#36873;&#39033;&#12290;&#28982;&#21518;&#21487;&#20197;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#26469;&#21457;&#29616;&#29305;&#23450;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#12290;&#35813;&#31995;&#32479;&#26500;&#24314;&#22312;Project Resilience&#24179;&#21488;&#19978;&#65292;&#24182;&#20351;&#29992;Land-Use Harmonization&#25968;&#25454;&#38598;LUH2&#21644;&#31807;&#35760;&#27169;&#22411;BLUE&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#29983;&#25104;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#30899;&#24433;&#21709;&#21644;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#37327;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#20174;&#32780;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
How areas of land are allocated for different uses, such as forests, urban areas, and agriculture, has a large effect on the terrestrial carbon balance, and therefore climate change. Based on available historical data on land-use changes and a simulation of the associated carbon emissions and removals, a surrogate model can be learned that makes it possible to evaluate the different options available to decision-makers efficiently. An evolutionary search process can then be used to discover effective land-use policies for specific locations. Such a system was built on the Project Resilience platform and evaluated with the Land-Use Harmonization dataset LUH2 and the bookkeeping model BLUE. It generates Pareto fronts that trade off carbon impact and amount of land-use change customized to different locations, thus providing a potentially useful tool for land-use planning.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20132;&#26367;&#21333;&#27169;&#24577;&#36866;&#24212;&#26469;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#24178;&#25200;&#24182;&#25429;&#33719;&#36328;&#27169;&#24577;&#20132;&#20114;</title><link>https://arxiv.org/abs/2311.10707</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#26367;&#21333;&#27169;&#24577;&#36866;&#24212;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multimodal Representation Learning by Alternating Unimodal Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10707
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20132;&#26367;&#21333;&#27169;&#24577;&#36866;&#24212;&#26469;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#24178;&#25200;&#24182;&#25429;&#33719;&#36328;&#27169;&#24577;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#22312;&#26576;&#20123;&#24863;&#30693;&#27169;&#24577;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#34920;&#29616;&#26356;&#26174;&#33879;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MLA&#65288;&#20855;&#26377;&#20132;&#26367;&#21333;&#27169;&#24577;&#36866;&#24212;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65289;&#12290;MLA&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#32852;&#21512;&#22810;&#27169;&#24577;&#23398;&#20064;&#36807;&#31243;&#36716;&#21270;&#20026;&#20132;&#26367;&#21333;&#27169;&#24577;&#23398;&#20064;&#36807;&#31243;&#26469;&#37325;&#26032;&#23450;&#20041;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#27169;&#24577;&#20043;&#38388;&#30340;&#24178;&#25200;&#12290;&#21516;&#26102;&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#22836;&#37096;&#25429;&#33719;&#36328;&#27169;&#24577;&#20132;&#20114;&#65292;&#35813;&#22836;&#37096;&#22312;&#19981;&#21516;&#27169;&#24577;&#38388;&#32463;&#21382;&#25345;&#32493;&#20248;&#21270;&#12290;&#36825;&#31181;&#20248;&#21270;&#36807;&#31243;&#30001;&#26799;&#24230;&#20462;&#25913;&#26426;&#21046;&#25511;&#21046;&#65292;&#20197;&#38450;&#27490;&#20849;&#20139;&#22836;&#37096;&#20002;&#22833;&#20808;&#21069;&#33719;&#21462;&#30340;&#20449;&#24687;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;MLA&#21033;&#29992;&#19968;&#20010;&#27979;&#35797;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10707v2 Announce Type: replace  Abstract: Multimodal learning, which integrates data from diverse sensory modes, plays a pivotal role in artificial intelligence. However, existing multimodal learning methods often struggle with challenges where some modalities appear more dominant than others during multimodal learning, resulting in suboptimal performance. To address this challenge, we propose MLA (Multimodal Learning with Alternating Unimodal Adaptation). MLA reframes the conventional joint multimodal learning process by transforming it into an alternating unimodal learning process, thereby minimizing interference between modalities. Simultaneously, it captures cross-modal interactions through a shared head, which undergoes continuous optimization across different modalities. This optimization process is controlled by a gradient modification mechanism to prevent the shared head from losing previously acquired information. During the inference phase, MLA utilizes a test-time
&lt;/p&gt;</description></item><item><title>LoRA &#22312;&#22810;&#35821;&#35328;&#25688;&#35201;&#26041;&#38754;&#31454;&#20105;&#21147;&#24378;&#65292;&#29305;&#21035;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2311.08572</link><description>&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#29992;&#20110;&#22810;&#35821;&#35328;&#25688;&#35201;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation for Multilingual Summarization: An Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08572
&lt;/p&gt;
&lt;p&gt;
LoRA &#22312;&#22810;&#35821;&#35328;&#25688;&#35201;&#26041;&#38754;&#31454;&#20105;&#21147;&#24378;&#65292;&#29305;&#21035;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#33879;&#21152;&#36895;&#20102;&#36817;&#24180;&#26469;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36827;&#27493;&#65292;&#20294;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#20307;&#31215;&#23545;&#20256;&#32479;&#30340;&#24494;&#35843;&#25552;&#20986;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#20869;&#23384;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#26159;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#28041;&#21450;&#22810;&#35821;&#35328;&#25688;&#35201;&#39046;&#22495;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65288;&#22240;&#20026;&#36755;&#20837;&#36890;&#24120;&#24456;&#38271;&#65289;&#65292;&#19988;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#19981;&#21516;&#25968;&#25454;&#21487;&#29992;&#24615;&#22330;&#26223;&#65292;&#21253;&#25324;&#39640;&#25968;&#25454;&#21644;&#20302;&#25968;&#25454;&#35774;&#32622;&#65292;&#20197;&#21450;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#21033;&#29992;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#35757;&#32451;&#26102;&#65292;LoRA&#19982;&#23436;&#20840;&#24494;&#35843;&#31454;&#20105;&#28608;&#28872;&#65292;&#24182;&#19988;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#23569;&#25968;&#25454;&#28857;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#65292;&#21457;&#29616;&#25345;&#32493;&#30340;LoRA&#35843;&#20248;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08572v2 Announce Type: replace-cross  Abstract: Although the advancements of pre-trained Large Language Models have significantly accelerated recent progress in NLP, their ever-increasing size poses significant challenges for conventional fine-tuning, especially in memory-intensive tasks. We investigate the potential of Parameter-Efficient Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of multilingual summarization, a task that is both challenging (due to typically long inputs), and relatively unexplored. We conduct an extensive study across different data availability scenarios, including high- and low-data settings, and cross-lingual transfer, leveraging models of different sizes. Our findings reveal that LoRA is competitive with full fine-tuning when trained with high quantities of data, and excels in low-data scenarios and cross-lingual transfer. We also study different strategies for few-shot cross-lingual transfer, finding that continued LoRA tuning
&lt;/p&gt;</description></item><item><title>&#35780;&#20215;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#37051;&#23621;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25552;&#20986;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#24182;&#21457;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;GNN&#39046;&#22495;&#30340;&#35299;&#37322;&#27809;&#26377;&#22826;&#22823;&#24046;&#24322;&#65292;&#21516;&#26102;&#21457;&#29616;&#24456;&#22810;&#25216;&#26415;&#22312;&#27809;&#26377;&#33258;&#29615;&#30340;GNNs&#19979;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#37325;&#35201;&#37051;&#23621;&#12290;</title><link>https://arxiv.org/abs/2311.08118</link><description>&lt;p&gt;
&#35780;&#20215;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#37051;&#23621;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Neighbor Explainability for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08118
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20215;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#37051;&#23621;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25552;&#20986;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#24182;&#21457;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;GNN&#39046;&#22495;&#30340;&#35299;&#37322;&#27809;&#26377;&#22826;&#22823;&#24046;&#24322;&#65292;&#21516;&#26102;&#21457;&#29616;&#24456;&#22810;&#25216;&#26415;&#22312;&#27809;&#26377;&#33258;&#29615;&#30340;GNNs&#19979;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#37325;&#35201;&#37051;&#23621;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#36817;&#24180;&#26469;&#26032;&#20852;&#39046;&#22495;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#23545;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;&#30830;&#23450;&#27599;&#20010;&#37051;&#23621;&#23545;&#20110; GNN &#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#20026;&#27492;&#65292;&#21508;&#31181;&#24050;&#30693;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#34987;&#37325;&#26032;&#26500;&#36896;&#20197;&#33719;&#21462;&#37051;&#23621;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312; GNN &#39046;&#22495;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#25552;&#20379;&#30340;&#35299;&#37322;&#20960;&#20046;&#27809;&#26377;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#22312;&#20351;&#29992;&#27809;&#26377;&#33258;&#29615;&#30340; GNNs &#26102;&#26410;&#33021;&#35782;&#21035;&#37325;&#35201;&#30340;&#37051;&#23621;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08118v2 Announce Type: replace-cross  Abstract: Explainability in Graph Neural Networks (GNNs) is a new field growing in the last few years. In this publication we address the problem of determining how important is each neighbor for the GNN when classifying a node and how to measure the performance for this specific task. To do this, various known explainability methods are reformulated to get the neighbor importance and four new metrics are presented. Our results show that there is almost no difference between the explanations provided by gradient-based techniques in the GNN domain. In addition, many explainability techniques failed to identify important neighbors when GNNs without self-loops are used.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#29616;&#23454;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#19982;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#20043;&#38388;&#30340;&#32852;&#31995;&#35777;&#25454;&#65292;&#25214;&#21040;&#20102;&#35780;&#20272;&#19978;&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;&#36981;&#24490;&#23618;&#27425;&#22240;&#26524;&#20851;&#31995;&#30340;&#31616;&#21333;GD&#20248;&#21270;&#31243;&#24207;&#26469;&#25913;&#21892;&#30456;&#20284;&#24615;&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2311.07772</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26799;&#24230;&#19979;&#38477;&#30340;&#20877;&#23457;&#35270;
&lt;/p&gt;
&lt;p&gt;
In-context Learning and Gradient Descent Revisited
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#29616;&#23454;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#19982;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#20043;&#38388;&#30340;&#32852;&#31995;&#35777;&#25454;&#65292;&#25214;&#21040;&#20102;&#35780;&#20272;&#19978;&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;&#36981;&#24490;&#23618;&#27425;&#22240;&#26524;&#20851;&#31995;&#30340;&#31616;&#21333;GD&#20248;&#21270;&#31243;&#24207;&#26469;&#25913;&#21892;&#30456;&#20284;&#24615;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#28982;&#32780;&#20854;&#22522;&#26412;&#26426;&#21046;&#20173;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#12290;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#34920;&#26126;&#65292;ICL&#38544;&#24335;&#22320;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#20248;&#21270;&#12290;&#23613;&#31649;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#24456;&#22810;&#30740;&#31350;&#38598;&#20013;&#22312;&#31616;&#21270;&#35774;&#32622;&#65292;&#20854;&#20013;&#20248;&#21270;&#27973;&#23618;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#38024;&#23545;&#29616;&#23454;NLP&#20219;&#21153;&#21644;&#27169;&#22411;&#30340;ICL-GD&#23545;&#24212;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#35780;&#20272;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#65292;&#26080;&#35770;&#26159;&#22312;&#26377;&#38382;&#39064;&#30340;&#25351;&#26631;&#36824;&#26159;&#19981;&#36275;&#30340;&#22522;&#32447;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20063;&#33021;&#23454;&#29616;&#21487;&#27604;&#30340;ICL-GD&#30456;&#20284;&#24615;&#20998;&#25968;&#65292;&#23613;&#31649;&#26410;&#34920;&#29616;&#20986;ICL&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#27169;&#22411;&#20013;&#20449;&#24687;&#27969;&#21160;&#22312;ICL&#21644;GD&#20043;&#38388;&#30340;&#20027;&#35201;&#24046;&#24322;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#23618;&#22240;&#26524;&#20851;&#31995;&#8221;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23562;&#37325;&#23618;&#22240;&#26524;&#20851;&#31995;&#30340;&#31616;&#21333;GD&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#34920;&#26126;&#23427;&#26174;&#33879;&#25913;&#21892;&#20102;&#30456;&#20284;&#24615;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07772v4 Announce Type: replace  Abstract: In-context learning (ICL) has shown impressive results in few-shot learning tasks, yet its underlying mechanism is still not fully understood. A recent line of work suggests that ICL performs gradient descent (GD)-based optimization implicitly. While appealing, much of the research focuses on simplified settings, where the parameters of a shallow model are optimized. In this work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks and models. We find gaps in evaluation, both in terms of problematic metrics and insufficient baselines. We show that surprisingly, even untrained models achieve comparable ICL-GD similarity scores despite not exhibiting ICL. Next, we explore a major discrepancy in the flow of information throughout the model between ICL and GD, which we term Layer Causality. We propose a simple GD-based optimization procedure that respects layer causality, and show it improves similarity scores significan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26377;&#25928;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#29301;&#24341;&#27169;&#22411;&#24182;&#35268;&#21010;&#39118;&#38505;&#24863;&#30693;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2311.06234</link><description>&lt;p&gt;
EVORA&#65306;&#38754;&#21521;&#39118;&#38505;&#24863;&#30693;&#36234;&#37326;&#33258;&#20027;&#34892;&#39542;&#30340;&#28145;&#24230;&#35777;&#25454;&#21487;&#31359;&#36234;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EVORA: Deep Evidential Traversability Learning for Risk-Aware Off-Road Autonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06234
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26377;&#25928;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#29301;&#24341;&#27169;&#22411;&#24182;&#35268;&#21010;&#39118;&#38505;&#24863;&#30693;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#29301;&#24341;&#30340;&#22320;&#24418;&#31359;&#36234;&#23545;&#20110;&#23454;&#29616;&#24555;&#36895;&#36234;&#37326;&#23548;&#33322;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22320;&#24418;&#29305;&#24615;&#65292;&#33258;&#21160;&#24809;&#32602;&#36890;&#36807;&#19981;&#33391;&#22320;&#24418;&#30340;&#36712;&#36857;&#65292;&#20294;&#22312;&#36866;&#24403;&#37327;&#21270;&#21644;&#20943;&#36731;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#39118;&#38505;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#29301;&#24341;&#27169;&#22411;&#24182;&#35268;&#21010;&#39118;&#38505;&#24863;&#30693;&#36712;&#36857;&#12290;&#20026;&#20102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#31163;&#25955;&#29301;&#24341;&#20998;&#24067;&#21644;&#29301;&#24341;&#39044;&#27979;&#22120;&#28508;&#22312;&#29305;&#24449;&#30340;&#27010;&#29575;&#23494;&#24230;&#65292;&#39640;&#25928;&#22320;&#24314;&#27169;&#20102;&#28151;&#21512;&#19981;&#30830;&#23450;&#24615;&#12290;&#21033;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65292;&#25105;&#20204;&#36890;&#36807;&#32593;&#32476;&#36755;&#20986;&#21442;&#25968;&#21270;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#24179;&#26041;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#25439;&#22833;&#65292;&#20854;&#20855;&#26377;&#19968;&#20010;&#38381;&#21512;&#30340;&#25439;&#22833;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06234v2 Announce Type: replace-cross  Abstract: Traversing terrain with good traction is crucial for achieving fast off-road navigation. Instead of manually designing costs based on terrain features, existing methods learn terrain properties directly from data via self-supervision to automatically penalize trajectories moving through undesirable terrain, but challenges remain to properly quantify and mitigate the risk due to uncertainty in learned models. To this end, this work proposes a unified framework to learn uncertainty-aware traction model and plan risk-aware trajectories. For uncertainty quantification, we efficiently model both aleatoric and epistemic uncertainty by learning discrete traction distributions and probability densities of the traction predictor's latent features. Leveraging evidential deep learning, we parameterize Dirichlet distributions with the network outputs and propose a novel uncertainty-aware squared Earth Mover's distance loss with a closed-fo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#31070;&#32463;&#31361;&#35302;&#26816;&#27979;&#21644;&#20998;&#31867;&#26550;&#26500;&#65292;&#21033;&#29992;&#26222;&#20140;&#32454;&#32990;&#30340;&#29305;&#24449;&#23454;&#26102;&#22788;&#29702;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23558;&#25968;&#25454;&#21387;&#32553;&#24182;&#23384;&#20648;&#22312;&#21487;&#31227;&#21160;&#35013;&#32622;&#19978;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2311.04808</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#31070;&#32463;&#31361;&#35302;&#20998;&#31867;&#30340;&#36731;&#37327;&#32423;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Lightweight Architecture for Real-Time Neuronal-Spike Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04808
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#31070;&#32463;&#31361;&#35302;&#26816;&#27979;&#21644;&#20998;&#31867;&#26550;&#26500;&#65292;&#21033;&#29992;&#26222;&#20140;&#32454;&#32990;&#30340;&#29305;&#24449;&#23454;&#26102;&#22788;&#29702;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23558;&#25968;&#25454;&#21387;&#32553;&#24182;&#23384;&#20648;&#22312;&#21487;&#31227;&#21160;&#35013;&#32622;&#19978;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31185;&#23398;&#23478;&#20204;&#38750;&#24120;&#21916;&#27426;&#20351;&#29992;&#23567;&#40736;&#22823;&#33041;&#30340;&#31070;&#32463;&#27963;&#21160;&#30005;&#29983;&#29702;&#35760;&#24405;&#26469;&#20102;&#35299;&#22823;&#33041;&#21151;&#33021;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#20174;&#23567;&#33041;&#20013;&#30340;&#26222;&#20140;&#32454;&#32990;&#33719;&#21462;&#35760;&#24405;&#65292;&#20197;&#20415;&#20102;&#35299;&#22823;&#33041;&#25439;&#20260;&#21644;&#36816;&#21160;&#21151;&#33021;&#20007;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#31070;&#32463;&#31361;&#35302;&#26816;&#27979;&#21644;&#20998;&#31867;&#26550;&#26500;&#65292;&#21033;&#29992;&#26222;&#20140;&#32454;&#32990;&#30340;&#29420;&#29305;&#29305;&#24449;&#23454;&#26102;&#20002;&#24323;&#31232;&#30095;&#31070;&#32463;&#25968;&#25454;&#20013;&#19981;&#38656;&#35201;&#30340;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#65288;&#21387;&#32553;&#65289;&#25968;&#25454;&#21487;&#20197;&#36731;&#26494;&#23384;&#20648;&#22312;&#22836;&#37096;&#35774;&#22791;&#19978;&#30340;&#21487;&#31227;&#21160;&#23384;&#20648;&#35774;&#22791;&#19978;&#65292;&#20943;&#36731;&#20102;&#38656;&#35201;&#20351;&#29992;&#36830;&#32447;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04808v2 Announce Type: replace-cross  Abstract: Electrophysiological recordings of neural activity in a mouse's brain are very popular among neuroscientists for understanding brain function. One particular area of interest is acquiring recordings from the Purkinje cells in the cerebellum in order to understand brain injuries and the loss of motor functions. However, current setups for such experiments do not allow the mouse to move freely and, thus, do not capture its natural behaviour since they have a wired connection between the animal's head stage and an acquisition device. In this work, we propose a lightweight neuronal-spike detection and classification architecture that leverages on the unique characteristics of the Purkinje cells to discard unneeded information from the sparse neural data in real time. This allows the (condensed) data to be easily stored on a removable storage device on the head stage, alleviating the need for wires. Synthesis results reveal a &gt;95% o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#35299;&#20915;&#22312;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#35757;&#32451;&#26497;&#28145;&#31364;&#21069;&#39304;&#32593;&#32476;&#26102;&#36935;&#21040;&#30340;&#8220;&#27515;&#20129;ReLU&#8221;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.03733</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#28145;&#31364;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21021;&#22987;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improved weight initialization for deep and narrow feedforward neural network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#35299;&#20915;&#22312;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#35757;&#32451;&#26497;&#28145;&#31364;&#21069;&#39304;&#32593;&#32476;&#26102;&#36935;&#21040;&#30340;&#8220;&#27515;&#20129;ReLU&#8221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24403;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#35774;&#32622;&#65292;&#20197;&#21450;ReLU&#28608;&#27963;&#20989;&#25968;&#65292;&#24050;&#25104;&#20026;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30340;&#22522;&#30707;&#65292;&#20351;&#24471;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#33021;&#22815;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#39046;&#22495;&#24471;&#20197;&#35757;&#32451;&#21644;&#25512;&#24191;&#12290;&#22312;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#8220;&#27515;&#20129;ReLU&#8221;&#38382;&#39064;&#65292;&#21363;ReLU&#31070;&#32463;&#20803;&#21464;&#24471;&#19981;&#27963;&#36291;&#24182;&#36755;&#20986;&#20026;&#38646;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#29702;&#35770;&#30740;&#31350;&#21644;&#21508;&#31181;&#26041;&#27861;&#24050;&#34987;&#24341;&#20837;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26377;&#20102;&#36825;&#20123;&#26041;&#27861;&#21644;&#30740;&#31350;&#65292;&#23545;&#20110;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#26497;&#28145;&#31364;&#21069;&#39304;&#32593;&#32476;&#30340;&#35757;&#32451;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#25105;&#20204;&#21021;&#22987;&#26435;&#37325;&#30697;&#38453;&#30340;&#20960;&#20010;&#29305;&#24615;&#65292;&#24182;&#28436;&#31034;&#20102;&#36825;&#20123;&#29305;&#24615;&#22914;&#20309;&#20351;&#26377;&#25928;&#20256;&#25773;&#26356;&#21152;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03733v2 Announce Type: replace  Abstract: Appropriate weight initialization settings, along with the ReLU activation function, have become cornerstones of modern deep learning, enabling the training and deployment of highly effective and efficient neural network models across diverse areas of artificial intelligence. The problem of \textquotedblleft dying ReLU," where ReLU neurons become inactive and yield zero output, presents a significant challenge in the training of deep neural networks with ReLU activation function. Theoretical research and various methods have been introduced to address the problem. However, even with these methods and research, training remains challenging for extremely deep and narrow feedforward networks with ReLU activation function. In this paper, we propose a novel weight initialization method to address this issue. We establish several properties of our initial weight matrix and demonstrate how these properties enable the effective propagation o
&lt;/p&gt;</description></item><item><title>&#40654;&#26364;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#30340;&#26032;&#26041;&#27861;&#21033;&#29992;Fisher&#24230;&#37327;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#36924;&#36817;&#26063;&#65292;&#35299;&#20915;&#20102;&#22312;&#26080;&#38480;&#25968;&#25454;&#26497;&#38480;&#19979;&#20808;&#21069;&#26041;&#27861;&#24230;&#37327;&#36873;&#25321;&#19981;&#24403;&#23548;&#33268;&#36924;&#36817;&#36807;&#20110;&#29421;&#31364;&#21644;&#26377;&#20559;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.02766</link><description>&lt;p&gt;
&#20855;&#26377;Fisher&#24230;&#37327;&#30340;&#40654;&#26364;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Riemannian Laplace Approximation with the Fisher Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02766
&lt;/p&gt;
&lt;p&gt;
&#40654;&#26364;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#30340;&#26032;&#26041;&#27861;&#21033;&#29992;Fisher&#24230;&#37327;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#36924;&#36817;&#26063;&#65292;&#35299;&#20915;&#20102;&#22312;&#26080;&#38480;&#25968;&#25454;&#26497;&#38480;&#19979;&#20808;&#21069;&#26041;&#27861;&#24230;&#37327;&#36873;&#25321;&#19981;&#24403;&#23548;&#33268;&#36924;&#36817;&#36807;&#20110;&#29421;&#31364;&#21644;&#26377;&#20559;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Laplace&#26041;&#27861;&#29992;&#39640;&#26031;&#20998;&#24067;&#22312;&#20854;&#27169;&#24335;&#22788;&#23545;&#30446;&#26631;&#23494;&#24230;&#36827;&#34892;&#36817;&#20284;&#12290;&#22522;&#20110;Bernstein-von Mises&#23450;&#29702;&#65292;&#23427;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#26159;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#28176;&#36817;&#20934;&#30830;&#30340;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#30446;&#26631;&#21644;&#26377;&#38480;&#25968;&#25454;&#21518;&#39564;&#65292;&#23427;&#24448;&#24448;&#26159;&#19968;&#31181;&#36807;&#20110;&#31895;&#31961;&#30340;&#36817;&#20284;&#12290;&#26368;&#36817;&#23545;Laplace&#36924;&#36817;&#30340;&#19968;&#33324;&#21270;&#26159;&#26681;&#25454;&#36873;&#25321;&#30340;&#40654;&#26364;&#20960;&#20309;&#23545;&#39640;&#26031;&#36817;&#20284;&#36827;&#34892;&#36716;&#25442;&#65292;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#36817;&#20284;&#26063;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#26412;&#25991;&#25152;&#31034;&#65292;&#20854;&#24615;&#36136;&#20005;&#37325;&#20381;&#36182;&#20110;&#25152;&#36873;&#25321;&#30340;&#24230;&#37327;&#65292;&#23454;&#38469;&#19978;&#65292;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#37319;&#29992;&#30340;&#24230;&#37327;&#23548;&#33268;&#30340;&#36924;&#36817;&#21363;&#20351;&#22312;&#26080;&#38480;&#25968;&#25454;&#37327;&#30340;&#26497;&#38480;&#19979;&#20063;&#36807;&#20110;&#29421;&#31364;&#19988;&#23384;&#22312;&#20559;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#19968;&#27493;&#21457;&#23637;&#36924;&#36817;&#26063;&#65292;&#25512;&#23548;&#20986;&#20004;&#31181;&#22312;&#26080;&#38480;&#25968;&#25454;&#26497;&#38480;&#19979;&#31934;&#30830;&#30340;&#26367;&#20195;&#21464;&#31181;&#65292;&#25193;&#23637;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02766v3 Announce Type: replace  Abstract: Laplace's method approximates a target density with a Gaussian distribution at its mode. It is computationally efficient and asymptotically exact for Bayesian inference due to the Bernstein-von Mises theorem, but for complex targets and finite-data posteriors it is often too crude an approximation. A recent generalization of the Laplace Approximation transforms the Gaussian approximation according to a chosen Riemannian geometry providing a richer approximation family, while still retaining computational efficiency. However, as shown here, its properties depend heavily on the chosen metric, indeed the metric adopted in previous work results in approximations that are overly narrow as well as being biased even at the limit of infinite data. We correct this shortcoming by developing the approximation family further, deriving two alternative variants that are exact at the limit of infinite data, extending the theoretical analysis of the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#25928;&#29992;&#30340;&#32570;&#21475;&#39118;&#38505;&#30340;&#20272;&#35745;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#38750;&#28176;&#36817;&#30028;&#38480;&#26469;&#35299;&#20915;UBSR&#30340;&#20272;&#35745;&#35823;&#24046;&#21644;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.18743</link><description>&lt;p&gt;
&#20248;&#21270;&#22522;&#20110;&#25928;&#29992;&#30340;&#32570;&#21475;&#39118;&#38505;&#65306;&#19968;&#31181;&#38750;&#28176;&#36817;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Optimization of utility-based shortfall risk: A non-asymptotic viewpoint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#25928;&#29992;&#30340;&#32570;&#21475;&#39118;&#38505;&#30340;&#20272;&#35745;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#38750;&#28176;&#36817;&#30028;&#38480;&#26469;&#35299;&#20915;UBSR&#30340;&#20272;&#35745;&#35823;&#24046;&#21644;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#25928;&#29992;&#22522;&#32570;&#21475;&#39118;&#38505;&#65288;UBSR&#65289;&#30340;&#20272;&#35745;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#26159;&#37329;&#34701;&#39046;&#22495;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#39118;&#38505;&#24230;&#37327;&#12290;&#22312;UBSR&#20272;&#35745;&#30340;&#24773;&#22659;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#23545;&#32463;&#20856;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#65288;SAA&#65289;&#30340;UBSR&#22343;&#26041;&#35823;&#24046;&#30340;&#38750;&#28176;&#36817;&#30028;&#38480;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;UBSR&#20248;&#21270;&#30340;&#24773;&#22659;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#24179;&#28369;&#21442;&#25968;&#21270;&#19979;UBSR&#26799;&#24230;&#30340;&#34920;&#36798;&#24335;&#12290;&#36825;&#20010;&#34920;&#36798;&#24335;&#26159;&#26399;&#26395;&#30340;&#27604;&#20540;&#65292;&#20004;&#32773;&#37117;&#28041;&#21450;UBSR&#12290;&#25105;&#20204;&#22312;UBSR&#26799;&#24230;&#34920;&#36798;&#24335;&#20013;&#20351;&#29992;SAA&#20316;&#20026;&#20998;&#23376;&#21644;&#20998;&#27597;&#65292;&#24471;&#20986;&#20102;&#19968;&#20010;&#26377;&#20559;&#26799;&#24230;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20272;&#35745;&#35823;&#24046;&#30340;&#38750;&#28176;&#36817;&#30028;&#38480;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#26159;&#28176;&#36817;&#26080;&#20559;&#30340;&#12290;&#25105;&#20204;&#23558;&#19978;&#36848;&#26799;&#24230;&#20272;&#35745;&#22120;&#32467;&#21512;&#21040;&#19968;&#20010;&#29992;&#20110;UBSR&#20248;&#21270;&#30340;&#38543;&#26426;&#26799;&#24230;&#65288;SG&#65289;&#31639;&#27861;&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#37327;&#21270;&#25105;&#20204;&#26041;&#27861;&#25910;&#25947;&#36895;&#24230;&#30340;&#38750;&#28176;&#36817;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18743v2 Announce Type: replace  Abstract: We consider the problems of estimation and optimization of utility-based shortfall risk (UBSR), which is a popular risk measure in finance. In the context of UBSR estimation, we derive a non-asymptotic bound on the mean-squared error of the classical sample average approximation (SAA) of UBSR. Next, in the context of UBSR optimization, we derive an expression for the UBSR gradient under a smooth parameterization. This expression is a ratio of expectations, both of which involve the UBSR. We use SAA for the numerator as well as denominator in the UBSR gradient expression to arrive at a biased gradient estimator. We derive non-asymptotic bounds on the estimation error, which show that our gradient estimator is asymptotically unbiased. We incorporate the aforementioned gradient estimator into a stochastic gradient (SG) algorithm for UBSR optimization. Finally, we derive non-asymptotic bounds that quantify the rate of convergence of our 
&lt;/p&gt;</description></item><item><title>DistillSpec&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#25913;&#36827;&#20102;&#25237;&#26426;&#24615;&#35299;&#30721;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;10-45%&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2310.08461</link><description>&lt;p&gt;
DistillSpec&#65306;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25913;&#36827;&#25237;&#26426;&#24615;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
DistillSpec: Improving Speculative Decoding via Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08461
&lt;/p&gt;
&lt;p&gt;
DistillSpec&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#25913;&#36827;&#20102;&#25237;&#26426;&#24615;&#35299;&#30721;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;10-45%&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#26426;&#24615;&#35299;&#30721;&#65288;SD&#65289;&#36890;&#36807;&#20351;&#29992;&#26356;&#24555;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#22810;&#20010;&#26631;&#35760;&#65292;&#28982;&#21518;&#30001;&#26356;&#22823;&#30340;&#30446;&#26631;&#27169;&#22411;&#24182;&#34892;&#39564;&#35777;&#36825;&#20123;&#26631;&#35760;&#65292;&#20174;&#32780;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#27169;&#22411;&#20998;&#24067;&#30340;&#25991;&#26412;&#65292;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#19982;&#30446;&#26631;&#27169;&#22411;&#33391;&#22909;&#23545;&#40784;&#30340;&#32039;&#20945;&#33609;&#31295;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistillSpec&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#26356;&#22909;&#22320;&#23558;&#33609;&#31295;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#23545;&#40784;&#65292;&#28982;&#21518;&#24212;&#29992;SD&#12290;DistillSpec&#20570;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#35777;&#26126;&#36825;&#23545;&#25913;&#36827;&#33609;&#31295;&#21644;&#30446;&#26631;&#23545;&#40784;&#33267;&#20851;&#37325;&#35201;&#65306;&#21033;&#29992;&#26469;&#33258;&#33609;&#31295;&#27169;&#22411;&#30340;on-policy&#25968;&#25454;&#29983;&#25104;&#65292;&#20197;&#21450;&#23558;&#21457;&#25955;&#20989;&#25968;&#23450;&#21046;&#21040;&#20219;&#21153;&#21644;&#35299;&#30721;&#31574;&#30053;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;DistillSpec&#22312;&#19968;&#31995;&#21015;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#27604;&#26631;&#20934;SD&#33719;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;10-45%&#30340;&#21152;&#36895;&#65292;&#20351;&#29992;&#36138;&#23146;&#21644;&#38750;&#36138;&#23146;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08461v2 Announce Type: replace-cross  Abstract: Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improving the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10 - 45% speedups over standard SD on a range of standard benchmarks, using both greedy and non-gr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22797;&#21512;&#21518;&#38376;&#25915;&#20987;(CBA)&#23637;&#31034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26893;&#20837;&#22810;&#20010;&#35302;&#21457;&#20851;&#38190;&#35789;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#20026;&#38544;&#34109;&#65292;&#24182;&#30830;&#20445;&#21482;&#26377;&#24403;&#25152;&#26377;&#35302;&#21457;&#20851;&#38190;&#35789;&#21516;&#26102;&#20986;&#29616;&#26102;&#21518;&#38376;&#25165;&#20250;&#34987;&#28608;&#27963;&#12290;</title><link>https://arxiv.org/abs/2310.07676</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#21512;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Composite Backdoor Attacks Against Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22797;&#21512;&#21518;&#38376;&#25915;&#20987;(CBA)&#23637;&#31034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26893;&#20837;&#22810;&#20010;&#35302;&#21457;&#20851;&#38190;&#35789;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#26356;&#20026;&#38544;&#34109;&#65292;&#24182;&#30830;&#20445;&#21482;&#26377;&#24403;&#25152;&#26377;&#35302;&#21457;&#20851;&#38190;&#35789;&#21516;&#26102;&#20986;&#29616;&#26102;&#21518;&#38376;&#25165;&#20250;&#34987;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36890;&#24120;&#20316;&#20026;&#35768;&#22810;&#30740;&#31350;&#21644;&#26381;&#21153;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19981;&#21487;&#20449;&#20219;&#30340;&#31532;&#19977;&#26041;LLMs&#21487;&#33021;&#20250;&#26263;&#20013;&#20026;&#19979;&#28216;&#20219;&#21153;&#24341;&#20837;&#28431;&#27934;&#12290;&#26412;&#25991;&#36890;&#36807;&#21518;&#38376;&#25915;&#20987;&#30340;&#35270;&#35282;&#25506;&#35752;&#20102;LLMs&#30340;&#33030;&#24369;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#23545;LLMs&#30340;&#21518;&#38376;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#25552;&#31034;&#32452;&#20214;&#20013;&#20998;&#25955;&#22810;&#20010;&#35302;&#21457;&#20851;&#38190;&#35789;&#12290;&#36825;&#31181;&#22797;&#21512;&#21518;&#38376;&#25915;&#20987;&#65288;CBA&#65289;&#34987;&#35777;&#26126;&#27604;&#20165;&#22312;&#21333;&#20010;&#32452;&#20214;&#20013;&#26893;&#20837;&#30456;&#21516;&#30340;&#22810;&#20010;&#35302;&#21457;&#20851;&#38190;&#35789;&#26356;&#38544;&#34109;&#12290;CBA&#30830;&#20445;&#21482;&#26377;&#24403;&#25152;&#26377;&#35302;&#21457;&#20851;&#38190;&#35789;&#20986;&#29616;&#26102;&#21518;&#38376;&#25165;&#20250;&#34987;&#28608;&#27963;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CBA&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22810;&#27169;&#24335;&#20219;&#21153;&#20013;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.07676v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with $3\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset, our att
&lt;/p&gt;</description></item><item><title>&#35757;&#32451;&#21160;&#24577;&#21487;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#26041;&#27861;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#65292;&#36890;&#36807;&#36873;&#23450;&#30340;&#35757;&#32451;&#23454;&#20363;&#24494;&#35843;&#20027;&#27169;&#22411;&#23454;&#29616;&#27604;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;</title><link>https://arxiv.org/abs/2310.06588</link><description>&lt;p&gt;
FTFT:&#36890;&#36807;&#36716;&#31227;&#35757;&#32451;&#21160;&#24577;&#23454;&#29616;&#39640;&#25928;&#19988;&#31283;&#20581;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
FTFT: Efficient and Robust Fine-Tuning by Transferring Training Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06588
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21160;&#24577;&#21487;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#26041;&#27861;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#65292;&#36890;&#36807;&#36873;&#23450;&#30340;&#35757;&#32451;&#23454;&#20363;&#24494;&#35843;&#20027;&#27169;&#22411;&#23454;&#29616;&#27604;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#20998;&#24067;&#22806;&#36755;&#20837;&#30340;&#24433;&#21709;&#12290; &#25968;&#25454;&#38598;&#21046;&#22270;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21452;&#27169;&#22411;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#24494;&#35843;PLMs&#30340;&#40065;&#26834;&#24615;&#12290; &#23427;&#28041;&#21450;&#22312;&#21407;&#22987;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65288;&#21363;&#21442;&#32771;&#27169;&#22411;&#65289;&#65292;&#26681;&#25454;&#35757;&#32451;&#21160;&#24577;&#36873;&#25321;&#19968;&#20123;&#37325;&#35201;&#30340;&#35757;&#32451;&#23454;&#20363;&#65292;&#24182;&#20165;&#23545;&#36825;&#20123;&#36873;&#23450;&#30340;&#31034;&#20363;&#20877;&#27425;&#36827;&#34892;&#24494;&#35843;&#65288;&#21363;&#20027;&#27169;&#22411;&#65289;&#12290; &#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#23545;&#21516;&#19968;&#27169;&#22411;&#36827;&#34892;&#20004;&#27425;&#24494;&#35843;&#65292;&#36825;&#23545;&#20110;&#22823;&#22411;PLMs&#32780;&#35328;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65288;1&#65289;&#35757;&#32451;&#21160;&#24577;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#26041;&#27861;&#20043;&#38388;&#20855;&#26377;&#39640;&#24230;&#21487;&#20256;&#36882;&#24615;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20351;&#29992;&#36825;&#20123;&#36873;&#23450;&#30340;&#35757;&#32451;&#23454;&#20363;&#23545;&#20027;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#27604;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#23454;&#29616;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290; &#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06588v2 Announce Type: replace  Abstract: Despite the massive success of fine-tuning Pre-trained Language Models (PLMs), they remain susceptible to out-of-distribution input. Dataset cartography is a simple yet effective dual-model approach that improves the robustness of fine-tuned PLMs. It involves fine-tuning a model on the original training set (i.e. reference model), selecting a subset of important training instances based on the training dynamics, and fine-tuning again only on these selected examples (i.e. main model). However, this approach requires fine-tuning the same model twice, which is computationally expensive for large PLMs. In this paper, we show that (1) training dynamics are highly transferable across model sizes and pre-training methods, and that (2) fine-tuning main models using these selected training instances achieves higher training efficiency than empirical risk minimization (ERM). Building on these observations, we propose a novel fine-tuning approa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36130;&#20135;&#35777;&#26126;&#27010;&#24565;&#65292;&#21487;&#20197;&#22312;&#19981;&#27844;&#38706;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21521;&#39564;&#35777;&#32773;&#23637;&#31034;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2308.09552</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#29305;&#24615;&#30340;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Attesting Distributional Properties of Training Data for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.09552
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36130;&#20135;&#35777;&#26126;&#27010;&#24565;&#65292;&#21487;&#20197;&#22312;&#19981;&#27844;&#38706;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21521;&#39564;&#35777;&#32773;&#23637;&#31034;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#25104;&#21151;&#20276;&#38543;&#30528;&#23545;&#20854;&#21487;&#20449;&#24230;&#30340;&#22686;&#21152;&#20851;&#27880;&#12290;&#19968;&#20123;&#21496;&#27861;&#31649;&#36758;&#21306;&#27491;&#22312;&#20934;&#22791;&#26426;&#22120;&#23398;&#20064;&#30417;&#31649;&#26694;&#26550;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#27880;&#28857;&#26159;&#30830;&#20445;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#26576;&#20123;&#29305;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#29702;&#24819;&#20998;&#24067;&#29305;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36130;&#20135;&#35777;&#26126;&#30340;&#27010;&#24565;&#65292;&#20801;&#35768;&#35777;&#26126;&#32773;&#65288;&#20363;&#22914;&#65292;&#27169;&#22411;&#35757;&#32451;&#32773;&#65289;&#21521;&#39564;&#35777;&#32773;&#65288;&#20363;&#22914;&#65292;&#23458;&#25143;&#65289;&#23637;&#31034;&#35757;&#32451;&#25968;&#25454;&#30340;&#30456;&#20851;&#20998;&#24067;&#29305;&#24615;&#65292;&#32780;&#19981;&#27844;&#38706;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#28151;&#21512;&#36130;&#20135;&#35777;&#26126;&#65292;&#32467;&#21512;&#20102;&#36130;&#20135;&#25512;&#29702;&#19982;&#21152;&#23494;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.09552v3 Announce Type: replace-cross  Abstract: The success of machine learning (ML) has been accompanied by increased concerns about its trustworthiness. Several jurisdictions are preparing ML regulatory frameworks. One such concern is ensuring that model training data has desirable distributional properties for certain sensitive attributes. For example, draft regulations indicate that model trainers are required to show that training datasets have specific distributional properties, such as reflecting diversity of the population. We propose the notion of property attestation allowing a prover (e.g., model trainer) to demonstrate relevant distributional properties of training data to a verifier (e.g., a customer) without revealing the data. We present an effective hybrid property attestation combining property inference with cryptographic mechanisms.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#26032;&#22411;&#26080;&#25968;&#25454;&#26367;&#20195;&#25915;&#20987;&#26041;&#26696;&#65292;&#24341;&#20837;&#28508;&#22312;&#32534;&#30721;&#22686;&#24378;&#65288;LCA&#65289;&#26469;&#25552;&#39640;&#26367;&#20195;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2307.12872</link><description>&lt;p&gt;
&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#28508;&#22312;&#32534;&#30721;&#22686;&#24378;&#30340;&#26080;&#25968;&#25454;&#26367;&#20195;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Latent Code Augmentation Based on Stable Diffusion for Data-free Substitute Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.12872
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#26032;&#22411;&#26080;&#25968;&#25454;&#26367;&#20195;&#25915;&#20987;&#26041;&#26696;&#65292;&#24341;&#20837;&#28508;&#22312;&#32534;&#30721;&#22686;&#24378;&#65288;LCA&#65289;&#26469;&#25552;&#39640;&#26367;&#20195;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#40657;&#30418;&#26367;&#20195;&#25915;&#20987;&#20013;&#65292;&#30001;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#19981;&#21487;&#29992;&#65292;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#26041;&#26696;&#21033;&#29992;GAN&#29983;&#25104;&#25968;&#25454;&#26469;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;GAN&#30340;&#26041;&#26696;&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#29983;&#25104;&#22120;&#22312;&#26367;&#20195;&#35757;&#32451;&#36807;&#31243;&#20013;&#27599;&#27425;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#20026;&#30446;&#26631;&#27169;&#22411;&#65292;&#21516;&#26102;&#29983;&#25104;&#36136;&#37327;&#19981;&#39640;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#32771;&#34385;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#30340;&#26032;&#22411;&#26080;&#25968;&#25454;&#26367;&#20195;&#25915;&#20987;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#26367;&#20195;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;SD&#29983;&#25104;&#30340;&#25968;&#25454;&#36136;&#37327;&#24456;&#39640;&#65292;&#20294;&#23427;&#21576;&#29616;&#20986;&#19981;&#21516;&#30340;&#39046;&#22495;&#20998;&#24067;&#20197;&#21450;&#30446;&#26631;&#27169;&#22411;&#27491;&#36127;&#26679;&#26412;&#30340;&#22823;&#21464;&#21270;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#32534;&#30721;&#22686;&#24378;&#65288;LCA&#65289;&#26469;&#20419;&#36827;SD&#29983;&#25104;&#19982;&#30446;&#26631;&#25968;&#25454;&#23545;&#40784;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.12872v2 Announce Type: replace-cross  Abstract: Since the training data of the target model is not available in the black-box substitute attack, most recent schemes utilize GANs to generate data for training the substitute model. However, these GANs-based schemes suffer from low training efficiency as the generator needs to be retrained for each target model during the substitute training process, as well as low generation quality. To overcome these limitations, we consider utilizing the diffusion model to generate data, and propose a novel data-free substitute attack scheme based on the Stable Diffusion (SD) to improve the efficiency and accuracy of substitute training. Despite the data generated by the SD exhibiting high quality, it presents a different distribution of domains and a large variation of positive and negative samples for the target model. For this problem, we propose Latent Code Augmentation (LCA) to facilitate SD in generating data that aligns with the data 
&lt;/p&gt;</description></item><item><title>&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DANSE&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#38750;&#32447;&#24615;&#29366;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#27169;&#22411;&#26080;&#20851;&#36807;&#31243;&#20013;&#30340;&#28508;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#12290;</title><link>https://arxiv.org/abs/2306.03897</link><description>&lt;p&gt;
DANSE: &#26080;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#20013;&#27169;&#22411;&#26080;&#20851;&#36807;&#31243;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#38750;&#32447;&#24615;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DANSE: Data-driven Non-linear State Estimation of Model-free Process in Unsupervised Learning Setup
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.03897
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DANSE&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#38750;&#32447;&#24615;&#29366;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#27169;&#22411;&#26080;&#20851;&#36807;&#31243;&#20013;&#30340;&#28508;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#20013;&#38024;&#23545;&#27169;&#22411;&#26080;&#20851;&#36807;&#31243;&#30340;&#36125;&#21494;&#26031;&#29366;&#24577;&#20272;&#35745;&#21644;&#39044;&#27979;&#20219;&#21153;&#12290;&#23545;&#20110;&#27169;&#22411;&#26080;&#20851;&#36807;&#31243;&#65292;&#25105;&#20204;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#36807;&#31243;&#21160;&#24577;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DANSE&#8212;&#8212;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#38750;&#32447;&#24615;&#29366;&#24577;&#20272;&#35745;&#26041;&#27861;&#12290;DANSE&#25552;&#20379;&#20102;&#32473;&#23450;&#29366;&#24577;&#30340;&#32447;&#24615;&#27979;&#37327;&#30340;&#23553;&#38381;&#24418;&#24335;&#21518;&#39564;&#27010;&#29575;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25552;&#20379;&#20102;&#39044;&#27979;&#30340;&#23553;&#38381;&#24418;&#24335;&#21518;&#39564;&#27010;&#29575;&#12290;DANSE&#20013;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#26469;&#25552;&#20379;&#29366;&#24577;&#20808;&#39564;&#30340;&#21442;&#25968;&#12290;&#20808;&#39564;&#20381;&#36182;&#20110;&#36807;&#21435;&#30340;&#27979;&#37327;&#20316;&#20026;&#36755;&#20837;&#65292;&#28982;&#21518;&#20351;&#29992;&#24403;&#21069;&#27979;&#37327;&#20316;&#20026;&#36755;&#20837;&#25214;&#21040;&#29366;&#24577;&#30340;&#23553;&#38381;&#24418;&#24335;&#21518;&#39564;&#27010;&#29575;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;RNN&#25429;&#25417;&#27169;&#22411;&#26080;&#20851;&#36807;&#31243;&#30340;&#28508;&#22312;&#38750;&#32447;&#24615;&#21160;&#24577;&#12290;DANSE&#30340;&#35757;&#32451;&#65292;&#20027;&#35201;&#26159;&#23398;&#20064;RNN&#30340;&#21442;&#25968;&#65292;&#26159;&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.03897v2 Announce Type: replace-cross  Abstract: We address the tasks of Bayesian state estimation and forecasting for a model-free process in an unsupervised learning setup. For a model-free process, we do not have any a-priori knowledge of the process dynamics. In the article, we propose DANSE -- a Data-driven Nonlinear State Estimation method. DANSE provides a closed-form posterior of the state of the model-free process, given linear measurements of the state. In addition, it provides a closed-form posterior for forecasting. A data-driven recurrent neural network (RNN) is used in DANSE to provide the parameters of a prior of the state. The prior depends on the past measurements as input, and then we find the closed-form posterior of the state using the current measurement as input. The data-driven RNN captures the underlying non-linear dynamics of the model-free process. The training of DANSE, mainly learning the parameters of the RNN, is executed using an unsupervised lea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#23454;&#20307;&#21305;&#37197;&#26694;&#26550; Juno&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#25991;&#26723;&#20013;&#25991;&#26412;&#36328;&#24230;&#19982;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#35821;&#20041;&#30456;&#20284;&#20803;&#32452;&#30340;&#21305;&#37197;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20449;&#24687;&#26816;&#32034;&#20013;&#32570;&#20047;&#19978;&#19979;&#25991;&#21644;&#35265;&#35299;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2303.00720</link><description>&lt;p&gt;
&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#30340;&#36328;&#27169;&#24577;&#23454;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Entity Matching for Visually Rich Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.00720
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#23454;&#20307;&#21305;&#37197;&#26694;&#26550; Juno&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#25991;&#26723;&#20013;&#25991;&#26412;&#36328;&#24230;&#19982;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#35821;&#20041;&#30456;&#20284;&#20803;&#32452;&#30340;&#21305;&#37197;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20449;&#24687;&#26816;&#32034;&#20013;&#32570;&#20047;&#19978;&#19979;&#25991;&#21644;&#35265;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#65288;&#22914;&#20256;&#21333;&#12289;&#27178;&#24133;&#12289;&#26434;&#24535;&#25991;&#31456;&#65289;&#26159;&#21033;&#29992;&#35270;&#35273;&#32447;&#32034;&#26469;&#22686;&#24378;&#35821;&#20041;&#30340;&#23454;&#20307;&#25110;&#25968;&#23383;&#21270;&#25991;&#26723;&#12290;&#36825;&#20123;&#25991;&#26723;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#24448;&#24448;&#26159;&#20020;&#26102;&#30340;&#65292;&#32463;&#24120;&#26159;&#19981;&#23436;&#25972;&#30340;&#12290;&#29616;&#26377;&#30340;&#20801;&#35768;&#23545;&#36825;&#20123;&#25991;&#26723;&#36827;&#34892;&#32467;&#26500;&#21270;&#26597;&#35810;&#30340;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#12290;&#36825;&#20351;&#24471;&#22312;&#20174;&#36825;&#20123;&#25991;&#26723;&#20013;&#36827;&#34892;&#26597;&#35810;&#24182;&#20174;&#20013;&#33719;&#21462;&#21487;&#25805;&#20316;&#35265;&#35299;&#26102;&#65292;&#24456;&#38590;&#23558;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#32622;&#20110;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Juno - &#19968;&#20010;&#36328;&#27169;&#24577;&#23454;&#20307;&#21305;&#37197;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#12290;&#23427;&#36890;&#36807;&#23558;&#25991;&#26723;&#20013;&#30340;&#25991;&#26412;&#36328;&#24230;&#19982;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#35821;&#20041;&#31867;&#20284;&#30340;&#20803;&#32452;&#36827;&#34892;&#21305;&#37197;&#65292;&#20174;&#32780;&#20026;&#24322;&#26500;&#25991;&#26723;&#25552;&#20379;&#34917;&#20805;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#20010;&#24102;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#21305;&#37197;&#65292;&#36890;&#36807;&#22312;&#22810;&#27169;&#24577;&#32534;&#30721;&#31354;&#38388;&#19978;&#23545;&#40784;&#25991;&#26412;&#36328;&#24230;&#21644;&#20851;&#31995;&#20803;&#32452;&#26469;&#25214;&#21040;&#21305;&#37197;&#30340;&#20803;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.00720v2 Announce Type: replace  Abstract: Visually rich documents (e.g. leaflets, banners, magazine articles) are physical or digital documents that utilize visual cues to augment their semantics. Information contained in these documents are ad-hoc and often incomplete. Existing works that enable structured querying on these documents do not take this into account. This makes it difficult to contextualize the information retrieved from querying these documents and gather actionable insights from them. We propose Juno -- a cross-modal entity matching framework to address this limitation. It augments heterogeneous documents with supplementary information by matching a text span in the document with semantically similar tuples from an external database. Our main contribution in this is a deep neural network with attention that goes beyond traditional keyword-based matching and finds matching tuples by aligning text spans and relational tuples on a multimodal encoding space with
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#39640;&#26031;-&#29275;&#39039;&#26102;&#24046;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#36827;&#23545;&#20840;&#23616;&#26368;&#20248;Q&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2302.13087</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#39640;&#26031;-&#29275;&#39039;&#26102;&#24046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Gauss-Newton Temporal Difference Learning with Nonlinear Function Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.13087
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#39640;&#26031;-&#29275;&#39039;&#26102;&#24046;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#36827;&#23545;&#20840;&#23616;&#26368;&#20248;Q&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39640;&#26031;-&#29275;&#39039;&#26102;&#24046;&#65288;GNTD&#65289;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;Q-learning&#38382;&#39064;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#39640;&#26031;-&#29275;&#39039;&#65288;GN&#65289;&#27493;&#39588;&#26469;&#20248;&#21270;&#19968;&#31181;&#21464;&#20307;&#30340;&#22343;&#26041;&#36125;&#23572;&#26364;&#35823;&#24046;&#65288;MSBE&#65289;&#65292;&#20854;&#20013;&#37319;&#29992;&#30446;&#26631;&#32593;&#32476;&#26469;&#36991;&#20813;&#21452;&#37325;&#37319;&#26679;&#12290;&#20998;&#26512;&#20102;&#19981;&#31934;&#30830;&#30340;GN&#27493;&#39588;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#24265;&#20215;&#30340;&#30697;&#38453;&#36845;&#20195;&#23433;&#20840;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;GN&#26356;&#26032;&#12290;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#38024;&#23545;&#21508;&#31181;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#25512;&#23548;&#20102;&#26377;&#38480;&#26679;&#26412;&#38750;&#28176;&#36817;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;Q&#20989;&#25968;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;relu&#28608;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;GNTD&#36798;&#21040;&#20102;$\tilde{\mathcal{O}}(\varepsilon^{-1})$&#30340;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#32780;&#29616;&#26377;&#31070;&#32463;&#32593;&#32476;TD&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{\mathcal{O}}(\varepsilon^{-2})$&#12290;&#26368;&#36817;&#38454;$\tilde{\mathcal{O}}(\varepsilon^{-1.5})$&#30340;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.13087v2 Announce Type: replace-cross  Abstract: In this paper, a Gauss-Newton Temporal Difference (GNTD) learning method is proposed to solve the Q-learning problem with nonlinear function approximation. In each iteration, our method takes one Gauss-Newton (GN) step to optimize a variant of Mean-Squared Bellman Error (MSBE), where target networks are adopted to avoid double sampling. Inexact GN steps are analyzed so that one can safely and efficiently compute the GN updates by cheap matrix iterations. Under mild conditions, non-asymptotic finite-sample convergence to the globally optimal Q function is derived for various nonlinear function approximations. In particular, for neural network parameterization with relu activation, GNTD achieves an improved sample complexity of $\tilde{\mathcal{O}}(\varepsilon^{-1})$, as opposed to the $\mathcal{\mathcal{O}}(\varepsilon^{-2})$ sample complexity of the existing neural TD methods. An $\tilde{\mathcal{O}}(\varepsilon^{-1.5})$ sample
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#34701;&#21512;&#30340;&#22810;&#31890;&#24230;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#19981;&#21516;&#31890;&#24230;&#30340;&#22478;&#24066;&#20043;&#38388;&#23454;&#29616;&#20132;&#36890;&#39044;&#27979;&#30693;&#35782;&#36716;&#31227;</title><link>https://arxiv.org/abs/2302.11774</link><description>&lt;p&gt;
&#35821;&#20041;&#34701;&#21512;&#30340;&#22810;&#31890;&#24230;&#36328;&#22478;&#24066;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semantic-Fused Multi-Granularity Cross-City Traffic Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.11774
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#34701;&#21512;&#30340;&#22810;&#31890;&#24230;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#19981;&#21516;&#31890;&#24230;&#30340;&#22478;&#24066;&#20043;&#38388;&#23454;&#29616;&#20132;&#36890;&#39044;&#27979;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#20132;&#36890;&#39044;&#27979;&#23545;&#26377;&#25928;&#22478;&#24066;&#31649;&#29702;&#21644;&#25552;&#39640;&#20132;&#36890;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#24471;&#21040;&#20102;&#24191;&#27867;&#37319;&#29992;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#65292;&#37492;&#20110;&#20256;&#24863;&#22522;&#30784;&#35774;&#26045;&#19981;&#36275;&#30340;&#22320;&#21306;&#26222;&#36941;&#23384;&#22312;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#36825;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#34701;&#21512;&#30340;&#22810;&#31890;&#24230;&#36801;&#31227;&#23398;&#20064;&#65288;SFMGTL&#65289;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#20855;&#26377;&#19981;&#21516;&#31890;&#24230;&#30340;&#34701;&#21512;&#35821;&#20041;&#30340;&#22478;&#24066;&#20043;&#38388;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35821;&#20041;&#34701;&#21512;&#27169;&#22359;&#65292;&#36890;&#36807;&#37325;&#26500;&#25439;&#22833;&#34701;&#21512;&#21508;&#31181;&#35821;&#20041;&#65292;&#21516;&#26102;&#20445;&#30041;&#38745;&#24577;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#33410;&#28857;&#29305;&#24449;&#26500;&#24314;&#20102;&#19968;&#20010;&#34701;&#21512;&#22270;&#65292;&#24182;&#36890;&#36807;&#22270;&#32467;&#26500;&#23398;&#20064;&#23454;&#29616;&#23618;&#27425;&#33410;&#28857;&#32858;&#31867;&#65292;&#29983;&#25104;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.11774v2 Announce Type: replace  Abstract: Accurate traffic prediction is essential for effective urban management and the improvement of transportation efficiency. Recently, data-driven traffic prediction methods have been widely adopted, with better performance than traditional approaches. However, they often require large amounts of data for effective training, which becomes challenging given the prevalence of data scarcity in regions with inadequate sensing infrastructures. To address this issue, we propose a Semantic-Fused Multi-Granularity Transfer Learning (SFMGTL) model to achieve knowledge transfer across cities with fused semantics at different granularities. In detail, we design a semantic fusion module to fuse various semantics while conserving static spatial dependencies via reconstruction losses. Then, a fused graph is constructed based on node features through graph structure learning. Afterwards, we implement hierarchical node clustering to generate graphs wit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#19981;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#30495;&#20266;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#38750;&#26126;&#30830;&#21306;&#20998;&#29305;&#24449;&#31354;&#38388;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#26469;&#28304;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2302.10174</link><description>&lt;p&gt;
&#36890;&#21521;&#36328;&#29983;&#25104;&#27169;&#22411;&#27867;&#21270;&#30340;&#36890;&#29992;&#20551;&#22270;&#20687;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards Universal Fake Image Detectors that Generalize Across Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.10174
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#19981;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#30495;&#20266;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#38750;&#26126;&#30830;&#21306;&#20998;&#29305;&#24449;&#31354;&#38388;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#26469;&#28304;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#22686;&#22810;&#65292;&#23545;&#20110;&#36890;&#29992;&#30446;&#30340;&#30340;&#20551;&#22270;&#20687;&#26816;&#27979;&#22120;&#30340;&#38656;&#27714;&#27491;&#22312;&#22686;&#38271;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#29616;&#26377;&#33539;&#24335;&#30340;&#22833;&#36133;&#65292;&#35813;&#33539;&#24335;&#21253;&#25324;&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#30495;&#20551;&#20998;&#31867;&#65292;&#24403;&#35757;&#32451;&#20197;&#26816;&#27979;GAN&#20266;&#22270;&#20687;&#26102;&#65292;&#26080;&#27861;&#26816;&#27979;&#21040;&#26469;&#33258;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#30340;&#20551;&#22270;&#20687;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24471;&#21040;&#30340;&#20998;&#31867;&#22120;&#23545;&#26816;&#27979;&#20351;&#22270;&#20687;&#20266;&#36896;&#30340;&#27169;&#24335;&#36827;&#34892;&#20102;&#19981;&#23545;&#31216;&#35843;&#25972;&#12290;&#30495;&#23454;&#31867;&#25104;&#20026;&#19968;&#20010;&#30427;&#25918;&#20219;&#20309;&#38750;&#20551;&#30340;&#19996;&#35199;&#30340;&#27719;&#31867;&#65292;&#21253;&#25324;&#22312;&#35757;&#32451;&#26399;&#38388;&#26080;&#27861;&#35775;&#38382;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#25191;&#34892;&#30495;&#20266;&#20998;&#31867;&#32780;&#19981;&#36827;&#34892;&#23398;&#20064;&#65307;&#21363;&#20351;&#29992;&#38750;&#26126;&#30830;&#35757;&#32451;&#20197;&#21306;&#20998;&#30495;&#20551;&#22270;&#20687;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#37051;&#21644;&#32447;&#24615;&#25506;&#26597;&#20316;&#20026;&#36825;&#19968;&#24819;&#27861;&#30340;&#23454;&#20363;&#21270;&#12290;&#24403;&#25552;&#20379;&#23545;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#29305;&#24449;&#31354;&#38388;&#30340;&#35775;&#38382;&#26102;&#65292;&#25105;&#20204;&#33021;&#22815;&#27604;&#29616;&#26377;&#30340;[image-based]&#23398;&#20064;&#26041;&#27861;&#26356;&#22909;&#22320;&#21306;&#20998;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#28304;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.10174v2 Announce Type: replace-cross  Abstract: With generative models proliferating at a rapid rate, there is a growing need for general purpose fake image detectors. In this work, we first show that the existing paradigm, which consists of training a deep network for real-vs-fake classification, fails to detect fake images from newer breeds of generative models when trained to detect GAN fake images. Upon analysis, we find that the resulting classifier is asymmetrically tuned to detect patterns that make an image fake. The real class becomes a sink class holding anything that is not fake, including generated images from models not accessible during training. Building upon this discovery, we propose to perform real-vs-fake classification without learning; i.e., using a feature space not explicitly trained to distinguish real from fake images. We use nearest neighbor and linear probing as instantiations of this idea. When given access to the feature space of a large pretrain
&lt;/p&gt;</description></item><item><title>&#30456;&#20284;&#24615;&#12289;&#21387;&#32553;&#21644;&#23616;&#37096;&#26356;&#26032;&#26159;&#26412;&#25991;&#25552;&#20986;&#30340;&#19977;&#22823;&#25216;&#26415;&#65292;&#29992;&#20110;&#20943;&#23569;&#20998;&#24067;&#24335;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#20013;&#36890;&#20449;&#36718;&#27425;&#21644;&#25104;&#26412;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#19977;&#37325;&#21327;&#21516;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2302.07615</link><description>&lt;p&gt;
&#30456;&#20284;&#24615;&#12289;&#21387;&#32553;&#21644;&#23616;&#37096;&#27493;&#39588;&#65306;&#20998;&#24067;&#24335;&#21464;&#20998;&#19981;&#31561;&#24335;&#39640;&#25928;&#36890;&#20449;&#30340;&#19977;&#22823;&#25903;&#26609;
&lt;/p&gt;
&lt;p&gt;
Similarity, Compression and Local Steps: Three Pillars of Efficient Communications for Distributed Variational Inequalities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.07615
&lt;/p&gt;
&lt;p&gt;
&#30456;&#20284;&#24615;&#12289;&#21387;&#32553;&#21644;&#23616;&#37096;&#26356;&#26032;&#26159;&#26412;&#25991;&#25552;&#20986;&#30340;&#19977;&#22823;&#25216;&#26415;&#65292;&#29992;&#20110;&#20943;&#23569;&#20998;&#24067;&#24335;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#20013;&#36890;&#20449;&#36718;&#27425;&#21644;&#25104;&#26412;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#19977;&#37325;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#19981;&#31561;&#24335;&#26159;&#19968;&#20010;&#24191;&#27867;&#32780;&#28789;&#27963;&#30340;&#38382;&#39064;&#31867;&#65292;&#21253;&#25324;&#26368;&#23567;&#21270;&#12289;&#38797;&#28857;&#21644;&#19981;&#21160;&#28857;&#38382;&#39064;&#20316;&#20026;&#29305;&#20363;&#12290;&#22240;&#27492;&#65292;&#21464;&#20998;&#19981;&#31561;&#24335;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#20351;&#29992;&#65292;&#20174;&#22343;&#34913;&#25628;&#32034;&#21040;&#23545;&#25239;&#23398;&#20064;&#37117;&#26377;&#28041;&#21450;&#12290;&#38543;&#30528;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#24403;&#20170;&#30340;&#23454;&#20363;&#38656;&#35201;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#35745;&#31639;&#26469;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#21487;&#20197;&#34920;&#31034;&#20026;&#21464;&#20998;&#19981;&#31561;&#24335;&#12290;&#21516;&#26102;&#65292;&#22823;&#22810;&#25968;&#20998;&#24067;&#24335;&#26041;&#27861;&#23384;&#22312;&#19968;&#20010;&#37325;&#22823;&#29942;&#39048; - &#36890;&#20449;&#25104;&#26412;&#12290;&#20943;&#23569;&#36890;&#20449;&#36718;&#27425;&#30340;&#24635;&#25968;&#21644;&#27599;&#36718;&#25104;&#26412;&#30340;&#19977;&#31181;&#20027;&#35201;&#25216;&#26415;&#26159;&#26412;&#22320;&#20989;&#25968;&#30340;&#30456;&#20284;&#24615;&#12289;&#20256;&#36755;&#20449;&#24687;&#30340;&#21387;&#32553;&#21644;&#23616;&#37096;&#26356;&#26032;&#12290;&#26412;&#25991;&#32467;&#21512;&#20102;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#12290;&#23545;&#20110;&#21464;&#20998;&#19981;&#31561;&#24335;&#21644;&#38797;&#28857;&#38382;&#39064;&#26469;&#35828;&#65292;&#36825;&#26679;&#30340;&#19977;&#37325;&#21327;&#21516;&#20316;&#29992;&#20197;&#21069;&#24182;&#19981;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.07615v2 Announce Type: replace-cross  Abstract: Variational inequalities are a broad and flexible class of problems that includes minimization, saddle point, and fixed point problems as special cases. Therefore, variational inequalities are used in various applications ranging from equilibrium search to adversarial learning. With the increasing size of data and models, today's instances demand parallel and distributed computing for real-world machine learning problems, most of which can be represented as variational inequalities. Meanwhile, most distributed approaches have a significant bottleneck - the cost of communications. The three main techniques to reduce the total number of communication rounds and the cost of one such round are the similarity of local functions, compression of transmitted information, and local updates. In this paper, we combine all these approaches. Such a triple synergy did not exist before for variational inequalities and saddle problems, nor eve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23558;&#35757;&#32451;&#26500;&#24314;&#20026;&#24369;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#19981;&#23436;&#25972;&#27880;&#37322;&#25968;&#25454;&#26816;&#27979;&#24694;&#24615;&#20083;&#33146;&#30149;&#21464;&#30340;&#22256;&#22659;</title><link>https://arxiv.org/abs/2301.13418</link><description>&lt;p&gt;
BRAIxDet&#65306;&#23398;&#20064;&#20351;&#29992;&#19981;&#23436;&#25972;&#27880;&#37322;&#26816;&#27979;&#24694;&#24615;&#20083;&#33146;&#30149;&#21464;
&lt;/p&gt;
&lt;p&gt;
BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23558;&#35757;&#32451;&#26500;&#24314;&#20026;&#24369;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#36890;&#36807;&#19981;&#23436;&#25972;&#27880;&#37322;&#25968;&#25454;&#26816;&#27979;&#24694;&#24615;&#20083;&#33146;&#30149;&#21464;&#30340;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31579;&#26597;&#20083;&#25151;X&#32447;&#29031;&#29255;&#20013;&#26816;&#27979;&#24694;&#24615;&#30149;&#21464;&#30340;&#26041;&#27861;&#36890;&#24120;&#26159;&#20351;&#29992;&#20855;&#26377;&#23436;&#20840;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#22270;&#20687;&#34987;&#26631;&#35760;&#20026;&#30284;&#30151;&#30149;&#21464;&#30340;&#23450;&#20301;&#21644;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#31579;&#26597;&#20083;&#25151;X&#32447;&#29031;&#29255;&#25968;&#25454;&#38598;&#36890;&#24120;&#26377;&#19968;&#20010;&#37096;&#20998;&#26159;&#23436;&#20840;&#27880;&#37322;&#30340;&#65292;&#21478;&#19968;&#20010;&#37096;&#20998;&#21482;&#26377;&#20840;&#23616;&#20998;&#31867;&#30340;&#24369;&#27880;&#37322;&#65288;&#21363;&#27809;&#26377;&#30149;&#21464;&#23450;&#20301;&#65289;&#12290;&#37492;&#20110;&#36825;&#31867;&#25968;&#25454;&#38598;&#30340;&#24222;&#22823;&#35268;&#27169;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#22312;&#24369;&#27880;&#37322;&#23376;&#38598;&#38754;&#20020;&#20004;&#38590;&#36873;&#25321;&#65306;&#35201;&#20040;&#19981;&#20351;&#29992;&#23427;&#65292;&#35201;&#20040;&#23436;&#20840;&#27880;&#37322;&#23427;&#12290;&#31532;&#19968;&#31181;&#36873;&#25321;&#20250;&#38477;&#20302;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#20351;&#29992;&#25972;&#20010;&#25968;&#25454;&#38598;&#65292;&#32780;&#31532;&#20108;&#31181;&#36873;&#25321;&#21017;&#36807;&#20110;&#26114;&#36149;&#65292;&#22240;&#20026;&#27880;&#37322;&#38656;&#35201;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#24072;&#23436;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#19968;&#22256;&#22659;&#30340;&#19968;&#20010;&#20013;&#38388;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23558;&#35757;&#32451;&#26500;&#24314;&#20026;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026;&#24694;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13418v3 Announce Type: replace-cross  Abstract: Methods to detect malignant lesions from screening mammograms are usually trained with fully annotated datasets, where images are labelled with the localisation and classification of cancerous lesions. However, real-world screening mammogram datasets commonly have a subset that is fully annotated and another subset that is weakly annotated with just the global classification (i.e., without lesion localisation). Given the large size of such datasets, researchers usually face a dilemma with the weakly annotated subset: to not use it or to fully annotate it. The first option will reduce detection accuracy because it does not use the whole dataset, and the second option is too expensive given that the annotation needs to be done by expert radiologists. In this paper, we propose a middle-ground solution for the dilemma, which is to formulate the training as a weakly- and semi-supervised learning problem that we refer to as malignant
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21487;&#20132;&#25442;&#24615;&#21644;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#27880;&#24847;&#21147;&#26426;&#21046;&#24037;&#20316;&#21407;&#29702;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;transformers&#22914;&#20309;&#23454;&#29616;&#23545;&#38271;&#24207;&#21015;&#36827;&#34892;&#20851;&#31995;&#25512;&#26029;&#20197;&#20135;&#29983;&#29702;&#24819;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2212.14852</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#20132;&#25442;&#24615;&#21644;&#28508;&#21464;&#37327;&#27169;&#22411;&#20998;&#26512;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Attention via the Lens of Exchangeability and Latent Variable Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.14852
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21487;&#20132;&#25442;&#24615;&#21644;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#27880;&#24847;&#21147;&#26426;&#21046;&#24037;&#20316;&#21407;&#29702;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;transformers&#22914;&#20309;&#23454;&#29616;&#23545;&#38271;&#24207;&#21015;&#36827;&#34892;&#20851;&#31995;&#25512;&#26029;&#20197;&#20135;&#29983;&#29702;&#24819;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;transformers&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#25104;&#21151;&#12290;&#23613;&#31649;&#25105;&#20204;&#30452;&#35273;&#22320;&#35748;&#20026;transformers&#36890;&#36807;&#23545;&#38271;&#24207;&#21015;&#36827;&#34892;&#20851;&#31995;&#25512;&#26029;&#26469;&#20135;&#29983;&#29702;&#24819;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#23545;&#27880;&#24847;&#21147;&#26426;&#21046;&#22914;&#20309;&#23454;&#29616;&#27492;&#30446;&#26631;&#30340;&#20005;&#26684;&#29702;&#35770;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#65292;&#27491;&#22914;&#22312;BERT&#21644;ViT&#20013;&#19968;&#26679;&#65292;&#36755;&#20837;&#26631;&#35760;&#36890;&#24120;&#26159;&#21487;&#20132;&#25442;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#24050;&#32463;&#21253;&#21547;&#20301;&#32622;&#32534;&#30721;&#12290;&#21487;&#20132;&#25442;&#24615;&#30340;&#27010;&#24565;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#36755;&#20837;&#22823;&#23567;&#19981;&#21464;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#24471;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.14852v3 Announce Type: replace  Abstract: With the attention mechanism, transformers achieve significant empirical successes. Despite the intuitive understanding that transformers perform relational inference over long sequences to produce desirable representations, we lack a rigorous theory on how the attention mechanism achieves it. In particular, several intriguing questions remain open: (a) What makes a desirable representation? (b) How does the attention mechanism infer the desirable representation within the forward pass? (c) How does a pretraining procedure learn to infer the desirable representation through the backward pass?   We observe that, as is the case in BERT and ViT, input tokens are often exchangeable since they already include positional encodings. The notion of exchangeability induces a latent variable model that is invariant to input sizes, which enables our theoretical analysis.   - To answer (a) on representation, we establish the existence of a suffic
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20197;&#22826;&#22346;Ponzi&#26816;&#27979;&#30340;&#26102;&#24207;&#20803;&#36335;&#24452;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#25429;&#33719;&#20132;&#26131;&#27169;&#24335;&#20449;&#24687;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2210.16863</link><description>&lt;p&gt;
&#20197;&#22826;&#22346;Ponzi&#26816;&#27979;&#30340;&#26102;&#24207;&#20803;&#36335;&#24452;&#29305;&#24449;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Time-aware Metapath Feature Augmentation for Ponzi Detection in Ethereum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.16863
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20197;&#22826;&#22346;Ponzi&#26816;&#27979;&#30340;&#26102;&#24207;&#20803;&#36335;&#24452;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#25429;&#33719;&#20132;&#26131;&#27169;&#24335;&#20449;&#24687;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#35843;&#21435;&#20013;&#24515;&#21270;&#30340;Web 3.0&#30340;&#21457;&#23637;&#65292;&#21306;&#22359;&#38142;&#25216;&#26415;&#36814;&#26469;&#20102;&#33258;&#24049;&#30340;&#38761;&#21629;&#65292;&#24182;&#24102;&#26469;&#20102;&#35768;&#22810;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#21152;&#23494;&#36135;&#24065;&#39046;&#22495;&#12290;&#26368;&#36817;&#65292;&#22823;&#37327;&#30340;&#29359;&#32618;&#34892;&#20026;&#19981;&#26029;&#22312;&#21306;&#22359;&#38142;&#19978;&#20986;&#29616;&#65292;&#22914;&#24222;&#27663;&#39575;&#23616;&#21644;&#38035;&#40060;&#27450;&#35784;&#65292;&#36825;&#20005;&#37325;&#21361;&#23475;&#20102;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#12290;&#29616;&#26377;&#22522;&#20110;&#22270;&#30340;&#21306;&#22359;&#38142;&#24322;&#24120;&#34892;&#20026;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#20391;&#37325;&#20110;&#26500;&#24314;&#21516;&#36136;&#20132;&#26131;&#22270;&#65292;&#32780;&#27809;&#26377;&#21306;&#20998;&#33410;&#28857;&#21644;&#36793;&#30340;&#24322;&#36136;&#24615;&#65292;&#23548;&#33268;&#20132;&#26131;&#27169;&#24335;&#20449;&#24687;&#30340;&#37096;&#20998;&#20002;&#22833;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#24322;&#36136;&#24314;&#27169;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20803;&#36335;&#24452;&#25551;&#36848;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20294;&#25552;&#21462;&#30340;&#20803;&#36335;&#24452;&#36890;&#24120;&#24573;&#35270;&#23454;&#20307;&#20043;&#38388;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#19981;&#21453;&#26144;&#30495;&#23454;&#34892;&#20026;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26102;&#38388;&#24863;&#30693;&#20803;&#36335;&#24452;&#29305;&#24449;&#22686;&#24378;&#65288;TMFAug&#65289;&#20316;&#20026;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#29992;&#20110;&#25429;&#33719;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.16863v2 Announce Type: replace  Abstract: With the development of Web 3.0 which emphasizes decentralization, blockchain technology ushers in its revolution and also brings numerous challenges, particularly in the field of cryptocurrency. Recently, a large number of criminal behaviors continuously emerge on blockchain, such as Ponzi schemes and phishing scams, which severely endanger decentralized finance. Existing graph-based abnormal behavior detection methods on blockchain usually focus on constructing homogeneous transaction graphs without distinguishing the heterogeneity of nodes and edges, resulting in partial loss of transaction pattern information. Although existing heterogeneous modeling methods can depict richer information through metapaths, the extracted metapaths generally neglect temporal dependencies between entities and do not reflect real behavior. In this paper, we introduce Time-aware Metapath Feature Augmentation (TMFAug) as a plug-and-play module to captu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#21608;&#26399;&#28608;&#27963;&#20989;&#25968;&#30340;&#21551;&#21457;&#21644;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMMs&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#21512;&#25104;&#36712;&#36857;&#25968;&#25454;&#22686;&#24378;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;G-PECNet&#22312;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;9.5%&#30340;&#26368;&#32456;&#20301;&#31227;&#35823;&#24046;&#65288;FDE&#65289;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2210.09846</link><description>&lt;p&gt;
&#26397;&#21521;&#36890;&#29992;&#21270;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#31995;&#32479;&#8212;&#8212;G-PECNet
&lt;/p&gt;
&lt;p&gt;
G-PECNet: Towards a Generalizable Pedestrian Trajectory Prediction System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.09846
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#21608;&#26399;&#28608;&#27963;&#20989;&#25968;&#30340;&#21551;&#21457;&#21644;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMMs&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#21512;&#25104;&#36712;&#36857;&#25968;&#25454;&#22686;&#24378;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;G-PECNet&#22312;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;9.5%&#30340;&#26368;&#32456;&#20301;&#31227;&#35823;&#24046;&#65288;FDE&#65289;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#35299;&#20915;&#20102;&#33258;&#20027;&#26080;&#20154;&#26426;&#23548;&#33322;&#20013;&#30340;&#23376;&#38382;&#39064;&#65292;&#21363;&#39044;&#27979;&#22495;&#22806;&#20154;&#31867;&#21644;&#20195;&#29702;&#20154;&#36712;&#36857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;General-PECNet&#25110;G-PECNet&#65292;&#36890;&#36807;&#21463;&#21608;&#26399;&#28608;&#27963;&#20989;&#25968;&#21551;&#21457;&#30340;&#26550;&#26500;&#25913;&#36827;&#21644;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMMs&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#21512;&#25104;&#36712;&#36857;&#25968;&#25454;&#22686;&#24378;&#65292;&#22312;2020&#24180;&#22522;&#20934;PECNet&#19978;&#23558;&#26368;&#32456;&#20301;&#31227;&#35823;&#24046;&#65288;FDE&#65289;&#25552;&#39640;&#20102;9.5&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31616;&#21333;&#20960;&#20309;&#24418;&#29366;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#36712;&#36857;&#38750;&#32447;&#24615;&#21644;&#24322;&#24120;&#20540;&#26816;&#27979;&#65292;&#26377;&#21161;&#20110;&#35813;&#20219;&#21153;&#30340;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.09846v3 Announce Type: replace  Abstract: Navigating dynamic physical environments without obstructing or damaging human assets is of quintessential importance for social robots. In this work, we solve autonomous drone navigation's sub-problem of predicting out-of-domain human and agent trajectories using a deep generative model. Our method: General-PECNet or G-PECNet observes an improvement of 9.5\% on the Final Displacement Error (FDE) on 2020's benchmark: PECNet through a combination of architectural improvements inspired by periodic activation functions and synthetic trajectory (data) augmentations using Hidden Markov Models (HMMs) and Reinforcement Learning (RL). Additionally, we propose a simple geometry-inspired metric for trajectory non-linearity and outlier detection, helpful for the task. Code available at https://github.com/Aryan-Garg/PECNet-Pedestrian-Trajectory-Prediction.git
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#23558;&#21512;&#36866;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20013;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#22312;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#31995;&#32479;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2210.06554</link><description>&lt;p&gt;
&#36816;&#29992;XAI&#26041;&#27861;&#20110;&#22522;&#20110;EEG&#30340;&#31995;&#32479;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Toward the application of XAI methods in EEG-based systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.06554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23558;&#21512;&#36866;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#35299;&#20915;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20013;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#22312;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#31995;&#32479;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#36259;&#26696;&#20363;&#26159;&#22312;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#32972;&#26223;&#19979;&#23545;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#12290; EEG&#20449;&#21495;&#30340;&#38750;&#38745;&#27490;&#24615;&#21487;&#33021;&#23548;&#33268;BCI&#20998;&#31867;&#31995;&#32479;&#22312;&#19981;&#21516;&#20250;&#35805;&#20013;&#20351;&#29992;&#26102;&#24615;&#33021;&#27867;&#21270;&#24046;&#65292;&#29978;&#33267;&#26159;&#21516;&#19968;&#34987;&#35797;&#39564;&#12290; &#26412;&#25991;&#30340;&#20986;&#21457;&#28857;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;&#21512;&#36866;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#23450;&#20301;&#21644;&#36716;&#25442;&#36755;&#20837;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#20174;&#32780;&#32531;&#35299;&#25968;&#25454;&#38598;&#36716;&#31227;&#38382;&#39064;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23545;&#20960;&#31181;XAI&#26041;&#27861;&#22312;&#22312;&#20856;&#22411;&#30340;&#29992;&#20110;&#24773;&#32490;&#35782;&#21035;&#30340;EEG&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;ML&#31995;&#32479;&#19978;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#23454;&#39564;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;XAI&#26041;&#27861;&#25214;&#21040;&#30340;&#35768;&#22810;&#30456;&#20851;&#32452;&#20214;&#22312;&#20250;&#35805;&#20043;&#38388;&#26159;&#20849;&#20139;&#30340;&#65292;&#21487;&#20197;&#29992;&#26469;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#26356;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.06554v3 Announce Type: replace-cross  Abstract: An interesting case of the well-known Dataset Shift Problem is the classification of Electroencephalogram (EEG) signals in the context of Brain-Computer Interface (BCI). The non-stationarity of EEG signals can lead to poor generalisation performance in BCI classification systems used in different sessions, also from the same subject. In this paper, we start from the hypothesis that the Dataset Shift problem can be alleviated by exploiting suitable eXplainable Artificial Intelligence (XAI) methods to locate and transform the relevant characteristics of the input for the goal of classification. In particular, we focus on an experimental analysis of explanations produced by several XAI methods on an ML system trained on a typical EEG dataset for emotion recognition. Results show that many relevant components found by XAI methods are shared across the sessions and can be used to build a system able to generalise better. However, re
&lt;/p&gt;</description></item><item><title>&#22312;miniXCOM&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#33258;&#36866;&#24212;MCTS&#31639;&#27861;MCTS-TD&#65292;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#22312;&#32447;&#36866;&#24212;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#28216;&#25103;&#20013;&#34920;&#29616;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2210.05014</link><description>&lt;p&gt;
&#22312;miniXCOM&#20013;&#25506;&#32034;&#33258;&#36866;&#24212;MCTS&#19982;TD&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploring Adaptive MCTS with TD Learning in miniXCOM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.05014
&lt;/p&gt;
&lt;p&gt;
&#22312;miniXCOM&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#33258;&#36866;&#24212;MCTS&#31639;&#27861;MCTS-TD&#65292;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#22312;&#32447;&#36866;&#24212;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#28216;&#25103;&#20013;&#34920;&#29616;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#24180;&#26469;&#65292;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#22312;&#28216;&#25103;&#31038;&#21306;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20351;&#29992;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#21508;&#31181;&#28216;&#25103;&#20013;&#23454;&#26045;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#20174;&#31616;&#21333;&#30340;&#26827;&#30424;&#28216;&#25103;&#21040;&#26356;&#22797;&#26434;&#30340;&#35270;&#39057;&#28216;&#25103;&#65292;&#22914;&#26143;&#38469;&#20105;&#38712;&#65292;&#32780;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20351;&#29992;&#21017;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;MCTS&#20013;&#30340;&#22312;&#32447;&#36866;&#24212;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MCTS-TD&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#25913;&#36827;&#30340;&#33258;&#36866;&#24212;MCTS&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#28216;&#25103;miniXCOM&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#26159;XCOM&#30340;&#19968;&#20010;&#31616;&#21270;&#29256;&#26412;&#65292;XCOM&#26159;&#19968;&#20010;&#21253;&#21547;&#22810;&#27454;&#22238;&#21512;&#21046;&#25112;&#26415;&#28216;&#25103;&#30340;&#27969;&#34892;&#21830;&#19994;&#31995;&#21015;&#65292;&#23637;&#31034;&#20102;MCTS-TD&#20013;&#30340;&#36866;&#24212;&#24615;&#22914;&#20309;&#25552;&#39640;&#20102;&#19982;&#23545;&#25163;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.05014v3 Announce Type: cross  Abstract: In recent years, Monte Carlo tree search (MCTS) has achieved widespread adoption within the game community. Its use in conjunction with deep reinforcement learning has produced success stories in many applications. While these approaches have been implemented in various games, from simple board games to more complicated video games such as StarCraft, the use of deep neural networks requires a substantial training period. In this work, we explore on-line adaptivity in MCTS without requiring pre-training. We present MCTS-TD, an adaptive MCTS algorithm improved with temporal difference learning. We demonstrate our new approach on the game miniXCOM, a simplified version of XCOM, a popular commercial franchise consisting of several turn-based tactical games, and show how adaptivity in MCTS-TD allows for improved performances against opponents.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#21547;&#21327;&#21464;&#37327;&#36716;&#31227;&#65288;LCS&#65289;&#33539;&#24335;&#65292;&#22686;&#21152;&#20102;&#39046;&#22495;&#38388;&#30340;&#21487;&#21464;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24674;&#22797;&#26631;&#31614;&#21464;&#37327;&#28508;&#22312;&#21407;&#22240;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2208.14161</link><description>&lt;p&gt;
&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#22240;&#26524;&#20869;&#23481;&#29992;&#20110;&#38544;&#21547;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Identifiable Latent Causal Content for Domain Adaptation under Latent Covariate Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.14161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#21547;&#21327;&#21464;&#37327;&#36716;&#31227;&#65288;LCS&#65289;&#33539;&#24335;&#65292;&#22686;&#21152;&#20102;&#39046;&#22495;&#38388;&#30340;&#21487;&#21464;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24674;&#22797;&#26631;&#31614;&#21464;&#37327;&#28508;&#22312;&#21407;&#22240;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#35299;&#20915;&#20102;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#28304;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#26469;&#33258;&#30446;&#26631;&#22495;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#23398;&#20064;&#38024;&#23545;&#26410;&#26631;&#35760;&#30446;&#26631;&#39046;&#22495;&#30340;&#26631;&#31614;&#39044;&#27979;&#20989;&#25968;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#65288;LCS&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#23427;&#24341;&#20837;&#20102;&#26356;&#22823;&#30340;&#39046;&#22495;&#38388;&#21487;&#21464;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#20026;&#24674;&#22797;&#26631;&#31614;&#21464;&#37327;&#30340;&#28508;&#22312;&#21407;&#22240;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.14161v3 Announce Type: replace  Abstract: Multi-source domain adaptation (MSDA) addresses the challenge of learning a label prediction function for an unlabeled target domain by leveraging both the labeled data from multiple source domains and the unlabeled data from the target domain. Conventional MSDA approaches often rely on covariate shift or conditional shift paradigms, which assume a consistent label distribution across domains. However, this assumption proves limiting in practical scenarios where label distributions do vary across domains, diminishing its applicability in real-world settings. For example, animals from different regions exhibit diverse characteristics due to varying diets and genetics.   Motivated by this, we propose a novel paradigm called latent covariate shift (LCS), which introduces significantly greater variability and adaptability across domains. Notably, it provides a theoretical assurance for recovering the latent cause of the label variable, w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;GANs&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#20351;&#29992;GANs&#33258;&#36523;&#36827;&#34892;&#21487;&#35270;&#21270;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#21345;&#26041;&#20998;&#24067;&#30340;&#26377;&#25928;&#21487;&#35270;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2208.02649</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#19979;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#33258;&#36523;&#36827;&#34892;&#21487;&#35270;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Visually Evaluating Generative Adversarial Networks Using Itself under Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.02649
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;GANs&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#20351;&#29992;GANs&#33258;&#36523;&#36827;&#34892;&#21487;&#35270;&#21270;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#21345;&#26041;&#20998;&#24067;&#30340;&#26377;&#25928;&#21487;&#35270;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35270;&#21270;&#35780;&#20272;&#29983;&#25104;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#22909;&#22351;&#22312;&#23454;&#29616;&#19978;&#24456;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#27169;&#22411;&#20026;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#39640;&#26031;GANs&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#20351;&#29992;GANs&#33258;&#36523;&#36827;&#34892;&#21487;&#35270;&#21270;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#26174;&#24335;&#37325;&#24314;GANs&#30340;&#26550;&#26500;&#26469;&#22312;&#22810;&#21464;&#37327; Kolmogorov Smirnov&#65288;MKS&#65289;&#27979;&#35797;&#20013;&#25214;&#21040;&#36716;&#25442;&#20989;&#25968;&#12290;&#20854;&#27425;&#65292;&#22312;&#36716;&#25442;&#21518;&#30340;MST&#19978;&#36827;&#34892;&#27491;&#24577;&#24615;&#27979;&#35797;&#65292;&#20854;&#20013;&#39640;&#26031;GANs&#20316;&#20026;MKS&#27979;&#35797;&#20013;&#30340;&#36716;&#25442;&#20989;&#25968;&#12290;&#20026;&#20102;&#31616;&#21270;&#27491;&#24577;&#24615;&#27979;&#35797;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#21345;&#26041;&#20998;&#24067;&#30340;&#26377;&#25928;&#21487;&#35270;&#21270;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;UniMiB&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#20351;&#29992;&#39640;&#26031;GANs&#21644;&#21345;&#26041;&#21487;&#35270;&#21270;&#30340;&#27491;&#24577;&#24615;&#27979;&#35797;&#26159;&#26377;&#25928;&#21644;&#21487;&#20449;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.02649v2 Announce Type: replace  Abstract: Visually evaluating the goodness of generated Multivariate Time Series (MTS) are difficult to implement, especially in the case that the generative model is Generative Adversarial Networks (GANs). We present a general framework named Gaussian GANs to visually evaluate GANs using itself under the MTS generation task. Firstly, we attempt to find the transformation function in the multivariate Kolmogorov Smirnov (MKS) test by explicitly reconstructing the architecture of GANs. Secondly, we conduct the normality test of transformed MST where the Gaussian GANs serves as the transformation function in the MKS test. In order to simplify the normality test, an efficient visualization is proposed using the chi square distribution. In the experiment, we use the UniMiB dataset and provide empirical evidence showing that the normality test using Gaussian GANs and chi sqaure visualization is effective and credible.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2207.14000</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#19978;&#30340;&#22810;&#27493;&#28436;&#32462;&#25512;&#29702;&#65306;&#22522;&#20110;&#36229;&#39046;&#22495;&#27867;&#21270;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.14000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;IMA-GloVe-GA&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#65292;&#22312;&#36229;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#31526;&#21495;&#36923;&#36753;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#24182;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21463;DeepLogic&#21551;&#21457;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#29992;&#20110;&#25191;&#34892;&#36923;&#36753;&#31243;&#24207;&#25512;&#29702;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IMA-GloVe-GA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#22810;&#27493;&#25512;&#29702;&#30340;&#36845;&#20195;&#31070;&#32463;&#25512;&#29702;&#32593;&#32476;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25512;&#29702;&#26159;&#20351;&#29992;&#22522;&#20110;RNN&#30340;&#36845;&#20195;&#20869;&#23384;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#38376;&#20851;&#27880;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;PARARULES&#12289;CONCEPTRULES V1&#21644;CONCEPTRULES V2&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;IMA-GloVe-GA&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#38376;&#20851;&#27880;&#26426;&#21046;&#30340;DeepLogic&#27604;DeepLogic&#21644;&#20854;&#20182;RNN&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35268;&#21017;&#34987;&#25171;&#20081;&#26102;&#27604;RoBERTa-Large&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36229;&#39046;&#22495;&#27867;&#21270;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#22810;&#27493;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#25512;&#29702;&#28145;&#24230;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.14000v2 Announce Type: replace-cross  Abstract: Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gate attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datase
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#27979;&#32858;&#31867;&#26641;&#30340;&#65288;&#20998;&#23618;&#65289;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2207.09237</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#39044;&#27979;&#32858;&#31867;&#26641;&#29992;&#20110;&#65288;&#20998;&#23618;&#65289;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Predictive Clustering Trees for (Hierarchical) Multi-label Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.09237
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#27979;&#32858;&#31867;&#26641;&#30340;&#65288;&#20998;&#23618;&#65289;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#20351;&#29992;&#26631;&#35760;&#30340;&#31034;&#20363;&#65292;&#36824;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#31034;&#20363;&#12290;&#23613;&#31649;SSL&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#30740;&#31350;&#30028;&#30340;&#20851;&#27880;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#32467;&#26500;&#30456;&#20851;&#21464;&#37327;&#30340;&#22797;&#26434;&#39044;&#27979;&#20219;&#21153;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35843;&#26597;&#12290;&#36825;&#31181;&#24773;&#20917;&#20986;&#29616;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#21644;&#20998;&#23618;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#26469;&#33258;&#25551;&#36848;&#31354;&#38388;&#20013;&#30001;&#26410;&#26631;&#35760;&#31034;&#20363;&#25552;&#20379;&#30340;&#22522;&#30784;&#20998;&#24067;&#65292;&#20197;&#26356;&#22909;&#22320;&#38754;&#23545;&#25361;&#25112;&#24615;&#30340;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#31867;&#26631;&#31614;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#19968;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#27979;&#32858;&#31867;&#26641;&#30340;&#65288;&#20998;&#23618;&#65289;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#38598;&#25104;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.09237v2 Announce Type: replace-cross  Abstract: Semi-supervised learning (SSL) is a common approach to learning predictive models using not only labeled examples, but also unlabeled examples. While SSL for the simple tasks of classification and regression has received a lot of attention from the research community, this is not properly investigated for complex prediction tasks with structurally dependent variables. This is the case of multi-label classification and hierarchical multi-label classification tasks, which may require additional information, possibly coming from the underlying distribution in the descriptive space provided by unlabeled examples, to better face the challenging task of predicting simultaneously multiple class labels.   In this paper, we investigate this aspect and propose a (hierarchical) multi-label classification method based on semi-supervised learning of predictive clustering trees. We also extend the method towards ensemble learning and propose
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Embed to Control&#65288;ETC&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#20004;&#20010;&#32423;&#21035;&#23398;&#20064;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#37096;&#20998;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#21033;&#29992;&#12290;</title><link>https://arxiv.org/abs/2205.13476</link><description>&lt;p&gt;
&#23884;&#20837;&#25511;&#21046;&#37096;&#20998;&#35266;&#23519;&#31995;&#32479;&#65306;&#20855;&#26377;&#21487;&#35777;&#26126;&#26679;&#26412;&#25928;&#29575;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Embed to Control Partially Observed Systems: Representation Learning with Provable Sample Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.13476
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Embed to Control&#65288;ETC&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#20004;&#20010;&#32423;&#21035;&#23398;&#20064;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#37096;&#20998;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#37096;&#20998;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#20013;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#19968;&#26159;&#36890;&#24120;&#38656;&#35201;&#20840;&#37096;&#21382;&#21490;&#35760;&#24405;&#26469;&#39044;&#27979;&#26410;&#26469;&#65292;&#36825;&#23548;&#33268;&#26679;&#26412;&#22797;&#26434;&#24230;&#38543;&#30528;&#26102;&#38388;&#36328;&#24230;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20108;&#26159;&#35266;&#27979;&#21644;&#29366;&#24577;&#31354;&#38388;&#36890;&#24120;&#26159;&#36830;&#32493;&#30340;&#65292;&#36825;&#23548;&#33268;&#26679;&#26412;&#22797;&#26434;&#24230;&#38543;&#22806;&#22312;&#32500;&#25968;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#36890;&#36807;&#21033;&#29992;POMDP&#30340;&#32467;&#26500;&#23398;&#20064;&#35266;&#27979;&#21644;&#29366;&#24577;&#21382;&#21490;&#30340;&#26368;&#23567;&#20294;&#36275;&#22815;&#30340;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Embed to Control (ETC)&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20248;&#21270;&#31574;&#30053;&#30340;&#21516;&#26102;&#23398;&#20064;&#20004;&#20010;&#32423;&#21035;&#30340;&#34920;&#31034;&#12290;(i)&#22312;&#27599;&#19968;&#27493;&#65292;ETC&#23398;&#20064;&#29992;&#20302;&#32500;&#29305;&#24449;&#34920;&#31034;&#29366;&#24577;&#65292;&#36825;&#23545;&#36716;&#31227;&#26680;&#36827;&#34892;&#22240;&#23376;&#20998;&#35299;&#12290;(ii)&#22312;&#22810;&#20010;&#27493;&#39588;&#20013;&#65292;ETC&#23398;&#20064;&#29992;&#20302;&#32500;&#34920;&#31034;&#23436;&#25972;&#21382;&#21490;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.13476v2 Announce Type: replace-cross  Abstract: Reinforcement learning in partially observed Markov decision processes (POMDPs) faces two challenges. (i) It often takes the full history to predict the future, which induces a sample complexity that scales exponentially with the horizon. (ii) The observation and state spaces are often continuous, which induces a sample complexity that scales exponentially with the extrinsic dimension. Addressing such challenges requires learning a minimal but sufficient representation of the observation and state histories by exploiting the structure of the POMDP.   To this end, we propose a reinforcement learning algorithm named Embed to Control (ETC), which learns the representation at two levels while optimizing the policy.~(i) For each step, ETC learns to represent the state with a low-dimensional feature, which factorizes the transition kernel. (ii) Across multiple steps, ETC learns to represent the full history with a low-dimensional emb
&lt;/p&gt;</description></item><item><title>ENS-t-SNE&#31639;&#27861;&#25512;&#24191;&#20102;t-SNE&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;3D&#23884;&#20837;&#20013;&#20351;&#29992;&#19981;&#21516;&#35270;&#35282;&#65292;&#21487;&#20197;&#21516;&#26102;&#21487;&#35270;&#21270;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#21516;&#31867;&#22411;&#32858;&#31867;&#12290;</title><link>https://arxiv.org/abs/2205.11720</link><description>&lt;p&gt;
ENS-t-SNE: &#21516;&#26102;&#23884;&#20837;&#37051;&#22495;&#30340;t-SNE
&lt;/p&gt;
&lt;p&gt;
ENS-t-SNE: Embedding Neighborhoods Simultaneously t-SNE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.11720
&lt;/p&gt;
&lt;p&gt;
ENS-t-SNE&#31639;&#27861;&#25512;&#24191;&#20102;t-SNE&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;3D&#23884;&#20837;&#20013;&#20351;&#29992;&#19981;&#21516;&#35270;&#35282;&#65292;&#21487;&#20197;&#21516;&#26102;&#21487;&#35270;&#21270;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#21516;&#31867;&#22411;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21487;&#35270;&#21270;&#39640;&#32500;&#25968;&#25454;&#38598;&#26102;&#65292;&#36890;&#24120;&#20250;&#37319;&#29992;&#38477;&#32500;&#25216;&#26415;&#65292;&#25552;&#20379;&#25968;&#25454;&#30340;&#21333;&#19968;&#20108;&#32500;&#35270;&#22270;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;ENS-t-SNE&#65306;&#19968;&#31181;&#29992;&#20110;&#21516;&#26102;&#23884;&#20837;&#37051;&#22495;&#30340;&#31639;&#27861;&#65292;&#23427;&#25512;&#24191;&#20102;t-&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;ENS-t-SNE&#30340;3D&#23884;&#20837;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#21487;&#20197;&#22312;&#30456;&#21516;&#30340;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#21487;&#35270;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#32858;&#31867;&#12290;&#36825;&#20351;&#24471;&#35266;&#23519;&#32773;&#33021;&#22815;&#30475;&#21040;&#24182;&#36319;&#36394;&#19981;&#21516;&#31867;&#22411;&#30340;&#32858;&#31867;&#65292;&#32780;&#24403;&#25552;&#20379;&#22810;&#20010;2D&#23884;&#20837;&#26102;&#65292;&#38590;&#20197;&#35782;&#21035;&#23545;&#24212;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#30495;&#23454;&#24212;&#29992;&#31243;&#24207;&#23637;&#31034;&#20102;ENS-t-SNE&#30340;&#25928;&#29992;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#21644;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.11720v3 Announce Type: replace  Abstract: When visualizing a high-dimensional dataset, dimension reduction techniques are commonly employed which provide a single 2-dimensional view of the data. We describe ENS-t-SNE: an algorithm for Embedding Neighborhoods Simultaneously that generalizes the t-Stochastic Neighborhood Embedding approach. By using different viewpoints in ENS-t-SNE's 3D embedding, one can visualize different types of clusters within the same high-dimensional dataset. This enables the viewer to see and keep track of the different types of clusters, which is harder to do when providing multiple 2D embeddings, where corresponding points cannot be easily identified. We illustrate the utility of ENS-t-SNE with real-world applications and provide an extensive quantitative evaluation with datasets of different types and sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40657;&#30418;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;MEKD&#65292;&#36890;&#36807;&#23558;&#25945;&#24072;&#21644;&#23398;&#29983;&#27169;&#22411;&#30340;&#20302;&#32500;&#24230;&#23545;&#25968;&#23545;&#40784;&#65292;&#23454;&#29616;&#23558;&#19968;&#20010;&#32321;&#29712;&#27169;&#22411;&#21387;&#32553;&#25104;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2205.10490</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;&#30340;&#23545;&#25968;&#36827;&#34892;&#20934;&#21017;&#23545;&#40784;&#30340;&#40657;&#30418;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Aligning Logits Generatively for Principled Black-Box Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.10490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40657;&#30418;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;MEKD&#65292;&#36890;&#36807;&#23558;&#25945;&#24072;&#21644;&#23398;&#29983;&#27169;&#22411;&#30340;&#20302;&#32500;&#24230;&#23545;&#25968;&#23545;&#40784;&#65292;&#23454;&#29616;&#23558;&#19968;&#20010;&#32321;&#29712;&#27169;&#22411;&#21387;&#32553;&#25104;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#30693;&#35782;&#33976;&#39311;&#65288;B2KD&#65289;&#26159;&#19968;&#20010;&#22788;&#29702;&#20113;&#31471;&#21040;&#36793;&#32536;&#27169;&#22411;&#21387;&#32553;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#27169;&#22411;&#25176;&#31649;&#22312;&#26381;&#21153;&#22120;&#19978;&#19988;&#26080;&#27861;&#30475;&#35265;&#12290;B2KD&#38754;&#20020;&#30340;&#25361;&#25112;&#21253;&#25324;&#20114;&#32852;&#32593;&#20132;&#25442;&#21463;&#38480;&#21644;&#25968;&#25454;&#20998;&#24067;&#22312;&#36793;&#32536;&#21644;&#20113;&#31471;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#21435;&#38544;&#21435;&#21644;&#33976;&#39311;&#20004;&#27493;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#23545;&#25968;&#21040;&#21333;&#20803;&#36793;&#30028;&#30340;&#26032;&#20248;&#21270;&#26041;&#21521;&#65292;&#19981;&#21516;&#20110;&#30452;&#25509;&#23545;&#25968;&#23545;&#40784;&#12290;&#22312;&#20854;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Mapping-Emulation KD&#65288;MEKD&#65289;&#65292;&#23558;&#19968;&#20010;&#40657;&#30418;&#32321;&#29712;&#27169;&#22411;&#33976;&#39311;&#25104;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21306;&#20998;&#36719;&#25110;&#30828;&#21709;&#24212;&#22788;&#29702;&#65292;&#24182;&#21253;&#25324;&#65306;1&#65289;&#21435;&#38544;&#21435;&#65306;&#36890;&#36807;&#29983;&#25104;&#22120;&#27169;&#25311;&#25945;&#24072;&#20989;&#25968;&#30340;&#36870;&#26144;&#23556;&#65292;&#21644;2&#65289;&#33976;&#39311;&#65306;&#36890;&#36807;&#20943;&#23567;&#39640;&#32500;&#22270;&#20687;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#23545;&#40784;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#30340;&#20302;&#32500;&#24230;&#23545;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.10490v2 Announce Type: replace-cross  Abstract: Black-Box Knowledge Distillation (B2KD) is a formulated problem for cloud-to-edge model compression with invisible data and models hosted on the server. B2KD faces challenges such as limited Internet exchange and edge-cloud disparity of data distributions. In this paper, we formalize a two-step workflow consisting of deprivatization and distillation, and theoretically provide a new optimization direction from logits to cell boundary different from direct logits alignment. With its guidance, we propose a new method Mapping-Emulation KD (MEKD) that distills a black-box cumbersome model into a lightweight one. Our method does not differentiate between treating soft or hard responses, and consists of: 1) deprivatization: emulating the inverse mapping of the teacher function with a generator, and 2) distillation: aligning low-dimensional logits of the teacher and student models by reducing the distance of high-dimensional image poin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#37096;&#20998;&#35266;&#27979;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;OP-TENET&#65289;&#65292;&#22312;&#26377;&#38480;&#30340;&#24773;&#33410;&#25968;&#20869;&#23454;&#29616;&#20102;$\epsilon$-&#26368;&#20248;&#31574;&#30053;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#32447;&#24615;&#32467;&#26500;&#30340;&#26412;&#24449;&#32500;&#24230;&#22810;&#39033;&#24335;&#32553;&#25918;&#65292;&#19982;&#35266;&#27979;&#21644;&#29366;&#24577;&#31354;&#38388;&#30340;&#22823;&#23567;&#26080;&#20851;&#12290;</title><link>https://arxiv.org/abs/2204.09787</link><description>&lt;p&gt;
&#22522;&#20110;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#37096;&#20998;&#35266;&#27979;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#21487;&#35777;&#26126;&#30340;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Partial Observation: Linear Function Approximation with Provable Sample Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.09787
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#37096;&#20998;&#35266;&#27979;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;OP-TENET&#65289;&#65292;&#22312;&#26377;&#38480;&#30340;&#24773;&#33410;&#25968;&#20869;&#23454;&#29616;&#20102;$\epsilon$-&#26368;&#20248;&#31574;&#30053;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#32447;&#24615;&#32467;&#26500;&#30340;&#26412;&#24449;&#32500;&#24230;&#22810;&#39033;&#24335;&#32553;&#25918;&#65292;&#19982;&#35266;&#27979;&#21644;&#29366;&#24577;&#31354;&#38388;&#30340;&#22823;&#23567;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20855;&#26377;&#26080;&#38480;&#35266;&#27979;&#21644;&#29366;&#24577;&#31354;&#38388;&#30340;&#37096;&#20998;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#29702;&#35770;&#19978;&#20173;&#28982;&#21463;&#21040;&#36739;&#23569;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#23558;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#19982;&#20855;&#26377;&#32447;&#24615;&#32467;&#26500;&#30340;&#19968;&#31867;POMDP&#30340;&#20989;&#25968;&#36924;&#36817;&#32852;&#31995;&#36215;&#26469;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;&#20048;&#35266;&#25506;&#32034;&#36890;&#36807;&#23545;&#25239;&#31215;&#20998;&#26041;&#31243;&#25110;OP-TENET&#65289;&#65292;&#22312;$ O&#65288;1 / \ epsilon ^ 2&#65289;$&#20010;&#24773;&#33410;&#20869;&#23454;&#29616;&#20102;$\ epsilon $-&#26368;&#20248;&#31574;&#30053;&#12290;&#29305;&#21035;&#22320;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#32447;&#24615;&#32467;&#26500;&#30340;&#26412;&#24449;&#32500;&#24230;&#22810;&#39033;&#24335;&#22320;&#32553;&#25918;&#65292;&#24182;&#19988;&#19982;&#35266;&#27979;&#21644;&#29366;&#24577;&#31354;&#38388;&#30340;&#22823;&#23567;&#26080;&#20851;&#12290;OP-TENET&#30340;&#26679;&#26412;&#25928;&#29575;&#30001;&#19968;&#31995;&#21015;&#22240;&#32032;&#23454;&#29616;&#65306;&#65288;i&#65289;&#20855;&#26377;&#26377;&#38480;&#35760;&#24518;&#30340;Bellman&#31639;&#23376;&#65292;&#20197;&#36882;&#24402;&#26041;&#24335;&#34920;&#31034;&#20540;&#20989;&#25968;&#65292;&#65288;ii&#65289;&#35782;&#21035;&#21644;&#20272;&#35745;&#36825;&#26679;&#19968;&#20010;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.09787v3 Announce Type: replace  Abstract: We study reinforcement learning for partially observed Markov decision processes (POMDPs) with infinite observation and state spaces, which remains less investigated theoretically. To this end, we make the first attempt at bridging partial observability and function approximation for a class of POMDPs with a linear structure. In detail, we propose a reinforcement learning algorithm (Optimistic Exploration via Adversarial Integral Equation or OP-TENET) that attains an $\epsilon$-optimal policy within $O(1/\epsilon^2)$ episodes. In particular, the sample complexity scales polynomially in the intrinsic dimension of the linear structure and is independent of the size of the observation and state spaces.   The sample efficiency of OP-TENET is enabled by a sequence of ingredients: (i) a Bellman operator with finite memory, which represents the value function in a recursive manner, (ii) the identification and estimation of such an operator 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22270;&#32467;&#26500;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35745;&#31639;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;RL&#26041;&#27861;&#22312;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2202.13046</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#35825;&#23548;&#30340;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributed Multi-Agent Reinforcement Learning Based on Graph-Induced Local Value Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.13046
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22270;&#32467;&#26500;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35745;&#31639;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;RL&#26041;&#27861;&#22312;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#22823;&#35268;&#27169;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;(RL)&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#65306;(i)&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#30340;&#20449;&#24687;&#65307;(ii)&#30001;&#20110;&#32500;&#24230;&#35781;&#21650;&#65292;&#20250;&#20986;&#29616;&#25910;&#25947;&#25110;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#35813;&#38382;&#39064;&#20013;&#28041;&#21450;&#30340;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#25551;&#36848;MARL&#20013;&#19977;&#31181;&#31867;&#22411;&#26234;&#33021;&#20307;&#32806;&#21512;&#30340;&#19977;&#20010;&#32806;&#21512;&#22270;&#65292;&#20998;&#21035;&#26159;&#29366;&#24577;&#22270;&#12289;&#35266;&#27979;&#22270;&#21644;&#22870;&#21169;&#22270;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#32771;&#34385;&#36890;&#20449;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#32806;&#21512;&#22270;&#20013;&#27966;&#29983;&#30340;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;RL&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#22312;&#21069;&#36848;&#22235;&#20010;&#22270;&#19978;&#30340;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#31532;&#20108;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.13046v4 Announce Type: replace-cross  Abstract: Achieving distributed reinforcement learning (RL) for large-scale cooperative multi-agent systems (MASs) is challenging because: (i) each agent has access to only limited information; (ii) issues on convergence or computational complexity emerge due to the curse of dimensionality. In this paper, we propose a general computationally efficient distributed framework for cooperative multi-agent reinforcement learning (MARL) by utilizing the structures of graphs involved in this problem. We introduce three coupling graphs describing three types of inter-agent couplings in MARL, namely, the state graph, the observation graph and the reward graph. By further considering a communication graph, we propose two distributed RL approaches based on local value-functions derived from the coupling graphs. The first approach is able to reduce sample complexity significantly under specific conditions on the aforementioned four graphs. The second
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#21069;&#30651;&#26799;&#24230;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19982;&#36229;&#35843;&#20840;&#23616;&#27169;&#22411;&#23545;&#40784;&#26469;&#35268;&#33539;&#26412;&#22320;&#26356;&#26032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25913;&#21892;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20419;&#36827;&#26381;&#21153;&#22120;&#27169;&#22411;&#30340;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2201.03172</link><description>&lt;p&gt;
&#20855;&#26377;&#21152;&#36895;&#23458;&#25143;&#26799;&#24230;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Learning with Accelerated Client Gradient
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.03172
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#21069;&#30651;&#26799;&#24230;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19982;&#36229;&#35843;&#20840;&#23616;&#27169;&#22411;&#23545;&#40784;&#26469;&#35268;&#33539;&#26412;&#22320;&#26356;&#26032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25913;&#21892;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20419;&#36827;&#26381;&#21153;&#22120;&#27169;&#22411;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24120;&#24120;&#22240;&#21442;&#19982;&#23458;&#25143;&#25968;&#25454;&#38598;&#30340;&#24322;&#36136;&#24615;&#29305;&#24449;&#32780;&#23548;&#33268;&#25910;&#25947;&#32531;&#24930;&#19981;&#31283;&#23450;&#12290;&#24403;&#23458;&#25143;&#21442;&#19982;&#29575;&#36739;&#20302;&#26102;&#65292;&#27492;&#36235;&#21183;&#20250;&#21152;&#21095;&#65292;&#22240;&#20026;&#20174;&#23458;&#25143;&#25910;&#38598;&#30340;&#20449;&#24687;&#20855;&#26377;&#36739;&#22823;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#25913;&#21892;&#20102;&#23458;&#25143;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20419;&#36827;&#20102;&#26381;&#21153;&#22120;&#27169;&#22411;&#30340;&#25910;&#25947;&#12290;&#36890;&#36807;&#20351;&#26381;&#21153;&#22120;&#24191;&#25773;&#20855;&#26377;&#21069;&#30651;&#26799;&#24230;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35813;&#31574;&#30053;&#65292;&#20351;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#21521;&#21442;&#19982;&#32773;&#20256;&#36798;&#25237;&#24433;&#30340;&#20840;&#23616;&#26356;&#26032;&#20449;&#24687;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#23458;&#25143;&#20869;&#23384;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#19982;&#36229;&#35843;&#20840;&#23616;&#27169;&#22411;&#23545;&#40784;&#26469;&#35268;&#33539;&#26412;&#22320;&#26356;&#26032;&#65292;&#20197;&#20943;&#23569;&#20559;&#24046;&#24182;&#25913;&#21892;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.03172v2 Announce Type: replace-cross  Abstract: Federated learning often suffers from slow and unstable convergence due to the heterogeneous characteristics of participating client datasets. Such a tendency is aggravated when the client participation ratio is low since the information collected from the clients has large variations. To address this challenge, we propose a simple but effective federated learning framework, which improves the consistency across clients and facilitates the convergence of the server model. This is achieved by making the server broadcast a global model with a lookahead gradient. This strategy enables the proposed approach to convey the projected global update information to participants effectively without additional client memory and extra communication costs. We also regularize local updates by aligning each client with the overshot global model to reduce bias and improve the stability of our algorithm. We provide the theoretical convergence ra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22343;&#22330;&#20998;&#26512;&#30740;&#31350;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#31070;&#32463;AC&#30340;&#28436;&#21270;&#21644;&#25910;&#25947;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#20004;&#20010;&#23398;&#20064;&#29575;&#26356;&#26032;&#30340;AC&#29256;&#26412;&#65292;&#20854;&#20013;&#35780;&#35770;&#23478;&#36890;&#36807;&#22823;&#27493;&#38271;&#36827;&#34892;TD&#23398;&#20064;&#26356;&#26032;&#65292;&#28436;&#21592;&#36890;&#36807;&#23567;&#27493;&#38271;&#36827;&#34892;PPO&#26356;&#26032;&#12290;</title><link>https://arxiv.org/abs/2112.13530</link><description>&lt;p&gt;
Wasserstein Flow&#36935;&#35265;&#22797;&#21046;&#21160;&#21147;&#23398;&#65306;Actor-Critic&#20013;&#20195;&#34920;&#23398;&#20064;&#30340;&#22343;&#22330;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Flow Meets Replicator Dynamics: A Mean-Field Analysis of Representation Learning in Actor-Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.13530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22343;&#22330;&#20998;&#26512;&#30740;&#31350;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#31070;&#32463;AC&#30340;&#28436;&#21270;&#21644;&#25910;&#25947;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#20004;&#20010;&#23398;&#20064;&#29575;&#26356;&#26032;&#30340;AC&#29256;&#26412;&#65292;&#20854;&#20013;&#35780;&#35770;&#23478;&#36890;&#36807;&#22823;&#27493;&#38271;&#36827;&#34892;TD&#23398;&#20064;&#26356;&#26032;&#65292;&#28436;&#21592;&#36890;&#36807;&#23567;&#27493;&#38271;&#36827;&#34892;PPO&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Actor-critic (AC)&#31639;&#27861;&#20511;&#21161;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32463;&#39564;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#37096;&#20998;&#20851;&#20110;AC&#31639;&#27861;&#30340;&#29702;&#35770;&#25903;&#25345;&#38598;&#20013;&#22312;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#25110;&#32447;&#24615;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#20013;&#29305;&#24449;&#34920;&#31034;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#31181;&#38480;&#21046;&#26410;&#33021;&#25429;&#25417;&#31070;&#32463;AC&#20013;&#20195;&#34920;&#23398;&#20064;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20174;&#22343;&#22330;&#30340;&#35282;&#24230;&#23545;&#22522;&#20110;&#29305;&#24449;&#30340;&#31070;&#32463;AC&#30340;&#28436;&#21270;&#21644;&#25910;&#25947;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;AC&#30340;&#29256;&#26412;&#65292;&#20854;&#20013;&#28436;&#21592;&#21644;&#35780;&#35770;&#23478;&#30001;&#36229;&#21442;&#25968;&#21270;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#23398;&#20064;&#29575;&#36827;&#34892;&#26356;&#26032;&#12290;&#35780;&#35770;&#23478;&#36890;&#36807;&#36739;&#22823;&#30340;&#27493;&#38271;&#36827;&#34892;&#26102;&#24046;&#65288;TD&#65289;&#23398;&#20064;&#26356;&#26032;&#65292;&#32780;&#28436;&#21592;&#36890;&#36807;&#36739;&#23567;&#27493;&#38271;&#36827;&#34892;&#37051;&#22495;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2112.13530v2 Announce Type: replace  Abstract: Actor-critic (AC) algorithms, empowered by neural networks, have had significant empirical success in recent years. However, most of the existing theoretical support for AC algorithms focuses on the case of linear function approximations, or linearized neural networks, where the feature representation is fixed throughout training. Such a limitation fails to capture the key aspect of representation learning in neural AC, which is pivotal in practical problems. In this work, we take a mean-field perspective on the evolution and convergence of feature-based neural AC. Specifically, we consider a version of AC where the actor and critic are represented by overparameterized two-layer neural networks and are updated with two-timescale learning rates. The critic is updated by temporal-difference (TD) learning with a larger stepsize while the actor is updated via proximal policy optimization (PPO) with a smaller stepsize. In the continuous-t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Collaborative Graph Contrastive Learning&#26694;&#26550;&#65288;CGCL&#65289;&#65292;&#21033;&#29992;&#22810;&#20010;&#22270;&#32534;&#30721;&#22120;&#35266;&#23519;&#22270;&#24418;&#65292;&#24182;&#36991;&#20813;&#24341;&#20837;&#19981;&#31283;&#23450;&#25200;&#21160;&#65292;&#20445;&#35777;&#22270;&#30340;&#19981;&#21464;&#24615;&#12290;</title><link>https://arxiv.org/abs/2111.03262</link><description>&lt;p&gt;
CGCL&#65306;&#26080;&#38656;&#25163;&#24037;&#22270;&#25968;&#25454;&#22686;&#24378;&#30340;&#21327;&#20316;&#24335;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CGCL: Collaborative Graph Contrastive Learning without Handcrafted Graph Data Augmentations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.03262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Collaborative Graph Contrastive Learning&#26694;&#26550;&#65288;CGCL&#65289;&#65292;&#21033;&#29992;&#22810;&#20010;&#22270;&#32534;&#30721;&#22120;&#35266;&#23519;&#22270;&#24418;&#65292;&#24182;&#36991;&#20813;&#24341;&#20837;&#19981;&#31283;&#23450;&#25200;&#21160;&#65292;&#20445;&#35777;&#22270;&#30340;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#30417;&#30563;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#19968;&#20010;&#38750;&#24120;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20013;&#23545;&#27604;&#26041;&#27861;&#30340;&#25104;&#21151;&#21551;&#21457;&#20102;&#23545;&#22270;&#31867;&#20284;&#23581;&#35797;&#12290;&#29616;&#26377;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#26088;&#22312;&#23398;&#20064;&#36328;&#22810;&#20010;&#22686;&#24378;&#35270;&#22270;&#30340;&#19981;&#21464;&#24615;&#65292;&#36825;&#20351;&#20854;&#20005;&#37325;&#20381;&#36182;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#22270;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#19981;&#24403;&#30340;&#22270;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#21361;&#23475;&#36825;&#31181;&#19981;&#21464;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#24403;&#22686;&#24378;&#30340;&#28508;&#22312;&#21361;&#38505;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#24335;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;CGCL&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#20010;&#22270;&#32534;&#30721;&#22120;&#35266;&#23519;&#22270;&#24418;&#12290;&#26469;&#33258;&#19981;&#21516;&#32534;&#30721;&#22120;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#20316;&#20026;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#35270;&#22270;&#65292;&#36991;&#20813;&#35825;&#21457;&#19981;&#31283;&#23450;&#30340;&#25200;&#21160;&#24182;&#20445;&#35777;&#19981;&#21464;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#19981;&#21516;&#22270;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.03262v2 Announce Type: replace-cross  Abstract: Unsupervised graph representation learning is a non-trivial topic. The success of contrastive methods in the unsupervised representation learning on structured data inspires similar attempts on the graph. Existing graph contrastive learning (GCL) aims to learn the invariance across multiple augmentation views, which renders it heavily reliant on the handcrafted graph augmentations. However, inappropriate graph data augmentations can potentially jeopardize such invariance. In this paper, we show the potential hazards of inappropriate augmentations and then propose a novel Collaborative Graph Contrastive Learning framework (CGCL). This framework harnesses multiple graph encoders to observe the graph. Features observed from different encoders serve as the contrastive views in contrastive learning, which avoids inducing unstable perturbation and guarantees the invariance. To ensure the collaboration among diverse graph encoders, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#22238;&#24402;&#38382;&#39064;&#20013;&#22235;&#31867;&#39044;&#27979;&#21306;&#38388;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#27874;&#21160;&#22823;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2107.00363</link><description>&lt;p&gt;
&#22238;&#24402;&#38382;&#39064;&#30340;&#26377;&#25928;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Valid prediction intervals for regression problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2107.00363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#22238;&#24402;&#38382;&#39064;&#20013;&#22235;&#31867;&#39044;&#27979;&#21306;&#38388;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#27874;&#21160;&#22823;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#38024;&#23545;&#22238;&#24402;&#35774;&#32622;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#39044;&#27979;&#21306;&#38388;&#65292;&#21253;&#25324;&#36125;&#21494;&#26031;&#26041;&#27861;&#12289;&#38598;&#25104;&#26041;&#27861;&#12289;&#30452;&#25509;&#21306;&#38388;&#20272;&#35745;&#26041;&#27861;&#21644;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#12290;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#36825;&#20123;&#26041;&#27861;&#30340;&#26657;&#20934;&#65306;&#29983;&#25104;&#30340;&#39044;&#27979;&#21306;&#38388;&#24212;&#35813;&#20855;&#26377;&#39044;&#23450;&#20041;&#30340;&#35206;&#30422;&#27700;&#24179;&#65292;&#32780;&#19981;&#24212;&#35813;&#36807;&#20110;&#20445;&#23432;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#27010;&#24565;&#21644;&#23454;&#39564;&#35282;&#24230;&#22238;&#39038;&#20102;&#19978;&#36848;&#22235;&#31867;&#26041;&#27861;&#12290;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#31361;&#26174;&#20986;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#24615;&#33021;&#26377;&#24456;&#22823;&#27874;&#21160;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#21487;&#24402;&#22240;&#20110;&#26576;&#20123;&#31867;&#26041;&#27861;&#22266;&#26377;&#20551;&#35774;&#30340;&#36829;&#32972;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#22914;&#20309;&#23558;&#31526;&#21512;&#39044;&#27979;&#29992;&#20316;&#27809;&#26377;&#26657;&#20934;&#27493;&#39588;&#20250;&#20135;&#29983;&#24046;&#32467;&#26524;&#30340;&#26041;&#27861;&#30340;&#36890;&#29992;&#26657;&#20934;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2107.00363v4 Announce Type: replace-cross  Abstract: Over the last few decades, various methods have been proposed for estimating prediction intervals in regression settings, including Bayesian methods, ensemble methods, direct interval estimation methods and conformal prediction methods. An important issue is the calibration of these methods: the generated prediction intervals should have a predefined coverage level, without being overly conservative. In this work, we review the above four classes of methods from a conceptual and experimental point of view. Results on benchmark data sets from various domains highlight large fluctuations in performance from one data set to another. These observations can be attributed to the violation of certain assumptions that are inherent to some classes of methods. We illustrate how conformal prediction can be used as a general calibration procedure for methods that deliver poor results without a calibration step.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphFM&#30340;&#22270;&#22240;&#23376;&#20998;&#35299;&#26426;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#33258;&#28982;&#34920;&#31034;&#29305;&#24449;&#65292;&#24182;&#23558;FM&#30340;&#20132;&#20114;&#21151;&#33021;&#38598;&#25104;&#21040;GNN&#30340;&#29305;&#24449;&#32858;&#21512;&#31574;&#30053;&#20013;&#65292;&#33021;&#22815;&#27169;&#25311;&#20219;&#24847;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2105.11866</link><description>&lt;p&gt;
GraphFM&#65306;&#22270;&#22240;&#23376;&#20998;&#35299;&#26426;&#29992;&#20110;&#29305;&#24449;&#20132;&#20114;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GraphFM: Graph Factorization Machines for Feature Interaction Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2105.11866
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphFM&#30340;&#22270;&#22240;&#23376;&#20998;&#35299;&#26426;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#33258;&#28982;&#34920;&#31034;&#29305;&#24449;&#65292;&#24182;&#23558;FM&#30340;&#20132;&#20114;&#21151;&#33021;&#38598;&#25104;&#21040;GNN&#30340;&#29305;&#24449;&#32858;&#21512;&#31574;&#30053;&#20013;&#65292;&#33021;&#22815;&#27169;&#25311;&#20219;&#24847;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#23376;&#20998;&#35299;&#26426;&#65288;FM&#65289;&#26159;&#22788;&#29702;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#26102;&#24314;&#27169;&#25104;&#23545;&#65288;&#20108;&#38454;&#65289;&#29305;&#24449;&#20132;&#20114;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19968;&#26041;&#38754;&#65292;FM&#26410;&#33021;&#25429;&#25417;&#21040;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#65292;&#21463;&#21040;&#32452;&#21512;&#25193;&#23637;&#30340;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32771;&#34385;&#27599;&#23545;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#24182;&#38477;&#20302;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;Graph Factorization Machine&#65288;GraphFM&#65289;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#33258;&#28982;&#34920;&#31034;&#25104;&#22270;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26426;&#21046;&#26469;&#36873;&#25321;&#26377;&#30410;&#30340;&#29305;&#24449;&#20132;&#20114;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#29305;&#24449;&#20043;&#38388;&#30340;&#36793;&#12290;&#28982;&#21518;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#23558;FM&#30340;&#20132;&#20114;&#21151;&#33021;&#25972;&#21512;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#29305;&#24449;&#32858;&#21512;&#31574;&#30053;&#20013;&#65292;&#36890;&#36807;&#22534;&#21472;&#23618;&#26469;&#27169;&#25311;&#22270;&#32467;&#26500;&#29305;&#24449;&#19978;&#30340;&#20219;&#24847;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2105.11866v4 Announce Type: replace-cross  Abstract: Factorization machine (FM) is a prevalent approach to modeling pairwise (second-order) feature interactions when dealing with high-dimensional sparse data. However, on the one hand, FM fails to capture higher-order feature interactions suffering from combinatorial expansion. On the other hand, taking into account interactions between every pair of features may introduce noise and degrade prediction accuracy. To solve the problems, we propose a novel approach, Graph Factorization Machine (GraphFM), by naturally representing features in the graph structure. In particular, we design a mechanism to select the beneficial feature interactions and formulate them as edges between features. Then the proposed model, which integrates the interaction function of FM into the feature aggregation strategy of Graph Neural Network (GNN), can model arbitrary-order feature interactions on the graph-structured features by stacking layers. Experime
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MetaVIM&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#25955;&#24335;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#32771;&#34385;&#37051;&#23621;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2101.00746</link><description>&lt;p&gt;
MetaVIM&#65306;&#20803;&#21464;&#20998;&#20869;&#22312;&#28608;&#21169;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20998;&#25955;&#24335;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
MetaVIM: Meta Variationally Intrinsic Motivated Reinforcement Learning for Decentralized Traffic Signal Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2101.00746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MetaVIM&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#25955;&#24335;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#32771;&#34385;&#37051;&#23621;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26088;&#22312;&#21327;&#35843;&#20132;&#21449;&#21475;&#30340;&#20132;&#36890;&#20449;&#21495;&#65292;&#20197;&#25913;&#21892;&#21306;&#22495;&#25110;&#22478;&#24066;&#30340;&#20132;&#36890;&#25928;&#29575;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#34987;&#24212;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65292;&#24182;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#27599;&#20010;&#20132;&#36890;&#20449;&#21495;&#34987;&#35270;&#20026;&#19968;&#20010;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#20173;&#23384;&#22312;&#19968;&#20123;&#21487;&#33021;&#38480;&#21046;&#20854;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;&#20174;&#35757;&#32451;&#22330;&#26223;&#20013;&#23398;&#21040;&#30340;&#31574;&#30053;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#30340;&#26410;&#35265;&#22330;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Meta Variationally Intrinsic Motivated&#65288;MetaVIM&#65289;RL&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#32771;&#34385;&#37051;&#23621;&#20449;&#24687;&#30340;&#27599;&#20010;&#20132;&#21449;&#21475;&#30340;&#20998;&#25955;&#24335;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#31574;&#30053;&#23398;&#20064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20851;&#20110;&#19968;&#32452;&#30456;&#20851;&#20219;&#21153;&#30340;&#20803;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#23545;&#24212;&#20110;&#19968;&#20010;&#20132;&#21449;&#21475;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65292;&#20854;&#37051;&#23621;&#34987;&#35270;&#20026;&#29366;&#24577;&#30340;&#26410;&#35266;&#23519;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2101.00746v5 Announce Type: replace-cross  Abstract: Traffic signal control aims to coordinate traffic signals across intersections to improve the traffic efficiency of a district or a city. Deep reinforcement learning (RL) has been applied to traffic signal control recently and demonstrated promising performance where each traffic signal is regarded as an agent. However, there are still several challenges that may limit its large-scale application in the real world. To make the policy learned from a training scenario generalizable to new unseen scenarios, a novel Meta Variationally Intrinsic Motivated (MetaVIM) RL method is proposed to learn the decentralized policy for each intersection that considers neighbor information in a latent way. Specifically, we formulate the policy learning as a meta-learning problem over a set of related tasks, where each task corresponds to traffic signal control at an intersection whose neighbors are regarded as the unobserved part of the state. T
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#21464;&#20998;&#20256;&#36755;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25512;&#21160;&#19968;&#32452;&#31890;&#23376;&#65292;&#22312;&#27010;&#29575;&#20998;&#24067;&#27969;&#24418;&#19978;&#36817;&#20284;&#25191;&#34892;&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2012.11554</link><description>&lt;p&gt;
&#21464;&#20998;&#20256;&#36755;&#65306;&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#20248;&#21270;&#30340;&#25910;&#25947;&#31890;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variational Transport: A Convergent Particle-BasedAlgorithm for Distributional Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2012.11554
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#21464;&#20998;&#20256;&#36755;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25512;&#21160;&#19968;&#32452;&#31890;&#23376;&#65292;&#22312;&#27010;&#29575;&#20998;&#24067;&#27969;&#24418;&#19978;&#36817;&#20284;&#25191;&#34892;&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#26368;&#23567;&#21270;&#19968;&#20010;&#22312;&#27010;&#29575;&#20998;&#24067;&#26063;&#19978;&#23450;&#20041;&#30340;&#20989;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#20551;&#23450;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;&#21464;&#20998;&#24418;&#24335;&#12290;&#36825;&#31181;&#20998;&#24067;&#20248;&#21270;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#33945;&#29305;&#21345;&#27931;&#25277;&#26679;&#12289;&#21464;&#20998;&#25512;&#26029;&#12289;&#31574;&#30053;&#20248;&#21270;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26159;&#20854;&#20013;&#30340;&#20363;&#23376;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#21464;&#20998;&#20256;&#36755;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25512;&#21160;&#19968;&#32452;&#31890;&#23376;&#65292;&#22312;&#27010;&#29575;&#20998;&#24067;&#27969;&#24418;&#19978;&#36817;&#20284;&#25191;&#34892;&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#19979;&#38477;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#27839;&#30528;&#20989;&#25968;&#26799;&#24230;&#30340;&#27979;&#22320;&#32447;&#26041;&#21521;&#31227;&#21160;&#65292;&#19982;&#23545;&#27010;&#29575;&#20998;&#24067;&#26045;&#21152;&#19968;&#20010;&#25512;&#21069;&#26144;&#23556;&#31561;&#20215;&#20110;&#36890;&#36807;&#25512;&#21160;&#19968;&#32452;&#31890;&#23376;&#26469;&#20934;&#30830;&#36817;&#20284;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2012.11554v2 Announce Type: replace  Abstract: We consider the optimization problem of minimizing a functional defined over a family of probability distributions, where the objective functional is assumed to possess a variational form. Such a distributional optimization problem arises widely in machine learning and statistics, with Monte-Carlo sampling, variational inference, policy optimization, and generative adversarial network as examples. For this problem, we propose a novel particle-based algorithm, dubbed as variational transport, which approximately performs Wasserstein gradient descent over the manifold of probability distributions via iteratively pushing a set of particles. Specifically, we prove that moving along the geodesic in the direction of functional gradient with respect to the second-order Wasserstein distance is equivalent to applying a pushforward mapping to a probability distribution, which can be approximated accurately by pushing a set of particles. Specif
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#31163;&#25955;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#21644;Q&#23398;&#20064;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#34920;&#31034;&#28436;&#21464;&#65292;&#35777;&#26126;&#21033;&#29992;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#36825;&#31181;&#28436;&#21464;&#65292;&#24182;&#20851;&#27880;&#29305;&#24449;&#34920;&#31034;&#23545;&#20110;&#31639;&#27861;&#25910;&#25947;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2006.04761</link><description>&lt;p&gt;
&#31163;&#25955;&#26102;&#38388;&#24046;&#20998;&#21644;Q&#23398;&#20064;&#33021;&#23398;&#24471;&#29305;&#24449;&#34920;&#31034;&#21527;&#65311;&#19968;&#31181;&#24179;&#22343;&#22330;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Can Temporal-Difference and Q-Learning Learn Representation? A Mean-Field Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2006.04761
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#31163;&#25955;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#21644;Q&#23398;&#20064;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#34920;&#31034;&#28436;&#21464;&#65292;&#35777;&#26126;&#21033;&#29992;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#36825;&#31181;&#28436;&#21464;&#65292;&#24182;&#20851;&#27880;&#29305;&#24449;&#34920;&#31034;&#23545;&#20110;&#31639;&#27861;&#25910;&#25947;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#26102;&#38388;&#24046;&#20998;&#21644;Q&#23398;&#20064;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#23427;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#31561;&#34920;&#36798;&#21147;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;&#23427;&#20204;&#30340;&#23454;&#35777;&#25104;&#21151;&#30340;&#26680;&#24515;&#26159;&#23398;&#24471;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#23558;&#20016;&#23500;&#30340;&#35266;&#27979;&#65292;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#65292;&#23884;&#20837;&#21040;&#32534;&#30721;&#35821;&#20041;&#32467;&#26500;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#21516;&#26102;&#65292;&#36825;&#31181;&#29305;&#24449;&#34920;&#31034;&#30340;&#28436;&#21464;&#23545;&#31163;&#25955;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#21644;Q&#23398;&#20064;&#30340;&#25910;&#25947;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#22320;&#65292;&#24403;&#20989;&#25968;&#36924;&#36817;&#22120;&#22312;&#29305;&#24449;&#34920;&#31034;&#20013;&#26159;&#32447;&#24615;&#30340;&#19988;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#26102;&#65292;&#31163;&#25955;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#20250;&#25910;&#25947;&#65292;&#21542;&#21017;&#21487;&#33021;&#21457;&#25955;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;&#24403;&#20989;&#25968;&#36924;&#36817;&#22120;&#26159;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#30456;&#20851;&#30340;&#29305;&#24449;&#34920;&#31034;&#22914;&#20309;&#28436;&#36827;&#65311;&#22914;&#26524;&#23427;&#25910;&#25947;&#65292;&#23427;&#26159;&#21542;&#25910;&#25947;&#33267;&#26368;&#20248;&#30340;&#29305;&#24449;&#34920;&#31034;&#65311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2006.04761v2 Announce Type: replace  Abstract: Temporal-difference and Q-learning play a key role in deep reinforcement learning, where they are empowered by expressive nonlinear function approximators such as neural networks. At the core of their empirical successes is the learned feature representation, which embeds rich observations, e.g., images and texts, into the latent space that encodes semantic structures. Meanwhile, the evolution of such a feature representation is crucial to the convergence of temporal-difference and Q-learning.   In particular, temporal-difference learning converges when the function approximator is linear in a feature representation, which is fixed throughout learning, and possibly diverges otherwise. We aim to answer the following questions: When the function approximator is a neural network, how does the associated feature representation evolve? If it converges, does it converge to the optimal one?   We prove that, utilizing an overparameterized tw
&lt;/p&gt;</description></item><item><title>OPPO&#26159;&#31532;&#19968;&#20010;&#22312;&#25506;&#32034;&#20013;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36817;&#20284;&#12289;&#26410;&#30693;&#36716;&#31227;&#21644;&#23545;&#25239;&#24615;&#22870;&#21169;&#30340;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102; $\tilde{O}(\sqrt{d^2 H^3 T} )$ &#30340;&#36951;&#25022;&#12290;</title><link>https://arxiv.org/abs/1912.05830</link><description>&lt;p&gt;
&#22312;&#31574;&#30053;&#20248;&#21270;&#20013;&#23454;&#29616;&#21487;&#35777;&#26126;&#39640;&#25928;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Exploration in Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1912.05830
&lt;/p&gt;
&lt;p&gt;
OPPO&#26159;&#31532;&#19968;&#20010;&#22312;&#25506;&#32034;&#20013;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36817;&#20284;&#12289;&#26410;&#30693;&#36716;&#31227;&#21644;&#23545;&#25239;&#24615;&#22870;&#21169;&#30340;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102; $\tilde{O}(\sqrt{d^2 H^3 T} )$ &#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#29702;&#35770;&#19978;&#21364;&#36828;&#19981;&#22914;&#22522;&#20110;&#20540;&#20989;&#25968;&#30340;RL&#34987;&#29702;&#35299;&#30340;&#20805;&#20998;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#22312;&#25506;&#32034;&#20013;&#32508;&#21512;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#20173;&#28982;&#26159;&#27169;&#31946;&#30340;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Proximal Policy Optimization&#31639;&#27861;&#30340;"&#20048;&#35266;&#21464;&#20307;"&#65288;OPPO&#65289;&#65292;&#20854;&#36981;&#24490;&#8220;&#31574;&#30053;&#26799;&#24230;&#26041;&#21521;&#8221;&#30340;&#8220;&#20048;&#35266;&#29256;&#26412;&#8221;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#23545;&#20110;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36817;&#20284;&#12289;&#26410;&#30693;&#36716;&#31227;&#21644;&#20855;&#26377;&#23436;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#23545;&#25239;&#24615;&#22870;&#21169;&#30340;&#22522;&#20110;&#24773;&#33410;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#65292;OPPO&#23454;&#29616;&#20102; $\tilde{O}(\sqrt{d^2 H^3 T} )$ &#30340;&#36951;&#25022;&#12290;&#20854;&#20013;&#65292;$d$ &#26159;&#29305;&#24449;&#32500;&#24230;&#65292;$H$ &#26159;&#24773;&#33410;&#38271;&#24230;&#65292;$T$ &#26159;&#24635;&#27493;&#25968;&#12290;&#23601;&#25105;&#20204;&#25152;&#30693;&#65292;OPPO&#26159;&#31532;&#19968;&#20010;&#21487;&#35777;&#26126;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1912.05830v4 Announce Type: replace  Abstract: While policy-based reinforcement learning (RL) achieves tremendous successes in practice, it is significantly less understood in theory, especially compared with value-based RL. In particular, it remains elusive how to design a provably efficient policy optimization algorithm that incorporates exploration. To bridge such a gap, this paper proposes an Optimistic variant of the Proximal Policy Optimization algorithm (OPPO), which follows an ``optimistic version'' of the policy gradient direction. This paper proves that, in the problem of episodic Markov decision process with linear function approximation, unknown transition, and adversarial reward with full-information feedback, OPPO achieves $\tilde{O}(\sqrt{d^2 H^3 T} )$ regret. Here $d$ is the feature dimension, $H$ is the episode horizon, and $T$ is the total number of steps. To the best of our knowledge, OPPO is the first provably efficient policy optimization algorithm that explo
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#33258;&#28982;&#21644;&#21307;&#23398;&#22270;&#20687;&#30340;&#28145;&#24230;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#26412;&#32508;&#36848;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#20026;&#20845;&#22823;&#31867;&#21035;&#65292;&#20840;&#38754;&#23457;&#26597;&#20102;&#21508;&#32452;&#26041;&#27861;&#30340;&#36129;&#29486;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/1910.07655</link><description>&lt;p&gt;
&#33258;&#28982;&#21644;&#21307;&#23398;&#22270;&#20687;&#30340;&#28145;&#24230;&#35821;&#20041;&#20998;&#21106;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Semantic Segmentation of Natural and Medical Images: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1910.07655
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33258;&#28982;&#21644;&#21307;&#23398;&#22270;&#20687;&#30340;&#28145;&#24230;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#26412;&#32508;&#36848;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#20026;&#20845;&#22823;&#31867;&#21035;&#65292;&#20840;&#38754;&#23457;&#26597;&#20102;&#21508;&#32452;&#26041;&#27861;&#30340;&#36129;&#29486;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#21253;&#25324;&#23558;&#22270;&#20687;&#30340;&#27599;&#20010;&#20687;&#32032;&#20998;&#31867;&#20026;&#19968;&#20010;&#23454;&#20363;&#65292;&#20854;&#20013;&#27599;&#20010;&#23454;&#20363;&#23545;&#24212;&#19968;&#20010;&#31867;&#21035;&#12290;&#36825;&#39033;&#20219;&#21153;&#26159;&#22330;&#26223;&#29702;&#35299;&#27010;&#24565;&#30340;&#19968;&#37096;&#20998;&#65292;&#25110;&#32773;&#26356;&#22909;&#22320;&#35299;&#37322;&#22270;&#20687;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#65292;&#22270;&#20687;&#20998;&#21106;&#21487;&#29992;&#20110;&#22270;&#20687;&#24341;&#23548;&#20171;&#20837;&#12289;&#25918;&#23556;&#27835;&#30103;&#25110;&#25913;&#21892;&#25918;&#23556;&#35786;&#26029;&#12290;&#26412;&#32508;&#36848;&#23558;&#39046;&#20808;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#21644;&#38750;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#35299;&#20915;&#26041;&#26696;&#20998;&#31867;&#20026;&#20845;&#22823;&#20027;&#35201;&#32452;&#65306;&#28145;&#24230;&#32467;&#26500;&#12289;&#25968;&#25454;&#21512;&#25104;&#12289;&#25439;&#22833;&#20989;&#25968;&#12289;&#24207;&#21015;&#27169;&#22411;&#12289;&#24369;&#30417;&#30563;&#21644;&#22810;&#20219;&#21153;&#26041;&#27861;&#65292;&#24182;&#20840;&#38754;&#23457;&#26597;&#20102;&#27599;&#20010;&#32452;&#20013;&#30340;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#27599;&#20010;&#32452;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27599;&#20010;&#21464;&#20307;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#30340;&#28508;&#22312;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1910.07655v4 Announce Type: replace-cross  Abstract: The semantic image segmentation task consists of classifying each pixel of an image into an instance, where each instance corresponds to a class. This task is a part of the concept of scene understanding or better explaining the global context of an image. In the medical image analysis domain, image segmentation can be used for image-guided interventions, radiotherapy, or improved radiological diagnostics. In this review, we categorize the leading deep learning-based medical and non-medical image segmentation solutions into six main groups of deep architectural, data synthesis-based, loss function-based, sequenced models, weakly supervised, and multi-task methods and provide a comprehensive review of the contributions in each of these groups. Further, for each group, we analyze each variant of these groups and discuss the limitations of the current approaches and present potential future research directions for semantic image s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15043</link><description>&lt;p&gt;
&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#65306;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#21644;&#22686;&#24378;&#23398;&#20064;&#30340;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20581;&#24247;&#25945;&#32946;&#26448;&#26009;&#30340;&#38405;&#35835;&#27700;&#24179;&#26174;&#33879;&#24433;&#21709;&#20449;&#24687;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#25509;&#35302;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#26063;&#35028;&#20154;&#32676;&#12290;&#35768;&#22810;&#24739;&#32773;&#25945;&#32946;&#36164;&#28304;&#36229;&#36807;&#20102;&#24191;&#27867;&#25509;&#21463;&#30340;&#26631;&#20934;&#30340;&#38405;&#35835;&#27700;&#24179;&#21644;&#22797;&#26434;&#24615;&#12290;&#22312;&#20581;&#24247;&#20449;&#24687;&#20013;&#65292;&#24613;&#38656;&#39640;&#24615;&#33021;&#30340;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#20197;&#22686;&#24378;&#20256;&#25773;&#21644;&#35782;&#23383;&#33021;&#21147;&#12290;&#36825;&#31181;&#38656;&#35201;&#22312;&#30284;&#30151;&#25945;&#32946;&#20013;&#23588;&#20026;&#36843;&#20999;&#65292;&#26377;&#25928;&#30340;&#39044;&#38450;&#21644;&#31579;&#26597;&#25945;&#32946;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24341;&#20837;&#20102;&#31616;&#21270;&#30340;&#28040;&#21270;&#30284;&#30151;&#65288;SimpleDC&#65289;&#24182;&#34892;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#12290;&#21033;&#29992;SimpleDC&#21644;&#29616;&#26377;&#30340;Med-EASi&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based app
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.14295</link><description>&lt;p&gt;
&#25512;&#29702;&#30340;&#25299;&#25169;&#23398;&#65306;&#25581;&#31192;&#24605;&#32500;&#38142;&#12289;&#26641;&#21644;&#22270;
&lt;/p&gt;
&lt;p&gt;
Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#26041;&#38754;&#12290;&#20854;&#20013;&#65292;&#19982;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#24037;&#31243;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#20854;&#35774;&#35745;&#22914;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#31561;&#65292;&#36890;&#36807;&#32467;&#26500;&#25351;&#23548;&#25972;&#20307;LLM&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#30340;&#35828;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#36923;&#36753;&#25110;&#25968;&#23398;&#25512;&#29702;&#12289;&#35268;&#21010;&#25110;&#21019;&#36896;&#24615;&#20889;&#20316;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#26041;&#20415;&#29702;&#35299;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#24182;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#26041;&#26696;&#30340;&#36890;&#29992;&#34013;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#25552;&#31034;&#25191;&#34892;&#27969;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#28548;&#28165;&#24182;&#26126;&#30830;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#31435;&#31532;&#19968;&#20010;&#20998;&#31867;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
&lt;/p&gt;</description></item><item><title>TelME&#26159;&#19968;&#31181;&#25945;&#24072;&#23548;&#21521;&#30340;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#23545;&#35805;&#20013;&#24773;&#32490;&#35782;&#21035;&#30340;&#20248;&#21270;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#35828;&#35805;&#20154;&#25968;&#25454;&#38598;MELD&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12987</link><description>&lt;p&gt;
TelME&#65306;&#25945;&#24072;&#23548;&#21521;&#30340;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation. (arXiv:2401.12987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12987
&lt;/p&gt;
&lt;p&gt;
TelME&#26159;&#19968;&#31181;&#25945;&#24072;&#23548;&#21521;&#30340;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#23545;&#35805;&#20013;&#24773;&#32490;&#35782;&#21035;&#30340;&#20248;&#21270;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#35828;&#35805;&#20154;&#25968;&#25454;&#38598;MELD&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#22312;&#20351;&#23545;&#35805;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22238;&#24212;&#29992;&#25143;&#35831;&#27714;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#21487;&#20197;&#36890;&#36807;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#31561;&#22810;&#31181;&#27169;&#24577;&#30340;&#34920;&#31034;&#36827;&#34892;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#35821;&#35328;&#27169;&#24577;&#23545;&#35782;&#21035;&#24773;&#32490;&#30340;&#36129;&#29486;&#36739;&#24369;&#65292;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#20013;&#24773;&#32490;&#35782;&#21035;&#30340;&#25945;&#24072;&#23548;&#21521;&#22810;&#27169;&#34701;&#21512;&#32593;&#32476;&#65288;TelME&#65289;&#12290;TelME&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#23558;&#20449;&#24687;&#20174;&#20316;&#20026;&#25945;&#24072;&#30340;&#35821;&#35328;&#27169;&#22411;&#20256;&#36882;&#32473;&#38750;&#35821;&#35328;&#30340;&#23398;&#29983;&#65292;&#20174;&#32780;&#20248;&#21270;&#20102;&#24369;&#27169;&#24577;&#30340;&#25928;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#31227;&#21160;&#34701;&#21512;&#26041;&#27861;&#23558;&#22810;&#27169;&#24577;&#29305;&#24449;&#32452;&#21512;&#36215;&#26469;&#65292;&#20854;&#20013;&#23398;&#29983;&#32593;&#32476;&#25903;&#25345;&#25945;&#24072;&#12290;TelME&#22312;MELD&#65288;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#30340;&#22810;&#35828;&#35805;&#20154;&#25968;&#25454;&#38598;&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#23454;&#39564;&#35770;&#35777;&#20102;&#25105;&#20204;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) plays a crucial role in enabling dialogue systems to effectively respond to user requests. The emotions in a conversation can be identified by the representations from various modalities, such as audio, visual, and text. However, due to the weak contribution of non-verbal modalities to recognize emotions, multimodal ERC has always been considered a challenging task. In this paper, we propose Teacher-leading Multimodal fusion network for ERC (TelME). TelME incorporates cross-modal knowledge distillation to transfer information from a language model acting as the teacher to the non-verbal students, thereby optimizing the efficacy of the weak modalities. We then combine multimodal features using a shifting fusion approach in which student networks support the teacher. TelME achieves state-of-the-art performance in MELD, a multi-speaker conversation dataset for ERC. Finally, we demonstrate the effectiveness of our components through additional expe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#21452;&#23792;&#29616;&#35937;&#65292;&#21457;&#29616;&#39640;&#24230;&#36807;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#21487;&#20197;&#36991;&#20813;&#36807;&#25311;&#21512;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#19982;&#20256;&#32479;&#30340;&#20559;&#24046;-&#26041;&#24046;&#25240;&#34935;&#27861;&#21017;&#19981;&#21516;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#24067;&#23572;&#22343;&#20540;&#32500;&#24230;&#65288;BMD&#65289;&#19982;&#32593;&#32476;&#22797;&#26434;&#24615;&#21644;&#25935;&#24863;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24471;&#21040;&#20102;&#22312;&#39640;&#32500;&#24230;&#33539;&#22260;&#20869;BMD&#30340;&#21487;&#35299;&#37322;&#34920;&#36798;&#24335;&#65292;&#21457;&#29616;BMD&#22312;&#32593;&#32476;&#36807;&#21442;&#25968;&#21270;&#31243;&#24230;&#22686;&#21152;&#26102;&#36798;&#21040;&#26497;&#20540;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.12610</link><description>&lt;p&gt;
&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#21452;&#23792;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
The twin peaks of learning neural networks. (arXiv:2401.12610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12610
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#21452;&#23792;&#29616;&#35937;&#65292;&#21457;&#29616;&#39640;&#24230;&#36807;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#21487;&#20197;&#36991;&#20813;&#36807;&#25311;&#21512;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#19982;&#20256;&#32479;&#30340;&#20559;&#24046;-&#26041;&#24046;&#25240;&#34935;&#27861;&#21017;&#19981;&#21516;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#24067;&#23572;&#22343;&#20540;&#32500;&#24230;&#65288;BMD&#65289;&#19982;&#32593;&#32476;&#22797;&#26434;&#24615;&#21644;&#25935;&#24863;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24471;&#21040;&#20102;&#22312;&#39640;&#32500;&#24230;&#33539;&#22260;&#20869;BMD&#30340;&#21487;&#35299;&#37322;&#34920;&#36798;&#24335;&#65292;&#21457;&#29616;BMD&#22312;&#32593;&#32476;&#36807;&#21442;&#25968;&#21270;&#31243;&#24230;&#22686;&#21152;&#26102;&#36798;&#21040;&#26497;&#20540;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#23384;&#22312;&#21452;&#23792;&#29616;&#35937;&#65292;&#21363;&#39640;&#24230;&#36807;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#21487;&#20197;&#36991;&#20813;&#36807;&#25311;&#21512;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#19982;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#25551;&#36848;&#30340;&#26631;&#20934;&#20559;&#24046;-&#26041;&#24046;&#25240;&#34935;&#27861;&#21017;&#19981;&#31526;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#19968;&#29616;&#35937;&#19982;&#31070;&#32463;&#32593;&#32476;&#25152;&#34920;&#31034;&#30340;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#21644;&#25935;&#24863;&#24615;&#22686;&#21152;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24067;&#23572;&#22343;&#20540;&#32500;&#24230;&#65288;BMD&#65289;&#65292;&#36825;&#26159;&#22312;&#24067;&#23572;&#20989;&#25968;&#20998;&#26512;&#32972;&#26223;&#19979;&#21457;&#23637;&#36215;&#26469;&#30340;&#19968;&#31181;&#24230;&#37327;&#12290;&#38024;&#23545;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#31616;&#21333;&#25945;&#24072;-&#23398;&#29983;&#35774;&#32622;&#65292;&#25105;&#20204;&#22522;&#20110;&#21103;&#26412;&#26041;&#27861;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;BMD&#34920;&#36798;&#24335;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#12289;&#29305;&#24449;&#30340;&#25968;&#37327;&#21644;&#36755;&#20837;&#22823;&#23567;&#22312;&#39640;&#32500;&#24230;&#33539;&#22260;&#20869;&#19981;&#26029;&#22686;&#38271;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#32593;&#32476;&#36807;&#21442;&#25968;&#21270;&#31243;&#24230;&#30340;&#22686;&#21152;&#65292;BMD&#36798;&#21040;&#19968;&#20010;&#26497;&#20540;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works demonstrated the existence of a double-descent phenomenon for the generalization error of neural networks, where highly overparameterized models escape overfitting and achieve good test performance, at odds with the standard bias-variance trade-off described by statistical learning theory. In the present work, we explore a link between this phenomenon and the increase of complexity and sensitivity of the function represented by neural networks. In particular, we study the Boolean mean dimension (BMD), a metric developed in the context of Boolean function analysis. Focusing on a simple teacher-student setting for the random feature model, we derive a theoretical analysis based on the replica method that yields an interpretable expression for the BMD, in the high dimensional regime where the number of data points, the number of features, and the input size grow to infinity. We find that, as the degree of overparameterization of the network is increased, the BMD reaches an ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26041;&#27861;GI-PIP&#65292;&#19981;&#38656;&#35201;&#20381;&#36182;&#19981;&#20999;&#23454;&#38469;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20174;&#36739;&#23569;&#30340;&#25968;&#25454;&#20013;&#25429;&#33719;&#24213;&#23618;&#20998;&#24067;&#65292;&#24182;&#33021;&#22312;&#22270;&#20687;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#20063;&#26356;&#24378;&#22823;&#12290;</title><link>http://arxiv.org/abs/2401.11748</link><description>&lt;p&gt;
GI-PIP&#65306;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26159;&#21542;&#38656;&#35201;&#19981;&#20999;&#23454;&#38469;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient Inversion Attacks?. (arXiv:2401.11748v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26041;&#27861;GI-PIP&#65292;&#19981;&#38656;&#35201;&#20381;&#36182;&#19981;&#20999;&#23454;&#38469;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20174;&#36739;&#23569;&#30340;&#25968;&#25454;&#20013;&#25429;&#33719;&#24213;&#23618;&#20998;&#24067;&#65292;&#24182;&#33021;&#22312;&#22270;&#20687;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#20063;&#26356;&#24378;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#36890;&#36807;&#20934;&#30830;&#22320;&#24674;&#22797;&#20849;&#20139;&#26799;&#24230;&#20013;&#30340;&#38544;&#31169;&#25968;&#25454;&#65292;&#23545;&#32852;&#37030;&#23398;&#20064;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#22312;&#35775;&#38382;&#36807;&#22810;&#30340;&#36741;&#21161;&#25968;&#25454;&#26041;&#38754;&#20381;&#36182;&#20110;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#36825;&#36829;&#21453;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#22522;&#26412;&#25968;&#25454;&#20998;&#21306;&#21407;&#21017;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#23454;&#29992;&#22270;&#20687;&#20808;&#39564;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#65288;GI-PIP&#65289;&#65292;&#22312;&#32463;&#36807;&#20462;&#35746;&#30340;&#23041;&#32961;&#27169;&#22411;&#19979;&#12290;GI-PIP&#21033;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#20174;&#26356;&#23569;&#30340;&#25968;&#25454;&#20013;&#25429;&#33719;&#24213;&#23618;&#20998;&#24067;&#65292;&#32780;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#38656;&#35201;&#28040;&#32791;&#26356;&#22810;&#30340;&#25968;&#25454;&#26469;&#21512;&#25104;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#25552;&#21462;&#20986;&#30340;&#20998;&#24067;&#26469;&#35843;&#33410;&#25915;&#20987;&#36807;&#31243;&#20316;&#20026;&#24322;&#24120;&#24471;&#20998;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GI-PIP&#21482;&#20351;&#29992;&#20102;ImageNet&#25968;&#25454;&#30340;3.8%&#21363;&#21487;&#23454;&#29616;16.12 dB&#30340;PSNR&#24674;&#22797;&#65292;&#32780;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#21017;&#38656;&#35201;&#36229;&#36807;70%&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;GI-PIP&#22312;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep gradient inversion attacks expose a serious threat to Federated Learning (FL) by accurately recovering private data from shared gradients. However, the state-of-the-art heavily relies on impractical assumptions to access excessive auxiliary data, which violates the basic data partitioning principle of FL. In this paper, a novel method, Gradient Inversion Attack using Practical Image Prior (GI-PIP), is proposed under a revised threat model. GI-PIP exploits anomaly detection models to capture the underlying distribution from fewer data, while GAN-based methods consume significant more data to synthesize images. The extracted distribution is then leveraged to regulate the attack process as Anomaly Score loss. Experimental results show that GI-PIP achieves a 16.12 dB PSNR recovery using only 3.8% data of ImageNet, while GAN-based methods necessitate over 70%. Moreover, GI-PIP exhibits superior capability on distribution generalization compared to GAN-based methods. Our approach signif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#28085;&#30422;&#20102;&#22312;NISQ&#25216;&#26415;&#21644;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#30828;&#20214;&#19978;&#20351;&#29992;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#24182;&#28145;&#20837;&#35752;&#35770;&#20102;&#19982;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.11351</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65306;&#20174;NISQ&#21040;&#23481;&#38169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning: from NISQ to Fault Tolerance. (arXiv:2401.11351v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#28085;&#30422;&#20102;&#22312;NISQ&#25216;&#26415;&#21644;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#30828;&#20214;&#19978;&#20351;&#29992;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#24182;&#28145;&#20837;&#35752;&#35770;&#20102;&#19982;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26159;&#22312;&#37327;&#23376;&#35774;&#22791;&#19978;&#36816;&#34892;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36807;&#31243;&#65292;&#22312;&#23398;&#26415;&#30028;&#21644;&#21830;&#19994;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#28044;&#29616;&#30340;&#21508;&#31181;&#27010;&#24565;&#36827;&#34892;&#20102;&#20840;&#38754;&#32780;&#20844;&#27491;&#30340;&#22238;&#39038;&#12290;&#36825;&#21253;&#25324;&#22312;&#22122;&#22768;&#20013;&#38388;&#23610;&#24230;&#37327;&#23376;&#65288;NISQ&#65289;&#25216;&#26415;&#20013;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#19982;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#30828;&#20214;&#20860;&#23481;&#30340;&#31639;&#27861;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#28085;&#30422;&#20102;&#19982;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#22522;&#26412;&#27010;&#24565;&#12289;&#31639;&#27861;&#21644;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning, which involves running machine learning algorithms on quantum devices, has garnered significant attention in both academic and business circles. In this paper, we offer a comprehensive and unbiased review of the various concepts that have emerged in the field of quantum machine learning. This includes techniques used in Noisy Intermediate-Scale Quantum (NISQ) technologies and approaches for algorithms compatible with fault-tolerant quantum computing hardware. Our review covers fundamental concepts, algorithms, and the statistical learning theory pertinent to quantum machine learning.
&lt;/p&gt;</description></item><item><title>SymTC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33136;&#26894;MR&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;Transformer&#21644;CNN&#30456;&#32467;&#21512;&#65292;&#24182;&#21033;&#29992;&#20301;&#32622;&#23884;&#20837;&#21644;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23454;&#20363;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2401.09627</link><description>&lt;p&gt;
SymTC:&#19968;&#31181;&#29992;&#20110;&#33136;&#26894;MRI&#23454;&#20363;&#20998;&#21106;&#30340;&#20849;&#29983;Transformer-CNN&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of Lumbar Spine MRI. (arXiv:2401.09627v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09627
&lt;/p&gt;
&lt;p&gt;
SymTC&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33136;&#26894;MR&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;Transformer&#21644;CNN&#30456;&#32467;&#21512;&#65292;&#24182;&#21033;&#29992;&#20301;&#32622;&#23884;&#20837;&#21644;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23454;&#20363;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26894;&#38388;&#30424;&#30142;&#30149;&#26159;&#19968;&#31181;&#24120;&#35265;&#30149;&#30151;&#65292;&#32463;&#24120;&#23548;&#33268;&#38388;&#27463;&#24615;&#25110;&#25345;&#32493;&#24615;&#30340;&#33136;&#32972;&#30140;&#30171;&#65292;&#23545;&#35813;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#35780;&#20272;&#20381;&#36182;&#20110;&#33136;&#26894;MR&#22270;&#20687;&#20013;&#26894;&#39592;&#21644;&#26894;&#38388;&#30424;&#20960;&#20309;&#24418;&#29366;&#30340;&#20934;&#30830;&#27979;&#37327;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#33258;&#21160;&#21270;&#22320;&#23545;&#33136;&#26894;&#30340;&#20010;&#20307;&#23454;&#20363;&#65288;&#26894;&#39592;&#21644;&#26894;&#38388;&#30424;&#65289;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#65292;&#36825;&#34987;&#31216;&#20026;&#23454;&#20363;&#22270;&#20687;&#20998;&#21106;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SymTC&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#33136;&#26894;MR&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;Transformer&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24182;&#34892;&#30340;&#21452;&#36335;&#24452;&#26550;&#26500;&#26469;&#34701;&#21512;CNN&#23618;&#21644;Transformer&#23618;&#65292;&#24182;&#22312;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#38598;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#23884;&#20837;&#65292;&#22686;&#24378;&#20102;&#20301;&#32622;&#20449;&#24687;&#30340;&#21033;&#29992;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23450;&#20301;&#31995;&#32479;&#36827;&#34892;&#27169;&#22411;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intervertebral disc disease, a prevalent ailment, frequently leads to intermittent or persistent low back pain, and diagnosing and assessing of this disease rely on accurate measurement of vertebral bone and intervertebral disc geometries from lumbar MR images. Deep neural network (DNN) models may assist clinicians with more efficient image segmentation of individual instances (disks and vertebrae) of the lumbar spine in an automated way, which is termed as instance image segmentation. In this work, we proposed SymTC, an innovative lumbar spine MR image segmentation model that combines the strengths of Transformer and Convolutional Neural Network (CNN). Specifically, we designed a parallel dual-path architecture to merge CNN layers and Transformer layers, and we integrated a novel position embedding into the self-attention module of Transformer, enhancing the utilization of positional information for more accurate segmentation. To further improves model performance, we introduced a new
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;PCA&#21644;&#20854;&#21464;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#27969;&#24418;&#30340;&#32771;&#34385;&#12290;&#36890;&#36807;&#22312;&#26071;&#24092;&#27969;&#24418;&#19978;&#36827;&#34892;&#20248;&#21270;&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#32467;&#21512;&#20027;&#27979;&#22320;&#32447;&#36817;&#20284;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04071</link><description>&lt;p&gt;
&#26071;&#24092;&#28216;&#25103;&#65306;&#36890;&#36807;&#26071;&#24092;&#27969;&#24418;&#26469;&#33719;&#24471;&#40065;&#26834;&#30340;&#20027;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Fun with Flags: Robust Principal Directions via Flag Manifolds. (arXiv:2401.04071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;PCA&#21644;&#20854;&#21464;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#27969;&#24418;&#30340;&#32771;&#34385;&#12290;&#36890;&#36807;&#22312;&#26071;&#24092;&#27969;&#24418;&#19978;&#36827;&#34892;&#20248;&#21270;&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#32467;&#21512;&#20027;&#27979;&#22320;&#32447;&#36817;&#20284;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21450;&#20854;&#23545;&#27969;&#24418;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#25193;&#23637;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PCA&#21450;&#20854;&#21464;&#31181;&#30340;&#32479;&#19968;&#24418;&#24335;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#30340;&#26694;&#26550;&#65292;&#21363;&#36880;&#28176;&#22686;&#21152;&#32500;&#24230;&#30340;&#23884;&#22871;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#19981;&#20165;&#20801;&#35768;&#20849;&#21516;&#23454;&#29616;&#65292;&#36824;&#20135;&#29983;&#20102;&#26032;&#30340;&#26410;&#26366;&#25506;&#32034;&#30340;&#21464;&#31181;&#12290;&#25105;&#20204;&#20174;&#24191;&#20041;&#21270;&#20256;&#32479;&#30340;PCA&#26041;&#27861;&#24320;&#22987;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#26368;&#22823;&#21270;&#26041;&#24046;&#65292;&#35201;&#20040;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#12290;&#25105;&#20204;&#25193;&#23637;&#36825;&#20123;&#35299;&#37322;&#65292;&#36890;&#36807;&#32771;&#34385;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#27969;&#24418;&#65292;&#24320;&#21457;&#20986;&#20102;&#22823;&#37327;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;&#20026;&#20102;&#35774;&#35745;&#19968;&#31181;&#36890;&#29992;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#40065;&#26834;&#21644;&#23545;&#20598;&#24418;&#24335;&#30340;PCA&#37325;&#26032;&#26500;&#24314;&#20026;&#22312;&#26071;&#24092;&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20027;&#27979;&#22320;&#32447;&#36817;&#20284;&#65288;&#20999;&#32447;PCA&#65289;&#25972;&#21512;&#21040;&#36825;&#20010;&#22522;&#20110;&#26071;&#24092;&#30340;&#26694;&#26550;&#20013;&#65292;&#21019;&#36896;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning. In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, \ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously. We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error. We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold. To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds. We then integrate tangent space approximations of principal geodesic analysis (tangent-PCA) into this flag-based framework, crea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30446;&#26631;&#35782;&#21035;&#20013;&#23558;&#22320;&#29702;&#22810;&#26679;&#30693;&#35782;&#34701;&#20837;&#25552;&#31034;&#20197;&#25552;&#39640;&#22320;&#29702;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#22320;&#29702;&#29305;&#23450;&#23545;&#35937;&#30693;&#35782;&#24182;&#32467;&#21512;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#23398;&#20064;&#36719;&#25552;&#31034;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22320;&#29702;&#30693;&#35782;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20174;&#28304;&#22320;&#29702;&#20301;&#32622;&#25512;&#24191;&#21040;&#26410;&#35265;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.01482</link><description>&lt;p&gt;
&#22312;&#30446;&#26631;&#35782;&#21035;&#20013;&#23558;&#22320;&#29702;&#22810;&#26679;&#30693;&#35782;&#34701;&#20837;&#25552;&#31034;&#20197;&#25552;&#39640;&#22320;&#29702;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition. (arXiv:2401.01482v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30446;&#26631;&#35782;&#21035;&#20013;&#23558;&#22320;&#29702;&#22810;&#26679;&#30693;&#35782;&#34701;&#20837;&#25552;&#31034;&#20197;&#25552;&#39640;&#22320;&#29702;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#22320;&#29702;&#29305;&#23450;&#23545;&#35937;&#30693;&#35782;&#24182;&#32467;&#21512;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#23398;&#20064;&#36719;&#25552;&#31034;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22320;&#29702;&#30693;&#35782;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20174;&#28304;&#22320;&#29702;&#20301;&#32622;&#25512;&#24191;&#21040;&#26410;&#35265;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#22312;&#19981;&#21516;&#22320;&#29702;&#24773;&#26223;&#19979;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#30001;&#20110;&#35774;&#35745;&#21644;&#29615;&#22659;&#20013;&#23384;&#22312;&#37325;&#35201;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#20026;&#20102;&#26356;&#20934;&#30830;&#22320;&#21453;&#26144;&#36825;&#20123;&#36716;&#31227;&#19979;&#30340;&#23545;&#35937;&#27010;&#24565;&#65292;&#38656;&#35201;&#35843;&#25972;&#31867;&#21035;&#34920;&#31034;&#12290;&#22312;&#32570;&#20047;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20551;&#35774;&#21487;&#20197;&#21033;&#29992;&#22320;&#29702;&#29305;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#25551;&#36848;&#24615;&#30693;&#35782;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36890;&#36807;&#25506;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29702;&#29305;&#23450;&#23545;&#35937;&#30693;&#35782;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;CLIP&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#38598;&#25104;&#30693;&#35782;&#30340;&#38646;&#26679;&#26412;&#21644;&#21487;&#23398;&#20064;&#36719;&#25552;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22320;&#29702;&#30693;&#35782;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#22312;&#19968;&#32452;&#28304;&#22320;&#29702;&#20301;&#32622;&#19978;&#35757;&#32451;&#30340;&#36719;&#25552;&#31034;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#22320;&#29702;&#20301;&#32622;&#38598;&#21512;&#12290;&#24403;&#20165;&#20381;&#36182;&#26469;&#33258;&#27431;&#27954;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#22312;DollarStreet&#19978;&#30340;&#22686;&#30410;&#36798;&#21040;&#20102;+2.8&#20010;&#22269;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing object recognition models have been shown to lack robustness in diverse geographical scenarios due to significant domain shifts in design and context. Class representations need to be adapted to more accurately reflect an object concept under these shifts. In the absence of training data from target geographies, we hypothesize that geography-specific descriptive knowledge of object categories can be leveraged to enhance robustness. For this purpose, we explore the feasibility of probing a large-language model for geography-specific object knowledge, and we investigate integrating knowledge in zero-shot and learnable soft prompting with the CLIP vision-language model. In particular, we propose a geography knowledge regularization method to ensure that soft prompts trained on a source set of geographies generalize to an unseen target set of geographies. Our gains on DollarStreet when generalizing from a model trained only on data from Europe are as large as +2.8 on countries fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;Score Distillation&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26694;&#26550;&#23384;&#22312;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#21464;&#20998;&#30446;&#26631;&#20013;&#21152;&#20837;&#29109;&#39033;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#35299;&#20915;&#20102;Janus&#20266;&#20687;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00909</link><description>&lt;p&gt;
Score Distillation&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20013;&#35299;&#20915;&#27169;&#24335;&#23849;&#28291;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Taming Mode Collapse in Score Distillation for Text-to-3D Generation. (arXiv:2401.00909v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;Score Distillation&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26694;&#26550;&#23384;&#22312;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#21464;&#20998;&#30446;&#26631;&#20013;&#21152;&#20837;&#29109;&#39033;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#35299;&#20915;&#20102;Janus&#20266;&#20687;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Score Distillation&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#26222;&#36941;&#23384;&#22312;&#35270;&#22270;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#20063;&#34987;&#31216;&#20026;&#8220;Janus&#8221;&#20266;&#20687;&#65292;&#21363;&#29983;&#25104;&#30340;&#23545;&#35937;&#20266;&#36896;&#20102;&#22810;&#20010;&#21069;&#35270;&#22270;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#32463;&#39564;&#26377;&#25928;&#30340;&#26041;&#27861;&#36890;&#36807;&#21435;&#20559;&#32622;&#25110;&#32773;&#24341;&#23548;&#24037;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#30340;&#26356;&#20005;&#26684;&#30340;&#35299;&#37322;&#21644;&#35299;&#20915;&#26041;&#27861;&#20173;&#28982;&#24456;&#38590;&#25214;&#21040;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;Score Distillation&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26694;&#26550;&#38519;&#20837;&#20102;&#22312;&#27599;&#20010;&#35270;&#22270;&#19978;&#29420;&#31435;&#26368;&#22823;&#20284;&#28982;&#27714;&#35299;&#30340;&#26368;&#22823;&#20284;&#28982;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#65292;&#23454;&#36341;&#20013;&#34920;&#29616;&#20026;Janus&#20266;&#20687;&#12290;&#20026;&#20102;&#36991;&#20813;&#27169;&#24335;&#23849;&#28291;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#30456;&#24212;&#21464;&#20998;&#30446;&#26631;&#20013;&#37325;&#26032;&#24341;&#20837;&#29109;&#39033;&#65292;&#23545;&#28210;&#26579;&#22270;&#20687;&#30340;&#20998;&#24067;&#36827;&#34892;&#25913;&#36827;&#12290;&#26368;&#22823;&#21270;&#29109;&#40723;&#21169;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#22312;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#20855;&#26377;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable performance of score distillation in text-to-3D generation, such techniques notoriously suffer from view inconsistency issues, also known as "Janus" artifact, where the generated objects fake each view with multiple front faces. Although empirically effective methods have approached this problem via score debiasing or prompt engineering, a more rigorous perspective to explain and tackle this problem remains elusive. In this paper, we reveal that the existing score distillation-based text-to-3D generation frameworks degenerate to maximal likelihood seeking on each view independently and thus suffer from the mode collapse problem, manifesting as the Janus artifact in practice. To tame mode collapse, we improve score distillation by re-establishing in entropy term in the corresponding variational objective, which is applied to the distribution of rendered images. Maximizing the entropy encourages diversity among different views in generated 3D assets, thereby mitiga
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23376;&#22270;&#30340;&#38271;&#31243;&#31070;&#32463;&#21407;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#23376;&#25237;&#23556;&#20026;&#31070;&#32463;&#21407;&#23376;&#24182;&#22312;&#20854;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36828;&#36317;&#31163;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#32553;&#23567;&#20102;&#20219;&#24847;&#33410;&#28857;&#23545;&#30340;&#30456;&#20114;&#20316;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2311.01276</link><description>&lt;p&gt;
&#38024;&#23545;&#20998;&#23376;&#22270;&#30340;&#38271;&#36317;&#31163;&#31070;&#32463;&#21407;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Long-Range Neural Atom Learning for Molecular Graphs. (arXiv:2311.01276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01276
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23376;&#22270;&#30340;&#38271;&#31243;&#31070;&#32463;&#21407;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#23376;&#25237;&#23556;&#20026;&#31070;&#32463;&#21407;&#23376;&#24182;&#22312;&#20854;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36828;&#36317;&#31163;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#32553;&#23567;&#20102;&#20219;&#24847;&#33410;&#28857;&#23545;&#30340;&#30456;&#20114;&#20316;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20998;&#23376;&#22270;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;GNN&#20027;&#35201;&#25797;&#38271;&#21033;&#29992;&#30701;&#31243;&#30456;&#20114;&#20316;&#29992;&#65288;SRI&#65289;&#65292;&#20294;&#38590;&#20197;&#25429;&#25417;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#65288;LRI&#65289;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#30830;&#23450;&#20998;&#23376;&#24615;&#36136;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25152;&#26377;&#21407;&#23376;&#38544;&#24335;&#25237;&#23556;&#20026;&#23569;&#25968;&#31070;&#32463;&#21407;&#23376;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#31070;&#32463;&#21407;&#23376;&#25277;&#35937;&#20986;&#20998;&#23376;&#20869;&#21407;&#23376;&#32452;&#30340;&#38598;&#20307;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#22312;&#31070;&#32463;&#21407;&#23376;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#19968;&#31181;&#22686;&#24378;&#23558;&#20854;&#37325;&#26032;&#25237;&#23556;&#21040;&#21407;&#23376;&#30340;&#34920;&#31034;&#19978;&#12290;&#36890;&#36807;&#36825;&#31181;&#26426;&#21046;&#65292;&#31070;&#32463;&#21407;&#23376;&#22312;&#36828;&#36317;&#31163;&#33410;&#28857;&#20043;&#38388;&#24314;&#31435;&#36890;&#20449;&#36890;&#36947;&#65292;&#26377;&#25928;&#22320;&#23558;&#20219;&#24847;&#33410;&#28857;&#23545;&#30340;&#30456;&#20114;&#20316;&#29992;&#33539;&#22260;&#20943;&#23569;&#21040;&#21333;&#27425;&#36339;&#36291;&#12290;&#20026;&#20102;&#20174;&#29289;&#29702;&#35282;&#24230;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#23457;&#26597;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#19982;&#20256;&#32479;&#30340;LRI&#35745;&#31639;&#26041;&#27861;Ewald&#27714;&#21644;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been widely adopted for drug discovery with molecular graphs. Nevertheless, current GNNs are mainly good at leveraging short-range interactions (SRI) but struggle to capture long-range interactions (LRI), both of which are crucial for determining molecular properties. To tackle this issue, we propose a method that implicitly projects all original atoms into a few Neural Atoms, which abstracts the collective information of atomic groups within a molecule. Specifically, we explicitly exchange the information among neural atoms and project them back to the atoms' representations as an enhancement. With this mechanism, neural atoms establish the communication channels among distant nodes, effectively reducing the interaction scope of arbitrary node pairs into a single hop. To provide an inspection of our method from a physical perspective, we reveal its connection with the traditional LRI calculation method, Ewald Summation. We conduct extensive experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2311.01017</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion. (arXiv:2311.01017v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#33258;&#21160;&#39550;&#39542;&#19990;&#30028;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#24182;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28857;&#20113;&#35266;&#23519;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#25945;&#20250;&#26234;&#33021;&#20307;&#19990;&#30028;&#30340;&#36816;&#20316;&#26041;&#24335;&#12290;&#23613;&#31649;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20294;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#19982;&#20351;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#25193;&#23637;&#19990;&#30028;&#27169;&#22411;&#30340;&#36827;&#23637;&#30456;&#23545;&#36739;&#24930;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#29942;&#39048;&#65306;&#22788;&#29702;&#22797;&#26434;&#21644;&#26080;&#32467;&#26500;&#30340;&#35266;&#23519;&#31354;&#38388;&#20197;&#21450;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;&#39318;&#20808;&#20351;&#29992;VQVAE&#23545;&#20256;&#24863;&#22120;&#35266;&#23519;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#28982;&#21518;&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#39044;&#27979;&#26410;&#26469;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#24182;&#34892;&#35299;&#30721;&#21644;&#21435;&#22122;&#26631;&#35760;&#65292;&#25105;&#20204;&#23558;&#36974;&#34109;&#29983;&#25104;&#22270;&#20687;&#36716;&#25442;&#22120;&#36716;&#25442;&#20026;&#31163;&#25955;&#25193;&#25955;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20123;&#31616;&#21333;&#30340;&#25913;&#36827;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;&#24403;&#24212;&#29992;&#20110;&#28857;&#20113;&#35266;&#23519;&#30340;&#19990;&#30028;&#27169;&#22411;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;1&#31186;&#39044;&#27979;&#30340;SOTA Chamfer&#36317;&#31163;&#38477;&#20302;&#20102;65%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with Generative Pre-trained Transformers (GPT). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose a novel world modeling approach that first tokenizes sensor observations with VQVAE, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast Masked Generative Image Transformer into the discrete diffusion framework with a few simple changes, resulting in notable improvement. When applied to learning world models on point cloud observations, our model reduces prior SOTA Chamfer distance by more than 65% for 1s prediction, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22312;&#27969;&#24418;&#19978;&#22788;&#29702;&#30690;&#37327;&#20540;&#20449;&#21495;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#20855;&#26377;&#20869;&#22312;&#23450;&#20041;&#21644;&#32771;&#34385;&#31354;&#38388;&#20960;&#20309;&#30340;&#29305;&#28857;&#65292;&#24182;&#20026;&#37096;&#32626;&#22312;&#20108;&#32500;&#29699;&#38754;&#21644;&#36229;&#26354;&#38754;&#19978;&#30340;Hodge-Mat\'ern&#39640;&#26031;&#21521;&#37327;&#22330;&#25552;&#20379;&#20102;&#35745;&#31639;&#22522;&#20803;&#12290;</title><link>http://arxiv.org/abs/2310.18824</link><description>&lt;p&gt;
&#29699;&#38754;&#19978;&#30340;&#20869;&#22312;&#39640;&#26031;&#21521;&#37327;&#22330;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Gaussian Vector Fields on Manifolds. (arXiv:2310.18824v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22312;&#27969;&#24418;&#19978;&#22788;&#29702;&#30690;&#37327;&#20540;&#20449;&#21495;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#20855;&#26377;&#20869;&#22312;&#23450;&#20041;&#21644;&#32771;&#34385;&#31354;&#38388;&#20960;&#20309;&#30340;&#29305;&#28857;&#65292;&#24182;&#20026;&#37096;&#32626;&#22312;&#20108;&#32500;&#29699;&#38754;&#21644;&#36229;&#26354;&#38754;&#19978;&#30340;Hodge-Mat\'ern&#39640;&#26031;&#21521;&#37327;&#22330;&#25552;&#20379;&#20102;&#35745;&#31639;&#22522;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26426;&#22120;&#20154;&#25216;&#26415;&#21040;&#27668;&#20505;&#31185;&#23398;&#31561;&#21508;&#31181;&#24212;&#29992;&#37117;&#38656;&#35201;&#23545;&#38750;&#27431;&#20960;&#37324;&#24471;&#22495;&#65288;&#22914;&#29699;&#38754;&#65289;&#19978;&#30340;&#20449;&#21495;&#36827;&#34892;&#24314;&#27169;&#12290;&#26368;&#36817;&#65292;&#22312;&#27969;&#34892;&#24230;&#37327;&#31354;&#38388;&#19978;&#25552;&#20986;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22312;&#38656;&#35201;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20219;&#21153;&#20013;&#12290;&#22312;&#27969;&#24418;&#35774;&#32622;&#20013;&#65292;&#19982;&#26631;&#37327;&#20540;&#20449;&#21495;&#30456;&#27604;&#65292;&#30690;&#37327;&#20540;&#20449;&#21495;&#21487;&#33021;&#34920;&#29616;&#20986;&#25130;&#28982;&#19981;&#21516;&#30340;&#34892;&#20026;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#22823;&#37096;&#20998;&#36827;&#23637;&#37117;&#38598;&#20013;&#22312;&#23545;&#21069;&#32773;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#65292;&#22914;&#23545;&#26410;&#30693;&#21160;&#21147;&#31995;&#32479;&#30340;&#39118;&#36895;&#25110;&#21147;&#22330;&#36827;&#34892;&#24314;&#27169;&#65292;&#21518;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27969;&#24418;&#19978;&#20026;&#30690;&#37327;&#20540;&#20449;&#21495;&#25552;&#20379;&#20869;&#22312;&#23450;&#20041;&#24182;&#32771;&#34385;&#31354;&#38388;&#20960;&#20309;&#30340;&#26032;&#22411;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#37096;&#32626;&#25152;&#24471;&#21040;&#30340;Hodge-Mat\'ern&#39640;&#26031;&#21521;&#37327;&#22330;&#22312;&#20108;&#32500;&#29699;&#38754;&#21644;&#36229;&#26354;&#38754;&#19978;&#25152;&#38656;&#30340;&#35745;&#31639;&#22522;&#20803;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#20004;&#20010;&#25512;&#24191;&#26041;&#21521;&#65306;&#31163;&#25955;&#30340;&#20108;&#32500;&#32593;&#26684;&#21644;&#8221;ide&#8220;&#65288;&#26242;&#19988;&#35793;&#20026;&#65306;&#24819;&#27861;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various applications ranging from robotics to climate science require modeling signals on non-Euclidean domains, such as the sphere. Gaussian process models on manifolds have recently been proposed for such tasks, in particular when uncertainty quantification is needed. In the manifold setting, vector-valued signals can behave very differently from scalar-valued ones, with much of the progress so far focused on modeling the latter. The former, however, are crucial for many applications, such as modeling wind speeds or force fields of unknown dynamical systems. In this paper, we propose novel Gaussian process models for vector-valued signals on manifolds that are intrinsically defined and account for the geometry of the space in consideration. We provide computational primitives needed to deploy the resulting Hodge-Mat\'ern Gaussian vector fields on the two-dimensional sphere and the hypertori. Further, we highlight two generalization directions: discrete two-dimensional meshes and "ide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;ROAM&#21644;ROOM&#31639;&#27861;&#26694;&#26550;&#36890;&#36807;&#23558;&#20013;&#20301;&#25968;&#27861;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#23545;&#37325;&#23614;&#22870;&#21169;&#30340;&#30452;&#25509;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18715</link><description>&lt;p&gt;
&#20855;&#26377;&#37325;&#23614;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#21644;&#20248;&#21270;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Robust Offline Policy Evaluation and Optimization with Heavy-Tailed Rewards. (arXiv:2310.18715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;ROAM&#21644;ROOM&#31639;&#27861;&#26694;&#26550;&#36890;&#36807;&#23558;&#20013;&#20301;&#25968;&#27861;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#23545;&#37325;&#23614;&#22870;&#21169;&#30340;&#30452;&#25509;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#22686;&#24378;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#37325;&#23614;&#22870;&#21169;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#31639;&#27861;&#26694;&#26550;&#65292;ROAM&#21644;ROOM&#65292;&#29992;&#20110;&#40065;&#26834;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#21644;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#23558;&#20013;&#20301;&#25968;&#27861;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#23545;&#20540;&#20989;&#25968;&#20272;&#35745;&#22120;&#36827;&#34892;&#30452;&#25509;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#36825;&#19981;&#20165;&#31526;&#21512;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#20013;&#30340;&#20445;&#23432;&#20027;&#20041;&#21407;&#21017;&#65292;&#32780;&#19988;&#28789;&#27963;&#22788;&#29702;&#37325;&#23614;&#22870;&#21169;&#12290;&#29702;&#35770;&#32467;&#26524;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#20004;&#20010;&#26694;&#26550;&#22312;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#20013;&#23637;&#31034;&#20102;&#20855;&#26377;&#37325;&#23614;&#22870;&#21169;&#20998;&#24067;&#26102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper endeavors to augment the robustness of offline reinforcement learning (RL) in scenarios laden with heavy-tailed rewards, a prevalent circumstance in real-world applications. We propose two algorithmic frameworks, ROAM and ROOM, for robust off-policy evaluation (OPE) and offline policy optimization (OPO), respectively. Central to our frameworks is the strategic incorporation of the median-of-means method with offline RL, enabling straightforward uncertainty estimation for the value function estimator. This not only adheres to the principle of pessimism in OPO but also adeptly manages heavy-tailed rewards. Theoretical results and extensive experiments demonstrate that our two frameworks outperform existing methods on the logged dataset exhibits heavy-tailed reward distributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#31070;&#32463;ODE&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#23558;&#23545;&#25239;&#35757;&#32451;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#25216;&#26415;&#26469;&#23454;&#29616;&#40065;&#26834;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.17584</link><description>&lt;p&gt;
&#19968;&#31181;&#40065;&#26834;&#31070;&#32463;ODE&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A minimax optimal control approach for robust neural ODEs. (arXiv:2310.17584v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#31070;&#32463;ODE&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#23558;&#23545;&#25239;&#35757;&#32451;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#25216;&#26415;&#26469;&#23454;&#29616;&#40065;&#26834;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#40065;&#26834;&#25511;&#21046;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#31070;&#32463;ODE&#30340;&#23545;&#25239;&#35757;&#32451;&#38382;&#39064;&#12290;&#36825;&#26159;&#19968;&#31181;&#26367;&#20195;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#30830;&#20445;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#21487;&#38752;&#32467;&#26524;&#12290;&#31070;&#32463;ODE&#20801;&#35768;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#20026;&#25511;&#21046;&#31995;&#32479;&#30340;&#31163;&#25955;&#21270;&#65292;&#20174;&#25511;&#21046;&#29702;&#35770;&#20013;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#21457;&#21644;&#29702;&#35299;&#12290;&#22312;&#36825;&#31181;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558;&#25200;&#21160;&#25968;&#25454;&#30340;&#23545;&#25239;&#35757;&#32451;&#20844;&#24335;&#21270;&#20026;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;Pontryagin&#26368;&#22823;&#20540;&#21407;&#29702;&#30340;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#40065;&#26834;&#35757;&#32451;&#30340;&#26032;&#35299;&#37322;&#65292;&#23548;&#33268;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#21152;&#26435;&#25216;&#26415;&#65292;&#24182;&#22312;&#20302;&#32500;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the adversarial training of neural ODEs from a robust control perspective. This is an alternative to the classical training via empirical risk minimization, and it is widely used to enforce reliable outcomes for input perturbations. Neural ODEs allow the interpretation of deep neural networks as discretizations of control systems, unlocking powerful tools from control theory for the development and the understanding of machine learning. In this specific case, we formulate the adversarial training with perturbed data as a minimax optimal control problem, for which we derive first order optimality conditions in the form of Pontryagin's Maximum Principle. We provide a novel interpretation of robust training leading to an alternative weighted technique, which we test on a low-dimensional classification task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;grokking&#29616;&#35937;&#19981;&#20165;&#23616;&#38480;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#20986;&#29616;&#22312;&#20854;&#20182;&#31639;&#27861;&#21644;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#28155;&#21152;&#34394;&#20551;&#20449;&#24687;&#30340;&#32500;&#24230;&#65292;&#21487;&#20197;&#35825;&#21457;grokking&#29616;&#35937;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;grokking&#29616;&#35937;&#22312;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#21463;&#22797;&#26434;&#24615;&#21644;&#38169;&#35823;&#25351;&#23548;&#30340;&#20219;&#20309;&#24773;&#20917;&#19979;&#21487;&#33021;&#21457;&#29983;&#12290;&#36825;&#23545;&#29702;&#35299;grokking&#29616;&#35937;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.17247</link><description>&lt;p&gt;
&#36229;&#36234;&#31070;&#32463;&#32593;&#32476;&#65306;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#32463;&#39564;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Grokking Beyond Neural Networks: An Empirical Exploration with Model Complexity. (arXiv:2310.17247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;grokking&#29616;&#35937;&#19981;&#20165;&#23616;&#38480;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#20986;&#29616;&#22312;&#20854;&#20182;&#31639;&#27861;&#21644;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#28155;&#21152;&#34394;&#20551;&#20449;&#24687;&#30340;&#32500;&#24230;&#65292;&#21487;&#20197;&#35825;&#21457;grokking&#29616;&#35937;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;grokking&#29616;&#35937;&#22312;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#21463;&#22797;&#26434;&#24615;&#21644;&#38169;&#35823;&#25351;&#23548;&#30340;&#20219;&#20309;&#24773;&#20917;&#19979;&#21487;&#33021;&#21457;&#29983;&#12290;&#36825;&#23545;&#29702;&#35299;grokking&#29616;&#35937;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#23637;&#29616;&#20986;&#19968;&#31181;&#31216;&#20026;&#8220;grokking&#8221;&#30340;&#29616;&#35937;&#65292;&#21363;&#23427;&#20204;&#22312;&#39564;&#35777;&#38598;&#19978;&#23454;&#29616;&#23436;&#32654;&#25110;&#25509;&#36817;&#23436;&#32654;&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#22312;&#35757;&#32451;&#38598;&#19978;&#21017;&#26089;&#24050;&#36798;&#21040;&#30456;&#21516;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;grokking&#19981;&#20165;&#38480;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#20986;&#29616;&#22312;&#20854;&#20182;&#35774;&#32622;&#20013;&#65292;&#20363;&#22914;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20998;&#31867;&#12289;GP&#22238;&#24402;&#21644;&#32447;&#24615;&#22238;&#24402;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#36890;&#36807;&#28155;&#21152;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#30340;&#32500;&#24230;&#26469;&#35825;&#21457;&#22522;&#20110;&#31639;&#27861;&#30340;&#25968;&#25454;&#38598;&#20013;&#30340;grokking&#29616;&#35937;&#30340;&#26426;&#21046;&#12290;&#38750;&#31070;&#32463;&#32467;&#26500;&#20013;&#30340;&#36825;&#31181;&#29616;&#35937;&#30340;&#23384;&#22312;&#35777;&#26126;&#20102;grokking&#19981;&#23616;&#38480;&#20110;SGD&#25110;&#26435;&#37325;&#33539;&#25968;&#27491;&#21017;&#21270;&#12290;&#30456;&#21453;&#65292;grokking&#21487;&#33021;&#21457;&#29983;&#22312;&#20219;&#20309;&#30001;&#22797;&#26434;&#24615;&#21644;&#38169;&#35823;&#25351;&#23548;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#30340;&#24773;&#20917;&#20013;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#21644;&#25105;&#20204;&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#21644;GP&#22238;&#24402;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#20013;&#35266;&#23519;&#21040;&#30340;&#36827;&#19968;&#27493;&#36235;&#21183;&#65292;&#25105;&#20204;&#22312;grokking&#30340;&#26356;&#19968;&#33324;&#30340;&#29702;&#35770;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In some settings neural networks exhibit a phenomenon known as grokking, where they achieve perfect or near-perfect accuracy on the validation set long after the same performance has been achieved on the training set. In this paper, we discover that grokking is not limited to neural networks but occurs in other settings such as Gaussian process (GP) classification, GP regression and linear regression. We also uncover a mechanism by which to induce grokking on algorithmic datasets via the addition of dimensions containing spurious information. The presence of the phenomenon in non-neural architectures provides evidence that grokking is not specific to SGD or weight norm regularisation. Instead, grokking may be possible in any setting where solution search is guided by complexity and error. Based on this insight and further trends we see in the training trajectories of a Bayesian neural network (BNN) and GP regression model, we make progress towards a more general theory of grokking. Spe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#21644;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21305;&#37197;&#24739;&#32773;&#30340;&#21307;&#23398;&#21644;&#33647;&#29289;&#29305;&#24615;&#26469;&#35782;&#21035;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#31574;&#30053;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#30340;&#30149;&#21490;&#21644;&#33647;&#29289;&#29305;&#24449;&#26469;&#21046;&#23450;&#65292;&#24182;&#21457;&#29616;&#20943;&#23569;&#33647;&#29289;&#21058;&#37327;&#21487;&#20197;&#20943;&#36731;&#30149;&#24773;&#32780;&#19981;&#20250;&#23545;&#27835;&#30103;&#25928;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.15333</link><description>&lt;p&gt;
&#20272;&#35745;&#21487;&#20449;&#36182;&#21644;&#23433;&#20840;&#30340;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Estimating Trustworthy and Safe Optimal Treatment Regimes. (arXiv:2310.15333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15333
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#21644;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21305;&#37197;&#24739;&#32773;&#30340;&#21307;&#23398;&#21644;&#33647;&#29289;&#29305;&#24615;&#26469;&#35782;&#21035;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#31574;&#30053;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#30340;&#30149;&#21490;&#21644;&#33647;&#29289;&#29305;&#24449;&#26469;&#21046;&#23450;&#65292;&#24182;&#21457;&#29616;&#20943;&#23569;&#33647;&#29289;&#21058;&#37327;&#21487;&#20197;&#20943;&#36731;&#30149;&#24773;&#32780;&#19981;&#20250;&#23545;&#27835;&#30103;&#25928;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#32479;&#35745;&#23398;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26174;&#33879;&#25512;&#21160;&#20102;&#24739;&#32773;&#25252;&#29702;&#31574;&#30053;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#24456;&#22823;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#32570;&#22833;&#25968;&#25454;&#12289;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#20197;&#21450;&#23545;&#35299;&#37322;&#24615;&#21644;&#24739;&#32773;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36816;&#29992;&#20102;&#19968;&#31181;&#23433;&#20840;&#19988;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#26469;&#35782;&#21035;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#23558;&#20855;&#26377;&#30456;&#20284;&#21307;&#23398;&#21644;&#33647;&#29289;&#29305;&#24449;&#30340;&#24739;&#32773;&#36827;&#34892;&#21305;&#37197;&#65292;&#20174;&#32780;&#36890;&#36807;&#25554;&#20540;&#26500;&#24314;&#26368;&#20339;&#25919;&#31574;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#35813;&#26694;&#26550;&#21363;&#20351;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20063;&#33021;&#22815;&#35782;&#21035;&#26368;&#20339;&#25919;&#31574;&#30340;&#33021;&#21147;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#30740;&#31350;&#23545;&#37325;&#30151;&#24739;&#32773;&#36827;&#34892;&#30315;&#30187;&#27835;&#30103;&#30340;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#28872;&#25903;&#25345;&#22522;&#20110;&#24739;&#32773;&#30340;&#21307;&#30103;&#21382;&#21490;&#21644;&#33647;&#29289;&#29305;&#24449;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#31574;&#30053;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20943;&#23569;&#33647;&#29289;&#21058;&#37327;&#21487;&#20197;&#20943;&#36731;&#30149;&#24773;&#32780;&#19981;&#20250;&#23545;&#27835;&#30103;&#30340;&#25928;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent statistical and reinforcement learning methods have significantly advanced patient care strategies. However, these approaches face substantial challenges in high-stakes contexts, including missing data, inherent stochasticity, and the critical requirements for interpretability and patient safety. Our work operationalizes a safe and interpretable framework to identify optimal treatment regimes. This approach involves matching patients with similar medical and pharmacological characteristics, allowing us to construct an optimal policy via interpolation. We perform a comprehensive simulation study to demonstrate the framework's ability to identify optimal policies even in complex settings. Ultimately, we operationalize our approach to study regimes for treating seizures in critically ill patients. Our findings strongly support personalized treatment strategies based on a patient's medical history and pharmacological features. Notably, we identify that reducing medication doses for 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;&#19987;&#23478;&#23398;&#20064;&#25512;&#36831;&#38382;&#39064;&#30340;&#20195;&#29702;&#25439;&#22833;&#21644;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#24378;H&#19968;&#33268;&#24615;&#30028;&#38480;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#23454;&#38469;&#24212;&#29992;&#30340;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#26368;&#23567;&#21270;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#30340;&#23398;&#20064;&#25512;&#36831;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22312;SVHN&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2310.14774</link><description>&lt;p&gt;
&#22810;&#20010;&#19987;&#23478;&#23398;&#20064;&#25512;&#36831;&#30340;&#21407;&#21017;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Principled Approaches for Learning to Defer with Multiple Experts. (arXiv:2310.14774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14774
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;&#19987;&#23478;&#23398;&#20064;&#25512;&#36831;&#38382;&#39064;&#30340;&#20195;&#29702;&#25439;&#22833;&#21644;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#24378;H&#19968;&#33268;&#24615;&#30028;&#38480;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#23454;&#38469;&#24212;&#29992;&#30340;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#26368;&#23567;&#21270;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#30340;&#23398;&#20064;&#25512;&#36831;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22312;SVHN&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#22810;&#20010;&#19987;&#23478;&#23398;&#20064;&#25512;&#36831;&#38382;&#39064;&#30340;&#20195;&#29702;&#25439;&#22833;&#21644;&#31639;&#27861;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31867;&#19987;&#38376;&#38024;&#23545;&#22810;&#19987;&#23478;&#35774;&#32622;&#30340;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#20013;&#39044;&#27979;&#21644;&#25512;&#36831;&#20989;&#25968;&#21516;&#26102;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#21463;&#30410;&#20110;&#24378;H&#19968;&#33268;&#24615;&#30028;&#38480;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#23454;&#38469;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#30340;&#31034;&#20363;&#23637;&#31034;&#20102;&#25105;&#20204;&#20998;&#26512;&#30340;&#24212;&#29992;&#65292;&#24182;&#32473;&#20986;&#20102;&#26126;&#30830;&#30340;&#20445;&#35777;&#12290;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#30452;&#25509;&#23548;&#33268;&#20102;&#22522;&#20110;&#23427;&#20204;&#26368;&#23567;&#21270;&#30340;&#26032;&#30340;&#23398;&#20064;&#25512;&#36831;&#31639;&#27861;&#30340;&#35774;&#35745;&#12290;&#34429;&#28982;&#26412;&#24037;&#20316;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#29702;&#35770;&#20998;&#26512;&#65292;&#20294;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#22312;SVHN&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#20010;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a study of surrogate losses and algorithms for the general problem of learning to defer with multiple experts. We first introduce a new family of surrogate losses specifically tailored for the multiple-expert setting, where the prediction and deferral functions are learned simultaneously. We then prove that these surrogate losses benefit from strong $H$-consistency bounds. We illustrate the application of our analysis through several examples of practical surrogate losses, for which we give explicit guarantees. These loss functions readily lead to the design of new learning to defer algorithms based on their minimization. While the main focus of this work is a theoretical analysis, we also report the results of several experiments on SVHN and CIFAR-10 datasets.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31867;&#21035;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#23398;&#20064;&#19982;&#25918;&#24323;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#32467;&#26524;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#29616;&#23384;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#36825;&#20123;&#20445;&#35777;&#20026;&#22522;&#20110;&#26368;&#23567;&#21270;&#25918;&#24323;&#25439;&#22833;&#30340;&#26032;&#30340;&#22810;&#31867;&#21035;&#25918;&#24323;&#31639;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.14772</link><description>&lt;p&gt;
&#39044;&#27979;-&#25298;&#32477;&#22810;&#31867;&#25918;&#24323;&#65306;&#29702;&#35770;&#20998;&#26512;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Predictor-Rejector Multi-Class Abstention: Theoretical Analysis and Algorithms. (arXiv:2310.14772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14772
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31867;&#21035;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#23398;&#20064;&#19982;&#25918;&#24323;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#32467;&#26524;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#29616;&#23384;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#36825;&#20123;&#20445;&#35777;&#20026;&#22522;&#20110;&#26368;&#23567;&#21270;&#25918;&#24323;&#25439;&#22833;&#30340;&#26032;&#30340;&#22810;&#31867;&#21035;&#25918;&#24323;&#31639;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31867;&#21035;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#23398;&#20064;&#19982;&#25918;&#24323;&#26694;&#26550;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#36873;&#25321;&#20197;&#19968;&#23450;&#30340;&#39044;&#23450;&#20041;&#25104;&#26412;&#25918;&#24323;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#32467;&#26524;&#65292;&#35299;&#20915;&#20102;&#39044;&#27979;-&#25298;&#32477;&#26694;&#26550;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#26032;&#30340;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#35777;&#26126;&#20102;&#24378;&#38750;&#28176;&#36827;&#21644;&#20551;&#35774;&#38598;&#29305;&#23450;&#30340;&#19968;&#33268;&#24615;&#20445;&#35777;&#65292;&#20174;&#32780;&#31215;&#26497;&#22320;&#35299;&#20915;&#20102;&#20004;&#20010;&#29616;&#23384;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#36825;&#20123;&#20445;&#35777;&#25552;&#20379;&#20102;&#25918;&#24323;&#25439;&#22833;&#20989;&#25968;&#30340;&#20272;&#35745;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#19982;&#26367;&#20195;&#25439;&#22833;&#30340;&#35823;&#24046;&#30456;&#20851;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21516;&#26102;&#23398;&#20064;&#39044;&#27979;&#22120;&#21644;&#25298;&#32477;&#22120;&#30340;&#21333;&#38454;&#27573;&#35774;&#32622;&#65292;&#20197;&#21450;&#22312;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20004;&#38454;&#27573;&#35774;&#32622;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#26631;&#20934;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#22914;&#20132;&#21449;&#29109;&#26469;&#23398;&#20064;&#39044;&#27979;&#22120;&#12290;&#36825;&#20123;&#20445;&#35777;&#20026;&#22522;&#20110;&#26368;&#23567;&#21270;&#25918;&#24323;&#25439;&#22833;&#30340;&#26032;&#30340;&#22810;&#31867;&#21035;&#25918;&#24323;&#31639;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the key framework of learning with abstention in the multi-class classification setting. In this setting, the learner can choose to abstain from making a prediction with some pre-defined cost. We present a series of new theoretical and algorithmic results for this learning problem in the predictor-rejector framework. We introduce several new families of surrogate losses for which we prove strong non-asymptotic and hypothesis set-specific consistency guarantees, thereby resolving positively two existing open questions. These guarantees provide upper bounds on the estimation error of the abstention loss function in terms of that of the surrogate loss. We analyze both a single-stage setting where the predictor and rejector are learned simultaneously and a two-stage setting crucial in applications, where the predictor is learned in a first stage using a standard surrogate loss such as cross-entropy. These guarantees suggest new multi-class abstention algorithms based on minimizing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#22810;&#31867;&#25918;&#24323;&#30340;&#29702;&#35770;&#22522;&#30784;&#25439;&#22833;&#20989;&#25968;&#21644;&#31639;&#27861;&#65292;&#21253;&#25324;&#24341;&#20837;&#20102;&#26032;&#30340;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#26063;&#32676;&#20197;&#21450;&#35777;&#26126;&#20102;&#36825;&#20123;&#20195;&#29702;&#25439;&#22833;&#30340;&#19968;&#33268;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.14770</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#22810;&#31867;&#25918;&#24323;&#30340;&#29702;&#35770;&#22522;&#30784;&#25439;&#22833;&#20989;&#25968;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Theoretically Grounded Loss Functions and Algorithms for Score-Based Multi-Class Abstention. (arXiv:2310.14770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#22810;&#31867;&#25918;&#24323;&#30340;&#29702;&#35770;&#22522;&#30784;&#25439;&#22833;&#20989;&#25968;&#21644;&#31639;&#27861;&#65292;&#21253;&#25324;&#24341;&#20837;&#20102;&#26032;&#30340;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#26063;&#32676;&#20197;&#21450;&#35777;&#26126;&#20102;&#36825;&#20123;&#20195;&#29702;&#25439;&#22833;&#30340;&#19968;&#33268;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20013;&#30340;&#25918;&#24323;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#22330;&#26223;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#36873;&#25321;&#22312;&#26576;&#20010;&#20195;&#20215;&#19979;&#25918;&#24323;&#36827;&#34892;&#39044;&#27979;&#12290;&#26412;&#25991;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#30340;&#35774;&#32622;&#19979;&#20998;&#26512;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#23398;&#20064;&#20013;&#30340;&#25918;&#24323;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#25918;&#24323;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#20195;&#29702;&#25439;&#22833;&#26063;&#32676;&#65292;&#20854;&#20013;&#21253;&#25324;&#21333;&#38454;&#27573;&#35774;&#32622;&#20013;&#26368;&#20808;&#36827;&#30340;&#20195;&#29702;&#25439;&#22833;&#20197;&#21450;&#20108;&#38454;&#27573;&#35774;&#32622;&#20013;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#20195;&#29702;&#25439;&#22833;&#30340;&#24378;&#38750;&#28176;&#36817;&#21644;&#20551;&#35774;&#38598;&#29305;&#23450;&#30340;&#19968;&#33268;&#24615;&#20445;&#35777;&#65292;&#36825;&#20123;&#20445;&#35777;&#19978;&#30028;&#20102;&#25918;&#24323;&#25439;&#22833;&#20989;&#25968;&#30340;&#20272;&#35745;&#35823;&#24046;&#65292;&#19982;&#20195;&#29702;&#25439;&#22833;&#30340;&#20272;&#35745;&#35823;&#24046;&#30456;&#20851;&#12290;&#25105;&#20204;&#30340;&#19978;&#30028;&#21487;&#20197;&#24110;&#21161;&#27604;&#36739;&#19981;&#21516;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#20195;&#29702;&#25439;&#22833;&#65292;&#25351;&#23548;&#36890;&#36807;&#26368;&#23567;&#21270;&#25552;&#20986;&#30340;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#26469;&#35774;&#35745;&#26032;&#30340;&#25918;&#24323;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;SVHN&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26032;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26032;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with abstention is a key scenario where the learner can abstain from making a prediction at some cost. In this paper, we analyze the score-based formulation of learning with abstention in the multi-class classification setting. We introduce new families of surrogate losses for the abstention loss function, which include the state-of-the-art surrogate losses in the single-stage setting and a novel family of loss functions in the two-stage setting. We prove strong non-asymptotic and hypothesis set-specific consistency guarantees for these surrogate losses, which upper-bound the estimation error of the abstention loss function in terms of the estimation error of the surrogate loss. Our bounds can help compare different score-based surrogates and guide the design of novel abstention algorithms by minimizing the proposed surrogate losses. We experimentally evaluate our new algorithms on CIFAR-10, CIFAR-100, and SVHN datasets and the practical significance of our new surrogate losse
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#20174;&#33258;&#21457;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.10908</link><description>&lt;p&gt;
&#33258;&#21457;&#24615;&#27169;&#22359;&#21270;&#32467;&#26500;&#65306;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#33021;&#21542;&#20174;&#33258;&#21457;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#65311;
&lt;/p&gt;
&lt;p&gt;
Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?. (arXiv:2310.10908v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10908
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#20174;&#33258;&#21457;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27169;&#22359;&#21270;&#35774;&#35745;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23637;&#31034;&#20986;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23398;&#20064;&#25928;&#29575;&#31561;&#20248;&#28857;&#12290;&#29616;&#26377;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#26159;&#8220;&#26174;&#24335;&#8221;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#27169;&#22359;&#21270;&#26550;&#26500;&#26159;&#39044;&#20808;&#23450;&#20041;&#30340;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#34987;&#26399;&#26395;&#23454;&#29616;&#19981;&#21516;&#30340;&#21151;&#33021;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#22312;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;Transformer&#20013;&#23384;&#22312;&#8220;&#38544;&#24335;&#8221;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#21363;&#8220;&#33258;&#21457;&#27169;&#22359;&#21270;&#8221;&#12290;&#20182;&#20204;&#34920;&#26126;&#36825;&#26679;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#22312;&#26089;&#26399;&#39044;&#35757;&#32451;&#38454;&#27573;&#23601;&#20250;&#20986;&#29616;&#65292;&#24182;&#19988;&#23436;&#20840;&#26159;&#33258;&#21457;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;Transformer&#27169;&#22411;&#20173;&#28982;&#34987;&#35270;&#20026;&#21333;&#20307;&#27169;&#22411;&#65292;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20854;&#27169;&#22359;&#21270;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#37492;&#20110;&#26174;&#24335;&#27169;&#22359;&#21270;&#26550;&#26500;&#30340;&#20248;&#33391;&#29305;&#24615;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#20174;&#33258;&#21457;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating modular designs into neural networks demonstrates superior out-of-generalization, learning efficiency, etc. Existing modular neural networks are generally $\textit{explicit}$ because their modular architectures are pre-defined, and individual modules are expected to implement distinct functions. Conversely, recent works reveal that there exist $\textit{implicit}$ modular structures in standard pre-trained transformers, namely $\textit{Emergent Modularity}$. They indicate that such modular structures exhibit during the early pre-training phase and are totally spontaneous. However, most transformers are still treated as monolithic models with their modular natures underutilized. Therefore, given the excellent properties of explicit modular architecture, we explore $\textit{whether and how dense pre-trained transformers can benefit from emergent modular structures.}$ To study this question, we construct \textbf{E}mergent $\textbf{M}$ixture-$\textbf{o}$f-$\textbf{E}$xperts (E
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09518</link><description>&lt;p&gt;
&#20154;&#31867;&#35838;&#31243;&#25351;&#23548;&#19979;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#20027;&#27969;&#33539;&#24335;&#26159;&#38543;&#26426;&#27927;&#29260;&#35757;&#32451;&#26368;&#22823;&#22810;&#26679;&#21270;&#25351;&#20196;-&#21709;&#24212;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#19982;&#20197;&#24448;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#25351;&#20196;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#25945;&#32946;&#30340;&#28176;&#36827;&#24615;&#21644;&#26377;&#32452;&#32455;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#19982;&#25945;&#32946;&#26694;&#26550;&#23545;&#40784;&#26469;&#31574;&#21010;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#27599;&#20010;&#26679;&#26412;&#21253;&#25324;&#20027;&#39064;&#21644;&#35748;&#30693;&#20005;&#35880;&#31243;&#24230;&#31561;&#20803;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#20013;&#23398;&#21040;&#30740;&#31350;&#29983;&#38454;&#27573;&#30340;&#20840;&#38754;&#32454;&#31890;&#24230;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#37117;&#26377;&#21508;&#31181;&#38382;&#39064;&#65292;&#20197;&#21033;&#29992;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#32423;&#27861;&#25552;&#39640;&#27010;&#24565;&#28145;&#24230;&#65292;&#35813;&#20998;&#32423;&#27861;&#29992;&#20110;&#21306;&#20998;&#27599;&#20010;&#27010;&#24565;&#30340;&#19981;&#21516;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
&lt;/p&gt;</description></item><item><title>Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09486</link><description>&lt;p&gt;
Mirage: &#22270;&#20998;&#31867;&#30340;&#27169;&#22411;&#26080;&#20851;&#22270;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Mirage: Model-Agnostic Graph Distillation for Graph Classification. (arXiv:2310.09486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09486
&lt;/p&gt;
&lt;p&gt;
Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GNNs&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#23545;&#25968;&#25454;&#21644;&#35745;&#31639;&#38656;&#27714;&#37327;&#24456;&#22823;&#12290;&#24613;&#38656;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#25193;&#23637;GNN&#30340;&#35757;&#32451;&#65292;&#20197;&#20415;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#22270;&#33976;&#39311;&#26159;&#20026;&#27492;&#30446;&#30340;&#32780;&#21162;&#21147;&#65292;&#26088;&#22312;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#34429;&#28982;&#21021;&#27493;&#24037;&#20316;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;(1)&#29616;&#26377;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#26412;&#36523;&#20381;&#36182;&#20110;&#20351;&#29992;&#23436;&#25972;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#23601;&#30772;&#22351;&#20102;&#22270;&#33976;&#39311;&#30340;&#21069;&#25552;&#12290;(2)&#33976;&#39311;&#36807;&#31243;&#23545;&#30446;&#26631;GNN&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#20855;&#26377;&#29305;&#24322;&#24615;&#65292;&#22240;&#27492;&#23545;&#24314;&#27169;&#27969;&#31243;&#30340;&#21464;&#21270;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;Mirage&#30340;&#22270;&#20998;&#31867;&#33976;&#39311;&#31639;&#27861;&#26469;&#36991;&#20813;&#36825;&#20123;&#38480;&#21046;&#12290;Mirage&#24314;&#31435;&#22312;&#19968;&#20010;&#27934;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#21363;&#19968;&#20010;&#28040;&#24687;&#20256;&#36882;&#30340;GNN&#23558;&#36755;&#20837;&#22270;&#20998;&#35299;&#20026;&#35745;&#31639;&#30340;&#22810;&#37325;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#21363;Ito&#38142;&#30340;Ito&#25193;&#25955;&#36924;&#36817;&#12290;&#19982;&#22823;&#22810;&#25968;&#30456;&#20851;&#35770;&#25991;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#38142;&#20855;&#26377;&#21508;&#21521;&#21516;&#24615;&#21644;&#29366;&#24577;&#30456;&#20851;&#30340;&#22122;&#22768;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Ito&#38142;&#19982;&#23545;&#24212;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20043;&#38388;&#30340;W2-&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#25913;&#36827;&#20102;&#24050;&#26377;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#22312;&#26576;&#20123;&#29305;&#27530;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#39318;&#27425;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.06081</link><description>&lt;p&gt;
&#29992;&#20110;&#37319;&#26679;&#12289;&#20248;&#21270;&#21644;&#25552;&#21319;&#30340;&#36890;&#29992;Ito&#38142;&#30340;Ito&#25193;&#25955;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting. (arXiv:2310.06081v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#21363;Ito&#38142;&#30340;Ito&#25193;&#25955;&#36924;&#36817;&#12290;&#19982;&#22823;&#22810;&#25968;&#30456;&#20851;&#35770;&#25991;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#38142;&#20855;&#26377;&#21508;&#21521;&#21516;&#24615;&#21644;&#29366;&#24577;&#30456;&#20851;&#30340;&#22122;&#22768;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Ito&#38142;&#19982;&#23545;&#24212;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20043;&#38388;&#30340;W2-&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#25913;&#36827;&#20102;&#24050;&#26377;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#22312;&#26576;&#20123;&#29305;&#27530;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#39318;&#27425;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#30456;&#24403;&#19968;&#33324;&#21644;&#24191;&#27867;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#21363;Ito&#38142;&#65292;&#20854;&#31867;&#20284;&#20110;&#26576;&#20123;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;Euler-Maruyama&#31163;&#25955;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#38142;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#20998;&#26512;&#26694;&#26550;&#12290;&#19982;&#22823;&#22810;&#25968;&#30456;&#20851;&#35770;&#25991;&#20013;&#30340;&#27491;&#24577;&#21644;&#29366;&#24577;&#29420;&#31435;&#22122;&#22768;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#38142;&#20855;&#26377;&#20960;&#20046;&#20219;&#24847;&#21508;&#21521;&#21516;&#24615;&#21644;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38142;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#21487;&#20197;&#26159;&#31934;&#30830;&#30340;&#65292;&#20197;&#28085;&#30422;&#35832;&#22914;&#38543;&#26426;&#26799;&#24230;Langevin&#21160;&#21147;&#23398;&#12289;&#37319;&#26679;&#12289;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25110;&#38543;&#26426;&#26799;&#24230;&#25552;&#21319;&#31561;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Ito&#38142;&#19982;&#23545;&#24212;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20043;&#38388;&#30340;W2-&#36317;&#31163;&#30340;&#19968;&#20010;&#19978;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#25913;&#36827;&#25110;&#35206;&#30422;&#20102;&#22823;&#37096;&#20998;&#24050;&#30693;&#30340;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26576;&#20123;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#26159;&#31532;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work considers a rather general and broad class of Markov chains, Ito chains that look like Euler-Maryama discretization of some Stochastic Differential Equation. The chain we study is a unified framework for theoretical analysis. It comes with almost arbitrary isotropic and state-dependent noise instead of normal and state-independent one, as in most related papers. Moreover, our chain's drift and diffusion coefficient can be inexact to cover a wide range of applications such as Stochastic Gradient Langevin Dynamics, sampling, Stochastic Gradient Descent, or Stochastic Gradient Boosting. We prove an upper bound for $W_{2}$-distance between laws of the Ito chain and the corresponding Stochastic Differential Equation. These results improve or cover most of the known estimates. Moreover, for some particular cases, our analysis is the first.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;OGDM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26465;&#20214;&#37492;&#21035;&#22120;&#30340;&#35266;&#27979;&#25152;&#20135;&#29983;&#30340;&#39069;&#22806;&#25439;&#22833;&#39033;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20248;&#21270;&#65292;&#22312;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#26377;&#38480;&#30340;&#25512;&#29702;&#38454;&#27573;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.04041</link><description>&lt;p&gt;
&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Observation-Guided Diffusion Probabilistic Models. (arXiv:2310.04041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;OGDM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26465;&#20214;&#37492;&#21035;&#22120;&#30340;&#35266;&#27979;&#25152;&#20135;&#29983;&#30340;&#39069;&#22806;&#25439;&#22833;&#39033;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#20248;&#21270;&#65292;&#22312;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#26377;&#38480;&#30340;&#25512;&#29702;&#38454;&#27573;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;&#35266;&#27979;&#24341;&#23548;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;OGDM&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36136;&#37327;&#25511;&#21046;&#21644;&#24555;&#36895;&#37319;&#26679;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#23558;&#35266;&#27979;&#36807;&#31243;&#30340;&#24341;&#23548;&#19982;&#39532;&#23572;&#21487;&#22827;&#38142;&#30456;&#32467;&#21512;&#65292;&#37325;&#26032;&#24314;&#31435;&#20102;&#35757;&#32451;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26465;&#20214;&#37492;&#21035;&#22120;&#30340;&#35266;&#27979;&#25152;&#20135;&#29983;&#30340;&#39069;&#22806;&#25439;&#22833;&#39033;&#65292;&#25105;&#20204;&#20351;&#24471;&#20248;&#21270;&#26356;&#20934;&#30830;&#30340;&#36127;&#23545;&#25968;&#20284;&#28982;&#25104;&#20026;&#21487;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#26377;&#38480;&#30340;&#25512;&#29702;&#38454;&#27573;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#24471;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#21363;&#20351;&#21482;&#29992;&#20110;&#24494;&#35843;&#36807;&#31243;&#20063;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#19982;&#21508;&#31181;&#24555;&#36895;&#25512;&#29702;&#31574;&#30053;&#20860;&#23481;&#65292;&#22240;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#23436;&#20840;&#30456;&#21516;&#30340;&#25512;&#29702;&#36807;&#31243;&#20135;&#29983;&#26356;&#22909;&#30340;&#21435;&#22122;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel diffusion model called observation-guided diffusion probabilistic model (OGDM), which effectively addresses the trade-off between quality control and fast sampling. Our approach reestablishes the training objective by integrating the guidance of the observation process with the Markov chain in a principled way. This is achieved by introducing an additional loss term derived from the observation based on the conditional discriminator on noise level, which employs Bernoulli distribution indicating whether its input lies on the (noisy) real manifold or not. This strategy allows us to optimize the more accurate negative log-likelihood induced in the inference stage especially when the number of function evaluations is limited. The proposed training method is also advantageous even when incorporated only into the fine-tuning process, and it is compatible with various fast inference strategies since our method yields better denoising networks using the exactly same inferen
&lt;/p&gt;</description></item><item><title>LightSeq&#26159;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;&#36716;&#25442;&#22120;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#24207;&#21015;&#32500;&#24230;&#36827;&#34892;&#20998;&#21306;&#65292;&#19982;&#19981;&#21516;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#30340;&#27169;&#22411;&#26550;&#26500;&#20860;&#23481;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#19982;Megatron-LM&#30456;&#27604;&#30340;&#36890;&#20449;&#37327;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#37325;&#21472;&#12290;</title><link>http://arxiv.org/abs/2310.03294</link><description>&lt;p&gt;
LightSeq&#65306;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;&#36716;&#25442;&#22120;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#24207;&#21015;&#32423;&#24182;&#34892;ism
&lt;/p&gt;
&lt;p&gt;
LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers. (arXiv:2310.03294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03294
&lt;/p&gt;
&lt;p&gt;
LightSeq&#26159;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;&#36716;&#25442;&#22120;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#24207;&#21015;&#32500;&#24230;&#36827;&#34892;&#20998;&#21306;&#65292;&#19982;&#19981;&#21516;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#30340;&#27169;&#22411;&#26550;&#26500;&#20860;&#23481;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#19982;Megatron-LM&#30456;&#27604;&#30340;&#36890;&#20449;&#37327;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#37325;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#21487;&#20197;&#35299;&#24320;&#22522;&#26412;&#19978;&#26032;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#26174;&#33879;&#22686;&#21152;&#20102;&#35757;&#32451;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#20197;&#24448;&#30340;&#27169;&#22411;&#24182;&#34892;&#31995;&#32479;&#65288;&#20363;&#22914;Megatron-LM&#65289;&#23545;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#21306;&#21644;&#35745;&#31639;&#65292;&#24182;&#34892;&#22788;&#29702;&#65292;&#23548;&#33268;&#22823;&#37327;&#36890;&#20449;&#37327;&#65292;&#22240;&#27492;&#19981;&#33021;&#22312;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#20043;&#22806;&#25193;&#23637;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#20854;&#37319;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;LightSeq&#65292;&#29992;&#20110;&#38271;&#19978;&#19979;&#25991;LLM&#30340;&#35757;&#32451;&#12290;LightSeq&#20855;&#26377;&#35768;&#22810;&#26174;&#33879;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;LightSeq&#36890;&#36807;&#24207;&#21015;&#32500;&#24230;&#36827;&#34892;&#20998;&#21306;&#65292;&#22240;&#27492;&#23545;&#20110;&#20855;&#26377;&#19981;&#21516;&#27880;&#24847;&#21147;&#22836;&#25968;&#37327;&#30340;&#27169;&#22411;&#26550;&#26500;&#26159;&#19981;&#21487;&#30693;&#30340;&#65292;&#36866;&#29992;&#20110;Multi-Head&#65292;Multi-Query&#21644;Grouped-Query attention&#31561;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;LightSeq&#19982;Megatron-LM&#30456;&#27604;&#65292;&#22312;&#27969;&#34892;&#30340;LLM&#19978;&#19981;&#20165;&#38656;&#27714;&#23569;&#33267;4.7&#20493;&#30340;&#36890;&#20449;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#23558;&#36890;&#20449;&#19982;&#35745;&#31639;&#37325;&#21472;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;LightSeq&#36824;&#20855;&#26377;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;che
&lt;/p&gt;
&lt;p&gt;
Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LightSeq, for long-context LLMs training. LightSeq has many notable advantages. First, LightSeq partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LightSeq not only requires up to 4.7x less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LightSeq features a novel gradient che
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02391</link><description>&lt;p&gt;
SE(3)-&#34507;&#30333;&#36136;&#20027;&#38142;&#29983;&#25104;&#20013;&#30340;&#38543;&#26426;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02391
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;SE(3)-Stochastic Flow Matching&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;FoldFlow&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#34507;&#30333;&#36136;&#20027;&#38142;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#21644;Riemannian&#26368;&#20248;&#20256;&#36755;&#30340;&#32467;&#21512;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31283;&#23450;&#24615;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#19977;&#32500;&#21018;&#20307;&#36816;&#21160;&#65288;&#21363;SE(3)&#32676;&#65289;&#30340;&#27969;&#21305;&#37197;&#33539;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#26029;&#22686;&#24378;&#24314;&#27169;&#33021;&#21147;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65306;FoldFlow&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#20027;&#38142;&#30340;&#20934;&#30830;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FoldFlow-Base&#65292;&#19968;&#31181;&#26080;&#38656;&#27169;&#25311;&#30340;&#23398;&#20064;&#30830;&#23450;&#24615;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#21644;&#21305;&#37197;&#19981;&#21464;&#30446;&#26631;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;Riemannian&#26368;&#20248;&#20256;&#36755;&#26469;&#21152;&#36895;&#35757;&#32451;&#65292;&#21019;&#24314;&#20102;FoldFlow-OT&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#26356;&#31616;&#21333;&#21644;&#31283;&#23450;&#30340;&#27969;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;FoldFlow-SFM&#65292;&#23558;Riemannian&#26368;&#20248;&#20256;&#36755;&#21644;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23398;&#20064;SE(3)&#19978;&#30340;&#38543;&#26426;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#30340;FoldFlow&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce $\text{FoldFlow}$ a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\text{D}$ rigid motions -i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\text{SE(3)}$. We next accelerate training by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$, leading to the construction of both more simple and stable flows. Finally, we design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our family of $\text{FoldFlow}$ generative models offer several key advantages over previous
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.02279</link><description>&lt;p&gt;
&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65306;&#23398;&#20064;&#25193;&#25955;&#30340;&#27010;&#29575;&#27969;ODE&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. (arXiv:2310.02279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02279
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CM&#65289;&#21152;&#36895;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#65292;&#20294;&#20197;&#29306;&#29298;&#26679;&#26412;&#36136;&#37327;&#20026;&#20195;&#20215;&#65292;&#32570;&#20047;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#26435;&#34913;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#26159;&#21253;&#25324;CM&#21644;&#22522;&#20110;&#24471;&#20998;&#27169;&#22411;&#22312;&#20869;&#30340;&#27867;&#21270;&#27169;&#22411;&#12290;CTM&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#36755;&#20986;&#24471;&#20998;&#65288;&#21363;&#23545;&#25968;&#23494;&#24230;&#30340;&#26799;&#24230;&#65289;&#65292;&#24182;&#20801;&#35768;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#20219;&#24847;&#21021;&#22987;&#21644;&#26368;&#32456;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#19981;&#21463;&#38480;&#21046;&#30340;&#36941;&#21382;&#27010;&#29575;&#27969;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;CTM&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#26377;&#25928;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10&#65288;FID 1.73&#65289;&#21644;64X64&#20998;&#36776;&#29575;&#30340;ImageNet&#19978;&#23454;&#29616;&#26032;&#30340;&#26368;&#20808;&#36827;FID&#12290;CTM&#36824;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#21253;&#25324;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#30340;ODE&#35299;&#20013;&#30340;&#38271;&#36339;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE soluti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;FP8&#26684;&#24335;&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#30456;&#23545;&#20110;INT8&#20855;&#26377;&#26356;&#22909;&#30340;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14592</link><description>&lt;p&gt;
&#20351;&#29992;FP8&#26684;&#24335;&#30340;&#39640;&#25928;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Post-training Quantization with FP8 Formats. (arXiv:2309.14592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;FP8&#26684;&#24335;&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#20013;&#30340;&#20248;&#21183;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#30456;&#23545;&#20110;INT8&#20855;&#26377;&#26356;&#22909;&#30340;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#36827;&#23637;&#65292;&#22914;LLMs&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#23545;&#25913;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#30340;&#38656;&#27714;&#65292;&#20197;&#28385;&#36275;&#36825;&#20123;&#29616;&#20195;&#26550;&#26500;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;FP8&#25968;&#25454;&#26684;&#24335;&#22312;75&#20010;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#19978;&#36827;&#34892;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#20248;&#21183;&#65292;&#36825;&#20123;&#32593;&#32476;&#26550;&#26500;&#28085;&#30422;&#20102;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#35328;&#24314;&#27169;&#12289;&#25991;&#26412;&#29983;&#25104;&#12289;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#20998;&#21106;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;FP8&#34920;&#31034;&#65288;E5M2&#12289;E4M3&#21644;E3M4&#65289;&#65292;&#20197;&#30740;&#31350;&#22312;&#21160;&#24577;&#33539;&#22260;&#21644;&#31934;&#24230;&#20043;&#38388;&#19981;&#21516;&#26435;&#34913;&#31243;&#24230;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#24191;&#27867;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#37327;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#21487;&#20197;&#27010;&#25324;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;FP8&#26684;&#24335;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;INT8&#65292;&#21253;&#25324;&#24037;&#20316;&#36127;&#36733;&#35206;&#30422;&#29575;&#65288;92.64&#65285;&#23545;65.87&#65285;&#65289;&#12289;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#25805;&#20316;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning methods such as LLMs and Diffusion models have created a need for improved quantization methods that can meet the computational demands of these modern architectures while maintaining accuracy. Towards this goal, we study the advantages of FP8 data formats for post-training quantization across 75 unique network architectures covering a wide range of tasks, including machine translation, language modeling, text generation, image classification, generation, and segmentation. We examine three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects of varying degrees of trade-off between dynamic range and precision on model accuracy. Based on our extensive study, we developed a quantization workflow that generalizes across different network architectures. Our empirical results show that FP8 formats outperform INT8 in multiple aspects, including workload coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader range of ope
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#25214;&#21040;&#26368;&#20248;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#22810;&#20010;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#26641;&#12290;</title><link>http://arxiv.org/abs/2309.12701</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#21457;&#29616;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#24085;&#32047;&#25176;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Discovering the Interpretability-Performance Pareto Front of Decision Trees with Dynamic Programming. (arXiv:2309.12701v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#25214;&#21040;&#26368;&#20248;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#22810;&#20010;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20915;&#31574;&#26641;&#30001;&#20110;&#21487;&#20197;&#34987;&#20154;&#31867;&#26816;&#26597;&#21644;&#35299;&#37322;&#32780;&#20855;&#26377;&#22266;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30828;&#20214;&#30340;&#36827;&#27493;&#37325;&#26032;&#24341;&#36215;&#20102;&#23545;&#26368;&#20248;&#20915;&#31574;&#26641;&#31639;&#27861;&#30340;&#20851;&#27880;&#65292;&#36825;&#20123;&#31639;&#27861;&#27604;&#36890;&#24120;&#30340;&#36138;&#23146;&#26041;&#27861;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#26641;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20248;&#31639;&#27861;&#36820;&#22238;&#30340;&#26159;&#19968;&#20010;&#20248;&#21270;&#25163;&#21160;&#23450;&#20041;&#30340;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#21333;&#20010;&#26641;&#65292;&#36890;&#36807;&#25351;&#23450;&#26368;&#22823;&#20915;&#31574;&#33410;&#28857;&#25968;&#37327;&#26469;&#33719;&#24471;&#65292;&#23545;&#20110;&#36825;&#20010;&#26435;&#34913;&#30340;&#36136;&#37327;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#27934;&#23519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65288;MDP&#65289;&#24418;&#24335;&#26469;&#25214;&#21040;&#26368;&#20248;&#20915;&#31574;&#26641;&#12290;&#36825;&#31181;&#24418;&#24335;&#30340;&#20027;&#35201;&#20248;&#28857;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#21333;&#19968;&#30340;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#35745;&#31639;&#20986;&#22810;&#20010;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#35753;&#29992;&#25143;&#20107;&#21518;&#36873;&#25321;&#26368;&#36866;&#21512;&#20182;&#20204;&#38656;&#27714;&#30340;&#26641;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are known to be intrinsically interpretable as they can be inspected and interpreted by humans. Furthermore, recent hardware advances have rekindled an interest for optimal decision tree algorithms, that produce more accurate trees than the usual greedy approaches. However, these optimal algorithms return a single tree optimizing a hand defined interpretability-performance trade-off, obtained by specifying a maximum number of decision nodes, giving no further insights about the quality of this trade-off. In this paper, we propose a new Markov Decision Problem (MDP) formulation for finding optimal decision trees. The main interest of this formulation is that we can compute the optimal decision trees for several interpretability-performance trade-offs by solving a single dynamic program, letting the user choose a posteriori the tree that best suits their needs. Empirically, we show that our method is competitive with state-of-the-art algorithms in terms of accuracy and run
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#31038;&#21306;&#32467;&#26500;&#26159;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#31038;&#20250;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#31185;&#23398;&#23478;&#20204;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20294;&#23578;&#26410;&#25214;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#31038;&#21306;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#21508;&#31181;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.11798</link><description>&lt;p&gt;
&#22270;&#20013;&#31038;&#21306;&#26816;&#27979;&#30340;&#32508;&#21512;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review of Community Detection in Graphs. (arXiv:2309.11798v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#31038;&#21306;&#32467;&#26500;&#26159;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#31038;&#20250;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#31185;&#23398;&#23478;&#20204;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20294;&#23578;&#26410;&#25214;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#31038;&#21306;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#21508;&#31181;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#30740;&#31350;&#26174;&#33879;&#20419;&#36827;&#20102;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#31038;&#21306;&#32467;&#26500;&#30340;&#29702;&#35299;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#31038;&#20250;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;&#23613;&#31649;&#36328;&#23398;&#31185;&#31185;&#23398;&#23478;&#31038;&#21306;&#30340;&#21162;&#21147;&#65292;&#20294;&#23578;&#26410;&#25214;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#35814;&#32454;&#20171;&#32461;&#20102;&#22270;&#20013;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#30340;&#32452;&#32455;&#21644;&#21151;&#33021;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#31038;&#21306;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#23427;&#25351;&#30340;&#26159;&#23558;&#39030;&#28857;&#21010;&#20998;&#20026;&#20855;&#26377;&#24378;&#20869;&#37096;&#36830;&#25509;&#21644;&#36739;&#24369;&#36830;&#25509;&#30340;&#38598;&#32676;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#38416;&#36848;&#65292;&#21253;&#25324;&#25105;&#20204;&#35774;&#35745;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#31038;&#21306;&#26816;&#27979;&#22312;&#21508;&#31181;&#32593;&#32476;&#20013;&#30340;&#30495;&#23454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of complex networks has significantly advanced our understanding of community structures which serves as a crucial feature of real-world graphs. Detecting communities in graphs is a challenging problem with applications in sociology, biology, and computer science. Despite the efforts of an interdisciplinary community of scientists, a satisfactory solution to this problem has not yet been achieved. This review article delves into the topic of community detection in graphs, which serves as a crucial role in understanding the organization and functioning of complex systems. We begin by introducing the concept of community structure, which refers to the arrangement of vertices into clusters, with strong internal connections and weaker connections between clusters. Then, we provide a thorough exposition of various community detection methods, including a new method designed by us. Additionally, we explore real-world applications of community detection in diverse networks. In concl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36712;&#36947;&#31283;&#23450;&#31995;&#32479;&#29992;&#20110;&#22270;&#31034;&#25945;&#23398;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24050;&#30693;&#30340;&#36712;&#36947;&#28176;&#36817;&#31283;&#23450;&#31995;&#32479;&#36827;&#34892;&#24418;&#21464;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#36319;&#38543;&#29992;&#25143;&#25351;&#23450;&#33609;&#22270;&#36827;&#34892;&#21608;&#26399;&#36816;&#21160;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.10298</link><description>&lt;p&gt;
&#23398;&#20064;&#36712;&#36947;&#31283;&#23450;&#31995;&#32479;&#20197;&#22270;&#31034;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
Learning Orbitally Stable Systems for Diagrammatically Teaching. (arXiv:2309.10298v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36712;&#36947;&#31283;&#23450;&#31995;&#32479;&#29992;&#20110;&#22270;&#31034;&#25945;&#23398;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24050;&#30693;&#30340;&#36712;&#36947;&#28176;&#36817;&#31283;&#23450;&#31995;&#32479;&#36827;&#34892;&#24418;&#21464;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#36319;&#38543;&#29992;&#25143;&#25351;&#23450;&#33609;&#22270;&#36827;&#34892;&#21608;&#26399;&#36816;&#21160;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31034;&#25945;&#23398;&#26159;&#19968;&#31181;&#26426;&#22120;&#20154;&#33719;&#21462;&#26032;&#25216;&#33021;&#30340;&#33539;&#24335;&#65292;&#29992;&#25143;&#22312;&#22330;&#26223;&#22270;&#20687;&#19978;&#25552;&#20379;2D&#33609;&#22270;&#26469;&#25351;&#23548;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#25945;&#23548;&#26426;&#22120;&#20154;&#25509;&#36817;&#34920;&#38754;&#24182;&#22312;&#20854;&#19978;&#36827;&#34892;&#21608;&#26399;&#36816;&#21160;&#30340;&#38382;&#39064;&#65292;&#36816;&#21160;&#30340;&#21608;&#26399;&#21487;&#20197;&#30001;&#29992;&#25143;&#25552;&#20379;&#30340;&#21333;&#20010;&#33609;&#22270;&#22312;&#26426;&#22120;&#20154;&#25668;&#20687;&#22836;&#30340;&#22270;&#20687;&#19978;&#20219;&#24847;&#25351;&#23450;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#31283;&#23450;&#36842;&#27498;&#24418;&#22270;&#31034;&#25945;&#23398;&#8221;&#65288;SDDT&#65289;&#26694;&#26550;&#65292;&#23558;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#24314;&#27169;&#20026;&#8220;&#36712;&#36947;&#28176;&#36817;&#31283;&#23450;&#8221;&#65288;O.A.S.&#65289;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#23398;&#20064;&#36319;&#38543;&#29992;&#25143;&#25351;&#23450;&#30340;&#33609;&#22270;&#12290;&#36890;&#36807;&#24212;&#29992;&#21487;&#24494;&#20998;&#19988;&#21487;&#36870;&#30340;&#20989;&#25968;&#26469;&#23545;&#24050;&#30693;&#30340;O.A.S.&#31995;&#32479;&#36827;&#34892;&#24418;&#21464;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#21442;&#25968;&#21270;&#30340;&#36842;&#27491;&#24418;&#21464;&#22312;&#25105;&#20204;&#24314;&#27169;&#31995;&#32479;&#30340;&#26497;&#38480;&#21608;&#26399;&#21644;&#33609;&#22270;&#20043;&#38388;&#36827;&#34892;Hausdorff&#36317;&#31163;&#20248;&#21270;&#65292;&#20135;&#29983;&#25152;&#38656;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagrammatic Teaching is a paradigm for robots to acquire novel skills, whereby the user provides 2D sketches over images of the scene to shape the robot's motion. In this work, we tackle the problem of teaching a robot to approach a surface and then follow cyclic motion on it, where the cycle of the motion can be arbitrarily specified by a single user-provided sketch over an image from the robot's camera. Accordingly, we introduce the \emph{Stable Diffeomorphic Diagrammatic Teaching} (SDDT) framework. SDDT models the robot's motion as an \emph{Orbitally Asymptotically Stable} (O.A.S.) dynamical system that learns to follow the user-specified sketch. This is achieved by applying a \emph{diffeomorphism}, i.e. a differentiable and invertible function, to morph a known O.A.S. system. The parameterised diffeomorphism is then optimised with respect to the Hausdorff distance between the limit cycle of our modelled system and the sketch, to produce the desired robot motion. We provide theoret
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31034;&#25945;&#23398;&#30340;&#31034;&#25945;&#23398;&#20064;&#30340;&#26367;&#20195;&#33539;&#24335;&#65292;&#36890;&#36807;&#35201;&#27714;&#29992;&#25143;&#22312;&#22330;&#26223;&#30340;&#20108;&#32500;&#22270;&#20687;&#19978;&#21246;&#21202;&#31034;&#33539;&#36712;&#36857;&#26469;&#25945;&#26426;&#22120;&#20154;&#26032;&#30340;&#25216;&#33021;&#65292;&#24182;&#23558;&#20854;&#21512;&#25104;&#20026;&#19977;&#32500;&#20219;&#21153;&#31354;&#38388;&#20013;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.03835</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#24615;&#22270;&#31034;&#25945;&#23398;&#36827;&#34892;&#31034;&#25945;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Demonstration via Probabilistic Diagrammatic Teaching. (arXiv:2309.03835v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31034;&#25945;&#23398;&#30340;&#31034;&#25945;&#23398;&#20064;&#30340;&#26367;&#20195;&#33539;&#24335;&#65292;&#36890;&#36807;&#35201;&#27714;&#29992;&#25143;&#22312;&#22330;&#26223;&#30340;&#20108;&#32500;&#22270;&#20687;&#19978;&#21246;&#21202;&#31034;&#33539;&#36712;&#36857;&#26469;&#25945;&#26426;&#22120;&#20154;&#26032;&#30340;&#25216;&#33021;&#65292;&#24182;&#23558;&#20854;&#21512;&#25104;&#20026;&#19977;&#32500;&#20219;&#21153;&#31354;&#38388;&#20013;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31034;&#25945;&#23398;&#20064;&#65288;Learning for Demonstration&#65292;LfD&#65289;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#31034;&#33539;&#26469;&#33719;&#24471;&#26032;&#25216;&#33021;&#65292;&#20801;&#35768;&#29992;&#25143;&#20197;&#30452;&#35266;&#30340;&#26041;&#24335;&#20256;&#36798;&#20182;&#20204;&#30340;&#25351;&#31034;&#12290;&#26368;&#36817;&#22312;LfD&#39046;&#22495;&#30340;&#36827;&#23637;&#24448;&#24448;&#20381;&#36182;&#20110;&#21160;&#20316;&#31034;&#33539;&#25945;&#23398;&#25110;&#36828;&#31243;&#25805;&#20316;&#20316;&#20026;&#29992;&#25143;&#25351;&#23450;&#31034;&#33539;&#30340;&#25163;&#27573;&#12290;&#21160;&#20316;&#31034;&#33539;&#25945;&#23398;&#38656;&#35201;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#29289;&#29702;&#25805;&#32437;&#65292;&#32780;&#36828;&#31243;&#25805;&#20316;&#21017;&#38656;&#35201;&#29087;&#32451;&#25484;&#25569;&#39069;&#22806;&#30340;&#30828;&#20214;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31034;&#25945;&#23398;&#30340;LfD&#30340;&#26367;&#20195;&#33539;&#24335;&#12290;&#22270;&#31034;&#25945;&#23398;&#26088;&#22312;&#36890;&#36807;&#35201;&#27714;&#29992;&#25143;&#22312;&#22330;&#26223;&#30340;&#20108;&#32500;&#22270;&#20687;&#19978;&#21246;&#21202;&#31034;&#33539;&#36712;&#36857;&#26469;&#25945;&#26426;&#22120;&#20154;&#26032;&#30340;&#25216;&#33021;&#65292;&#28982;&#21518;&#36825;&#20123;&#36712;&#36857;&#23558;&#34987;&#21512;&#25104;&#20026;&#19977;&#32500;&#20219;&#21153;&#31354;&#38388;&#20013;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29992;&#20110;&#22270;&#31034;&#25945;&#23398;&#30340;&#23556;&#32447;&#36861;&#36394;&#27010;&#29575;&#36712;&#36857;&#23398;&#20064;&#65288;RPTL&#65289;&#26694;&#26550;&#12290;RPTL&#20174;&#20108;&#32500;&#22270;&#31034;&#20013;&#25552;&#21462;&#26102;&#38388;&#21464;&#21270;&#30340;&#27010;&#29575;&#23494;&#24230;&#65292;&#24182;&#24212;&#29992;&#23556;&#32447;&#36861;&#36394;&#26469;&#23547;&#25214;&#30456;&#24212;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning for Demonstration (LfD) enables robots to acquire new skills by imitating expert demonstrations, allowing users to communicate their instructions in an intuitive manner. Recent progress in LfD often relies on kinesthetic teaching or teleoperation as the medium for users to specify the demonstrations. Kinesthetic teaching requires physical handling of the robot, while teleoperation demands proficiency with additional hardware. This paper introduces an alternative paradigm for LfD called Diagrammatic Teaching. Diagrammatic Teaching aims to teach robots novel skills by prompting the user to sketch out demonstration trajectories on 2D images of the scene, these are then synthesised as a generative model of motion trajectories in 3D task space. Additionally, we present the Ray-tracing Probabilistic Trajectory Learning (RPTL) framework for Diagrammatic Teaching. RPTL extracts time-varying probability densities from the 2D sketches, applies ray-tracing to find corresponding regions i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#21160;&#24577;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DDAG&#65289;&#30340;&#20449;&#24687;&#29702;&#35770;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#30340;&#21151;&#29575;&#35889;&#23494;&#24230;&#30697;&#38453;&#30340;&#24230;&#37327;&#21644;&#31639;&#27861;&#26469;&#37325;&#24314;DDAG&#12290;</title><link>http://arxiv.org/abs/2308.16859</link><description>&lt;p&gt;
&#23398;&#20064;&#21160;&#24577;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#20449;&#24687;&#29702;&#35770;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs. (arXiv:2308.16859v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#21160;&#24577;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DDAG&#65289;&#30340;&#20449;&#24687;&#29702;&#35770;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#30340;&#21151;&#29575;&#35889;&#23494;&#24230;&#30697;&#38453;&#30340;&#24230;&#37327;&#21644;&#31639;&#27861;&#26469;&#37325;&#24314;DDAG&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;LDS&#65289;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#19978;&#30340;&#24213;&#23618;&#30456;&#20114;&#20316;&#29992;/&#20381;&#36182;&#20851;&#31995;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#23398;&#20064;DAG&#32467;&#26500;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#38745;&#24577;&#31995;&#32479;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#33410;&#28857;&#29366;&#24577;&#30340;&#26679;&#26412;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65288;i.i.d.&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#21160;&#24577;&#31995;&#32479;&#30340;DAG&#20013;&#65292;&#36825;&#26679;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;DAG&#31216;&#20026;\emph{&#21160;&#24577;}DAG&#65288;DDAG&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;DDAG&#65292;&#20854;&#20013;&#33410;&#28857;&#21160;&#21147;&#23398;&#30001;&#26410;&#35266;&#27979;&#30340;&#22806;&#29983;&#22122;&#22768;&#28304;&#39537;&#21160;&#65292;&#36825;&#20123;&#22122;&#22768;&#28304;&#22312;&#26102;&#38388;&#19978;&#26159;&#23485;&#24133;&#24179;&#31283;&#30340;&#65288;WSS&#65289;&#65292;&#20294;&#24444;&#27492;&#20043;&#38388;&#26159;&#19981;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#30456;&#21516;&#30340;&#21151;&#29575;&#35889;&#23494;&#24230;&#65288;PSD&#65289;&#12290;&#21463;&#38745;&#24577;&#35774;&#32622;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#30340;PSD&#30697;&#38453;&#30340;&#24230;&#37327;&#21644;&#31639;&#27861;&#26469;&#37325;&#24314;DDAG&#12290;&#22122;&#22768;PSD&#30456;&#31561;&#30340;&#20551;&#35774;&#21487;&#20197;&#25918;&#23485;&#65292;&#20197;&#20351;&#20854;&#21487;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, the optimal sample complexity of learning the underlying interaction/dependencies of a Linear Dynamical System (LDS) over a Directed Acyclic Graph (DAG) is studied. The sample complexity of learning a DAG's structure is well-studied for static systems, where the samples of nodal states are independent and identically distributed (i.i.d.). However, such a study is less explored for DAGs with dynamical systems, where the nodal states are temporally correlated. We call such a DAG underlying an LDS as \emph{dynamical} DAG (DDAG). In particular, we consider a DDAG where the nodal dynamics are driven by unobserved exogenous noise sources that are wide-sense stationary (WSS) in time but are mutually uncorrelated, and have the same {power spectral density (PSD)}. Inspired by the static settings, a metric and an algorithm based on the PSD matrix of the observed time series are proposed to reconstruct the DDAG. The equal noise PSD assumption can be relaxed such that identifiabil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FADE&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#31574;&#30053;&#21160;&#24577;&#20943;&#36731;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.15651</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#25512;&#33616;&#31995;&#32479;&#20013;&#30830;&#20445;&#29992;&#25143;&#20391;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Ensuring User-side Fairness in Dynamic Recommender Systems. (arXiv:2308.15651v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FADE&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#31574;&#30053;&#21160;&#24577;&#20943;&#36731;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#20391;&#32676;&#20307;&#20844;&#24179;&#24615;&#23545;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#26088;&#22312;&#20943;&#36731;&#30001;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#25110;&#24180;&#40836;&#65289;&#23450;&#20041;&#30340;&#29992;&#25143;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#24046;&#24322;&#24448;&#24448;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#25345;&#32493;&#23384;&#22312;&#29978;&#33267;&#22686;&#21152;&#12290;&#36825;&#38656;&#35201;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#26377;&#25928;&#35299;&#20915;&#29992;&#25143;&#20391;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#28982;&#32780;&#36825;&#22312;&#25991;&#29486;&#20013;&#24456;&#23569;&#34987;&#25506;&#35752;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#30830;&#20445;&#29992;&#25143;&#20391;&#20844;&#24179;&#24615;&#65288;&#21363;&#20943;&#23569;&#24615;&#33021;&#24046;&#24322;&#65289;&#30340;&#20856;&#22411;&#26041;&#27861;&#8212;&#8212;&#20844;&#24179;&#32422;&#26463;&#37325;&#26032;&#25490;&#21517;&#65292;&#22312;&#21160;&#24577;&#35774;&#23450;&#20013;&#38754;&#20020;&#20004;&#20010;&#22522;&#26412;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22522;&#20110;&#25490;&#21517;&#30340;&#20844;&#24179;&#32422;&#26463;&#30340;&#38750;&#21487;&#24494;&#24615;&#65292;&#38459;&#30861;&#20102;&#31471;&#21040;&#31471;&#35757;&#32451;&#33539;&#24335;&#65307;&#65288;2&#65289;&#26102;&#38388;&#25928;&#29575;&#20302;&#19979;&#65292;&#38459;&#30861;&#20102;&#23545;&#29992;&#25143;&#20559;&#22909;&#21464;&#21270;&#30340;&#24555;&#36895;&#36866;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FADE&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#31574;&#30053;&#21160;&#24577;&#20943;&#36731;&#24615;&#33021;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;FADE&#25552;&#20986;&#20102;&#19968;&#31181; fine-tuning &#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
User-side group fairness is crucial for modern recommender systems, as it aims to alleviate performance disparity between groups of users defined by sensitive attributes such as gender, race, or age. We find that the disparity tends to persist or even increase over time. This calls for effective ways to address user-side fairness in a dynamic environment, which has been infrequently explored in the literature. However, fairness-constrained re-ranking, a typical method to ensure user-side fairness (i.e., reducing performance disparity), faces two fundamental challenges in the dynamic setting: (1) non-differentiability of the ranking-based fairness constraint, which hinders the end-to-end training paradigm, and (2) time-inefficiency, which impedes quick adaptation to changes in user preferences. In this paper, we propose FAir Dynamic rEcommender (FADE), an end-to-end framework with fine-tuning strategy to dynamically alleviate performance disparity. To tackle the above challenges, FADE u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.11696</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#23548;&#33268;&#20102;&#19968;&#31867;&#20840;&#38754;&#35780;&#20272;&#24191;&#27867;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#19982;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#65292;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35780;&#20272;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;HELM&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#35745;&#31639;-&#21487;&#38752;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#36890;&#36807;&#20174;&#22522;&#20934;&#27979;&#35797;&#20013;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#65292;&#24403;&#21069;&#22312;HELM&#19978;&#30340;&#39046;&#20808;&#32773;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#21363;&#21487;&#33719;&#24471;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#24615;&#30340;&#20020;&#24202;&#21644;&#34394;&#25311;&#29983;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;COVID-19&#35786;&#26029;&#30340;AI&#27169;&#22411;&#65292;&#21457;&#29616;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#20110;AI&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#23481;&#26131;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#26368;&#39640;&#19979;&#38477;20&#65285;&#12290;</title><link>http://arxiv.org/abs/2308.09730</link><description>&lt;p&gt;
&#22522;&#20110;COVID-19&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#34394;&#25311;&#25104;&#20687;&#30340;AI&#35786;&#26029;&#65306;&#20197;&#30149;&#20363;&#30740;&#31350;&#20026;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Data diversity and virtual imaging in AI-based diagnosis: A case study based on COVID-19. (arXiv:2308.09730v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#24615;&#30340;&#20020;&#24202;&#21644;&#34394;&#25311;&#29983;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;COVID-19&#35786;&#26029;&#30340;AI&#27169;&#22411;&#65292;&#21457;&#29616;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#20110;AI&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#23481;&#26131;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#26368;&#39640;&#19979;&#38477;20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#22312;&#26032;&#22411;&#20896;&#29366;&#30149;&#27602;&#65288;COVID-19&#65289;&#30340;&#21307;&#23398;&#24433;&#20687;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#35768;&#22810;&#25253;&#36947;&#31216;&#20854;&#24615;&#33021;&#20960;&#20046;&#23436;&#32654;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#30340;&#21464;&#24322;&#24615;&#21644;&#28508;&#22312;&#30340;&#25968;&#25454;&#20559;&#24046;&#24341;&#21457;&#20102;&#23545;&#20020;&#24202;&#36866;&#29992;&#24615;&#30340;&#25285;&#24551;&#12290;&#26412;&#22238;&#39038;&#24615;&#30740;&#31350;&#28041;&#21450;&#20351;&#29992;&#20020;&#24202;&#22810;&#26679;&#24615;&#21644;&#34394;&#25311;&#29983;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#24320;&#21457;&#21644;&#35780;&#20272;COVID-19&#35786;&#26029;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#27425;&#34394;&#25311;&#25104;&#20687;&#35797;&#39564;&#65292;&#20197;&#35780;&#20272;AI&#24615;&#33021;&#21463;&#30142;&#30149;&#33539;&#22260;&#12289;&#36752;&#23556;&#21058;&#37327;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#21644;&#33016;&#37096;&#25918;&#23556;&#25668;&#24433;&#65288;CXR&#65289;&#25104;&#20687;&#27169;&#24577;&#31561;&#20960;&#20010;&#24739;&#32773;&#21644;&#29289;&#29702;&#24615;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#25968;&#25454;&#38598;&#29305;&#24449;&#65288;&#21253;&#25324;&#25968;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#24739;&#30149;&#29575;&#65289;&#24378;&#28872;&#24433;&#21709;&#20102;AI&#30340;&#24615;&#33021;&#65292;&#23548;&#33268;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#19979;&#38477;&#20102;&#39640;&#36798;20&#65285;&#65292;&#19988;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many studies have investigated deep-learning-based artificial intelligence (AI) models for medical imaging diagnosis of the novel coronavirus (COVID-19), with many reports of near-perfect performance. However, variability in performance and underlying data biases raise concerns about clinical generalizability. This retrospective study involved the development and evaluation of artificial intelligence (AI) models for COVID-19 diagnosis using both diverse clinical and virtually generated medical images. In addition, we conducted a virtual imaging trial to assess how AI performance is affected by several patient- and physics-based factors, including the extent of disease, radiation dose, and imaging modality of computed tomography (CT) and chest radiography (CXR). AI performance was strongly influenced by dataset characteristics including quantity, diversity, and prevalence, leading to poor generalization with up to 20% drop in receiver operating characteristic area under the curve. Model
&lt;/p&gt;</description></item><item><title>RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07922</link><description>&lt;p&gt;
RAVEN&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models. (arXiv:2308.07922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07922
&lt;/p&gt;
&lt;p&gt;
RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;ATLAS&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#39044;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#38271;&#24230;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RAVEN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26356;&#22810;&#19978;&#19979;&#25991;&#31034;&#20363;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#27169;&#22411;&#20462;&#25913;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RAVEN&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#26126;&#26174;&#20248;&#20110;ATLAS&#65292;&#24182;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder lang
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#32454;&#32990;&#20998;&#21106;&#22522;&#20934;&#65292;&#37319;&#29992;Transformer-based&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#20165;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#26174;&#24494;&#25104;&#20687;&#24179;&#21488;&#21644;&#32452;&#32455;&#31867;&#22411;&#30340;&#22270;&#20687;&#65292;&#26080;&#38656;&#25163;&#21160;&#21442;&#25968;&#35843;&#25972;&#65292;&#20026;&#26174;&#24494;&#25104;&#20687;&#20013;&#26356;&#20934;&#30830;&#21644;&#22810;&#21151;&#33021;&#30340;&#32454;&#32990;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2308.05864</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#32454;&#32990;&#20998;&#21106;&#25361;&#25112;&#65306;&#36808;&#21521;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions. (arXiv:2308.05864v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#32454;&#32990;&#20998;&#21106;&#22522;&#20934;&#65292;&#37319;&#29992;Transformer-based&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#20165;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#26174;&#24494;&#25104;&#20687;&#24179;&#21488;&#21644;&#32452;&#32455;&#31867;&#22411;&#30340;&#22270;&#20687;&#65292;&#26080;&#38656;&#25163;&#21160;&#21442;&#25968;&#35843;&#25972;&#65292;&#20026;&#26174;&#24494;&#25104;&#20687;&#20013;&#26356;&#20934;&#30830;&#21644;&#22810;&#21151;&#33021;&#30340;&#32454;&#32990;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#20998;&#21106;&#26159;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#36827;&#34892;&#23450;&#37327;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#29616;&#26377;&#30340;&#32454;&#32990;&#20998;&#21106;&#26041;&#27861;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#27169;&#24577;&#25110;&#38656;&#35201;&#25163;&#21160;&#24178;&#39044;&#26469;&#25351;&#23450;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#20013;&#30340;&#36229;&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#32454;&#32990;&#20998;&#21106;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;50&#22810;&#20010;&#19981;&#21516;&#29983;&#29289;&#23454;&#39564;&#30340;1500&#22810;&#20010;&#26631;&#35760;&#22270;&#20687;&#12290;&#21069;&#20960;&#21517;&#21442;&#19982;&#32773;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#20165;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#26174;&#24494;&#25104;&#20687;&#24179;&#21488;&#21644;&#32452;&#32455;&#31867;&#22411;&#30340;&#22810;&#26679;&#26174;&#24494;&#38236;&#22270;&#20687;&#65292;&#26080;&#38656;&#25163;&#21160;&#21442;&#25968;&#35843;&#25972;&#12290;&#36825;&#20010;&#22522;&#20934;&#21644;&#25913;&#36827;&#30340;&#31639;&#27861;&#20026;&#26174;&#24494;&#25104;&#20687;&#20013;&#26356;&#20934;&#30830;&#21644;&#22810;&#21151;&#33021;&#30340;&#32454;&#32990;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cell segmentation is a critical step for quantitative single-cell analysis in microscopy images. Existing cell segmentation methods are often tailored to specific modalities or require manual interventions to specify hyperparameters in different experimental settings. Here, we present a multi-modality cell segmentation benchmark, comprising over 1500 labeled images derived from more than 50 diverse biological experiments. The top participants developed a Transformer-based deep-learning algorithm that not only exceeds existing methods, but can also be applied to diverse microscopy images across imaging platforms and tissue types without manual parameter adjustments. This benchmark and the improved algorithm offer promising avenues for more accurate and versatile cell analysis in microscopy imaging.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#33609;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#20998;&#24067;&#24335;&#32447;&#24615;&#22238;&#24402;&#35745;&#31639;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#33609;&#22270;&#25216;&#26415;&#21644;&#25913;&#36827;&#24322;&#27493;&#31995;&#32479;&#20013;&#30340;&#22359;&#25928;&#24212;&#38887;&#24615;&#65292;&#23558;&#20449;&#24687;&#20445;&#25252;&#19982;&#22238;&#24402;&#38382;&#39064;&#32500;&#24230;&#30340;&#20943;&#23567;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#27491;&#20132;&#30697;&#38453;&#21644;&#23376;&#37319;&#26679;"&#22359;"&#65292;&#23454;&#29616;&#20102;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#32771;&#34385;&#26032;&#33609;&#22270;&#30340;&#20998;&#24067;&#24335;&#36845;&#20195;&#33609;&#22270;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#23545;&#23376;&#37319;&#26679;&#38543;&#26426;&#21704;&#36798;&#29595;&#21464;&#25442;&#36827;&#34892;&#20102;&#25512;&#24191;&#24182;&#20462;&#25913;&#20197;&#20445;&#35777;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04185</link><description>&lt;p&gt;
&#36845;&#20195;&#24335;&#33609;&#22270;&#29992;&#20110;&#23433;&#20840;&#32534;&#30721;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Iterative Sketching for Secure Coded Regression. (arXiv:2308.04185v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04185
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#33609;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#20998;&#24067;&#24335;&#32447;&#24615;&#22238;&#24402;&#35745;&#31639;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#33609;&#22270;&#25216;&#26415;&#21644;&#25913;&#36827;&#24322;&#27493;&#31995;&#32479;&#20013;&#30340;&#22359;&#25928;&#24212;&#38887;&#24615;&#65292;&#23558;&#20449;&#24687;&#20445;&#25252;&#19982;&#22238;&#24402;&#38382;&#39064;&#32500;&#24230;&#30340;&#20943;&#23567;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#27491;&#20132;&#30697;&#38453;&#21644;&#23376;&#37319;&#26679;"&#22359;"&#65292;&#23454;&#29616;&#20102;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#32771;&#34385;&#26032;&#33609;&#22270;&#30340;&#20998;&#24067;&#24335;&#36845;&#20195;&#33609;&#22270;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#23545;&#23376;&#37319;&#26679;&#38543;&#26426;&#21704;&#36798;&#29595;&#21464;&#25442;&#36827;&#34892;&#20102;&#25512;&#24191;&#24182;&#20462;&#25913;&#20197;&#20445;&#35777;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#32447;&#24615;&#22238;&#24402;&#20998;&#24067;&#24335;&#35745;&#31639;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#38543;&#26426;&#33609;&#22270;&#25216;&#26415;&#65292;&#24182;&#25913;&#21892;&#20102;&#24322;&#27493;&#31995;&#32479;&#20013;&#30340;&#22359;&#25928;&#24212;&#38887;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#20010;&#38543;&#26426;&#27491;&#20132;&#30697;&#38453;&#65292;&#28982;&#21518;&#23545;"&#22359;"&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#20197;&#21516;&#26102;&#20445;&#25252;&#20449;&#24687;&#24182;&#20943;&#23567;&#22238;&#24402;&#38382;&#39064;&#30340;&#32500;&#25968;&#12290;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#36825;&#20010;&#36716;&#25442;&#23545;&#24212;&#20110;"&#36817;&#20284;&#26799;&#24230;&#32534;&#30721;&#26041;&#26696;"&#20013;&#30340;&#32534;&#30721;&#21152;&#23494;&#65292;&#32780;&#23376;&#37319;&#26679;&#23545;&#24212;&#20110;&#38750;&#25955;&#20081;&#24037;&#20316;&#33410;&#28857;&#30340;&#21709;&#24212;&#65307;&#22312;&#19968;&#20010;&#38598;&#20013;&#24335;&#32534;&#30721;&#35745;&#31639;&#32593;&#32476;&#20013;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#30340;"&#36845;&#20195;&#33609;&#22270;"&#26041;&#27861;&#65292;&#29992;&#20110;$\ell_2$-&#23376;&#31354;&#38388;&#23884;&#20837;&#65292;&#21363;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#32771;&#34385;&#19968;&#20010;&#26032;&#30340;&#33609;&#22270;&#12290;&#25105;&#20204;&#36824;&#19987;&#27880;&#20110;"&#23376;&#37319;&#26679;&#38543;&#26426;&#21704;&#36798;&#29595;&#21464;&#25442;"&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#23558;&#20854;&#25512;&#24191;&#20026;&#22359;&#37319;&#26679;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#20462;&#25913;&#35813;&#26041;&#27861;&#20197;&#30830;&#20445;&#25968;&#25454;&#30340;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose methods for speeding up linear regression distributively, while ensuring security. We leverage randomized sketching techniques, and improve straggler resilience in asynchronous systems. Specifically, we apply a random orthonormal matrix and then subsample \textit{blocks}, to simultaneously secure the information and reduce the dimension of the regression problem. In our setup, the transformation corresponds to an encoded encryption in an \textit{approximate gradient coding scheme}, and the subsampling corresponds to the responses of the non-straggling workers; in a centralized coded computing network. This results in a distributive \textit{iterative sketching} approach for an $\ell_2$-subspace embedding, \textit{i.e.} a new sketch is considered at each iteration. We also focus on the special case of the \textit{Subsampled Randomized Hadamard Transform}, which we generalize to block sampling; and discuss how it can be modified in order to secure the data.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#21576;&#29616;&#20986;&#26032;&#20852;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#24320;&#21457;&#26356;&#21487;&#38752;&#21644;&#26080;&#20559;&#30340;&#35821;&#35328;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.00225</link><description>&lt;p&gt;
&#34987;&#25351;&#23548;&#30340;&#20559;&#35265;&#65306;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#21576;&#29616;&#20986;&#26032;&#20852;&#30340;&#35748;&#30693;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias. (arXiv:2308.00225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00225
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#21576;&#29616;&#20986;&#26032;&#20852;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#24320;&#21457;&#26356;&#21487;&#38752;&#21644;&#26080;&#20559;&#30340;&#35821;&#35328;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25351;&#23548;&#35843;&#20248;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#36825;&#20123;&#35843;&#20248;&#26041;&#27861;&#21487;&#20197;&#20351;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#20294;&#25105;&#20204;&#25512;&#27979;&#36825;&#20123;&#32463;&#36807;&#35843;&#20248;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#26356;&#22810;&#38544;&#21547;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#36825;&#20123;&#32463;&#36807;&#35843;&#20248;&#30340;&#27169;&#22411;&#21576;&#29616;&#20986;&#20808;&#21069;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#19981;&#23384;&#22312;&#25110;&#36739;&#19981;&#26126;&#26174;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#23545;&#19977;&#31181;&#35748;&#30693;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21253;&#25324;&#30683;&#30462;&#25928;&#24212;&#12289;&#30830;&#23450;&#24615;&#25928;&#24212;&#21644;&#20449;&#24565;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#24050;&#34987;&#35777;&#23454;&#23545;&#20154;&#31867;&#30340;&#20915;&#31574;&#21644;&#25512;&#29702;&#26377;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#36825;&#20123;&#20559;&#35265;&#22312;&#21508;&#31181;&#27169;&#22411;&#20013;&#30340;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#27169;&#22411;&#65292;&#22914;Flan-T5&#12289;GPT3.5&#21644;GPT4&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#29702;&#35299;&#25351;&#23548;&#35843;&#20248;&#30340;LMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#26377;&#21161;&#20110;&#24320;&#21457;&#26356;&#21487;&#38752;&#21644;&#26080;&#20559;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that instruction tuning and learning from human feedback improve the abilities of large language models (LMs) dramatically. While these tuning methods can make models generate high-quality text, we conjecture that more implicit cognitive biases may arise in these fine-tuned models. Our work provides evidence that these fine-tuned models exhibit biases that were absent or less pronounced in their pretrained predecessors. We examine the extent of this phenomenon in three cognitive biases - the decoy effect, the certainty effect, and the belief bias - all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models, especially those that have undergone instruction tuning, such as Flan-T5, GPT3.5, and GPT4. This research constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;&#30340;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#36870;&#21521;&#25104;&#20687;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#20445;&#30041;&#20808;&#21069;&#26799;&#24230;&#30340;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#38750;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.16120</link><description>&lt;p&gt;
&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#19982;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;&#29992;&#20110;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems. (arXiv:2307.16120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;&#30340;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#36870;&#21521;&#25104;&#20687;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#20445;&#30041;&#20808;&#21069;&#26799;&#24230;&#30340;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#38750;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#22522;&#20110;&#27169;&#22411;&#30340;&#36845;&#20195;&#31639;&#27861;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;(DuNets)&#24050;&#25104;&#20026;&#35299;&#20915;&#36870;&#21521;&#25104;&#20687;&#38382;&#39064;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#34429;&#28982;DuNets&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#65292;&#20294;&#38750;&#32447;&#24615;&#38382;&#39064;&#24448;&#24448;&#20250;&#24433;&#21709;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#21463;&#20248;&#21270;&#31639;&#27861;&#20013;&#24120;&#29992;&#30340;&#21160;&#37327;&#21152;&#36895;&#25216;&#26415;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;(RMA)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(LSTM-RNN)&#26469;&#27169;&#25311;&#21160;&#37327;&#21152;&#36895;&#36807;&#31243;&#12290;RMA&#27169;&#22359;&#21033;&#29992;LSTM-RNN&#23398;&#20064;&#21644;&#20445;&#30041;&#20808;&#21069;&#26799;&#24230;&#30340;&#30693;&#35782;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;RMA&#24212;&#29992;&#20110;&#20004;&#31181;&#27969;&#34892;&#30340;DuNets&#8212;&#8212;&#23398;&#20064;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;(LPGD)&#21644;&#23398;&#20064;&#30340;&#21407;&#22987;-&#23545;&#20598;(LPD)&#26041;&#27861;&#65292;&#20998;&#21035;&#24471;&#21040;LPGD-RMA&#21644;LPD-RMA&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#38750;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#19978;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#65306;&#38750;&#32447;&#24615;&#21435;&#21367;&#31215;&#38382;&#39064;&#12289;
&lt;/p&gt;
&lt;p&gt;
Combining the strengths of model-based iterative algorithms and data-driven deep learning solutions, deep unrolling networks (DuNets) have become a popular tool to solve inverse imaging problems. While DuNets have been successfully applied to many linear inverse problems, nonlinear problems tend to impair the performance of the method. Inspired by momentum acceleration techniques that are often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, an
&lt;/p&gt;</description></item><item><title>&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#37327;&#21270;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;AutoGNNUQ&#65292;&#36890;&#36807;&#26550;&#26500;&#25628;&#32034;&#29983;&#25104;&#39640;&#24615;&#33021;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#65292;&#24182;&#21033;&#29992;&#26041;&#24046;&#20998;&#35299;&#23558;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#24320;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.10438</link><description>&lt;p&gt;
&#29992;&#22270;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#36827;&#34892;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search. (arXiv:2307.10438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10438
&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#37327;&#21270;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;AutoGNNUQ&#65292;&#36890;&#36807;&#26550;&#26500;&#25628;&#32034;&#29983;&#25104;&#39640;&#24615;&#33021;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#65292;&#24182;&#21033;&#29992;&#26041;&#24046;&#20998;&#35299;&#23558;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#24320;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#31361;&#20986;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;GNN&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#26080;&#27861;&#37327;&#21270;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#21487;&#20449;&#22320;&#20351;&#29992;&#21644;&#37096;&#32626;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AutoGNNUQ&#65292;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;AutoGNNUQ&#21033;&#29992;&#26550;&#26500;&#25628;&#32034;&#29983;&#25104;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;GNN&#38598;&#21512;&#65292;&#33021;&#22815;&#20272;&#35745;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26041;&#24046;&#20998;&#35299;&#26469;&#20998;&#31163;&#25968;&#25454;&#65288;aleatoric&#65289;&#21644;&#27169;&#22411;&#65288;epistemic&#65289;&#19981;&#30830;&#23450;&#24615;&#65292;&#20026;&#20943;&#23569;&#23427;&#20204;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#22312;&#25105;&#20204;&#30340;&#35745;&#31639;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AutoGNNUQ&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;t-SNE&#21487;&#35270;&#21270;&#26469;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21644;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a prominent class of data-driven methods for molecular property prediction. However, a key limitation of typical GNN models is their inability to quantify uncertainties in the predictions. This capability is crucial for ensuring the trustworthy use and deployment of models in downstream tasks. To that end, we introduce AutoGNNUQ, an automated uncertainty quantification (UQ) approach for molecular property prediction. AutoGNNUQ leverages architecture search to generate an ensemble of high-performing GNNs, enabling the estimation of predictive uncertainties. Our approach employs variance decomposition to separate data (aleatoric) and model (epistemic) uncertainties, providing valuable insights for reducing them. In our computational experiments, we demonstrate that AutoGNNUQ outperforms existing UQ methods in terms of both prediction accuracy and UQ performance on multiple benchmark datasets. Additionally, we utilize t-SNE visualization to exp
&lt;/p&gt;</description></item><item><title>&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#20934;&#30830;&#24615;&#19982;&#26102;&#38388;&#21069;&#27839;&#36827;&#34892;&#27604;&#36739;&#65292;&#36890;&#36807;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#30740;&#31350;&#26469;&#22238;&#31572;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.08919</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#20934;&#30830;&#24615;&#19982;&#26102;&#38388;&#21069;&#27839;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images. (arXiv:2307.08919v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08919
&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#20934;&#30830;&#24615;&#19982;&#26102;&#38388;&#21069;&#27839;&#36827;&#34892;&#27604;&#36739;&#65292;&#36890;&#36807;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#30740;&#31350;&#26469;&#22238;&#31572;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24212;&#29992;&#26469;&#35828;&#65292;&#27599;&#20010;&#22270;&#20687;&#37117;&#24456;&#38590;&#25110;&#26114;&#36149;&#22320;&#33719;&#24471;&#19968;&#20010;&#21487;&#20449;&#30340;&#26631;&#31614;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#27809;&#26377;&#26631;&#31614;&#30340;&#22270;&#20687;&#26356;&#23481;&#26131;&#33719;&#21462;&#12290;&#20004;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#37117;&#25215;&#35834;&#39069;&#22806;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#20165;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#26377;&#29992;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#26631;&#35760;&#38598;&#23545;&#36825;&#20123;&#34920;&#31034;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#20998;&#31867;&#22120;&#65307;&#21322;&#30417;&#30563;&#23398;&#20064;&#21516;&#26102;&#22312;&#26631;&#35760;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#30452;&#25509;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20174;&#20004;&#20010;&#26041;&#21521;&#19978;&#37117;&#22768;&#31216;&#22312;&#38750;&#21307;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#20294;&#27809;&#26377;&#31995;&#32479;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#65292;&#24182;&#19988;&#22823;&#22810;&#21482;&#19982;&#21516;&#19968;&#26041;&#21521;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#26469;&#22238;&#31572;&#20174;&#19994;&#32773;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22312;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#26377;&#38480;&#30340;&#22521;&#35757;&#26102;&#38388;&#39044;&#31639;&#19979;&#65292;&#39069;&#22806;&#30340;&#26080;&#26631;&#31614;&#22270;&#20687;&#33021;&#22815;&#20135;&#29983;&#22810;&#22823;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many applications of classifiers to medical images, a trustworthy label for each image can be difficult or expensive to obtain. In contrast, images without labels are more readily available. Two major research directions both promise that additional unlabeled data can improve classifier performance: self-supervised learning pretrains useful representations on unlabeled data only, then fine-tunes a classifier on these representations via the labeled set; semi-supervised learning directly trains a classifier on labeled and unlabeled data simultaneously. Recent methods from both directions have claimed significant gains on non-medical tasks, but do not systematically assess medical images and mostly compare only to methods in the same direction. This study contributes a carefully-designed benchmark to help answer a practitioner's key question: given a small labeled dataset and a limited budget of hours to spend on training, what gains from additional unlabeled images are possible and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;$k$-WL&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#20934;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22806;&#24310;&#23450;&#20041;&#21644;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.05775</link><description>&lt;p&gt;
Weisfeiler&#21644;Lehman&#36827;&#34892;&#24230;&#37327;&#24314;&#27169;&#65306;&#25506;&#32034;WL&#27979;&#35797;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Weisfeiler and Lehman Go Measurement Modeling: Probing the Validity of the WL Test. (arXiv:2307.05775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;$k$-WL&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#20934;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22806;&#24310;&#23450;&#20041;&#21644;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#27604;&#36739;&#19968;&#20010;&#26550;&#26500;&#33021;&#22815;&#21306;&#20998;&#30340;&#38750;&#21516;&#26500;&#22270;&#25110;&#33410;&#28857;&#23545;&#30340;&#25968;&#37327;&#19982;$k$-&#32500;Weisfeiler-Lehman ($k$-WL)&#27979;&#35797;&#33021;&#22815;&#21306;&#20998;&#30340;&#25968;&#37327;&#26469;&#34913;&#37327;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;$k$-WL&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#20174;&#19994;&#32773;&#23545;&#34920;&#36798;&#33021;&#21147;&#21644;$k$-WL&#30340;&#27010;&#24565;&#21270;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36827;&#34892;&#20102;&#23545;&#20174;&#19994;&#32773;&#30340;&#35843;&#26597;&#65288;n=18&#65289;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#23545;&#34920;&#36798;&#33021;&#21147;&#30340;&#27010;&#24565;&#20197;&#21450;&#23545;$k$-WL&#30340;&#20551;&#35774;&#12290;&#19982;&#20174;&#19994;&#32773;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#65288;&#20511;&#37492;&#20102;&#22270;&#35770;&#21644;&#22522;&#20934;&#23457;&#26680;&#65289;&#25581;&#31034;&#20102;$k$-WL&#19981;&#33021;&#20445;&#35777;&#31561;&#36317;&#12289;&#21487;&#33021;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20219;&#21153;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#33021;&#19981;&#20419;&#36827;&#27867;&#21270;&#25110;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#20027;&#24352;&#22522;&#20110;&#22522;&#20934;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22806;&#24310;&#23450;&#20041;&#21644;&#27979;&#37327;&#65292;&#36827;&#19968;&#27493;&#36129;&#29486;&#20102;&#26500;&#24314;&#27492;&#31867;&#22522;&#20934;&#30340;&#25351;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expressive power of graph neural networks is usually measured by comparing how many pairs of graphs or nodes an architecture can possibly distinguish as non-isomorphic to those distinguishable by the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test. In this paper, we uncover misalignments between practitioners' conceptualizations of expressive power and $k$-WL through a systematic analysis of the reliability and validity of $k$-WL. We further conduct a survey ($n = 18$) of practitioners to surface their conceptualizations of expressive power and their assumptions about $k$-WL. In contrast to practitioners' opinions, our analysis (which draws from graph theory and benchmark auditing) reveals that $k$-WL does not guarantee isometry, can be irrelevant to real-world graph tasks, and may not promote generalization or trustworthiness. We argue for extensional definitions and measurement of expressive power based on benchmarks; we further contribute guiding questions for constructing such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#36890;&#36807;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#21644;&#33410;&#28857;&#27744;&#31574;&#30053;&#35299;&#20915;&#20102;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00185</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#24212;&#29992;&#30340;&#26500;&#36896;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Constructive Algorithm for Incremental Random Weight Neural Networks and Its Application. (arXiv:2307.00185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#36890;&#36807;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#21644;&#33410;&#28857;&#27744;&#31574;&#30053;&#35299;&#20915;&#20102;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;(IRWNNs)&#30001;&#20110;&#26131;&#20110;&#23454;&#29616;&#21644;&#24555;&#36895;&#23398;&#20064;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;IRWNNs&#30340;&#19968;&#20010;&#26174;&#33879;&#32570;&#28857;&#26159;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#65288;&#33410;&#28857;&#65289;&#19982;&#27531;&#24046;&#35823;&#24046;&#65288;&#27169;&#22411;&#24615;&#33021;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#30340;&#21487;&#35299;&#37322;&#30340;&#26500;&#36896;&#31639;&#27861;(ICA)&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#26469;&#38543;&#26426;&#20998;&#37197;&#38544;&#34255;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#33410;&#28857;&#27744;&#31574;&#30053;&#33719;&#21462;&#26356;&#26377;&#21033;&#20110;&#25910;&#25947;&#30340;&#38544;&#34255;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#35777;&#26126;&#20102;ICA&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;ICA&#30340;&#36731;&#37327;&#32423;&#29256;&#26412;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#12290;&#22312;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental random weight neural networks (IRWNNs) have gained attention in view of its easy implementation and fast learning. However, a significant drawback of IRWNNs is that the elationship between the hidden parameters (node)and the residual error (model performance) is difficult to be interpreted. To address the above issue, this article proposes an interpretable constructive algorithm (ICA) with geometric information constraint. First, based on the geometric relationship between the hidden parameters and the residual error, an interpretable geometric information constraint is proposed to randomly assign the hidden parameters. Meanwhile, a node pool strategy is employed to obtain hidden parameters that is more conducive to convergence from hidden parameters satisfying the proposed constraint. Furthermore, the universal approximation property of the ICA is proved. Finally, a lightweight version of ICA is presented for large-scale data modeling tasks. Experimental results on six ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.13549</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22823;&#33041;&#25191;&#34892;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;MLLM &#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#22914;&#22522;&#20110;&#22270;&#20687;&#32534;&#20889;&#25925;&#20107;&#21644;&#26080;OCR&#25968;&#23398;&#25512;&#29702;&#31561;&#65292;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#24456;&#23569;&#35265;&#65292;&#34920;&#26126;&#20102;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;&#26412;&#25991;&#26088;&#22312;&#36861;&#36394;&#21644;&#24635;&#32467; MLLM &#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MLLM &#30340;&#26500;&#25104;&#65292;&#27010;&#36848;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#65288;M-IT&#65289;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;M-ICL&#65289;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#65288;M-CoT&#65289;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#65288;LAVR&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#37492;&#20110; MLLM &#26102;&#20195;&#25165;&#21018;&#21018;&#24320;&#22987;&#65292;&#25105;&#20204;&#20250;&#19981;&#26029;&#26356;&#26032;&#36825;&#20010;&#32508;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#26356;&#22810;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#22522;&#20110;&#23376;&#22270;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;(SGNCL)&#65292;&#36890;&#36807;&#24212;&#29992;&#23376;&#22270;&#32593;&#32476;&#29983;&#25104;&#31574;&#30053;&#20197;&#20135;&#29983;&#22686;&#24378;&#35270;&#22270;&#65292;&#24182;&#25506;&#31350;&#20102;&#23376;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#23545;&#22270;&#24418;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.03506</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#22270;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Subgraph Networks Based Contrastive Learning. (arXiv:2306.03506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#22522;&#20110;&#23376;&#22270;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;(SGNCL)&#65292;&#36890;&#36807;&#24212;&#29992;&#23376;&#22270;&#32593;&#32476;&#29983;&#25104;&#31574;&#30053;&#20197;&#20135;&#29983;&#22686;&#24378;&#35270;&#22270;&#65292;&#24182;&#25506;&#31350;&#20102;&#23376;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#23545;&#22270;&#24418;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;(GCL)&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#35299;&#20915;&#27880;&#37322;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290; &#23427;&#22312;&#26410;&#27880;&#37322;&#30340;&#22270;&#24418;&#20013;&#25366;&#25496;&#26174;&#24335;&#29305;&#24449;&#20197;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#21033;&#22270;&#24418;&#34920;&#31034;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GCL&#26041;&#27861;&#20391;&#37325;&#20110;&#22270;&#24418;&#22686;&#24378;&#31574;&#30053;&#21644;&#30456;&#20114;&#20449;&#24687;&#20272;&#35745;&#25805;&#20316;&#30340;&#35774;&#35745;&#12290; &#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#23376;&#22270;&#20013;&#23384;&#22312;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#25506;&#32034;&#23376;&#32467;&#26500;&#30456;&#20114;&#20316;&#29992;&#23545;&#22270;&#24418;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;subgraph network-based contrastive learning (SGNCL)&#30340;&#26032;&#26694;&#26550;&#12290;SGNCL&#24212;&#29992;&#23376;&#22270;&#32593;&#32476;&#29983;&#25104;&#31574;&#30053;&#20197;&#20135;&#29983;&#22686;&#24378;&#35270;&#22270;&#12290;&#35813;&#31574;&#30053;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#20026;&#20855;&#26377;&#25299;&#25169;&#21644;&#23646;&#24615;&#29305;&#24449;&#30340;&#36793;&#21040;&#33410;&#28857;&#26144;&#23556;&#32593;&#32476;&#12290;&#21333;&#27425;&#22686;&#24378;&#35270;&#22270;&#26159;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning (GCL), as a self-supervised learning method, can solve the problem of annotated data scarcity. It mines explicit features in unannotated graphs to generate favorable graph representations for downstream tasks. Most existing GCL methods focus on the design of graph augmentation strategies and mutual information estimation operations. Graph augmentation produces augmented views by graph perturbations. These views preserve a locally similar structure and exploit explicit features. However, these methods have not considered the interaction existing in subgraphs. To explore the impact of substructure interactions on graph representations, we propose a novel framework called subgraph network-based contrastive learning (SGNCL). SGNCL applies a subgraph network generation strategy to produce augmented views. This strategy converts the original graph into an Edge-to-Node mapping network with both topological and attribute features. The single-shot augmented view is a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#27010;&#29575;&#36317;&#31163;&#21644;&#20004;&#31181;&#20248;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#36807;&#37325;&#21644;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.18171</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improved Probabilistic Image-Text Representations. (arXiv:2305.18171v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#27010;&#29575;&#36317;&#31163;&#21644;&#20004;&#31181;&#20248;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#36807;&#37325;&#21644;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#65292;&#30001;&#20110;&#22810;&#26679;&#24615;&#21644;&#19981;&#23436;&#32654;&#27880;&#37322;&#23548;&#33268;&#30340;&#22266;&#26377;&#27495;&#20041;&#20351;&#20854;&#21463;&#21040;&#22256;&#25200;&#12290;&#30830;&#23450;&#24615;&#20989;&#25968;&#26080;&#27861;&#36275;&#22815;&#24378;&#22823;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#27495;&#20041;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;&#27010;&#29575;&#23884;&#20837;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#32570;&#28857;&#65306;&#33945;&#29305;&#21345;&#27931;&#36924;&#36817;&#23548;&#33268;&#35745;&#31639;&#36127;&#25285;&#36739;&#37325;&#65292;&#19988;&#22312;&#22823;&#37327;&#35823;&#26816;&#24773;&#20917;&#19979;&#23481;&#26131;&#20986;&#29616;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27010;&#29575;&#36328;&#27169;&#24577;&#23884;&#20837;&#26041;&#27861;&#65288;PCME++&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#38381;&#21512;&#24418;&#24335;&#35299;&#30340;&#26032;&#30340;&#27010;&#29575;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#25216;&#26415;&#36827;&#19968;&#27493;&#22686;&#24378;PCME++&#65306;&#39318;&#20808;&#65292;&#24341;&#20837;&#20266;&#27491;&#26679;&#26412;&#20197;&#38450;&#27490;&#22823;&#37327;&#35823;&#26816;&#24773;&#20917;&#19979;&#30340;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#65307;&#20854;&#27425;&#65292;&#37319;&#29992;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#27010;&#29575;&#21305;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PCME++&#22312;ITM&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-Text Matching (ITM) task, a fundamental vision-language (VL) task, suffers from the inherent ambiguity arising from multiplicity and imperfect annotations. Deterministic functions are not sufficiently powerful to capture ambiguity, prompting the exploration of probabilistic embeddings to tackle the challenge. However, the existing probabilistic ITM approach encounters two key shortcomings; the burden of heavy computations due to the Monte Carlo approximation, and the loss saturation issue in the face of abundant false negatives. To overcome the issues, this paper presents an improved Probabilistic Cross-Modal Embeddings (named PCME++) by introducing a new probabilistic distance with a closed-form solution. In addition, two optimization techniques are proposed to enhance PCME++ further; first, the incorporation of pseudo-positives to prevent the loss saturation problem under massive false negatives; second, mixed sample data augmentation for probabilistic matching. Experimental re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#39532;&#23572;&#31185;&#22827;&#22122;&#22768;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#20984;&#21644;&#24378;&#20984;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#19968;&#38454;&#26799;&#24230;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#22810;&#23618;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30340;&#38543;&#26426;&#25209;&#22788;&#29702;&#26041;&#26696;&#20197;&#33719;&#24471;&#26368;&#20248;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#28040;&#38500;&#20102;&#20197;&#21069;&#30740;&#31350;&#20013;&#30340;&#38480;&#21046;&#26465;&#20214;&#12290;&#22312;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#19979;&#23545;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#25193;&#23637;&#26159;&#21407;&#21019;&#24615;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.15938</link><description>&lt;p&gt;
&#20855;&#26377;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#19968;&#38454;&#26041;&#27861;&#65306;&#20174;&#21152;&#36895;&#21040;&#21464;&#20998;&#19981;&#31561;&#24335;
&lt;/p&gt;
&lt;p&gt;
First Order Methods with Markovian Noise: from Acceleration to Variational Inequalities. (arXiv:2305.15938v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#39532;&#23572;&#31185;&#22827;&#22122;&#22768;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#20984;&#21644;&#24378;&#20984;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#19968;&#38454;&#26799;&#24230;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#22810;&#23618;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30340;&#38543;&#26426;&#25209;&#22788;&#29702;&#26041;&#26696;&#20197;&#33719;&#24471;&#26368;&#20248;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#28040;&#38500;&#20102;&#20197;&#21069;&#30740;&#31350;&#20013;&#30340;&#38480;&#21046;&#26465;&#20214;&#12290;&#22312;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#19979;&#23545;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#25193;&#23637;&#26159;&#21407;&#21019;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#28041;&#21450;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#29702;&#35770;&#20998;&#26512;&#19968;&#38454;&#26799;&#24230;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#20248;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28085;&#30422;&#20102;&#38750;&#20984;&#21644;&#24378;&#20984;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#23454;&#29616;&#19968;&#20010;&#20381;&#36182;&#20110;&#24213;&#23618;&#22122;&#22768;&#24207;&#21015;&#28151;&#21512;&#26102;&#38388;&#30340;&#26368;&#20248;(&#32447;&#24615;)&#20851;&#31995;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22810;&#23618;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30340;&#38543;&#26426;&#25209;&#22788;&#29702;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#20801;&#35768;&#25105;&#20204;&#28040;&#38500;&#20197;&#21069;&#20851;&#20110;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#30740;&#31350;&#20013;&#30340;&#38480;&#21046;&#26465;&#20214;&#65292;&#20363;&#22914;&#38656;&#35201;&#26377;&#30028;&#22495;&#21644;&#22343;&#21248;&#26377;&#30028;&#38543;&#26426;&#26799;&#24230;&#12290;&#25105;&#20204;&#22312;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#19979;&#23545;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#25193;&#23637;&#26159;&#21407;&#21019;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21305;&#37197;&#24378;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#29702;&#35770;&#26368;&#20248;&#35299;&#30340;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into stochastic optimization problems that involve Markovian noise. We present a unified approach for the theoretical analysis of first-order gradient methods for stochastic optimization and variational inequalities. Our approach covers scenarios for both non-convex and strongly convex minimization problems. To achieve an optimal (linear) dependence on the mixing time of the underlying noise sequence, we use the randomized batching scheme, which is based on the multilevel Monte Carlo method. Moreover, our technique allows us to eliminate the limiting assumptions of previous research on Markov noise, such as the need for a bounded domain and uniformly bounded stochastic gradients. Our extension to variational inequalities under Markovian noise is original. Additionally, we provide lower bounds that match the oracle complexity of our method in the case of strongly convex optimization problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27714;&#35299;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#35299;&#20915;&#25193;&#25955;ODE&#38382;&#39064;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#31283;&#23450;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#20013;&#37319;&#26679;&#39640;&#36136;&#37327;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2305.15357</link><description>&lt;p&gt;
&#36890;&#36807;&#27714;&#35299;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#35299;&#20915;&#25193;&#25955;ODE&#38382;&#39064;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution. (arXiv:2305.15357v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27714;&#35299;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#35299;&#20915;&#25193;&#25955;ODE&#38382;&#39064;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#31283;&#23450;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#20013;&#37319;&#26679;&#39640;&#36136;&#37327;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25193;&#25955;&#27169;&#22411;&#21453;&#21521;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#38543;&#26426;&#24615;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#22312;&#27599;&#27425;&#37319;&#26679;&#26102;&#24615;&#33021;&#27874;&#21160;&#24456;&#22823;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#23569;&#37327;&#37325;&#26032;&#37319;&#26679;&#27493;&#39588;&#30340;&#37319;&#26679;&#22120;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#36825;&#31181;&#22266;&#26377;&#38543;&#26426;&#24615;&#23548;&#33268;&#20854;&#26080;&#25928;&#21644;&#19981;&#31283;&#23450;&#65292;&#20351;&#29992;&#25143;&#38590;&#20197;&#20445;&#35777;&#36229;&#20998;&#36776;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36825;&#31181;&#38543;&#26426;&#24615;&#35270;&#20026;&#19968;&#31181;&#26426;&#36935;&#65306;&#20840;&#38754;&#20998;&#26512;&#21644;&#21033;&#29992;&#23427;&#23548;&#33268;&#20102;&#26500;&#24314;&#19968;&#31181;&#26377;&#25928;&#30340;&#21363;&#25554;&#21363;&#29992;&#37319;&#26679;&#26041;&#27861;&#65292;&#20855;&#26377;&#28508;&#21147;&#20351;&#19968;&#31995;&#21015;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#21463;&#30410;&#12290;&#26356;&#35814;&#32454;&#22320;&#35828;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#27714;&#35299;&#25193;&#25955;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;&#25193;&#25955;ODE&#65289;&#21644;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#65288;BC&#65289;&#65292;&#31283;&#23450;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#20013;&#37319;&#26679;&#39640;&#36136;&#37327;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#24182;&#20998;&#26512;&#20854;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models, as a kind of powerful generative model, have given impressive results on image super-resolution (SR) tasks. However, due to the randomness introduced in the reverse process of diffusion models, the performances of diffusion-based SR models are fluctuating at every time of sampling, especially for samplers with few resampled steps. This inherent randomness of diffusion models results in ineffectiveness and instability, making it challenging for users to guarantee the quality of SR results. However, our work takes this randomness as an opportunity: fully analyzing and leveraging it leads to the construction of an effective plug-and-play sampling method that owns the potential to benefit a series of diffusion-based SR methods. More in detail, we propose to steadily sample high-quality SR images from pretrained diffusion-based SR models by solving diffusion ordinary differential equations (diffusion ODEs) with optimal boundary conditions (BCs) and analyze the characterist
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#29366;&#24577;RNA&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#32771;&#34385;&#21644;&#21453;&#26144;RNA&#26500;&#35937;&#22810;&#26679;&#24615;&#22312;&#20854;&#35774;&#35745;&#20013;&#12290;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21407;&#29983;&#24207;&#21015;&#30340;&#24674;&#22797;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#29366;&#24577;&#21644;&#32467;&#26500;&#22810;&#26679;&#21270;&#30340;RNA&#12290;</title><link>http://arxiv.org/abs/2305.14749</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#29366;&#24577;RNA&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-State RNA Design with Geometric Multi-Graph Neural Networks. (arXiv:2305.14749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#29366;&#24577;RNA&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#32771;&#34385;&#21644;&#21453;&#26144;RNA&#26500;&#35937;&#22810;&#26679;&#24615;&#22312;&#20854;&#35774;&#35745;&#20013;&#12290;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21407;&#29983;&#24207;&#21015;&#30340;&#24674;&#22797;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#29366;&#24577;&#21644;&#32467;&#26500;&#22810;&#26679;&#21270;&#30340;RNA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;RNA&#35774;&#35745;&#22312;&#21512;&#25104;&#29983;&#29289;&#23398;&#21644;&#27835;&#30103;&#24320;&#21457;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;RNA&#22810;&#26679;&#30340;&#29983;&#29289;&#23398;&#21151;&#33021;&#30340;&#22522;&#30784;&#26159;&#23427;&#30340;&#26500;&#35937;&#28789;&#27963;&#24615;&#65292;&#20351;&#21333;&#19968;&#24207;&#21015;&#33021;&#22815;&#37319;&#29992;&#22810;&#31181;&#19981;&#21516;&#30340;&#19977;&#32500;&#32467;&#26500;&#29366;&#24577;&#12290;&#30446;&#21069;&#65292;&#35745;&#31639;&#29983;&#29289;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#32463;&#24120;&#34987;&#25552;&#20986;&#20026;&#36870;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#37319;&#29992;&#21333;&#19968;&#39044;&#26399;&#32467;&#26500;&#26500;&#35937;&#26469;&#35774;&#35745;&#24207;&#21015;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;gRNAde&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#19968;&#32452;&#19977;&#32500;RNA&#39592;&#26550;&#32467;&#26500;&#25805;&#20316;&#30340;&#20960;&#20309;RNA&#35774;&#35745;&#27969;&#31243;&#65292;&#20197;&#26126;&#30830;&#32771;&#34385;&#21644;&#21453;&#26144;RNA&#26500;&#35937;&#22810;&#26679;&#24615;&#22312;&#20854;&#35774;&#35745;&#20013;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#19977;&#32500;RNA&#35774;&#35745;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;gRNAde&#30340;&#25928;&#29992;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#29366;&#24577;&#21644;&#32467;&#26500;&#22810;&#26679;&#21270;&#30340;RNA&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21407;&#29983;&#24207;&#21015;&#30340;&#24674;&#22797;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/chaitjo/geometric-rna-design&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational RNA design has broad applications across synthetic biology and therapeutic development. Fundamental to the diverse biological functions of RNA is its conformational flexibility, enabling single sequences to adopt a variety of distinct 3D states. Currently, computational biomolecule design tasks are often posed as inverse problems, where sequences are designed based on adopting a single desired structural conformation. In this work, we propose gRNAde, a geometric RNA design pipeline that operates on sets of 3D RNA backbone structures to explicitly account for and reflect RNA conformational diversity in its designs. We demonstrate the utility of gRNAde for improving native sequence recovery over single-state approaches on a new large-scale 3D RNA design dataset, especially for multi-state and structurally diverse RNAs. Our code is available at https://github.com/chaitjo/geometric-rna-design
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27491;&#21017;&#21270;&#38160;&#24230;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#20248;&#21270;&#21644;&#27867;&#21270;&#30446;&#26631;&#26469;&#39640;&#25928;&#22320;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11584</link><description>&lt;p&gt;
&#21160;&#24577;&#27491;&#21017;&#21270;&#38160;&#24230;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20840;&#23616;&#19968;&#33268;&#24615;&#21644;&#24179;&#28369;&#22330;&#26223;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic Regularized Sharpness Aware Minimization in Federated Learning: Approaching Global Consistency and Smooth Landscape. (arXiv:2305.11584v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27491;&#21017;&#21270;&#38160;&#24230;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#20248;&#21270;&#21644;&#27867;&#21270;&#30446;&#26631;&#26469;&#39640;&#25928;&#22320;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#19968;&#32452;&#26412;&#22320;&#23458;&#25143;&#31471;&#22312;&#20840;&#23616;&#26381;&#21153;&#22120;&#30340;&#21327;&#35843;&#19979;&#21327;&#20316;&#35757;&#32451;&#24102;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#21644;&#38548;&#31163;&#30340;&#38750; iid &#25968;&#25454;&#38598;&#65292;&#23458;&#25143;&#31471;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#21040;&#33258;&#24049;&#30340;&#33258;&#36523;&#26368;&#20248;&#35299;&#65292;&#36825;&#26497;&#22823;&#22320;&#20559;&#31163;&#20102;&#20840;&#23616;&#30446;&#26631;&#24182;&#20005;&#37325;&#21066;&#24369;&#20102;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#31639;&#27861; FedSMOO&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#20248;&#21270;&#21644;&#27867;&#21270;&#30446;&#26631;&#26469;&#39640;&#25928;&#22320;&#25552;&#39640; FL &#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning (FL), a cluster of local clients are chaired under the coordination of the global server and cooperatively train one model with privacy protection. Due to the multiple local updates and the isolated non-iid dataset, clients are prone to overfit into their own optima, which extremely deviates from the global objective and significantly undermines the performance. Most previous works only focus on enhancing the consistency between the local and global objectives to alleviate this prejudicial client drifts from the perspective of the optimization view, whose performance would be prominently deteriorated on the high heterogeneity. In this work, we propose a novel and general algorithm {\ttfamily FedSMOO} by jointly considering the optimization and generalization targets to efficiently improve the performance in FL. Concretely, {\ttfamily FedSMOO} adopts a dynamic regularizer to guarantee the local optima towards the global objective, which is meanwhile revised by the 
&lt;/p&gt;</description></item><item><title>SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.09781</link><description>&lt;p&gt;
SpecInfer&#65306;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09781
&lt;/p&gt;
&lt;p&gt;
SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#22240;&#27492;&#24555;&#36895;&#21644;&#24265;&#20215;&#22320;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;SpecInfer&#65292;&#19968;&#20010;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;LLM&#25512;&#26029;&#12290;SpecInfer&#32972;&#21518;&#30340;&#20851;&#38190;&#26159;&#23558;&#21508;&#31181;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#20307;&#25552;&#21319;&#35843;&#25972;&#65292;&#20849;&#21516;&#39044;&#27979;LLM&#30340;&#36755;&#20986;&#65307; &#39044;&#27979;&#32467;&#26524;&#32452;&#32455;&#25104;&#19968;&#20010;&#20196;&#29260;&#26641;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#37117;&#34920;&#31034;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26641;&#30340;&#24182;&#34892;&#35299;&#30721;&#26426;&#21046;&#65292;&#20197;LMM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#26469;&#39564;&#35777;&#20196;&#29260;&#26641;&#25152;&#20195;&#34920;&#30340;&#25152;&#26377;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#30340;&#27491;&#30830;&#24615;&#12290;SpecInfer&#20351;&#29992;LLM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#65292;&#32780;&#19981;&#26159;&#22686;&#37327;&#35299;&#30721;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#29983;&#25104;&#24335;LLM&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#21487;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#24191;&#20102; Hopkins field &#20998;&#23618;&#32593;&#32476;&#65292;&#24182;&#20171;&#32461;&#20102; iMixer&#65292;MLP-Mixer &#27169;&#22411;&#30340;&#26032;&#27010;&#25324;&#65292;&#19981;&#21516;&#20110;&#26222;&#36890;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;iMixer &#28041;&#21450;&#21040;&#20174;&#36755;&#20986;&#21040;&#36755;&#20837;&#30340;&#20256;&#25773;&#30340; MLP &#23618;&#65292;&#34987;&#29305;&#24449;&#21270;&#20026;&#19968;&#20010;&#21487;&#36870;&#12289;&#38544;&#24335;&#12289;&#36845;&#20195;&#30340; mixing block&#12290;</title><link>http://arxiv.org/abs/2304.13061</link><description>&lt;p&gt;
iMixer: &#20998;&#23618;Hopfield&#32593;&#32476;&#26263;&#31034;&#20102;&#21487;&#36870;&#12289;&#38544;&#24335;&#21644;&#36845;&#20195;&#30340;MLP-Mixer
&lt;/p&gt;
&lt;p&gt;
iMixer: hierarchical Hopfield network implies an invertible, implicit and iterative MLP-Mixer. (arXiv:2304.13061v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#24191;&#20102; Hopkins field &#20998;&#23618;&#32593;&#32476;&#65292;&#24182;&#20171;&#32461;&#20102; iMixer&#65292;MLP-Mixer &#27169;&#22411;&#30340;&#26032;&#27010;&#25324;&#65292;&#19981;&#21516;&#20110;&#26222;&#36890;&#30340;&#21069;&#39304;&#32593;&#32476;&#65292;iMixer &#28041;&#21450;&#21040;&#20174;&#36755;&#20986;&#21040;&#36755;&#20837;&#30340;&#20256;&#25773;&#30340; MLP &#23618;&#65292;&#34987;&#29305;&#24449;&#21270;&#20026;&#19968;&#20010;&#21487;&#36870;&#12289;&#38544;&#24335;&#12289;&#36845;&#20195;&#30340; mixing block&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#25104;&#21151;&#20419;&#20351;&#23547;&#25214;&#21487;&#20197;&#19982;&#20043;&#31454;&#20105;&#30340;&#35768;&#22810;&#26367;&#20195;&#27169;&#22411;&#65292;&#22914;MLP-Mixer&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#30340;&#24341;&#20837;&#20559;&#24046;&#36739;&#24369;&#65292;&#20294;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#19982;&#30740;&#31350;&#36739;&#22810;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#23218;&#32654;&#12290;&#26368;&#36817;&#23545;&#29616;&#20195;Hopfield&#32593;&#32476;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102;&#26576;&#20123;&#22522;&#20110;&#33021;&#37327;&#30340;&#20851;&#32852;&#35760;&#24518;&#27169;&#22411;&#19982;Transformer&#25110;MLP-Mixer&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25581;&#31034;&#20102;Transformer&#31867;&#22411;&#26550;&#26500;&#35774;&#35745;&#30340;&#29702;&#35770;&#32972;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#23545;&#24212;&#20851;&#31995;&#25512;&#24191;&#21040;&#26368;&#36817;&#24341;&#20837;&#30340;&#20998;&#23618;Hopfield&#32593;&#32476;&#65292;&#24182;&#25214;&#21040;&#20102;iMixer&#65292;&#36825;&#26159;MLP-Mixer&#27169;&#22411;&#30340;&#26032;&#30340;&#27010;&#25324;&#12290;&#19982;&#26222;&#36890;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65292;iMixer&#28041;&#21450;&#20174;&#36755;&#20986;&#20391;&#21521;&#36755;&#20837;&#20391;&#20256;&#25773;&#30340;MLP&#23618;&#12290;&#25105;&#20204;&#23558;&#35813;&#27169;&#22359;&#29305;&#24449;&#21270;&#20026;&#21487;&#36870;&#12289;&#38544;&#24335;&#21644;&#36845;&#20195;&#28151;&#21512;&#27169;&#22359;&#30340;&#19968;&#20010;&#20363;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#20219;&#21153;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last few years, the success of Transformers in computer vision has stimulated the discovery of many alternative models that compete with Transformers, such as the MLP-Mixer. Despite their weak induced bias, these models have achieved performance comparable to well-studied convolutional neural networks. Recent studies on modern Hopfield networks suggest the correspondence between certain energy-based associative memory models and Transformers or MLP-Mixer, and shed some light on the theoretical background of the Transformer-type architectures design. In this paper we generalize the correspondence to the recently introduced hierarchical Hopfield network, and find iMixer, a novel generalization of MLP-Mixer model. Unlike ordinary feedforward neural networks, iMixer involves MLP layers that propagate forward from the output side to the input side. We characterize the module as an example of invertible, implicit, and iterative mixing module. We evaluate the model performance with var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MAPSeg &#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992; 3D &#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#36328;&#24180;&#40836;&#12289;&#36328;&#27169;&#24577;&#21644;&#36328;&#22330;&#26223;&#19979;&#23545;&#23156;&#20799;&#33041; MRI &#20013;&#20122;&#30382;&#36136;&#21306;&#22495;&#30340;&#20998;&#21106;&#65292;&#20805;&#20998;&#32771;&#34385;&#19981;&#21516; MRI &#25195;&#25551;&#20202;&#12289;&#20379;&#24212;&#21830;&#25110;&#37319;&#38598;&#24207;&#21015;&#20197;&#21450;&#19981;&#21516;&#30340;&#31070;&#32463;&#21457;&#32946;&#38454;&#27573;&#25152;&#36896;&#25104;&#30340;&#20869;&#22312;&#24322;&#36136;&#24615;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09373</link><description>&lt;p&gt;
3D&#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#20266;&#26631;&#31614;&#29992;&#20110;&#24322;&#26500;&#23156;&#20799;&#33041; MRI &#39046;&#22495;&#38388;&#36866;&#24212;&#24615;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive Segmentation of Heterogeneous Infant Brain MRI. (arXiv:2303.09373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MAPSeg &#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992; 3D &#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#36328;&#24180;&#40836;&#12289;&#36328;&#27169;&#24577;&#21644;&#36328;&#22330;&#26223;&#19979;&#23545;&#23156;&#20799;&#33041; MRI &#20013;&#20122;&#30382;&#36136;&#21306;&#22495;&#30340;&#20998;&#21106;&#65292;&#20805;&#20998;&#32771;&#34385;&#19981;&#21516; MRI &#25195;&#25551;&#20202;&#12289;&#20379;&#24212;&#21830;&#25110;&#37319;&#38598;&#24207;&#21015;&#20197;&#21450;&#19981;&#21516;&#30340;&#31070;&#32463;&#21457;&#32946;&#38454;&#27573;&#25152;&#36896;&#25104;&#30340;&#20869;&#22312;&#24322;&#36136;&#24615;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23156;&#20799;&#33041; MRI &#22312;&#36328;&#24180;&#40836;&#12289;&#36328;&#27169;&#24577;&#12289;&#36328;&#22330;&#26223;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#20998;&#21106;&#20173;&#28982;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; MAPSeg &#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992; 3D &#33945;&#29256;&#33258;&#32534;&#30721;&#21644;&#33945;&#29256;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#26469;&#23545;&#23156;&#20799;&#33041;MRI&#30340;&#19981;&#21516;&#20122;&#30382;&#36136;&#21306;&#22495;&#36827;&#34892;&#20998;&#21106;&#65292;&#24182;&#32852;&#21512;&#23398;&#20064;&#26631;&#35760;&#28304;&#22495;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30446;&#26631;&#22495;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#20998;&#21106;&#32467;&#26524;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust segmentation of infant brain MRI across multiple ages, modalities, and sites remains challenging due to the intrinsic heterogeneity caused by different MRI scanners, vendors, or acquisition sequences, as well as varying stages of neurodevelopment. To address this challenge, previous studies have explored domain adaptation (DA) algorithms from various perspectives, including feature alignment, entropy minimization, contrast synthesis (style transfer), and pseudo-labeling. This paper introduces a novel framework called MAPSeg (Masked Autoencoding and Pseudo-labelling Segmentation) to address the challenges of cross-age, cross-modality, and cross-site segmentation of subcortical regions in infant brain MRI. Utilizing 3D masked autoencoding as well as masked pseudo-labeling, the model is able to jointly learn from labeled source domain data and unlabeled target domain data. We evaluated our framework on expert-annotated datasets acquired from different ages and sites. MAPSeg consist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38750;&#23545;&#35282;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#20013;&#20351;&#29992;&#65292;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;&#20855;&#26377;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#21644;&#20855;&#26377;&#30456;&#20851;&#20808;&#39564;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2303.05101</link><description>&lt;p&gt;
&#38750;&#23545;&#35282;&#24230;&#37327;&#20013;&#30340;&#21487;&#25193;&#23637;&#38543;&#26426;&#26799;&#24230;&#37324;&#26364;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Scalable Stochastic Gradient Riemannian Langevin Dynamics in Non-Diagonal Metrics. (arXiv:2303.05101v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38750;&#23545;&#35282;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#20013;&#20351;&#29992;&#65292;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;&#20855;&#26377;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#21644;&#20855;&#26377;&#30456;&#20851;&#20808;&#39564;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#35266;&#23519;&#21040;&#65292;&#21253;&#21547;&#24494;&#20998;&#20960;&#20309;&#27010;&#24565;&#30340;&#26041;&#27861;&#24448;&#24448;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#37324;&#26364;&#24230;&#37327;&#36890;&#36807;&#32771;&#34385;&#23616;&#37096;&#26354;&#29575;&#26469;&#25913;&#21892;&#21518;&#39564;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#31616;&#21333;&#30340;&#23545;&#35282;&#24230;&#37327;&#20197;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#65292;&#36825;&#20250;&#25439;&#22833;&#19968;&#20123;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#38750;&#23545;&#35282;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#26799;&#24230;&#37319;&#26679;&#20013;&#20351;&#29992;&#65292;&#20197;&#25913;&#21892;&#25910;&#25947;&#24615;&#21644;&#25506;&#32034;&#24615;&#65292;&#22312;&#23545;&#27604;&#23545;&#35282;&#24230;&#37327;&#21482;&#26377;&#36731;&#24494;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20855;&#26377;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#21644;&#20855;&#26377;&#30456;&#20851;&#20808;&#39564;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#36825;&#20123;&#24230;&#37327;&#21487;&#20197;&#25552;&#20379;&#25913;&#36827;&#12290;&#23545;&#20110;&#20854;&#20182;&#19968;&#20123;&#36873;&#25321;&#65292;&#21518;&#39564;&#20998;&#24067;&#22312;&#31616;&#21333;&#24230;&#37327;&#19979;&#20063;&#36275;&#22815;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic-gradient sampling methods are often used to perform Bayesian inference on neural networks. It has been observed that the methods in which notions of differential geometry are included tend to have better performances, with the Riemannian metric improving posterior exploration by accounting for the local curvature. However, the existing methods often resort to simple diagonal metrics to remain computationally efficient. This loses some of the gains. We propose two non-diagonal metrics that can be used in stochastic-gradient samplers to improve convergence and exploration but have only a minor computational overhead over diagonal metrics. We show that for fully connected neural networks (NNs) with sparsity-inducing priors and convolutional NNs with correlated priors, using these metrics can provide improvements. For some other choices the posterior is sufficiently easy also for the simpler metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#32593;&#32476;CECT&#65292;&#21033;&#29992;&#21487;&#25511;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#36827;&#34892;&#32452;&#21512;&#65292;&#33021;&#21516;&#26102;&#25429;&#25417;&#22810;&#20010;&#23616;&#37096;&#21644;&#20840;&#23616;&#23610;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;COVID-19&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#21518;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#36825;&#19968;&#26032;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2302.02314</link><description>&lt;p&gt;
CECT&#65306;&#21487;&#25511;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#29992;&#20110;COVID-19&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image Classification. (arXiv:2302.02314v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#32593;&#32476;CECT&#65292;&#21033;&#29992;&#21487;&#25511;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#36827;&#34892;&#32452;&#21512;&#65292;&#33021;&#21516;&#26102;&#25429;&#25417;&#22810;&#20010;&#23616;&#37096;&#21644;&#20840;&#23616;&#23610;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;COVID-19&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#21518;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#36825;&#19968;&#26032;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#25110;Transformer&#24320;&#21457;&#65292;&#21069;&#32773;&#65288;&#21518;&#32773;&#65289;&#21487;&#20197;&#25429;&#25417;&#23616;&#37096;&#65288;&#20840;&#23616;&#65289;&#29305;&#24449;&#12290;&#20026;&#20102;&#20943;&#36731;&#27169;&#22411;&#24615;&#33021;&#21463;&#23616;&#37096;&#65288;&#20840;&#23616;&#65289;&#29305;&#24449;&#32570;&#20047;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#32593;&#32476;CECT&#65292;&#36890;&#36807;&#21487;&#25511;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#36827;&#34892;&#32452;&#21512;&#12290;CECT&#30001;&#21367;&#31215;&#32534;&#30721;&#22359;&#12289;&#36716;&#32622;&#21367;&#31215;&#35299;&#30721;&#22359;&#21644;Transformer&#20998;&#31867;&#22359;&#32452;&#25104;&#12290;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;CNN&#25110;Transformer&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;CECT&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#22810;&#20010;&#23616;&#37096;&#21644;&#20840;&#23616;&#23610;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#38598;&#21512;&#31995;&#25968;&#21487;&#20197;&#25511;&#21046;&#19981;&#21516;&#23610;&#24230;&#23616;&#37096;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#20849;COVID-19&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;CECT&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#29305;&#24449;&#25429;&#25417;&#33021;&#21147;&#65292;&#25105;&#20204;&#30456;&#20449;CECT&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#32780;&#26377;&#25928;&#30340;&#24037;&#20855;&#25193;&#23637;&#21040;&#20854;&#20182;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most computer vision models are developed based on either convolutional neural network (CNN) or transformer, while the former (latter) method captures local (global) features. To relieve model performance limitations due to the lack of global (local) features, we develop a novel classification network CECT by controllable ensemble CNN and transformer. CECT is composed of a convolutional encoder block, a transposed-convolutional decoder block, and a transformer classification block. Different from conventional CNN- or transformer-based methods, our CECT can capture features at both multi-local and global scales. Besides, the contribution of local features at different scales can be controlled with the proposed ensemble coefficients. We evaluate CECT on two public COVID-19 datasets and it outperforms existing state-of-the-art methods on all evaluation metrics. With remarkable feature capture ability, we believe CECT can be extended to other medical image classification scenarios as a dia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2301.10856</link><description>&lt;p&gt;
&#37096;&#20998;&#21160;&#21592;&#65306;&#36319;&#36394;&#20420;&#32599;&#26031;&#23186;&#20307;&#21644;&#30005;&#25253;&#20043;&#38388;&#30340;&#22810;&#35821;&#35328;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20420;&#32599;&#26031;&#20837;&#20405;&#20044;&#20811;&#20848;&#21518;&#65292;&#38024;&#23545;&#20420;&#32599;&#26031;&#22312;&#32447;&#23186;&#20307;&#30340;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#65292;&#21253;&#25324;&#20420;&#32599;&#26031;&#20043;&#22768;&#21644;&#21355;&#26143;&#26032;&#38395;&#22312;&#20869;&#30340;&#20420;&#32599;&#26031;&#23186;&#20307;&#22312;&#27431;&#27954;&#36973;&#21040;&#31105;&#27490;&#12290;&#20026;&#20102;&#20445;&#25345;&#35266;&#20247;&#25968;&#37327;&#65292;&#35768;&#22810;&#20420;&#32599;&#26031;&#23186;&#20307;&#24320;&#22987;&#22312;&#30005;&#25253;&#31561;&#28040;&#24687;&#26381;&#21153;&#19978;&#22823;&#21147;&#23459;&#20256;&#20854;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;2022&#24180;&#26399;&#38388;16&#23478;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#22914;&#20309;&#19982;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20114;&#21160;&#21644;&#21033;&#29992;&#12290;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;MPNet&#12289;DP-means&#32858;&#31867;&#21644;Hawkes&#36807;&#31243;&#65292;&#25105;&#20204;&#36319;&#36394;&#26032;&#38395;&#32593;&#31449;&#21644;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#21465;&#20107;&#20256;&#25773;&#24773;&#20917;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20182;&#20204;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#12290;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#32593;&#31449;&#20013;&#65292;2.3&#65285;&#65288;ura.news&#65289;&#33267;26.7&#65285;&#65288;ukraina.ru&#65289;&#30340;&#25991;&#31456;&#35752;&#35770;&#20102;&#28304;&#20110;/&#23548;&#33268;&#30005;&#25253;&#27963;&#21160;&#30340;&#20869;&#23481;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36319;&#36394;&#20010;&#21035;&#20027;&#39064;&#30340;&#25193;&#25955;&#65292;&#25105;&#20204;&#27979;&#37327;&#26032;&#38395;&#32593;&#31449;&#21457;&#34920;&#25991;&#31456;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to disinformation and propaganda from Russian online media following the Russian invasion of Ukraine, Russian outlets including Russia Today and Sputnik News were banned throughout Europe. To maintain viewership, many of these Russian outlets began to heavily promote their content on messaging services like Telegram. In this work, we study how 16 Russian media outlets interacted with and utilized 732 Telegram channels throughout 2022. Leveraging the foundational model MPNet, DP-means clustering, and Hawkes Processes, we trace how narratives spread between news sites and Telegram channels. We show that news outlets not only propagate existing narratives through Telegram, but that they source material from the messaging platform. Across the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of articles discuss content that originated/resulted from activity on Telegram. Finally, tracking the spread of individual topics, we measure the rate at which news website
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#20998;&#26512;&#19982;&#20272;&#35745;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#36890;&#36807;&#37327;&#21270;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#23545;&#26368;&#32456;&#27169;&#22411;&#30340;&#25913;&#21464;&#31243;&#24230;&#25581;&#31034;&#20102;&#35757;&#32451;&#30340;&#22522;&#26412;&#30456;&#20114;&#20316;&#29992;&#12290;&#35843;&#26597;&#20102;&#24403;&#21069;&#30340;&#24433;&#21709;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2212.04612</link><description>&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#20998;&#26512;&#19982;&#20272;&#35745;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Training Data Influence Analysis and Estimation: A Survey. (arXiv:2212.04612v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#20998;&#26512;&#19982;&#20272;&#35745;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#36890;&#36807;&#37327;&#21270;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#23545;&#26368;&#32456;&#27169;&#22411;&#30340;&#25913;&#21464;&#31243;&#24230;&#25581;&#31034;&#20102;&#35757;&#32451;&#30340;&#22522;&#26412;&#30456;&#20114;&#20316;&#29992;&#12290;&#35843;&#26597;&#20102;&#24403;&#21069;&#30340;&#24433;&#21709;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22909;&#30340;&#27169;&#22411;&#38656;&#35201;&#22909;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23545;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#35757;&#32451;&#25968;&#25454;&#19982;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#36879;&#26126;&#21644;&#38590;&#20197;&#29702;&#35299;&#12290;&#36890;&#36807;&#37327;&#21270;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#23545;&#26368;&#32456;&#27169;&#22411;&#30340;&#25913;&#21464;&#31243;&#24230;&#65292;&#24433;&#21709;&#20998;&#26512;&#37096;&#20998;&#25581;&#31034;&#20102;&#35757;&#32451;&#30340;&#22522;&#26412;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#27979;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#21147;&#26159;&#26377;&#21487;&#33021;&#35777;&#26126;&#22256;&#38590;&#30340;&#65307;&#22240;&#27492;&#65292;&#21457;&#23637;&#21644;&#20351;&#29992;&#24433;&#21709;&#21147;&#20272;&#35745;&#22120;&#65292;&#20165;&#23545;&#30495;&#23454;&#24433;&#21709;&#36827;&#34892;&#36817;&#20284;&#20272;&#35745;&#12290;&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#20998;&#26512;&#19982;&#20272;&#35745;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31995;&#32479;&#21270;&#21508;&#31181;&#20851;&#20110;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#30340;&#23450;&#20041;&#65292;&#24182;&#22312;&#26576;&#20123;&#26041;&#38754;&#36827;&#34892;&#35299;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#26368;&#20808;&#36827;&#30340;&#24433;&#21709;&#20998;&#26512;&#26041;&#27861;&#20998;&#31867;&#25972;&#29702;&#65307;&#35814;&#32454;&#25551;&#36848;&#27599;&#31181;&#26041;&#27861;&#65292;&#27604;&#36739;&#20854;&#22522;&#26412;&#20551;&#35774;&#12289;&#28176;&#36817;&#22797;&#26434;&#24230;&#20197;&#21450;&#25972;&#20307;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Good models require good training data. For overparameterized deep models, the causal relationship between training data and model predictions is increasingly opaque and poorly understood. Influence analysis partially demystifies training's underlying interactions by quantifying the amount each training instance alters the final model. Measuring the training data's influence exactly can be provably hard in the worst case; this has led to the development and use of influence estimators, which only approximate the true influence. This paper provides the first comprehensive survey of training data influence analysis and estimation. We begin by formalizing the various, and in places orthogonal, definitions of training data influence. We then organize state-of-the-art influence analysis methods into a taxonomy; we describe each of these methods in detail and compare their underlying assumptions, asymptotic complexities, and overall strengths and weaknesses. Finally, we propose future resear
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#24418;&#29366;&#24341;&#23548;&#25193;&#25955;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22806;&#37096;&#27880;&#24847;&#26426;&#21046;&#23558;&#24418;&#29366;&#38480;&#21046;&#24212;&#29992;&#20110;&#36328;&#27880;&#24847;&#21147;&#22270;&#21644;&#33258;&#27880;&#24847;&#21147;&#22270;&#19978;&#65292;&#20174;&#32780;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#32771;&#34385;&#21040;&#23545;&#35937;&#24418;&#29366;&#65292;&#36827;&#32780;&#21487;&#20197;&#23454;&#29616;&#23545;&#35937;&#24418;&#29366;&#24544;&#23454;&#24230;&#26356;&#39640;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2212.00210</link><description>&lt;p&gt;
&#24102;&#26377;&#20869;&#22806;&#37096;&#27880;&#24847;&#21147;&#30340;&#24418;&#29366;&#24341;&#23548;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Shape-Guided Diffusion with Inside-Outside Attention. (arXiv:2212.00210v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#24418;&#29366;&#24341;&#23548;&#25193;&#25955;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22806;&#37096;&#27880;&#24847;&#26426;&#21046;&#23558;&#24418;&#29366;&#38480;&#21046;&#24212;&#29992;&#20110;&#36328;&#27880;&#24847;&#21147;&#22270;&#21644;&#33258;&#27880;&#24847;&#21147;&#22270;&#19978;&#65292;&#20174;&#32780;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#32771;&#34385;&#21040;&#23545;&#35937;&#24418;&#29366;&#65292;&#36827;&#32780;&#21487;&#20197;&#23454;&#29616;&#23545;&#35937;&#24418;&#29366;&#24544;&#23454;&#24230;&#26356;&#39640;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25805;&#20316;&#23545;&#35937;&#26102;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#24573;&#30053;&#23545;&#35937;&#30340;&#24418;&#29366;&#24182;&#29983;&#25104;&#38169;&#35823;&#27604;&#20363;&#12289;&#34987;&#25130;&#26029;&#25110;&#34987;&#32972;&#26223;&#20869;&#23481;&#26367;&#25442;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861; Shape-Guided Diffusion&#65292;&#35813;&#26041;&#27861;&#20462;&#25913;&#20102;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#20043;&#23545;&#29992;&#25143;&#25351;&#23450;&#30340;&#24418;&#29366;&#36755;&#20837;&#25110;&#20174;&#25991;&#26412;&#20013;&#33258;&#21160;&#25512;&#26029;&#30340;&#24418;&#29366;&#25935;&#24863;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22806;&#37096;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#21453;&#28436;&#21644;&#29983;&#25104;&#36807;&#31243;&#20013;&#23558;&#27492;&#24418;&#29366;&#38480;&#21046;&#24212;&#29992;&#20110;&#36328;&#27880;&#24847;&#21147;&#22270;&#21644;&#33258;&#27880;&#24847;&#21147;&#22270;&#19978;&#12290;&#25105;&#20204;&#30340;&#26426;&#21046;&#25351;&#23450;&#31354;&#38388;&#21306;&#22495;&#26159;&#23545;&#35937;&#65288;&#20869;&#37096;&#65289;&#36824;&#26159;&#32972;&#26223;&#65288;&#22806;&#37096;&#65289;&#65292;&#28982;&#21518;&#23558;&#25991;&#26412;&#25552;&#31034;&#25351;&#23450;&#30340;&#32534;&#36753;&#19982;&#27491;&#30830;&#30340;&#21306;&#22495;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#22312;&#24418;&#29366;&#24341;&#23548;&#32534;&#36753;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#27169;&#22411;&#24517;&#39035;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#21644;&#23545;&#35937;&#25513;&#30721;&#26367;&#25442;&#23545;&#35937;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#20174; MS-COCO &#34893;&#29983;&#30340; ShapePrompts &#22522;&#20934;&#65292;&#24182;&#22312;&#24418;&#29366;&#24544;&#23454;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102; SOTA &#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#38656;&#35201;&#38477;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
When manipulating an object, existing text-to-image diffusion models often ignore the shape of the object and generate content that is incorrectly scaled, cut off, or replaced with background content. We propose a training-free method, Shape-Guided Diffusion, that modifies pretrained diffusion models to be sensitive to shape input specified by a user or automatically inferred from text. We use a novel Inside-Outside Attention mechanism during the inversion and generation process to apply this shape constraint to the cross- and self-attention maps. Our mechanism designates which spatial region is the object (inside) vs. background (outside) then associates edits specified by text prompts to the correct region. We demonstrate the efficacy of our method on the shape-guided editing task, where the model must replace an object according to a text prompt and object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness without a degra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2211.05985</link><description>&lt;p&gt;
&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#26469;&#35299;&#37322;&#21644;&#26816;&#27979;&#20581;&#24247;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Using Persuasive Writing Strategies to Explain and Detect Health Misinformation. (arXiv:2211.05985v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#26159;&#24403;&#20170;&#31038;&#20250;&#30340;&#19968;&#22823;&#38382;&#39064;&#65292;&#35768;&#22810;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#21162;&#21147;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#27599;&#22825;&#21019;&#36896;&#30340;&#34394;&#20551;&#20449;&#24687;&#25968;&#37327;&#24040;&#22823;&#65292;&#23558;&#27492;&#20219;&#21153;&#30041;&#32473;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#30740;&#31350;&#20154;&#21592;&#22810;&#24180;&#26469;&#19968;&#30452;&#33268;&#21147;&#20110;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65292;&#20294;&#20170;&#22825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#20026;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#28155;&#21152;&#19968;&#20010;&#26032;&#23618;&#27425;&#65307;&#20351;&#29992;&#20855;&#26377;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#65292;&#35828;&#26126;&#20026;&#20160;&#20040;&#36825;&#31687;&#25991;&#31456;&#21487;&#20197;&#26631;&#35760;&#20026;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#35768;&#22810;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#26032;&#27880;&#37322;&#26041;&#26696;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#26469;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#32467;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of misinformation is a prominent problem in today's society, and many researchers in academia and industry are trying to combat it. Due to the vast amount of misinformation that is created every day, it is unrealistic to leave this task to human fact-checkers. Data scientists and researchers have been working on automated misinformation detection for years, and it is still a challenging problem today. The goal of our research is to add a new level to automated misinformation detection; classifying segments of text with persuasive writing techniques in order to produce interpretable reasoning for why an article can be marked as misinformation. To accomplish this, we present a novel annotation scheme containing many common persuasive writing tactics, along with a dataset with human annotations accordingly. For this task, we make use of a RoBERTa model for text classification, due to its high performance in NLP. We develop several language model-based baselines and present the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#20915;&#31574;&#21382;&#21490;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#29992;&#20110;&#37327;&#21270;&#20381;&#36182;&#31243;&#24230;&#30340;$p$-&#26377;&#25928;&#20869;&#23384;&#23481;&#37327;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2210.09903</link><description>&lt;p&gt;
&#20855;&#26377;&#26080;&#38480;&#21046;&#20869;&#23384;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Convex Optimization with Unbounded Memory. (arXiv:2210.09903v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#20915;&#31574;&#21382;&#21490;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#29992;&#20110;&#37327;&#21270;&#20381;&#36182;&#31243;&#24230;&#30340;$p$-&#26377;&#25928;&#20869;&#23384;&#23481;&#37327;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#26159;&#22312;&#32447;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22312;&#24456;&#22810;&#24212;&#29992;&#20013;&#65292;&#23398;&#20064;&#32773;&#30340;&#25439;&#22833;&#19981;&#20165;&#21462;&#20915;&#20110;&#24403;&#21069;&#30340;&#20915;&#31574;&#65292;&#36824;&#21462;&#20915;&#20110;&#30452;&#21040;&#37027;&#20010;&#26102;&#38388;&#28857;&#30340;&#25152;&#26377;&#20915;&#31574;&#21382;&#21490;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;OCO&#30340;&#25193;&#23637;&#26694;&#26550;&#65292;&#8220;&#20855;&#26377;&#26080;&#38480;&#21046;&#20869;&#23384;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#8221;&#65292;&#26469;&#25429;&#25417;&#23545;&#36807;&#21435;&#20915;&#31574;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;$p$-&#26377;&#25928;&#20869;&#23384;&#23481;&#37327;&#30340;&#27010;&#24565;&#65292;$H_p$&#65292;&#23427;&#37327;&#21270;&#20102;$p$&#38454;&#24433;&#21709;&#30340;&#26368;&#22823;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online convex optimization (OCO) is a widely used framework in online learning. In each round, the learner chooses a decision in a convex set and an adversary chooses a convex loss function, and then the learner suffers the loss associated with their current decision. However, in many applications the learner's loss depends not only on the current decision but on the entire history of decisions until that point. The OCO framework and its existing generalizations do not capture this, and they can only be applied to many settings of interest after a long series of approximation arguments. They also leave open the question of whether the dependence on memory is tight because there are no non-trivial lower bounds. In this work we introduce a generalization of the OCO framework, ``Online Convex Optimization with Unbounded Memory'', that captures long-term dependence on past decisions. We introduce the notion of $p$-effective memory capacity, $H_p$, that quantifies the maximum influence of p
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#28041;&#21450;&#22823;&#37327;&#29420;&#31435;&#20174;&#23646;&#32773;&#30340;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#19968;&#20010;&#37319;&#26679;&#30340;&#23376;&#38598;&#20013;&#20272;&#35745;&#26410;&#37319;&#26679;&#20174;&#23646;&#32773;&#30340;&#30446;&#26631;&#20540;&#26469;&#20248;&#21270;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#19968;&#33324;&#20174;&#23646;&#32773;&#29305;&#24449;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2209.09404</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#27714;&#35299;&#22823;&#35268;&#27169;&#21452;&#23618;&#21644;&#38543;&#26426;&#35268;&#21010;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;: &#20197;&#33258;&#34892;&#36710;&#32593;&#32476;&#35774;&#35745;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Approach to Solving Large Bilevel and Stochastic Programs: Application to Cycling Network Design. (arXiv:2209.09404v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09404
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#28041;&#21450;&#22823;&#37327;&#29420;&#31435;&#20174;&#23646;&#32773;&#30340;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#19968;&#20010;&#37319;&#26679;&#30340;&#23376;&#38598;&#20013;&#20272;&#35745;&#26410;&#37319;&#26679;&#20174;&#23646;&#32773;&#30340;&#30446;&#26631;&#20540;&#26469;&#20248;&#21270;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#19968;&#33324;&#20174;&#23646;&#32773;&#29305;&#24449;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#28041;&#21450;&#22823;&#37327;&#29420;&#31435;&#20174;&#23646;&#32773;&#30340;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#65292;&#20854;&#20013;&#29305;&#27530;&#24773;&#20917;&#21253;&#25324;&#20004;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#27169;&#22411;&#65292;&#26126;&#30830;&#32771;&#34385;&#21040;&#20174;&#19968;&#20010;&#37319;&#26679;&#30340;&#23376;&#38598;&#30340;&#20174;&#23646;&#32773;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20272;&#35745;&#26410;&#37319;&#26679;&#30340;&#20174;&#23646;&#32773;&#30340;&#30446;&#26631;&#20540;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#23884;&#20837;&#21040;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#36825;&#20801;&#35768;&#25105;&#20204;&#20351;&#29992;&#26080;&#27861;&#36890;&#36807;&#39046;&#23548;&#32773;&#20915;&#31574;&#34920;&#31034;&#30340;&#19968;&#33324;&#20174;&#23646;&#32773;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29983;&#25104;&#30340;&#39046;&#23548;&#32773;&#20915;&#31574;&#22312;&#21407;&#22987;&#30446;&#26631;&#20989;&#25968;&#30340;&#26368;&#20248;&#38388;&#38553;&#19978;&#30340;&#30028;&#38480;&#65292;&#35813;&#30446;&#26631;&#20989;&#25968;&#32771;&#34385;&#21040;&#23436;&#25972;&#30340;&#20174;&#23646;&#32773;&#38598;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20174;&#23646;&#32773;&#37319;&#26679;&#31639;&#27861;&#26469;&#32553;&#23567;&#30028;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#20174;&#23646;&#32773;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#29992;&#20316;&#23884;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#33258;&#34892;&#36710;&#32593;&#32476;&#35774;&#35745;&#38382;&#39064;&#30340;&#21512;&#25104;&#23454;&#20363;&#36827;&#34892;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
We present a novel machine learning-based approach to solving bilevel programs that involve a large number of independent followers, which as a special case include two-stage stochastic programming. We propose an optimization model that explicitly considers a sampled subset of followers and exploits a machine learning model to estimate the objective values of unsampled followers. Unlike existing approaches, we embed machine learning model training into the optimization problem, which allows us to employ general follower features that can not be represented using leader decisions. We prove bounds on the optimality gap of the generated leader decision as measured by the original objective function that considers the full follower set. We then develop follower sampling algorithms to tighten the bounds and a representation learning approach to learn follower features, which can be used as inputs to the embedded machine learning model. Using synthetic instances of a cycling network design p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21464;&#37327;&#25968;&#37327;&#36828;&#22823;&#20110;&#26679;&#26412;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#22320;&#20272;&#35745;&#22823;&#35268;&#27169;&#22240;&#26524;&#22810;&#26641;&#32467;&#26500;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#20219;&#20309;&#20998;&#24067;&#25110;&#24314;&#27169;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2209.07028</link><description>&lt;p&gt;
&#20174;&#23567;&#26679;&#26412;&#20013;&#20272;&#35745;&#22823;&#30340;&#22240;&#26524;&#22810;&#26641;
&lt;/p&gt;
&lt;p&gt;
Estimating large causal polytrees from small samples. (arXiv:2209.07028v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21464;&#37327;&#25968;&#37327;&#36828;&#22823;&#20110;&#26679;&#26412;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#22320;&#20272;&#35745;&#22823;&#35268;&#27169;&#22240;&#26524;&#22810;&#26641;&#32467;&#26500;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#20219;&#20309;&#20998;&#24067;&#25110;&#24314;&#27169;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#30456;&#23545;&#36739;&#23567;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#20013;&#20272;&#35745;&#22823;&#30340;&#22240;&#26524;&#22810;&#26641;&#30340;&#38382;&#39064;&#12290;&#36825;&#26159;&#22312;&#21464;&#37327;&#25968;&#37327;&#19982;&#26679;&#26412;&#22823;&#23567;&#30456;&#27604;&#38750;&#24120;&#22823;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#22240;&#26524;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20197;&#39640;&#20934;&#30830;&#24230;&#24674;&#22797;&#26641;&#24418;&#32467;&#26500;&#12290;&#35813;&#31639;&#27861;&#38500;&#20102;&#19968;&#20123;&#28201;&#21644;&#30340;&#38750;&#36864;&#21270;&#26465;&#20214;&#22806;&#65292;&#22522;&#26412;&#19981;&#38656;&#35201;&#20998;&#24067;&#25110;&#24314;&#27169;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating a large causal polytree from a relatively small i.i.d. sample. This is motivated by the problem of determining causal structure when the number of variables is very large compared to the sample size, such as in gene regulatory networks. We give an algorithm that recovers the tree with high accuracy in such settings. The algorithm works under essentially no distributional or modeling assumptions other than some mild non-degeneracy conditions.
&lt;/p&gt;</description></item><item><title>WaveMix&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#36798;&#21040;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;GPU RAM&#65292;&#23454;&#29616;&#26102;&#38388;&#12289;&#25104;&#26412;&#21644;&#33021;&#37327;&#30340;&#33410;&#30465;&#12290;</title><link>http://arxiv.org/abs/2205.14375</link><description>&lt;p&gt;
WaveMix: &#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#20998;&#26512;&#30340;&#36164;&#28304;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
WaveMix: A Resource-efficient Neural Network for Image Analysis. (arXiv:2205.14375v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14375
&lt;/p&gt;
&lt;p&gt;
WaveMix&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#36798;&#21040;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;GPU RAM&#65292;&#23454;&#29616;&#26102;&#38388;&#12289;&#25104;&#26412;&#21644;&#33021;&#37327;&#30340;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;WaveMix&#8212;&#8212;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#26082;&#20855;&#26377;&#36164;&#28304;&#25928;&#29575;&#24615;&#65292;&#21448;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;WaveMix&#32593;&#32476;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#65292;&#21253;&#25324;Cityscapes&#20013;&#30340;&#20998;&#21106;&#21644;Places-365&#12289;&#20116;&#20010;EMNIST&#25968;&#25454;&#38598;&#21644;iNAT-mini&#20013;&#30340;&#20998;&#31867;&#65292;&#24182;&#24314;&#31435;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;&#20196;&#20154;&#24778;&#22855;&#30340;&#26159;&#65292;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;&#65292;WaveMix&#32467;&#26500;&#25152;&#38656;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;&#27492;&#22806;&#65292;&#24403;&#25511;&#21046;&#21442;&#25968;&#25968;&#37327;&#26102;&#65292;WaveMix&#25152;&#38656;&#30340;GPU RAM&#26356;&#23569;&#65292;&#36825;&#24847;&#21619;&#30528;&#33410;&#30465;&#26102;&#38388;&#12289;&#25104;&#26412;&#21644;&#33021;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#25910;&#30410;&#65292;&#25105;&#20204;&#22312;WaveMix&#22359;&#20013;&#20351;&#29992;&#20102;&#22810;&#32423;&#20108;&#32500;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;2D-DWT&#65289;&#65292;&#23427;&#20855;&#26377;&#20197;&#19979;&#20248;&#28857;:(1)&#23427;&#22522;&#20110;&#19977;&#31181;&#24378;&#22270;&#20687;&#20808;&#39564;&#26465;&#20214;&#37325;&#26032;&#32452;&#32455;&#31354;&#38388;&#20449;&#24687;&#8212;&#8212;&#23610;&#24230;&#19981;&#21464;&#24615;&#65292;&#20301;&#31227;&#19981;&#21464;&#24615;&#21644;&#36793;&#32536;&#30340;&#31232;&#30095;&#24615;,(2) i
&lt;/p&gt;
&lt;p&gt;
We propose WaveMix -- a novel neural architecture for computer vision that is resource-efficient yet generalizable and scalable. WaveMix networks achieve comparable or better accuracy than the state-of-the-art convolutional neural networks, vision transformers, and token mixers for several tasks, establishing new benchmarks for segmentation on Cityscapes; and for classification on Places-365, five EMNIST datasets, and iNAT-mini. Remarkably, WaveMix architectures require fewer parameters to achieve these benchmarks compared to the previous state-of-the-art. Moreover, when controlled for the number of parameters, WaveMix requires lesser GPU RAM, which translates to savings in time, cost, and energy. To achieve these gains we used multi-level two-dimensional discrete wavelet transform (2D-DWT) in WaveMix blocks, which has the following advantages: (1) It reorganizes spatial information based on three strong image priors -- scale-invariance, shift-invariance, and sparseness of edges, (2) i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20195;&#29702;&#21464;&#37327;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270; (P3O) &#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#36817;&#31471;&#22240;&#26524;&#25512;&#26029;&#26500;&#24314;&#24754;&#35266;&#32622;&#20449;&#21306;&#38388;&#32806;&#21512;&#24207;&#21015;&#35299;&#20915;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28151;&#28102;&#20559;&#24046;&#21644;&#26368;&#20248;&#31574;&#30053;&#19982;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.13589</link><description>&lt;p&gt;
&#38754;&#23545;&#28151;&#28102;&#22240;&#32032;&#30340;&#24754;&#35266;&#24773;&#32490;&#65306;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#35777;&#26126;&#26377;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes. (arXiv:2205.13589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20195;&#29702;&#21464;&#37327;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270; (P3O) &#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#36817;&#31471;&#22240;&#26524;&#25512;&#26029;&#26500;&#24314;&#24754;&#35266;&#32622;&#20449;&#21306;&#38388;&#32806;&#21512;&#24207;&#21015;&#35299;&#20915;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28151;&#28102;&#20559;&#24046;&#21644;&#26368;&#20248;&#31574;&#30053;&#19982;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26088;&#22312;&#20174;&#30001;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#33021;&#21462;&#20915;&#20110;&#28508;&#22312;&#29366;&#24577;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#22312;&#28151;&#28102;&#24847;&#20041;&#19978;&#21516;&#26102;&#24433;&#21709;&#34892;&#21160;&#21644;&#35266;&#27979;&#20540;&#65292;&#36825;&#23545;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35828;&#26159;&#31105;&#27490;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#36817;&#31471;&#22240;&#26524;&#25512;&#26029;&#26500;&#24314;&#30340;&#24754;&#35266;&#32622;&#20449;&#21306;&#38388;&#32806;&#21512;&#24207;&#21015;&#30340;&#20195;&#29702;&#21464;&#37327;&#24754;&#35266;&#31574;&#30053;&#20248;&#21270;&#65288;P3O&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#24191;&#20041;&#20989;&#25968;&#36924;&#36817;&#30340;&#19978;&#19979;&#25991;&#20013;&#35299;&#20915;&#20102;&#28151;&#28102;&#20559;&#24046;&#21644;&#26368;&#20248;&#31574;&#30053;&#19982;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28151;&#28102;&#25968;&#25454;&#38598;&#30340;&#37096;&#20998;&#35206;&#30422;&#20551;&#35774;&#19979;&#65292;P3O&#21487;&#20197;&#23454;&#29616;n^{-1/2}&#30340;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study offline reinforcement learning (RL) in partially observable Markov decision processes. In particular, we aim to learn an optimal policy from a dataset collected by a behavior policy which possibly depends on the latent state. Such a dataset is confounded in the sense that the latent state simultaneously affects the action and the observation, which is prohibitive for existing offline RL algorithms. To this end, we propose the \underline{P}roxy variable \underline{P}essimistic \underline{P}olicy \underline{O}ptimization (\texttt{P3O}) algorithm, which addresses the confounding bias and the distributional shift between the optimal and behavior policies in the context of general function approximation. At the core of \texttt{P3O} is a coupled sequence of pessimistic confidence regions constructed via proximal causal inference, which is formulated as minimax estimation. Under a partial coverage assumption on the confounded dataset, we prove that \texttt{P3O} achieves a $n^{-1/2}$-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;GRADCAM&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#20351;&#29992;&#28909;&#22270;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.03583</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33016;&#37096;&#30142;&#30149;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs. (arXiv:2202.03583v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;GRADCAM&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#20351;&#29992;&#28909;&#22270;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;X&#20809;&#22270;&#20687;&#30149;&#29702;&#35782;&#21035;&#26041;&#27861;&#20381;&#36182;&#20110;&#29087;&#32451;&#30340;&#20154;&#31867;&#35299;&#37322;&#65292;&#24182;&#19988;&#24448;&#24448;&#32791;&#26102;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20986;&#29616;&#20351;&#33258;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#24320;&#21457;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#36825;&#31867;&#31995;&#32479;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#23427;&#25552;&#20379;&#30340;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DenseNet&#65289;&#21644;GRADCAM&#36827;&#34892;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#21069;&#32622;X&#20809;&#35757;&#32451;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#23450;&#37327;&#25351;&#26631;&#65288;&#21253;&#25324;&#21463;&#35797;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#65289;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#33719;&#24471;&#20102;0.826&#30340;&#20934;&#30830;&#24230;&#12290;&#32780;&#22312;Nodule&#26465;&#20214;&#19979;&#33719;&#24471;&#20102;&#26368;&#20302;&#30340;AUC&#24471;&#20998;0.655&#65292;&#20934;&#30830;&#24230;&#20026;0.66&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#20915;&#31574;&#26041;&#38754;&#24314;&#31435;&#20449;&#20219;&#65292;&#25105;&#20204;&#20351;&#29992;GRADCAM&#29983;&#25104;&#20102;&#28909;&#22270;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;&#23545;&#35786;&#26029;&#26368;&#37325;&#35201;&#30340;X&#20809;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional methods of identifying pathologies in X-ray images rely heavily on skilled human interpretation and are often time-consuming. The advent of deep learning techniques has enabled the development of automated disease diagnosis systems, but the performance of such systems is dependent on the quality of the model and the level of interpretability it provides. In this paper, we propose a multi-label disease diagnosis model for chest X-rays using a dense convolutional neural network (DenseNet) and model interpretability using GRADCAM. We trained our model using frontal X-rays and evaluated its performance using various quantitative metrics, including the area under the receiver operating characteristic curve (AUC). Our proposed model achieved the highest AUC score of 0.896 for the condition Cardiomegaly with an accuracy of 0.826, while the lowest AUC score was obtained for Nodule, at 0.655 with an accuracy of 0.66. To promote model interpretability and build trust in decision maki
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20197;&#21450;Bandits&#26041;&#27861;&#30340;&#20581;&#22766;&#32479;&#35745;&#23398;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#22810;&#20219;&#21153;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#35813;&#20272;&#35745;&#22120;&#20197;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#20849;&#20139;&#20840;&#23616;&#21442;&#25968;&#21644;&#31232;&#30095;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2112.14233</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;Bandits&#36890;&#36807;&#20581;&#22766;&#32479;&#35745;&#23398;
&lt;/p&gt;
&lt;p&gt;
Multitask Learning and Bandits via Robust Statistics. (arXiv:2112.14233v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20197;&#21450;Bandits&#26041;&#27861;&#30340;&#20581;&#22766;&#32479;&#35745;&#23398;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#22810;&#20219;&#21153;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#35813;&#20272;&#35745;&#22120;&#20197;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#20849;&#20139;&#20840;&#23616;&#21442;&#25968;&#21644;&#31232;&#30095;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#32773;&#32463;&#24120;&#21516;&#26102;&#38754;&#23545;&#35768;&#22810;&#30456;&#20851;&#20294;&#24322;&#36136;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#20010;&#23398;&#20064;&#23454;&#20363;&#20013;&#30340;&#26410;&#30693;&#21442;&#25968;&#21487;&#20197;&#20998;&#35299;&#20026;&#20849;&#20139;&#20840;&#23616;&#21442;&#25968;&#21152;&#19978;&#31232;&#30095;&#30340;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#22810;&#20219;&#21153;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#20197;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#65292;&#20351;&#29992;&#20581;&#22766;&#32479;&#35745;&#23398;&#65288;&#22312;&#30456;&#20284;&#23454;&#20363;&#19978;&#23398;&#20064;&#65289;&#21644;LASSO&#22238;&#24402;&#65288;&#21435;&#20559;&#24046;&#32467;&#26524;&#65289;&#30340;&#29420;&#29305;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-makers often simultaneously face many related but heterogeneous learning problems. For instance, a large retailer may wish to learn product demand at different stores to solve pricing or inventory problems, making it desirable to learn jointly for stores serving similar customers; alternatively, a hospital network may wish to learn patient risk at different providers to allocate personalized interventions, making it desirable to learn jointly for hospitals serving similar patient populations. Motivated by real datasets, we study a natural setting where the unknown parameter in each learning instance can be decomposed into a shared global parameter plus a sparse instance-specific term. We propose a novel two-stage multitask learning estimator that exploits this structure in a sample-efficient way, using a unique combination of robust statistics (to learn across similar instances) and LASSO regression (to debias the results). Our estimator yields improved sample complexity bound
&lt;/p&gt;</description></item></channel></rss>