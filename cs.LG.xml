<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65288;SVT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20809;&#35889;&#28151;&#21512;&#26469;&#35299;&#20915;&#35270;&#35273;&#21464;&#25442;&#20013;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24615;&#21644;&#20449;&#24687;&#25429;&#25417;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01310</link><description>&lt;p&gt;
&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65306;&#20809;&#35889;&#28151;&#21512;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scattering Vision Transformer: Spectral Mixing Matters. (arXiv:2311.01310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65288;SVT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20809;&#35889;&#28151;&#21512;&#26469;&#35299;&#20915;&#35270;&#35273;&#21464;&#25442;&#20013;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24615;&#21644;&#20449;&#24687;&#25429;&#25417;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#23454;&#20363;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#27880;&#24847;&#21147;&#22797;&#26434;&#24615;&#21644;&#26377;&#25928;&#25429;&#25417;&#22270;&#20687;&#20013;&#32454;&#31890;&#24230;&#20449;&#24687;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#37319;&#29992;&#38477;&#37319;&#26679;&#25805;&#20316;&#65288;&#22914;&#27744;&#21270;&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25805;&#20316;&#26159;&#19981;&#21487;&#36870;&#30340;&#65292;&#21487;&#33021;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25955;&#23556;&#35270;&#35273;&#21464;&#25442;&#65288;SVT&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;SVT&#32467;&#21512;&#20102;&#19968;&#20010;&#20809;&#35889;&#25955;&#23556;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#22270;&#20687;&#32454;&#33410;&#30340;&#25429;&#25417;&#12290;SVT&#36890;&#36807;&#20998;&#31163;&#20302;&#39057;&#21644;&#39640;&#39057;&#20998;&#37327;&#65292;&#20811;&#26381;&#20102;&#19982;&#38477;&#37319;&#26679;&#25805;&#20316;&#30456;&#20851;&#30340;&#19981;&#21487;&#36870;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;SVT&#24341;&#20837;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#20809;&#35889;&#38376;&#25511;&#32593;&#32476;&#65292;&#21033;&#29992;Einstein&#20056;&#27861;&#26469;&#22788;&#29702;&#20196;&#29260;&#21644;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have gained significant attention and achieved state-of-the-art performance in various computer vision tasks, including image classification, instance segmentation, and object detection. However, challenges remain in addressing attention complexity and effectively capturing fine-grained information within images. Existing solutions often resort to down-sampling operations, such as pooling, to reduce computational cost. Unfortunately, such operations are non-invertible and can result in information loss. In this paper, we present a novel approach called Scattering Vision Transformer (SVT) to tackle these challenges. SVT incorporates a spectrally scattering network that enables the capture of intricate image details. SVT overcomes the invertibility issue associated with down-sampling operations by separating low-frequency and high-frequency components. Furthermore, SVT introduces a unique spectral gating network utilizing Einstein multiplication for token and channel 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#30456;&#20851;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#25512;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;Lipschitz&#27491;&#21017;&#21270;&#12289;&#25439;&#22833;&#21152;&#26435;&#21644;&#20449;&#20219;&#21306;&#22495;&#37325;&#26032;&#21327;&#35843;&#65292;&#20197;&#20943;&#23567;&#22312;&#28508;&#22312;&#31354;&#38388;&#21644;&#30446;&#26631;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;&#22810;&#20010;&#20248;&#21270;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20258</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#30456;&#20851;&#30340;&#28508;&#22312;&#31354;&#38388;&#25512;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Advancing Bayesian Optimization via Learning Correlated Latent Space. (arXiv:2310.20258v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#30456;&#20851;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#25512;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;Lipschitz&#27491;&#21017;&#21270;&#12289;&#25439;&#22833;&#21152;&#26435;&#21644;&#20449;&#20219;&#21306;&#22495;&#37325;&#26032;&#21327;&#35843;&#65292;&#20197;&#20943;&#23567;&#22312;&#28508;&#22312;&#31354;&#38388;&#21644;&#30446;&#26631;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;&#22810;&#20010;&#20248;&#21270;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#36890;&#36807;&#26377;&#38480;&#30340;&#20989;&#25968;&#35780;&#20272;&#26469;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65289;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#21487;&#20197;&#23545;&#32467;&#26500;&#21270;&#25110;&#31163;&#25955;&#25968;&#25454;&#36827;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20248;&#21270;&#19981;&#26159;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#65292;&#36825;&#23548;&#33268;&#20102;&#28508;&#22312;&#30340;&#24046;&#36317;&#65292;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#20851;&#28508;&#22312;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CoBO&#65289;&#65292;&#23427;&#19987;&#27880;&#20110;&#23398;&#20064;&#30456;&#20851;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20854;&#29305;&#28857;&#26159;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#21644;&#30446;&#26631;&#20989;&#25968;&#20869;&#30340;&#36317;&#31163;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;Lipschitz&#27491;&#21017;&#21270;&#12289;&#25439;&#22833;&#21152;&#26435;&#21644;&#20449;&#20219;&#21306;&#22495;&#37325;&#26032;&#21327;&#35843;&#65292;&#20197;&#26368;&#23567;&#21270;&#26377;&#24076;&#26395;&#21306;&#22495;&#21608;&#22260;&#30340;&#28508;&#22312;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20248;&#21270;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization is a powerful method for optimizing black-box functions with limited function evaluations. Recent works have shown that optimization in a latent space through deep generative models such as variational autoencoders leads to effective and efficient Bayesian optimization for structured or discrete data. However, as the optimization does not take place in the input space, it leads to an inherent gap that results in potentially suboptimal solutions. To alleviate the discrepancy, we propose Correlated latent space Bayesian Optimization (CoBO), which focuses on learning correlated latent spaces characterized by a strong correlation between the distances in the latent space and the distances within the objective function. Specifically, our method introduces Lipschitz regularization, loss weighting, and trust region recoordination to minimize the inherent gap around the promising areas. We demonstrate the effectiveness of our approach on several optimization tasks in disc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SURF&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;SURF&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20855;&#20307;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20049</link><description>&lt;p&gt;
SURF: GNN&#39044;&#27979;&#27969;&#20307;&#21160;&#21147;&#23398;&#30340;&#27867;&#21270;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics. (arXiv:2310.20049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20049
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SURF&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;SURF&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20855;&#20307;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#27969;&#20307;&#21160;&#21147;&#23398;&#23545;&#20110;&#35774;&#35745;&#21644;&#24320;&#21457;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#28085;&#30422;&#20102;&#20174;&#31616;&#21333;&#38400;&#38376;&#21040;&#22797;&#26434;&#28065;&#36718;&#26426;&#26800;&#30340;&#33539;&#22260;&#12290;&#20934;&#30830;&#27714;&#35299;&#28508;&#22312;&#30340;&#29289;&#29702;&#26041;&#31243;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#29305;&#28857;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#27714;&#35299;&#22120;&#22312;&#32593;&#26684;&#19978;&#24314;&#27169;&#30456;&#20114;&#20316;&#29992;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#21152;&#36895;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#30495;&#27491;&#29702;&#35299;&#28508;&#22312;&#30340;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#27867;&#21270;&#32780;&#38750;&#25554;&#20540;&#12290;&#27867;&#21270;&#26159;&#36890;&#29992;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#23427;&#24212;&#35813;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#25299;&#25169;&#32467;&#26500;&#12289;&#20998;&#36776;&#29575;&#25110;&#28909;&#21147;&#23398;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SURF&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#27979;&#35797;&#23398;&#20064;&#30340;&#22522;&#20110;&#22270;&#30340;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;SURF&#21253;&#25324;&#21508;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#20855;&#20307;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#23454;&#35777;&#22320;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating fluid dynamics is crucial for the design and development process, ranging from simple valves to complex turbomachinery. Accurately solving the underlying physical equations is computationally expensive. Therefore, learning-based solvers that model interactions on meshes have gained interest due to their promising speed-ups. However, it is unknown to what extent these models truly understand the underlying physical principles and can generalize rather than interpolate. Generalization is a key requirement for a general-purpose fluid simulator, which should adapt to different topologies, resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to test the \textit{generalization} of learned graph-based fluid simulators. SURF comprises individual datasets and provides specific performance and generalization metrics for evaluating and comparing different models. We empirically demonstrate the applicability of SURF by thoroughly investigating the two state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#27604;&#36739;&#65292;&#24110;&#21161;&#20174;&#19994;&#32773;&#26356;&#22909;&#22320;&#36873;&#25321;&#39592;&#24178;&#32593;&#32476;&#65292;&#20174;&#32780;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#30740;&#31350;&#36827;&#23637;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.19909</link><description>&lt;p&gt;
&#39592;&#24178;&#32593;&#32476;&#20043;&#25112;&#65306;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks. (arXiv:2310.19909v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#27604;&#36739;&#65292;&#24110;&#21161;&#20174;&#19994;&#32773;&#26356;&#22909;&#22320;&#36873;&#25321;&#39592;&#24178;&#32593;&#32476;&#65292;&#20174;&#32780;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#30740;&#31350;&#36827;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#36890;&#24120;&#30001;&#19968;&#20010;&#39592;&#24178;&#32593;&#32476;&#26500;&#25104;&#65292;&#21363;&#39044;&#35757;&#32451;&#25110;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#20960;&#24180;&#21069;&#65292;&#20351;&#29992;ImageNet&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26159;&#40664;&#35748;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#21457;&#23637;&#20986;&#29616;&#20102;&#20351;&#29992;&#21508;&#31181;&#31639;&#27861;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#20247;&#22810;&#39592;&#24178;&#32593;&#32476;&#12290;&#34429;&#28982;&#36825;&#31181;&#36873;&#25321;&#20016;&#23500;&#24615;&#25552;&#39640;&#20102;&#19968;&#31995;&#21015;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#20174;&#19994;&#32773;&#26469;&#35828;&#65292;&#24456;&#38590;&#20570;&#20986;&#26126;&#26234;&#30340;&#36873;&#25321;&#12290;&#39592;&#24178;&#32593;&#32476;&#20043;&#25112;&#65288;BoB&#65289;&#36890;&#36807;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#31283;&#23450;&#25193;&#25955;&#39592;&#24178;&#31561;&#31561;&#65292;&#20197;&#21450;&#38024;&#23545;&#20174;&#20998;&#31867;&#21040;&#30446;&#26631;&#26816;&#27979;&#21040;OOD&#27867;&#21270;&#31561;&#22810;&#26679;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#36825;&#20010;&#36873;&#25321;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;BoB&#36890;&#36807;&#25581;&#31034;&#26377;&#24076;&#26395;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20026;&#30740;&#31350;&#31038;&#21306;&#25512;&#21160;&#35745;&#31639;&#26426;&#35270;&#35273;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network based computer vision systems are typically built on a backbone, a pretrained or randomly initialized feature extractor. Several years ago, the default option was an ImageNet-trained convolutional neural network. However, the recent past has seen the emergence of countless backbones pretrained using various algorithms and datasets. While this abundance of choice has led to performance increases for a range of systems, it is difficult for practitioners to make informed decisions about which backbone to choose. Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more. Furthermore, BoB sheds light on promising directions for the research community to advance computer vision by illuminating
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#38477;&#20302;&#32500;&#24230;&#21644;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20960;&#20309;&#24179;&#22343;&#21644;&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.19845</link><description>&lt;p&gt;
&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#65306;&#20197;XGBoost&#22312;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#20013;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction. (arXiv:2310.19845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#38477;&#20302;&#32500;&#24230;&#21644;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20960;&#20309;&#24179;&#22343;&#21644;&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#22403;&#22334;&#37038;&#20214;&#38382;&#39064;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#21644;&#21830;&#19994;&#30028;&#30340;&#20851;&#27880;&#12290;Twitter&#24050;&#25104;&#20026;&#20256;&#25773;&#22403;&#22334;&#37038;&#20214;&#20869;&#23481;&#30340;&#39318;&#36873;&#23186;&#20171;&#12290;&#35768;&#22810;&#30740;&#31350;&#21162;&#21147;&#35797;&#22270;&#24212;&#23545;&#31038;&#20132;&#32593;&#32476;&#22403;&#22334;&#37038;&#20214;&#12290;Twitter&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#29305;&#24449;&#31354;&#38388;&#30340;&#22823;&#23567;&#21644;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#36890;&#24120;&#65292;&#30456;&#20851;&#30740;&#31350;&#24037;&#20316;&#20851;&#27880;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#20027;&#35201;&#25361;&#25112;&#65292;&#25110;&#32773;&#20135;&#29983;&#40657;&#30418;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#38477;&#20302;&#32500;&#24230;&#21644;&#20248;&#21270;&#36229;&#21442;&#25968;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#12290;&#35813;&#31639;&#27861;&#21021;&#22987;&#21270;&#20102;&#19968;&#20010;eXtreme Gradient Boosting&#20998;&#31867;&#22120;&#65292;&#24182;&#20943;&#23569;&#20102;&#25512;&#25991;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#29983;&#25104;&#19968;&#20010;&#22403;&#22334;&#37038;&#20214;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;50&#27425;&#37325;&#22797;&#30340;10&#20493;&#20998;&#23618;&#20132;&#21449;&#39564;&#35777;&#36827;&#34892;&#39564;&#35777;&#65292;&#24182;&#20351;&#29992;&#38750;&#21442;&#25968;&#32479;&#35745;&#26816;&#39564;&#36827;&#34892;&#20998;&#26512;&#12290;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#22312;&#20960;&#20309;&#24179;&#22343;&#21644;&#20934;&#30830;&#29575;&#19978;&#24179;&#22343;&#36798;&#21040;82.32&#65285;&#21644;92.67&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, spam on online social networks has attracted attention in the research and business world. Twitter has become the preferred medium to spread spam content. Many research efforts attempted to encounter social networks spam. Twitter brought extra challenges represented by the feature space size, and imbalanced data distributions. Usually, the related research works focus on part of these main challenges or produce black-box models. In this paper, we propose a modified genetic algorithm for simultaneous dimensionality reduction and hyper parameter optimization over imbalanced datasets. The algorithm initialized an eXtreme Gradient Boosting classifier and reduced the features space of tweets dataset; to generate a spam prediction model. The model is validated using a 50 times repeated 10-fold stratified cross-validation, and analyzed using nonparametric statistical tests. The resulted prediction model attains on average 82.32\% and 92.67\% in terms of geometric mean and accuracy r
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21152;&#36895;&#23547;&#25214;&#24378;&#24230;&#21644;&#21487;&#25345;&#32493;&#24615;&#28151;&#20957;&#22303;&#37197;&#26041;&#65292;&#36890;&#36807;&#30456;&#23545;&#36739;&#23569;&#30340;&#27979;&#37327;&#26377;&#25928;&#39044;&#27979;&#28151;&#20957;&#22303;&#24378;&#24230;&#65292;&#24182;&#23558;&#23547;&#25214;&#21487;&#25345;&#32493;&#28151;&#20957;&#22303;&#35270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18288</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#23454;&#29616;&#21487;&#25345;&#32493;&#28151;&#20957;&#22303;
&lt;/p&gt;
&lt;p&gt;
Sustainable Concrete via Bayesian Optimization. (arXiv:2310.18288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18288
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21152;&#36895;&#23547;&#25214;&#24378;&#24230;&#21644;&#21487;&#25345;&#32493;&#24615;&#28151;&#20957;&#22303;&#37197;&#26041;&#65292;&#36890;&#36807;&#30456;&#23545;&#36739;&#23569;&#30340;&#27979;&#37327;&#26377;&#25928;&#39044;&#27979;&#28151;&#20957;&#22303;&#24378;&#24230;&#65292;&#24182;&#23558;&#23547;&#25214;&#21487;&#25345;&#32493;&#28151;&#20957;&#22303;&#35270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#20843;&#20998;&#20043;&#19968;&#30340;&#20108;&#27687;&#21270;&#30899;&#25490;&#25918;&#21487;&#20197;&#24402;&#22240;&#20110;&#27700;&#27877;&#30340;&#29983;&#20135;&#65292;&#27700;&#27877;&#26159;&#28151;&#20957;&#22303;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20063;&#26159;&#25968;&#25454;&#20013;&#24515;&#24314;&#35774;&#20013;&#20108;&#27687;&#21270;&#30899;&#25490;&#25918;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20302;&#30899;&#28151;&#20957;&#22303;&#37197;&#26041;&#23545;&#21487;&#25345;&#32493;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23581;&#35797;&#26032;&#30340;&#28151;&#20957;&#22303;&#37197;&#26041;&#38750;&#24120;&#32791;&#26102;&#21644;&#21171;&#21160;&#23494;&#38598;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#31561;&#24453;&#35760;&#24405;&#28151;&#20957;&#22303;&#30340;28&#22825;&#25239;&#21387;&#24378;&#24230;&#65292;&#32780;&#36825;&#20010;&#37327;&#30340;&#27979;&#37327;&#26080;&#27861;&#21152;&#36895;&#12290;&#22240;&#27492;&#65292;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#22914;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#21487;&#20197;&#21152;&#36895;&#23547;&#25214;&#24378;&#24230;&#21644;&#21487;&#25345;&#32493;&#24615;&#28151;&#20957;&#22303;&#37197;&#26041;&#30340;&#25628;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#27493;&#39588;&#65292;&#36890;&#36807;&#30456;&#23545;&#36739;&#23569;&#30340;&#27979;&#37327;&#65292;&#20351;&#28151;&#20957;&#22303;&#24378;&#24230;&#36866;&#21512;&#29992;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#65307;&#23558;&#23547;&#25214;&#21487;&#25345;&#32493;&#28151;&#20957;&#22303;&#35270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65307;&#24182;&#21033;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Eight percent of global carbon dioxide emissions can be attributed to the production of cement, the main component of concrete, which is also the dominant source of CO2 emissions in the construction of data centers. The discovery of lower-carbon concrete formulae is therefore of high significance for sustainability. However, experimenting with new concrete formulae is time consuming and labor intensive, as one usually has to wait to record the concrete's 28-day compressive strength, a quantity whose measurement can by its definition not be accelerated. This provides an opportunity for experimental design methodology like Bayesian Optimization (BO) to accelerate the search for strong and sustainable concrete formulae. Herein, we 1) propose modeling steps that make concrete strength amenable to be predicted accurately by a Gaussian process model with relatively few measurements, 2) formulate the search for sustainable concrete as a multi-objective optimization problem, and 3) leverage th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#23558;&#35777;&#26126;&#30340;&#33539;&#22260;&#20174;&#29420;&#31435;&#21516;&#20998;&#24067;&#26435;&#37325;&#25193;&#23637;&#21040;&#20102;&#26356;&#22823;&#30340;&#26435;&#37325;&#20998;&#24067;&#31867;&#21035;(PSEUDO-IID)&#65292;&#21253;&#25324;&#20302;&#31209;&#21644;&#31232;&#30095;&#35774;&#32622;&#12290;&#20316;&#32773;&#21457;&#29616;&#20351;&#29992;PSEUDO-IID&#20998;&#24067;&#21021;&#22987;&#21270;&#30340;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#32593;&#32476;&#22312;&#26041;&#24046;&#19978;&#37117;&#26159;&#31561;&#25928;&#30340;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#35782;&#21035;&#26356;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36793;&#30028;&#28151;&#27788;&#29366;&#24577;&#65292;&#24182;&#36827;&#34892;&#24615;&#33021;&#35843;&#20248;&#12290;</title><link>http://arxiv.org/abs/2310.16597</link><description>&lt;p&gt;
&#36229;&#36234;&#29420;&#31435;&#21516;&#20998;&#24067;&#26435;&#37325;&#65306;&#31232;&#30095;&#21644;&#20302;&#31209;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20063;&#26159;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes. (arXiv:2310.16597v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#23558;&#35777;&#26126;&#30340;&#33539;&#22260;&#20174;&#29420;&#31435;&#21516;&#20998;&#24067;&#26435;&#37325;&#25193;&#23637;&#21040;&#20102;&#26356;&#22823;&#30340;&#26435;&#37325;&#20998;&#24067;&#31867;&#21035;(PSEUDO-IID)&#65292;&#21253;&#25324;&#20302;&#31209;&#21644;&#31232;&#30095;&#35774;&#32622;&#12290;&#20316;&#32773;&#21457;&#29616;&#20351;&#29992;PSEUDO-IID&#20998;&#24067;&#21021;&#22987;&#21270;&#30340;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#32593;&#32476;&#22312;&#26041;&#24046;&#19978;&#37117;&#26159;&#31561;&#25928;&#30340;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#35782;&#21035;&#26356;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36793;&#30028;&#28151;&#27788;&#29366;&#24577;&#65292;&#24182;&#36827;&#34892;&#24615;&#33021;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38480;&#23485;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#26377;&#29992;&#19988;&#21487;&#31649;&#29702;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#35768;&#22810;&#29616;&#35937;&#12290;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#26159;&#38543;&#26426;&#28145;&#23618;&#32593;&#32476;&#25910;&#25947;&#21040;&#39640;&#26031;&#36807;&#31243;&#65292;&#20174;&#32780;&#33021;&#22815;&#23545;&#28608;&#27963;&#20989;&#25968;&#21644;&#32593;&#32476;&#26435;&#37325;&#36873;&#25321;&#23545;&#35757;&#32451;&#21160;&#24577;&#30340;&#24433;&#21709;&#36827;&#34892;&#20005;&#26684;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;Matthews&#31561;&#20154;(2018)&#30340;&#24320;&#21019;&#24615;&#35777;&#26126;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#21021;&#22987;&#26435;&#37325;&#20998;&#24067;&#31867;&#21035;(&#25105;&#20204;&#31216;&#20043;&#20026;PSEUDO-IID)&#65292;&#20854;&#20013;&#21253;&#25324;&#29420;&#31435;&#21516;&#20998;&#24067;&#21644;&#27491;&#20132;&#26435;&#37325;&#30340;&#24050;&#26377;&#24773;&#20917;&#65292;&#20197;&#21450;&#22240;&#20854;&#35745;&#31639;&#21152;&#36895;&#20248;&#21183;&#32780;&#21463;&#21040;&#36190;&#35465;&#30340;&#26032;&#20852;&#20302;&#31209;&#21644;&#32467;&#26500;&#31232;&#30095;&#35774;&#32622;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;PSEUDO-IID&#20998;&#24067;&#21021;&#22987;&#21270;&#30340;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#32593;&#32476;&#22312;&#26041;&#24046;&#19978;&#37117;&#26159;&#31561;&#25928;&#30340;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#35782;&#21035;&#26356;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36793;&#30028;&#28151;&#27788;&#29366;&#24577;&#65292;&#24182;&#35843;&#25972;&#23427;&#20204;&#30340;&#20020;&#30028;&#24615;&#65292;&#20197;&#22686;&#24378;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The infinitely wide neural network has been proven a useful and manageable mathematical model that enables the understanding of many phenomena appearing in deep learning. One example is the convergence of random deep networks to Gaussian processes that allows a rigorous analysis of the way the choice of activation function and network weights impacts the training dynamics. In this paper, we extend the seminal proof of Matthews et al. (2018) to a larger class of initial weight distributions (which we call PSEUDO-IID), including the established cases of IID and orthogonal weights, as well as the emerging low-rank and structured sparse settings celebrated for their computational speed-up benefits. We show that fully-connected and convolutional networks initialized with PSEUDO-IID distributions are all effectively equivalent up to their variance. Using our results, one can identify the Edge-of-Chaos for a broader class of neural networks and tune them at criticality in order to enhance the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#28909;&#22330;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#21644;&#37325;&#24314;&#25216;&#26415;&#65292;&#22312;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#20351;&#29992;&#23569;&#37327;&#20256;&#24863;&#22120;&#23454;&#29616;&#23578;&#26410;&#25171;&#21360;&#37096;&#20214;&#30340;&#28909;&#22330;&#39044;&#27979;&#21644;&#24615;&#33021;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.16125</link><description>&lt;p&gt;
&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#34180;&#22721;&#37096;&#20214;&#30340;&#22312;&#32447;&#28909;&#22330;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Thermal Field Prediction for Metal Additive Manufacturing of Thin Walls. (arXiv:2310.16125v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#28909;&#22330;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#21644;&#37325;&#24314;&#25216;&#26415;&#65292;&#22312;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#20351;&#29992;&#23569;&#37327;&#20256;&#24863;&#22120;&#23454;&#29616;&#23578;&#26410;&#25171;&#21360;&#37096;&#20214;&#30340;&#28909;&#22330;&#39044;&#27979;&#21644;&#24615;&#33021;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#19968;&#20010;&#23454;&#38469;&#38382;&#39064;&#65292;&#21363;&#24403;&#21482;&#26377;&#23569;&#25968;&#20256;&#24863;&#22120;&#21487;&#29992;&#26102;&#65292;&#22914;&#20309;&#22312;&#32447;&#39044;&#27979;&#23578;&#26410;&#25171;&#21360;&#30340;&#38646;&#20214;&#30340;&#28909;&#22330;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26144;&#23556;&#21644;&#37325;&#24314;&#30340;&#22312;&#32447;&#28909;&#22330;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#20013;&#36827;&#34892;&#22312;&#32447;&#24615;&#33021;&#25511;&#21046;&#12290;&#22522;&#20110;&#28201;&#24230;&#26354;&#32447;&#30340;&#30456;&#20284;&#24615;&#65288;&#19968;&#20010;&#28857;&#30340;&#28201;&#24230;&#26354;&#32447;&#30340;&#26354;&#32447;&#27573;&#65289;&#65292;&#28909;&#22330;&#26144;&#23556;&#24212;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#23578;&#26410;&#25171;&#21360;&#23618;&#19978;&#28857;&#30340;&#28201;&#24230;&#26354;&#32447;&#65292;&#35813;&#26354;&#32447;&#26159;&#30001;&#20043;&#21069;&#25171;&#21360;&#23618;&#19978;&#26576;&#20123;&#28857;&#30340;&#27979;&#37327;&#28201;&#24230;&#24471;&#21040;&#12290;&#21033;&#29992;&#21516;&#19968;&#23618;&#19978;&#20960;&#20010;&#28857;&#30340;&#27979;&#37327;/&#39044;&#27979;&#28201;&#24230;&#26354;&#32447;&#65292;&#28909;&#22330;&#37325;&#24314;&#25552;&#20986;&#20102;&#19968;&#20010;&#38477;&#38454;&#27169;&#22411;&#65288;ROM&#65289;&#26469;&#26500;&#24314;&#21516;&#19968;&#23618;&#19978;&#25152;&#26377;&#28857;&#30340;&#28201;&#24230;&#26354;&#32447;&#65292;&#20174;&#32780;&#26500;&#24314;&#25972;&#20010;&#23618;&#30340;&#28201;&#24230;&#22330;&#12290;ROM&#30340;&#35757;&#32451;&#26159;&#36890;&#36807;&#26497;&#38480;&#23398;&#20064;&#26426;&#31639;&#27861;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to study a practical issue in metal AM, i.e., how to predict the thermal field of yet-to-print parts online when only a few sensors are available. This work proposes an online thermal field prediction method using mapping and reconstruction, which could be integrated into a metal AM process for online performance control. Based on the similarity of temperature curves (curve segments of a temperature profile of one point), the thermal field mapping applies an artificial neural network to estimate the temperature curves of points on the yet-to-print layer from measured temperatures of certain points on the previously printed layer. With measured/predicted temperature profiles of several points on the same layer, the thermal field reconstruction proposes a reduced order model (ROM) to construct the temperature profiles of all points on the same layer, which could be used to build the temperature field of the entire layer. The training of ROM is performed with an extreme le
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#30340;&#36335;&#30001;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#31561;&#21464;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#33021;&#22815;&#25429;&#25417;&#36755;&#20837;&#22270;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23616;&#37096;&#25110;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15543</link><description>&lt;p&gt;
&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#36335;&#30001;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Symmetry-preserving graph attention network to solve routing problems at multiple resolutions. (arXiv:2310.15543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15543
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#30340;&#36335;&#30001;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#31561;&#21464;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#33021;&#22815;&#25429;&#25417;&#36755;&#20837;&#22270;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23616;&#37096;&#25110;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#21644;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#21512;&#29702;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#37117;&#27809;&#26377;&#23436;&#20840;&#23562;&#37325;TSP&#21644;VRP&#20013;&#20135;&#29983;&#30340;&#23545;&#31216;&#24615;&#65292;&#21253;&#25324;&#26059;&#36716;&#12289;&#24179;&#31227;&#12289;&#25490;&#21015;&#21644;&#32553;&#25918;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#23436;&#20840;&#31561;&#21464;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#22823;&#22411;&#21644;&#38271;&#36317;&#31163;&#22270;&#30340;&#24773;&#20917;&#65292;&#25429;&#25417;&#36755;&#20837;&#22270;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#65288;&#21363;&#20174;&#23616;&#37096;&#21040;&#20840;&#23616;&#20449;&#24687;&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#20197;&#21069;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#25552;&#21462;&#23616;&#37096;&#20449;&#24687;&#65292;&#21487;&#33021;&#23548;&#33268;&#23616;&#37096;&#25110;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20998;&#36776;&#29575;&#26041;&#26696;&#19982;&#31561;&#21464;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;mEGAT&#65289;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22522;&#20110;&#20302;&#32423;&#21035;&#21644;&#39640;&#32423;&#21035;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26368;&#20339;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Travelling Salesperson Problems (TSPs) and Vehicle Routing Problems (VRPs) have achieved reasonable improvement in accuracy and computation time with the adaptation of Machine Learning (ML) methods. However, none of the previous works completely respects the symmetries arising from TSPs and VRPs including rotation, translation, permutation, and scaling. In this work, we introduce the first-ever completely equivariant model and training to solve combinatorial problems. Furthermore, it is essential to capture the multiscale structure (i.e. from local to global information) of the input graph, especially for the cases of large and long-range graphs, while previous methods are limited to extracting only local information that can lead to a local or sub-optimal solution. To tackle the above limitation, we propose a Multiresolution scheme in combination with Equivariant Graph Attention network (mEGAT) architecture, which can learn the optimal route based on low-level and high-level graph res
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#30340;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#26469;&#26377;&#25928;&#22788;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15516</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#30340;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph Attention-based Deep Reinforcement Learning for solving the Chinese Postman Problem with Load-dependent costs. (arXiv:2310.15516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#30340;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#26469;&#26377;&#25928;&#22788;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#27169;&#22411;&#22312;&#35299;&#20915;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#26041;&#38754;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;DRL&#27714;&#35299;&#22120;&#36890;&#24120;&#26159;&#29992;&#26469;&#35299;&#20915;&#33410;&#28857;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#20363;&#22914;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;TSP&#65289;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20851;&#20110;&#24212;&#29992;&#31070;&#32463;&#26041;&#27861;&#26469;&#35299;&#20915;&#24359;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#20363;&#22914;&#20013;&#22269;&#37038;&#36882;&#21592;&#38382;&#39064;&#65288;CPP&#65289;&#65292;&#30340;&#30740;&#31350;&#21364;&#21313;&#20998;&#26377;&#38480;&#65292;&#22240;&#20026;&#19982;TSP&#30456;&#27604;&#65292;&#23427;&#20204;&#30340;&#35299;&#31354;&#38388;&#36890;&#24120;&#26356;&#21152;&#19981;&#35268;&#21017;&#21644;&#22797;&#26434;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;DRL&#26694;&#26550;&#65292;&#26469;&#35299;&#20915;&#24102;&#26377;&#36127;&#36733;&#30456;&#20851;&#25104;&#26412;&#65288;CPP-LC&#65289;&#30340;CPP&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36127;&#36733;&#32422;&#26463;&#30340;&#22797;&#26434;&#24359;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21019;&#26032;&#28857;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;CPP-LC&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#39034;&#24207;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;DRL&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21363;Arc-DRL&#27169;&#22411;&#65292;&#23427;&#30001;&#19968;&#20010;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#32452;&#25104;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;CPP-LC&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#20351;&#24471;DRL&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Deep reinforcement learning (DRL) models have shown promising results in solving routing problems. However, most DRL solvers are commonly proposed to solve node routing problems, such as the Traveling Salesman Problem (TSP). Meanwhile, there has been limited research on applying neural methods to arc routing problems, such as the Chinese Postman Problem (CPP), since they often feature irregular and complex solution spaces compared to TSP. To fill these gaps, this paper proposes a novel DRL framework to address the CPP with load-dependent costs (CPP-LC) (Corberan et al., 2018), which is a complex arc routing problem with load constraints. The novelty of our method is two-fold. First, we formulate the CPP-LC as a Markov Decision Process (MDP) sequential model. Subsequently, we introduce an autoregressive model based on DRL, namely Arc-DRL, consisting of an encoder and decoder to address the CPP-LC challenge effectively. Such a framework allows the DRL model to work efficiently 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#24072;&#29983;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36739;&#23569;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#12290;&#36890;&#36807;&#24212;&#29992;&#35813;&#26041;&#27861;&#20110;SAM&#21644;CLIP&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;SAM-CLIP&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;</title><link>http://arxiv.org/abs/2310.15308</link><description>&lt;p&gt;
SAM-CLIP: &#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#21512;&#24182;&#20026;&#35821;&#20041;&#21644;&#31354;&#38388;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding. (arXiv:2310.15308v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#24072;&#29983;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36739;&#23569;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#12290;&#36890;&#36807;&#24212;&#29992;&#35813;&#26041;&#27861;&#20110;SAM&#21644;CLIP&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;SAM-CLIP&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24320;&#21487;&#29992;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65288;VFMs&#65289;&#30340;&#39046;&#22495;&#65292;&#22914;CLIP&#21644;Segment Anything Model&#65288;SAM&#65289;&#65292;&#27491;&#22312;&#36805;&#36895;&#25193;&#22823;&#12290;VFMs&#20855;&#26377;&#28304;&#33258;&#23427;&#20204;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#19981;&#21516;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;CLIP&#22312;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;SAM&#19987;&#27880;&#20110;&#20998;&#21106;&#30340;&#31354;&#38388;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;VFMs&#39640;&#25928;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#21560;&#25910;&#23427;&#20204;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#24072;&#29983;&#33976;&#39311;&#12290;&#19982;&#20256;&#32479;&#30340;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#30456;&#27604;&#65292;&#36825;&#31181;&#31574;&#30053;&#20855;&#26377;&#26174;&#33879;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#23427;&#21482;&#38656;&#35201;&#26368;&#21021;&#29992;&#20110;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;SAM&#21644;CLIP&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;SAM-CLIP&#65306;&#23558;SAM&#21644;CLIP&#30340;&#20248;&#21183;&#34701;&#21512;&#20026;&#21333;&#19968;&#20027;&#24178;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
The landscape of publicly available vision foundation models (VFMs), such as CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed with distinct capabilities stemming from their pre-training objectives. For instance, CLIP excels in semantic understanding, while SAM specializes in spatial understanding for segmentation. In this work, we introduce a simple recipe to efficiently merge VFMs into a unified model that assimilates their expertise. Our proposed method integrates multi-task learning, continual learning techniques, and teacher-student distillation. This strategy entails significantly less computational cost compared to traditional multi-task training from scratch. Additionally, it only demands a small fraction of the pre-training datasets that were initially used to train individual models. By applying our method to SAM and CLIP, we derive SAM-CLIP: a unified model that amalgamates the strengths of SAM and CLIP into a single backbone, making it apt for ed
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19981;&#21516;&#24418;&#24577;&#26426;&#22120;&#20154;&#38388;&#23545;&#24212;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28436;&#31034;&#23454;&#29616;&#20102;&#20849;&#21516;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#65292;&#20351;&#24471;&#19981;&#21516;&#26426;&#22120;&#20154;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#36716;&#31227;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13458</link><description>&lt;p&gt;
&#19981;&#21516;&#24418;&#24577;&#26426;&#22120;&#20154;&#38388;&#30340;&#20219;&#21153;&#28436;&#31034;&#23398;&#20064;&#19982;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Correspondence learning between morphologically different robots through task demonstrations. (arXiv:2310.13458v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19981;&#21516;&#24418;&#24577;&#26426;&#22120;&#20154;&#38388;&#23545;&#24212;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28436;&#31034;&#23454;&#29616;&#20102;&#20849;&#21516;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#65292;&#20351;&#24471;&#19981;&#21516;&#26426;&#22120;&#20154;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#36716;&#31227;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#26426;&#22120;&#20154;&#22312;&#20854;&#26426;&#36523;&#12289;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#26041;&#38754;&#26377;&#30528;&#21508;&#31181;&#21508;&#26679;&#30340;&#24046;&#24322;&#12290;&#32771;&#34385;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#65292;&#29420;&#31435;&#22320;&#25945;&#23548;&#27599;&#20010;&#19981;&#21516;&#26426;&#22120;&#20154;&#30340;&#27599;&#20010;&#25216;&#33021;&#26159;&#20302;&#25928;&#19988;&#19981;&#21487;&#25193;&#23637;&#30340;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#19981;&#21516;&#26426;&#22120;&#20154;&#30340;&#24863;&#23448;&#36816;&#21160;&#31354;&#38388;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#37027;&#20040;&#25105;&#20204;&#21487;&#20197;&#26399;&#26395;&#22312;&#19968;&#20010;&#26426;&#22120;&#20154;&#19978;&#23398;&#20064;&#30340;&#25216;&#33021;&#21487;&#20197;&#26356;&#30452;&#25509;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#26426;&#22120;&#20154;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#20851;&#33410;&#25511;&#21046;&#30340;&#22266;&#23450;&#22522;&#24231;&#25805;&#32437;&#26426;&#22120;&#20154;&#21644;&#24046;&#21160;&#39537;&#21160;&#31227;&#21160;&#26426;&#22120;&#20154;&#20043;&#38388;&#23398;&#20064;&#23545;&#24212;&#20851;&#31995;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#35753;&#20004;&#20010;&#26426;&#22120;&#20154;&#36827;&#34892;&#25191;&#34892;&#30456;&#21516;&#20219;&#21153;&#30340;&#28436;&#31034;&#12290;&#22312;&#23398;&#20064;&#23545;&#24212;&#31574;&#30053;&#30340;&#21516;&#26102;&#24418;&#25104;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#12290;&#22312;&#36825;&#20010;&#21021;&#22987;&#23398;&#20064;&#38454;&#27573;&#20043;&#21518;&#65292;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26426;&#22120;&#20154;&#30340;&#26032;&#20219;&#21153;&#25191;&#34892;&#23601;&#36275;&#20197;&#29983;&#25104;&#19968;&#20010;&#28508;&#21464;&#37327;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24212;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We observe a large variety of robots in terms of their bodies, sensors, and actuators. Given the commonalities in the skill sets, teaching each skill to each different robot independently is inefficient and not scalable when the large variety in the robotic landscape is considered. If we can learn the correspondences between the sensorimotor spaces of different robots, we can expect a skill that is learned in one robot can be more directly and easily transferred to the other robots. In this paper, we propose a method to learn correspondences between robots that have significant differences in their morphologies: a fixed-based manipulator robot with joint control and a differential drive mobile robot. For this, both robots are first given demonstrations that achieve the same tasks. A common latent representation is formed while learning the corresponding policies. After this initial learning stage, the observation of a new task execution by one robot becomes sufficient to generate a lat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;Frank-Wolfe&#31639;&#27861;&#65292;&#25913;&#36827;&#20102;Metarounding&#31639;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#22312;&#32452;&#21512;&#31867;&#38382;&#39064;&#19978;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.12629</link><description>&lt;p&gt;
&#36890;&#36807;Frank-Wolfe&#31639;&#27861;&#25913;&#36827;&#30340;Metarounding&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Improved Metarounding Algorithm via Frank-Wolfe. (arXiv:2310.12629v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12629
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;Frank-Wolfe&#31639;&#27861;&#65292;&#25913;&#36827;&#20102;Metarounding&#31639;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#22312;&#32452;&#21512;&#31867;&#38382;&#39064;&#19978;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Metarounding&#26159;&#19968;&#31181;&#23558;&#32447;&#24615;&#20248;&#21270;&#30340;&#36817;&#20284;&#31639;&#27861;&#36716;&#21270;&#20026;&#21516;&#19968;&#31867;&#30340;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Metarounding&#31639;&#27861;&#65292;&#22522;&#20110;&#19968;&#20010;&#23545;&#20110;&#32452;&#21512;&#31867;&#23384;&#22312;&#22522;&#20110;&#26494;&#24347;&#30340;&#36817;&#20284;&#31639;&#27861;&#30340;&#33258;&#28982;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#26041;&#38754;&#37117;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metarounding is an approach to convert an approximation algorithm for linear optimization over some combinatorial classes to an online linear optimization algorithm for the same class. We propose a new metarounding algorithm under a natural assumption that a relax-based approximation algorithm exists for the combinatorial class. Our algorithm is much more efficient in both theoretical and practical aspects.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#23384;&#22312;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#23545;&#31572;&#26696;&#20915;&#31574;&#21644;&#26684;&#24335;&#20559;&#22909;&#36127;&#36131;&#12290;&#23545;&#40784;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#28151;&#28102;&#12290;</title><link>http://arxiv.org/abs/2310.11732</link><description>&lt;p&gt;
&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#30740;&#31350;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting. (arXiv:2310.11732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#23384;&#22312;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#23545;&#31572;&#26696;&#20915;&#31574;&#21644;&#26684;&#24335;&#20559;&#22909;&#36127;&#36131;&#12290;&#23545;&#40784;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20542;&#21521;&#20110;&#19982;&#39044;&#35757;&#32451;&#30340;LM&#30456;&#27604;&#65292;&#22312;&#36755;&#20986;&#31572;&#26696;&#26102;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#23545;&#40784;&#36807;&#31243;&#23545;&#22810;&#36873;&#35774;&#32622;&#19979;LM&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#23545;&#40784;LM&#22312;&#26657;&#20934;&#26041;&#38754;&#19982;&#20854;&#39044;&#35757;&#32451;&#23545;&#24212;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#36827;&#34892;&#20102;&#35748;&#30495;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#65292;LM&#23384;&#22312;&#20004;&#31181;&#26126;&#26174;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#36127;&#36131;&#31572;&#26696;&#20915;&#31574;&#21644;LM&#30340;&#26684;&#24335;&#20559;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#31616;&#21333;&#30340;&#21512;&#25104;&#23545;&#40784;&#26041;&#26696;&#20013;&#36827;&#34892;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#22312;&#23545;&#40784;LM&#30340;&#26657;&#20934;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#23545;&#40784;LM&#36807;&#24230;&#33258;&#20449;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#28151;&#28102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;&#24120;&#35265;&#30340;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant progress made in practical applications of aligned language models (LMs), they tend to be overconfident in output answers compared to the corresponding pre-trained LMs. In this work, we systematically evaluate the impact of the alignment process on logit-based uncertainty calibration of LMs under the multiple-choice setting. We first conduct a thoughtful empirical study on how aligned LMs differ in calibration from their pre-trained counterparts. Experimental results reveal that there are two distinct uncertainties in LMs under the multiple-choice setting, which are responsible for the answer decision and the format preference of the LMs, respectively. Then, we investigate the role of these two uncertainties on aligned LM's calibration through fine-tuning in simple synthetic alignment schemes and conclude that one reason for aligned LMs' overconfidence is the conflation of these two types of uncertainty. Furthermore, we examine the utility of common post-hoc cal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#21453;&#23556;&#31561;&#21464;&#24615;&#30340;&#25193;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21516;&#20301;&#32032;&#26059;&#36716;&#20809;&#35889;&#20013;&#27979;&#23450;&#26377;&#26426;&#20998;&#23376;&#30340;&#19977;&#32500;&#32467;&#26500;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#30001;&#20110;&#32570;&#22833;&#27491;&#36127;&#31526;&#21495;&#32780;&#38590;&#20197;&#30830;&#23450;&#23454;&#38469;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.11609</link><description>&lt;p&gt;
&#20174;&#22825;&#28982;&#23384;&#22312;&#30340;&#21516;&#20301;&#32032;&#26059;&#36716;&#20809;&#35889;&#20013;&#30340;&#19977;&#32500;&#32467;&#26500;&#27979;&#23450;&#26469;&#30475;&#65292;&#20855;&#26377;&#21453;&#23556;&#31561;&#21464;&#24615;&#30340;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reflection-Equivariant Diffusion for 3D Structure Determination from Isotopologue Rotational Spectra in Natural Abundance. (arXiv:2310.11609v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#21453;&#23556;&#31561;&#21464;&#24615;&#30340;&#25193;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21516;&#20301;&#32032;&#26059;&#36716;&#20809;&#35889;&#20013;&#27979;&#23450;&#26377;&#26426;&#20998;&#23376;&#30340;&#19977;&#32500;&#32467;&#26500;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#30001;&#20110;&#32570;&#22833;&#27491;&#36127;&#31526;&#21495;&#32780;&#38590;&#20197;&#30830;&#23450;&#23454;&#38469;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#27979;&#23450;&#23545;&#20110;&#35782;&#21035;&#26410;&#30693;&#30340;&#26377;&#26426;&#20998;&#23376;&#26159;&#24517;&#35201;&#30340;&#65292;&#20363;&#22914;&#22825;&#28982;&#20135;&#29289;&#12289;&#27861;&#21307;&#26679;&#26412;&#12289;&#26143;&#38469;&#20171;&#36136;&#21644;&#23454;&#39564;&#23460;&#21512;&#25104;&#29289;&#31561;&#12290;&#26059;&#36716;&#20809;&#35889;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#23567;&#26377;&#26426;&#20998;&#23376;&#30340;&#24815;&#37327;&#30697;&#26469;&#36827;&#34892;&#32467;&#26500;&#27979;&#23450;&#65292;&#20174;&#32780;&#25552;&#20379;&#31934;&#30830;&#30340;&#19977;&#32500;&#20449;&#24687;&#12290;&#21033;&#29992;&#36825;&#20123;&#24815;&#37327;&#30697;&#65292;Kraitchman&#20998;&#26512;&#30830;&#23450;&#21516;&#20301;&#32032;&#32622;&#25442;&#22352;&#26631;&#65292;&#36825;&#20123;&#22352;&#26631;&#26159;&#20855;&#26377;&#22825;&#28982;&#21516;&#20301;&#32032;&#20016;&#24230;&#30340;&#25152;&#26377;&#21407;&#23376;&#30340;&#26080;&#31526;&#21495;|x|&#12289;|y|&#21644;|z|&#22352;&#26631;&#65292;&#21253;&#25324;&#30899;&#12289;&#27694;&#21644;&#27687;&#12290;&#34429;&#28982;&#26080;&#31526;&#21495;&#30340;&#32622;&#25442;&#22352;&#26631;&#21487;&#20197;&#39564;&#35777;&#32467;&#26500;&#30340;&#29468;&#27979;&#65292;&#20294;&#26159;&#32570;&#22833;&#30340;&#27491;&#36127;&#31526;&#21495;&#20351;&#24471;&#20165;&#20973;&#32622;&#25442;&#22352;&#26631;&#38590;&#20197;&#30830;&#23450;&#23454;&#38469;&#30340;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#36870;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;KREED&#65288;Kraitchman&#20855;&#26377;&#21453;&#23556;&#31561;&#21464;&#24615;&#30340;&#25193;&#25955;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20998;&#23376;&#30340;&#20998;&#23376;&#24335;&#12289;&#24815;&#37327;&#30697;&#21644;&#26080;&#31526;&#21495;&#30340;&#32622;&#25442;&#22352;&#26631;&#20013;&#25512;&#26029;&#20986;&#20998;&#23376;&#30340;&#23436;&#25972;&#19977;&#32500;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure determination is necessary to identify unknown organic molecules, such as those in natural products, forensic samples, the interstellar medium, and laboratory syntheses. Rotational spectroscopy enables structure determination by providing accurate 3D information about small organic molecules via their moments of inertia. Using these moments, Kraitchman analysis determines isotopic substitution coordinates, which are the unsigned $|x|,|y|,|z|$ coordinates of all atoms with natural isotopic abundance, including carbon, nitrogen, and oxygen. While unsigned substitution coordinates can verify guesses of structures, the missing $+/-$ signs make it challenging to determine the actual structure from the substitution coordinates alone. To tackle this inverse problem, we develop KREED (Kraitchman REflection-Equivariant Diffusion), a generative diffusion model that infers a molecule's complete 3D structure from its molecular formula, moments of inertia, and unsigned substitution coordi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#20462;&#21098;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#36825;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.08782</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#39537;&#21160;&#29983;&#20135;&#21147;&#65306;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning. (arXiv:2310.08782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#20462;&#21098;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#36825;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20294;&#21516;&#26102;&#20063;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#22522;&#30784;&#35774;&#26045;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#20462;&#21098;&#65288;DP&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#20986;&#29616;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#21024;&#38500;&#20887;&#20313;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;DP&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#19979;&#28216;&#30446;&#26631;&#20219;&#21153;&#20013;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#21644;&#23436;&#25972;&#24494;&#35843;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20462;&#21098;&#28304;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36801;&#31227;&#23398;&#20064;&#30340;DP&#38382;&#39064;&#20173;&#28982;&#26410;&#35299;&#20915;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;DP&#21644;&#36801;&#31227;&#23398;&#20064;&#35270;&#20026;&#29420;&#31435;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#23558;DP&#19982;&#36801;&#31227;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;DP&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;DP&#26041;&#27861;&#65292;&#21363;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#65292;&#29992;&#20110;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive data is often considered essential for deep learning applications, but it also incurs significant computational and infrastructural costs. Therefore, dataset pruning (DP) has emerged as an effective way to improve data efficiency by identifying and removing redundant training samples without sacrificing performance. In this work, we aim to address the problem of DP for transfer learning, i.e., how to prune a source dataset for improved pretraining efficiency and lossless finetuning accuracy on downstream target tasks. To our best knowledge, the problem of DP for transfer learning remains open, as previous studies have primarily addressed DP and transfer learning as separate problems. By contrast, we establish a unified viewpoint to integrate DP with transfer learning and find that existing DP methods are not suitable for the transfer learning paradigm. We then propose two new DP methods, label mapping and feature mapping, for supervised and self-supervised pretraining settings 
&lt;/p&gt;</description></item><item><title>Lag-Llama&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35757;&#32451;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#27169;&#22411;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#25193;&#23637;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.08278</link><description>&lt;p&gt;
Lag-Llama: &#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lag-Llama: Towards Foundation Models for Time Series Forecasting. (arXiv:2310.08278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08278
&lt;/p&gt;
&lt;p&gt;
Lag-Llama&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35757;&#32451;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#27169;&#22411;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#25193;&#23637;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#24182;&#30740;&#31350;&#20854;&#25193;&#23637;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#20013;&#30340; Lag-Llama &#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#8220;&#20998;&#24067;&#22806;&#8221;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#20248;&#20110;&#26377;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#25193;&#23637;&#34892;&#20026;&#12290;&#24320;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/kashif/pytorch-transformer-ts &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming to build foundation models for time-series forecasting and study their scaling behavior, we present here our work-in-progress on Lag-Llama, a general-purpose univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The model shows good zero-shot prediction capabilities on unseen "out-of-distribution" time-series datasets, outperforming supervised baselines. We use smoothly broken power-laws to fit and predict model scaling behavior. The open source code is made available at https://github.com/kashif/pytorch-transformer-ts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26080;&#38480;&#23485;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#36830;&#25509;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;/&#26680;&#26041;&#27861;&#65292;&#30740;&#31350;&#25512;&#24191;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#38381;&#24335;&#24418;&#24335;&#30340;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22238;&#24402;/&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.08176</link><description>&lt;p&gt;
&#26080;&#38480;&#23485;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33410;&#28857;&#22238;&#24402;/&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Infinite Width Graph Neural Networks for Node Regression/ Classification. (arXiv:2310.08176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26080;&#38480;&#23485;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#36830;&#25509;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;/&#26680;&#26041;&#27861;&#65292;&#30740;&#31350;&#25512;&#24191;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#38381;&#24335;&#24418;&#24335;&#30340;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22238;&#24402;/&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#27599;&#20010;&#20840;&#36830;&#25509;&#23618;&#30340;&#33410;&#28857;&#25968;&#37327;&#36235;&#36817;&#26080;&#31351;&#22823;&#26102;&#65292;&#23427;&#26159;&#23545;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#20840;&#36830;&#25509;&#28145;&#24230;&#31070;&#32463;&#32593;&#30340;&#19968;&#31181;&#25512;&#24191;&#12290;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#30456;&#36830;&#25509;&#65292;&#21518;&#32773;&#37117;&#26159;&#20855;&#26377;&#24736;&#20037;&#20256;&#32479;&#21644;&#20016;&#23500;&#29702;&#35770;&#22522;&#30784;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#30340;&#36229;&#21442;&#25968;&#36739;&#23569;&#65292;&#21487;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20351;&#20854;&#22312;&#24212;&#29992;&#20013;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#12290;&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#23558;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#36830;&#25509;&#30340;&#30740;&#31350;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#30340;&#36235;&#21183;&#12290;&#23545;&#20110;&#22810;&#31181;&#26550;&#26500;&#65288;&#21253;&#25324;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#25512;&#23548;&#20986;&#20102;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#38381;&#24335;&#24418;&#24335;&#12290;&#23545;&#36825;&#20123;&#26550;&#26500;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#22238;&#24402;/&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work analyzes Graph Neural Networks, a generalization of Fully-Connected Deep Neural Nets on Graph structured data, when their width, that is the number of nodes in each fullyconnected layer is increasing to infinity. Infinite Width Neural Networks are connecting Deep Learning to Gaussian Processes and Kernels, both Machine Learning Frameworks with long traditions and extensive theoretical foundations. Gaussian Processes and Kernels have much less hyperparameters then Neural Networks and can be used for uncertainty estimation, making them more user friendly for applications. This works extends the increasing amount of research connecting Gaussian Processes and Kernels to Neural Networks. The Kernel and Gaussian Process closed forms are derived for a variety of architectures, namely the standard Graph Neural Network, the Graph Neural Network with Skip-Concatenate Connections and the Graph Attention Neural Network. All architectures are evaluated on a variety of datasets on the task
&lt;/p&gt;</description></item><item><title>FedMFS&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#27169;&#24577;&#36890;&#20449;&#35299;&#20915;&#20102;&#32570;&#20047;&#29305;&#23450;&#27169;&#24577;&#30340;&#24322;&#26500;&#23458;&#25143;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26368;&#20248;&#30340;&#27169;&#24577;&#19978;&#20256;&#31574;&#30053;&#20197;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07048</link><description>&lt;p&gt;
FedMFS: &#36873;&#25321;&#24615;&#27169;&#24577;&#36890;&#20449;&#30340;&#32852;&#37030;&#22810;&#27169;&#24577;&#34701;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedMFS: Federated Multimodal Fusion Learning with Selective Modality Communication. (arXiv:2310.07048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07048
&lt;/p&gt;
&lt;p&gt;
FedMFS&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#27169;&#24577;&#36890;&#20449;&#35299;&#20915;&#20102;&#32570;&#20047;&#29305;&#23450;&#27169;&#24577;&#30340;&#24322;&#26500;&#23458;&#25143;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26368;&#20248;&#30340;&#27169;&#24577;&#19978;&#20256;&#31574;&#30053;&#20197;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#20165;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#32780;&#19981;&#35775;&#38382;&#12289;&#20405;&#29359;&#25110;&#27844;&#38706;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#20351;&#23458;&#25143;&#33021;&#22815;&#21512;&#20316;&#12290;&#22312;&#29289;&#32852;&#32593;&#20013;&#65292;&#36793;&#32536;&#35774;&#22791;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#32452;&#21512;&#21644;&#34701;&#21512;&#33539;&#24335;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;&#19968;&#65289;&#35299;&#20915;&#30001;&#20110;&#32570;&#20047;&#29305;&#23450;&#27169;&#24577;&#30340;&#24322;&#26500;&#23458;&#25143;&#24341;&#36215;&#30340;&#38382;&#39064;&#65307;&#65288;&#20108;&#65289;&#35774;&#35745;&#19968;&#31181;&#26368;&#20248;&#30340;&#27169;&#24577;&#19978;&#20256;&#31574;&#30053;&#65292;&#20197;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#21516;&#26102;&#26368;&#22823;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;FedMFS&#65292;&#21487;&#20197;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;Shapley&#20540;&#26469;&#37327;&#21270;&#27599;&#20010;&#27169;&#24577;&#30340;&#36129;&#29486;&#21644;&#27169;&#24577;&#27169;&#22411;&#22823;&#23567;&#26469;&#34913;&#37327;&#36890;&#20449;&#24320;&#38144;&#65292;&#20197;&#20415;&#27599;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning (ML) paradigm that enables clients to collaborate without accessing, infringing upon, or leaking original user data by sharing only model parameters. In the Internet of Things (IoT), edge devices are increasingly leveraging multimodal data compositions and fusion paradigms to enhance model performance. However, in FL applications, two main challenges remain open: (i) addressing the issues caused by heterogeneous clients lacking specific modalities and (ii) devising an optimal modality upload strategy to minimize communication overhead while maximizing learning performance. In this paper, we propose Federated Multimodal Fusion learning with Selective modality communication (FedMFS), a new multimodal fusion FL methodology that can tackle the above mentioned challenges. The key idea is to utilize Shapley values to quantify each modality's contribution and modality model size to gauge communication overhead, so that each client can 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#36866;&#24212;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#30340;&#20449;&#21495;&#24674;&#22797;&#65292;&#36890;&#36807;&#20248;&#21270;&#37319;&#26679;&#20998;&#24067;&#21644;&#26032;&#30340;&#29702;&#35770;&#24674;&#22797;&#20445;&#35777;&#25216;&#26415;&#65292;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#27979;&#37327;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.04984</link><description>&lt;p&gt;
&#27169;&#22411;&#36866;&#24212;&#30340;&#20613;&#31435;&#21494;&#37319;&#26679;&#29992;&#20110;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Model-adapted Fourier sampling for generative compressed sensing. (arXiv:2310.04984v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04984
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#36866;&#24212;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#20013;&#30340;&#20449;&#21495;&#24674;&#22797;&#65292;&#36890;&#36807;&#20248;&#21270;&#37319;&#26679;&#20998;&#24067;&#21644;&#26032;&#30340;&#29702;&#35770;&#24674;&#22797;&#20445;&#35777;&#25216;&#26415;&#65292;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#27979;&#37327;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#27979;&#37327;&#30697;&#38453;&#26159;&#20174;&#19968;&#20010;&#37193;&#30697;&#38453;&#20013;&#38543;&#26426;&#23376;&#37319;&#26679;&#24471;&#21040;&#26102;&#30340;&#29983;&#25104;&#24335;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65288;&#31163;&#25955;&#20613;&#31435;&#21494;&#21464;&#25442;&#26159;&#37325;&#35201;&#30340;&#29305;&#27530;&#24773;&#20917;&#65289;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#27599;&#20010;&#20613;&#31435;&#21494;&#21521;&#37327;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#21462;&#20540;&#33539;&#22260;&#23545;&#40784;&#26102;&#65292;&#21482;&#38656;&#35201;$O(kdn\|\boldsymbol{\alpha}\|_{\infty}^{2})$&#20010;&#22343;&#21248;&#38543;&#26426;&#20613;&#31435;&#21494;&#27979;&#37327;&#23601;&#36275;&#20197;&#24674;&#22797;&#36755;&#20986;&#20449;&#21495;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#36866;&#24212;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#24471;&#21040;&#20102;&#25913;&#36827;&#65292;&#21482;&#38656;&#35201;$O(kd\|\boldsymbol{\alpha}\|_{2}^{2})$&#20010;&#27979;&#37327;&#12290;&#36825;&#26159;&#36890;&#36807;&#20197;&#19979;&#27493;&#39588;&#23454;&#29616;&#30340;&#65306;&#65288;1&#65289;&#25105;&#20204;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#38750;&#22343;&#21248;&#38543;&#26426;&#37319;&#26679;&#20998;&#24067;  &#30340;&#26032;&#30340;&#29702;&#35770;&#24674;&#22797;&#20445;&#35777;&#65292;&#28982;&#21518;&#65288;2&#65289;&#20248;&#21270;&#37319;&#26679;&#20998;&#24067;&#20197;&#26368;&#23567;&#21270;&#36825;&#20123;&#20445;&#35777;&#25152;&#38656;&#30340;&#27979;&#37327;&#25968;&#37327;&#12290;&#36825;&#31181;&#25216;&#26415;&#21457;&#23637;&#25552;&#20379;&#20102;&#36866;&#29992;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study generative compressed sensing when the measurement matrix is randomly subsampled from a unitary matrix (with the DFT as an important special case). It was recently shown that $\textit{O}(kdn\| \boldsymbol{\alpha}\|_{\infty}^{2})$ uniformly random Fourier measurements are sufficient to recover signals in the range of a neural network $G:\mathbb{R}^k \to \mathbb{R}^n$ of depth $d$, where each component of the so-called local coherence vector $\boldsymbol{\alpha}$ quantifies the alignment of a corresponding Fourier vector with the range of $G$. We construct a model-adapted sampling strategy with an improved sample complexity of $\textit{O}(kd\| \boldsymbol{\alpha}\|_{2}^{2})$ measurements. This is enabled by: (1) new theoretical recovery guarantees that we develop for nonuniformly random sampling distributions and then (2) optimizing the sampling distribution to minimize the number of measurements needed for these guarantees. This development offers a sample complexity applicable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RDAC&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35299;&#21078;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#20960;&#31181;&#24120;&#29992;&#31639;&#27861;&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.04741</link><description>&lt;p&gt;
&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#24179;&#34913;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#65306;&#28608;&#27963;&#21464;&#21270;&#30340;&#35835;&#20986;&#20998;&#35299;&#65288;RDAC&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework. (arXiv:2310.04741v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RDAC&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35299;&#21078;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#20960;&#31181;&#24120;&#29992;&#31639;&#27861;&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#20013;&#24515;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23545;&#36825;&#31181;&#24179;&#34913;&#36827;&#34892;&#20102;&#35299;&#21078;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;&#28608;&#27963;&#21464;&#21270;&#30340;&#35835;&#20986;&#20998;&#35299;&#65288;RDAC&#65289;&#26694;&#26550;&#39318;&#20808;&#35299;&#20915;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#22256;&#22659;&#21450;&#20854;&#19982;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#20851;&#31995;&#12290;&#23427;&#23558;&#23398;&#20064;&#35825;&#23548;&#30340;&#28608;&#27963;&#21464;&#21270;&#19982;&#20808;&#21069;&#35835;&#20986;&#33539;&#22260;&#20869;&#30340;&#31283;&#23450;&#24615;&#31243;&#24230;&#21644;&#38646;&#31354;&#38388;&#30340;&#21464;&#21270;&#19982;&#21487;&#22609;&#24615;&#31243;&#24230;&#30456;&#20851;&#32852;&#12290;&#22312;&#22788;&#29702;&#20998;&#35010;CIFAR-110&#20219;&#21153;&#30340;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#20013;&#65292;&#35813;&#26694;&#26550;&#38416;&#26126;&#20102;&#24120;&#29992;&#27491;&#21017;&#21270;&#31639;&#27861;&#65288;SI&#12289;EWC&#21644;LwF&#65289;&#20197;&#21450;&#37325;&#25918;&#31639;&#27861;&#65288;GEM&#21644;&#25968;&#25454;&#37325;&#25918;&#65289;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) algorithms strive to acquire new knowledge while preserving prior information. However, this stability-plasticity trade-off remains a central challenge. This paper introduces a framework that dissects this trade-off, offering valuable insights into CL algorithms. The Readout-Decomposition of Activation Change (RDAC) framework first addresses the stability-plasticity dilemma and its relation to catastrophic forgetting. It relates learning-induced activation changes in the range of prior readouts to the degree of stability and changes in the null space to the degree of plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the framework clarifies the stability-plasticity trade-offs of the popular regularization algorithms Synaptic intelligence (SI), Elastic-weight consolidation (EWC), and learning without Forgetting (LwF), and replay-based algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay preserved stability and plast
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#65292;&#23398;&#20064;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#24449;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03358</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#36827;&#34892;&#40065;&#26834;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Representation Learning via Asymmetric Negative Contrast and Reverse Attention. (arXiv:2310.03358v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#65292;&#23398;&#20064;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#24449;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#22122;&#22768;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#34987;&#35777;&#26126;&#26159;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#20813;&#21463;&#27450;&#39575;&#30340;&#26368;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;AT&#24573;&#35270;&#20102;&#23398;&#20064;&#40065;&#26834;&#29305;&#24449;&#65292;&#23548;&#33268;&#23545;&#25239;&#40065;&#26834;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#40065;&#26834;&#34920;&#24449;&#30340;&#20004;&#20010;&#29305;&#24449;&#65306;&#65288;1&#65289;&#25490;&#20182;&#24615;&#65306;&#33258;&#28982;&#26679;&#26412;&#30340;&#29305;&#24449;&#36828;&#31163;&#20854;&#20182;&#31867;&#21035;&#30340;&#29305;&#24449;&#65307;&#65288;2&#65289;&#23545;&#40784;&#24615;&#65306;&#33258;&#28982;&#26679;&#26412;&#21644;&#30456;&#24212;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#29305;&#24449;&#24444;&#27492;&#25509;&#36817;&#12290;&#36825;&#20123;&#29305;&#28857;&#28608;&#21457;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;AT&#26694;&#26550;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#21644;&#21453;&#21521;&#27880;&#24847;&#21147;&#26469;&#33719;&#24471;&#40065;&#26834;&#30340;&#34920;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#39044;&#27979;&#27010;&#29575;&#30340;&#38750;&#23545;&#31216;&#36127;&#23545;&#27604;&#24230;&#65292;&#23558;&#29305;&#24449;&#31354;&#38388;&#20013;&#19981;&#21516;&#31867;&#21035;&#30340;&#26679;&#26412;&#25512;&#24320;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#21442;&#25968;&#23545;&#29305;&#24449;&#36827;&#34892;&#21152;&#26435;&#65292;&#20316;&#20026;&#21453;&#21521;&#27880;&#24847;&#21147;&#65292;&#20197;&#33719;&#24471;&#40065;&#26834;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are vulnerable to adversarial noise. Adversarial training (AT) has been demonstrated to be the most effective defense strategy to protect neural networks from being fooled. However, we find AT omits to learning robust features, resulting in poor performance of adversarial robustness. To address this issue, we highlight two characteristics of robust representation: (1) $\bf{exclusion}$: the feature of natural examples keeps away from that of other classes; (2) $\bf{alignment}$: the feature of natural and corresponding adversarial examples is close to each other. These motivate us to propose a generic framework of AT to gain robust representation, by the asymmetric negative contrast and reverse attention. Specifically, we design an asymmetric negative contrast based on predicted probabilities, to push away examples of different classes in the feature space. Moreover, we propose to weight feature by parameters of the linear classifier as the reverse attention, to obta
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#20803;&#36335;&#24452;&#21644;&#20803;&#36335;&#24452;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20851;&#38190;&#26159;&#20351;&#29992;&#35780;&#20998;&#20989;&#25968;&#26469;&#34913;&#37327;&#20851;&#31995;&#30340;&#28508;&#22312;&#20449;&#24687;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;GNNs&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17113</link><description>&lt;p&gt;
&#22810;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#36335;&#24452;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta-Path Learning for Multi-relational Graph Neural Networks. (arXiv:2309.17113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17113
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#20803;&#36335;&#24452;&#21644;&#20803;&#36335;&#24452;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20851;&#38190;&#26159;&#20351;&#29992;&#35780;&#20998;&#20989;&#25968;&#26469;&#34913;&#37327;&#20851;&#31995;&#30340;&#28508;&#22312;&#20449;&#24687;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;GNNs&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#20004;&#31181;&#31574;&#30053;&#26469;&#30830;&#23450;&#20449;&#24687;&#30456;&#20851;&#30340;&#20851;&#31995;&#65306;&#35201;&#20040;&#23558;&#36825;&#20010;&#38382;&#39064;&#31616;&#21270;&#20026;&#20302;&#32423;&#26435;&#37325;&#23398;&#20064;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#20851;&#31995;&#20381;&#36182;&#38142;&#65292;&#31216;&#20026;&#20803;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#21069;&#19968;&#31181;&#26041;&#27861;&#22312;&#23384;&#22312;&#22823;&#37327;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#30693;&#35782;&#22270;&#35889;&#65289;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#21518;&#19968;&#31181;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#30830;&#23450;&#30456;&#20851;&#30340;&#20803;&#36335;&#24452;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20803;&#36335;&#24452;&#21644;&#20803;&#36335;&#24452;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#32593;&#32476;&#22522;&#20110;&#23569;&#37327;&#26377;&#20449;&#24687;&#37327;&#30340;&#20803;&#36335;&#24452;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#19968;&#20010;&#35780;&#20998;&#20989;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#20803;&#36335;&#24452;&#30340;&#22686;&#37327;&#26500;&#24314;&#20013;&#20851;&#31995;&#30340;&#28508;&#22312;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#21363;&#20351;&#26377;&#22823;&#37327;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20173;&#33021;&#27491;&#30830;&#35782;&#21035;&#30456;&#20851;&#30340;&#20803;&#36335;&#24452;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22810;&#20851;&#31995;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing multi-relational graph neural networks use one of two strategies for identifying informative relations: either they reduce this problem to low-level weight learning, or they rely on handcrafted chains of relational dependencies, called meta-paths. However, the former approach faces challenges in the presence of many relations (e.g., knowledge graphs), while the latter requires substantial domain expertise to identify relevant meta-paths. In this work we propose a novel approach to learn meta-paths and meta-path GNNs that are highly accurate based on a small number of informative meta-paths. Key element of our approach is a scoring function for measuring the potential informativeness of a relation in the incremental construction of the meta-path. Our experimental evaluation shows that the approach manages to correctly identify relevant meta-paths even with a large number of relations, and substantially outperforms existing multi-relational GNNs on synthetic and real-world exper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#39044;&#27979;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;FD&#24212;&#29992;&#20110;&#37329;&#34701;&#25968;&#25454;&#24182;&#32467;&#21512;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;FD&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#25972;&#25968;&#24046;&#20998;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.13409</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#25968;&#25454;&#37322;&#25918;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Time-Series Forecasting: Unleashing Long-Term Dependencies with Fractionally Differenced Data. (arXiv:2309.13409v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#39044;&#27979;&#31574;&#30053;&#12290;&#36890;&#36807;&#23558;FD&#24212;&#29992;&#20110;&#37329;&#34701;&#25968;&#25454;&#24182;&#32467;&#21512;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;FD&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#25972;&#25968;&#24046;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#27979;&#31574;&#30053;&#65292;&#21033;&#29992;&#20998;&#25968;&#24046;&#20998;&#65288;FD&#65289;&#30340;&#33021;&#21147;&#26469;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#19982;&#20256;&#32479;&#30340;&#25972;&#25968;&#24046;&#20998;&#26041;&#27861;&#19981;&#21516;&#65292;FD&#22312;&#20445;&#25345;&#31995;&#21015;&#35760;&#24518;&#30340;&#21516;&#26102;&#31283;&#23450;&#20102;&#23427;&#20197;&#20379;&#24314;&#27169;&#30446;&#30340;&#12290;&#36890;&#36807;&#23558;FD&#24212;&#29992;&#20110;&#26469;&#33258;SPY&#25351;&#25968;&#30340;&#37329;&#34701;&#25968;&#25454;&#65292;&#24182;&#32467;&#21512;&#26032;&#38395;&#25253;&#36947;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36825;&#20010;&#23454;&#35777;&#20998;&#26512;&#25506;&#35752;&#20102;FD&#19982;&#30446;&#26631;&#21464;&#37327;&#30340;&#20108;&#20803;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;&#37319;&#29992;&#20102;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#26469;&#39564;&#35777;FD&#31995;&#21015;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;FD&#30456;&#27604;&#25972;&#25968;&#24046;&#20998;&#20855;&#26377;&#20248;&#36234;&#24615;&#65292;&#36825;&#19968;&#28857;&#36890;&#36807;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24449;/&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;ROCAUC&#65289;&#21644;&#39532;&#20462;&#26031;&#30456;&#20851;&#31995;&#25968;&#65288;MCC&#65289;&#30340;&#35780;&#20272;&#24471;&#21040;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces a novel forecasting strategy that leverages the power of fractional differencing (FD) to capture both short- and long-term dependencies in time series data. Unlike traditional integer differencing methods, FD preserves memory in series while stabilizing it for modeling purposes. By applying FD to financial data from the SPY index and incorporating sentiment analysis from news reports, this empirical analysis explores the effectiveness of FD in conjunction with binary classification of target variables. Supervised classification algorithms were employed to validate the performance of FD series. The results demonstrate the superiority of FD over integer differencing, as confirmed by Receiver Operating Characteristic/Area Under the Curve (ROCAUC) and Mathews Correlation Coefficient (MCC) evaluations.
&lt;/p&gt;</description></item><item><title>SALSA-CLRS&#26159;&#19968;&#31181;&#31232;&#30095;&#19988;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#65292;&#20248;&#20808;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#34920;&#31034;&#30340;&#21033;&#29992;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#36866;&#24212;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;CLRS&#22522;&#20934;&#20013;&#20869;&#23384;&#38656;&#27714;&#39640;&#21644;&#36816;&#34892;&#26102;&#38388;&#38590;&#20197;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12253</link><description>&lt;p&gt;
SALSA-CLRS:&#19968;&#31181;&#31232;&#30095;&#19988;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning. (arXiv:2309.12253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12253
&lt;/p&gt;
&lt;p&gt;
SALSA-CLRS&#26159;&#19968;&#31181;&#31232;&#30095;&#19988;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#65292;&#20248;&#20808;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#34920;&#31034;&#30340;&#21033;&#29992;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#36866;&#24212;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;CLRS&#22522;&#20934;&#20013;&#20869;&#23384;&#38656;&#27714;&#39640;&#21644;&#36816;&#34892;&#26102;&#38388;&#38590;&#20197;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CLRS&#31639;&#27861;&#23398;&#20064;&#22522;&#20934;&#30340;&#25193;&#23637;&#65292;&#20248;&#20808;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#34920;&#31034;&#30340;&#21033;&#29992;&#12290;CLRS&#20013;&#30340;&#35768;&#22810;&#31639;&#27861;&#38656;&#35201;&#20840;&#23616;&#23384;&#20648;&#22120;&#25110;&#20449;&#24687;&#20132;&#25442;&#65292;&#22312;&#20854;&#25191;&#34892;&#27169;&#22411;&#20013;&#38236;&#20687;&#34920;&#36798;&#20026;&#22522;&#20110;&#24213;&#23618;&#38382;&#39064;&#26500;&#24314;&#23436;&#20840;&#36830;&#25509;&#65288;&#32780;&#38750;&#31232;&#30095;&#65289;&#22270;&#30340;&#25805;&#20316;&#12290;&#23613;&#31649;CLRS&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#23398;&#20064;&#31639;&#27861;&#22312;&#26356;&#22823;&#23454;&#20363;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#29616;&#26377;&#30340;&#25191;&#34892;&#27169;&#22411;&#30001;&#20110;&#20854;&#35201;&#27714;&#39640;&#30340;&#20869;&#23384;&#38656;&#27714;&#21644;&#36816;&#34892;&#26102;&#38388;&#32780;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65288;&#38590;&#20197;&#25193;&#23637;&#65289;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#37325;&#35201;&#30340;&#31639;&#27861;&#24182;&#19981;&#38656;&#35201;&#23436;&#20840;&#36830;&#25509;&#30340;&#22270;&#65307;&#36825;&#20123;&#20027;&#35201;&#20998;&#24067;&#24335;&#31639;&#27861;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#37319;&#29992;&#30340;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SALSA-CLRS&#65292;&#19968;&#20010;&#19987;&#38376;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#24615;&#30340;CLRS&#22522;&#20934;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20174;&#21407;&#22987;CLRS&#22522;&#20934;&#20013;&#25913;&#32534;&#30340;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an extension to the CLRS algorithmic learning benchmark, prioritizing scalability and the utilization of sparse representations. Many algorithms in CLRS require global memory or information exchange, mirrored in its execution model, which constructs fully connected (not sparse) graphs based on the underlying problem. Despite CLRS's aim of assessing how effectively learned algorithms can generalize to larger instances, the existing execution model becomes a significant constraint due to its demanding memory requirements and runtime (hard to scale). However, many important algorithms do not demand a fully connected graph; these algorithms, primarily distributed in nature, align closely with the message-passing paradigm employed by Graph Neural Networks. Hence, we propose SALSA-CLRS, an extension of the current CLRS benchmark specifically with scalability and sparseness in mind. Our approach includes adapted algorithms from the original CLRS benchmark and introduces new probl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.10003</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19987;&#21033;&#26435;&#35201;&#27714;&#30340;&#33539;&#22260;&#27979;&#37327;&#20026;&#35813;&#35201;&#27714;&#25152;&#21253;&#21547;&#30340;&#33258;&#20449;&#24687;&#30340;&#20498;&#25968;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20449;&#24687;&#35770;&#65292;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#32597;&#35265;&#30340;&#27010;&#24565;&#27604;&#24179;&#24120;&#30340;&#27010;&#24565;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#22240;&#20026;&#23427;&#26356;&#20196;&#20154;&#24778;&#35766;&#12290;&#33258;&#20449;&#24687;&#26159;&#20174;&#35813;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#35745;&#31639;&#24471;&#20986;&#30340;&#65292;&#20854;&#20013;&#27010;&#29575;&#26159;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20116;&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#65288;&#27599;&#20010;&#21333;&#35789;&#25110;&#23383;&#31526;&#22343;&#20174;&#22343;&#21248;&#20998;&#24067;&#20013;&#25277;&#21462;&#65289;&#21040;&#20013;&#31561;&#27169;&#22411;&#65288;&#20351;&#29992;&#24179;&#22343;&#35789;&#25110;&#23383;&#31526;&#39057;&#29575;&#65289;&#65292;&#20877;&#21040;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT2&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#33539;&#22260;&#24230;&#37327;&#20943;&#23569;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#65292;&#36825;&#26159;&#20808;&#21069;&#20316;&#21697;&#20013;&#24050;&#32463;&#20351;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20061;&#20010;&#31995;&#21015;&#30340;&#38024;&#23545;&#19981;&#21516;&#21457;&#26126;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#65292;&#20854;&#20013;&#27599;&#20010;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes to measure the scope of a patent claim as the reciprocal of the self-information contained in this claim. Grounded in information theory, this approach is based on the assumption that a rare concept is more informative than a usual concept, inasmuch as it is more surprising. The self-information is calculated from the probability of occurrence of that claim, where the probability is calculated in accordance with a language model. Five language models are considered, ranging from the simplest models (each word or character is drawn from a uniform distribution) to intermediate models (using average word or character frequencies), to a large language model (GPT2). Interestingly, the simplest language models reduce the scope measure to the reciprocal of the word or character count, a metric already used in previous works. Application is made to nine series of patent claims directed to distinct inventions, where the claims in each series have a gradually decreasing scope.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#21407;&#35821;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#22810;&#23610;&#24230;&#26597;&#25214;&#34920;&#20316;&#20026;&#22270;&#24418;&#31649;&#32447;&#20013;&#20256;&#32479;&#35745;&#31639;&#21644;&#25968;&#25454;&#25805;&#20316;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#23427;&#22312;&#22810;&#20010;&#22270;&#24418;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08387</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#38388;&#25509;&#24341;&#29992;&#30340;&#39640;&#25928;&#22270;&#24418;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Efficient Graphics Representation with Differentiable Indirection. (arXiv:2309.08387v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#21407;&#35821;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#22810;&#23610;&#24230;&#26597;&#25214;&#34920;&#20316;&#20026;&#22270;&#24418;&#31649;&#32447;&#20013;&#20256;&#32479;&#35745;&#31639;&#21644;&#25968;&#25454;&#25805;&#20316;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#23427;&#22312;&#22810;&#20010;&#22270;&#24418;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#24494;&#38388;&#25509;&#24341;&#29992;&#8212;&#8212;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#21407;&#35821;&#65292;&#23427;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#22810;&#23610;&#24230;&#26597;&#25214;&#34920;&#20316;&#20026;&#22270;&#24418;&#31649;&#32447;&#20013;&#20256;&#32479;&#35745;&#31639;&#21644;&#25968;&#25454;&#25805;&#20316;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22270;&#24418;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#65292;&#21253;&#25324;&#20960;&#20309;&#21644;&#22270;&#20687;&#34920;&#31034;&#12289;&#32441;&#29702;&#26144;&#23556;&#12289;&#30528;&#33394;&#21644;&#36752;&#23556;&#22330;&#34920;&#31034;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#21487;&#24494;&#38388;&#25509;&#24341;&#29992;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#26550;&#26500;&#20013;&#65292;&#24555;&#36895;&#35757;&#32451;&#65292;&#24182;&#20135;&#29983;&#22810;&#26679;&#19988;&#39640;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce differentiable indirection -- a novel learned primitive that employs differentiable multi-scale lookup tables as an effective substitute for traditional compute and data operations across the graphics pipeline. We demonstrate its flexibility on a number of graphics tasks, i.e., geometric and image representation, texture mapping, shading, and radiance field representation. In all cases, differentiable indirection seamlessly integrates into existing architectures, trains rapidly, and yields both versatile and efficient results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#29983;&#25104;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#35775;&#38382;&#65292;&#24182;&#22312;&#20248;&#21270;&#30005;&#36335;&#37197;&#32622;&#26102;&#21516;&#26102;&#32771;&#34385;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#38376;&#25104;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26368;&#20339;&#30005;&#36335;&#37197;&#32622;&#20013;&#32416;&#32544;&#38376;&#38656;&#35201;&#30456;&#24212;&#30340;&#25968;&#37327;&#65292;&#19982;&#20043;&#21069;&#30740;&#31350;&#30456;&#21453;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#25968;&#25454;&#30340;&#21487;&#20998;&#31163;&#24615;&#25351;&#25968;&#30830;&#23450;&#26368;&#20339;&#37197;&#32622;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03307</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#29983;&#25104;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generating quantum feature maps using multi-objective genetic algorithm. (arXiv:2309.03307v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#29983;&#25104;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#35775;&#38382;&#65292;&#24182;&#22312;&#20248;&#21270;&#30005;&#36335;&#37197;&#32622;&#26102;&#21516;&#26102;&#32771;&#34385;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#38376;&#25104;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26368;&#20339;&#30005;&#36335;&#37197;&#32622;&#20013;&#32416;&#32544;&#38376;&#38656;&#35201;&#30456;&#24212;&#30340;&#25968;&#37327;&#65292;&#19982;&#20043;&#21069;&#30740;&#31350;&#30456;&#21453;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#25968;&#25454;&#30340;&#21487;&#20998;&#31163;&#24615;&#25351;&#25968;&#30830;&#23450;&#26368;&#20339;&#37197;&#32622;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#29992;&#20110;&#37327;&#23376;&#22686;&#24378;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#39640;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#35775;&#38382;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#26368;&#22823;&#21270;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#30005;&#36335;&#30340;&#26412;&#22320;&#38376;&#25104;&#26412;&#21644;&#38750;&#26412;&#22320;&#38376;&#25104;&#26412;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20026;&#26412;&#22320;&#38376;&#21644;&#32416;&#32544;&#38376;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#36866;&#24212;&#24230;&#20989;&#25968;&#12290;&#19982;&#32463;&#20856;&#20998;&#31867;&#22120;&#30340;&#27604;&#36739;&#26377;&#21161;&#20110;&#29702;&#35299;&#20351;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#37327;&#23376;&#26680;&#26041;&#27861;&#30340;&#26368;&#20339;&#30005;&#36335;&#37197;&#32622;&#20013;&#21253;&#21547;&#20102;&#30456;&#24212;&#25968;&#37327;&#30340;&#38750;&#26412;&#22320;&#38376;&#29992;&#20110;&#32416;&#32544;&#65292;&#19982;&#20043;&#21069;&#30340;&#25991;&#29486;&#30456;&#21453;&#65292;&#20043;&#21069;&#30340;&#25991;&#29486;&#20013;&#38750;&#26412;&#22320;&#38376;&#34987;&#22823;&#37096;&#20998;&#25233;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#25968;&#25454;&#30340;&#21487;&#20998;&#31163;&#24615;&#25351;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#30830;&#23450;&#26368;&#20339;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for efficiently generating quantum feature maps for quantum-enhanced support vector machines, a kernel-based classifier, enabling access to high-dimensional Hilbert space. Our method employs a multi-objective genetic algorithm that simultaneously maximizes classification accuracy while minimizing both the local and non-local gate costs of the quantum feature map's circuit. To achieve this, we define distinct fitness functions for local gates and entanglement gates. Comparisons with classical classifiers are given in order to understand the advantages of using quantum machine learning. Surprisingly, our experiments reveal that the optimal configuration of quantum circuits for the quantum kernel method incorporates a proportional number of non-local gates for entanglement, contrary to previous literature where non-local gates were largely suppressed.  Furthermore, we demonstrate that the separability indexes of data can be effectively leveraged to determine th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#20132;&#26131;&#24341;&#23548;&#30340;&#24066;&#22330;&#27169;&#25311;&#26041;&#27861;(ATMS)&#65292;&#36890;&#36807;&#20248;&#21270;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#20132;&#26131;&#27963;&#21160;&#30340;&#24207;&#21015;&#21644;&#21160;&#24577;&#29305;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21322;&#30495;&#23454;&#24066;&#22330;&#19978;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.01784</link><description>&lt;p&gt;
ATMS: &#31639;&#27861;&#20132;&#26131;&#24341;&#23548;&#30340;&#24066;&#22330;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
ATMS: Algorithmic Trading-Guided Market Simulation. (arXiv:2309.01784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#20132;&#26131;&#24341;&#23548;&#30340;&#24066;&#22330;&#27169;&#25311;&#26041;&#27861;(ATMS)&#65292;&#36890;&#36807;&#20248;&#21270;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#20132;&#26131;&#27963;&#21160;&#30340;&#24207;&#21015;&#21644;&#21160;&#24577;&#29305;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21322;&#30495;&#23454;&#24066;&#22330;&#19978;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#26500;&#24314;&#31639;&#27861;&#20132;&#26131;&#31574;&#30053;&#36890;&#24120;&#20381;&#36182;&#20110;&#24066;&#22330;&#27169;&#25311;&#22120;&#65292;&#28982;&#32780;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#36866;&#24212;&#20132;&#26131;&#27963;&#21160;&#30340;&#24207;&#21015;&#21644;&#21160;&#24577;&#29305;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#34913;&#37327;&#24066;&#22330;&#24046;&#24322;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#35813;&#24230;&#37327;&#26631;&#20934;&#36890;&#36807;&#31639;&#27861;&#20132;&#26131;&#20195;&#29702;&#21644;&#24066;&#22330;&#20043;&#38388;&#30340;&#20132;&#20114;&#26469;&#35780;&#20272;&#24213;&#23618;&#24066;&#22330;&#30340;&#22240;&#26524;&#25928;&#24212;&#24046;&#24322;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31639;&#27861;&#20132;&#26131;&#24341;&#23548;&#30340;&#24066;&#22330;&#27169;&#25311;(ATMS)&#65292;&#36890;&#36807;&#20248;&#21270;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#21463;SeqGAN&#30340;&#21551;&#21457;&#65292;ATMS&#23558;&#27169;&#25311;&#22120;&#24418;&#24335;&#21270;&#20026;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38543;&#26426;&#31574;&#30053;&#65292;&#20197;&#32771;&#34385;&#20132;&#26131;&#30340;&#24207;&#21015;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;ATMS&#21033;&#29992;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#26469;&#32469;&#36807;&#23545;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#24494;&#20998;&#65292;&#36825;&#28041;&#21450;&#21040;&#38750;&#21487;&#24494;&#20998;&#25805;&#20316;&#65292;&#22914;&#20174;&#24066;&#22330;&#20013;&#21024;&#38500;&#35746;&#21333;&#12290;&#36890;&#36807;&#22312;&#21322;&#30495;&#23454;&#24066;&#22330;&#19978;&#36827;&#34892;&#22823;&#37327;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
The effective construction of an Algorithmic Trading (AT) strategy often relies on market simulators, which remains challenging due to existing methods' inability to adapt to the sequential and dynamic nature of trading activities. This work fills this gap by proposing a metric to quantify market discrepancy. This metric measures the difference between a causal effect from underlying market unique characteristics and it is evaluated through the interaction between the AT agent and the market. Most importantly, we introduce Algorithmic Trading-guided Market Simulation (ATMS) by optimizing our proposed metric. Inspired by SeqGAN, ATMS formulates the simulator as a stochastic policy in reinforcement learning (RL) to account for the sequential nature of trading. Moreover, ATMS utilizes the policy gradient update to bypass differentiating the proposed metric, which involves non-differentiable operations such as order deletion from the market. Through extensive experiments on semi-real marke
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.15363</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#25991;&#26412;&#21040;SQL&#30340;&#30740;&#31350;&#65306;&#19968;&#20010;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#19968;&#31181;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#22522;&#20934;&#38459;&#30861;&#20102;&#35774;&#35745;&#26377;&#25928;&#12289;&#39640;&#25928;&#21644;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#21644;&#24191;&#27867;&#30340;&#27604;&#36739;&#65292;&#21253;&#25324;&#38382;&#39064;&#34920;&#31034;&#12289;&#31034;&#20363;&#36873;&#25321;&#21644;&#31034;&#20363;&#32452;&#32455;&#65292;&#24182;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#35814;&#32454;&#38416;&#36848;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#21517;&#20026;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#65292;&#36798;&#21040;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#26438;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24378;&#35843;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#65292;&#24182;&#22312;&#27492;&#24230;&#37327;&#19979;&#27604;&#36739;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24320;&#28304;LLMs&#65292;&#24182;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#30417;&#30563;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborates their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. Towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. Additionally, we investigate open-source LLMs in in-context learning, and further enhance their performance with task-specific superv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21516;&#24577;&#35745;&#25968;&#30340;&#32467;&#26500;&#33410;&#28857;&#23884;&#20837;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36825;&#31181;&#23884;&#20837;&#22312;&#35299;&#37322;&#21644;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.15283</link><description>&lt;p&gt;
&#20351;&#29992;&#21516;&#24577;&#35745;&#25968;&#30340;&#32467;&#26500;&#33410;&#28857;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Structural Node Embeddings with Homomorphism Counts. (arXiv:2308.15283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21516;&#24577;&#35745;&#25968;&#30340;&#32467;&#26500;&#33410;&#28857;&#23884;&#20837;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36825;&#31181;&#23884;&#20837;&#22312;&#35299;&#37322;&#21644;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21516;&#24577;&#35745;&#25968;&#26159;1967&#24180;&#30001;Lov\'asz&#39318;&#27425;&#25506;&#32034;&#30340;&#65292;&#26368;&#36817;&#22312;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;Grohe (PODS 2020)&#25552;&#20986;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#22270;&#32423;&#21035;&#21644;&#33410;&#28857;&#32423;&#21035;&#20219;&#21153;&#20013;&#20351;&#29992;&#21516;&#24577;&#35745;&#25968;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#30001;&#20110;&#20854;&#26412;&#36136;&#19978;&#25429;&#25417;&#20102;&#23616;&#37096;&#32467;&#26500;&#20449;&#24687;&#65292;&#36825;&#20351;&#24471;&#21019;&#24314;&#31283;&#20581;&#30340;&#32467;&#26500;&#23884;&#20837;&#25104;&#20026;&#21487;&#33021;&#12290;&#34429;&#28982;Nguyen&#21644;Maehara (ICML 2020)&#24050;&#32463;&#25552;&#20986;&#20102;&#29992;&#20110;&#22270;&#32423;&#20219;&#21153;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22522;&#20110;&#21516;&#24577;&#35745;&#25968;&#30340;&#33410;&#28857;&#23884;&#20837;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#23884;&#20837;&#19982;&#33410;&#28857;&#26631;&#31614;&#12289;&#33410;&#28857;&#26435;&#37325;&#21644;&#36793;&#26435;&#37325;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#23545;&#22270;&#25968;&#25454;&#30340;&#21487;&#35299;&#37322;&#24615;&#34920;&#31034;&#65292;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26500;&#19981;&#21464;&#30340;&#22522;&#20110;&#21516;&#24577;&#35745;&#25968;&#30340;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;&#39640;&#25928;&#35745;&#31639;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph homomorphism counts, first explored by Lov\'asz in 1967, have recently garnered interest as a powerful tool in graph-based machine learning. Grohe (PODS 2020) proposed the theoretical foundations for using homomorphism counts in machine learning on graph level as well as node level tasks. By their very nature, these capture local structural information, which enables the creation of robust structural embeddings. While a first approach for graph level tasks has been made by Nguyen and Maehara (ICML 2020), we experimentally show the effectiveness of homomorphism count based node embeddings. Enriched with node labels, node weights, and edge weights, these offer an interpretable representation of graph data, allowing for enhanced explainability of machine learning models.  We propose a theoretical framework for isomorphism-invariant homomorphism count based embeddings which lend themselves to a wide variety of downstream tasks. Our approach capitalises on the efficient computability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21306;&#22495;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#34701;&#21512;&#21306;&#22495;&#34917;&#19969;&#20449;&#24687;&#20197;&#24471;&#20986;&#28369;&#29255;&#32423;&#21035;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#21306;&#22495;&#32858;&#21512;&#26469;&#20998;&#23618;&#22788;&#29702;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32858;&#28966;&#20110;&#39640;&#20851;&#27880;&#21306;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.12634</link><description>&lt;p&gt;
&#38754;&#21521;&#20998;&#23618;&#21306;&#22495;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Hierarchical Regional Transformer-based Multiple Instance Learning. (arXiv:2308.12634v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21306;&#22495;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#34701;&#21512;&#21306;&#22495;&#34917;&#19969;&#20449;&#24687;&#20197;&#24471;&#20986;&#28369;&#29255;&#32423;&#21035;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#21306;&#22495;&#32858;&#21512;&#26469;&#20998;&#23618;&#22788;&#29702;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32858;&#28966;&#20110;&#39640;&#20851;&#27880;&#21306;&#22495;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#21644;&#31934;&#30830;&#21307;&#23398;&#20013;&#65292;&#20351;&#29992;&#28145;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;&#27169;&#22411;&#23545;&#24040;&#20687;&#32032;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#24050;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29992;&#21306;&#22495;&#24615;&#30340;&#12289;&#21463;&#21040;&#35270;&#35273;&#21464;&#21387;&#22120;&#21551;&#21457;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26367;&#20195;&#20102;&#20256;&#32479;&#30340;&#23398;&#20064;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#21306;&#22495;&#34917;&#19969;&#20449;&#24687;&#20197;&#24471;&#20986;&#28369;&#29255;&#32423;&#21035;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#22534;&#21472;&#36825;&#31181;&#21306;&#22495;&#32858;&#21512;&#20197;&#20998;&#23618;&#22320;&#22788;&#29702;&#19981;&#21516;&#36317;&#31163;&#27700;&#24179;&#19978;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#23567;&#30340;&#23616;&#37096;&#24418;&#24577;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26399;&#38388;&#23558;&#22270;&#20687;&#22788;&#29702;&#38598;&#20013;&#22312;&#39640;&#20851;&#27880;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#20004;&#20010;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24182;&#25351;&#21521;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classification of gigapixel histopathology images with deep multiple instance learning models has become a critical task in digital pathology and precision medicine. In this work, we propose a Transformer-based multiple instance learning approach that replaces the traditional learned attention mechanism with a regional, Vision Transformer inspired self-attention mechanism. We present a method that fuses regional patch information to derive slide-level predictions and show how this regional aggregation can be stacked to hierarchically process features on different distance levels. To increase predictive accuracy, especially for datasets with small, local morphological features, we introduce a method to focus the image processing on high attention regions during inference. Our approach is able to significantly improve performance over the baseline on two histopathology datasets and points towards promising directions for further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20986;&#29616;&#29702;&#35770;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#20013;&#23398;&#20064;&#23439;&#35266;&#21160;&#21147;&#23398;&#21644;&#37327;&#21270;&#20986;&#29616;&#31243;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#25104;&#21151;&#25429;&#25417;&#20986;&#29616;&#27169;&#24335;&#65292;&#24182;&#23398;&#20064;&#31895;&#31890;&#21270;&#31574;&#30053;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09952</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#20013;&#23547;&#25214;&#20986;&#29616;&#65306;&#21551;&#21457;&#20110;&#22240;&#26524;&#20986;&#29616;&#30340;&#21160;&#21147;&#23398;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Finding emergence in data: causal emergence inspired dynamics learning. (arXiv:2308.09952v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20986;&#29616;&#29702;&#35770;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#20013;&#23398;&#20064;&#23439;&#35266;&#21160;&#21147;&#23398;&#21644;&#37327;&#21270;&#20986;&#29616;&#31243;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#25104;&#21151;&#25429;&#25417;&#20986;&#29616;&#27169;&#24335;&#65292;&#24182;&#23398;&#20064;&#31895;&#31890;&#21270;&#31574;&#30053;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#24494;&#35266;&#23618;&#38754;&#30340;&#35266;&#27979;&#25968;&#25454;&#26080;&#27861;&#30452;&#25509;&#25429;&#25417;&#21040;&#20986;&#29616;&#34892;&#20026;&#21644;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#23439;&#35266;&#23618;&#38754;&#20986;&#29616;&#21160;&#21147;&#23398;&#24182;&#26681;&#25454;&#21487;&#29992;&#25968;&#25454;&#37327;&#21270;&#20986;&#29616;&#30340;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;&#22240;&#26524;&#20986;&#29616;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#21253;&#21547;&#20986;&#29616;&#28508;&#22312;&#31354;&#38388;&#30340;&#23439;&#35266;&#21160;&#21147;&#23398;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#22823;&#21270;&#26377;&#25928;&#20449;&#24687;&#65288;EI&#65289;&#26469;&#33719;&#24471;&#19968;&#20010;&#20855;&#26377;&#26356;&#24378;&#22240;&#26524;&#25928;&#26524;&#30340;&#23439;&#35266;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#23545;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#23427;&#19981;&#20165;&#25104;&#21151;&#25429;&#25417;&#21040;&#20986;&#29616;&#27169;&#24335;&#65292;&#36824;&#23398;&#20064;&#20102;&#31895;&#31890;&#21270;&#31574;&#30053;&#24182;&#37327;&#21270;&#20102;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20986;&#29616;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#23545;&#19981;&#21516;&#29615;&#22659;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#26694;&#26550;&#22312;&#24314;&#27169;&#21508;&#31181;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling complex dynamical systems in a data-driven manner is challenging due to the presence of emergent behaviors and properties that cannot be directly captured by micro-level observational data. Therefore, it is crucial to develop a model that can effectively capture emergent dynamics at the macro-level and quantify emergence based on the available data. Drawing inspiration from the theory of causal emergence, this paper introduces a machine learning framework aimed at learning macro-dynamics within an emergent latent space. The framework achieves this by maximizing the effective information (EI) to obtain a macro-dynamics model with stronger causal effects. Experimental results on both simulated and real data demonstrate the effectiveness of the proposed framework. Not only does it successfully capture emergent patterns, but it also learns the coarse-graining strategy and quantifies the degree of causal emergence in the data. Furthermore, experiments conducted on environments dif
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#38750;&#20984;&#20248;&#21270;&#20013;&#25209;&#22823;&#23567;&#21644;&#27493;&#25968;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#25209;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#25152;&#38656;&#30340;&#27493;&#25968;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2307.13831</link><description>&lt;p&gt;
&#25209;&#22823;&#23567;&#21644;&#27493;&#25968;&#19982;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#38750;&#20984;&#20248;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search. (arXiv:2307.13831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13831
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#38750;&#20984;&#20248;&#21270;&#20013;&#25209;&#22823;&#23567;&#21644;&#27493;&#25968;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#25209;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#25152;&#38656;&#30340;&#27493;&#25968;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#12290;&#34429;&#28982;SGD&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#23398;&#20064;&#29575;&#65292;&#22914;&#24120;&#25968;&#25110;&#36882;&#20943;&#30340;&#23398;&#20064;&#29575;&#65292;&#20294;&#20043;&#21069;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;SGD&#20351;&#29992;&#32447;&#25628;&#32034;&#26041;&#27861;&#32473;&#20986;&#30340;&#23398;&#20064;&#29575;&#26102;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#32473;&#20986;&#23398;&#20064;&#29575;&#30340;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;SGD&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#27493;&#25968;&#21644;&#25209;&#22823;&#23567;&#37117;&#24456;&#22823;&#26102;&#65292;&#20840;&#26799;&#24230;&#30340;&#24179;&#26041;&#33539;&#25968;&#30340;&#26399;&#26395;&#19978;&#30028;&#21464;&#23567;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20351;&#29992;Armijo&#32447;&#25628;&#32034;&#23398;&#20064;&#29575;&#30340;SGD&#26469;&#35828;&#65292;&#38750;&#20984;&#20248;&#21270;&#25152;&#38656;&#30340;&#27493;&#25968;&#26159;&#25209;&#22823;&#23567;&#30340;&#21333;&#35843;&#36882;&#20943;&#20984;&#20989;&#25968;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#38543;&#30528;&#25209;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#38750;&#20984;&#20248;&#21270;&#25152;&#38656;&#30340;&#27493;&#25968;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38543;&#26426;&#28779;&#28798;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent (SGD) is the simplest deep learning optimizer with which to train deep neural networks. While SGD can use various learning rates, such as constant or diminishing rates, the previous numerical results showed that SGD performs better than other deep learning optimizers using when it uses learning rates given by line search methods. In this paper, we perform a convergence analysis on SGD with a learning rate given by an Armijo line search for nonconvex optimization. The analysis indicates that the upper bound of the expectation of the squared norm of the full gradient becomes small when the number of steps and the batch size are large. Next, we show that, for SGD with the Armijo-line-search learning rate, the number of steps needed for nonconvex optimization is a monotone decreasing convex function of the batch size; that is, the number of steps needed for nonconvex optimization decreases as the batch size increases. Furthermore, we show that the stochastic fir
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#20998;&#31867;&#32534;&#30721;&#22120;&#22522;&#20934;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;&#19981;&#21516;&#23478;&#26063;&#30340;32&#31181;&#32534;&#30721;&#22120;&#37197;&#32622;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#20197;&#21450;36&#31181;&#23454;&#39564;&#22240;&#32032;&#21644;50&#20010;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#23454;&#39564;&#22240;&#32032;&#21644;&#32858;&#21512;&#31574;&#30053;&#23545;&#22522;&#20934;&#30740;&#31350;&#32467;&#35770;&#30340;&#28145;&#36828;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.09191</link><description>&lt;p&gt;
&#29992;&#20110;&#20108;&#20998;&#31867;&#30340;&#20998;&#31867;&#32534;&#30721;&#22120;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A benchmark of categorical encoders for binary classification. (arXiv:2307.09191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#20998;&#31867;&#32534;&#30721;&#22120;&#22522;&#20934;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;&#19981;&#21516;&#23478;&#26063;&#30340;32&#31181;&#32534;&#30721;&#22120;&#37197;&#32622;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#20197;&#21450;36&#31181;&#23454;&#39564;&#22240;&#32032;&#21644;50&#20010;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#23454;&#39564;&#22240;&#32032;&#21644;&#32858;&#21512;&#31574;&#30053;&#23545;&#22522;&#20934;&#30740;&#31350;&#32467;&#35770;&#30340;&#28145;&#36828;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#32534;&#30721;&#22120;&#23558;&#20998;&#31867;&#29305;&#24449;&#36716;&#21270;&#20026;&#25968;&#23383;&#34920;&#31034;&#65292;&#23545;&#20110;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;&#22522;&#20934;&#30740;&#31350;&#30001;&#20110;&#36873;&#25321;&#26377;&#38480;&#30340;&#32534;&#30721;&#22120;&#12289;&#23454;&#39564;&#22240;&#32032;&#21644;&#25968;&#25454;&#38598;&#65292;&#32570;&#20047;&#26222;&#36866;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#32467;&#26524;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#20998;&#31867;&#32534;&#30721;&#22120;&#22522;&#20934;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;&#26469;&#33258;&#19981;&#21516;&#23478;&#26063;&#30340;32&#31181;&#32534;&#30721;&#22120;&#37197;&#32622;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#20197;&#21450;36&#31181;&#23454;&#39564;&#22240;&#32032;&#21644;50&#20010;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#12290;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#23454;&#39564;&#22240;&#32032;&#21644;&#32858;&#21512;&#31574;&#30053;&#23545;&#22522;&#20934;&#30740;&#31350;&#32467;&#35770;&#30340;&#28145;&#36828;&#24433;&#21709;&#65292;&#36825;&#26159;&#20197;&#21069;&#30340;&#32534;&#30721;&#22120;&#22522;&#20934;&#30740;&#31350;&#24573;&#35270;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Categorical encoders transform categorical features into numerical representations that are indispensable for a wide range of machine learning models. Existing encoder benchmark studies lack generalizability because of their limited choice of (1) encoders, (2) experimental factors, and (3) datasets. Additionally, inconsistencies arise from the adoption of varying aggregation strategies. This paper is the most comprehensive benchmark of categorical encoders to date, including an extensive evaluation of 32 configurations of encoders from diverse families, with 36 combinations of experimental factors, and on 50 datasets. The study shows the profound influence of dataset selection, experimental factors, and aggregation strategies on the benchmark's conclusions -- aspects disregarded in previous encoder benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#21147;&#23398;&#32593;&#32476;&#21644;&#29289;&#29702;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#27491;&#36816;&#21160;&#23398;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.07975</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#32593;&#32476;: &#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#23398;&#20064;&#21512;&#29702;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Finite element inspired networks: Learning physically-plausible deformable object dynamics from partial observations. (arXiv:2307.07975v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07975
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#21147;&#23398;&#32593;&#32476;&#21644;&#29289;&#29702;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#27491;&#36816;&#21160;&#23398;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#27169;&#25311;&#21487;&#21464;&#24418;&#32447;&#24615;&#29289;&#20307;&#65288;DLO&#65289;&#30340;&#21160;&#21147;&#23398;&#22312;&#38656;&#35201;&#19968;&#20010;&#21487;&#20197;&#34987;&#20154;&#35299;&#35835;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#27169;&#22411;&#24182;&#33021;&#22815;&#25552;&#20379;&#24555;&#36895;&#39044;&#27979;&#30340;&#20219;&#21153;&#20013;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#24471;&#21040;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#21018;&#24615;&#26377;&#38480;&#20803;&#26041;&#27861;&#65288;R-FEM&#65289;&#65292;&#23558;DLO&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#21018;&#20307;&#38142;&#65292;&#20854;&#20869;&#37096;&#29366;&#24577;&#36890;&#36807;&#21160;&#21147;&#23398;&#32593;&#32476;&#20197;&#26102;&#38388;&#23637;&#24320;&#12290;&#30001;&#20110;&#35813;&#29366;&#24577;&#19981;&#33021;&#30452;&#25509;&#35266;&#23519;&#21040;&#65292;&#21160;&#21147;&#23398;&#32593;&#32476;&#19982;&#19968;&#20010;&#29289;&#29702;&#24863;&#30693;&#30340;&#32534;&#30721;&#22120;&#20849;&#21516;&#35757;&#32451;&#65292;&#23558;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#21464;&#37327;&#26144;&#23556;&#21040;&#21018;&#20307;&#38142;&#30340;&#29366;&#24577;&#12290;&#20026;&#20102;&#20419;&#20351;&#29366;&#24577;&#33719;&#24471;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21033;&#29992;&#24213;&#23618;R-FEM&#27169;&#22411;&#30340;&#27491;&#36816;&#21160;&#23398;&#65288;FK&#65289;&#20316;&#20026;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#35777;&#26126;&#65292;&#36825;&#31181;&#34987;&#31216;&#20026;&#8220;&#26377;&#38480;&#20803;&#21551;&#21457;&#32593;&#32476;&#8221;&#30340;&#26550;&#26500;&#26159;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#20294;&#21151;&#33021;&#24378;&#22823;&#30340;DLO&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#24471;&#20986;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate simulation of deformable linear object (DLO) dynamics is challenging if the task at hand requires a human-interpretable and data-efficient model that also yields fast predictions. To arrive at such model, we draw inspiration from the rigid finite element method (R-FEM) and model a DLO as a serial chain of rigid bodies whose internal state is unrolled through time by a dynamics network. As this state is not observed directly, the dynamics network is trained jointly with a physics-informed encoder mapping observed motion variables to the body chain's state. To encourage that the state acquires a physically meaningful representation, we leverage the forward kinematics (FK) of the underlying R-FEM model as a decoder. We demonstrate in a robot experiment that this architecture - being termed "Finite element inspired network" - forms an easy to handle, yet capable DLO dynamics model yielding physically interpretable predictions from partial observations.  The project code is ava
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#24773;&#22659;&#36172;&#21338;&#20013;&#30340;&#26368;&#26174;&#33879;&#21464;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#35745;&#31639;&#26174;&#33879;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#23616;&#37096;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05341</link><description>&lt;p&gt;
&#36319;&#36394;&#38750;&#21442;&#25968;&#24773;&#22659;&#36172;&#21338;&#20013;&#26368;&#26174;&#33879;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Tracking Most Significant Shifts in Nonparametric Contextual Bandits. (arXiv:2307.05341v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05341
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#24773;&#22659;&#36172;&#21338;&#20013;&#30340;&#26368;&#26174;&#33879;&#21464;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#35745;&#31639;&#26174;&#33879;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#23616;&#37096;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#24773;&#22659;&#36172;&#21338;&#65292;&#20854;&#20013;Lipschitz&#22343;&#20540;&#22870;&#21169;&#20989;&#25968;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#36825;&#20010;&#36739;&#23569;&#34987;&#29702;&#35299;&#30340;&#24773;&#22659;&#19979;&#24314;&#31435;&#20102;&#21160;&#24577;&#36951;&#25022;&#29575;&#30340;&#26497;&#23567;&#26497;&#22823;&#20540;&#65292;&#36825;&#20123;&#20540;&#19982;&#21464;&#21270;&#25968;&#37327;L&#21644;&#24635;&#21464;&#24046;V&#26377;&#20851;&#65292;&#20004;&#32773;&#37117;&#21487;&#20197;&#25429;&#25417;&#21040;&#19978;&#19979;&#25991;&#31354;&#38388;&#30340;&#25152;&#26377;&#20998;&#24067;&#21464;&#21270;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#30446;&#21069;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#24773;&#22659;&#19979;&#26159;&#27425;&#20248;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#31181;&#24773;&#22659;&#19979;&#30340;&#36866;&#24212;&#24615;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#30693;&#36947;L&#25110;V&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26497;&#23567;&#26497;&#22823;&#20540;&#12290;&#38750;&#24120;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;X_t&#22788;&#65292;&#36172;&#21338;&#38382;&#39064;&#22312;&#19978;&#19979;&#25991;&#31354;&#38388;&#20854;&#20182;&#37096;&#20998;&#20013;&#30340;&#22870;&#21169;&#21464;&#21270;&#19981;&#24212;&#35813;&#20135;&#29983;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#21270;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32463;&#39564;&#26174;&#33879;&#21464;&#21270;&#65292;&#26356;&#22909;&#22320;&#32771;&#34385;&#20102;&#23616;&#37096;&#24615;&#65292;&#22240;&#27492;&#27604;L&#21644;V&#35745;&#25968;&#26356;&#23569;&#12290;&#27492;&#22806;&#65292;&#31867;&#20284;&#20110;&#26368;&#36817;&#22312;&#38750;&#24179;&#31283;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#24037;&#20316;&#65288;Suk&#21644;Kpotufe&#65292;2022&#65289;&#65292;&#32463;&#39564;&#26174;&#33879;&#21464;&#21270;&#21482;&#35745;&#31639;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study nonparametric contextual bandits where Lipschitz mean reward functions may change over time. We first establish the minimax dynamic regret rate in this less understood setting in terms of number of changes $L$ and total-variation $V$, both capturing all changes in distribution over context space, and argue that state-of-the-art procedures are suboptimal in this setting.  Next, we tend to the question of an adaptivity for this setting, i.e. achieving the minimax rate without knowledge of $L$ or $V$. Quite importantly, we posit that the bandit problem, viewed locally at a given context $X_t$, should not be affected by reward changes in other parts of context space $\cal X$. We therefore propose a notion of change, which we term experienced significant shifts, that better accounts for locality, and thus counts considerably less changes than $L$ and $V$. Furthermore, similar to recent work on non-stationary MAB (Suk &amp; Kpotufe, 2022), experienced significant shifts only count the m
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.04962</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#32593;&#32476;&#29702;&#35770;&#36827;&#34892;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04962
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#22312;&#39537;&#21160;&#30340;&#25506;&#32034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#29992;&#36884;&#65292;&#21363;&#20351;&#27809;&#26377;&#39069;&#22806;&#30340;&#22806;&#22312;&#22870;&#21169;&#12290;&#24403;&#29615;&#22659;&#33258;&#28982;&#34920;&#31034;&#20026;&#22270;&#26102;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#24341;&#23548;&#25506;&#32034;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65306;&#20449;&#24687;&#24046;&#29702;&#35770;&#21644;&#21387;&#32553;&#36827;&#23637;&#29702;&#35770;&#65292;&#26469;&#28608;&#21169;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#25506;&#32034;&#12290;&#36825;&#20123;&#29702;&#35770;&#23558;&#22909;&#22855;&#24515;&#35270;&#20026;&#23545;&#29615;&#22659;&#20013;&#35775;&#38382;&#33410;&#28857;&#25152;&#24341;&#21457;&#30340;&#23376;&#22270;&#30340;&#25299;&#25169;&#29305;&#24449;&#36827;&#34892;&#20248;&#21270;&#30340;&#20869;&#22312;&#21160;&#26426;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25552;&#20986;&#30340;&#29305;&#24449;&#20316;&#20026;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#22870;&#21169;&#12290;&#22312;&#22810;&#20010;&#31867;&#21035;&#30340;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#20195;&#29702;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#21644;&#27604;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#38271;&#30340;&#25506;&#32034;&#24615;&#27493;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#20110;&#30456;&#20851;&#25299;&#25169;&#23646;&#24615;&#30340;&#36138;&#23146;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#20869;&#22312;&#21160;&#26426;&#20135;&#29983;&#30340;&#22870;&#21169;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#29983;&#25104;&#19978;&#25512;&#24191;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#33021;&#22815;&#22312;&#26356;&#22823;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated exploration has proven useful for reinforcement learning, even without additional extrinsic rewards. When the environment is naturally represented as a graph, how to guide exploration best remains an open question. In this work, we propose a novel approach for exploring graph-structured data motivated by two theories of human curiosity: the information gap theory and the compression progress theory. The theories view curiosity as an intrinsic motivation to optimize for topological features of subgraphs induced by the visited nodes in the environment. We use these proposed features as rewards for graph neural-network-based reinforcement learning. On multiple classes of synthetically generated graphs, we find that trained agents generalize to larger environments and to longer exploratory walks than are seen during training. Our method computes more efficiently than the greedy evaluation of the relevant topological properties. The proposed intrinsic motivations bea
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#30446;&#26631;&#20197;&#21450;&#32447;&#24615;&#21644;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#20110;&#19981;&#21516;&#36924;&#36817;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02842</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#39640;&#25928;&#30340;&#36845;&#20195;CVaR&#24378;&#21270;&#23398;&#20064;&#19982;&#20989;&#25968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation. (arXiv:2307.02842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02842
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#30446;&#26631;&#20197;&#21450;&#32447;&#24615;&#21644;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#20110;&#19981;&#21516;&#36924;&#36817;&#26041;&#27861;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20248;&#21270;&#24179;&#34913;&#26399;&#26395;&#22870;&#21169;&#21644;&#39118;&#38505;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#24418;&#24335;&#65292;&#37319;&#29992;&#36845;&#20195;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#65288;CVaR&#65289;&#30446;&#26631;&#20197;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#12290;&#36825;&#31181;&#21517;&#20026;&#24102;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;ICVaR-RL&#30340;&#26032;&#24418;&#24335;&#65292;&#20026;&#27599;&#20010;&#20915;&#31574;&#27493;&#39588;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#23433;&#20840;&#20445;&#35777;&#26041;&#24335;&#12290;&#23545;&#20110;&#37319;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;ICVaR-RL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;ICVaR-L&#65292;&#35813;&#31639;&#27861;&#30340;&#21518;&#24724;&#24230;&#20026;$\widetilde{O}(\sqrt{\alpha^{-(H+1)}(d^2H^4+dH^6)K})$&#65292;&#20854;&#20013;$\alpha$&#26159;&#39118;&#38505;&#27700;&#24179;&#65292;$d$&#26159;&#29366;&#24577;&#34892;&#21160;&#29305;&#24449;&#30340;&#32500;&#24230;&#65292;$H$&#26159;&#27599;&#20010;episode&#30340;&#38271;&#24230;&#65292;$K$&#26159;episode&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#30456;&#21305;&#37197;&#30340;&#19979;&#30028;$\Omega(\sqrt{\alpha^{-(H-1)}d^2K})$&#65292;&#20197;&#39564;&#35777;ICVaR-L&#22312;$d$&#21644;$K$&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;&#23545;&#20110;&#37319;&#29992;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#30340;ICVaR-RL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31639;&#27861;ICVaR-G&#65292;&#23427;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Risk-sensitive reinforcement learning (RL) aims to optimize policies that balance the expected reward and risk. In this paper, we investigate a novel risk-sensitive RL formulation with an Iterated Conditional Value-at-Risk (CVaR) objective under linear and general function approximations. This new formulation, named ICVaR-RL with function approximation, provides a principled way to guarantee safety at each decision step. For ICVaR-RL with linear function approximation, we propose a computationally efficient algorithm ICVaR-L, which achieves an $\widetilde{O}(\sqrt{\alpha^{-(H+1)}(d^2H^4+dH^6)K})$ regret, where $\alpha$ is the risk level, $d$ is the dimension of state-action features, $H$ is the length of each episode, and $K$ is the number of episodes. We also establish a matching lower bound $\Omega(\sqrt{\alpha^{-(H-1)}d^2K})$ to validate the optimality of ICVaR-L with respect to $d$ and $K$. For ICVaR-RL with general function approximation, we propose algorithm ICVaR-G, which achiev
&lt;/p&gt;</description></item><item><title>Waypoint Transformer&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;RL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20013;&#38388;&#30446;&#26631;&#26469;&#23454;&#29616;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#23588;&#20854;&#22312;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#21644;&#25968;&#25454;&#37197;&#32622;&#20013;&#34920;&#29616;&#24471;&#26356;&#21152;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2306.14069</link><description>&lt;p&gt;
Waypoint Transformer: &#36890;&#36807;&#20013;&#38388;&#30446;&#26631;&#30340;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Waypoint Transformer: Reinforcement Learning via Supervised Learning with Intermediate Targets. (arXiv:2306.14069v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14069
&lt;/p&gt;
&lt;p&gt;
Waypoint Transformer&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;RL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20013;&#38388;&#30446;&#26631;&#26469;&#23454;&#29616;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#23588;&#20854;&#22312;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#21644;&#25968;&#25454;&#37197;&#32622;&#20013;&#34920;&#29616;&#24471;&#26356;&#21152;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#24456;&#22810;&#36827;&#23637;&#65292;&#20197;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;DT&#65289;&#26550;&#26500;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#25104;&#21151;&#32780;&#35328;&#65292;&#20294;&#22312;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;DT&#36824;&#26159;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#19968;&#20302;&#24615;&#33021;&#30340;&#26681;&#26412;&#21407;&#22240;&#22312;&#20110;&#23427;&#20204;&#26080;&#27861;&#26080;&#32541;&#36830;&#25509;&#20122;&#20248;&#21270;&#36712;&#36857;&#30340;&#29255;&#27573;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;RvS&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#25972;&#21512;&#20013;&#38388;&#30446;&#26631;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;Waypoint Transformer&#65288;WT&#65289;&#65292;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;DT&#26694;&#26550;&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#26159;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#36335;&#24452;&#28857;&#36827;&#34892;&#26465;&#20214;&#21270;&#30340;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;RvS&#26041;&#27861;&#30456;&#27604;&#65292;&#26368;&#32456;&#22238;&#25253;&#26174;&#33879;&#22686;&#21152;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#20248;&#12290;&#27492;&#22806;&#65292;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#30340;&#25913;&#36827;&#26368;&#22823;&#30340;&#26159;&#22312;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#21644;&#25968;&#25454;&#37197;&#32622;&#20013;&#65292;&#21253;&#25324;AntMaze Large Play/Diverse&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent advancements in offline reinforcement learning via supervised learning (RvS) and the success of the decision transformer (DT) architecture in various domains, DTs have fallen short in several challenging benchmarks. The root cause of this underperformance lies in their inability to seamlessly connect segments of suboptimal trajectories. To overcome this limitation, we present a novel approach to enhance RvS methods by integrating intermediate targets. We introduce the Waypoint Transformer (WT), using an architecture that builds upon the DT framework and conditioned on automatically-generated waypoints. The results show a significant increase in the final return compared to existing RvS methods, with performance on par or greater than existing state-of-the-art temporal difference learning-based methods. Additionally, the performance and stability improvements are largest in the most challenging environments and data configurations, including AntMaze Large Play/Diverse
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986; TeCoS-LVM &#27169;&#22411;&#65292;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20197;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36991;&#20813;&#20002;&#22833;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.12045</link><description>&lt;p&gt;
&#22788;&#29702;&#33258;&#28982;&#35270;&#35273;&#22330;&#26223;&#31070;&#32463;&#21709;&#24212;&#30340;&#26102;&#38388;&#26465;&#20214;&#33033;&#20914;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes. (arXiv:2306.12045v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986; TeCoS-LVM &#27169;&#22411;&#65292;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20197;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36991;&#20813;&#20002;&#22833;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#31070;&#32463;&#21709;&#24212;&#30340;&#35745;&#31639;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#24863;&#30693;&#22788;&#29702;&#21644;&#31070;&#32463;&#35745;&#31639;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#20351;&#29992;&#26102;&#38388;&#36807;&#28388;&#22120;&#26469;&#22788;&#29702;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#23548;&#33268;&#22788;&#29702;&#27969;&#31243;&#19981;&#29616;&#23454;&#19988;&#19981;&#28789;&#27963;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38024;&#23545;&#35797;&#39564;&#24179;&#22343;&#21457;&#25918;&#29575;&#65292;&#26410;&#33021;&#25429;&#25417;&#21040;&#33033;&#20914;&#21015;&#20013;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#26102;&#38388;&#26465;&#20214;&#33033;&#20914;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;TeCoS-LVM&#65289;&#26469;&#27169;&#25311;&#33258;&#28982;&#35270;&#35273;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#20135;&#29983;&#30452;&#25509;&#21305;&#37197;&#35760;&#24405;&#33033;&#20914;&#21015;&#30340;&#33033;&#20914;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#36991;&#20813;&#20002;&#22833;&#23884;&#20837;&#22312;&#21407;&#22987;&#33033;&#20914;&#21015;&#20013;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20174;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20013;&#25490;&#38500;&#26102;&#38388;&#32500;&#24230;&#65292;&#24182;&#24341;&#20837;&#26102;&#38388;&#26465;&#20214;&#25805;&#20316;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#33258;&#28982;&#33539;&#24335;&#20013;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#21644;&#21033;&#29992;&#21050;&#28608;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; TeCoS-LVM &#27169;&#22411;&#33021;&#22815;&#20135;&#29983;...
&lt;/p&gt;
&lt;p&gt;
Developing computational models of neural response is crucial for understanding sensory processing and neural computations. Current state-of-the-art neural network methods use temporal filters to handle temporal dependencies, resulting in an unrealistic and inflexible processing flow. Meanwhile, these methods target trial-averaged firing rates and fail to capture important features in spike trains. This work presents the temporal conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural response to natural visual stimuli. We use spiking neurons to produce spike outputs that directly match the recorded trains. This approach helps to avoid losing information embedded in the original spike trains. We exclude the temporal dimension from the model parameter space and introduce a temporal conditioning operation to allow the model to adaptively explore and exploit temporal dependencies in stimuli sequences in a natural paradigm. We show that TeCoS-LVM models can produce m
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#30456;&#20851;&#37319;&#26679;&#25216;&#26415;&#30340;&#23454;&#29992;&#35780;&#20272;&#35774;&#32622;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10453</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#24403;&#21069;&#30340;&#38382;&#39064;&#19982;&#26032;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking. (arXiv:2306.10453v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10453
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21551;&#21457;&#24335;&#30456;&#20851;&#37319;&#26679;&#25216;&#26415;&#30340;&#23454;&#29992;&#35780;&#20272;&#35774;&#32622;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#35797;&#22270;&#26681;&#25454;&#22270;&#30340;&#19968;&#37096;&#20998;&#36793;&#26469;&#39044;&#27979;&#26159;&#21542;&#23384;&#22312;&#26410;&#35265;&#30340;&#36793;&#12290;&#36817;&#24180;&#26469;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#35797;&#22270;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#36825;&#19968;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#21019;&#24314;&#20102;&#26032;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#36825;&#20123;&#26032;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23384;&#22312;&#22810;&#20010;&#38382;&#39064;&#38459;&#30861;&#25105;&#20204;&#33021;&#22815;&#27491;&#30830;&#35780;&#20272;&#36825;&#20123;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#38382;&#39064;&#20027;&#35201;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#34920;&#29616;&#20302;&#20110;&#23454;&#38469;&#34920;&#29616;&#65292;&#65288;2&#65289;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#32570;&#20047;&#32479;&#19968;&#30340;&#25968;&#25454;&#21010;&#20998;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20351;&#29992;&#31616;&#21333;&#36127;&#26679;&#26412;&#30340;&#19981;&#29616;&#23454;&#30340;&#35780;&#20272;&#35774;&#32622;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#30693;&#21517;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#36229;&#21442;&#25968;&#25628;&#32034;&#35774;&#32622;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#21551;&#21457;&#24335;&#30456;&#20851;&#37319;&#26679;&#25216;&#26415;&#65288;HeaRT&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#35780;&#20272;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction attempts to predict whether an unseen edge exists based on only a portion of edges of a graph. A flurry of methods have been introduced in recent years that attempt to make use of graph neural networks (GNNs) for this task. Furthermore, new and diverse datasets have also been created to better evaluate the effectiveness of these new models. However, multiple pitfalls currently exist that hinder our ability to properly evaluate these new methods. These pitfalls mainly include: (1) Lower than actual performance on multiple baselines, (2) A lack of a unified data split and evaluation metric on some datasets, and (3) An unrealistic evaluation setting that uses easy negative samples. To overcome these challenges, we first conduct a fair comparison across prominent methods and datasets, utilizing the same dataset and hyperparameter search settings. We then create a more practical evaluation setting based on a Heuristic Related Sampling Technique (HeaRT), which samples hard ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleTTS 2&#30340;TTS&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#19982;&#20197;&#24448;&#27169;&#22411;&#19981;&#21516;&#65292;StyleTTS 2&#23558;&#26679;&#24335;&#35270;&#20026;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#22823;&#22411;SLMs&#21644;&#26032;&#30340;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2306.07691</link><description>&lt;p&gt;
StyleTTS 2&#65306;&#36890;&#36807;&#39118;&#26684;&#25193;&#25955;&#21644;&#19982;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models. (arXiv:2306.07691v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StyleTTS 2&#30340;TTS&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#19982;&#20197;&#24448;&#27169;&#22411;&#19981;&#21516;&#65292;StyleTTS 2&#23558;&#26679;&#24335;&#35270;&#20026;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#22823;&#22411;SLMs&#21644;&#26032;&#30340;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;StyleTTS2&#65292;&#19968;&#31181;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#39118;&#26684;&#25193;&#25955;&#21644;&#19982;&#22823;&#22411;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#23454;&#29616;&#20154;&#31867;&#32423;&#21035;&#30340;TTS&#21512;&#25104;&#12290; StyleTTS 2&#36890;&#36807;&#23558;&#26679;&#24335;&#24314;&#27169;&#20026;&#28508;&#22312;&#30340;&#38543;&#26426;&#21464;&#37327;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26368;&#36866;&#21512;&#25991;&#26412;&#30340;&#26679;&#24335;&#65292;&#26080;&#38656;&#21442;&#32771;&#35821;&#38899;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#28508;&#22312;&#25193;&#25955;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#30340;&#22810;&#26679;&#21270;&#35821;&#38899;&#21512;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;SLMs&#65288;&#20363;&#22914;WavLM&#65289;&#20316;&#20026;&#37492;&#21035;&#22120;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#22411;&#21487;&#24494;&#20998;&#25345;&#32493;&#26102;&#38388;&#24314;&#27169;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#38899;&#30340;&#33258;&#28982;&#24230;&#12290; StyleTTS 2&#22312;&#21333;&#25196;&#22768;&#22120;LJSpeech&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#20154;&#31867;&#24405;&#38899;&#65292;&#24182;&#22312;&#22810;&#25196;&#22768;&#22120;VCTK&#25968;&#25454;&#38598;&#19978;&#19982;&#20043;&#21305;&#37197;&#65292;&#32463;&#36807;&#27597;&#35821;&#20026;&#33521;&#35821;&#30340;&#20154;&#21592;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#24403;&#22312;LibriTTS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32988;&#36807;&#20102;&#20197;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#38646;&#26679;&#26412;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23545;&#25239;&#35757;&#32451;baseline&#65292;&#20351;&#29992;&#20102;&#37325;&#26032;&#35843;&#25972;&#30340;&#24179;&#26041;&#25439;&#22833;&#12289;&#24490;&#29615;&#23398;&#20064;&#29575;&#21644;&#22522;&#20110;&#25830;&#38500;&#30340;&#25968;&#25454;&#22686;&#24378;&#31561;&#26041;&#27861;&#65292;&#22312;&#23545;&#25239;&#21644;&#33258;&#28982;&#20934;&#30830;&#24230;&#20043;&#38388;&#20135;&#29983;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#24182;&#21487;&#26377;&#25928;&#38477;&#20302;robust overfitting&#39118;&#38505;&#65292;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.07613</link><description>&lt;p&gt;
&#29992;&#19968;&#20010;&#31616;&#21333;baseline&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Rethinking Adversarial Training with A Simple Baseline. (arXiv:2306.07613v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07613
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23545;&#25239;&#35757;&#32451;baseline&#65292;&#20351;&#29992;&#20102;&#37325;&#26032;&#35843;&#25972;&#30340;&#24179;&#26041;&#25439;&#22833;&#12289;&#24490;&#29615;&#23398;&#20064;&#29575;&#21644;&#22522;&#20110;&#25830;&#38500;&#30340;&#25968;&#25454;&#22686;&#24378;&#31561;&#26041;&#27861;&#65292;&#22312;&#23545;&#25239;&#21644;&#33258;&#28982;&#20934;&#30830;&#24230;&#20043;&#38388;&#20135;&#29983;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#24182;&#21487;&#26377;&#25928;&#38477;&#20302;robust overfitting&#39118;&#38505;&#65292;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;baseline&#26041;&#27861;&#65292;&#38024;&#23545;CIFAR&#21644;SVHN&#25968;&#25454;&#38598;&#22312;RobustBench&#19978;&#33719;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#35757;&#32451;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#38598;&#25104;&#20102;&#37325;&#26032;&#35843;&#25972;&#30340;&#24179;&#26041;&#25439;&#22833;&#65292;&#24490;&#29615;&#23398;&#20064;&#29575;&#21644;&#22522;&#20110;&#25830;&#38500;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#25105;&#20204;&#23454;&#29616;&#30340;&#32467;&#26524;&#21487;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#25216;&#26415;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#36825;&#30446;&#21069;&#26159;&#23545;&#25239;&#35757;&#32451;&#30340;&#20027;&#35201;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;baseline&#34987;&#31216;&#20026;SimpleAT&#65292;&#20135;&#29983;&#20102;&#19977;&#20010;&#26032;&#39062;&#30340;&#32463;&#39564;&#27934;&#23519;&#65306;(i) &#36890;&#36807;&#36716;&#25442;&#20026;&#24179;&#26041;&#25439;&#22833;&#65292;&#20934;&#30830;&#24230;&#21487;&#19982;&#20351;&#29992;&#20107;&#23454;&#19978;&#30340;&#35757;&#32451;&#21327;&#35758;&#21152;&#25968;&#25454;&#22686;&#24378;&#25152;&#33719;&#24471;&#30340;&#20934;&#30830;&#24230;&#30456;&#24403;&#12290; (ii) &#19968;&#20010;&#24490;&#29615;&#23398;&#20064;&#29575;&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;scheduler&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;robust overfitting&#30340;&#39118;&#38505;&#12290; (iii) &#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#37325;&#26032;&#35843;&#25972;&#30340;&#24179;&#26041;&#25439;&#22833;&#21487;&#20197;&#22312;&#23545;&#25239;&#21644;&#33258;&#28982;&#20934;&#30830;&#24230;&#20043;&#38388;&#20135;&#29983;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SimpleAT&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;robust overfitting&#65292;&#24182;&#22987;&#32456;&#23454;&#29616;&#20102;&#19982;&#23545;&#25239;&#35757;&#32451;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#31454;&#20105;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report competitive results on RobustBench for CIFAR and SVHN using a simple yet effective baseline approach. Our approach involves a training protocol that integrates rescaled square loss, cyclic learning rates, and erasing-based data augmentation. The outcomes we have achieved are comparable to those of the model trained with state-of-the-art techniques, which is currently the predominant choice for adversarial training. Our baseline, referred to as SimpleAT, yields three novel empirical insights. (i) By switching to square loss, the accuracy is comparable to that obtained by using both de-facto training protocol plus data augmentation. (ii) One cyclic learning rate is a good scheduler, which can effectively reduce the risk of robust overfitting. (iii) Employing rescaled square loss during model training can yield a favorable balance between adversarial and natural accuracy. In general, our experimental results show that SimpleAT effectively mitigates robust overfitting and consist
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22312;&#19968;&#33324;&#30340;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#37327;&#21270;FL&#31639;&#27861;&#65292;&#37319;&#29992;&#26032;&#30340;&#38543;&#26426;&#37327;&#21270;&#26041;&#26696;&#21644;&#21152;&#26435;&#24179;&#22343;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#30340;&#24191;&#20041;&#23567;&#25209;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#20013;&#65292;&#20197;&#36866;&#24212;&#22312;&#24037;&#20316;&#33410;&#28857;&#20855;&#26377;&#22343;&#21248;&#25110;&#38750;&#22343;&#21248;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.07497</link><description>&lt;p&gt;
GQFedWAvg&#65306;&#22522;&#20110;&#20248;&#21270;&#30340;&#37327;&#21270;&#32852;&#37030;&#23398;&#20064;&#22312;&#19968;&#33324;&#30340;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
GQFedWAvg: Optimization-Based Quantized Federated Learning in General Edge Computing Systems. (arXiv:2306.07497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22312;&#19968;&#33324;&#30340;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#37327;&#21270;FL&#31639;&#27861;&#65292;&#37319;&#29992;&#26032;&#30340;&#38543;&#26426;&#37327;&#21270;&#26041;&#26696;&#21644;&#21152;&#26435;&#24179;&#22343;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#30340;&#24191;&#20041;&#23567;&#25209;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#20013;&#65292;&#20197;&#36866;&#24212;&#22312;&#24037;&#20316;&#33410;&#28857;&#20855;&#26377;&#22343;&#21248;&#25110;&#38750;&#22343;&#21248;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#26368;&#20339;&#23454;&#29616;&#19968;&#30452;&#26159;&#19968;&#20010;&#31361;&#20986;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#37327;&#21270;FL&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#24037;&#20316;&#33410;&#28857;&#20855;&#26377;&#22343;&#21248;&#25110;&#38750;&#22343;&#21248;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#30340;&#19968;&#33324;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#36866;&#24403;&#22320;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#37327;&#21270;&#26041;&#26696;&#24182;&#20998;&#26512;&#20102;&#20854;&#24615;&#36136;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#37327;&#21270;FL&#31639;&#27861;&#65292;&#21363;GQFedWAvg&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GQFedWAvg&#23558;&#25152;&#25552;&#20986;&#30340;&#37327;&#21270;&#26041;&#26696;&#24212;&#29992;&#20110;&#26234;&#33021;&#36873;&#25321;&#30340;&#27169;&#22411;&#26356;&#26032;&#30456;&#20851;&#21521;&#37327;&#65292;&#24182;&#37319;&#29992;&#21152;&#26435;&#24179;&#22343;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#30340;&#24191;&#20041;&#23567;&#25209;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#22312;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#20013;&#12290;&#27492;&#22806;&#65292;GQFedWAvg&#26377;&#19968;&#20123;&#21487;&#35843;&#25972;&#30340;&#31639;&#27861;&#21442;&#25968;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#36866;&#24212;&#26381;&#21153;&#22120;&#21644;&#24037;&#20316;&#33410;&#28857;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;GQFedWAvg&#30340;&#25910;&#25947;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20248;&#21270;&#35813;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal implementation of federated learning (FL) in practical edge computing systems has been an outstanding problem. In this paper, we propose an optimization-based quantized FL algorithm, which can appropriately fit a general edge computing system with uniform or nonuniform computing and communication resources at the workers. Specifically, we first present a new random quantization scheme and analyze its properties. Then, we propose a general quantized FL algorithm, namely GQFedWAvg. Specifically, GQFedWAvg applies the proposed quantization scheme to quantize wisely chosen model update-related vectors and adopts a generalized mini-batch stochastic gradient descent (SGD) method with the weighted average local model updates in global model aggregation. Besides, GQFedWAvg has several adjustable algorithm parameters to flexibly adapt to the computing and communication resources at the server and workers. We also analyze the convergence of GQFedWAvg. Next, we optimize the algorithm 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#35813;&#35770;&#25991;&#34920;&#26126;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#22312;&#23454;&#29616;&#35774;&#32622;&#19979;&#19981;&#31561;&#20215;&#65292;&#24182;&#23558;&#22312;&#32447;&#22810;&#26631;&#31614;&#25490;&#21517;&#21644;&#22312;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#31561;&#23454;&#38469;&#23398;&#20064;&#35774;&#32622;&#20316;&#20026;&#20854;&#29305;&#23450;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.06247</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning with Set-Valued Feedback. (arXiv:2306.06247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#35813;&#35770;&#25991;&#34920;&#26126;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#22312;&#23454;&#29616;&#35774;&#32622;&#19979;&#19981;&#31561;&#20215;&#65292;&#24182;&#23558;&#22312;&#32447;&#22810;&#26631;&#31614;&#25490;&#21517;&#21644;&#22312;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#31561;&#23454;&#38469;&#23398;&#20064;&#35774;&#32622;&#20316;&#20026;&#20854;&#29305;&#23450;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20854;&#20013;&#23398;&#20064;&#22120;&#39044;&#27979;&#21333;&#20010;&#26631;&#31614;&#65292;&#20294;&#25509;&#25910;&#21040;&#19968;&#20010;&#26631;&#31614;&#30340;&#38598;&#21512;&#20316;&#20026;&#21453;&#39304;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#22914;&#26524;&#23398;&#20064;&#22120;&#27809;&#26377;&#36755;&#20986;&#21253;&#21547;&#22312;&#21453;&#39304;&#38598;&#21512;&#20013;&#30340;&#26631;&#31614;&#65292;&#21017;&#20250;&#21463;&#21040;&#24809;&#32602;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19982;&#20855;&#26377;&#21333;&#26631;&#31614;&#21453;&#39304;&#30340;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#19981;&#21516;&#65292;&#22312;&#23454;&#29616;&#35774;&#32622;&#20013;&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#26102;&#65292;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21270;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;\textit{&#19981;&#31561;&#20215;}&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#26032;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#20998;&#21035;&#21629;&#21517;&#20026;&#38598;&#21512;&#23567;&#30707;&#21644;&#24230;&#37327;&#30772;&#35010;&#32500;&#24230;&#65292;&#20005;&#26684;&#25551;&#36848;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21270;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#24230;&#37327;&#30772;&#35010;&#32500;&#24230;&#22312;&#24735;&#24615;&#35774;&#32622;&#19979;&#20005;&#26684;&#25551;&#36848;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#22810;&#26631;&#31614;&#25490;&#21517;&#21644;&#22312;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#31561;&#23454;&#38469;&#23398;&#20064;&#35774;&#32622;&#26159;&#25105;&#20204;&#36890;&#29992;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#30340;&#20855;&#20307;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a variant of online multiclass classification where the learner predicts a single label but receives a \textit{set of labels} as feedback. In this model, the learner is penalized for not outputting a label contained in the revealed set. We show that unlike online multiclass learning with single-label feedback, deterministic and randomized online learnability are \textit{not equivalent} even in the realizable setting with set-valued feedback. Accordingly, we give two new combinatorial dimensions, named the Set Littlestone and Measure Shattering dimension, that tightly characterize deterministic and randomized online learnability respectively in the realizable setting. In addition, we show that the Measure Shattering dimension tightly characterizes online learnability in the agnostic setting. Finally, we show that practical learning settings like online multilabel ranking and online multilabel classification are specific instances of our general online learning framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22521;&#35757;&#26041;&#27861;&#24433;&#21709;&#28145;&#23618;&#32593;&#32476;&#20998;&#31867;&#22120;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#25968;&#25454;&#38544;&#34255;&#34920;&#31034;&#20013;&#36798;&#21040;&#26356;&#39640;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06146</link><description>&lt;p&gt;
&#38544;&#34255;&#20998;&#31867;&#23618;&#65306;&#20851;&#20110;&#25968;&#25454;&#38544;&#34255;&#34920;&#31034;&#20013;&#26356;&#39640;&#32447;&#24615;&#21487;&#20998;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hidden Classification Layers: a study on Data Hidden Representations with a Higher Degree of Linear Separability between the Classes. (arXiv:2306.06146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22521;&#35757;&#26041;&#27861;&#24433;&#21709;&#28145;&#23618;&#32593;&#32476;&#20998;&#31867;&#22120;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#25968;&#25454;&#38544;&#34255;&#34920;&#31034;&#20013;&#36798;&#21040;&#26356;&#39640;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#20195;&#34920;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#37117;&#22522;&#20110;&#26631;&#20934;&#30340;&#22810;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#31181;&#12290;&#36825;&#20123;&#20063;&#34987;&#31216;&#20026;&#28145;&#24230;&#32593;&#32476;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#27599;&#20010;&#38544;&#34255;&#31070;&#32463;&#23618;&#23436;&#25104;&#19968;&#31181;&#25968;&#25454;&#36716;&#25442;&#65292;&#39044;&#26399;&#20351;&#25968;&#25454;&#34920;&#31034;&#8220;&#27604;&#20043;&#21069;&#26356;&#32447;&#24615;&#21487;&#20998;&#8221;&#65292;&#20197;&#33719;&#24471;&#23613;&#21487;&#33021;&#32447;&#24615;&#21487;&#20998;&#30340;&#26368;&#32456;&#25968;&#25454;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#21487;&#20197;&#25191;&#34892;&#36825;&#20123;&#36716;&#25442;&#30340;&#36866;&#24403;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22521;&#35757;&#26041;&#27861;&#23545;&#28145;&#23618;&#32593;&#32476;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#20542;&#21521;&#20110;&#20351;&#29992;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#38544;&#34255;&#23618;&#30340;&#25968;&#25454;&#34920;&#31034;&#20855;&#26377;&#26356;&#39640;&#30340;&#31867;&#20043;&#38388;&#32447;&#24615;&#21487;&#20998;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#24341;&#20837;&#20102;&#19968;&#20010;&#28041;&#21450;&#35823;&#24046;&#20989;&#25968;&#30340;&#26032;&#39062;&#22521;&#35757;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of classification problems, Deep Learning (DL) approaches represent state of art. Many DL approaches are based on variations of standard multi-layer feed-forward neural networks. These are also referred to as deep networks. The basic idea is that each hidden neural layer accomplishes a data transformation which is expected to make the data representation "somewhat more linearly separable" than the previous one to obtain a final data representation which is as linearly separable as possible. However, determining the appropriate neural network parameters that can perform these transformations is a critical problem. In this paper, we investigate the impact on deep network classifier performances of a training approach favouring solutions where data representations at the hidden layers have a higher degree of linear separability between the classes with respect to standard methods. To this aim, we propose a neural network architecture which induces an error function involvin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#22312;TSP&#23454;&#20363;&#35757;&#32451;&#20043;&#21069;&#65292;&#23558;&#31070;&#32463;&#27169;&#22411;&#29992;&#30456;&#20851;&#31639;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TSP&#38382;&#39064;&#30340;&#35299;&#20915;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.06064</link><description>&lt;p&gt;
&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neural Algorithmic Reasoning for Combinatorial Optimisation. (arXiv:2306.06064v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#22312;TSP&#23454;&#20363;&#35757;&#32451;&#20043;&#21069;&#65292;&#23558;&#31070;&#32463;&#27169;&#22411;&#29992;&#30456;&#20851;&#31639;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TSP&#38382;&#39064;&#30340;&#35299;&#20915;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;NP&#38590;/&#23436;&#20840;&#32452;&#21512;&#38382;&#39064;&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#36229;&#36234;&#20256;&#32479;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;&#20854;&#38271;&#26399;&#30446;&#26631;&#26159;&#36890;&#36807;&#23398;&#20064;&#20165;&#20174;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#26356;&#20248;&#35299;&#26469;&#36229;&#36234;&#25163;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#32780;&#26053;&#34892;&#21830;&#38382;&#39064;(TSP)&#26159;&#32463;&#24120;&#34987;&#36825;&#20123;&#26041;&#27861;&#30596;&#20934;&#30340;&#19968;&#20010;&#37325;&#35201;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#35299;&#20915;TSP&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24120;&#24120;&#24573;&#30053;&#20102;&#38382;&#39064;&#22266;&#26377;&#30340;&#8220;&#31639;&#27861;&#8221;&#26412;&#36136;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#35774;&#35745;&#29992;&#20110;TSP&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#24120;&#24120;&#21033;&#29992;&#35832;&#22914;&#26597;&#25214;&#26368;&#23567;&#29983;&#25104;&#26641;&#20043;&#31867;&#30340;&#25104;&#29087;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#25913;&#36827;TSP&#38382;&#39064;&#30340;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#23545;TSP&#23454;&#20363;&#36827;&#34892;&#35757;&#32451;&#20043;&#21069;&#65292;&#22312;&#30456;&#20851;&#31639;&#27861;&#19978;&#23545;&#25105;&#20204;&#30340;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TSP&#38382;&#39064;&#30340;&#35299;&#20915;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving NP-hard/complete combinatorial problems with neural networks is a challenging research area that aims to surpass classical approximate algorithms. The long-term objective is to outperform hand-designed heuristics for NP-hard/complete problems by learning to generate superior solutions solely from training data. The Travelling Salesman Problem (TSP) is a prominent combinatorial optimisation problem often targeted by such approaches. However, current neural-based methods for solving TSP often overlook the inherent "algorithmic" nature of the problem. In contrast, heuristics designed for TSP frequently leverage well-established algorithms, such as those for finding the minimum spanning tree. In this paper, we propose leveraging recent advancements in neural algorithmic reasoning to improve the learning of TSP problems. Specifically, we suggest pre-training our neural model on relevant algorithms before training it on TSP instances. Our results demonstrate that, using this learning
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#38024;&#23545;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;11&#31181;&#36866;&#24212;&#26041;&#27861;&#22312;&#19981;&#21516;&#27745;&#26579;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#36866;&#24212;&#26041;&#27861;&#23545;&#25991;&#26412;&#27745;&#26579;&#26356;&#25935;&#24863;&#65292;&#21333;&#29420;&#20351;&#29992;&#23567;&#22411;&#25991;&#26412;&#36866;&#37197;&#22120;&#27604;&#20849;&#20139;&#36866;&#37197;&#22120;&#26356;&#40065;&#26834;&#65292;&#21487;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#24178;&#20928;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02080</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models. (arXiv:2306.02080v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02080
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#38024;&#23545;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;11&#31181;&#36866;&#24212;&#26041;&#27861;&#22312;&#19981;&#21516;&#27745;&#26579;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#36866;&#24212;&#26041;&#27861;&#23545;&#25991;&#26412;&#27745;&#26579;&#26356;&#25935;&#24863;&#65292;&#21333;&#29420;&#20351;&#29992;&#23567;&#22411;&#25991;&#26412;&#36866;&#37197;&#22120;&#27604;&#20849;&#20139;&#36866;&#37197;&#22120;&#26356;&#40065;&#26834;&#65292;&#21487;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#24178;&#20928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#34920;&#29616;&#30340;&#21508;&#31181;&#36866;&#24212;&#26041;&#27861;&#65292;&#22914; LoRA&#12289;prompts &#21644; adapters &#31561;&#24050;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36866;&#24212;&#26041;&#27861;&#23545;&#20110;&#20998;&#24067;&#20301;&#31227;&#30340;&#40065;&#26834;&#24615;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;11&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36866;&#24212;&#26041;&#27861;&#22312;4&#20010;&#35270;&#35273;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#32771;&#23519;&#20102;&#21487;&#29992;&#36866;&#24212;&#31034;&#20363;&#21644;&#36866;&#24212;&#36807;&#31243;&#20013;&#21487;&#35757;&#32451;&#21442;&#25968;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#22320;&#65292;&#24341;&#20837;&#20102;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;96&#31181;&#35270;&#35273;&#21644;87&#31181;&#25991;&#26412;&#27745;&#25439;&#65292;&#20197;&#30740;&#31350;&#19981;&#21516;&#36866;&#24212;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#65306;1&#65289;&#36866;&#24212;&#26041;&#27861;&#23545;&#25991;&#26412;&#27745;&#26579;&#27604;&#35270;&#35273;&#27745;&#26579;&#26356;&#25935;&#24863;&#12290;2) &#20840;&#37327;&#24494;&#35843;&#24182;&#19981;&#24635;&#33021;&#25552;&#20379;&#26368;&#39640;&#30340;&#40065;&#26834;&#24615;&#65307;&#30456;&#21453;&#65292;&#36866;&#37197;&#22120;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24178;&#20928;&#24615;&#33021;&#12290;3&#65289;&#19982;&#39044;&#26399;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21333;&#29420;&#20351;&#29992;&#23567;&#22411;&#25991;&#26412;&#36866;&#37197;&#22120;&#36890;&#24120;&#27604;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#31354;&#38388;&#20013;&#20849;&#20139;&#36866;&#37197;&#22120;&#26356;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various adaptation methods, such as LoRA, prompts, and adapters, have been proposed to enhance the performance of pre-trained vision-language models in specific domains. The robustness of these adaptation methods against distribution shifts have not been studied. In this study, we assess the robustness of 11 widely-used adaptation methods across 4 vision-language datasets under multimodal corruptions. Concretely, we introduce 7 benchmark datasets, including 96 visual and 87 textual corruptions, to investigate the robustness of different adaptation methods, the impact of available adaptation examples, and the influence of trainable parameter size during adaptation. Our analysis reveals that: 1) Adaptation methods are more sensitive to text corruptions than visual corruptions. 2) Full fine-tuning does not consistently provide the highest robustness; instead, adapters can achieve better robustness with comparable clean performance. 3) Contrary to expectations, our findings indicate that i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26550;&#26500;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#65288;GATr&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20309;&#25968;&#25454;&#38382;&#39064;&#12290;GATr&#20351;&#29992;&#25237;&#24433;&#20960;&#20309;&#20195;&#25968;&#34920;&#31034;&#36755;&#20837;&#36755;&#20986;&#21644;&#29366;&#24577;&#65292;&#20855;&#26377;&#21487;&#32553;&#25918;&#24615;&#12289;&#34920;&#36798;&#24615;&#12289;&#22810;&#21151;&#33021;&#24615;&#12290;&#22312;n&#20307;&#24314;&#27169;&#21644;&#26426;&#22120;&#20154;&#35268;&#21010;&#30340;&#23454;&#39564;&#20013;&#65292;GATr&#30456;&#23545;&#20110;&#38750;&#20960;&#20309;&#22522;&#32447;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.18415</link><description>&lt;p&gt;
&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Geometric Algebra Transformers. (arXiv:2305.18415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26550;&#26500;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#65288;GATr&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20309;&#25968;&#25454;&#38382;&#39064;&#12290;GATr&#20351;&#29992;&#25237;&#24433;&#20960;&#20309;&#20195;&#25968;&#34920;&#31034;&#36755;&#20837;&#36755;&#20986;&#21644;&#29366;&#24577;&#65292;&#20855;&#26377;&#21487;&#32553;&#25918;&#24615;&#12289;&#34920;&#36798;&#24615;&#12289;&#22810;&#21151;&#33021;&#24615;&#12290;&#22312;n&#20307;&#24314;&#27169;&#21644;&#26426;&#22120;&#20154;&#35268;&#21010;&#30340;&#23454;&#39564;&#20013;&#65292;GATr&#30456;&#23545;&#20110;&#38750;&#20960;&#20309;&#22522;&#32447;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#25968;&#25454;&#38382;&#39064;&#28041;&#21450;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26426;&#22120;&#20154;&#12289;&#21270;&#23398;&#21644;&#29289;&#29702;&#39046;&#22495;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#37319;&#29992;&#35768;&#22810;&#24418;&#24335;&#65292;&#20363;&#22914;&#28857;&#12289;&#26041;&#21521;&#21521;&#37327;&#12289;&#24179;&#38754;&#25110;&#21464;&#25442;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22914;&#27492;&#22810;&#31181;&#20960;&#20309;&#31867;&#22411;, &#21516;&#26102;&#23562;&#37325;&#23427;&#20204;&#30340;&#23545;&#31216;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#65288;GATr&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#20960;&#20309;&#25968;&#25454;&#30340;&#36890;&#29992;&#26550;&#26500;&#12290;GATr&#20351;&#29992;&#25237;&#24433;&#20960;&#20309;&#20195;&#25968;&#26469;&#34920;&#31034;&#36755;&#20837;&#12289;&#36755;&#20986;&#21644;&#38544;&#34255;&#29366;&#24577;&#65292;&#20854;&#25552;&#20379;&#24120;&#35265;&#20960;&#20309;&#23545;&#35937;&#30340;&#39640;&#25928;16&#32500;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#20197;&#21450;&#20316;&#29992;&#20110;&#23427;&#20204;&#30340;&#36816;&#31639;&#31526;&#12290;GATr&#26159;&#30456;&#23545;&#20110;E(3)&#65288;3D&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#23545;&#31216;&#32676;&#65289;&#31561;&#21464;&#30340;&#12290;&#20316;&#20026;&#21464;&#25442;&#22120;&#65292;GATr&#21487;&#25193;&#23637;&#12289;&#34920;&#36798;&#20016;&#23500;&#19988;&#22810;&#21151;&#33021;&#12290;&#22312;n&#20307;&#24314;&#27169;&#21644;&#26426;&#22120;&#20154;&#35268;&#21010;&#30340;&#23454;&#39564;&#20013;&#65292;GATr&#30456;&#23545;&#20110;&#38750;&#20960;&#20309;&#22522;&#32447;&#22343;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Problems involving geometric data arise in a variety of fields, including computer vision, robotics, chemistry, and physics. Such data can take numerous forms, such as points, direction vectors, planes, or transformations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric algebra, which offers an efficient 16-dimensional vector space representation of common geometric objects as well as operators acting on them. GATr is equivariant with respect to E(3), the symmetry group of 3D Euclidean space. As a transformer, GATr is scalable, expressive, and versatile. In experiments with n-body modeling and robotic planning, GATr shows strong improvements over non-geometric baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; Gram &#30697;&#38453;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#21644;&#23618;&#35268;&#33539;&#21270;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2305.18399</link><description>&lt;p&gt;
&#20851;&#20110;&#28608;&#27963;&#20989;&#25968;&#21644;&#35268;&#33539;&#21270;&#23545;&#21021;&#22987;&#21270;&#31561;&#36317;&#23884;&#20837;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the impact of activation and normalization in obtaining isometric embeddings at initialization. (arXiv:2305.18399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; Gram &#30697;&#38453;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#21644;&#23618;&#35268;&#33539;&#21270;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20498;&#25968;&#31532;&#20108;&#20010; Gram &#30697;&#38453;&#30340;&#32467;&#26500;&#65292;&#35813;&#30697;&#38453;&#21253;&#21547;&#19982;&#19968;&#25209;&#36755;&#20837;&#23545;&#24212;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#25104;&#23545;&#20869;&#31215;&#12290;&#22312;&#20960;&#31181;&#26550;&#26500;&#20013;&#65292;&#35266;&#23519;&#21040;&#22312;&#21021;&#22987;&#21270;&#26102;&#35813; Gram &#30697;&#38453;&#20250;&#38543;&#30528;&#28145;&#24230;&#21464;&#24471;&#36864;&#21270;&#65292;&#20174;&#32780;&#20005;&#37325;&#20943;&#32531;&#35757;&#32451;&#36895;&#24230;&#12290;&#35268;&#33539;&#21270;&#23618;&#22914;&#25209;&#22788;&#29702;&#35268;&#33539;&#21270;&#25110;&#23618;&#35268;&#33539;&#21270;&#65292;&#22312;&#38450;&#27490;&#31209;&#23849;&#28291;&#38382;&#39064;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#29616;&#26377;&#30340;&#29702;&#35770;&#32467;&#26524;&#26080;&#27861;&#20840;&#38754;&#35206;&#30422;&#24191;&#27867;&#29992;&#20110; transformer &#20013;&#30340;&#23618;&#35268;&#33539;&#21270;&#21644;&#26377;&#38480;&#28145;&#24230;&#19979;&#35268;&#33539;&#21270;&#30340;&#37327;&#21270;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#32467;&#21512;&#28608;&#27963;&#20989;&#25968;&#23618;&#20351;&#29992;&#30340;&#23618;&#35268;&#33539;&#21270;&#21487;&#20197;&#20351;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340; Gram &#30697;&#38453;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#24182;&#20351;&#29992;&#28608;&#27963;&#20989;&#25968;&#30340; Hermite &#23637;&#24320;&#26469;&#37327;&#21270;&#36825;&#20010;&#36895;&#24230;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the structure of the penultimate Gram matrix in deep neural networks, which contains the pairwise inner products of outputs corresponding to a batch of inputs. In several architectures it has been observed that this Gram matrix becomes degenerate with depth at initialization, which dramatically slows training. Normalization layers, such as batch or layer normalization, play a pivotal role in preventing the rank collapse issue. Despite promising advances, the existing theoretical results (i) do not extend to layer normalization, which is widely used in transformers, (ii) can not characterize the bias of normalization quantitatively at finite depth.  To bridge this gap, we provide a proof that layer normalization, in conjunction with activation layers, biases the Gram matrix of a multilayer perceptron towards isometry at an exponential rate with depth at initialization. We quantify this rate using the Hermite expansion of the activation function, highlighting th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GFlowNets&#30340;&#26426;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.17010</link><description>&lt;p&gt;
&#21033;&#29992;GFlowNets&#35299;&#20915;&#22270;&#24418;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Let the Flows Tell: Solving Graph Combinatorial Optimization Problems with GFlowNets. (arXiv:2305.17010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GFlowNets&#30340;&#26426;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#35757;&#32451;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#26159;NP&#38590;&#39064;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#31934;&#30830;&#31639;&#27861;&#65292;&#36825;&#20351;&#23427;&#20204;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#29702;&#24819;&#39046;&#22495;&#12290;&#36825;&#20123;&#38382;&#39064;&#20013;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#38480;&#21046;&#21487;&#33021;&#20250;&#30452;&#25509;&#38459;&#30861;&#20248;&#21270;&#25110;&#37319;&#26679;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;GFlowNets&#26368;&#36817;&#34987;&#21457;&#29616;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26426;&#22120;&#65292;&#21487;&#20197;&#39034;&#24207;&#22320;&#20174;&#22797;&#21512;&#38750;&#35268;&#33539;&#21270;&#23494;&#24230;&#20013;&#26377;&#25928;&#22320;&#37319;&#26679;&#65292;&#24182;&#20855;&#26377;&#22312;CO&#20013;&#20998;&#25674;&#27492;&#31867;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#36807;&#31243;&#20197;&#21450;&#29983;&#25104;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#20505;&#36873;&#39033;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#32452;&#21512;&#38382;&#39064;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#25552;&#20986;&#35757;&#32451;&#26377;&#26465;&#20214;&#30340;GFlowNets&#20174;&#35299;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#31574;&#30053;&#12290;&#36824;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#25216;&#26415;&#26469;&#21463;&#30410;&#20110;&#36828;&#31243;&#20449;&#29992;&#20998;&#37197;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#20351;&#29992;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#19981;&#21516;CO&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GFlowNet&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combinatorial optimization (CO) problems are often NP-hard and thus out of reach for exact algorithms, making them a tempting domain to apply machine learning methods. The highly structured constraints in these problems can hinder either optimization or sampling directly in the solution space. On the other hand, GFlowNets have recently emerged as a powerful machinery to efficiently sample from composite unnormalized densities sequentially and have the potential to amortize such solution-searching processes in CO, as well as generate diverse solution candidates. In this paper, we design Markov decision processes (MDPs) for different combinatorial problems and propose to train conditional GFlowNets to sample from the solution space. Efficient training techniques are also developed to benefit long-range credit assignment. Through extensive experiments on a variety of different CO tasks with synthetic and realistic data, we demonstrate that GFlowNet policies can efficiently find high-quali
&lt;/p&gt;</description></item><item><title>PDP&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.11203</link><description>&lt;p&gt;
PDP&#65306;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#21363;&#21487;&#25630;&#23450;
&lt;/p&gt;
&lt;p&gt;
PDP: Parameter-free Differentiable Pruning is All You Need. (arXiv:2305.11203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11203
&lt;/p&gt;
&lt;p&gt;
PDP&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#21098;&#26525;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#25552;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#24182;&#26368;&#23567;&#21270;DNN&#21152;&#36895;&#22120;&#19978;&#30340;&#21151;&#32791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#12289;&#26114;&#36149;&#25110;&#26080;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;/&#35821;&#35328;&#20219;&#21153;&#12289;DNN&#20307;&#31995;&#32467;&#26500;&#24182;&#36981;&#23432;&#32467;&#26500;&#21270;&#21098;&#26525;&#32422;&#26463;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;&#26102;&#38388;&#21098;&#26525;&#26041;&#26696;&#8212;&#8212;PDP&#65288;&#21442;&#25968;&#33258;&#30001;&#21487;&#24494;&#21098;&#26525;&#65289;&#65292;&#23427;&#22312;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;PDP&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26435;&#37325;&#30340;&#21160;&#24577;&#20989;&#25968;&#65292;&#20197;&#21442;&#25968;&#26080;&#20851;&#30340;&#26041;&#24335;&#20026;&#32473;&#23450;&#30340;&#21098;&#26525;&#30446;&#26631;&#29983;&#25104;&#36719;&#21098;&#26525;&#25513;&#30721;&#12290;&#34429;&#28982;&#26159;&#21487;&#24494;&#30340;&#65292;&#20294;&#26159;PDP&#30340;&#31616;&#21333;&#21644;&#39640;&#25928;&#20351;&#20854;&#36275;&#22815;&#26222;&#36941;&#65292;&#20197;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#38543;&#26426;/&#32467;&#26500;&#21270;/&#36890;&#36947;&#21098;&#26525;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;MobileNet-v1&#65292;PDP&#21487;&#20197;&#22312;86.6%&#30340;&#31232;&#30095;&#24230;&#19979;&#36798;&#21040;68.2%&#30340;ImageNet1k top-1&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNN pruning is a popular way to reduce the size of a model, improve the inference latency, and minimize the power consumption on DNN accelerators. However, existing approaches might be too complex, expensive or ineffective to apply to a variety of vision/language tasks, DNN architectures and to honor structured pruning constraints. In this paper, we propose an efficient yet effective train-time pruning scheme, Parameter-free Differentiable Pruning (PDP), which offers state-of-the-art qualities in model size, accuracy, and training cost. PDP uses a dynamic function of weights during training to generate soft pruning masks for the weights in a parameter-free manner for a given pruning target. While differentiable, the simplicity and efficiency of PDP make it universal enough to deliver state-of-the-art random/structured/channel pruning results on various vision and natural language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1 ImageNet1k accuracy at 86.6% sparsity, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARQ-Learning&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#38469;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#21516;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#32780;&#19988;&#65292;&#26412;&#25991;&#36824;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;PR-DQN&#21644;PR-DDPG&#12290;</title><link>http://arxiv.org/abs/2305.06657</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65306;&#30456;&#37051;&#19981;&#30830;&#23450;&#24615;&#38598;&#21644;&#21452;&#20195;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm. (arXiv:2305.06657v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARQ-Learning&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#38469;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#21516;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#32780;&#19988;&#65292;&#26412;&#25991;&#36824;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;PR-DQN&#21644;PR-DDPG&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#38598;&#19978;&#20248;&#21270;&#26368;&#24046;&#24615;&#33021;&#12290;&#32473;&#23450;&#19968;&#20010;&#20135;&#29983;&#35757;&#32451;&#26679;&#26412;&#30340;&#26631;&#20934;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;N-MDP&#65289;&#65292;&#35813;&#38598;&#21512;&#21253;&#21547;&#36890;&#36807;&#23545;N-MDP&#36827;&#34892;&#26576;&#20123;&#25200;&#21160;&#32780;&#33719;&#24471;&#30340;MDP&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#27604;&#29616;&#26377;&#38598;&#21512;&#26356;&#23454;&#38469;&#30340;MDP&#12290;&#20351;&#29992;&#36825;&#20010;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;RL&#31639;&#27861;&#65292;&#21517;&#20026;ARQ-Learning&#65292;&#29992;&#20110;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#35823;&#24046;&#30028;&#24182;&#35777;&#26126;&#23427;&#19982;Q-Learning&#21644;&#40065;&#26834;Q-Learning&#65288;&#21363;&#29616;&#26377;&#30340;&#40065;&#26834;RL&#26041;&#27861;&#65289;&#19968;&#26679;&#24555;&#22320;&#25910;&#25947;&#65292;&#21516;&#26102;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#23558;ARQ-Learning&#25193;&#23637;&#21040;&#22823;&#22411;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;&#20851;&#38190;&#29942;&#39048;&#12290;&#21033;&#29992;&#36825;&#19968;&#25216;&#26415;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;PRQ-Learning&#12290;&#25509;&#30528;&#65292;&#23558;&#20854;&#19982;DQN&#21644;DDPG&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#20998;&#21035;&#24320;&#21457;&#20102;PR-DQN&#21644;PR-DDPG&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#39046;&#22495;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust reinforcement learning (RL) aims at learning a policy that optimizes the worst-case performance over an uncertainty set. Given nominal Markov decision process (N-MDP) that generates samples for training, the set contains MDPs obtained by some perturbations from N-MDP. In this paper, we introduce a new uncertainty set containing more realistic MDPs in practice than the existing sets. Using this uncertainty set, we present a robust RL, named ARQ-Learning, for tabular cases. Also, we characterize the finite-time error bounds and prove that it converges as fast as Q-Learning and robust Q-Learning (i.e., the state-of-the-art robust RL method) while providing better robustness for real applications. We propose {\em pessimistic agent} that efficiently tackles the key bottleneck for the extension of ARQ-Learning into large or continuous state spaces. Using this technique, we first propose PRQ-Learning. To the next, combining this with DQN and DDPG, we develop PR-DQN and PR-DDPG, respect
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#25945;&#23398;&#24605;&#24819;&#30340;&#32852;&#37030;&#22810;&#33218;&#32769;&#34382;&#26426;&#35774;&#35745;&#65292;&#36890;&#36807;&#38544;&#24335;&#26412;&#22320;&#22870;&#21169;&#35843;&#25972;&#26469;&#25351;&#23548;&#23458;&#25143;&#31471;&#26397;&#30528;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#38431;&#26381;&#21153;&#31471;&#25552;&#20986;&#20102;&#32769;&#34382;&#26426;&#23398;&#20064;&#21644;&#30446;&#26631;&#25945;&#23398;&#20219;&#21153;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.02441</link><description>&lt;p&gt;
&#22522;&#20110;&#22870;&#21169;&#25945;&#23398;&#30340;&#32852;&#37030;&#22810;&#33218;&#32769;&#34382;&#26426;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Reward Teaching for Federated Multi-armed Bandits. (arXiv:2305.02441v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#25945;&#23398;&#24605;&#24819;&#30340;&#32852;&#37030;&#22810;&#33218;&#32769;&#34382;&#26426;&#35774;&#35745;&#65292;&#36890;&#36807;&#38544;&#24335;&#26412;&#22320;&#22870;&#21169;&#35843;&#25972;&#26469;&#25351;&#23548;&#23458;&#25143;&#31471;&#26397;&#30528;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#38431;&#26381;&#21153;&#31471;&#25552;&#20986;&#20102;&#32769;&#34382;&#26426;&#23398;&#20064;&#21644;&#30446;&#26631;&#25945;&#23398;&#20219;&#21153;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#37096;&#20998;&#24050;&#26377;&#30340;&#32852;&#37030;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;FMAB&#65289;&#35774;&#35745;&#37117;&#22522;&#20110;&#20551;&#35774;&#23458;&#25143;&#31471;&#20250;&#23454;&#29616;&#25351;&#23450;&#30340;&#35774;&#35745;&#26469;&#19982;&#26381;&#21153;&#22120;&#21327;&#20316;&#12290;&#20294;&#23454;&#38469;&#19978;&#65292;&#21487;&#33021;&#26080;&#27861;&#20462;&#25913;&#23458;&#25143;&#31471;&#29616;&#26377;&#30340;&#21327;&#35758;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#35813;&#24037;&#20316;&#20851;&#27880;&#22987;&#32456;&#26368;&#22823;&#21270;&#20854;&#20010;&#20307;&#32047;&#31215;&#22870;&#21169;&#30340;&#23458;&#25143;&#31471;&#65292;&#24182;&#24341;&#20837;&#20102;&#8220;&#22870;&#21169;&#25945;&#23398;&#8221;&#30340;&#26032;&#24605;&#24819;&#65292;&#21363;&#36890;&#36807;&#38544;&#24335;&#30340;&#26412;&#22320;&#22870;&#21169;&#35843;&#25972;&#25351;&#23548;&#23458;&#25143;&#31471;&#26397;&#30528;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#26381;&#21153;&#22120;&#38754;&#20020;&#20004;&#20010;&#23494;&#20999;&#32806;&#21512;&#30340;&#20219;&#21153;&#65292;&#21363;&#32769;&#34382;&#26426;&#23398;&#20064;&#21644;&#30446;&#26631;&#25945;&#23398;&#65292;&#23427;&#20204;&#30340;&#32467;&#21512;&#38750;&#24120;&#22797;&#26434;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026; &#8220;Teaching-After-Learning&#65288;TAL&#65289;&#8221; &#30340;&#20998;&#38454;&#27573;&#26041;&#27861;&#65292;&#20998;&#21035;&#40723;&#21169;&#21644;&#38480;&#21046;&#23458;&#25143;&#31471;&#30340;&#25506;&#32034;&#12290;&#24403;&#23458;&#25143;&#31471;&#31574;&#30053;&#28385;&#36275;&#19968;&#23450;&#30340;&#28201;&#21644;&#35201;&#27714;&#26102;&#65292;&#24314;&#31435;&#20102;TAL&#30340;&#32508;&#21512;&#24615;&#33021;&#20998;&#26512;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#25216;&#26415;&#26041;&#27861;&#26469;&#20998;&#26512;TAL&#30340;&#28909;&#21551;&#21160;&#21644;&#31639;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;TAL&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;FMAB&#35774;&#35745;&#24102;&#26469;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the existing federated multi-armed bandits (FMAB) designs are based on the presumption that clients will implement the specified design to collaborate with the server. In reality, however, it may not be possible to modify the client's existing protocols. To address this challenge, this work focuses on clients who always maximize their individual cumulative rewards, and introduces a novel idea of "reward teaching", where the server guides the clients towards global optimality through implicit local reward adjustments. Under this framework, the server faces two tightly coupled tasks of bandit learning and target teaching, whose combination is non-trivial and challenging. A phased approach, called Teaching-After-Learning (TAL), is first designed to encourage and discourage clients' explorations separately. General performance analyses of TAL are established when the clients' strategies satisfy certain mild requirements. With novel technical approaches developed to analyze the warm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04934</link><description>&lt;p&gt;
&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#31616;&#21270;&#26426;&#22120;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25968;&#25454;&#31649;&#21046;&#35201;&#27714;&#26426;&#22120;&#21453;&#23398;&#20064;&#65288;MU&#65289;&#65306;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#25351;&#23450;&#26679;&#20363;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26469;&#36827;&#34892;&#31934;&#30830;&#21453;&#23398;&#20064;&#65292;&#20294;&#26159;&#20854;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#20102;&#36817;&#20284;&#20294;&#39640;&#25928;&#30340;&#21453;&#23398;&#20064;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#38500;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;MU&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35270;&#35282;&#25512;&#36827;MU&#65306;&#36890;&#36807;&#26435;&#20540;&#20462;&#21098;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#31232;&#30095;&#24615;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#32553;&#23567;&#36817;&#20284;&#38388;&#38553;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#26377;&#20102;&#36825;&#20010;&#35748;&#35782;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#20010;&#26032;&#30340;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#20803;&#26041;&#26696;&#65292;&#31216;&#20026;&#8220;&#20808;&#20462;&#21098;&#65292;&#28982;&#21518;&#21453;&#23398;&#20064;&#8221;&#21644;&#8220;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#25552;&#35758;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#22987;&#32456;&#26377;&#30410;&#20110;MU&#65292;&#21253;&#25324;&#25353;&#31867;&#25968;&#25454;&#25830;&#38500;&#12289;&#38543;&#26426;&#25968;&#25454;&#25830;&#38500;&#21644;&#21518;&#38376;&#25968;&#25454;&#20266;&#36896;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#38754;&#37096;&#29305;&#24449;&#21457;&#29616;&#20102;&#20154;&#20204;&#30340;&#25919;&#27835;&#21462;&#21521;&#65292;&#29978;&#33267;&#21487;&#20197;&#20174;&#33258;&#28982;&#22270;&#20687;&#20013;&#25512;&#24191;&#12290;&#36825;&#31181;&#39044;&#27979;&#30340;&#31934;&#24230;&#27604;&#20154;&#31867;&#35780;&#20998;&#32773;&#39640;&#65292;&#30456;&#24403;&#20110;&#19968;&#20123;&#24037;&#20316;&#38754;&#35797;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16343</link><description>&lt;p&gt;
&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#21487;&#20197;&#20174;&#38754;&#37096;&#22270;&#20687;&#20013;&#26174;&#31034;&#25919;&#27835;&#21462;&#21521;&#65292;&#21363;&#20351;&#25511;&#21046;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#21644;&#33258;&#25105;&#34920;&#29616;&#12290;(arXiv: 2303.16343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
Facial recognition technology can expose political orientation from facial images even when controlling for demographics and self-presentation. (arXiv:2303.16343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#38754;&#37096;&#29305;&#24449;&#21457;&#29616;&#20102;&#20154;&#20204;&#30340;&#25919;&#27835;&#21462;&#21521;&#65292;&#29978;&#33267;&#21487;&#20197;&#20174;&#33258;&#28982;&#22270;&#20687;&#20013;&#25512;&#24191;&#12290;&#36825;&#31181;&#39044;&#27979;&#30340;&#31934;&#24230;&#27604;&#20154;&#31867;&#35780;&#20998;&#32773;&#39640;&#65292;&#30456;&#24403;&#20110;&#19968;&#20123;&#24037;&#20316;&#38754;&#35797;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36816;&#29992;&#38754;&#37096;&#35782;&#21035;&#31639;&#27861;&#65292;&#20174;&#23454;&#39564;&#23460;&#35774;&#32622;&#19979;&#25293;&#25668;&#30340;591&#24352;&#20013;&#24615;&#38754;&#37096;&#22270;&#20687;&#20013;&#25552;&#21462;&#38754;&#37096;&#25551;&#36848;&#31526;&#12290;&#22312;&#25511;&#21046;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#39044;&#27979;&#21442;&#19982;&#32773;&#22312;&#25919;&#27835;&#21462;&#21521;&#37327;&#34920;&#19978;&#30340;&#24471;&#20998;(Cronbach&#30340;&#945;=0.94)&#12290;&#27169;&#22411;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;r = 0.20&#65292;&#36828;&#20248;&#20110;&#20154;&#31867;&#35780;&#20998;&#32773;&#65292;&#19982;&#24037;&#20316;&#38754;&#35797;&#39044;&#27979;&#24037;&#20316;&#25104;&#21151;&#12289;&#37202;&#31934;&#39537;&#21160;&#25915;&#20987;&#24615;&#25110;&#24515;&#29702;&#27835;&#30103;&#25913;&#21892;&#24515;&#29702;&#20581;&#24247;&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#20174;&#26631;&#20934;&#21270;&#22270;&#20687;&#34893;&#29983;&#20986;&#30340;&#27169;&#22411;&#22312;3,401&#21517;&#26469;&#33258;&#32654;&#22269;&#12289;&#33521;&#22269;&#21644;&#21152;&#25343;&#22823;&#30340;&#25919;&#27835;&#20154;&#29289;&#30340;&#33258;&#28982;&#22270;&#20687;&#26679;&#26412;&#20013;&#34920;&#29616;&#33391;&#22909;(r = 0.12)&#65292;&#34920;&#26126;&#38754;&#37096;&#22806;&#35980;&#21644;&#25919;&#27835;&#21462;&#21521;&#20043;&#38388;&#30340;&#20851;&#32852;&#21487;&#25512;&#24191;&#21040;&#25105;&#20204;&#20043;&#22806;&#30340;&#20154;&#32676;&#12290;&#38754;&#37096;&#29305;&#24449;&#19982;&#25919;&#27835;&#21462;&#21521;&#30456;&#20851;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#20445;&#23432;&#27966;&#30340;&#19979;&#21322;&#33080;&#37096;&#20998;&#26356;&#22823;&#65292;&#34429;&#28982;&#25919;&#27835;&#21462;&#21521;&#19981;&#33021;&#20934;&#30830;&#22320;&#39044;&#27979;&#20010;&#20307;&#38754;&#37096;&#29305;&#24449;&#30340;&#25152;&#26377;&#21464;&#21270;&#65292;&#20294;&#26159;&#36825;&#31181;&#21457;&#29616;&#36824;&#26159;&#23500;&#26377;&#21551;&#21457;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A facial recognition algorithm was used to extract face descriptors from carefully standardized images of 591 neutral faces taken in the laboratory setting. Face descriptors were entered into a cross-validated linear regression to predict participants' scores on a political orientation scale (Cronbach's alpha=.94) while controlling for age, gender, and ethnicity. The model's performance exceeded r=.20: much better than that of human raters and on par with how well job interviews predict job success, alcohol drives aggressiveness, or psychological therapy improves mental health. Moreover, the model derived from standardized images performed well (r=.12) in a sample of naturalistic images of 3,401 politicians from the U.S., UK, and Canada, suggesting that the associations between facial appearance and political orientation generalize beyond our sample. The analysis of facial features associated with political orientation revealed that conservatives had larger lower faces, although politi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26368;&#20248;&#25191;&#34892;&#20132;&#26131;&#26234;&#33021;&#20307;&#19982;&#21453;&#24212;&#24335;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#27169;&#22411;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#24179;&#34913;&#25191;&#34892;&#24046;&#20215;&#21644;&#26410;&#33021;&#21450;&#26102;&#25191;&#34892;&#35746;&#21333;&#30340;&#24809;&#32602;&#65292;&#35828;&#26126;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#25968;&#37327;&#12289;&#21021;&#22987;&#35746;&#21333;&#22823;&#23567;&#21644;&#29366;&#24577;&#31354;&#38388;&#30340;&#21464;&#21270;&#65292;&#20250;&#23545;&#26368;&#23567;&#26234;&#33021;&#24066;&#22330;&#27169;&#25311;&#36896;&#25104;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.07393</link><description>&lt;p&gt;
&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#19982;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#24066;&#22330;&#27169;&#22411;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Many learning agents interacting with an agent-based market model. (arXiv:2303.07393v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26368;&#20248;&#25191;&#34892;&#20132;&#26131;&#26234;&#33021;&#20307;&#19982;&#21453;&#24212;&#24335;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#27169;&#22411;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#24179;&#34913;&#25191;&#34892;&#24046;&#20215;&#21644;&#26410;&#33021;&#21450;&#26102;&#25191;&#34892;&#35746;&#21333;&#30340;&#24809;&#32602;&#65292;&#35828;&#26126;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#25968;&#37327;&#12289;&#21021;&#22987;&#35746;&#21333;&#22823;&#23567;&#21644;&#29366;&#24577;&#31354;&#38388;&#30340;&#21464;&#21270;&#65292;&#20250;&#23545;&#26368;&#23567;&#26234;&#33021;&#24066;&#22330;&#27169;&#25311;&#36896;&#25104;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26368;&#20248;&#25191;&#34892;&#20132;&#26131;&#26234;&#33021;&#20307;&#19982;&#22312;&#20107;&#20214;&#26102;&#38388;&#19979;&#30340;&#21453;&#24212;&#24335;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#27169;&#22411;&#30340;&#21160;&#24577;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#20010;&#24066;&#22330;&#29983;&#24577;&#31995;&#32479;&#65292;&#30001;&#19977;&#20010;&#33829;&#20859;&#32423;&#21035;&#20195;&#34920;&#65306;&#26368;&#20248;&#25191;&#34892;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#26368;&#23567;&#26234;&#33021;&#30340;&#27969;&#21160;&#24615;&#38656;&#35201;&#32773;&#21644;&#24555;&#36895;&#30340;&#30005;&#23376;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#12290;&#26368;&#20248;&#25191;&#34892;&#20195;&#29702;&#31867;&#21035;&#21253;&#25324;&#20080;&#20837;&#21644;&#21334;&#20986;&#20195;&#29702;&#65292;&#21487;&#20197;&#20351;&#29992;&#38480;&#20215;&#21333;&#21644;&#24066;&#20215;&#21333;&#30340;&#32452;&#21512;&#65292;&#25110;&#32773;&#20165;&#20351;&#29992;&#24066;&#20215;&#21333;&#36827;&#34892;&#20132;&#26131;&#12290;&#22870;&#21169;&#20989;&#25968;&#26126;&#30830;&#24179;&#34913;&#20102;&#20132;&#26131;&#25191;&#34892;&#24046;&#20215;&#19982;&#26410;&#33021;&#21450;&#26102;&#25191;&#34892;&#35746;&#21333;&#30340;&#24809;&#32602;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22810;&#20010;&#31454;&#20105;&#23398;&#20064;&#26234;&#33021;&#20307;&#22914;&#20309;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#21021;&#22987;&#35746;&#21333;&#30340;&#22823;&#23567;&#21644;&#29992;&#20110;&#23398;&#20064;&#30340;&#29366;&#24577;&#31354;&#38388;&#30340;&#20989;&#25968;&#24433;&#21709;&#26368;&#23567;&#26234;&#33021;&#24066;&#22330;&#27169;&#25311;&#12290;&#25105;&#20204;&#20351;&#29992;&#30456;&#31354;&#38388;&#22270;&#26469;&#30740;&#31350;ABM&#30340;&#21160;&#24577;&#65292;&#24403;&#29305;&#23450;&#35268;&#33539;&#34987;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
We consider the dynamics and the interactions of multiple reinforcement learning optimal execution trading agents interacting with a reactive Agent-Based Model (ABM) of a financial market in event time. The model represents a market ecology with 3-trophic levels represented by: optimal execution learning agents, minimally intelligent liquidity takers, and fast electronic liquidity providers. The optimal execution agent classes include buying and selling agents that can either use a combination of limit orders and market orders, or only trade using market orders. The reward function explicitly balances trade execution slippage against the penalty of not executing the order timeously. This work demonstrates how multiple competing learning agents impact a minimally intelligent market simulation as functions of the number of agents, the size of agents' initial orders, and the state spaces used for learning. We use phase space plots to examine the dynamics of the ABM, when various specifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MSGBiO&#21644;VR-MSGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#38543;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#25910;&#25947;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;MGBiO&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.03944</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#26041;&#27861;&#22312;&#38750;&#20984;&#19979;&#23618;&#21452;&#23618;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Momentum-Based Gradient Methods for Bilevel Optimization with Nonconvex Lower-Level. (arXiv:2303.03944v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MSGBiO&#21644;VR-MSGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#38543;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#25910;&#25947;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;MGBiO&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20004;&#32423;&#23618;&#27425;&#20248;&#21270;&#26041;&#27861;&#65292;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#36229;&#21442;&#25968;&#23398;&#20064;&#12289;&#20803;&#23398;&#20064;&#21644;&#25345;&#32493;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#19979;&#23618;&#38382;&#39064;&#20026;&#38750;&#20984;&#26102;&#65292;&#21452;&#23618;&#26041;&#27861;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#19978;&#23618;&#38382;&#39064;&#21644;&#19979;&#23618;&#38382;&#39064;&#22343;&#20026;&#38750;&#20984;&#38382;&#39064;&#65292;&#24182;&#19988;&#19979;&#23618;&#38382;&#39064;&#28385;&#36275;Polyak-Lojasiewicz (PL)&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#39640;&#25928;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#21452;&#23618;&#26041;&#27861;(MSGBiO&#21644;VR-MSGBiO)&#26469;&#35299;&#20915;&#36825;&#20123;&#38543;&#26426;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;&#25910;&#25947;&#20998;&#26512;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#19968;&#20123;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MGBiO&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization is a popular two-level hierarchical optimization, which has been widely applied to many machine learning tasks such as hyperparameter learning, meta learning and continual learning. Although many bilevel optimization methods recently have been developed, the bilevel methods are not well studied when the lower-level problem is nonconvex. To fill this gap, in the paper, we study a class of nonconvex bilevel optimization problems, where both upper-level and lower-level problems are nonconvex, and the lower-level problem satisfies Polyak-{\L}ojasiewicz (PL) condition. We propose an efficient momentum-based gradient bilevel method (MGBiO) to solve these deterministic problems. Meanwhile, we propose a class of efficient momentum-based stochastic gradient bilevel methods (MSGBiO and VR-MSGBiO) to solve these stochastic problems. Moreover, we provide a useful convergence analysis framework for our methods. Specifically, under some mild conditions, we prove that our MGBiO m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;&#65288;DBC&#65289;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#20998;&#24067;&#30340;&#26465;&#20214;&#21644;&#32852;&#21512;&#27010;&#29575;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#24314;&#27169;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13335</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model-Augmented Behavioral Cloning. (arXiv:2302.13335v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;&#65288;DBC&#65289;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#20998;&#24067;&#30340;&#26465;&#20214;&#21644;&#32852;&#21512;&#27010;&#29575;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#24314;&#27169;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#35299;&#20915;&#20102;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#28436;&#31034;&#32780;&#27809;&#26377;&#35775;&#38382;&#29615;&#22659;&#22870;&#21169;&#20449;&#21495;&#30340;&#23398;&#20064;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#19981;&#38656;&#35201;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#35201;&#20040;&#23558;&#19987;&#23478;&#20998;&#24067;&#24314;&#27169;&#20026;&#26465;&#20214;&#27010;&#29575;p(a|s)&#65288;&#20363;&#22914;&#65292;&#34892;&#20026;&#20811;&#38534;&#65292;BC&#65289;&#65292;&#35201;&#20040;&#23558;&#32852;&#21512;&#27010;&#29575;p(s,a)&#24314;&#27169;&#65288;&#20363;&#22914;&#65292;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#65289;&#12290;&#23613;&#31649;&#34892;&#20026;&#20811;&#38534;&#23545;&#20110;&#24314;&#27169;&#26465;&#20214;&#27010;&#29575;&#30340;&#31616;&#21333;&#24615;&#65292;&#20294;&#36890;&#24120;&#38590;&#20197;&#27867;&#21270;&#12290;&#34429;&#28982;&#23545;&#32852;&#21512;&#27010;&#29575;&#36827;&#34892;&#24314;&#27169;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#25512;&#29702;&#36807;&#31243;&#21487;&#33021;&#32791;&#26102;&#65292;&#24182;&#19988;&#24448;&#24448;&#20250;&#36973;&#21463;&#27969;&#24418;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20174;&#24314;&#27169;&#19987;&#23478;&#20998;&#24067;&#30340;&#26465;&#20214;&#21644;&#32852;&#21512;&#27010;&#29575;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#30340;&#34892;&#20026;&#20811;&#38534;&#65288;DBC&#65289;&#37319;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#19987;&#23478;&#34892;&#20026;&#65292;&#24182;&#23398;&#20064;&#19968;&#31181;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#26681;&#25454;&#28151;&#21512;&#27010;&#29575;&#20998;&#24067;&#37319;&#26679;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning addresses the challenge of learning by observing an expert's demonstrations without access to reward signals from environments. Most existing imitation learning methods that do not require interacting with environments either model the expert distribution as the conditional probability p(a|s) (e.g., behavioral cloning, BC) or the joint probability p(s, a) (e.g., implicit behavioral cloning). Despite its simplicity, modeling the conditional probability with BC usually struggles with generalization. While modeling the joint probability can lead to improved generalization performance, the inference procedure can be time-consuming and it often suffers from manifold overfitting. This work proposes an imitation learning framework that benefits from modeling both the conditional and joint probability of the expert distribution. Our proposed diffusion model-augmented behavioral cloning (DBC) employs a diffusion model trained to model expert behaviors and learns a policy to o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DeepONets&#30340;&#22810;&#20449;&#24230;&#26041;&#27861;&#26469;&#25552;&#39640;&#38477;&#38454;&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;&#36890;&#36807;&#23558;&#27169;&#22411;&#38477;&#38454;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#27531;&#24046;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23398;&#20064;&#24182;&#25512;&#26029;&#26032;&#39044;&#27979;&#30340;&#35823;&#24046;&#12290;&#35813;&#26694;&#26550;&#26368;&#22823;&#21270;&#21033;&#29992;&#39640;&#20449;&#24230;&#20449;&#24687;&#65292;&#29992;&#20110;&#26500;&#24314;&#38477;&#38454;&#27169;&#22411;&#21644;&#23398;&#20064;&#27531;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.12682</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38477;&#38454;&#24314;&#27169;&#20013;&#27531;&#24046;&#23398;&#20064;&#30340;&#22810;&#20449;&#24230;DeepONet&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A DeepONet multi-fidelity approach for residual learning in reduced order modeling. (arXiv:2302.12682v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DeepONets&#30340;&#22810;&#20449;&#24230;&#26041;&#27861;&#26469;&#25552;&#39640;&#38477;&#38454;&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;&#36890;&#36807;&#23558;&#27169;&#22411;&#38477;&#38454;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#27531;&#24046;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23398;&#20064;&#24182;&#25512;&#26029;&#26032;&#39044;&#27979;&#30340;&#35823;&#24046;&#12290;&#35813;&#26694;&#26550;&#26368;&#22823;&#21270;&#21033;&#29992;&#39640;&#20449;&#24230;&#20449;&#24687;&#65292;&#29992;&#20110;&#26500;&#24314;&#38477;&#38454;&#27169;&#22411;&#21644;&#23398;&#20064;&#27531;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#20449;&#24230;&#35270;&#35282;&#21644;DeepONets&#26469;&#25552;&#39640;&#38477;&#38454;&#27169;&#22411;&#31934;&#24230;&#30340;&#26032;&#26041;&#27861;&#12290;&#38477;&#38454;&#27169;&#22411;&#36890;&#36807;&#31616;&#21270;&#21407;&#22987;&#27169;&#22411;&#25552;&#20379;&#23454;&#26102;&#25968;&#20540;&#36817;&#20284;&#12290;&#36890;&#24120;&#24573;&#30053;&#36825;&#31181;&#25805;&#20316;&#24341;&#20837;&#30340;&#35823;&#24046;&#20197;&#23454;&#29616;&#24555;&#36895;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#27169;&#22411;&#38477;&#38454;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#27531;&#24046;&#23398;&#20064;&#30456;&#32806;&#21512;&#65292;&#20197;&#20415;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#24182;&#25512;&#26029;&#26032;&#39044;&#27979;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#24378;&#35843;&#35813;&#26694;&#26550;&#26368;&#22823;&#21270;&#21033;&#29992;&#39640;&#20449;&#24230;&#20449;&#24687;&#65292;&#29992;&#20110;&#26500;&#24314;&#38477;&#38454;&#27169;&#22411;&#21644;&#23398;&#20064;&#27531;&#24046;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25506;&#32034;&#20102;&#36866;&#29992;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20027;&#20803;&#27491;&#20132;&#20998;&#35299;&#65288;POD&#65289;&#21644;&#32570;&#22833;POD&#19982;&#26368;&#26032;&#30340;DeepONet&#26550;&#26500;&#30340;&#38598;&#25104;&#12290;&#23545;&#19968;&#20010;&#21442;&#25968;&#22522;&#20934;&#20989;&#25968;&#21644;&#19968;&#20010;&#38750;&#32447;&#24615;&#21442;&#25968;Navier-Stokes&#38382;&#39064;&#36827;&#34892;&#20102;&#25968;&#20540;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the present work, we introduce a novel approach to enhance the precision of reduced order models by exploiting a multi-fidelity perspective and DeepONets. Reduced models provide a real-time numerical approximation by simplifying the original model. The error introduced by the such operation is usually neglected and sacrificed in order to reach a fast computation. We propose to couple the model reduction to a machine learning residual learning, such that the above-mentioned error can be learned by a neural network and inferred for new predictions. We emphasize that the framework maximizes the exploitation of high-fidelity information, using it for building the reduced order model and for learning the residual. In this work, we explore the integration of proper orthogonal decomposition (POD), and gappy POD for sensors data, with the recent DeepONet architecture. Numerical investigations for a parametric benchmark function and a nonlinear parametric Navier-Stokes problem are presented.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.11552</link><description>&lt;p&gt;
&#20943;&#23569;&#12289;&#37325;&#22797;&#21033;&#29992;&#12289;&#22238;&#25910;&#65306;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC. (arXiv:2302.11552v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11552
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#25193;&#25955;&#27169;&#22411;&#38382;&#19990;&#20197;&#26469;&#65292;&#23427;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#32463;&#36805;&#36895;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#23427;&#20204;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#23398;&#20064;&#19968;&#31995;&#21015;&#26102;&#21464;&#30340;&#23545;&#25968;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#36825;&#31181;&#35299;&#37322;&#24050;&#32463;&#28608;&#21457;&#20102;&#22522;&#20110;&#20998;&#31867;&#22120;&#21644;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#24605;&#24819;&#25104;&#20026;&#21518;&#32493;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#36825;&#20123;&#24819;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#25968;-based&#35299;&#37322;&#65292;&#25506;&#32034;&#20102;&#29992;&#20110;&#28041;&#21450;&#32452;&#21512;&#29983;&#25104;&#21644;&#25351;&#23548;&#30340;&#26465;&#20214;&#12289;&#20462;&#25913;&#21644;&#37325;&#22797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20026;&#20160;&#20040;&#26576;&#20123;&#31867;&#22411;&#30340;&#32452;&#21512;&#20351;&#29992;&#24403;&#21069;&#25216;&#26415;&#22833;&#36133;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#37319;&#26679;&#32773;(&#32780;&#19981;&#26159;&#27169;&#22411;)&#23545;&#27492;&#22833;&#36133;&#36127;&#26377;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#37319;&#26679;&#22120;&#65292;&#21463;MCMC&#30340;&#21551;&#21457;&#65292;&#20351;&#32452;&#21512;&#29983;&#25104;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#26356;&#21152;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#36873;&#25321;&#26694;&#26550;&#65288;DSIR&#65289;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;KL&#20943;&#23569;&#20316;&#20026;&#25968;&#25454;&#24230;&#37327;&#26469;&#30830;&#23450;&#21512;&#36866;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#22312;&#38477;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#20272;&#35745;&#37325;&#35201;&#24615;&#26435;&#37325;&#20197;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2302.03169</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Data Selection for Language Models via Importance Resampling. (arXiv:2302.03169v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03169
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#36873;&#25321;&#26694;&#26550;&#65288;DSIR&#65289;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;KL&#20943;&#23569;&#20316;&#20026;&#25968;&#25454;&#24230;&#37327;&#26469;&#30830;&#23450;&#21512;&#36866;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#22312;&#38477;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#20272;&#35745;&#37325;&#35201;&#24615;&#26435;&#37325;&#20197;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#20110;&#36890;&#29992;&#39046;&#22495;&#65288;&#22914;GPT-3&#65289;&#21644;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;Codex&#65289;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20174;&#22823;&#22411;&#21407;&#22987;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#19968;&#20010;&#23376;&#38598;&#65292;&#20197;&#21305;&#37197;&#32473;&#23450;&#19968;&#20123;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#30340;&#25152;&#38656;&#30446;&#26631;&#20998;&#24067;&#12290;&#37492;&#20110;&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#24230;&#65292;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#19987;&#23478;&#25163;&#21160;&#31574;&#21010;&#25968;&#25454;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22312;LM&#25968;&#25454;&#36873;&#25321;&#20013;&#20351;&#29992;&#30340;&#32463;&#20856;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#20302;&#32500;&#31354;&#38388;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#25968;&#25454;&#36873;&#25321;&#19982;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#65288;DSIR&#65289;&#65292;&#23427;&#22312;&#19968;&#20010;&#38477;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#20272;&#35745;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#20197;&#20415;&#26681;&#25454;&#36825;&#20123;&#26435;&#37325;&#36827;&#34892;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#25968;&#25454;&#36873;&#25321;&#12290;&#20026;&#20102;&#30830;&#23450;&#19968;&#20010;&#21512;&#36866;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;KL&#20943;&#23569;&#65292;&#19968;&#31181;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#34913;&#37327;&#25152;&#36873;&#39044;&#35757;&#32451;&#25968;&#25454;&#19982;&#30446;&#26631;&#20043;&#38388;&#30456;&#20284;&#24230;&#30340;&#25968;&#25454;&#24230;&#37327;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting a suitable pretraining dataset is crucial for both general-domain (e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We formalize this problem as selecting a subset of a large raw unlabeled dataset to match a desired target distribution given some unlabeled target samples. Due to the large scale and dimensionality of the raw text data, existing methods use simple heuristics or use experts to manually curate data. Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. We propose Data Selection with Importance Resampling (DSIR), an efficient and scalable framework that estimates importance weights in a reduced feature space for tractability and selects data with importance resampling according to these weights. To determine an appropriate feature space, we show that KL reduction, a data metric that measures the proximity between selected pretraining data and the target in a feature space, has high correlat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21333;&#27425;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#65292;&#24182;&#32473;&#20986;&#20102;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.10890</link><description>&lt;p&gt;
&#21333;&#27425;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#36866;&#29992;&#20110;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;
&lt;/p&gt;
&lt;p&gt;
Single-Pass Contrastive Learning Can Work for Both Homophilic and Heterophilic Graph. (arXiv:2211.10890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21333;&#27425;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#65292;&#24182;&#32473;&#20986;&#20102;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#25165;&#33021;&#26500;&#24314;&#20135;&#29983;&#23545;&#27604;&#30340;&#25439;&#22833;&#65292;&#36825;&#23545;&#20110;&#25429;&#25417;&#33410;&#28857;&#29305;&#24449;&#30340;&#20302;&#39057;&#20449;&#21495;&#26159;&#26377;&#25928;&#30340;&#12290;&#36825;&#31181;&#21452;&#36890;&#36947;&#35774;&#35745;&#24050;&#32463;&#22312;&#21516;&#26500;&#22270;&#19978;&#34920;&#29616;&#20986;&#23454;&#35777;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#22312;&#24322;&#26500;&#22270;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#30452;&#25509;&#36830;&#25509;&#30340;&#33410;&#28857;&#36890;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#65292;&#23578;&#19981;&#28165;&#26970;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;GCL&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#21463;&#21040;GCL&#26041;&#27861;&#22312;&#24322;&#26500;&#22270;&#19978;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#30340;&#24433;&#21709;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#20110;&#26159;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#23601;&#20986;&#29616;&#20102;&#65306;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#19968;&#31181;&#36866;&#29992;&#20110;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#30340;GCL&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22312;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#19978;&#36890;&#36807;&#37051;&#22495;&#32858;&#21512;&#24471;&#21040;&#30340;&#29305;&#24449;&#30340;&#38598;&#20013;&#24615;&#36136;&#65292;&#24341;&#20837;&#20102;&#21333;&#36890;&#36947;&#22270;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing graph contrastive learning (GCL) techniques typically require two forward passes for a single instance to construct the contrastive loss, which is effective for capturing the low-frequency signals of node features. Such a dual-pass design has shown empirical success on homophilic graphs, but its effectiveness on heterophilic graphs, where directly connected nodes typically have different labels, is unknown. In addition, existing GCL approaches fail to provide strong performance guarantees. Coupled with the unpredictability of GCL approaches on heterophilic graphs, their applicability in real-world contexts is limited. Then, a natural question arises: Can we design a GCL method that works for both homophilic and heterophilic graphs with a performance guarantee? To answer this question, we theoretically study the concentration property of features obtained by neighborhood aggregation on homophilic and heterophilic graphs, introduce the single-pass graph contrastive learning loss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PopArt&#30340;&#39640;&#25928;&#31232;&#30095;&#32447;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;Lasso&#65292;&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#20855;&#26377;&#26356;&#32039;&#30340;$\ell_1$&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#22522;&#20110;&#27492;&#25512;&#23548;&#20986;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#31639;&#27861;&#65292;&#20855;&#26377;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#30340;&#21305;&#37197;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2210.15345</link><description>&lt;p&gt;
PopArt: &#39640;&#25928;&#31232;&#30095;&#22238;&#24402;&#21644;&#20248;&#21270;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#30340;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
PopArt: Efficient Sparse Regression and Experimental Design for Optimal Sparse Linear Bandits. (arXiv:2210.15345v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PopArt&#30340;&#39640;&#25928;&#31232;&#30095;&#32447;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;Lasso&#65292;&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#20855;&#26377;&#26356;&#32039;&#30340;$\ell_1$&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#22522;&#20110;&#27492;&#25512;&#23548;&#20986;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#31639;&#27861;&#65292;&#20855;&#26377;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#30340;&#21305;&#37197;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#25353;&#39034;&#24207;&#36873;&#25321;&#19968;&#20010;&#21160;&#20316;&#24182;&#25509;&#25910;&#22870;&#21169;&#21453;&#39304;&#65292;&#32780;&#22870;&#21169;&#20989;&#25968;&#32447;&#24615;&#20381;&#36182;&#20110;&#21160;&#20316;&#30340;&#19968;&#20123;&#22352;&#26631;&#30340;&#21327;&#21464;&#37327;&#12290;&#36825;&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#37117;&#26377;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#35745;&#31639;&#39640;&#25928;&#30340;&#31232;&#30095;&#32447;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;PopArt&#65292;&#19982;Lasso&#65288;Tibshirani, 1996&#65289;&#30456;&#27604;&#65292;&#23427;&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#20855;&#26377;&#26356;&#32039;&#30340;$\ell_1$&#24674;&#22797;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#33258;&#28982;&#22320;&#28608;&#21457;&#20102;&#19968;&#31181;&#20984;&#23454;&#39564;&#35774;&#35745;&#20934;&#21017;&#65292;&#22240;&#27492;&#22312;&#35745;&#31639;&#19978;&#26159;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26032;&#20272;&#35745;&#22120;&#21644;&#35774;&#35745;&#20934;&#21017;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#31639;&#27861;&#65292;&#20854;&#22312;&#32473;&#23450;&#21160;&#20316;&#38598;&#30340;&#20960;&#20309;&#24615;&#26041;&#38754;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#65288;Hao et al., 2020&#65289;&#20855;&#26377;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#30340;&#21305;&#37197;&#19979;&#30028;&#65292;&#36825;&#22635;&#34917;&#20102;&#19978;&#30028;&#21644;&#19979;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
In sparse linear bandits, a learning agent sequentially selects an action and receive reward feedback, and the reward function depends linearly on a few coordinates of the covariates of the actions. This has applications in many real-world sequential decision making problems. In this paper, we propose a simple and computationally efficient sparse linear estimation method called PopArt that enjoys a tighter $\ell_1$ recovery guarantee compared to Lasso (Tibshirani, 1996) in many problems. Our bound naturally motivates an experimental design criterion that is convex and thus computationally efficient to solve. Based on our novel estimator and design criterion, we derive sparse linear bandit algorithms that enjoy improved regret upper bounds upon the state of the art (Hao et al., 2020), especially w.r.t. the geometry of the given action set. Finally, we prove a matching lower bound for sparse linear bandits in the data-poor regime, which closes the gap between upper and lower bounds in pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaxNet&#30340;&#29289;&#29702;&#24863;&#30693;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#22320;&#23398;&#20064;3D&#23567;&#20998;&#23376;&#21644;&#22823;&#20998;&#23376;&#22797;&#21512;&#29289;&#30340;&#34920;&#31034;&#12290;PaxNet&#36890;&#36807;&#20998;&#31163;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#30340;&#24314;&#27169;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#35282;&#24230;&#30456;&#20851;&#30340;&#35745;&#31639;&#12290;&#23427;&#21487;&#20197;&#39044;&#27979;&#30690;&#37327;&#23646;&#24615;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.02789</link><description>&lt;p&gt;
&#39640;&#25928;&#20934;&#30830;&#30340;&#29289;&#29702;&#24863;&#30693;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;3D&#23567;&#20998;&#23376;&#21644;&#22823;&#20998;&#23376;&#22797;&#21512;&#29289;
&lt;/p&gt;
&lt;p&gt;
Efficient and Accurate Physics-aware Multiplex Graph Neural Networks for 3D Small Molecules and Macromolecule Complexes. (arXiv:2206.02789v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaxNet&#30340;&#29289;&#29702;&#24863;&#30693;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#22320;&#23398;&#20064;3D&#23567;&#20998;&#23376;&#21644;&#22823;&#20998;&#23376;&#22797;&#21512;&#29289;&#30340;&#34920;&#31034;&#12290;PaxNet&#36890;&#36807;&#20998;&#31163;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#30340;&#24314;&#27169;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#35282;&#24230;&#30456;&#20851;&#30340;&#35745;&#31639;&#12290;&#23427;&#21487;&#20197;&#39044;&#27979;&#30690;&#37327;&#23646;&#24615;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#20998;&#23376;&#31185;&#23398;&#20013;&#24212;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#20351;&#29992;GNNs&#23398;&#20064;&#19977;&#32500;&#65288;3D&#65289;&#32467;&#26500;&#34920;&#31034;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GNNs&#23384;&#22312;&#22810;&#26679;&#24615;&#30456;&#20114;&#20316;&#29992;&#24314;&#27169;&#19981;&#36275;&#12289;&#35745;&#31639;&#24320;&#38144;&#22823;&#21644;&#24573;&#35270;&#30690;&#37327;&#20540;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#27169;&#22411;&#65292;&#21363;&#29289;&#29702;&#24863;&#30693;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PaxNet&#65289;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#20197;&#20415;&#22312;&#23567;&#26377;&#26426;&#21270;&#21512;&#29289;&#21644;&#22823;&#20998;&#23376;&#22797;&#21512;&#29289;&#30340;3D&#20998;&#23376;&#34920;&#31034;&#20013;&#39640;&#25928;&#20934;&#30830;&#22320;&#23398;&#20064;&#12290;PaxNet&#36890;&#36807;&#20998;&#31163;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#30340;&#24314;&#27169;&#65292;&#21463;&#21040;&#20998;&#23376;&#21147;&#23398;&#30340;&#21551;&#21457;&#65292;&#24182;&#20943;&#23569;&#20102;&#26114;&#36149;&#30340;&#19982;&#35282;&#24230;&#30456;&#20851;&#30340;&#35745;&#31639;&#12290;&#38500;&#20102;&#26631;&#37327;&#23646;&#24615;&#22806;&#65292;PaxNet&#36824;&#21487;&#20197;&#36890;&#36807;&#20026;&#27599;&#20010;&#21407;&#23376;&#23398;&#20064;&#19968;&#20010;&#30456;&#20851;&#32852;&#30340;&#30690;&#37327;&#26469;&#39044;&#27979;&#30690;&#37327;&#23646;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;PaxNet&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20004;&#20010;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in applying Graph Neural Networks (GNNs) to molecular science have showcased the power of learning three-dimensional (3D) structure representations with GNNs. However, most existing GNNs suffer from the limitations of insufficient modeling of diverse interactions, computational expensive operations, and ignorance of vectorial values. Here, we tackle these limitations by proposing a novel GNN model, Physics-aware Multiplex Graph Neural Network (PaxNet), to efficiently and accurately learn the representations of 3D molecules for both small organic compounds and macromolecule complexes. PaxNet separates the modeling of local and non-local interactions inspired by molecular mechanics, and reduces the expensive angle-related computations. Besides scalar properties, PaxNet can also predict vectorial properties by learning an associated vector for each atom. To evaluate the performance of PaxNet, we compare it with state-of-the-art baselines in two tasks. On small molecule dat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29616;&#26377;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;RL&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#24182;&#22312;&#20498;&#31435;&#25670;&#21644;&#22235;&#26059;&#32764;&#31283;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#37117;&#26159;&#23433;&#20840;&#30340;&#19988;&#34920;&#29616;&#19982;&#19981;&#23433;&#20840;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2205.06750</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#29702;&#35770;&#21644;&#23454;&#39564;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Provably Safe Reinforcement Learning: A Theoretical and Experimental Comparison. (arXiv:2205.06750v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.06750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29616;&#26377;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;RL&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#24182;&#22312;&#20498;&#31435;&#25670;&#21644;&#22235;&#26059;&#32764;&#31283;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#37117;&#26159;&#23433;&#20840;&#30340;&#19988;&#34920;&#29616;&#19982;&#19981;&#23433;&#20840;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#23433;&#20840;&#24615;&#23545;&#20110;&#24320;&#21457;&#20854;&#22312;&#35768;&#22810;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26222;&#36890;&#30340;RL&#24182;&#19981;&#20445;&#35777;&#23433;&#20840;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#35774;&#35745;&#26469;&#25552;&#20379;RL&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;RL&#26041;&#27861;&#36824;&#27809;&#26377;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29616;&#26377;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;RL&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#20171;&#32461;&#20102;&#36830;&#32493;&#21644;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#20123;&#26041;&#27861;&#26681;&#25454;&#23433;&#20840;&#26041;&#27861;&#22914;&#20309;&#36866;&#24212;&#21160;&#20316;&#36827;&#34892;&#20998;&#31867;&#65306;&#21160;&#20316;&#26367;&#25442;&#12289;&#21160;&#20316;&#25237;&#24433;&#21644;&#21160;&#20316;&#25513;&#34109;&#12290;&#25105;&#20204;&#22312;&#20498;&#31435;&#25670;&#21644;&#22235;&#26059;&#32764;&#31283;&#23450;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#26377;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;&#26041;&#27861;&#30830;&#23454;&#26159;&#23433;&#20840;&#30340;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#34920;&#29616;&#21487;&#19982;&#19981;&#23433;&#20840;&#30340;&#22522;&#32447;&#30456;&#23218;&#32654;&#12290;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;&#24212;&#36873;&#25321;&#19981;&#21516;&#30340;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;RL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring safety of reinforcement learning (RL) algorithms is crucial to unlock their potential for many real-world tasks. However, vanilla RL does not guarantee safety. In recent years, several methods have been proposed to provide safety guarantees for RL by design. Yet, there is no comprehensive comparison of these provably safe RL methods. We therefore introduce a categorization of existing provably safe RL methods, present the theoretical foundations for both continuous and discrete action spaces, and benchmark the methods' performance empirically. The methods are categorized based on how the action is adapted by the safety method: action replacement, action projection, and action masking. Our experiments on an inverted pendulum and quadrotor stabilization task show that all provably safe methods are indeed always safe. Furthermore, their trained performance is comparable to unsafe baselines. The benchmarking suggests that different provably safe RL approaches should be selected de
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#30002;&#22411;&#27969;&#24863;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#65292;&#20026;&#26089;&#26399;&#21644;&#24555;&#36895;&#25511;&#21046;&#30149;&#27602;&#20256;&#25773;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2201.01140</link><description>&lt;p&gt;
&#21033;&#29992;PSSM&#21644;&#35789;&#23884;&#20837;&#39044;&#27979;&#30002;&#22411;&#27969;&#24863;&#30149;&#27602;&#23487;&#20027;
&lt;/p&gt;
&lt;p&gt;
Predicting Influenza A Viral Host Using PSSM and Word Embeddings. (arXiv:2201.01140v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01140
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#30002;&#22411;&#27969;&#24863;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#65292;&#20026;&#26089;&#26399;&#21644;&#24555;&#36895;&#25511;&#21046;&#30149;&#27602;&#20256;&#25773;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24863;&#30149;&#27602;&#30340;&#24555;&#36895;&#31361;&#21464;&#23041;&#32961;&#20844;&#20849;&#20581;&#24247;&#65292;&#21487;&#33021;&#24341;&#21457;&#33268;&#21629;&#30340;&#22823;&#27969;&#34892;&#30149;&#12290;&#28982;&#32780;&#65292;&#26816;&#27979;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#22312;&#29190;&#21457;&#26399;&#38388;&#25110;&#29190;&#21457;&#21518;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#27969;&#24863;&#30149;&#27602;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#29289;&#31181;&#20043;&#38388;&#24490;&#29615;&#20256;&#25773;&#12290;&#22240;&#27492;&#65292;&#26089;&#26399;&#21644;&#24555;&#36895;&#26816;&#27979;&#30149;&#27602;&#23487;&#20027;&#23558;&#26377;&#21161;&#20110;&#20943;&#23569;&#30149;&#27602;&#30340;&#36827;&#19968;&#27493;&#20256;&#25773;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#20301;&#32622;&#29305;&#24322;&#24615;&#24471;&#20998;&#30697;&#38453;&#65288;PSSM&#65289;&#21644;&#23398;&#20064;&#33258;&#35789;&#23884;&#20837;&#21644;&#35789;&#32534;&#30721;&#30340;&#29305;&#24449;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25512;&#26029;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;PSSM&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#36798;&#21040;&#20102;95%&#24038;&#21491;&#30340;MCC&#21644;96%&#24038;&#21491;&#30340;F1&#12290;&#20351;&#29992;&#35789;&#23884;&#20837;&#30340;&#27169;&#22411;&#24471;&#21040;&#30340;MCC&#32422;&#20026;96&#65285;&#65292;F1&#32422;&#20026;97&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid mutation of the influenza virus threatens public health. Reassortment among viruses with different hosts can lead to a fatal pandemic. However, it is difficult to detect the original host of the virus during or after an outbreak as influenza viruses can circulate between different species. Therefore, early and rapid detection of the viral host would help reduce the further spread of the virus. We use various machine learning models with features derived from the position-specific scoring matrix (PSSM) and features learned from word embedding and word encoding to infer the origin host of viruses. The results show that the performance of the PSSM-based model reaches the MCC around 95%, and the F1 around 96%. The MCC obtained using the model with word embedding is around 96%, and the F1 is around 97%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#20284;&#28982;&#20551;&#35774;&#19979;&#30340;&#39057;&#29575;&#23398;&#27966;&#25512;&#26029;&#65288;LF2I&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#32463;&#20856;&#32479;&#35745;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#26500;&#24314;&#20855;&#26377;&#27491;&#30830;&#26465;&#20214;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;&#30340;&#23454;&#29992;&#31243;&#24207;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#22312;&#21253;&#25324;&#23431;&#23449;&#23398;&#21442;&#25968;&#25512;&#26029;&#22312;&#20869;&#30340;&#22810;&#20010;&#20363;&#23376;&#20013;&#37117;&#23454;&#29616;&#20102;&#35206;&#30422;&#24615;&#36136;&#24471;&#21040;&#22823;&#24133;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2107.03920</link><description>&lt;p&gt;
&#26080;&#20284;&#28982;&#20551;&#35774;&#19979;&#22522;&#20110;&#39057;&#29575;&#23398;&#27966;&#25512;&#26029;&#65306;&#20855;&#26377;&#27491;&#30830;&#26465;&#20214;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Likelihood-Free Frequentist Inference: Confidence Sets with Correct Conditional Coverage. (arXiv:2107.03920v6 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.03920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#20284;&#28982;&#20551;&#35774;&#19979;&#30340;&#39057;&#29575;&#23398;&#27966;&#25512;&#26029;&#65288;LF2I&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#32463;&#20856;&#32479;&#35745;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#26500;&#24314;&#20855;&#26377;&#27491;&#30830;&#26465;&#20214;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;&#30340;&#23454;&#29992;&#31243;&#24207;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#22312;&#21253;&#25324;&#23431;&#23449;&#23398;&#21442;&#25968;&#25512;&#26029;&#22312;&#20869;&#30340;&#22810;&#20010;&#20363;&#23376;&#20013;&#37117;&#23454;&#29616;&#20102;&#35206;&#30422;&#24615;&#36136;&#24471;&#21040;&#22823;&#24133;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#37117;&#24191;&#27867;&#20351;&#29992;&#35745;&#31639;&#26426;&#27169;&#25311;&#22120;&#20197;&#38544;&#21547;&#22797;&#26434;&#31995;&#32479;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;&#20256;&#32479;&#30340;&#32479;&#35745;&#26041;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;&#36825;&#20123;&#31216;&#20026;&#26080;&#20284;&#28982;&#20551;&#35774;&#19979;&#25512;&#26029;&#65288;LFI&#65289;&#30340;&#24773;&#20917;&#65292;&#23588;&#20854;&#26159;&#22312;&#28176;&#36817;&#21644;&#20302;&#32500;&#30340;&#26465;&#20214;&#19979;&#12290;&#34429;&#28982;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#24402;&#19968;&#21270;&#27969;&#65292;&#24050;&#32463;&#38761;&#26032;&#20102;LFI&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#23481;&#37327;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#33021;&#20026;&#23567;&#26679;&#26412;&#22823;&#23567;&#20135;&#29983;&#20855;&#26377;&#27491;&#30830;&#26465;&#20214;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#32463;&#20856;&#32479;&#35745;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#65288;i&#65289;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#21517;&#20041;&#35206;&#30422;&#30340;&#20869;&#26364;&#21306;&#38388;&#24314;&#35774;&#30340;&#23454;&#29992;&#31243;&#24207;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20272;&#35745;&#25972;&#20010;&#21442;&#25968;&#31354;&#38388;&#30340;&#26465;&#20214;&#35206;&#30422;&#30340;&#35786;&#26029;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#31216;&#20026;&#26080;&#20284;&#28982;&#20551;&#35774;&#19979;&#30340;&#39057;&#29575;&#23398;&#27966;&#25512;&#26029;&#65288;LF2I&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20351;&#29992;&#23450;&#20041;&#27979;&#35797;&#32479;&#35745;&#37327;&#30340;&#20219;&#20309;&#26041;&#27861;&#65292;&#22914;&#20284;&#28982;&#27604;&#65292;&#22240;&#27492;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#21512;&#25104;&#21644;&#23454;&#38469;&#30340;&#20363;&#23376;&#65292;&#21253;&#25324;&#23431;&#23449;&#23398;&#21442;&#25968;&#25512;&#26029;&#65292;&#24182;&#35777;&#26126;&#19982;&#29616;&#26377;&#30340;LFI&#26041;&#27861;&#30456;&#27604;&#65292;&#35206;&#30422;&#24615;&#36136;&#24471;&#21040;&#20102;&#22823;&#24133;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many areas of science make extensive use of computer simulators that implicitly encode likelihood functions of complex systems. Classical statistical methods are poorly suited for these so-called likelihood-free inference (LFI) settings, particularly outside asymptotic and low-dimensional regimes. Although new machine learning methods, such as normalizing flows, have revolutionized the sample efficiency and capacity of LFI methods, it remains an open question whether they produce confidence sets with correct conditional coverage for small sample sizes. This paper unifies classical statistics with modern machine learning to present (i) a practical procedure for the Neyman construction of confidence sets with finite-sample guarantees of nominal coverage, and (ii) diagnostics that estimate conditional coverage over the entire parameter space. We refer to our framework as likelihood-free frequentist inference (LF2I). Any method that defines a test statistic, like the likelihood ratio, can 
&lt;/p&gt;</description></item></channel></rss>