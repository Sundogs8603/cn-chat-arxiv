<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#29616;&#20195;&#35782;&#21035;&#26041;&#27861;&#27604;&#36739;&#20915;&#31574;&#25919;&#31574;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20851;&#38190;&#22312;&#20110;&#21487;&#20197;&#23433;&#20840;&#24573;&#30053;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2404.00848</link><description>&lt;p&gt;
&#20915;&#31574;&#25919;&#31574;&#22312;&#28151;&#26434;&#24773;&#20917;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Predictive Performance Comparison of Decision Policies Under Confounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00848
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#29616;&#20195;&#35782;&#21035;&#26041;&#27861;&#27604;&#36739;&#20915;&#31574;&#25919;&#31574;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20851;&#38190;&#22312;&#20110;&#21487;&#20197;&#23433;&#20840;&#24573;&#30053;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#34987;&#24341;&#20837;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#20854;&#22522;&#26412;&#29702;&#24565;&#26159;&#23427;&#20204;&#21487;&#20197;&#25552;&#21319;&#20915;&#31574;&#25919;&#31574;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#36890;&#24120;&#23384;&#22312;&#20110;&#26410;&#26126;&#30830;&#35268;&#23450;&#21644;&#20381;&#36182;&#19981;&#21487;&#35266;&#27979;&#22240;&#32032;&#30340;&#29616;&#26377;&#20915;&#31574;&#25919;&#31574;&#30456;&#27604;&#36739;&#39044;&#27979;&#24615;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#36890;&#24120;&#22312;&#23454;&#36341;&#20013;&#34987;&#36890;&#36807;&#23545;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#36827;&#34892;&#24378;&#20551;&#35774;&#26469;&#22788;&#29702;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26469;&#27604;&#36739;&#20915;&#31574;&#25919;&#31574;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#26681;&#25454;&#22240;&#26524;&#25512;&#26029;&#21644;&#31163;&#32447;&#35780;&#20272;&#25991;&#29486;&#20013;&#30340;&#21508;&#31181;&#29616;&#20195;&#35782;&#21035;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65288;&#20363;&#22914;&#65292;&#24037;&#20855;&#21464;&#37327;&#65292;&#36793;&#38469;&#25935;&#24863;&#24615;&#27169;&#22411;&#65292;&#36817;&#31471;&#21464;&#37327;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#25105;&#20204;&#21487;&#20197;&#23433;&#20840;&#22320;&#24573;&#30053;&#25919;&#31574;&#27604;&#36739;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#38480;&#26679;&#26412;&#20272;&#35745;&#36951;&#25022;&#21306;&#38388;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00848v1 Announce Type: new  Abstract: Predictive models are often introduced to decision-making tasks under the rationale that they improve performance over an existing decision-making policy. However, it is challenging to compare predictive performance against an existing decision-making policy that is generally under-specified and dependent on unobservable factors. These sources of uncertainty are often addressed in practice by making strong assumptions about the data-generating mechanism. In this work, we propose a method to compare the predictive performance of decision policies under a variety of modern identification approaches from the causal inference and off-policy evaluation literatures (e.g., instrumental variable, marginal sensitivity model, proximal variable). Key to our method is the insight that there are regions of uncertainty that we can safely ignore in the policy comparison. We develop a practical approach for finite-sample estimation of regret intervals u
&lt;/p&gt;</description></item><item><title>&#39640;&#20284;&#28982;&#21306;&#22495;&#23558;&#19981;&#20250;&#34987;&#29983;&#25104;&#22914;&#26524;&#23427;&#20204;&#21253;&#21547;&#26368;&#23567;&#27010;&#29575;&#36136;&#37327;&#65292;&#22522;&#20110;&#27492;&#35266;&#23519;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26412;&#22320;&#22266;&#26377;&#32500;&#24230;&#20272;&#35745;&#36827;&#34892;&#31163;&#32676;&#26816;&#27979;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.18910</link><description>&lt;p&gt;
&#23545;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#24726;&#35770;&#30340;&#20284;&#28982;&#20960;&#20309;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
A Geometric Explanation of the Likelihood OOD Detection Paradox
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18910
&lt;/p&gt;
&lt;p&gt;
&#39640;&#20284;&#28982;&#21306;&#22495;&#23558;&#19981;&#20250;&#34987;&#29983;&#25104;&#22914;&#26524;&#23427;&#20204;&#21253;&#21547;&#26368;&#23567;&#27010;&#29575;&#36136;&#37327;&#65292;&#22522;&#20110;&#27492;&#35266;&#23519;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26412;&#22320;&#22266;&#26377;&#32500;&#24230;&#20272;&#35745;&#36827;&#34892;&#31163;&#32676;&#26816;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#36890;&#24120;&#34920;&#29616;&#20986;&#20196;&#20154;&#22256;&#24785;&#30340;&#34892;&#20026;&#65306;&#24403;&#22312;&#30456;&#23545;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#20250;&#32473;&#26469;&#33258;&#26356;&#31616;&#21333;&#26469;&#28304;&#30340;&#31163;&#32676;&#25968;&#25454;&#36171;&#20104;&#26356;&#39640;&#30340;&#20284;&#28982;&#20540;&#12290;&#26356;&#20351;&#20154;&#24863;&#21040;&#31070;&#31192;&#30340;&#26159;&#65292;&#23613;&#31649;&#20855;&#26377;&#26356;&#39640;&#30340;&#20284;&#28982;&#20540;&#65292;&#20294;&#36825;&#20123;DGMs&#20174;&#26410;&#29983;&#25104;&#36807;&#31163;&#32676;&#26679;&#26412;&#12290;&#36825;&#20010;&#21452;&#31649;&#40784;&#19979;&#30340;&#24726;&#35770;&#23578;&#26410;&#24471;&#21040;&#26368;&#32456;&#35299;&#37322;&#65292;&#20351;&#24471;&#22522;&#20110;&#20284;&#28982;&#30340;&#31163;&#32676;&#26816;&#27979;&#19981;&#21487;&#38752;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#26159;&#65292;&#22914;&#26524;&#39640;&#20284;&#28982;&#21306;&#22495;&#20013;&#21253;&#21547;&#20102;&#26368;&#23567;&#27010;&#29575;&#36136;&#37327;&#65292;&#37027;&#20040;&#36825;&#20123;&#21306;&#22495;&#23558;&#19981;&#20250;&#34987;&#29983;&#25104;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#22260;&#32469;&#20302;&#32500;&#27969;&#24418;&#25968;&#25454;&#30340;&#22320;&#26041;&#21487;&#33021;&#20986;&#29616;&#22823;&#23494;&#24230;&#20294;&#20302;&#27010;&#29575;&#36136;&#37327;&#30340;&#30475;&#20284;&#30683;&#30462;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26412;&#22320;&#22266;&#26377;&#32500;&#24230;(LID)&#20272;&#35745;&#21487;&#20197;&#35782;&#21035;&#36825;&#31181;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;DGM&#33719;&#24471;&#30340;&#20284;&#28982;&#21644;LID&#20272;&#35745;&#30456;&#37197;&#23545;&#30340;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18910v1 Announce Type: cross  Abstract: Likelihood-based deep generative models (DGMs) commonly exhibit a puzzling behaviour: when trained on a relatively complex dataset, they assign higher likelihood values to out-of-distribution (OOD) data from simpler sources. Adding to the mystery, OOD samples are never generated by these DGMs despite having higher likelihoods. This two-pronged paradox has yet to be conclusively explained, making likelihood-based OOD detection unreliable. Our primary observation is that high-likelihood regions will not be generated if they contain minimal probability mass. We demonstrate how this seeming contradiction of large densities yet low probability mass can occur around data confined to low-dimensional manifolds. We also show that this scenario can be identified through local intrinsic dimension (LID) estimation, and propose a method for OOD detection which pairs the likelihoods and LID estimates obtained from a pre-trained DGM. Our method can b
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#38750;&#32447;&#24615;&#36870;&#30340;&#34920;&#36798;&#25968;&#25454;&#20808;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.17042</link><description>&lt;p&gt;
&#21487;&#35777;&#23454;&#40065;&#26834;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#21518;&#39564;&#37319;&#26679;&#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17042
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#38750;&#32447;&#24615;&#36870;&#30340;&#34920;&#36798;&#25968;&#25454;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#65292;&#30446;&#26631;&#26159;&#20174;&#24050;&#30693;&#25551;&#36848;&#26576;&#31181;&#24863;&#30693;&#25110;&#25104;&#20687;&#27169;&#24335;&#30340;&#24050;&#30693;&#21069;&#21521;&#27169;&#22411;&#25910;&#38598;&#30340;&#23569;&#37327;&#27979;&#37327;&#20013;&#25512;&#26029;&#26410;&#30693;&#22270;&#20687;&#12290;&#30001;&#20110;&#36164;&#28304;&#38480;&#21046;&#65292;&#36825;&#20010;&#20219;&#21153;&#36890;&#24120;&#38750;&#24120;&#19981;&#36866;&#21512;&#65292;&#36825;&#23601;&#38656;&#35201;&#37319;&#32435;&#34920;&#36798;&#20016;&#23500;&#30340;&#20808;&#39564;&#20449;&#24687;&#26469;&#35268;&#33539;&#35299;&#31354;&#38388;&#12290;&#30001;&#20110;&#20854;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#25104;&#21151;&#65292;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#22270;&#20687;&#37325;&#24314;&#20013;&#19968;&#20010;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#34920;&#36798;&#20808;&#39564;&#30340;&#20505;&#36873;&#32773;&#12290;&#20026;&#20102;&#19968;&#27425;&#24615;&#23481;&#32435;&#22810;&#26679;&#30340;&#20219;&#21153;&#65292;&#24320;&#21457;&#23558;&#22270;&#20687;&#20808;&#39564;&#20998;&#24067;&#30340;&#26080;&#26465;&#20214;&#35780;&#20998;&#20989;&#25968;&#19982;&#28789;&#27963;&#30340;&#21069;&#21521;&#27169;&#22411;&#36873;&#25321;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#12289;&#19968;&#33268;&#21644;&#40065;&#26834;&#31639;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#38750;&#32447;&#24615;&#36870;&#30340;&#34920;&#36798;&#25968;&#25454;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17042v1 Announce Type: cross  Abstract: In a great number of tasks in science and engineering, the goal is to infer an unknown image from a small number of measurements collected from a known forward model describing certain sensing or imaging modality. Due to resource constraints, this task is often extremely ill-posed, which necessitates the adoption of expressive prior information to regularize the solution space. Score-based diffusion models, due to its impressive empirical success, have emerged as an appealing candidate of an expressive prior in image reconstruction. In order to accommodate diverse tasks at once, it is of great interest to develop efficient, consistent and robust algorithms that incorporate {\em unconditional} score functions of an image prior distribution in conjunction with flexible choices of forward models.   This work develops an algorithmic framework for employing score-based diffusion models as an expressive data prior in general nonlinear invers
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#24490;&#29615;&#24335;&#38543;&#26426;&#26463;&#25628;&#32034;&#21644;&#26469;&#33258;&#21487;&#35777;&#31574;&#30053;&#25913;&#36827;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#26412;&#30740;&#31350;&#22312;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#26368;&#23567;&#37319;&#26679;&#27425;&#25968;&#20013;&#23454;&#29616;&#36880;&#27493;&#25913;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.15180</link><description>&lt;p&gt;
&#33258;&#25105;&#25913;&#36827;&#29992;&#20110;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65306;&#26080;&#38656;&#26367;&#25442;&#36827;&#34892;&#37319;&#26679;&#65292;&#20294;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Self-Improvement for Neural Combinatorial Optimization: Sample without Replacement, but Improvement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15180
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#24490;&#29615;&#24335;&#38543;&#26426;&#26463;&#25628;&#32034;&#21644;&#26469;&#33258;&#21487;&#35777;&#31574;&#30053;&#25913;&#36827;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#26412;&#30740;&#31350;&#22312;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#26368;&#23567;&#37319;&#26679;&#27425;&#25968;&#20013;&#23454;&#29616;&#36880;&#27493;&#25913;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23545;&#20110;&#31471;&#21040;&#31471;&#30340;&#26500;&#36896;&#24615;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#26041;&#27861;&#36890;&#24120;&#26159;&#20351;&#29992;&#34892;&#20026;&#20811;&#38534;&#26469;&#35757;&#32451;&#31574;&#30053;&#65292;&#20174;&#19987;&#23478;&#35299;&#20915;&#26041;&#26696;&#20013;&#25110;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#20174;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#34892;&#20026;&#20811;&#38534;&#26041;&#27861;&#24456;&#30452;&#25509;&#65292;&#20294;&#38656;&#35201;&#26114;&#36149;&#30340;&#19987;&#23478;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#24448;&#24448;&#35745;&#31639;&#35201;&#27714;&#24456;&#39640;&#65292;&#38590;&#20197;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26725;&#25509;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;&#27599;&#20010;&#32426;&#20803;&#20013;&#20351;&#29992;&#24403;&#21069;&#27169;&#22411;&#23545;&#38543;&#26426;&#23454;&#20363;&#36827;&#34892;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#30340;&#37319;&#26679;&#65292;&#28982;&#21518;&#36873;&#25321;&#26368;&#20339;&#35299;&#20316;&#20026;&#19987;&#23478;&#36712;&#36857;&#36827;&#34892;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;&#20026;&#20102;&#22312;&#26368;&#23567;&#37319;&#26679;&#27425;&#25968;&#20013;&#23454;&#29616;&#36880;&#27493;&#25913;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#24490;&#29615;&#24335;&#38543;&#26426;&#26463;&#25628;&#32034;&#19982;&#19968;&#31181;&#25512;&#23548;&#33258;&#21487;&#35777;&#31574;&#30053;&#25913;&#36827;&#30340;&#26356;&#26032;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#35813;&#31574;&#30053;&#36890;&#36807;&#21033;&#29992;&#20960;&#20046;&#27809;&#26377;&#36890;&#20449;&#24320;&#38144;&#30340;&#26679;&#26412;&#24207;&#21015;&#30340;&#20248;&#21183;&#65292;&#22312;&#36718;&#20043;&#38388;&#35843;&#25972;&#31574;&#30053;&#65292;&#20197;&#31934;&#32454;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15180v1 Announce Type: new  Abstract: Current methods for end-to-end constructive neural combinatorial optimization usually train a policy using behavior cloning from expert solutions or policy gradient methods from reinforcement learning. While behavior cloning is straightforward, it requires expensive expert solutions, and policy gradient methods are often computationally demanding and complex to fine-tune. In this work, we bridge the two and simplify the training process by sampling multiple solutions for random instances using the current model in each epoch and then selecting the best solution as an expert trajectory for supervised imitation learning. To achieve progressively improving solutions with minimal sampling, we introduce a method that combines round-wise Stochastic Beam Search with an update strategy derived from a provable policy improvement. This strategy refines the policy between rounds by utilizing the advantage of the sampled sequences with almost no com
&lt;/p&gt;</description></item><item><title>Larimar&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#22686;&#24378;LLMs&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#30340;&#30693;&#35782;&#26356;&#26032;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#19988;&#22312;&#36895;&#24230;&#21644;&#28789;&#27963;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.11901</link><description>&lt;p&gt;
Larimar: &#20855;&#26377;&#24773;&#33410;&#35760;&#24518;&#25511;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Larimar: Large Language Models with Episodic Memory Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11901
&lt;/p&gt;
&lt;p&gt;
Larimar&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#22686;&#24378;LLMs&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#30340;&#30693;&#35782;&#26356;&#26032;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#19988;&#22312;&#36895;&#24230;&#21644;&#28789;&#27963;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Larimar - &#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#12290; Larimar&#30340;&#35760;&#24518;&#20801;&#35768;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#26356;&#26032;&#30693;&#35782;&#65292;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#20107;&#23454;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Larimar&#22312;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322; - &#26681;&#25454;&#22522;&#30784;LLM&#30340;&#19981;&#21516;&#65292;&#36895;&#24230;&#25552;&#21319;&#20026;4-10&#20493;&#65292;&#24182;&#19988;&#30001;&#20110;&#25552;&#20986;&#30340;&#26550;&#26500;&#31616;&#21333;&#12289;&#19981;&#20381;&#36182;&#20110;LLM&#65292;&#22240;&#27492;&#20855;&#26377;&#33391;&#22909;&#30340;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#36873;&#25321;&#24615;&#20107;&#23454;&#36951;&#24536;&#21644;&#36755;&#20837;&#19978;&#19979;&#25991;&#38271;&#24230;&#27010;&#25324;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11901v1 Announce Type: cross  Abstract: Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 4-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting and input context length generalization with Larimar and show their effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#21644;&#23450;&#20301;&#20219;&#21153;&#25193;&#23637;&#20026;&#20855;&#26377;&#36317;&#31163;&#20272;&#35745;&#30340;3D SELD&#65292;&#25506;&#35752;&#20102;&#20004;&#31181;&#38598;&#25104;&#36317;&#31163;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;Ambisonic&#21644;&#21452;&#32819;&#29256;&#26412;&#30340;&#22768;&#38899;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.11827</link><description>&lt;p&gt;
&#20855;&#26377;&#36317;&#31163;&#20272;&#35745;&#30340;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#21644;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Sound Event Detection and Localization with Distance Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#21644;&#23450;&#20301;&#20219;&#21153;&#25193;&#23637;&#20026;&#20855;&#26377;&#36317;&#31163;&#20272;&#35745;&#30340;3D SELD&#65292;&#25506;&#35752;&#20102;&#20004;&#31181;&#38598;&#25104;&#36317;&#31163;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;Ambisonic&#21644;&#21452;&#32819;&#29256;&#26412;&#30340;&#22768;&#38899;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#21644;&#23450;&#20301;(SELD)&#26159;&#35782;&#21035;&#22768;&#38899;&#20107;&#20214;&#21450;&#20854;&#23545;&#24212;&#21040;&#36798;&#26041;&#21521;(DOA)&#30340;&#32508;&#21512;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#19968;&#20219;&#21153;&#22312;&#36817;&#24180;&#26469;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#24182;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#20294;&#23427;&#26410;&#33021;&#25552;&#20379;&#26377;&#20851;&#22768;&#28304;&#20301;&#32622;&#30340;&#23436;&#25972;&#20449;&#24687;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#20219;&#21153;&#25193;&#23637;&#20026;&#20855;&#26377;&#36317;&#31163;&#20272;&#35745;&#30340;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#12289;&#23450;&#20301;(3D SELD)&#26469;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#38598;&#25104;&#36317;&#31163;&#20272;&#35745;&#22312;SELD&#26680;&#24515;&#20013;&#30340;&#26041;&#27861; - &#19968;&#31181;&#26159;&#22810;&#20219;&#21153;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#29420;&#27169;&#22411;&#36755;&#20986;&#26469;&#22788;&#29702;&#38382;&#39064;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#23558;&#22810;ACCDOA&#26041;&#27861;&#25193;&#23637;&#20197;&#21253;&#25324;&#36317;&#31163;&#20449;&#24687;&#32780;&#33719;&#24471;&#30340;&#21333;&#20219;&#21153;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;Ambisonic&#21644;&#21452;&#32819;&#29256;&#26412;&#30340;STARSS23&#65306;Sony-TAU Realistic Spatial Soundscapes 2023&#24320;&#23637;&#20102;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#28041;&#21450;&#19982;&#36317;&#31163;&#20272;&#35745;&#37096;&#20998;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11827v1 Announce Type: cross  Abstract: Sound Event Detection and Localization (SELD) is a combined task of identifying sound events and their corresponding direction-of-arrival (DOA). While this task has numerous applications and has been extensively researched in recent years, it fails to provide full information about the sound source position. In this paper, we overcome this problem by extending the task to Sound Event Detection, Localization with Distance Estimation (3D SELD). We study two ways of integrating distance estimation within the SELD core - a multi-task approach, in which the problem is tackled by a separate model output, and a single-task approach obtained by extending the multi-ACCDOA method to include distance information. We investigate both methods for the Ambisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial Soundscapes 2023. Moreover, our study involves experiments on the loss function related to the distance estimation part. Our resu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#31070;&#32463;&#31639;&#23376;&#26469;&#25552;&#39640;&#23545;&#21306;&#22495;&#22806;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;OOD&#27979;&#35797;&#36755;&#20837;&#19978;&#30340;&#22833;&#36133;</title><link>https://arxiv.org/abs/2403.10642</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#34920;&#24449;&#21644;&#25913;&#36827;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#21306;&#22495;&#22806;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10642
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#31070;&#32463;&#31639;&#23376;&#26469;&#25552;&#39640;&#23545;&#21306;&#22495;&#22806;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;OOD&#27979;&#35797;&#36755;&#20837;&#19978;&#30340;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#39046;&#22495;&#20013;&#30340;&#29616;&#26377;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#35299;&#31639;&#31526;&#21487;&#20197;&#20026;&#32463;&#20856;&#25968;&#20540;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#27714;&#35299;&#22120;&#25552;&#20379;&#19968;&#20010;&#24555;&#36895;&#30340;&#36817;&#20284;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#20854;&#20013;&#65292;&#31070;&#32463;&#31639;&#23376;&#65288;NOs&#65289;&#24050;&#32463;&#34987;&#35748;&#20026;&#23588;&#20026;&#20855;&#26377;&#21069;&#26223;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;&#21306;&#22495;&#22806;&#65288;OOD&#65289;&#27979;&#35797;&#36755;&#20837;&#65292;&#20960;&#31181;NOs&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26041;&#27861;&#29978;&#33267;&#22312;&#27169;&#22411;&#23545;&#20110;&#22495;&#20869;&#20219;&#21153;&#30340;&#35299;&#36817;&#20284;&#33391;&#22909;&#26102;&#20063;&#20250;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38598;&#25104;&#20960;&#20010;NOs&#21487;&#20197;&#35782;&#21035;&#39640;&#35823;&#24046;&#21306;&#22495;&#65292;&#24182;&#25552;&#20379;&#33391;&#22909;&#19982;&#39044;&#27979;&#35823;&#24046;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;DiverseNO&#65292;&#36890;&#36807;&#40723;&#21169;&#20854;&#26368;&#21518;&#21069;&#21521;&#20256;&#25773;&#23618;&#20013;&#30340;&#22810;&#20010;&#22836;&#37096;&#36827;&#34892;&#22810;&#26679;&#21270;&#39044;&#27979;&#26469;&#27169;&#25311;&#38598;&#25104;&#30340;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10642v1 Announce Type: new  Abstract: Existing work in scientific machine learning (SciML) has shown that data-driven learning of solution operators can provide a fast approximate alternative to classical numerical partial differential equation (PDE) solvers. Of these, Neural Operators (NOs) have emerged as particularly promising. We observe that several uncertainty quantification (UQ) methods for NOs fail for test inputs that are even moderately out-of-domain (OOD), even when the model approximates the solution well for in-domain tasks. To address this limitation, we show that ensembling several NOs can identify high-error regions and provide good uncertainty estimates that are well-correlated with prediction errors. Based on this, we propose a cost-effective alternative, DiverseNO, that mimics the properties of the ensemble by encouraging diverse predictions from its multiple heads in the last feed-forward layer. We then introduce Operator-ProbConserv, a method that uses t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25972;&#21512;&#21040;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;(TSC)&#31995;&#32479;&#20013;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;TSC&#31995;&#32479;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#22330;&#26223;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#20351;&#24471;LLMs&#19982;&#19968;&#31995;&#21015;&#24863;&#30693;&#21644;&#20915;&#31574;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25552;&#21319;TSC&#31995;&#32479;&#23545;&#22478;&#24066;&#20132;&#36890;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#30340;&#31649;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08337</link><description>&lt;p&gt;
LLM&#36741;&#21161;&#19979;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#22478;&#24066;&#29615;&#22659;&#20013;&#23454;&#29616;&#20154;&#31867;&#20223;&#29983;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25972;&#21512;&#21040;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;(TSC)&#31995;&#32479;&#20013;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;TSC&#31995;&#32479;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#22330;&#26223;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#20351;&#24471;LLMs&#19982;&#19968;&#31995;&#21015;&#24863;&#30693;&#21644;&#20915;&#31574;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25552;&#21319;TSC&#31995;&#32479;&#23545;&#22478;&#24066;&#20132;&#36890;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#30340;&#31649;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37117;&#24066;&#22320;&#21306;&#30340;&#20132;&#36890;&#25317;&#22581;&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#36828;&#32463;&#27982;&#12289;&#29615;&#22659;&#21644;&#31038;&#20250;&#24433;&#21709;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26377;&#25928;&#30340;&#25317;&#22581;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;(TSC)&#31995;&#32479;&#22312;&#36825;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#22238;&#24212;&#20256;&#32479;TSC&#31995;&#32479;&#22312;&#31649;&#29702;&#22478;&#24066;&#20132;&#36890;&#27969;&#21160;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#26041;&#38754;&#32463;&#24120;&#34920;&#29616;&#20986;&#30340;&#19981;&#36275;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25972;&#21512;&#21040;TSC&#20013;&#65292;&#21033;&#29992;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#23558;LLMs&#19982;&#19968;&#22871;&#24863;&#30693;&#21644;&#20915;&#31574;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#26377;&#21161;&#20110;&#25506;&#35752;&#38745;&#24577;&#21644;&#21160;&#24577;&#20132;&#36890;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08337v1 Announce Type: cross  Abstract: Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic informatio
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;LLM&#25512;&#29702;&#35843;&#24230;&#31243;&#24207;Sarathi-Serve&#65292;&#36890;&#36807;&#20998;&#22359;&#39044;&#35013;&#22635;&#25216;&#26415;&#24179;&#34913;&#20102;GPU&#35745;&#31639;&#39281;&#21644;&#21644;&#21333;&#20010;&#26631;&#35760;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2403.02310</link><description>&lt;p&gt;
&#22312;LLM&#25512;&#29702;&#20013;&#24179;&#34913;&#21534;&#21520;&#37327;&#21644;&#24310;&#36831;&#26435;&#34913;&#30340;&#30740;&#31350;&#65306;Sarathi-Serve&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02310
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;LLM&#25512;&#29702;&#35843;&#24230;&#31243;&#24207;Sarathi-Serve&#65292;&#36890;&#36807;&#20998;&#22359;&#39044;&#35013;&#22635;&#25216;&#26415;&#24179;&#34913;&#20102;GPU&#35745;&#31639;&#39281;&#21644;&#21644;&#21333;&#20010;&#26631;&#35760;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#20010;LLM&#26381;&#21153;&#35831;&#27714;&#32463;&#21382;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#26159;prefill&#38454;&#27573;&#65292;&#22788;&#29702;&#25972;&#20010;&#36755;&#20837;&#25552;&#31034;&#20197;&#29983;&#25104;&#19968;&#20010;&#36755;&#20986;&#26631;&#35760;&#65307;&#31532;&#20108;&#20010;&#26159;decode&#38454;&#27573;&#65292;&#36880;&#20010;&#29983;&#25104;&#20854;&#20313;&#30340;&#36755;&#20986;&#26631;&#35760;&#12290;Prefill&#36845;&#20195;&#20855;&#26377;&#36739;&#39640;&#30340;&#24310;&#36831;&#65292;&#20294;&#30001;&#20110;&#36755;&#20837;&#25552;&#31034;&#30340;&#24182;&#34892;&#22788;&#29702;&#65292;&#21487;&#20197;&#20351;GPU&#35745;&#31639;&#39281;&#21644;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;decode&#36845;&#20195;&#20855;&#26377;&#36739;&#20302;&#30340;&#24310;&#36831;&#65292;&#20294;&#20063;&#20165;&#20351;&#29992;&#36739;&#20302;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#20026;&#27599;&#20010;&#35831;&#27714;&#21482;&#22788;&#29702;&#19968;&#20010;&#26631;&#35760;&#12290;&#36825;&#20351;&#24471;&#23545;&#35299;&#30721;&#26469;&#35828;&#25209;&#22788;&#29702;&#38750;&#24120;&#26377;&#25928;&#65292;&#22240;&#27492;&#23545;&#25972;&#20307;&#21534;&#21520;&#37327;&#20063;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#25209;&#37327;&#22788;&#29702;&#22810;&#20010;&#35831;&#27714;&#20250;&#23548;&#33268;prefill&#21644;decode&#36845;&#20195;&#20132;&#38169;&#36827;&#34892;&#65292;&#36825;&#20351;&#24471;&#22312;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#20043;&#38388;&#30340;&#24179;&#34913;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#35843;&#24230;&#31243;&#24207;Sarathi-Serve&#65292;&#28789;&#24863;&#26469;&#33258;&#25105;&#20204;&#26368;&#21021;&#20026;&#20248;&#21270;Sarathi&#30340;&#21534;&#21520;&#37327;&#25552;&#20986;&#30340;&#25216;&#26415;&#12290;Sarathi-Serve&#21033;&#29992;&#20102;&#20174;Sarathi&#20013;&#24341;&#20837;&#30340;&#20998;&#22359;prefill&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02310v1 Announce Type: new  Abstract: Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt to produce one output token and the second is decode which generates the rest of output tokens, one-at-a-time. Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input prompt. In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request. This makes batching highly effective for decodes and consequently for overall throughput. However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency.   We introduce an efficient LLM inference scheduler Sarathi-Serve inspired by the techniques we originally proposed for optimizing throughput in Sarathi. Sarathi-Serve leverages chunked-prefills from Sarathi 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#32479;&#19968;&#26694;&#26550;&#19979;&#24314;&#31435;&#20102;&#8220;&#36890;&#36807;&#36845;&#20195;&#23454;&#29616;&#38544;&#31169;&#25918;&#22823;&#8221;&#29616;&#35937;&#65292;&#25552;&#39640;&#20102;&#20808;&#21069;&#20998;&#26512;&#30340;&#27700;&#24179;&#65292;&#24182;&#30001;&#27492;&#33719;&#24471;&#20102;&#20854;&#20182;&#24046;&#20998;&#38544;&#31169;&#27010;&#24565;&#26356;&#32039;&#23494;&#30340;&#38544;&#31169;&#26680;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.00278</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#30340;&#24179;&#31227;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Shifted Interpolation for Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#32479;&#19968;&#26694;&#26550;&#19979;&#24314;&#31435;&#20102;&#8220;&#36890;&#36807;&#36845;&#20195;&#23454;&#29616;&#38544;&#31169;&#25918;&#22823;&#8221;&#29616;&#35937;&#65292;&#25552;&#39640;&#20102;&#20808;&#21069;&#20998;&#26512;&#30340;&#27700;&#24179;&#65292;&#24182;&#30001;&#27492;&#33719;&#24471;&#20102;&#20854;&#20182;&#24046;&#20998;&#38544;&#31169;&#27010;&#24565;&#26356;&#32039;&#23494;&#30340;&#38544;&#31169;&#26680;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21927;&#22179;&#30340;&#26799;&#24230;&#19979;&#38477;&#21450;&#20854;&#21464;&#31181;&#26159;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#20013;&#20027;&#23548;&#30340;&#31639;&#27861;&#12290;&#37327;&#21270;&#23427;&#20204;&#30340;&#38544;&#31169;&#27844;&#28431;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#28982;&#32780;&#21363;&#20351;&#22312;&#20984;&#25439;&#22833;&#30340;&#22522;&#30784;&#35774;&#32622;&#20013;&#65292;&#32039;&#33268;&#30340;&#34920;&#24449;&#20173;&#28982;&#26159;&#24320;&#25918;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;$f$-&#24046;&#20998;&#38544;&#31169;&#30340;&#32479;&#19968;&#26694;&#26550;&#19979;&#24314;&#31435;&#65288;&#21644;&#25913;&#36827;&#65289;&#8220;&#36890;&#36807;&#36845;&#20195;&#23454;&#29616;&#38544;&#31169;&#25918;&#22823;&#8221;&#29616;&#35937;&#65292;&#25552;&#39640;&#20102;&#20808;&#21069;&#20998;&#26512;&#30340;&#27700;&#24179;--&#36825;&#31181;&#26041;&#27861;&#32039;&#32039;&#25429;&#25417;&#20102;&#38544;&#31169;&#25439;&#22833;&#30340;&#25152;&#26377;&#26041;&#38754;&#65292;&#24182;&#31435;&#21363;&#33719;&#24471;&#20102;&#20854;&#20182;&#24046;&#20998;&#38544;&#31169;&#27010;&#24565;&#65288;&#22914;$(\varepsilon,\delta)$-DP&#21644;Renyi DP&#65289;&#26356;&#32039;&#23494;&#30340;&#38544;&#31169;&#26680;&#31639;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#35265;&#35299;&#26159;&#26500;&#24314;&#20102;&#25581;&#31034;&#20102;&#27969;&#34892;&#30340;&#24179;&#31227;&#25955;&#24230;&#35770;&#35777;&#30340;&#24179;&#31227;&#25554;&#20540;&#36807;&#31243;&#65292;&#20351;&#24471;&#36229;&#36234;&#22522;&#20110;&#25955;&#24230;&#30340;&#24046;&#20998;&#38544;&#31169;&#25918;&#23485;&#30340;&#27867;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#24378;&#20984;&#22522;&#30784;&#35774;&#32622;&#20013;&#30340;&#31532;&#19968;&#20010;&#31934;&#30830;&#38544;&#31169;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00278v1 Announce Type: new  Abstract: Noisy gradient descent and its variants are the predominant algorithms for differentially private machine learning. It is a fundamental question to quantify their privacy leakage, yet tight characterizations remain open even in the foundational setting of convex losses. This paper improves over previous analyses by establishing (and refining) the "privacy amplification by iteration" phenomenon in the unifying framework of $f$-differential privacy--which tightly captures all aspects of the privacy loss and immediately implies tighter privacy accounting in other notions of differential privacy, e.g., $(\varepsilon,\delta)$-DP and Renyi DP. Our key technical insight is the construction of shifted interpolated processes that unravel the popular shifted-divergences argument, enabling generalizations beyond divergence-based relaxations of DP. Notably, this leads to the first exact privacy analysis in the foundational setting of strongly convex
&lt;/p&gt;</description></item><item><title>BaM&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;BBVI&#26367;&#20195;&#26041;&#27861;&#65292;&#38024;&#23545;&#39640;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#24930;&#25910;&#25947;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#21464;&#20998;&#26063;&#20013;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#30340;&#36817;&#31471;&#26356;&#26032;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#26102;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#36235;&#20110;&#26080;&#31351;&#26102;&#21464;&#20998;&#21442;&#25968;&#26356;&#26032;&#23558;&#25351;&#25968;&#24555;&#36895;&#25910;&#25947;&#21040;&#30446;&#26631;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#65292;BaM&#22312;&#22810;&#31181;&#29983;&#25104;&#27169;&#22411;&#25512;&#26029;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14758</link><description>&lt;p&gt;
&#25209;&#22788;&#29702;&#21644;&#21305;&#37197;&#65306;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;&#40657;&#21283;&#23376;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Batch and match: black-box variational inference with a score-based divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14758
&lt;/p&gt;
&lt;p&gt;
BaM&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;BBVI&#26367;&#20195;&#26041;&#27861;&#65292;&#38024;&#23545;&#39640;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#24930;&#25910;&#25947;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#21464;&#20998;&#26063;&#20013;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#30340;&#36817;&#31471;&#26356;&#26032;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#26102;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#36235;&#20110;&#26080;&#31351;&#26102;&#21464;&#20998;&#21442;&#25968;&#26356;&#26032;&#23558;&#25351;&#25968;&#24555;&#36895;&#25910;&#25947;&#21040;&#30446;&#26631;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#65292;BaM&#22312;&#22810;&#31181;&#29983;&#25104;&#27169;&#22411;&#25512;&#26029;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20027;&#35201;&#30340;&#40657;&#21283;&#23376;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#23454;&#29616;&#37117;&#26159;&#22522;&#20110;&#20248;&#21270;&#38543;&#26426;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;BBVI&#26041;&#27861;&#36890;&#24120;&#30001;&#20110;&#20854;&#26799;&#24230;&#20272;&#35745;&#30340;&#39640;&#26041;&#24046;&#32780;&#25910;&#25947;&#32531;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#22788;&#29702;&#21644;&#21305;&#37197;&#65288;BaM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;BBVI&#26367;&#20195;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#21487;&#20197;&#36890;&#36807;&#23545;&#20855;&#26377;&#20840;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#26031;&#21464;&#20998;&#26063;&#20351;&#29992;&#23553;&#38381;&#24418;&#24335;&#30340;&#36817;&#31471;&#26356;&#26032;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#20998;&#24067;&#26102;BaM&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#22312;&#25209;&#37327;&#22823;&#23567;&#36235;&#20110;&#26080;&#31351;&#26102;&#21464;&#20998;&#21442;&#25968;&#26356;&#26032;&#20250;&#25351;&#25968;&#25910;&#25947;&#21040;&#30446;&#26631;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;BaM&#22312;&#28304;&#33258;&#23618;&#27425;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21518;&#39564;&#25512;&#26029;&#30340;&#39640;&#26031;&#21644;&#38750;&#39640;&#26031;&#30446;&#26631;&#20998;&#24067;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;BaM&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14758v1 Announce Type: cross  Abstract: Most leading implementations of black-box variational inference (BBVI) are based on optimizing a stochastic evidence lower bound (ELBO). But such approaches to BBVI often converge slowly due to the high variance of their gradient estimates. In this work, we propose batch and match (BaM), an alternative approach to BBVI based on a score-based divergence. Notably, this score-based divergence can be optimized by a closed-form proximal update for Gaussian variational families with full covariance matrices. We analyze the convergence of BaM when the target distribution is Gaussian, and we prove that in the limit of infinite batch size the variational parameter updates converge exponentially quickly to the target mean and covariance. We also evaluate the performance of BaM on Gaussian and non-Gaussian target distributions that arise from posterior inference in hierarchical and deep generative models. In these experiments, we find that BaM ty
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#23548;&#19982;&#30456;&#20851;&#24615;&#21644;&#35206;&#30422;&#24615;&#30456;&#20851;&#30340;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#30028;&#38480;&#65292;&#20026;&#23376;&#27169;&#20114;&#20449;&#24687;&#22312;&#30446;&#26631;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#20013;&#30340;&#34920;&#29616;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;</title><link>https://arxiv.org/abs/2402.13454</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#30340;&#23376;&#27169;&#20449;&#24687;&#37327;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Theoretical Analysis of Submodular Information Measures for Targeted Data Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13454
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#23548;&#19982;&#30456;&#20851;&#24615;&#21644;&#35206;&#30422;&#24615;&#30456;&#20851;&#30340;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#30028;&#38480;&#65292;&#20026;&#23376;&#27169;&#20114;&#20449;&#24687;&#22312;&#30446;&#26631;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#20013;&#30340;&#34920;&#29616;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#22686;&#21152;&#65292;&#23450;&#20301;&#29305;&#23450;&#25968;&#25454;&#23376;&#38598;&#30340;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20026;&#20102;&#24110;&#21161;&#23454;&#29616;&#36825;&#19968;&#33021;&#21147;&#65292;&#26368;&#36817;&#25552;&#20986;&#30340;&#23376;&#27169;&#20114;&#20449;&#24687;&#65288;SMI&#65289;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#26377;&#25928;&#24212;&#29992;&#20110;&#25191;&#34892;&#20351;&#29992;&#31034;&#20363;&#26597;&#35810;&#38598;&#36827;&#34892;&#23450;&#20301;&#23376;&#38598;&#36873;&#25321;&#30340;&#22810;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#24037;&#20316;&#37117;&#27809;&#26377;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;SMI&#23545;&#20110;&#23376;&#38598;&#30456;&#20851;&#24615;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#35206;&#30422;&#24615;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#39318;&#27425;&#36890;&#36807;&#25512;&#23548;&#19982;&#30456;&#20851;&#24615;&#21644;&#30446;&#26631;&#25968;&#25454;&#35206;&#30422;&#30456;&#20851;&#30340;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#30028;&#38480;&#65292;&#25552;&#20379;&#20102;&#27492;&#31867;&#20445;&#35777;&#12290;&#36890;&#36807;&#36825;&#20123;&#30028;&#38480;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#24050;&#32463;&#34920;&#29616;&#25104;&#21151;&#30340;SMI&#20989;&#25968;&#22312;&#29702;&#35770;&#19978;&#30830;&#20445;&#23454;&#29616;&#33391;&#22909;&#30340;&#26597;&#35810;&#30456;&#20851;&#24615;&#21644;&#26597;&#35810;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13454v1 Announce Type: new  Abstract: With increasing volume of data being used across machine learning tasks, the capability to target specific subsets of data becomes more important. To aid in this capability, the recently proposed Submodular Mutual Information (SMI) has been effectively applied across numerous tasks in literature to perform targeted subset selection with the aid of a exemplar query set. However, all such works are deficient in providing theoretical guarantees for SMI in terms of its sensitivity to a subset's relevance and coverage of the targeted data. For the first time, we provide such guarantees by deriving similarity-based bounds on quantities related to relevance and coverage of the targeted data. With these bounds, we show that the SMI functions, which have empirically shown success in multiple applications, are theoretically sound in achieving good query relevance and query coverage.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13254</link><description>&lt;p&gt;
CounterCurate: &#36890;&#36807;&#23545;&#29031;&#20363;&#23376;&#22686;&#24378;&#29289;&#29702;&#21644;&#35821;&#20041;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;CounterCurate&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#20840;&#38754;&#25552;&#21319;&#23545;&#27604;&#21644;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#20851;&#38190;&#38382;&#39064;&#65306;&#24573;&#35270;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#25512;&#29702;&#65288;&#35745;&#25968;&#21644;&#20301;&#32622;&#29702;&#35299;&#65289;&#65292;&#20197;&#21450;&#21033;&#29992;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21453;&#20107;&#23454;&#24494;&#35843;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21019;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#31354;&#30333;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#31361;&#20986;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;LLaVA&#65289;&#22312;&#22522;&#20110;&#29289;&#29702;&#30340;&#32452;&#21512;&#25512;&#29702;&#20013;&#20960;&#20046;&#26080;&#27861;&#32988;&#20219;&#30340;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;GLIGEN&#29983;&#25104;&#24494;&#35843;&#25968;&#25454;&#65292;&#20351;&#24471;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65306;&#22312;&#25105;&#20204;&#26032;&#30340;&#31574;&#21010;&#30340;Flickr30k-Positions&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;CLIP&#21644;LLaVA&#30340;&#24615;&#33021;&#20998;&#21035;&#25552;&#39640;&#20102;+33%&#21644;+37%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.10487</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#38543;&#26426;&#25237;&#24433;&#23618;
&lt;/p&gt;
&lt;p&gt;
Random Projection Layers for Multidimensional Time Sires Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10487
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#28151;&#21512;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#27492;&#31867;&#27169;&#22411;&#24212;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#65288;&#20363;&#22914;&#31354;&#38388;-&#26102;&#38388;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#65289;&#26102;&#65292;&#30001;&#20110;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#31216;&#20026;RPMixer&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#24335;&#34892;&#20026;&#65292;&#20854;&#20013;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#21333;&#29420;&#22359;&#30340;&#20316;&#29992;&#31867;&#20284;&#20110;&#38598;&#25104;&#27169;&#22411;&#20013;&#30340;&#22522;&#26412;&#23398;&#20064;&#22120;&#65292;&#29305;&#21035;&#26159;&#22312;&#24341;&#20837;&#36523;&#20221;&#26144;&#23556;&#27531;&#24046;&#36830;&#25509;&#26102;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;RPMixer&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#23545;&#22823;&#35268;&#27169;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10487v1 Announce Type: cross  Abstract: All-Multi-Layer Perceptron (all-MLP) mixer models have been shown to be effective for time series forecasting problems. However, when such a model is applied to high-dimensional time series (e.g., the time series in a spatial-temporal dataset), its performance is likely to degrade due to overfitting issues. In this paper, we propose an all-MLP time series forecasting architecture, referred to as RPMixer. Our method leverages the ensemble-like behavior of deep neural networks, where each individual block within the network acts like a base learner in an ensemble model, especially when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks' outputs, thereby enhancing the overall performance of RPMixer. Extensive experiments conducted on large-scale spatial-temporal forecasting benchmark datasets demonstrate that our proposed method outperf
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#25193;&#25955;&#27169;&#22411;&#65288;CDMs&#65289;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#30340;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#21152;&#22312;&#24178;&#20928;&#20449;&#21495;&#19978;&#30340;&#22122;&#22768;&#37327;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10095</link><description>&lt;p&gt;
&#20998;&#31867;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Classification Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10095
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#25193;&#25955;&#27169;&#22411;&#65288;CDMs&#65289;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#30340;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#21152;&#22312;&#24178;&#20928;&#20449;&#21495;&#19978;&#30340;&#22122;&#22768;&#37327;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.10095v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#19968;&#31181;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#30340;&#31361;&#20986;&#26041;&#27861;&#23478;&#26063;&#20381;&#36182;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#65288;DRE&#65289;&#65292;&#20854;&#20013;&#27169;&#22411;&#34987;&#35757;&#32451;&#26469;$\textit{&#20998;&#31867;}$&#25968;&#25454;&#26679;&#26412;&#21644;&#26469;&#33258;&#26576;&#20010;&#21442;&#32771;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#36825;&#20123;&#25216;&#26415;&#22312;&#31616;&#21333;&#30340;&#20302;&#32500;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#65289;&#20013;&#26080;&#27861;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#23398;&#20064;&#20998;&#24067;&#30340;&#21478;&#19968;&#31181;&#26041;&#27861;&#23478;&#26063;&#26159;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#65292;&#20854;&#20013;&#27169;&#22411;&#34987;&#35757;&#32451;&#26469;$\textit{&#21435;&#22122;}$&#25968;&#25454;&#26679;&#26412;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{&#20998;&#31867;&#25193;&#25955;&#27169;&#22411;}$&#65288;CDMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#25216;&#26415;&#65292;&#23427;&#37319;&#29992;&#20102;DDM&#30340;&#21435;&#22122;&#22522;&#26412;&#24418;&#24335;&#65292;&#21516;&#26102;&#21033;&#29992;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#21152;&#22312;&#24178;&#20928;&#20449;&#21495;&#19978;&#30340;&#22122;&#22768;&#37327;&#65292;&#31867;&#20284;&#20110;DRE&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#23519;&#65292;&#21363;MSE&#26368;&#20248;&#21270;&#30340;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10095v1 Announce Type: new  Abstract: A prominent family of methods for learning data distributions relies on density ratio estimation (DRE), where a model is trained to $\textit{classify}$ between data samples and samples from some reference distribution. These techniques are successful in simple low-dimensional settings but fail to achieve good results on complex high-dimensional data, like images. A different family of methods for learning distributions is that of denoising diffusion models (DDMs), in which a model is trained to $\textit{denoise}$ data samples. These approaches achieve state-of-the-art results in image, video, and audio generation. In this work, we present $\textit{Classification Diffusion Models}$ (CDMs), a generative technique that adopts the denoising-based formalism of DDMs while making use of a classifier that predicts the amount of noise added to a clean signal, similarly to DRE methods. Our approach is based on the observation that an MSE-optimal d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#27979;&#24615;&#32447;&#24615;&#22312;&#32447;&#36861;&#36394;&#65288;PLOT&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#36861;&#36394;&#26410;&#30693;&#30446;&#26631;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#20855;&#26377;&#25351;&#25968;&#36951;&#24536;&#30340;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#23398;&#20064;&#30446;&#26631;&#30340;&#26102;&#21464;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#22312;&#36882;&#25512;&#35270;&#32447;&#25511;&#21046;&#30340;&#26694;&#26550;&#19979;&#20351;&#29992;&#25152;&#23398;&#27169;&#22411;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.10036</link><description>&lt;p&gt;
&#39044;&#27979;&#24615;&#32447;&#24615;&#22312;&#32447;&#36861;&#36394;&#26410;&#30693;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Predictive Linear Online Tracking for Unknown Targets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#27979;&#24615;&#32447;&#24615;&#22312;&#32447;&#36861;&#36394;&#65288;PLOT&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#36861;&#36394;&#26410;&#30693;&#30446;&#26631;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#20855;&#26377;&#25351;&#25968;&#36951;&#24536;&#30340;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#23398;&#20064;&#30446;&#26631;&#30340;&#26102;&#21464;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#22312;&#36882;&#25512;&#35270;&#32447;&#25511;&#21046;&#30340;&#26694;&#26550;&#19979;&#20351;&#29992;&#25152;&#23398;&#27169;&#22411;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#32447;&#24615;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#36861;&#36394;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#36319;&#38543;&#19968;&#20010;&#31227;&#21160;&#30340;&#30446;&#26631;&#12290;&#19982;&#32463;&#20856;&#30340;&#36861;&#36394;&#25511;&#21046;&#19981;&#21516;&#65292;&#30446;&#26631;&#26159;&#26410;&#30693;&#30340;&#12289;&#38750;&#24179;&#31283;&#30340;&#65292;&#24182;&#19988;&#23427;&#30340;&#29366;&#24577;&#36880;&#27493;&#25581;&#31034;&#65292;&#22240;&#27492;&#36866;&#21512;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20108;&#27425;&#25104;&#26412;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;&#39044;&#27979;&#24615;&#32447;&#24615;&#22312;&#32447;&#36861;&#36394;&#65288;PLOT&#65289;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#20855;&#26377;&#25351;&#25968;&#36951;&#24536;&#30340;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#23398;&#20064;&#30446;&#26631;&#30340;&#26102;&#21464;&#21160;&#24577;&#27169;&#22411;&#12290;&#25152;&#23398;&#27169;&#22411;&#22312;&#36882;&#25512;&#35270;&#32447;&#25511;&#21046;&#30340;&#26694;&#26550;&#19979;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;PLOT&#30340;&#21160;&#24577;&#36951;&#25022;&#19982;$\mathcal{O}(\sqrt{TV_T})$&#25104;&#27604;&#20363;&#65292;&#20854;&#20013;$V_T$&#26159;&#30446;&#26631;&#21160;&#21147;&#23398;&#30340;&#24635;&#21464;&#21270;&#37327;&#65292;$T$&#26159;&#26102;&#38388;&#38271;&#24230;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#22235;&#26059;&#32764;&#26426;&#19978;&#23454;&#29616;&#20102;PLOT&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10036v1 Announce Type: cross  Abstract: In this paper, we study the problem of online tracking in linear control systems, where the objective is to follow a moving target. Unlike classical tracking control, the target is unknown, non-stationary, and its state is revealed sequentially, thus, fitting the framework of online non-stochastic control. We consider the case of quadratic costs and propose a new algorithm, called predictive linear online tracking (PLOT). The algorithm uses recursive least squares with exponential forgetting to learn a time-varying dynamic model of the target. The learned model is used in the optimal policy under the framework of receding horizon control. We show the dynamic regret of PLOT scales with $\mathcal{O}(\sqrt{TV_T})$, where $V_T$ is the total variation of the target dynamics and $T$ is the time horizon. Unlike prior work, our theoretical results hold for non-stationary targets. We implement PLOT on a real quadrotor and provide open-source so
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;LESS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#19968;&#20010;&#22266;&#23450;&#23610;&#23544;&#30340;&#32531;&#23384;&#21644;&#22522;&#20110;&#39537;&#36880;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20943;&#23567;&#20869;&#23384;&#21344;&#29992;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20840;&#37096;&#26631;&#35760;&#30340;&#21487;&#26597;&#35810;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09398</link><description>&lt;p&gt;
&#20351;&#29992;KV&#32531;&#23384;&#21387;&#32553;&#21512;&#25104;&#24490;&#29615;&#20197;&#25552;&#39640;LLM&#25512;&#26029;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09398
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;LESS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#19968;&#20010;&#22266;&#23450;&#23610;&#23544;&#30340;&#32531;&#23384;&#21644;&#22522;&#20110;&#39537;&#36880;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20943;&#23567;&#20869;&#23384;&#21344;&#29992;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20840;&#37096;&#26631;&#35760;&#30340;&#21487;&#26597;&#35810;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35745;&#31639;&#22240;&#32032;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#30001;&#38190;&#20540;(KV)&#32531;&#23384;&#24341;&#36215;&#30340;&#20869;&#23384;&#29942;&#39048;&#65292;&#36825;&#26159;&#19968;&#31181;&#35745;&#31639;&#24555;&#25463;&#26041;&#24335;&#65292;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#38656;&#35201;&#23384;&#20648;&#20808;&#21069;&#30340;KV&#23545;&#12290;&#29616;&#26377;&#30340;KV&#32531;&#23384;&#26041;&#27861;&#36890;&#36807;&#20462;&#21098;&#25110;&#39537;&#36880;&#30456;&#23545;&#19981;&#37325;&#35201;&#30340;KV&#23545;&#30340;&#22823;&#29255;&#21306;&#22495;&#65292;&#26174;&#33879;&#20943;&#23569;&#32531;&#23384;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#20294;&#22312;&#38656;&#35201;&#37325;&#26032;&#25910;&#38598;&#22823;&#22810;&#25968;&#21069;&#19968;&#20010;&#26631;&#35760;&#30340;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LESS&#65292;&#23427;&#23558;&#19968;&#20010;&#65288;&#20960;&#20046;&#20813;&#36153;&#30340;&#65289;&#22266;&#23450;&#23610;&#23544;&#30340;&#32531;&#23384;&#19982;&#22522;&#20110;&#39537;&#36880;&#30340;&#32531;&#23384;&#26041;&#27861;&#31616;&#21333;&#22320;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20197;&#20415;&#25152;&#26377;&#30340;&#26631;&#35760;&#21487;&#20197;&#22312;&#21518;&#32493;&#30340;&#35299;&#30721;&#27493;&#39588;&#20013;&#26597;&#35810;&#12290;&#23427;&#33021;&#22815;&#22312;&#26102;&#38388;&#19978;&#20445;&#30041;&#20449;&#24687;&#65292;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#21512;&#29702;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LESS&#21487;&#20197;&#24110;&#21161;&#20943;&#23567;&#32531;&#23384;&#25152;&#26377;&#20869;&#23481;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#26377;&#26102;&#29978;&#33267;&#21487;&#20197;&#19982;&#20854;&#30456;&#21305;&#37197;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09398v1 Announce Type: cross Abstract: Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;Chinese MentalBERT&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#24515;&#29702;&#20581;&#24247;&#25991;&#26412;&#20998;&#26512;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#24515;&#29702;&#23398;&#35789;&#20856;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09151</link><description>&lt;p&gt;
Chinese MentalBERT: &#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#38024;&#23545;&#20013;&#22269;&#24515;&#29702;&#20581;&#24247;&#25991;&#26412;&#20998;&#26512;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;Chinese MentalBERT&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#19978;&#24515;&#29702;&#20581;&#24247;&#25991;&#26412;&#20998;&#26512;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#24515;&#29702;&#23398;&#35789;&#20856;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#31038;&#20132;&#23186;&#20307;&#30340;&#24433;&#21709;&#65292;&#24515;&#29702;&#38382;&#39064;&#22312;&#24403;&#21069;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#31038;&#20132;&#23186;&#20307;&#25104;&#20026;&#20010;&#20154;&#20998;&#20139;&#24863;&#21463;&#30340;&#37325;&#35201;&#20986;&#21475;&#12290;&#36825;&#23548;&#33268;&#27599;&#22825;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#20854;&#20013;&#36127;&#38754;&#24773;&#32490;&#26377;&#28508;&#21147;&#24341;&#21457;&#21361;&#26426;&#12290;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#20986;&#33021;&#22815;&#39640;&#25928;&#20998;&#26512;&#36825;&#20123;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#26174;&#31034;&#20986;&#25928;&#26524;&#65292;&#20294;&#38024;&#23545;&#24515;&#29702;&#23398;&#31561;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23384;&#22312;&#26126;&#26174;&#32570;&#22833;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25910;&#38598;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#24182;&#20016;&#23500;&#20102;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;336&#19975;&#26465;&#25991;&#26412;&#26465;&#30446;&#30340;&#32508;&#21512;&#25968;&#25454;&#24211;&#12290;&#20026;&#25552;&#39640;&#27169;&#22411;&#22312;&#24515;&#29702;&#25991;&#26412;&#20998;&#26512;&#19978;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#24515;&#29702;&#23398;&#35789;&#20856;&#34701;&#20837;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#26426;&#21046;&#12290;&#22312;&#29616;&#26377;&#30340;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#19978;&#36827;&#34892;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09151v1 Announce Type: new Abstract: In the current environment, psychological issues are prevalent and widespread, with social media serving as a key outlet for individuals to share their feelings. This results in the generation of vast quantities of data daily, where negative emotions have the potential to precipitate crisis situations. There is a recognized need for models capable of efficient analysis. While pre-trained language models have demonstrated their effectiveness broadly, there's a noticeable gap in pre-trained models tailored for specialized domains like psychology. To address this, we have collected a huge dataset from Chinese social media platforms and enriched it with publicly available datasets to create a comprehensive database encompassing 3.36 million text entries. To enhance the model's applicability to psychological text analysis, we integrated psychological lexicons into the pre-training masking mechanism. Building on an existing Chinese language mod
&lt;/p&gt;</description></item><item><title>SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09025</link><description>&lt;p&gt;
SLEB: &#36890;&#36807;&#20887;&#20313;&#39564;&#35777;&#21644;&#28040;&#38500;Transformer&#22359;&#20248;&#21270;LLM&#30340;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09025
&lt;/p&gt;
&lt;p&gt;
SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#32473;&#23454;&#38469;&#37096;&#32626;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#31934;&#31616;&#65292;&#19968;&#31181;&#26088;&#22312;&#20943;&#23567;LLM&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#21024;&#38500;&#20887;&#20313;&#32452;&#20214;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#31934;&#31616;&#26377;&#24076;&#26395;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31471;&#21040;&#31471;LLM&#25512;&#29702;&#21152;&#36895;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SLEB&#65292;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36873;&#25321;Transformer&#22359;&#20316;&#20026;&#31934;&#31616;&#30340;&#22522;&#26412;&#21333;&#20301;&#65292;&#22240;&#20026;LLM&#22312;&#30456;&#37051;&#22359;&#30340;&#36755;&#20986;&#20043;&#38388;&#20855;&#26377;&#22359;&#32423;&#21035;&#30340;&#20887;&#20313;&#21644;&#39640;&#30456;&#20284;&#24615;&#12290;&#36825;&#20010;&#36873;&#25321;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22686;&#24378;LLM&#30340;&#22788;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SLEB&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09025v1 Announce Type: new Abstract: Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB successfully accelerates LLM inference without
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#32454;&#35843;&#22823;&#22411;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#38469;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#25968;&#37327;&#20197;&#21450;&#35757;&#32451;&#32467;&#26463;&#26102;&#30340;&#22823;&#26799;&#24230;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2402.06674</link><description>&lt;p&gt;
&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#38469;&#25104;&#21592;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Understanding Practical Membership Privacy of Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#32454;&#35843;&#22823;&#22411;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#38469;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#25968;&#37327;&#20197;&#21450;&#35757;&#32451;&#32467;&#26463;&#26102;&#30340;&#22823;&#26799;&#24230;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#26469;&#31995;&#32479;&#22320;&#27979;&#35797;&#32454;&#35843;&#22823;&#22411;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#38469;&#38544;&#31169;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#29702;&#35299;&#20351;&#25968;&#25454;&#38598;&#21644;&#26679;&#26412;&#23481;&#26131;&#21463;&#21040;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#29305;&#24615;&#12290;&#22312;&#25968;&#25454;&#38598;&#29305;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#25968;&#37327;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#24130;&#24459;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#26159;&#20197;&#25915;&#20987;&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;&#22312;&#20302;&#20551;&#38451;&#24615;&#29575;&#19979;&#27979;&#37327;&#65289;&#26469;&#34913;&#37327;&#30340;&#12290;&#23545;&#20110;&#20010;&#21035;&#26679;&#26412;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#32467;&#26463;&#26102;&#20135;&#29983;&#30340;&#22823;&#26799;&#24230;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply a state-of-the-art membership inference attack (MIA) to systematically test the practical privacy vulnerability of fine-tuning large image classification models.We focus on understanding the properties of data sets and samples that make them vulnerable to membership inference. In terms of data set properties, we find a strong power law dependence between the number of examples per class in the data and the MIA vulnerability, as measured by true positive rate of the attack at a low false positive rate. For an individual sample, large gradients at the end of training are strongly correlated with MIA vulnerability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU-like&#28608;&#27963;&#20989;&#25968;&#20197;&#32463;&#39564;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#28857;&#26465;&#20214;&#21644;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#38797;&#28857;&#36867;&#36920;&#19982;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#21464;&#21270;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>https://arxiv.org/abs/2402.05626</link><description>&lt;p&gt;
&#27973;&#23618;ReLU-like&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65306;&#31283;&#23450;&#28857;&#12289;&#38797;&#28857;&#36867;&#36920;&#21644;&#32593;&#32476;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
The Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escaping, and Network Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU-like&#28608;&#27963;&#20989;&#25968;&#20197;&#32463;&#39564;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#28857;&#26465;&#20214;&#21644;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#38797;&#28857;&#36867;&#36920;&#19982;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#21464;&#21270;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU-like&#28608;&#27963;&#20989;&#25968;&#20197;&#32463;&#39564;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#12290;&#30001;&#20110;&#28608;&#27963;&#20989;&#25968;&#26159;&#19981;&#21487;&#24494;&#30340;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#22914;&#20309;&#23436;&#20840;&#25551;&#36848;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#38750;&#21487;&#24494;&#21644;&#21487;&#24494;&#24773;&#20917;&#30340;&#31283;&#23450;&#28857;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#26524;&#19968;&#20010;&#31283;&#23450;&#28857;&#19981;&#21253;&#21547;&#8220;&#36867;&#36920;&#31070;&#32463;&#20803;&#8221;&#65288;&#36890;&#36807;&#19968;&#38454;&#26465;&#20214;&#23450;&#20041;&#65289;&#65292;&#37027;&#20040;&#23427;&#24517;&#23450;&#26159;&#19968;&#20010;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#22312;&#26631;&#37327;&#36755;&#20986;&#24773;&#20917;&#19979;&#65292;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#20445;&#35777;&#20102;&#31283;&#23450;&#28857;&#19981;&#26159;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#25551;&#36848;&#20102;&#20174;&#26080;&#31351;&#23567;&#65288;&#28040;&#22833;&#65289;&#21021;&#22987;&#21270;&#24320;&#22987;&#30340;&#27973;&#23618;ReLU-like&#32593;&#32476;&#30340;&#38797;&#28857;&#21040;&#38797;&#28857;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#30452;&#25509;&#23558;&#38797;&#28857;&#36867;&#36920;&#19982;&#36867;&#36920;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#21464;&#21270;&#32852;&#31995;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23436;&#20840;&#35752;&#35770;&#20102;&#32593;&#32476;&#23884;&#20837;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the loss landscape of one-hidden-layer neural networks with ReLU-like activation functions trained with the empirical squared loss. As the activation function is non-differentiable, it is so far unclear how to completely characterize the stationary points. We propose the conditions for stationarity that apply to both non-differentiable and differentiable cases. Additionally, we show that, if a stationary point does not contain "escape neurons", which are defined with first-order conditions, then it must be a local minimum. Moreover, for the scalar-output case, the presence of an escape neuron guarantees that the stationary point is not a local minimum. Our results refine the description of the saddle-to-saddle training process starting from infinitesimally small (vanishing) initialization for shallow ReLU-like networks, linking saddle escaping directly with the parameter changes of escape neurons. Moreover, we are also able to fully discuss how network emb
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31995;&#32479;&#12289;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644;CATE&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#23567;&#21306;&#38388;&#23485;&#24230;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#23454;&#39564;&#35206;&#30422;&#33539;&#22260;&#65292;&#21487;&#20197;&#25552;&#20379;&#30495;&#23454;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.04906</link><description>&lt;p&gt;
&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931;&#20803;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31995;&#32479;&#12289;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644;CATE&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#23567;&#21306;&#38388;&#23485;&#24230;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#23454;&#39564;&#35206;&#30422;&#33539;&#22260;&#65292;&#21487;&#20197;&#25552;&#20379;&#30495;&#23454;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#35782;&#24178;&#39044;&#25928;&#26524;&#65292;&#21363;&#27835;&#30103;&#25928;&#26524;&#65292;&#23545;&#20110;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#29992;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524; (CATE) &#20272;&#35745;&#31561;&#26041;&#27861;&#36890;&#24120;&#21482;&#25552;&#20379;&#27835;&#30103;&#25928;&#26524;&#30340;&#28857;&#20272;&#35745;&#65292;&#32780;&#24120;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#21363;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931; (CMC) &#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31995;&#32479;&#12289;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644; CATE &#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#26469;&#20135;&#29983;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32467;&#26524;&#22122;&#22768;&#20998;&#24067;&#30340;&#29305;&#23450;&#20551;&#35774;&#22914;&#20309;&#20005;&#37325;&#24433;&#21709;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;CMC&#26694;&#26550;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#23454;&#39564;&#35206;&#30422;&#33539;&#22260;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#23567;&#30340;&#21306;&#38388;&#23485;&#24230;&#65292;&#20197;&#25552;&#20379;&#30495;&#23454;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge of the effect of interventions, called the treatment effect, is paramount for decision-making. Approaches to estimating this treatment effect, e.g. by using Conditional Average Treatment Effect (CATE) estimators, often only provide a point estimate of this treatment effect, while additional uncertainty quantification is frequently desired instead. Therefore, we present a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to instead produce a predictive distribution usable in individualized decision-making. Furthermore, we show how specific assumptions on the noise distribution of the outcome heavily affect these uncertainty predictions. Nonetheless, the CMC framework shows strong experimental coverage while retaining small interval widths to provide estimates of the true individual treatment effect.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#20449;&#24687;&#30340;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#32467;&#26500;&#21270;&#30340;&#35268;&#21017;&#25163;&#20876;&#20316;&#20026;&#30693;&#35782;&#28304;&#65292;&#21033;&#29992;&#24773;&#22659;&#24863;&#30693;&#30340;&#22870;&#21169;&#35774;&#35745;&#35780;&#20272;&#23398;&#20064;&#36712;&#36857;&#65292;&#20197;&#23454;&#29616;&#23545;&#38656;&#35201;&#25511;&#21046;&#20132;&#36890;&#35268;&#21017;&#20363;&#22806;&#30340;&#24773;&#26223;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.04168</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#30340;&#22686;&#24378;&#23398;&#20064;&#29992;&#20110;&#24773;&#22659;&#24863;&#30693;&#20132;&#36890;&#35268;&#21017;&#20363;&#22806;
&lt;/p&gt;
&lt;p&gt;
Informed Reinforcement Learning for Situation-Aware Traffic Rule Exceptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#20449;&#24687;&#30340;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#32467;&#26500;&#21270;&#30340;&#35268;&#21017;&#25163;&#20876;&#20316;&#20026;&#30693;&#35782;&#28304;&#65292;&#21033;&#29992;&#24773;&#22659;&#24863;&#30693;&#30340;&#22870;&#21169;&#35774;&#35745;&#35780;&#20272;&#23398;&#20064;&#36712;&#36857;&#65292;&#20197;&#23454;&#29616;&#23545;&#38656;&#35201;&#25511;&#21046;&#20132;&#36890;&#35268;&#21017;&#20363;&#22806;&#30340;&#24773;&#26223;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#23398;&#20064;&#26159;&#19968;&#20010;&#38750;&#24120;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20855;&#26377;&#24456;&#22810;&#26377;&#21069;&#26223;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#65292;&#36890;&#24120;&#21482;&#30740;&#31350;&#38750;&#24120;&#31616;&#21333;&#30340;&#22330;&#26223;&#12290;&#24120;&#35265;&#30340;&#26041;&#27861;&#20351;&#29992;&#38750;&#21487;&#35299;&#37322;&#30340;&#25511;&#21046;&#21629;&#20196;&#20316;&#20026;&#21160;&#20316;&#31354;&#38388;&#65292;&#20197;&#21450;&#32570;&#20047;&#32467;&#26500;&#30340;&#22870;&#21169;&#35774;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20449;&#24687;&#30340;&#22686;&#24378;&#23398;&#20064;&#65292;&#23558;&#32467;&#26500;&#21270;&#30340;&#35268;&#21017;&#25163;&#20876;&#20316;&#20026;&#30693;&#35782;&#28304;&#38598;&#25104;&#36827;&#26469;&#12290;&#25105;&#20204;&#23398;&#20064;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#24773;&#22659;&#24863;&#30693;&#30340;&#22870;&#21169;&#35774;&#35745;&#26469;&#35780;&#20272;&#23427;&#20204;&#65292;&#20174;&#32780;&#20135;&#29983;&#21160;&#24577;&#22870;&#21169;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#23398;&#20064;&#38656;&#35201;&#25511;&#21046;&#20132;&#36890;&#35268;&#21017;&#20363;&#22806;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#24847;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23637;&#31034;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#39640;&#23436;&#25104;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning is a highly active research field with promising advancements. In the field of autonomous driving, however, often very simple scenarios are being examined. Common approaches use non-interpretable control commands as the action space and unstructured reward designs which lack structure. In this work, we introduce Informed Reinforcement Learning, where a structured rulebook is integrated as a knowledge source. We learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward which allows the agent to learn situations which require controlled traffic rule exceptions. Our method is applicable to arbitrary RL models. We successfully demonstrate high completion rates of complex scenarios with recent model-based agents.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#20013;&#30340;&#24635;&#26799;&#24230;&#26041;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#25209;&#27425;&#22823;&#23567;&#26377;&#21161;&#20110;&#20943;&#23567;&#21063;&#37319;&#27171;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20248;&#21270;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03990</link><description>&lt;p&gt;
&#21063;&#37319;&#27171;&#24182;&#19981;&#26159;&#39764;&#27861;: &#22823;&#25209;&#37327;&#22823;&#23567;&#28858;&#20160;&#40636;&#36969;&#29992;&#26044;&#24046;&#20998;&#38577;&#31169;&#38568;&#27231;&#20778;&#21270;
&lt;/p&gt;
&lt;p&gt;
Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private Stochastic Optimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03990
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#20013;&#30340;&#24635;&#26799;&#24230;&#26041;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#25209;&#27425;&#22823;&#23567;&#26377;&#21161;&#20110;&#20943;&#23567;&#21063;&#37319;&#27171;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20497;&#30740;&#31350;&#20102;&#25209;&#27425;&#22823;&#23567;&#23565;&#24046;&#20998;&#38577;&#31169;&#38568;&#27231;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#20013;&#32317;&#26799;&#24230;&#26041;&#24046;&#30340;&#24433;&#38911;&#65292;&#23563;&#27714;&#23565;&#22823;&#25209;&#27425;&#22823;&#23567;&#26377;&#29992;&#24615;&#30340;&#29702;&#35542;&#35299;&#37323;&#12290;&#30001;&#26044;DP-SGD&#26159;&#29694;&#20195;&#24046;&#20998;&#38577;&#31169;&#28145;&#24230;&#23416;&#32722;&#30340;&#22522;&#30990;&#65292;&#20854;&#24615;&#36074;&#24050;&#34987;&#24291;&#27867;&#30740;&#31350;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#22312;&#23526;&#36368;&#20013;&#30332;&#29694;&#22823;&#25209;&#27425;&#22823;&#23567;&#26377;&#30410;&#12290;&#28982;&#32780;&#65292;&#23565;&#26044;&#36889;&#31278;&#22909;&#34389;&#30340;&#29702;&#35542;&#35299;&#37323;&#30446;&#21069;&#26368;&#22810;&#21482;&#33021;&#35498;&#26159;&#21855;&#30332;&#24335;&#30340;&#12290;&#25105;&#20497;&#39318;&#20808;&#35264;&#23519;&#21040;&#65292;&#22312;DP-SGD&#20013;&#65292;&#32317;&#26799;&#24230;&#26041;&#24046;&#21487;&#20197;&#20998;&#35299;&#28858;&#30001;&#21063;&#37319;&#27171;&#21644;&#22122;&#32882;&#24341;&#36215;&#30340;&#26041;&#24046;&#12290;&#28982;&#24460;&#65292;&#25105;&#20497;&#35657;&#26126;&#22312;&#28961;&#38480;&#27425;&#36845;&#20195;&#30340;&#26997;&#38480;&#24773;&#27841;&#19979;&#65292;&#26377;&#25928;&#30340;&#22122;&#32882;&#24341;&#36215;&#30340;&#26041;&#24046;&#23565;&#25209;&#27425;&#22823;&#23567;&#26159;&#19981;&#35722;&#30340;&#12290;&#21097;&#19979;&#30340;&#21063;&#37319;&#27171;&#24341;&#36215;&#30340;&#26041;&#24046;&#38568;&#33879;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#22823;&#32780;&#28187;&#23567;&#65292;&#22240;&#27492;&#22823;&#25209;&#27425;&#22823;&#23567;&#28187;&#23567;&#20102;&#26377;&#25928;&#30340;&#32317;&#26799;&#24230;&#26041;&#24046;&#12290;&#25105;&#20497;&#22312;&#25976;&#20540;&#19978;&#30906;&#35469;&#36889;&#31278;&#28472;&#36914;&#30340;&#24773;&#27841;&#22312;&#23526;&#38555;&#29872;&#22659;&#20013;&#26159;&#30456;&#38364;&#30340;&#65292;&#30070;&#25209;&#27425;&#22823;&#23567;&#19981;&#23567;&#30340;&#26178;&#20505;&#26371;&#36215;&#20316;&#29992;&#65292;&#20006;&#19988;&#30332;&#29694;
&lt;/p&gt;
&lt;p&gt;
We study the effect of the batch size to the total gradient variance in differentially private stochastic gradient descent (DP-SGD), seeking a theoretical explanation for the usefulness of large batch sizes. As DP-SGD is the basis of modern DP deep learning, its properties have been widely studied, and recent works have empirically found large batch sizes to be beneficial. However, theoretical explanations of this benefit are currently heuristic at best. We first observe that the total gradient variance in DP-SGD can be decomposed into subsampling-induced and noise-induced variances. We then prove that in the limit of an infinite number of iterations, the effective noise-induced variance is invariant to the batch size. The remaining subsampling-induced variance decreases with larger batch sizes, so large batches reduce the effective total gradient variance. We confirm numerically that the asymptotic regime is relevant in practical settings when the batch size is not small, and find tha
&lt;/p&gt;</description></item><item><title>LLMs&#26080;&#27861;&#29420;&#33258;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65292;&#20294;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#21457;&#25381;&#26356;&#22823;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.01817</link><description>&lt;p&gt;
LLMs&#26080;&#27861;&#35268;&#21010;&#65292;&#20294;&#21487;&#20197;&#22312;LLM-Modulo&#26694;&#26550;&#20013;&#24110;&#21161;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01817
&lt;/p&gt;
&lt;p&gt;
LLMs&#26080;&#27861;&#29420;&#33258;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65292;&#20294;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#21457;&#25381;&#26356;&#22823;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35268;&#21010;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#35282;&#33394;&#23384;&#22312;&#24456;&#22823;&#30340;&#22256;&#24785;&#12290;&#19968;&#26041;&#38754;&#26377;&#20154;&#36807;&#20110;&#20048;&#35266;&#22320;&#22768;&#31216;&#21482;&#38656;&#27491;&#30830;&#25552;&#31034;&#25110;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;LLMs&#23601;&#33021;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20063;&#26377;&#20154;&#36807;&#20110;&#24754;&#35266;&#22320;&#35748;&#20026;LLMs&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#20165;&#33021;&#20316;&#20026;&#38382;&#39064;&#35268;&#33539;&#30340;&#31616;&#21333;&#32763;&#35793;&#22120;&#65292;&#24182;&#23558;&#38382;&#39064;&#20132;&#32473;&#22806;&#37096;&#31526;&#21495;&#27714;&#35299;&#22120;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20004;&#31181;&#26497;&#31471;&#35266;&#28857;&#37117;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#33258;&#22238;&#24402;LLMs&#26412;&#36523;&#19981;&#33021;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65288;&#27605;&#31455;&#36825;&#26159;&#19968;&#31181;&#25512;&#29702;&#24418;&#24335;&#65289;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#35823;&#35299;&#21407;&#22240;&#36827;&#34892;&#20102;&#19968;&#20123;&#38416;&#36848;&#12290;&#25105;&#20204;&#36824;&#23558;&#36777;&#31216;LLMs&#24212;&#35813;&#34987;&#35270;&#20026;&#20855;&#26377;&#26356;&#26377;&#24847;&#20041;&#30340;&#35282;&#33394;&#30340;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#65292;&#33021;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21457;&#25381;&#26356;&#22823;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end forma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#22810;&#32452;&#23398;&#20064;&#25193;&#23637;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#24773;&#20917;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#36755;&#20986;&#21487;&#35299;&#37322;&#19988;&#30830;&#23450;&#24615;&#30340;&#20915;&#31574;&#26641;&#39044;&#27979;&#22120;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#21560;&#24341;&#21147;&#30340;&#24191;&#20041;&#21270;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00258</link><description>&lt;p&gt;
&#22810;&#32452;&#23398;&#20064;&#30340;&#23618;&#27425;&#32452;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multi-group Learning for Hierarchical Groups
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#22810;&#32452;&#23398;&#20064;&#25193;&#23637;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#24773;&#20917;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#36755;&#20986;&#21487;&#35299;&#37322;&#19988;&#30830;&#23450;&#24615;&#30340;&#20915;&#31574;&#26641;&#39044;&#27979;&#22120;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#21560;&#24341;&#21147;&#30340;&#24191;&#20041;&#21270;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32452;&#23398;&#20064;&#27169;&#22411;&#23558;&#23398;&#20064;&#22330;&#26223;&#35268;&#33539;&#21270;&#20026;&#21333;&#19968;&#39044;&#27979;&#22120;&#22312;&#22810;&#20010;&#21487;&#33021;&#37325;&#21472;&#30340;&#20852;&#36259;&#23376;&#32452;&#19978;&#24517;&#39035;&#24191;&#20041;&#21270;&#12290;&#25105;&#20204;&#23558;&#22810;&#32452;&#23398;&#20064;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#20102;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#33258;&#28982;&#24773;&#20917;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#29992;&#20110;&#36755;&#20986;&#21487;&#35299;&#37322;&#19988;&#30830;&#23450;&#24615;&#30340;&#20915;&#31574;&#26641;&#39044;&#27979;&#22120;&#65292;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#35813;&#31639;&#27861;&#36827;&#34892;&#32463;&#39564;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#20855;&#26377;&#23618;&#27425;&#32452;&#32467;&#26500;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26377;&#21560;&#24341;&#21147;&#30340;&#24191;&#20041;&#21270;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-group learning model formalizes the learning scenario in which a single predictor must generalize well on multiple, possibly overlapping subgroups of interest. We extend the study of multi-group learning to the natural case where the groups are hierarchically structured. We design an algorithm for this setting that outputs an interpretable and deterministic decision tree predictor with near-optimal sample complexity. We then conduct an empirical evaluation of our algorithm and find that it achieves attractive generalization properties on real datasets with hierarchical group structure.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24378;&#35843;&#20102;&#31185;&#23398;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#27169;&#22411;&#20551;&#35774;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#20551;&#35774;&#35748;&#35782;&#35770;&#22797;&#26434;&#24615;&#30340;&#20998;&#26512;&#65292;&#21516;&#26102;&#32467;&#21512;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#29305;&#28857;&#65292;&#26469;&#35780;&#20272;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.07359</link><description>&lt;p&gt;
&#31185;&#23398;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reliability and Interpretability in Science and Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07359
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24378;&#35843;&#20102;&#31185;&#23398;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#27169;&#22411;&#20551;&#35774;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#20551;&#35774;&#35748;&#35782;&#35770;&#22797;&#26434;&#24615;&#30340;&#20998;&#26512;&#65292;&#21516;&#26102;&#32467;&#21512;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#29305;&#28857;&#65292;&#26469;&#35780;&#20272;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#26085;&#30410;&#37325;&#35201;&#65292;&#24182;&#19988;&#19982;&#27492;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#24050;&#32463;&#28608;&#21457;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#20165;&#23558;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19982;&#26631;&#20934;&#31185;&#23398;&#24314;&#27169;&#26377;&#25152;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#19982;&#23545;DNN&#27169;&#22411;&#19982;&#26631;&#20934;&#31185;&#23398;&#24314;&#27169;&#30340;&#21487;&#33021;&#24046;&#24322;&#20197;&#21450;&#36825;&#20123;&#24046;&#24322;&#22312;&#21487;&#38752;&#24615;&#35780;&#20272;&#20013;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#30340;&#26356;&#28145;&#23618;&#27425;&#30340;&#35748;&#35782;&#35770;&#20998;&#26512;&#30456;&#32467;&#21512;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20960;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#24378;&#35843;&#20102;&#27169;&#22411;&#20551;&#35774;&#65288;&#22312;ML&#21644;&#20256;&#32479;&#31185;&#23398;&#20013;&#22343;&#23384;&#22312;&#65289;&#22312;&#26080;&#29702;&#35770;&#31185;&#23398;&#30340;&#38169;&#35273;&#19979;&#30340;&#26222;&#36941;&#20316;&#29992;&#12290;&#20854;&#27425;&#65292;&#20174;&#65288;&#35748;&#35782;&#35770;&#30340;&#65289;&#22797;&#26434;&#24615;&#35282;&#24230;&#20998;&#26512;&#20102;&#27169;&#22411;&#20551;&#35774;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#27169;&#22411;&#20551;&#35774;&#22312;&#21487;&#38752;&#24615;&#35780;&#20272;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the question of the reliability of Machine Learning (ML) methods has acquired significant importance, and the analysis of the associated uncertainties has motivated a growing amount of research. However, most of these studies have applied standard error analysis to ML models, and in particular Deep Neural Network (DNN) models, which represent a rather significant departure from standard scientific modelling. It is therefore necessary to integrate the standard error analysis with a deeper epistemological analysis of the possible differences between DNN models and standard scientific modelling and the possible implications of these differences in the assessment of reliability. This article offers several contributions. First, it emphasises the ubiquitous role of model assumptions (both in ML and traditional Science) against the illusion of theory-free science. Secondly, model assumptions are analysed from the point of view of their (epistemic) complexity, which is shown 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22122;&#22768;&#20013;&#29983;&#25104;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22122;&#22768;&#27880;&#20837;&#35838;&#31243;&#26469;&#33719;&#21462;&#23398;&#20064;&#21382;&#21490;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#36229;&#36807;&#26368;&#20248;&#31574;&#30053;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.12275</link><description>&lt;p&gt;
&#20174;&#22122;&#22768;&#33976;&#39311;&#20013;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Emergence of In-Context Reinforcement Learning from Noise Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22122;&#22768;&#20013;&#29983;&#25104;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22122;&#22768;&#27880;&#20837;&#35838;&#31243;&#26469;&#33719;&#21462;&#23398;&#20064;&#21382;&#21490;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#36229;&#36807;&#26368;&#20248;&#31574;&#30053;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#20851;&#20110;&#21464;&#24418;&#37329;&#21018;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#29615;&#22659;&#21644;&#20219;&#21153;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#30446;&#21069;&#30340;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21463;&#21040;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#30001;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#29983;&#25104;&#25110;&#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AD$^\varepsilon$&#65292;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#33719;&#21462;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22122;&#22768;&#35825;&#23548;&#30340;&#35838;&#31243;&#26469;&#23454;&#29616;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26500;&#24314;&#19968;&#20010;&#24110;&#21161;&#33719;&#21462;&#23398;&#20064;&#21382;&#21490;&#30340;&#21512;&#25104;&#22122;&#22768;&#27880;&#20837;&#35838;&#31243;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#26080;&#38656;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#29983;&#25104;&#65292;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#33021;&#22815;&#20197;2&#20493;&#30340;&#36793;&#30028;&#20248;&#20110;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, extensive studies in Reinforcement Learning have been carried out on the ability of transformers to adapt in-context to various environments and tasks. Current in-context RL methods are limited by their strict requirements for data, which needs to be generated by RL agents or labeled with actions from an optimal policy. In order to address this prevalent problem, we propose AD$^\varepsilon$, a new data acquisition approach that enables in-context Reinforcement Learning from noise-induced curriculum. We show that it is viable to construct a synthetic noise injection curriculum which helps to obtain learning histories. Moreover, we experimentally demonstrate that it is possible to alleviate the need for generation using optimal policies, with in-context RL still able to outperform the best suboptimal policy in a learning dataset by a 2x margin.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;RMIA&#65289;&#65292;&#20855;&#26377;&#26356;&#20934;&#30830;&#30340;&#24314;&#27169;&#21644;&#26356;&#39640;&#30340;&#27979;&#35797;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#38544;&#31169;&#39118;&#38505;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2312.03262</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#39640;&#21151;&#29575;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Low-Cost High-Power Membership Inference Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;RMIA&#65289;&#65292;&#20855;&#26377;&#26356;&#20934;&#30830;&#30340;&#24314;&#27169;&#21644;&#26356;&#39640;&#30340;&#27979;&#35797;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#38544;&#31169;&#39118;&#38505;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#26088;&#22312;&#26816;&#27979;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#20351;&#29992;&#12290;&#26368;&#36817;&#19968;&#20123;&#24378;&#22823;&#30340;&#25915;&#20987;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#34920;&#29616;&#19981;&#19968;&#33268;&#65292;&#20351;&#23427;&#20204;&#23545;&#20110;&#23454;&#38469;&#30340;&#38544;&#31169;&#39118;&#38505;&#35780;&#20272;&#19981;&#21487;&#38752;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;RMIA&#65289;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#27169;&#22411;&#30340;&#24635;&#20307;&#25968;&#25454;&#21644;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20284;&#28982;&#27604;&#26816;&#39564;&#20013;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;&#38646;&#20551;&#35774;&#35774;&#32622;&#65292;&#24182;&#26377;&#25928;&#22320;&#21033;&#29992;&#26469;&#33258;&#24635;&#20307;&#30340;&#21442;&#32771;&#27169;&#22411;&#21644;&#21442;&#32771;&#25968;&#25454;&#26679;&#26412;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#30495;&#27491;&#29575;&#65288;true-positive rate&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#39640;&#30340;&#27979;&#35797;&#33021;&#21147;&#65292;&#25972;&#20010;TPR-FPR&#26354;&#32447;&#37117;&#20855;&#22791;&#36825;&#31181;&#20248;&#21183;&#65292;&#21363;&#20351;&#22312;&#26497;&#20302;&#30340;&#35823;&#25253;&#29575;&#19979;&#65288;&#20302;&#33267;0&#65289;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#35745;&#31639;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03262v2 Announce Type: replace-cross  Abstract: Membership inference attacks (MIA) aim to detect if a particular data point was used in training a machine learning model. Recent strong attacks have high computational costs and inconsistent performance under varying conditions, rendering them unreliable for practical privacy risk assessment. We design a novel, efficient, and robust membership inference attack (RMIA) which accurately differentiates between population data and training data of a model, with minimal computational overhead. We achieve this by a more accurate modeling of the null hypothesis setting in our likelihood ratio tests, and effectively leveraging both reference models and reference data samples from the population. Our algorithm exhibits superior test power (true-positive rate) compared to prior methods, throughout the TPR-FPR curve including at extremely low false-positive rates (as low as 0). Under computation constraints, where only a limited number of
&lt;/p&gt;</description></item><item><title>LLM-Pat&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#24182;&#27604;&#36739;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;&#23545;&#24212;&#30340;&#8220;&#20804;&#24351;&#8221;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#21028;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2305.12519</link><description>&lt;p&gt;
LLM&#20146;&#23376;&#37492;&#23450;&#65306;LLM&#36951;&#20256;&#32487;&#25215;&#20013;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LLM Paternity Test: Generated Text Detection with LLM Genetic Inheritance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12519
&lt;/p&gt;
&lt;p&gt;
LLM-Pat&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#24182;&#27604;&#36739;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;&#23545;&#24212;&#30340;&#8220;&#20804;&#24351;&#8221;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#21028;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#29983;&#25104;&#25658;&#24102;&#21508;&#31181;&#28389;&#29992;&#39118;&#38505;&#30340;&#25991;&#26412;&#65292;&#21253;&#25324;&#25220;&#34989;&#12289;&#22312;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#21457;&#24067;&#34394;&#20551;&#35780;&#35770;&#65292;&#25110;&#32773;&#21046;&#20316;&#24341;&#20154;&#27880;&#30446;&#30340;&#34394;&#20551;&#25512;&#25991;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20005;&#37325;&#20381;&#36182;&#35757;&#32451;&#25968;&#25454;&#65292;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#21363;LLM&#20146;&#23376;&#37492;&#23450;&#65288;LLM-Pat&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#20219;&#20309;&#20505;&#36873;&#25991;&#26412;&#65288;"&#23376;&#31867;"&#65289;&#65292;LLM-Pat&#20351;&#29992;&#19968;&#20010;&#20013;&#38388;LLM&#65288;"&#29238;&#31867;"&#65289;&#37325;&#24314;&#19982;&#32473;&#23450;&#25991;&#26412;&#23545;&#24212;&#30340;"&#20804;&#24351;"&#25991;&#26412;&#65292;&#28982;&#21518;&#34913;&#37327;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;"&#20804;&#24351;"&#25991;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#39640;&#30456;&#20284;&#24615;&#34920;&#26126;&#20505;&#36873;&#25991;&#26412;&#26159;&#30001;&#26426;&#22120;&#29983;&#25104;&#65292;&#31867;&#20284;&#20110;&#22522;&#22240;&#29305;&#24449;&#12290;&#25105;&#20204;&#24050;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12519v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) can generate texts that carry the risk of various misuses, including plagiarism, planting fake reviews on e-commerce platforms, or creating inflammatory false tweets. Detecting whether a text is machine-generated has thus become increasingly important. While existing detection methods exhibit superior performance, they often lack generalizability due to their heavy dependence on training data. To alleviate this problem, we propose a model-related generated text detection method, the LLM Paternity Test (LLM-Pat). Specifically, given any candidate text (\textit{child}), LLM-Pat employs an intermediary LLM (\textit{parent}) to reconstruct a \textit{sibling} text corresponding to the given text and then measures the similarity between candidate texts and their sibling texts. High similarity indicates that the candidate text is machine-generated, akin to genetic traits. We have constructed datasets encom
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#24179;&#21488;KGLiDS&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#30693;&#35782;&#22270;&#25216;&#26415;&#26469;&#25277;&#35937;&#21644;&#25429;&#33719;&#25968;&#25454;&#31185;&#23398;&#24037;&#20855;&#21450;&#20854;&#32852;&#31995;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#25903;&#25345;&#25968;&#25454;&#21457;&#29616;&#21644;&#31649;&#36947;&#33258;&#21160;&#21270;&#12290;</title><link>https://arxiv.org/abs/2303.02204</link><description>&lt;p&gt;
KGLiDS&#65306;&#29992;&#20110;&#25968;&#25454;&#31185;&#23398;&#30340;&#35821;&#20041;&#25277;&#35937;&#12289;&#38142;&#25509;&#21644;&#33258;&#21160;&#21270;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
KGLiDS: A Platform for Semantic Abstraction, Linking, and Automation of Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.02204
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#24179;&#21488;KGLiDS&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#30693;&#35782;&#22270;&#25216;&#26415;&#26469;&#25277;&#35937;&#21644;&#25429;&#33719;&#25968;&#25454;&#31185;&#23398;&#24037;&#20855;&#21450;&#20854;&#32852;&#31995;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#25903;&#25345;&#25968;&#25454;&#21457;&#29616;&#21644;&#31649;&#36947;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25105;&#20204;&#35265;&#35777;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#25216;&#26415;&#26469;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#30340;&#26085;&#30410;&#27987;&#21402;&#20852;&#36259;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21019;&#36896;&#20102;&#22823;&#37327;&#30340;&#24037;&#20855;&#65288;&#25968;&#25454;&#38598;&#12289;&#31649;&#36947;&#33050;&#26412;&#31561;&#65289;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#26377;&#31995;&#32479;&#24615;&#30340;&#23581;&#35797;&#26469;&#20840;&#38754;&#25910;&#38598;&#21644;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#20013;&#38544;&#21547;&#30340;&#25152;&#26377;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#30456;&#21453;&#65292;&#25968;&#25454;&#31185;&#23398;&#23478;&#20174;&#21516;&#20107;&#37027;&#37324;&#24674;&#22797;&#20449;&#24687;&#21644;&#19987;&#19994;&#30693;&#35782;&#65292;&#25110;&#36890;&#36807;&#21453;&#22797;&#35797;&#39564;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24179;&#21488;&#65292;KGLiDS&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#30693;&#35782;&#22270;&#25216;&#26415;&#26469;&#25277;&#35937;&#21644;&#25429;&#33719;&#25968;&#25454;&#31185;&#23398;&#24037;&#20855;&#21450;&#20854;&#32852;&#31995;&#30340;&#35821;&#20041;&#12290;&#22522;&#20110;&#36825;&#20123;&#20449;&#24687;&#65292;KGLiDS&#33021;&#22815;&#25903;&#25345;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#25968;&#25454;&#21457;&#29616;&#21644;&#31649;&#36947;&#33258;&#21160;&#21270;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35780;&#20272;&#28085;&#30422;&#20102;&#25968;&#25454;&#21457;&#29616;&#12289;&#25968;&#25454;&#28165;&#27927;&#12289;&#36716;&#25442;&#21644;AutoM&#31561;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.02204v3 Announce Type: replace  Abstract: In recent years, we have witnessed the growing interest from academia and industry in applying data science technologies to analyze large amounts of data. In this process, a myriad of artifacts (datasets, pipeline scripts, etc.) are created. However, there has been no systematic attempt to holistically collect and exploit all the knowledge and experiences that are implicitly contained in those artifacts. Instead, data scientists recover information and expertise from colleagues or learn via trial and error. Hence, this paper presents a scalable platform, KGLiDS, that employs machine learning and knowledge graph technologies to abstract and capture the semantics of data science artifacts and their connections. Based on this information, KGLiDS enables various downstream applications, such as data discovery and pipeline automation. Our comprehensive evaluation covers use cases in data discovery, data cleaning, transformation, and AutoM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#26080;&#20559;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#38468;&#21152;&#33410;&#28857;&#21644;&#20351;&#29992;&#27491;&#36127;&#26435;&#37325;&#37325;&#36830;&#36830;&#25509;&#26469;&#20016;&#23500;&#22270;&#32467;&#26500;&#65292;&#20197;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14580</link><description>&lt;p&gt;
&#35774;&#35745;&#20320;&#33258;&#24049;&#30340;&#23431;&#23449;&#65306;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#26080;&#20559;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Design Your Own Universe: A Physics-Informed Agnostic Method for Enhancing Graph Neural Networks. (arXiv:2401.14580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#26080;&#20559;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#38468;&#21152;&#33410;&#28857;&#21644;&#20351;&#29992;&#27491;&#36127;&#26435;&#37325;&#37325;&#36830;&#36830;&#25509;&#26469;&#20016;&#23500;&#22270;&#32467;&#26500;&#65292;&#20197;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#32531;&#35299;&#24120;&#35265;&#30340;GNN&#25361;&#25112;&#65288;&#22914;&#36807;&#24230;&#24179;&#28369;&#21270;&#12289;&#36807;&#24230;&#21387;&#32553;&#21644;&#24322;&#36136;&#36866;&#24212;&#65289;&#22312;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20173;&#28982;&#22312;&#24320;&#21457;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33539;&#24335;&#26469;&#36866;&#24403;&#22320;&#25972;&#21512;&#22788;&#29702;&#25152;&#26377;&#36825;&#20123;&#25361;&#25112;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;GNN&#30340;&#20256;&#25773;&#19982;&#29289;&#29702;&#31890;&#23376;&#31995;&#32479;&#36827;&#34892;&#31867;&#27604;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22686;&#24378;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#38468;&#21152;&#33410;&#28857;&#21644;&#20351;&#29992;&#27491;&#36127;&#26435;&#37325;&#37325;&#36830;&#36830;&#25509;&#26469;&#20016;&#23500;&#22270;&#32467;&#26500;&#65292;&#21463;&#33410;&#28857;&#26631;&#35760;&#20449;&#24687;&#30340;&#25351;&#23548;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#39564;&#35777;&#20102;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#30340;GNN&#21487;&#20197;&#26377;&#25928;&#22320;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#24182;&#23545;&#36807;&#24230;&#21387;&#32553;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#37325;&#36830;&#22270;&#36827;&#34892;&#20102;&#35889;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#30456;&#24212;&#30340;GNN&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
Physics-informed Graph Neural Networks have achieved remarkable performance in learning through graph-structured data by mitigating common GNN challenges such as over-smoothing, over-squashing, and heterophily adaption. Despite these advancements, the development of a simple yet effective paradigm that appropriately integrates previous methods for handling all these challenges is still underway. In this paper, we draw an analogy between the propagation of GNNs and particle systems in physics, proposing a model-agnostic enhancement framework. This framework enriches the graph structure by introducing additional nodes and rewiring connections with both positive and negative weights, guided by node labeling information. We theoretically verify that GNNs enhanced through our approach can effectively circumvent the over-smoothing issue and exhibit robustness against over-squashing. Moreover, we conduct a spectral analysis on the rewired graph to demonstrate that the corresponding GNNs can f
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09074</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20195;&#30721;&#27169;&#25311;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09074
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#30452;&#32447;&#31243;&#24207;&#65292;&#24182;&#23637;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#22788;&#29702;&#36825;&#26679;&#31616;&#21333;&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#36739;&#24046;&#8212;&#8212;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#27169;&#25311;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#24207;&#31639;&#27861;&#21644;&#23884;&#22871;&#24490;&#29615;&#36229;&#36234;&#20102;&#30452;&#32447;&#31243;&#24207;&#30340;&#27169;&#25311;&#65292;&#24182;&#23637;&#31034;&#20102;&#31243;&#24207;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30452;&#25509;&#24433;&#21709;LLMs&#27169;&#25311;&#20854;&#25191;&#34892;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#21482;&#26377;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#25165;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#12290;LLMs&#30340;&#20195;&#30721;&#27169;&#25311;&#19982;&#23427;&#20204;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#35760;&#24518;&#33021;&#21147;&#23384;&#22312;&#30683;&#30462;&#65306;&#22312;&#35760;&#24518;&#23545;&#20219;&#21153;&#26377;&#23475;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the extent to which Large Language Models (LLMs) can simulate the execution of computer code and algorithms. We begin by looking straight line programs, and show that current LLMs demonstrate poor performance even with such simple programs -- performance rapidly degrades with the length of code. We then investigate the ability of LLMs to simulate programs that contain critical paths and redundant instructions. We also go beyond straight line program simulation with sorting algorithms and nested loops, and we show the computational complexity of a routine directly affects the ability of an LLM to simulate its execution. We observe that LLMs execute instructions sequentially and with a low error margin only for short programs or standard procedures. LLMs' code simulation is in tension with their pattern recognition and memorisation capabilities: on tasks where memorisation is detrimental, we propose a novel prompting method to simulate code execution line by line. Empirica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#26469;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23398;&#20064;&#20844;&#20849;&#25968;&#25454;&#20013;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#21487;&#20197;&#22312;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#23454;&#29616;&#26368;&#20248;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#65292;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#21487;&#20197;&#28040;&#38500;&#31169;&#26377;&#21327;&#35843;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#32431;&#23616;&#37096;&#23398;&#20064;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.15551</link><description>&lt;p&gt;
&#21033;&#29992;&#20844;&#20849;&#34920;&#31034;&#26469;&#36827;&#34892;&#31169;&#26377;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Public Representations for Private Transfer Learning. (arXiv:2312.15551v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#26469;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23398;&#20064;&#20844;&#20849;&#25968;&#25454;&#20013;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#21487;&#20197;&#22312;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#23454;&#29616;&#26368;&#20248;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#65292;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#21487;&#20197;&#28040;&#38500;&#31169;&#26377;&#21327;&#35843;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#32431;&#23616;&#37096;&#23398;&#20064;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23558;&#20844;&#20849;&#25968;&#25454;&#32435;&#20837;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#30340;&#26368;&#26032;&#23454;&#35777;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#20174;&#20844;&#20849;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#20849;&#20139;&#34920;&#31034;&#22914;&#20309;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#32447;&#24615;&#22238;&#24402;&#30340;&#20004;&#31181;&#24120;&#35265;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#65292;&#20004;&#32773;&#37117;&#20551;&#35774;&#20844;&#20849;&#20219;&#21153;&#21644;&#31169;&#26377;&#20219;&#21153;&#65288;&#22238;&#24402;&#21521;&#37327;&#65289;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20849;&#20139;&#19968;&#20010;&#20302;&#31209;&#23376;&#31354;&#38388;&#12290;&#22312;&#31532;&#19968;&#31181;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#22312;&#25152;&#26377;&#29992;&#25143;&#20043;&#38388;&#20849;&#20139;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#27599;&#20010;&#29992;&#25143;&#23545;&#24212;&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#34892;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#20272;&#35745;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#30340;&#31639;&#27861;&#31867;&#20013;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#27169;&#22411;&#20010;&#24615;&#21270;&#30340;&#31532;&#20108;&#31181;&#24773;&#26223;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#26377;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#21487;&#20197;&#36991;&#20813;&#31169;&#26377;&#21327;&#35843;&#65292;&#22240;&#20026;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#20869;&#32431;&#31929;&#30340;&#23616;&#37096;&#23398;&#20064;&#21487;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the recent empirical success of incorporating public data into differentially private learning, we theoretically investigate how a shared representation learned from public data can improve private learning. We explore two common scenarios of transfer learning for linear regression, both of which assume the public and private tasks (regression vectors) share a low-rank subspace in a high-dimensional space. In the first single-task transfer scenario, the goal is to learn a single model shared across all users, each corresponding to a row in a dataset. We provide matching upper and lower bounds showing that our algorithm achieves the optimal excess risk within a natural class of algorithms that search for the linear model within the given subspace estimate. In the second scenario of multitask model personalization, we show that with sufficient public data, users can avoid private coordination, as purely local learning within the given subspace achieves the same utility. Take
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20256;&#25773;&#30340;&#31639;&#27861;&#26469;&#22788;&#29702;&#20219;&#24847;&#24322;&#36136;&#24615;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#65292;&#20197;&#32416;&#27491;&#22122;&#22768;&#26631;&#31614;&#24182;&#20026;&#26410;&#26631;&#35760;&#30340;&#33410;&#28857;&#20998;&#37197;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2310.16560</link><description>&lt;p&gt;
&#22270;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;&#24212;&#23545;&#22270;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Label Propagation for Graph Label Noise. (arXiv:2310.16560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20256;&#25773;&#30340;&#31639;&#27861;&#26469;&#22788;&#29702;&#20219;&#24847;&#24322;&#36136;&#24615;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#65292;&#20197;&#32416;&#27491;&#22122;&#22768;&#26631;&#31614;&#24182;&#20026;&#26410;&#26631;&#35760;&#30340;&#33410;&#28857;&#20998;&#37197;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#26159;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#24120;&#35265;&#30340;&#25361;&#25112;&#65292;&#23427;&#20250;&#26174;&#33879;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22122;&#22768;&#26631;&#31614;&#65292;&#28982;&#32780;&#65292;&#22270;&#27169;&#22411;&#23558;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#25299;&#25169;&#32467;&#26500;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#26356;&#23481;&#26131;&#21463;&#21040;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#36817;&#26399;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#31687;&#25991;&#31456;&#25552;&#20986;&#20102;&#35299;&#20915;&#22270;&#20013;&#26631;&#31614;&#22122;&#22768;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#23427;&#20204;&#20551;&#35774;&#22270;&#26159;&#21516;&#26500;&#30340;&#65292;&#24182;&#19988;&#26631;&#31614;&#26159;&#24179;&#28369;&#20998;&#24067;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#21487;&#33021;&#21253;&#21547;&#19981;&#21516;&#31243;&#24230;&#30340;&#24322;&#36136;&#24615;&#29978;&#33267;&#26159;&#24322;&#36136;&#24615;&#30340;&#20027;&#23548;&#65292;&#23548;&#33268;&#24403;&#21069;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;&#26412;&#25991;&#30740;&#31350;&#20219;&#24847;&#24322;&#36136;&#24615;&#26465;&#20214;&#19979;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#26088;&#22312;&#32416;&#27491;&#22122;&#22768;&#26631;&#31614;&#24182;&#20026;&#20043;&#21069;&#26410;&#26631;&#35760;&#30340;&#33410;&#28857;&#20998;&#37197;&#26631;&#31614;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#20004;&#20010;&#23454;&#35777;&#20998;&#26512;&#65292;&#25506;&#35752;&#22270;&#21516;&#36136;&#24615;&#23545;&#22270;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20256;&#25773;&#30340;&#31639;&#27861;&#26469;&#22788;&#29702;&#20219;&#24847;&#24322;&#36136;&#24615;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label noise is a common challenge in large datasets, as it can significantly degrade the generalization ability of deep neural networks. Most existing studies focus on noisy labels in computer vision; however, graph models encompass both node features and graph topology as input, and become more susceptible to label noise through message-passing mechanisms. Recently, only a few works have been proposed to tackle the label noise on graphs. One major limitation is that they assume the graph is homophilous and the labels are smoothly distributed. Nevertheless, real-world graphs may contain varying degrees of heterophily or even be heterophily-dominated, leading to the inadequacy of current methods. In this paper, we study graph label noise in the context of arbitrary heterophily, with the aim of rectifying noisy labels and assigning labels to previously unlabeled nodes. We begin by conducting two empirical analyses to explore the impact of graph homophily on graph label noise. Following o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#21018;&#24615;&#25991;&#26412;&#32534;&#36753;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20445;&#25345;&#36755;&#20837;&#38899;&#39057;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.12858</link><description>&lt;p&gt;
&#38750;&#21018;&#24615;&#25991;&#26412;&#25552;&#31034;&#30340;&#38899;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Audio Editing with Non-Rigid Text Prompts. (arXiv:2310.12858v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#21018;&#24615;&#25991;&#26412;&#32534;&#36753;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20445;&#25345;&#36755;&#20837;&#38899;&#39057;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#38750;&#21018;&#24615;&#25991;&#26412;&#32534;&#36753;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#32534;&#36753;&#27969;&#31243;&#33021;&#22815;&#21019;&#24314;&#19982;&#36755;&#20837;&#38899;&#39057;&#20445;&#25345;&#19968;&#33268;&#30340;&#38899;&#39057;&#32534;&#36753;&#32467;&#26524;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#33021;&#22815;&#36827;&#34892;&#28155;&#21152;&#12289;&#39118;&#26684;&#36716;&#25442;&#21644;&#20462;&#22797;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35777;&#26126;&#20102;&#36825;&#20123;&#32534;&#36753;&#33021;&#22815;&#20248;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#25991;&#26412;&#25552;&#31034;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;Audio-LDM&#30340;&#32467;&#26524;&#12290;&#23545;&#32467;&#26524;&#30340;&#23450;&#24615;&#26816;&#26597;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32473;&#20986;&#20102;&#26356;&#21152;&#20445;&#25345;&#36755;&#20837;&#38899;&#39057;&#21407;&#22987;&#36215;&#22987;&#21644;&#32467;&#26463;&#30340;&#32534;&#36753;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore audio-editing with non-rigid text edits. We show that the proposed editing pipeline is able to create audio edits that remain faithful to the input audio. We explore text prompts that perform addition, style transfer, and in-painting. We quantitatively and qualitatively show that the edits are able to obtain results which outperform Audio-LDM, a recently released text-prompted audio generation model. Qualitative inspection of the results points out that the edits given by our approach remain more faithful to the input audio in terms of keeping the original onsets and offsets of the audio events.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#19982;&#25191;&#34892;&#22120;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#25512;&#29702;&#12290;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20855;&#26377;&#29420;&#29305;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#24212;&#29992;&#20869;&#23884;&#30340;&#19990;&#30028;&#30693;&#35782;&#35299;&#37322;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#36827;&#34892;&#29289;&#29702;&#39046;&#22495;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.09605</link><description>&lt;p&gt;
&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#20351;LLMs&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Penetrative AI: Making LLMs Comprehend the Physical World. (arXiv:2310.09605v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#19982;&#25191;&#34892;&#22120;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#25512;&#29702;&#12290;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20855;&#26377;&#29420;&#29305;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#24212;&#29992;&#20869;&#23884;&#30340;&#19990;&#30028;&#30693;&#35782;&#35299;&#37322;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#36827;&#34892;&#29289;&#29702;&#39046;&#22495;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#30340;&#24615;&#36136;&#20197;&#21450;&#23427;&#20204;&#22312;&#28041;&#21450;&#30495;&#23454;&#29289;&#29702;&#19990;&#30028;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#25972;&#21512;&#24120;&#35782;&#20154;&#31867;&#30693;&#35782;&#30340;&#28508;&#21147;&#20173;&#23384;&#22312;&#30097;&#38382;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;LLMs&#22914;&#20309;&#36890;&#36807;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#25512;&#29702;&#26469;&#25506;&#35752;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#19968;&#27010;&#24565;&#31216;&#20026;&#8220;&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#8221;&#12290;&#35770;&#25991;&#22312;LLMs&#33021;&#22815;&#36879;&#36807;&#22788;&#29702;&#24863;&#30693;&#20449;&#21495;&#30340;&#20004;&#20010;&#23618;&#38754;&#19978;&#25506;&#32034;&#20102;&#36825;&#31181;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#65288;ChatGPT&#26159;&#25105;&#20204;&#30740;&#31350;&#20013;&#30340;&#20195;&#34920;&#24615;&#20363;&#23376;&#65289;&#22312;&#24212;&#29992;&#20869;&#23884;&#30340;&#19990;&#30028;&#30693;&#35782;&#35299;&#37322;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#23545;&#29289;&#29702;&#39046;&#22495;&#30340;&#20219;&#21153;&#36827;&#34892;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#30456;&#24403;&#29420;&#29305;&#30340;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#20026;LLMs&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in Large Language Models (LLMs) have demonstrated their remarkable capabilities across a range of tasks. Questions, however, persist about the nature of LLMs and their potential to integrate common-sense human knowledge when performing tasks involving information about the real physical world. This paper delves into these questions by exploring how LLMs can be extended to interact with and reason about the physical world through IoT sensors and actuators, a concept that we term "Penetrative AI". The paper explores such an extension at two levels of LLMs' ability to penetrate into the physical world via the processing of sensory signals. Our preliminary findings indicate that LLMs, with ChatGPT being the representative example in our exploration, have considerable and unique proficiency in employing the embedded world knowledge for interpreting IoT sensor data and reasoning over them about tasks in the physical realm. Not only this opens up new applications for LLMs 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#27491;&#20132;&#30697;&#38453;&#38598;&#21512;&#21021;&#22987;&#21270;&#26435;&#37325;&#24182;&#20351;&#29992;tanh&#28608;&#27963;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20840;&#36830;&#25509;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#20013;&#20855;&#26377;&#19982;&#28145;&#24230;&#26080;&#20851;&#30340;&#32447;&#24615;&#27874;&#21160;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21450;&#20854;&#21518;&#20195;&#30340;&#25152;&#26377;&#30456;&#20851;&#20989;&#25968;&#22312;&#36870;&#23485;&#24230;&#30340;&#20027;&#23548;&#38454;&#27573;&#22312;&#28145;&#24230;&#32422;&#20026;20&#30340;&#20301;&#32622;&#39281;&#21644;&#65292;&#32780;&#19981;&#26159;&#19981;&#26029;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2310.07765</link><description>&lt;p&gt;
&#20855;&#26377;&#27491;&#20132;&#26435;&#37325;&#30340;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#19982;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Feature Learning and Generalization in Deep Networks with Orthogonal Weights. (arXiv:2310.07765v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07765
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#27491;&#20132;&#30697;&#38453;&#38598;&#21512;&#21021;&#22987;&#21270;&#26435;&#37325;&#24182;&#20351;&#29992;tanh&#28608;&#27963;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20840;&#36830;&#25509;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#20013;&#20855;&#26377;&#19982;&#28145;&#24230;&#26080;&#20851;&#30340;&#32447;&#24615;&#27874;&#21160;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21450;&#20854;&#21518;&#20195;&#30340;&#25152;&#26377;&#30456;&#20851;&#20989;&#25968;&#22312;&#36870;&#23485;&#24230;&#30340;&#20027;&#23548;&#38454;&#27573;&#22312;&#28145;&#24230;&#32422;&#20026;20&#30340;&#20301;&#32622;&#39281;&#21644;&#65292;&#32780;&#19981;&#26159;&#19981;&#26029;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20174;&#27491;&#20132;&#30697;&#38453;&#38598;&#21512;&#21021;&#22987;&#21270;&#30340;&#26435;&#37325;&#21644;tanh&#28608;&#27963;&#20989;&#25968;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20840;&#36830;&#25509;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#26102;&#20855;&#26377;&#19982;&#23485;&#24230;&#26080;&#20851;&#30340;&#21069;&#28608;&#27963;&#27874;&#21160;&#65292;&#36825;&#26159;&#36890;&#36807;&#35745;&#31639;&#35777;&#26126;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#28041;&#21450;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21450;&#20854;&#21518;&#20195;&#30340;&#25152;&#26377;&#30456;&#20851;&#20989;&#25968;&#22312;&#36870;&#23485;&#24230;&#30340;&#20027;&#23548;&#38454;&#27573;&#39281;&#21644;&#22312;&#28145;&#24230;&#32422;&#20026;20&#30340;&#20301;&#32622;&#65292;&#32780;&#19981;&#26159;&#20687;&#39640;&#26031;&#21021;&#22987;&#21270;&#30340;&#24773;&#20917;&#37027;&#26679;&#19981;&#26029;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully-connected deep neural networks with weights initialized from independent Gaussian distributions can be tuned to criticality, which prevents the exponential growth or decay of signals propagating through the network. However, such networks still exhibit fluctuations that grow linearly with the depth of the network, which may impair the training of networks with width comparable to depth. We show analytically that rectangular networks with tanh activations and weights initialized from the ensemble of orthogonal matrices have corresponding preactivation fluctuations which are independent of depth, to leading order in inverse width. Moreover, we demonstrate numerically that, at initialization, all correlators involving the neural tangent kernel (NTK) and its descendants at leading order in inverse width -- which govern the evolution of observables during training -- saturate at a depth of $\sim 20$, rather than growing without bound as in the case of Gaussian initializations. We spec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.01770</link><description>&lt;p&gt;
&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#21387;&#32553;&#34920;&#31034;&#30340;&#31616;&#21333;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#30740;&#31350;&#30340;&#26041;&#27861;&#26377;&#24456;&#22810;&#31181;&#65292;&#21253;&#25324;&#33267;&#23569;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#31354;&#38388;&#20013;&#25439;&#22833;&#26223;&#35266;&#30340;&#24418;&#29366;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#20013;&#34920;&#31034;&#27969;&#24418;&#30340;&#32467;&#26500;&#65288;&#21363;&#21333;&#20301;&#27963;&#21160;&#30340;&#31354;&#38388;&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#20851;&#20294;&#24456;&#23569;&#21516;&#26102;&#36827;&#34892;&#30740;&#31350;&#21644;&#26126;&#30830;&#20851;&#32852;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#26512;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#32852;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#31070;&#32463;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#19982;&#27491;&#22312;&#36827;&#34892;&#30340;&#21442;&#25968;&#20248;&#21270;&#25152;&#25506;&#32034;&#30340;&#26368;&#23567;&#20540;&#21608;&#22260;&#30340;&#25439;&#22833;&#24179;&#22374;&#24615;&#30456;&#20851;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#21487;&#20197;&#30001;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#26469;&#39044;&#27979;&#65306;&#25439;&#22833;&#24179;&#22374;&#24615;&#24847;&#21619;&#30528;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;\citet{ma_linear_2021}&#30340;&#20808;&#21069;&#30740;&#31350;&#23494;&#20999;&#30456;&#20851;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#24179;&#22374;&#24615;&#65288;&#21363;&#23567;&#29305;&#24449;&#20540;&#65289;&#19982;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#24314;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21463;&#20010;&#20307;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.15375</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#24314;&#27169;&#65292;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PPG to ECG Signal Translation for Continuous Atrial Fibrillation Detection via Attention-based Deep State-Space Modeling. (arXiv:2309.15375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15375
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#24314;&#27169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21463;&#20010;&#20307;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#29992;&#20110;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#20449;&#21495;&#22270;&#65288;ECG&#25110;EKG&#65289;&#26159;&#19968;&#31181;&#27979;&#37327;&#24515;&#33039;&#30005;&#27963;&#21160;&#30340;&#21307;&#23398;&#27979;&#35797;&#12290;ECG&#24120;&#29992;&#20110;&#35786;&#26029;&#21644;&#30417;&#27979;&#21508;&#31181;&#24515;&#33039;&#30142;&#30149;&#65292;&#21253;&#25324;&#24515;&#24459;&#22833;&#24120;&#12289;&#24515;&#32908;&#26775;&#22622;&#21644;&#24515;&#21147;&#34928;&#31469;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ECG&#38656;&#35201;&#20020;&#24202;&#27979;&#37327;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#21307;&#30103;&#26426;&#26500;&#30340;&#24212;&#29992;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21333;&#23548;&#32852;ECG&#24050;&#32463;&#22312;&#20329;&#25140;&#24335;&#35774;&#22791;&#19978;&#24212;&#29992;&#24191;&#27867;&#12290;&#21478;&#19968;&#31181;ECG&#30340;&#26367;&#20195;&#26041;&#27861;&#26159;&#20809;&#27978;&#24230;&#33033;&#25615;&#26816;&#27979;&#65288;PPG&#65289;&#65292;&#23427;&#37319;&#29992;&#38750;&#20405;&#20837;&#24615;&#12289;&#20302;&#25104;&#26412;&#30340;&#20809;&#23398;&#26041;&#27861;&#26469;&#27979;&#37327;&#24515;&#33039;&#29983;&#29702;&#23398;&#65292;&#20351;&#20854;&#25104;&#20026;&#25429;&#25417;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#24515;&#33039;&#20449;&#21495;&#30340;&#21512;&#36866;&#36873;&#25321;&#12290;&#34429;&#28982;ECG&#21644;PPG&#20043;&#38388;&#20855;&#26377;&#24378;&#28872;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#21518;&#32773;&#24182;&#27809;&#26377;&#25552;&#20379;&#26126;&#26174;&#30340;&#20020;&#24202;&#35786;&#26029;&#20215;&#20540;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21463;&#20010;&#20307;&#38480;&#21046;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;PPG&#20449;&#21495;&#36716;&#25442;&#20026;ECG&#65292;&#20174;&#32780;&#23454;&#29616;&#36830;&#32493;&#24615;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
An electrocardiogram (ECG or EKG) is a medical test that measures the heart's electrical activity. ECGs are often used to diagnose and monitor a wide range of heart conditions, including arrhythmias, heart attacks, and heart failure. On the one hand, the conventional ECG requires clinical measurement, which restricts its deployment to medical facilities. On the other hand, single-lead ECG has become popular on wearable devices using administered procedures. An alternative to ECG is Photoplethysmography (PPG), which uses non-invasive, low-cost optical methods to measure cardiac physiology, making it a suitable option for capturing vital heart signs in daily life. As a result, it has become increasingly popular in health monitoring and is used in various clinical and commercial wearable devices. While ECG and PPG correlate strongly, the latter does not offer significant clinical diagnostic value. Here, we propose a subject-independent attention-based deep state-space model to translate P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14348</link><description>&lt;p&gt;
&#36890;&#36807;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#25269;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM. (arXiv:2309.14348v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;LLMs&#21487;&#33021;&#34987;&#28389;&#29992;&#26469;&#29983;&#25104;&#26377;&#23475;&#25110;&#24694;&#24847;&#20869;&#23481;&#12290;&#23613;&#31649;&#26377;&#19968;&#31995;&#21015;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#40784;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#24182;&#38450;&#27490;&#23427;&#20204;&#29983;&#25104;&#19981;&#36866;&#24403;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#23545;&#40784;&#36890;&#24120;&#26159;&#33030;&#24369;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#20248;&#21270;&#25110;&#25163;&#24037;&#26500;&#24314;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#20197;&#38450;&#33539;&#28508;&#22312;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#26500;&#24314;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#65292;&#36890;&#36807;&#20855;&#26377;&#31283;&#20581;&#23545;&#40784;&#26816;&#26597;&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#23545;&#21407;&#22987;LLM&#36827;&#34892;&#20219;&#20309;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;RA-LLM&#22312;&#38450;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#29305;&#24615;&#20197;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#30340;&#20027;&#35201;&#35745;&#31639;&#22256;&#38590;&#65292;&#24182;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#35270;&#20026;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#32780;&#38750;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12013</link><description>&lt;p&gt;
&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Quantum-Noise-driven Generative Diffusion Models. (arXiv:2308.12013v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#29305;&#24615;&#20197;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#30340;&#20027;&#35201;&#35745;&#31639;&#22256;&#38590;&#65292;&#24182;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#35270;&#20026;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#32780;&#38750;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#20174;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#26029;&#20986;&#22797;&#26434;&#21644;&#26410;&#30693;&#25968;&#25454;&#20998;&#24067;&#24182;&#20135;&#29983;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26694;&#26550;&#65292;&#26368;&#36817;&#22312;&#21019;&#24314;&#21512;&#25104;&#25991;&#26412;&#21644;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#24050;&#32463;&#36229;&#36234;&#20102;&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#37327;&#23376;&#25512;generalization&#65292;&#21363;&#19977;&#31181;&#21487;&#33021;&#22312;&#23454;&#38469;&#37327;&#23376;&#31995;&#32479;&#19978;&#36827;&#34892;&#23454;&#39564;&#30340;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#21033;&#29992;&#29420;&#29305;&#30340;&#37327;&#23376;&#29305;&#24615;&#65292;&#29305;&#21035;&#26159;&#30446;&#21069;&#21487;&#29992;&#30340;&#26377;&#22122;&#22768;&#37327;&#23376;&#22788;&#29702;&#22120;&#19981;&#21487;&#36991;&#20813;&#22320;&#21463;&#21040;&#30340;&#30456;&#24178;&#24615;&#12289;&#32416;&#32544;&#24615;&#21644;&#22122;&#22768;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#20027;&#35201;&#35745;&#31639;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#19981;&#20316;&#20026;&#38656;&#35201;&#26816;&#27979;&#21644;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#32780;&#26159;&#20316;&#20026;&#19968;&#31181;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#65292;&#20351;&#24471;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a ver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;transformer&#26469;&#22312;&#26410;&#30693;&#31995;&#32479;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21629;&#21517;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#65292;&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2308.08536</link><description>&lt;p&gt;
Transformers&#33021;&#21542;&#23398;&#20064;&#29992;&#20110;&#26410;&#30693;&#31995;&#32479;&#30340;&#26368;&#20248;&#28388;&#27874;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Transformers Learn Optimal Filtering for Unknown Systems?. (arXiv:2308.08536v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;transformer&#26469;&#22312;&#26410;&#30693;&#31995;&#32479;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21629;&#21517;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#65292;&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#23427;&#20351;&#29992;&#36807;&#21435;&#30340;&#25152;&#26377;&#36755;&#20986;&#26469;&#29983;&#25104;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#20808;&#39564;&#20998;&#24067;&#30340;&#21508;&#31181;&#31995;&#32479;&#26469;&#35757;&#32451;transformer&#65292;&#28982;&#21518;&#22312;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#30456;&#21516;&#20998;&#24067;&#30340;&#31995;&#32479;&#19978;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#33719;&#24471;&#30340;transformer&#23601;&#20687;&#19968;&#20010;&#39044;&#27979;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#24182;&#24555;&#36895;&#36866;&#24212;&#21644;&#39044;&#27979;&#19981;&#21516;&#30340;&#31995;&#32479;&#65292;&#22240;&#27492;&#25105;&#20204;&#31216;&#20043;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;MOP&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated remarkable success in natural language processing; however, their potential remains mostly unexplored for problems arising in dynamical systems. In this work, we investigate the optimal output estimation problem using transformers, which generate output predictions using all the past ones. We train the transformer using various systems drawn from a prior distribution and then evaluate its performance on previously unseen systems from the same distribution. As a result, the obtained transformer acts like a prediction algorithm that learns in-context and quickly adapts to and predicts well for different systems - thus we call it meta-output-predictor (MOP). MOP matches the performance of the optimal output estimator, based on Kalman filter, for most linear dynamical systems even though it does not have access to a model. We observe via extensive numerical experiments that MOP also performs well in challenging scenarios with non-i.i.d. noise, time-varying dy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25209;&#21028;&#24615;&#23457;&#35270;&#20102;(Normalised) Discounted Cumulative Gain&#20316;&#20026;Top-n&#25512;&#33616;&#31163;&#32447;&#35780;&#20272;&#25351;&#26631;&#30340;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#20309;&#26102;&#21487;&#20197;&#26399;&#26395;&#36825;&#20123;&#25351;&#26631;&#36924;&#36817;&#22312;&#32447;&#23454;&#39564;&#30340;&#37329;&#26631;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.15053</link><description>&lt;p&gt;
&#20851;&#20110;(Normalised) Discounted Cumulative Gain&#20316;&#20026;Top-n&#25512;&#33616;&#30340;&#31163;&#32447;&#35780;&#20272;&#25351;&#26631;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
On (Normalised) Discounted Cumulative Gain as an Offline Evaluation Metric for Top-$n$ Recommendation. (arXiv:2307.15053v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25209;&#21028;&#24615;&#23457;&#35270;&#20102;(Normalised) Discounted Cumulative Gain&#20316;&#20026;Top-n&#25512;&#33616;&#31163;&#32447;&#35780;&#20272;&#25351;&#26631;&#30340;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#20309;&#26102;&#21487;&#20197;&#26399;&#26395;&#36825;&#20123;&#25351;&#26631;&#36924;&#36817;&#22312;&#32447;&#23454;&#39564;&#30340;&#37329;&#26631;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65306;(1) &#36890;&#36807;(&#27169;&#25311;)&#22312;&#32447;&#23454;&#39564;&#65292;&#36890;&#24120;&#34987;&#35270;&#20026;&#37329;&#26631;&#20934;&#65292;&#25110;&#32773;(2) &#36890;&#36807;&#19968;&#20123;&#31163;&#32447;&#35780;&#20272;&#31243;&#24207;&#65292;&#30446;&#26631;&#26159;&#36817;&#20284;&#22312;&#32447;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;&#25991;&#29486;&#20013;&#37319;&#29992;&#20102;&#20960;&#31181;&#31163;&#32447;&#35780;&#20272;&#25351;&#26631;&#65292;&#21463;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#20013;&#24120;&#35265;&#30340;&#25490;&#21517;&#25351;&#26631;&#30340;&#21551;&#21457;&#12290;(Normalised) Discounted Cumulative Gain (nDCG)&#26159;&#20854;&#20013;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22312;&#24456;&#22810;&#24180;&#37324;&#65292;&#26356;&#39640;&#30340;(n)DCG&#20540;&#34987;&#29992;&#26469;&#23637;&#31034;&#26032;&#26041;&#27861;&#22312;Top-n&#25512;&#33616;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#23457;&#35270;&#65292;&#24182;&#30740;&#31350;&#20102;&#25105;&#20204;&#20309;&#26102;&#21487;&#20197;&#26399;&#26395;&#36825;&#20123;&#25351;&#26631;&#36924;&#36817;&#22312;&#32447;&#23454;&#39564;&#30340;&#37329;&#26631;&#20934;&#32467;&#26524;&#12290;&#25105;&#20204;&#20174;&#31532;&#19968;&#21407;&#29702;&#19978;&#27491;&#24335;&#25552;&#20986;&#20102;DCG&#34987;&#35748;&#20026;&#26159;&#22312;&#32447;&#22870;&#21169;&#30340;&#26080;&#20559;&#20272;&#35745;&#30340;&#20551;&#35774;&#65292;&#24182;&#32473;&#20986;&#20102;&#36825;&#20010;&#25351;&#26631;&#30340;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approaches to recommendation are typically evaluated in one of two ways: (1) via a (simulated) online experiment, often seen as the gold standard, or (2) via some offline evaluation procedure, where the goal is to approximate the outcome of an online experiment. Several offline evaluation metrics have been adopted in the literature, inspired by ranking metrics prevalent in the field of Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one such metric that has seen widespread adoption in empirical studies, and higher (n)DCG values have been used to present new methods as the state-of-the-art in top-$n$ recommendation for many years.  Our work takes a critical look at this approach, and investigates when we can expect such metrics to approximate the gold standard outcome of an online experiment. We formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward and provide a derivation for this metric from first principles
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#31995;&#32479;&#22312;&#24050;&#30693;&#24378;&#36843;&#20989;&#25968;&#24433;&#21709;&#19979;&#30340;&#35266;&#27979;&#65292;&#35782;&#21035;&#21644;&#25233;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#36825;&#39033;&#26041;&#27861;&#23545;&#35757;&#32451;&#20989;&#25968;&#26377;&#38750;&#24120;&#28201;&#21644;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#22823;&#31867;&#21035;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2307.03690</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25233;&#21046;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26410;&#30693;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Suppressing unknown disturbances to dynamical systems using machine learning. (arXiv:2307.03690v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#31995;&#32479;&#22312;&#24050;&#30693;&#24378;&#36843;&#20989;&#25968;&#24433;&#21709;&#19979;&#30340;&#35266;&#27979;&#65292;&#35782;&#21035;&#21644;&#25233;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#36825;&#39033;&#26041;&#27861;&#23545;&#35757;&#32451;&#20989;&#25968;&#26377;&#38750;&#24120;&#28201;&#21644;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#22823;&#31867;&#21035;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#25233;&#21046;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26410;&#30693;&#24178;&#25200;&#26159;&#19968;&#20010;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20165;&#22522;&#20110;&#31995;&#32479;&#22312;&#24050;&#30693;&#24378;&#36843;&#20989;&#25968;&#24433;&#21709;&#19979;&#30340;&#20808;&#21069;&#35266;&#27979;&#26469;&#35782;&#21035;&#21644;&#25233;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#35757;&#32451;&#20989;&#25968;&#26377;&#38750;&#24120;&#28201;&#21644;&#30340;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#31283;&#20581;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#22823;&#31867;&#21035;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#31034;&#20363;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#26696;&#65292;&#20854;&#20013;&#35782;&#21035;&#21644;&#25233;&#21046;&#20102; Lorenz &#31995;&#32479;&#30340;&#28151;&#27788;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying and suppressing unknown disturbances to dynamical systems is a problem with applications in many different fields. In this Letter, we present a model-free method to identify and suppress an unknown disturbance to an unknown system based only on previous observations of the system under the influence of a known forcing function. We find that, under very mild restrictions on the training function, our method is able to robustly identify and suppress a large class of unknown disturbances. We illustrate our scheme with an example where a chaotic disturbance to the Lorenz system is identified and suppressed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Informed POMDP&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#27169;&#22411;&#26469;&#21033;&#29992;&#35757;&#32451;&#26102;&#21487;&#29992;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;Dreamer&#31639;&#27861;&#31574;&#30053;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.11488</link><description>&lt;p&gt;
&#36879;&#35270;&#39069;&#22806;&#20449;&#24687;&#30340;Informed POMDP: &#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Informed POMDP: Leveraging Additional Information in Model-Based RL. (arXiv:2306.11488v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Informed POMDP&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#27169;&#22411;&#26469;&#21033;&#29992;&#35757;&#32451;&#26102;&#21487;&#29992;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;Dreamer&#31639;&#27861;&#31574;&#30053;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32771;&#34385;&#35757;&#32451;&#26102;&#21487;&#29992;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#25512;&#24191;&#20102;POMDP&#20013;&#30340;&#20132;&#20114;&#23398;&#20064;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Informed POMDP&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#28165;&#26224;&#21306;&#20998;&#20102;&#35757;&#32451;&#20449;&#24687;&#21644;&#25191;&#34892;&#35266;&#23519;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30446;&#26631;&#65292;&#29992;&#20110;&#20174;&#21382;&#21490;&#35760;&#24405;&#20013;&#23398;&#20064;&#20986;&#20805;&#20998;&#32479;&#35745;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#26368;&#20248;&#25511;&#21046;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;Informed&#30446;&#26631;&#30001;&#23398;&#20064;&#19968;&#20010;&#29615;&#22659;&#27169;&#22411;&#32452;&#25104;&#65292;&#20174;&#20854;&#20013;&#25105;&#20204;&#21487;&#20197;&#37319;&#26679;&#38544;&#24335;&#36712;&#36857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#20960;&#20010;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#36825;&#20010;Informed&#29615;&#22659;&#27169;&#22411;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;Dreamer&#31639;&#27861;&#31574;&#30053;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#20123;&#32467;&#26524;&#20197;&#21450;&#25552;&#35758;&#30340;&#31616;&#21333;&#36866;&#24212;&#24615;&#65292;&#20513;&#23548;&#22312;&#20351;&#29992;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;POMDP&#26102;&#65292;&#35201;&#31995;&#32479;&#22320;&#32771;&#34385;&#21487;&#29992;&#30340;&#39069;&#22806;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we generalize the problem of learning through interaction in a POMDP by accounting for eventual additional information available at training time. First, we introduce the informed POMDP, a new learning paradigm offering a clear distinction between the training information and the execution observation. Next, we propose an objective for learning a sufficient statistic from the history for the optimal control that leverages this information. We then show that this informed objective consists of learning an environment model from which we can sample latent trajectories. Finally, we show for the Dreamer algorithm that the convergence speed of the policies is sometimes greatly improved on several environments by using this informed environment model. Those results and the simplicity of the proposed adaptation advocate for a systematic consideration of eventual additional information when learning in a POMDP using model-based RL.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#25512;&#24191;&#21040;&#25351;&#25968;&#26063;&#22122;&#22768;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#20449;&#21495;&#20013;&#20272;&#35745;&#22270;&#25289;&#26222;&#25289;&#26031;&#21644;&#26410;&#35266;&#27979;&#30340;&#24179;&#28369;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#22122;&#22768;&#27169;&#22411;&#19981;&#21305;&#37197;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#25289;&#26222;&#25289;&#26031;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08201</link><description>&lt;p&gt;
&#25351;&#25968;&#26063;&#22122;&#22768;&#19979;&#30340;&#22270;&#25289;&#26222;&#25289;&#26031;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Laplacian Learning with Exponential Family Noise. (arXiv:2306.08201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08201
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#25512;&#24191;&#21040;&#25351;&#25968;&#26063;&#22122;&#22768;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#20449;&#21495;&#20013;&#20272;&#35745;&#22270;&#25289;&#26222;&#25289;&#26031;&#21644;&#26410;&#35266;&#27979;&#30340;&#24179;&#28369;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#22122;&#22768;&#27169;&#22411;&#19981;&#21305;&#37197;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#25289;&#26222;&#25289;&#26031;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26102;&#65292;&#38754;&#20020;&#30340;&#24120;&#35265;&#25361;&#25112;&#26159;&#31995;&#32479;&#30340;&#24213;&#23618;&#22270;&#24448;&#24448;&#26159;&#26410;&#30693;&#30340;&#12290;&#34429;&#28982;&#20026;&#36830;&#32493;&#30340;&#22270;&#20449;&#21495;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#22270;&#25512;&#26029;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#65288;&#20363;&#22914;&#31163;&#25955;&#35745;&#25968;&#65289;&#65292;&#25512;&#26029;&#20854;&#24213;&#23618;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#23558;&#22270;&#20449;&#21495;&#22788;&#29702;&#65288;GSP&#65289;&#26694;&#26550;&#25512;&#24191;&#21040;&#25351;&#25968;&#26063;&#22122;&#22768;&#20998;&#24067;&#65292;&#20197;&#24314;&#27169;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#22270;&#23398;&#20064;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#31639;&#27861;&#65292;&#20174;&#22122;&#22768;&#20449;&#21495;&#20013;&#20272;&#35745;&#22270;&#25289;&#26222;&#25289;&#26031;&#21644;&#26410;&#35266;&#27979;&#30340;&#24179;&#28369;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26032;&#31639;&#27861;&#22312;&#22122;&#22768;&#27169;&#22411;&#19981;&#21305;&#37197;&#24773;&#20917;&#19979;&#20248;&#20110;&#31454;&#20105;&#25289;&#26222;&#25289;&#26031;&#20272;&#35745;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common challenge in applying graph machine learning methods is that the underlying graph of a system is often unknown. Although different graph inference methods have been proposed for continuous graph signals, inferring the graph structure underlying other types of data, such as discrete counts, is under-explored. In this paper, we generalize a graph signal processing (GSP) framework for learning a graph from smooth graph signals to the exponential family noise distribution to model various data types. We propose an alternating algorithm that estimates the graph Laplacian as well as the unobserved smooth representation from the noisy signals. We demonstrate in synthetic and real-world data that our new algorithm outperforms competing Laplacian estimation methods under noise model mismatch.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#20855;&#26377;&#33258;&#36866;&#24212;&#31232;&#30095;&#24230;&#32423;&#21035;&#30340;&#20462;&#21098;&#8221;(PALS), &#36890;&#36807;&#31232;&#30095;&#35757;&#32451;&#21644;&#35757;&#32451;&#26399;&#38388;&#26041;&#27861;&#20013;&#30340;&#8220;&#25193;&#24352;&#8221;&#26426;&#21046;&#65292;&#22312;Transformer&#27169;&#22411;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.18382</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#31232;&#30095;&#24230;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21464;&#21270;&#65292;&#29992;&#20110;&#21033;&#29992;Transformer&#36827;&#34892;&#39640;&#25928;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Sparsity Level during Training for Efficient Time Series Forecasting with Transformers. (arXiv:2305.18382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#20855;&#26377;&#33258;&#36866;&#24212;&#31232;&#30095;&#24230;&#32423;&#21035;&#30340;&#20462;&#21098;&#8221;(PALS), &#36890;&#36807;&#31232;&#30095;&#35757;&#32451;&#21644;&#35757;&#32451;&#26399;&#38388;&#26041;&#27861;&#20013;&#30340;&#8220;&#25193;&#24352;&#8221;&#26426;&#21046;&#65292;&#22312;Transformer&#27169;&#22411;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#31232;&#30095;&#36830;&#25509;&#21644;&#20943;&#23567;&#27169;&#22411;&#23610;&#23544;&#65292;&#21487;&#20197;&#23454;&#29616;DNN&#30340;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#21160;&#30830;&#23450;&#31232;&#30095;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#25439;&#22833;&#31232;&#30095;&#24230;&#26435;&#34913;&#26159;&#24322;&#26500;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#20855;&#26377;&#33258;&#36866;&#24212;&#31232;&#30095;&#24230;&#32423;&#21035;&#30340;&#20462;&#21098;&#8221;(PALS)&#65292;&#26469;&#33258;&#21160;&#23547;&#27714;&#25439;&#22833;&#21644;&#31232;&#30095;&#24615;&#20043;&#38388;&#30340;&#26368;&#20339;&#24179;&#34913;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#31232;&#30095;&#27700;&#24179;&#12290;PALS&#20174;&#31232;&#30095;&#35757;&#32451;&#21644;&#35757;&#32451;&#26399;&#38388;&#26041;&#27861;&#20013;&#21560;&#21462;&#28789;&#24863;&#12290;&#23427;&#22312;&#35757;&#32451;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#8220;&#25193;&#24352;&#8221;&#26426;&#21046;&#65292;&#20801;&#35768;&#27169;&#22411;&#21160;&#24577;&#25910;&#32553;&#12289;&#25193;&#24352;&#25110;&#20445;&#25345;&#31283;&#23450;&#65292;&#20197;&#25214;&#21040;&#36866;&#24403;&#30340;&#31232;&#30095;&#24230;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#22312;&#20197;Transformer&#33879;&#31216;&#30340;&#27169;&#22411;&#20013;&#23454;&#29616;&#25928;&#29575;, &#35813;&#27169;&#22411;&#20197;&#20854;&#20986;&#33394;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#34920;&#29616;&#32780;&#38395;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient time series forecasting has become critical for real-world applications, particularly with deep neural networks (DNNs). Efficiency in DNNs can be achieved through sparse connectivity and reducing the model size. However, finding the sparsity level automatically during training remains a challenging task due to the heterogeneity in the loss-sparsity tradeoffs across the datasets. In this paper, we propose \enquote{\textbf{P}runing with \textbf{A}daptive \textbf{S}parsity \textbf{L}evel} (\textbf{PALS}), to automatically seek an optimal balance between loss and sparsity, all without the need for a predefined sparsity level. PALS draws inspiration from both sparse training and during-training methods. It introduces the novel "expand" mechanism in training sparse neural networks, allowing the model to dynamically shrink, expand, or remain stable to find a proper sparsity level. In this paper, we focus on achieving efficiency in transformers known for their excellent time series f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#39046;&#22495;&#27867;&#21270;&#65288;CDG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#28872;&#23545;&#27604;&#30340;&#25968;&#25454;&#23545;&#25152;&#23637;&#31034;&#30340;&#35821;&#20041;&#19981;&#21464;&#24615;&#36827;&#34892;&#21033;&#29992;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#8212;&#8212;Logit Attribution Matching (LAM)&#65292;&#20197;&#23454;&#29616;CDG&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LAM&#20165;&#20351;&#29992;&#23569;&#37327;&#37197;&#23545;&#25968;&#25454;&#23601;&#33021;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;DG&#26041;&#27861;&#65292;&#19988;&#26377;&#21161;&#20110;&#27169;&#22411;&#26356;&#22909;&#22320;&#20851;&#27880;&#23545;&#39046;&#22495;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.07888</link><description>&lt;p&gt;
&#36890;&#36807;Logit Attribution&#21305;&#37197;&#23454;&#29616;&#23545;&#27604;&#24230;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Contrastive Domain Generalization via Logit Attribution Matching. (arXiv:2305.07888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#39046;&#22495;&#27867;&#21270;&#65288;CDG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#28872;&#23545;&#27604;&#30340;&#25968;&#25454;&#23545;&#25152;&#23637;&#31034;&#30340;&#35821;&#20041;&#19981;&#21464;&#24615;&#36827;&#34892;&#21033;&#29992;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#8212;&#8212;Logit Attribution Matching (LAM)&#65292;&#20197;&#23454;&#29616;CDG&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LAM&#20165;&#20351;&#29992;&#23569;&#37327;&#37197;&#23545;&#25968;&#25454;&#23601;&#33021;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;DG&#26041;&#27861;&#65292;&#19988;&#26377;&#21161;&#20110;&#27169;&#22411;&#26356;&#22909;&#22320;&#20851;&#27880;&#23545;&#39046;&#22495;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#28145;&#24230;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#29978;&#33267;&#24494;&#23567;&#31243;&#24230;&#30340;&#39046;&#22495;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20005;&#37325;&#25439;&#23475;&#20854;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#35757;&#32451;&#39046;&#22495;&#19978;&#21152;&#24378;&#21508;&#31181;&#19981;&#21464;&#37327;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#19981;&#33021;&#20026;&#26032;&#30340;&#27979;&#35797;&#39046;&#22495;&#25552;&#20379;&#24456;&#22909;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#23545;&#27604;&#39046;&#22495;&#27867;&#21270;&#65288;CDG&#65289;&#65292;&#23427;&#21033;&#29992;&#24378;&#28872;&#23545;&#27604;&#30340;&#25968;&#25454;&#23545;&#25152;&#23637;&#31034;&#30340;&#35821;&#20041;&#19981;&#21464;&#24615;&#65292;&#32780;&#19981;&#26159;&#22810;&#20010;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#39046;&#22495;&#27867;&#21270;&#29702;&#35770;&#65292;&#23637;&#31034;&#20102;CDG&#30340;&#28508;&#22312;&#33021;&#21147;&#65307;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#8212;&#8212;Logit Attribution Matching (LAM)&#65292;&#20197;&#23454;&#29616;CDG&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#23637;&#31034;&#20102;&#65292;LAM&#20165;&#20351;&#29992;&#23569;&#37327;&#37197;&#23545;&#25968;&#25454;&#23601;&#33021;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;DG&#26041;&#27861;&#65292;&#32780;&#19988;LAM&#26377;&#21161;&#20110;&#27169;&#22411;&#26356;&#22909;&#22320;&#20851;&#27880;&#23545;&#39046;&#22495;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization (DG) is an important open problem in machine learning. Deep models are susceptible to domain shifts of even minute degrees, which severely compromises their reliability in real applications. To alleviate the issue, most existing methods enforce various invariant constraints across multiple training domains. However,such an approach provides little performance guarantee for novel test domains in general. In this paper, we investigate a different approach named Contrastive Domain Generalization (CDG), which exploits semantic invariance exhibited by strongly contrastive data pairs in lieu of multiple domains. We present a causal DG theory that shows the potential capability of CDG; together with a regularization technique, Logit Attribution Matching (LAM), for realizing CDG. We empirically show that LAM outperforms state-of-the-art DG methods with only a small portion of paired data and that LAM helps models better focus on semantic features which are crucial to DG.
&lt;/p&gt;</description></item><item><title>LASER&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;&#65292;&#36890;&#36807;&#36923;&#36753;&#35268;&#33539;&#25429;&#25417;&#35270;&#39057;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#23646;&#24615;&#65292;&#33021;&#22815;&#23545;&#40784;&#21407;&#22987;&#35270;&#39057;&#21644;&#35268;&#33539;&#65292;&#26377;&#25928;&#22320;&#35757;&#32451;&#20302;&#32423;&#24863;&#30693;&#27169;&#22411;&#20197;&#25552;&#21462;&#31526;&#21512;&#25152;&#38656;&#39640;&#32423;&#35268;&#33539;&#30340;&#35270;&#39057;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.07647</link><description>&lt;p&gt;
LASER&#65306;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
LASER: Neuro-Symbolic Learning of Semantic Video Representations. (arXiv:2304.07647v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07647
&lt;/p&gt;
&lt;p&gt;
LASER&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;&#65292;&#36890;&#36807;&#36923;&#36753;&#35268;&#33539;&#25429;&#25417;&#35270;&#39057;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#23646;&#24615;&#65292;&#33021;&#22815;&#23545;&#40784;&#21407;&#22987;&#35270;&#39057;&#21644;&#35268;&#33539;&#65292;&#26377;&#25928;&#22320;&#35757;&#32451;&#20302;&#32423;&#24863;&#30693;&#27169;&#22411;&#20197;&#25552;&#21462;&#31526;&#21512;&#25152;&#38656;&#39640;&#32423;&#35268;&#33539;&#30340;&#35270;&#39057;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28041;&#21450;&#35270;&#39057;&#30340;AI&#24212;&#29992;&#65288;&#22914;&#35270;&#39057;-&#25991;&#26412;&#23545;&#40784;&#12289;&#35270;&#39057;&#25628;&#32034;&#21644;&#35270;&#39057;&#23383;&#24149;&#65289;&#21463;&#30410;&#20110;&#23545;&#35270;&#39057;&#35821;&#20041;&#30340;&#32454;&#33268;&#29702;&#35299;&#12290;&#29616;&#26377;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#65292;&#35201;&#20040;&#22522;&#20110;&#19981;&#21487;&#35299;&#37322;&#30340;&#36890;&#29992;&#23884;&#20837;&#65292;&#21487;&#33021;&#20250;&#24573;&#30053;&#37325;&#35201;&#32454;&#33410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LASER&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33021;&#22815;&#25429;&#25417;&#35270;&#39057;&#25968;&#25454;&#20013;&#20016;&#23500;&#30340;&#26102;&#31354;&#23646;&#24615;&#30340;&#36923;&#36753;&#35268;&#33539;&#26469;&#23398;&#20064;&#35821;&#20041;&#35270;&#39057;&#34920;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#21407;&#22987;&#35270;&#39057;&#19982;&#35268;&#33539;&#20043;&#38388;&#30340;&#23545;&#40784;&#26469;&#20844;&#24335;&#21270;&#38382;&#39064;&#12290;&#23545;&#40784;&#36807;&#31243;&#26377;&#25928;&#22320;&#35757;&#32451;&#20102;&#20302;&#23618;&#24863;&#30693;&#27169;&#22411;&#65292;&#20197;&#25552;&#21462;&#31526;&#21512;&#25152;&#38656;&#39640;&#23618;&#35268;&#33539;&#30340;&#32454;&#31890;&#24230;&#35270;&#39057;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#65292;&#24182;&#21487;&#32435;&#20837;&#20174;&#35268;&#33539;&#23548;&#20986;&#30340;&#23545;&#27604;&#21644;&#35821;&#20041;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#20016;&#23500;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern AI applications involving video, such as video-text alignment, video search, and video captioning, benefit from a fine-grained understanding of video semantics. Existing approaches for video understanding are either data-hungry and need low-level annotation, or are based on general embeddings that are uninterpretable and can miss important details. We propose LASER, a neuro-symbolic approach that learns semantic video representations by leveraging logic specifications that can capture rich spatial and temporal properties in video data. In particular, we formulate the problem in terms of alignment between raw videos and specifications. The alignment process efficiently trains low-level perception models to extract a fine-grained video representation that conforms to the desired high-level specification. Our pipeline can be trained end-to-end and can incorporate contrastive and semantic loss functions derived from specifications. We evaluate our method on two datasets with rich sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22270;&#35770;&#30340;&#35821;&#35328;&#21051;&#30011;&#20102;&#24046;&#20998;&#38544;&#31169;&#30340;&#20004;&#31181;&#24773;&#24418;&#19979;&#65292;&#32431;&#31929;&#21644;&#36817;&#20284;&#30340;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#30683;&#30462;&#22270;$G$&#26469;&#25429;&#25417; $\mathcal{H}$ &#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#21457;&#29616;&#20998;&#25968;&#22242;&#25968;&#21644;&#22242;&#25968;&#26159;&#25551;&#36848;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#24615;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#23545;&#20854;&#36827;&#34892;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.03996</link><description>&lt;p&gt;
&#29992;&#22270;&#35770;&#32479;&#19968;&#21051;&#30011;&#24046;&#20998;&#38544;&#31169;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Unified Characterization of Private Learnability via Graph Theory. (arXiv:2304.03996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22270;&#35770;&#30340;&#35821;&#35328;&#21051;&#30011;&#20102;&#24046;&#20998;&#38544;&#31169;&#30340;&#20004;&#31181;&#24773;&#24418;&#19979;&#65292;&#32431;&#31929;&#21644;&#36817;&#20284;&#30340;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#20041;&#30683;&#30462;&#22270;$G$&#26469;&#25429;&#25417; $\mathcal{H}$ &#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#21457;&#29616;&#20998;&#25968;&#22242;&#25968;&#21644;&#22242;&#25968;&#26159;&#25551;&#36848;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#24615;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#23545;&#20854;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#21051;&#30011;&#32431;&#31929;&#30340;&#21644;&#36817;&#20284;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#24615;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#22270;&#35770;&#30340;&#35821;&#35328;:&#23545;&#20110;&#19968;&#20010;&#27010;&#24565;&#31867; $\mathcal{H}$,&#25105;&#20204;&#23450;&#20041;&#20102; $\mathcal{H}$ &#30340;&#30683;&#30462;&#22270; $G$&#12290;&#23427;&#30340;&#39030;&#28857;&#26159;&#21487;&#23454;&#29616;&#30340;&#25968;&#25454;&#38598;&#65292;&#22914;&#26524;&#20004;&#20010;&#25968;&#25454;&#38598; $S$&#65292;$S'$ &#30456;&#20114;&#30683;&#30462;(&#21363;&#65292;&#22312; $S$ &#21644; $S'$ &#20013;&#26377;&#19968;&#20010;&#28857; $x$ &#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#35760;)&#65292;&#21017;&#23427;&#20204;&#20043;&#38388;&#26377;&#19968;&#26465;&#36793;&#36830;&#25509;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;$G$ &#30340;&#32452;&#21512;&#32467;&#26500;&#19982;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064; $\mathcal{H}$ &#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#32431;&#31929;&#30340;&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064; $\mathcal{H}$ &#30340;&#25429;&#33719;&#20026; $G$ &#30340;&#20998;&#25968;&#22242;&#25968;&#12290;&#22312;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#19979;&#23398;&#20064; $\mathcal{H}$ &#30340;&#25429;&#33719;&#20026; $G$ &#30340;&#22242;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25551;&#36848;&#24046;&#20998;&#38544;&#31169;&#21487;&#23398;&#20064;&#24615;&#30340;&#22270;&#35770;&#32500;&#24230;&#65306;&#22242;&#32500;&#21644;&#20998;&#25968;&#22242;&#32500;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#30683;&#30462;&#22270;&#30340;&#19968;&#20123;&#24615;&#36136;&#65292;&#36825;&#20123;&#24615;&#36136;&#21487;&#33021;&#26159;&#29420;&#31435;&#24863;&#20852;&#36259;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#26469;&#20272;&#35745; $G$ &#30340;&#36825;&#20123;&#24230;&#37327;&#65292;&#36890;&#36807;&#36825;&#20123;&#31639;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20960;&#31181;&#27010;&#24565;&#31867;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a unified framework for characterizing pure and approximate differentially private (DP) learnabiliity. The framework uses the language of graph theory: for a concept class $\mathcal{H}$, we define the contradiction graph $G$ of $\mathcal{H}$. It vertices are realizable datasets, and two datasets $S,S'$ are connected by an edge if they contradict each other (i.e., there is a point $x$ that is labeled differently in $S$ and $S'$). Our main finding is that the combinatorial structure of $G$ is deeply related to learning $\mathcal{H}$ under DP. Learning $\mathcal{H}$ under pure DP is captured by the fractional clique number of $G$. Learning $\mathcal{H}$ under approximate DP is captured by the clique number of $G$. Consequently, we identify graph-theoretic dimensions that characterize DP learnability: the clique dimension and fractional clique dimension. Along the way, we reveal properties of the contradiction graph which may be of independent interest. We also suggest several o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#28151;&#21512;VAE&#27169;&#22411;&#23398;&#20064;&#27969;&#24418;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#27169;&#31946;&#21644;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.15244</link><description>&lt;p&gt;
&#29992;&#28151;&#21512;VAE&#27169;&#22411;&#23398;&#20064;&#27969;&#24418;&#26469;&#35299;&#20915;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Manifold Learning by Mixture Models of VAEs for Inverse Problems. (arXiv:2303.15244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#28151;&#21512;VAE&#27169;&#22411;&#23398;&#20064;&#27969;&#24418;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#27169;&#31946;&#21644;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#34920;&#31034;&#39640;&#32500;&#25968;&#25454;&#30340;&#27969;&#24418;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#35201;&#27714;&#25968;&#25454;&#27969;&#24418;&#20855;&#26377;&#20840;&#23616;&#21442;&#25968;&#21270;&#12290;&#20026;&#20102;&#34920;&#31034;&#20219;&#24847;&#25299;&#25169;&#30340;&#27969;&#24418;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#36825;&#37324;&#65292;&#27599;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#34920;&#31034;&#27969;&#24418;&#30340;&#19968;&#20010;&#22270;&#34920;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25439;&#22833;&#20989;&#25968;&#26469;&#26368;&#22823;&#21270;&#20284;&#28982;&#20272;&#35745;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#36873;&#25321;&#19968;&#20010;&#26550;&#26500;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#22270;&#34920;&#21450;&#20854;&#36870;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#12290;&#19968;&#26086;&#23398;&#20064;&#20102;&#27969;&#24418;&#65292;&#25105;&#20204;&#23558;&#20854;&#29992;&#20110;&#36890;&#36807;&#23558;&#25968;&#25454;&#25311;&#21512;&#39033;&#38480;&#21046;&#22312;&#23398;&#20064;&#30340;&#27969;&#24418;&#19978;&#26469;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#25152;&#20135;&#29983;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#23398;&#20064;&#30340;&#27969;&#24418;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#32500;&#29609;&#20855;&#20363;&#23376;&#20197;&#21450;&#27169;&#31946;&#21644;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representing a manifold of very high-dimensional data with generative models has been shown to be computationally efficient in practice. However, this requires that the data manifold admits a global parameterization. In order to represent manifolds of arbitrary topology, we propose to learn a mixture model of variational autoencoders. Here, every encoder-decoder pair represents one chart of a manifold. We propose a loss function for maximum likelihood estimation of the model weights and choose an architecture that provides us the analytical expression of the charts and of their inverses. Once the manifold is learned, we use it for solving inverse problems by minimizing a data fidelity term restricted to the learned manifold. To solve the arising minimization problem we propose a Riemannian gradient descent algorithm on the learned manifold. We demonstrate the performance of our method for low-dimensional toy examples as well as for deblurring and electrical impedance tomography on cert
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65288;DALUPI&#65289;&#31639;&#27861;&#65292;&#20197;&#22312;&#23398;&#20064;&#20013;&#25918;&#23485;&#20551;&#35774;&#26465;&#20214;&#24182;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#20943;&#23569;&#38169;&#35823;&#26469;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31561;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.09350</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation by learning using privileged information. (arXiv:2303.09350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65288;DALUPI&#65289;&#31639;&#27861;&#65292;&#20197;&#22312;&#23398;&#20064;&#20013;&#25918;&#23485;&#20551;&#35774;&#26465;&#20214;&#24182;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#20943;&#23569;&#38169;&#35823;&#26469;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31561;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#21482;&#22312;&#24378;&#20551;&#35774;&#26465;&#20214;&#19979;&#24471;&#20197;&#23454;&#29616;&#65292;&#22914;&#21327;&#21464;&#37327;&#31227;&#20301;&#21644;&#36755;&#20837;&#39046;&#22495;&#20043;&#38388;&#30340;&#37325;&#21472;&#12290;&#21518;&#32773;&#22312;&#39640;&#32500;&#24212;&#29992;&#20013;&#32463;&#24120;&#34987;&#36829;&#21453;&#65292;&#27604;&#22914;&#22270;&#20687;&#20998;&#31867;&#65292;&#22312;&#38754;&#23545;&#36825;&#31181;&#25361;&#25112;&#26102;&#65292;&#22270;&#20687;&#20998;&#31867;&#20173;&#28982;&#26159;&#31639;&#27861;&#24320;&#21457;&#30340;&#28789;&#24863;&#21644;&#22522;&#20934;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#33719;&#21462;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#26679;&#26412;&#30340;&#26377;&#20851;&#20449;&#24687;&#33021;&#22815;&#24110;&#21161;&#25918;&#23485;&#36825;&#20123;&#20551;&#35774;&#65292;&#24182;&#22312;&#23398;&#20064;&#20013;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#20195;&#20215;&#26159;&#25910;&#38598;&#26356;&#20016;&#23500;&#30340;&#21464;&#37327;&#38598;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65288;DALUPI&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#30340;&#23454;&#29992;&#31471;&#21040;&#31471;&#31639;&#27861;&#65292;&#21463;&#21040;&#25105;&#20204;&#20998;&#26512;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#29305;&#26435;&#20449;&#24687;&#21487;&#20197;&#20943;&#23569;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful unsupervised domain adaptation (UDA) is guaranteed only under strong assumptions such as covariate shift and overlap between input domains. The latter is often violated in high-dimensional applications such as image classification which, despite this challenge, continues to serve as inspiration and benchmark for algorithm development. In this work, we show that access to side information about examples from the source and target domains can help relax these assumptions and increase sample efficiency in learning, at the cost of collecting a richer variable set. We call this domain adaptation by learning using privileged information (DALUPI). Tailored for this task, we propose a simple two-stage learning algorithm inspired by our analysis and a practical end-to-end algorithm for multi-label image classification. In a suite of experiments, including an application to medical image analysis, we demonstrate that incorporating privileged information in learning can reduce errors i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#20102;&#19968;&#20010;"&#35748;&#35777;&#24726;&#35770;"&#65292;&#35748;&#35777;&#34429;&#28982;&#21487;&#20197;&#23637;&#31034;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#39069;&#22806;&#25581;&#31034;&#20102;&#26377;&#20851;&#35748;&#35777;&#27169;&#22411;&#30340;&#20449;&#24687;&#20063;&#25104;&#20026;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#23548;&#33268;&#26356;&#22909;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04379</link><description>&lt;p&gt;
&#35748;&#35777;&#24726;&#35770;: &#35748;&#35777;&#20250;&#25581;&#31034;&#26356;&#22909;&#30340;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The Certification Paradox: Certifications Admit Better Attacks. (arXiv:2302.04379v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#20102;&#19968;&#20010;"&#35748;&#35777;&#24726;&#35770;"&#65292;&#35748;&#35777;&#34429;&#28982;&#21487;&#20197;&#23637;&#31034;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#39069;&#22806;&#25581;&#31034;&#20102;&#26377;&#20851;&#35748;&#35777;&#27169;&#22411;&#30340;&#20449;&#24687;&#20063;&#25104;&#20026;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#23548;&#33268;&#26356;&#22909;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20445;&#35777;&#26377;&#19968;&#20010;&#26377;&#30028;&#21306;&#22495;&#20869;&#19981;&#23384;&#22312;&#23545;&#25239;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#35748;&#35777;&#26426;&#21046;&#22312;&#23637;&#31034;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#26041;&#38754;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;: &#35748;&#35777;&#26159;&#21542;&#20250;&#26377;&#20219;&#20309;&#24847;&#24819;&#19981;&#21040;&#30340;&#21518;&#26524;&#65292;&#36890;&#36807;&#25581;&#31034;&#26377;&#20851;&#35748;&#35777;&#27169;&#22411;&#30340;&#39069;&#22806;&#20449;&#24687;&#65311;&#25105;&#20204;&#20197;&#32943;&#23450;&#30340;&#31572;&#26696;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#35748;&#35777;&#19981;&#20165;&#27979;&#37327;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#23637;&#29616;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;"&#35748;&#35777;&#24863;&#30693;&#25915;&#20987;"&#65292;&#22312;&#38024;&#23545;&#32463;&#36807;&#35748;&#35777;&#30340;&#27169;&#22411;&#36827;&#34892;&#25915;&#20987;&#26102;&#65292;&#36825;&#31181;&#25915;&#20987;&#20250;&#27604;&#20197;&#21069;&#30340;&#20219;&#20309;&#26041;&#27861;&#26356;&#39057;&#32321;&#22320;&#20135;&#29983;&#26356;&#23567;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#23454;&#29616;&#20102;&#26368;&#22810;34%&#30340;&#25200;&#21160;&#35268;&#33539;&#20013;&#20301;&#25968;&#30340;&#20943;&#23567;(&#27604;&#36739;&#30446;&#26631;&#21644;&#25915;&#20987;&#23454;&#20363;)&#65292;&#21516;&#26102;&#38656;&#35201;&#30340;&#35745;&#31639;&#26102;&#38388;&#27604;PGD&#31561;&#26041;&#27861;&#23569;&#20102;90%&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#23454;&#29616;&#20102;&#22914;&#27492;&#26174;&#30528;&#30340;&#25200;&#21160;&#22823;&#23567;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38477;&#20302;&#65292;&#31361;&#26174;&#20102;&#20197;&#35748;&#35777;&#20316;&#20026;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#30340;&#19968;&#31181;&#24726;&#35770;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35748;&#35777;&#19981;&#20165;&#25581;&#31034;&#20102;&#31283;&#20581;&#27169;&#22411;&#30340;&#23646;&#24615;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#29992;&#26469;&#21457;&#36215;&#26356;&#26377;&#25928;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
In guaranteeing that no adversarial examples exist within a bounded region, certification mechanisms play an important role in demonstrating the robustness of neural networks. In this work we ask: Could certifications have any unintended consequences, through exposing additional information about certified models? We answer this question in the affirmative, demonstrating that certifications not only measure model robustness but also present a new attack surface. We propose \emph{Certification Aware Attacks}, that produce smaller adversarial perturbations more than twice as frequently as any prior approach, when launched against certified models. Our attacks achieve an up to $34\%$ reduction in the median perturbation norm (comparing target and attack instances), while requiring $90 \%$ less computational time than approaches like PGD. That our attacks achieve such significant reductions in perturbation size and computational cost highlights an apparent paradox in deploying certificatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#25293;&#21334;&#26041;&#24335;&#19979;&#28385;&#36275;&#39044;&#31639;&#21644;ROI&#32422;&#26463;&#65292;&#24182;&#36798;&#21040;&#20010;&#20307;&#21518;&#24724;&#36880;&#28176;&#20943;&#23567;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21508;&#33258;&#31454;&#20105;&#26102;&#65292;&#26399;&#26395;&#36164;&#37329;&#27969;&#21160;&#33267;&#23569;&#36798;&#21040;&#26368;&#20248;&#20998;&#37197;&#30340;&#26399;&#26395;&#27969;&#21160;&#30340;&#19968;&#21322;&#12290;</title><link>http://arxiv.org/abs/2301.13306</link><description>&lt;p&gt;
&#24102;&#26377;&#39044;&#31639;&#21644;ROI&#32422;&#26463;&#30340;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#65306;&#25928;&#29575;&#12289;&#21518;&#24724;&#21644;&#33410;&#22863;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Autobidders with Budget and ROI Constraints: Efficiency, Regret, and Pacing Dynamics. (arXiv:2301.13306v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#25293;&#21334;&#26041;&#24335;&#19979;&#28385;&#36275;&#39044;&#31639;&#21644;ROI&#32422;&#26463;&#65292;&#24182;&#36798;&#21040;&#20010;&#20307;&#21518;&#24724;&#36880;&#28176;&#20943;&#23567;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21508;&#33258;&#31454;&#20105;&#26102;&#65292;&#26399;&#26395;&#36164;&#37329;&#27969;&#21160;&#33267;&#23569;&#36798;&#21040;&#26368;&#20248;&#20998;&#37197;&#30340;&#26399;&#26395;&#27969;&#21160;&#30340;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#22312;&#22312;&#32447;&#24191;&#21578;&#24179;&#21488;&#19978;&#36827;&#34892;&#21338;&#24328;&#30340;&#24773;&#20917;&#12290;&#27599;&#20010;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#34987;&#36171;&#20104;&#20219;&#21153;&#65292;&#22312;&#22810;&#36718;&#37325;&#22797;&#25293;&#21334;&#20013;&#65292;&#26368;&#22823;&#21270;&#20854;&#24191;&#21578;&#20027;&#30340;&#24635;&#20215;&#20540;&#65292;&#21516;&#26102;&#21463;&#21040;&#39044;&#31639;&#21644;/&#25110;&#25237;&#36164;&#22238;&#25253;&#29575;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#20445;&#35777;&#28385;&#36275;&#25152;&#26377;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#36798;&#21040;&#36880;&#28176;&#20943;&#23567;&#30340;&#20010;&#20307;&#21518;&#24724;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20165;&#20351;&#29992;&#33258;&#21161;&#21453;&#39304;&#65292;&#24182;&#21487;&#19982;&#31532;&#19968;&#25110;&#31532;&#20108;&#20215;&#26684;&#25293;&#21334;&#20197;&#21450;&#20219;&#20309;&#8220;&#20013;&#38388;&#8221;&#25293;&#21334;&#26684;&#24335;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#24403;&#36825;&#20123;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#30456;&#20114;&#31454;&#20105;&#26102;&#65292;&#25152;&#26377;&#36718;&#27425;&#30340;&#26399;&#26395;&#36164;&#37329;&#27969;&#21160; welfare &#37117;&#33267;&#23569;&#36798;&#21040;&#20102;&#20219;&#20309;&#20998;&#37197;&#25152;&#23454;&#29616;&#30340;&#26399;&#26395;&#26368;&#20248;&#27969;&#21160; welfare &#30340;&#19968;&#21322;&#12290;&#36825;&#22312;&#20986;&#20215;&#21160;&#24577;&#26159;&#21542;&#25910;&#25947;&#21040;&#22343;&#34913;&#20197;&#21450;&#24191;&#21578;&#20027;&#20272;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#32467;&#26500;&#22914;&#20309;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#22343;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a game between autobidding algorithms that compete in an online advertising platform. Each autobidder is tasked with maximizing its advertiser's total value over multiple rounds of a repeated auction, subject to budget and/or return-on-investment constraints. We propose a gradient-based learning algorithm that is guaranteed to satisfy all constraints and achieves vanishing individual regret. Our algorithm uses only bandit feedback and can be used with the first- or second-price auction, as well as with any "intermediate" auction format. Our main result is that when these autobidders play against each other, the resulting expected liquid welfare over all rounds is at least half of the expected optimal liquid welfare achieved by any allocation. This holds whether or not the bidding dynamics converges to an equilibrium and regardless of the correlation structure between advertiser valuations.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#35770;&#28436;&#21592;&#31639;&#27861;&#65292;&#23427;&#22312;&#24555;&#36895;&#21644;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#20215;&#20540;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#19982;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2210.04470</link><description>&lt;p&gt;
&#28436;&#21592;&#35780;&#35770;&#25110;&#35780;&#35770;&#28436;&#21592;&#65311;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor-Critic or Critic-Actor? A Tale of Two Time Scales. (arXiv:2210.04470v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04470
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#35770;&#28436;&#21592;&#31639;&#27861;&#65292;&#23427;&#22312;&#24555;&#36895;&#21644;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#20215;&#20540;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#19982;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#34920;&#26684;&#30340;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#30340;&#26631;&#20934;&#20844;&#24335;&#65292;&#23558;&#20854;&#35270;&#20026;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#38543;&#26426;&#36924;&#36817;&#65292;&#20854;&#20013;&#20215;&#20540;&#20989;&#25968;&#22312;&#24555;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#65292;&#31574;&#30053;&#22312;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#12290;&#36825;&#27169;&#25311;&#20102;&#31574;&#30053;&#36845;&#20195;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#26102;&#38388;&#23610;&#24230;&#30340;&#21453;&#36716;&#23454;&#38469;&#19978;&#20250;&#27169;&#25311;&#20540;&#36845;&#20195;&#65292;&#24182;&#19988;&#26159;&#19968;&#31181;&#21512;&#27861;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#24182;&#36890;&#36807;&#24102;&#26377;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20989;&#25968;&#36924;&#36817;&#27979;&#35797;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#25105;&#20204;&#25552;&#20986;&#30340;&#35780;&#35770;&#28436;&#21592;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#19982;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the standard formulation of tabular actor-critic algorithm as a two time-scale stochastic approximation with value function computed on a faster time-scale and policy computed on a slower time-scale. This emulates policy iteration. We begin by observing that reversal of the time scales will in fact emulate value iteration and is a legitimate algorithm. We provide a proof of convergence and compare the two empirically with and without function approximation (with both linear and nonlinear function approximators) and observe that our proposed critic-actor algorithm performs on par with actor-critic in terms of both accuracy and computational effort.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#23398;&#20064;&#26410;&#30693;&#32447;&#24615;&#39640;&#26031;&#31995;&#32479;&#19982;&#20108;&#27425;&#20195;&#20215;&#26102;&#65292;&#23384;&#22312;&#36951;&#25022;&#19979;&#38480;&#65292;&#24182;&#19988;&#36825;&#20010;&#19979;&#38480;&#30340;&#27604;&#20363;&#23610;&#24230;&#32423;&#21035;&#20026; $\sqrt{T}$&#12290;&#36890;&#36807;&#23545;&#25511;&#21046;&#29702;&#35770;&#21442;&#25968;&#30340;&#20934;&#30830;&#25429;&#25417;&#65292;&#25105;&#20204;&#35777;&#26126;&#38590;&#20197;&#25511;&#21046;&#30340;&#31995;&#32479;&#20063;&#38590;&#20197;&#23398;&#20064;&#25511;&#21046;&#12290;&#21516;&#26679;&#22320;&#65292;&#23545;&#20110;&#19968;&#31867;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#31995;&#32479;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#20855;&#26377;&#36739;&#24046;&#21487;&#35266;&#27979;&#32467;&#26500;&#30340;&#31995;&#32479;&#20063;&#38590;&#20197;&#23398;&#20064;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2201.01680</link><description>&lt;p&gt;
&#23398;&#20064;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#31995;&#32479;&#30340;&#36951;&#25022;&#19979;&#38480;
&lt;/p&gt;
&lt;p&gt;
Regret Lower Bounds for Learning Linear Quadratic Gaussian Systems. (arXiv:2201.01680v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#23398;&#20064;&#26410;&#30693;&#32447;&#24615;&#39640;&#26031;&#31995;&#32479;&#19982;&#20108;&#27425;&#20195;&#20215;&#26102;&#65292;&#23384;&#22312;&#36951;&#25022;&#19979;&#38480;&#65292;&#24182;&#19988;&#36825;&#20010;&#19979;&#38480;&#30340;&#27604;&#20363;&#23610;&#24230;&#32423;&#21035;&#20026; $\sqrt{T}$&#12290;&#36890;&#36807;&#23545;&#25511;&#21046;&#29702;&#35770;&#21442;&#25968;&#30340;&#20934;&#30830;&#25429;&#25417;&#65292;&#25105;&#20204;&#35777;&#26126;&#38590;&#20197;&#25511;&#21046;&#30340;&#31995;&#32479;&#20063;&#38590;&#20197;&#23398;&#20064;&#25511;&#21046;&#12290;&#21516;&#26679;&#22320;&#65292;&#23545;&#20110;&#19968;&#31867;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#31995;&#32479;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#20855;&#26377;&#36739;&#24046;&#21487;&#35266;&#27979;&#32467;&#26500;&#30340;&#31995;&#32479;&#20063;&#38590;&#20197;&#23398;&#20064;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#33258;&#36866;&#24212;&#25511;&#21046;&#26410;&#30693;&#30340;&#32447;&#24615;&#39640;&#26031;&#31995;&#32479;&#19982;&#20108;&#27425;&#20195;&#20215;&#24314;&#31435;&#36951;&#25022;&#19979;&#38480;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#23454;&#39564;&#35774;&#35745;&#12289;&#20272;&#35745;&#29702;&#35770;&#21644;&#26576;&#20123;&#20449;&#24687;&#30697;&#38453;&#30340;&#25200;&#21160;&#30028;&#38480;&#30340;&#24605;&#24819;&#65292;&#24471;&#21040;&#20102;&#20851;&#20110;&#26102;&#38388;&#36328;&#24230;$T$&#30340;&#36951;&#25022;&#19979;&#38480;&#65292;&#20854;&#27604;&#20363;&#23610;&#24230;&#32423;&#21035;&#20026; $\sqrt{T}$&#12290;&#25105;&#20204;&#30340;&#19979;&#38480;&#20934;&#30830;&#22320;&#25429;&#25417;&#20102;&#25511;&#21046;&#29702;&#35770;&#21442;&#25968;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#25105;&#20204;&#33021;&#22815;&#34920;&#26126;&#38590;&#20197;&#25511;&#21046;&#30340;&#31995;&#32479;&#20063;&#38590;&#20197;&#23398;&#20064;&#25511;&#21046;&#65307;&#24403;&#20855;&#20307;&#21270;&#20026;&#29366;&#24577;&#21453;&#39304;&#31995;&#32479;&#26102;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#26089;&#26399;&#24037;&#20316;&#30340;&#32500;&#24230;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#25913;&#21892;&#20102;&#38543;&#31995;&#32479;&#29702;&#35770;&#24120;&#25968;&#65288;&#22914;&#31995;&#32479;&#25104;&#26412;&#21644;&#26684;&#25289;&#31859;&#24681;&#30697;&#38453;&#65289;&#30340;&#27604;&#20363;&#23610;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#19968;&#31867;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#31995;&#32479;&#65292;&#24182;&#35777;&#26126;&#20855;&#26377;&#36739;&#24046;&#21487;&#35266;&#27979;&#32467;&#26500;&#30340;&#31995;&#32479;&#20063;&#38590;&#20197;&#23398;&#20064;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
TWe establish regret lower bounds for adaptively controlling an unknown linear Gaussian system with quadratic costs. We combine ideas from experiment design, estimation theory and a perturbation bound of certain information matrices to derive regret lower bounds exhibiting scaling on the order of magnitude $\sqrt{T}$ in the time horizon $T$. Our bounds accurately capture the role of control-theoretic parameters and we are able to show that systems that are hard to control are also hard to learn to control; when instantiated to state feedback systems we recover the dimensional dependency of earlier work but with improved scaling with system-theoretic constants such as system costs and Gramians. Furthermore, we extend our results to a class of partially observed systems and demonstrate that systems with poor observability structure also are hard to learn to control.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30456;&#20851;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#27491;&#36127;&#37197;&#23545;&#19981;&#30456;&#20284;&#24230;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#27492;&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2002.07756</link><description>&lt;p&gt;
&#20998;&#23618;&#30456;&#20851;&#32858;&#31867;&#21644;&#32500;&#25345;&#26641;&#32467;&#26500;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Correlation Clustering and Tree Preserving Embedding. (arXiv:2002.07756v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.07756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30456;&#20851;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#27491;&#36127;&#37197;&#23545;&#19981;&#30456;&#20284;&#24230;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#27492;&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30456;&#20851;&#32858;&#31867;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#33879;&#21517;&#30340;&#30456;&#20851;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#36866;&#29992;&#20110;&#27491;&#36127;&#37197;&#23545;&#19981;&#30456;&#20284;&#24230;&#30340;&#20998;&#23618;&#32858;&#31867;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#36825;&#31181;&#20998;&#23618;&#30456;&#20851;&#32858;&#31867;&#30340;&#26080;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#23558;&#30456;&#24212;&#30340;&#20998;&#23618;&#23884;&#20837;&#29992;&#20110;&#32500;&#25345;&#26641;&#32467;&#26500;&#23884;&#20837;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#23567;&#26368;&#22823;&#36317;&#31163;&#24230;&#37327;&#25193;&#23637;&#21040;&#30456;&#20851;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#21478;&#19968;&#31181;&#34920;&#24449;&#23398;&#20064;&#33539;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a hierarchical correlation clustering method that extends the well-known correlation clustering to produce hierarchical clusters applicable to both positive and negative pairwise dissimilarities. Then, in the following, we study unsupervised representation learning with such hierarchical correlation clustering. For this purpose, we first investigate embedding the respective hierarchy to be used for tree-preserving embedding and feature extraction. Thereafter, we study the extension of minimax distance measures to correlation clustering, as another representation learning paradigm. Finally, we demonstrate the performance of our methods on several datasets.
&lt;/p&gt;</description></item></channel></rss>