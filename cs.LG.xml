<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#21644;&#20840;&#23616;&#31283;&#23450;&#24615;&#65292;&#24182;&#35777;&#26126;&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#21482;&#33021;&#24369;&#21270;&#22320;&#23454;&#29616;&#20840;&#23616;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03757</link><description>&lt;p&gt;
&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Replicability and stability in learning. (arXiv:2304.03757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#21644;&#20840;&#23616;&#31283;&#23450;&#24615;&#65292;&#24182;&#35777;&#26126;&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#21482;&#33021;&#24369;&#21270;&#22320;&#23454;&#29616;&#20840;&#23616;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#22797;&#21046;&#24615;&#26159;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#65292;&#22240;&#20026;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#39564;&#35777;&#21644;&#39564;&#35777;&#30740;&#31350;&#32467;&#26524;&#12290;Impagliazzo&#12289;Lei&#12289;Pitassi&#21644;Sorrell&#65288;'22&#65289;&#26368;&#36817;&#24320;&#22987;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#12290;&#22914;&#26524;&#21516;&#19968;&#31639;&#27861;&#22312;&#20004;&#20010;&#29420;&#31435;&#21516;&#20998;&#24067;&#36755;&#20837;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#20869;&#37096;&#38543;&#26426;&#24615;&#26102;&#36890;&#24120;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65292;&#21017;&#23398;&#20064;&#31639;&#27861;&#26159;&#21487;&#22797;&#21046;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#19981;&#28041;&#21450;&#22266;&#23450;&#38543;&#26426;&#24615;&#30340;&#21487;&#22797;&#21046;&#24615;&#21464;&#20307;&#12290;&#22914;&#26524;&#19968;&#20010;&#31639;&#27861;&#22312;&#20004;&#20010;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#36755;&#20837;&#19978;&#65288;&#19981;&#22266;&#23450;&#20869;&#37096;&#38543;&#26426;&#24615;&#65289;&#24212;&#29992;&#26102;&#36890;&#24120;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65292;&#21017;&#31639;&#27861;&#28385;&#36275;&#36825;&#31181;&#24418;&#24335;&#30340;&#21487;&#22797;&#21046;&#24615;&#12290;&#36825;&#20010;&#21464;&#31181;&#34987;&#31216;&#20026;&#20840;&#23616;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#19978;&#19979;&#25991;&#20013;&#30001;Bun&#12289;Livni&#21644;Moran&#65288;'20&#65289;&#20171;&#32461;&#12290; Impagliazzo&#31561;&#20154;&#23637;&#31034;&#20102;&#22914;&#20309;&#25552;&#39640;&#20219;&#20309;&#21487;&#22797;&#21046;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#20197;&#20351;&#20854;&#20135;&#29983;&#30340;&#36755;&#20986;&#27010;&#29575;&#26080;&#38480;&#25509;&#36817;&#20110;1&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#65292;&#21482;&#33021;&#24369;&#21270;&#22320;&#23454;&#29616;&#20840;&#23616;&#31283;&#23450;&#24615;&#65292;&#36825;&#37324;&#36755;&#20986;&#21482;&#26377;&#30456;&#21516;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Replicability is essential in science as it allows us to validate and verify research findings. Impagliazzo, Lei, Pitassi and Sorrell (`22) recently initiated the study of replicability in machine learning. A learning algorithm is replicable if it typically produces the same output when applied on two i.i.d. inputs using the same internal randomness. We study a variant of replicability that does not involve fixing the randomness. An algorithm satisfies this form of replicability if it typically produces the same output when applied on two i.i.d. inputs (without fixing the internal randomness). This variant is called global stability and was introduced by Bun, Livni and Moran (`20) in the context of differential privacy.  Impagliazzo et al. showed how to boost any replicable algorithm so that it produces the same output with probability arbitrarily close to 1. In contrast, we demonstrate that for numerous learning tasks, global stability can only be accomplished weakly, where the same o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#35843;&#24230;&#25216;&#26415;&#65292;&#38024;&#23545;&#21333;&#20010;&#38382;&#39064;&#23454;&#20363;&#37319;&#29992;&#36866;&#24212;&#24615;&#26694;&#26550;&#65292;&#36229;&#36234;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24037;&#20316;&#65292;&#21516;&#26102;&#25511;&#21046;&#20004;&#20010;&#19981;&#21516;&#31867;&#22411;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32463;&#36807;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#26377;&#25928;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03755</link><description>&lt;p&gt;
&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21551;&#21457;&#24335;&#35843;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Learning for Scheduling MIP Heuristics. (arXiv:2304.03755v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#35843;&#24230;&#25216;&#26415;&#65292;&#38024;&#23545;&#21333;&#20010;&#38382;&#39064;&#23454;&#20363;&#37319;&#29992;&#36866;&#24212;&#24615;&#26694;&#26550;&#65292;&#36229;&#36234;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24037;&#20316;&#65292;&#21516;&#26102;&#25511;&#21046;&#20004;&#20010;&#19981;&#21516;&#31867;&#22411;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32463;&#36807;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#26377;&#25928;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;(MIP)&#26159;NP&#38590;&#38382;&#39064;&#65292;&#20294;&#29616;&#20195;&#27714;&#35299;&#22120;&#36890;&#24120;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#35299;&#20915;&#22823;&#22411;&#23454;&#38469;&#38382;&#39064;&#12290;&#36825;&#31181;&#25104;&#21151;&#37096;&#20998;&#24402;&#21151;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#34892;&#20026;&#39640;&#24230;&#20381;&#36182;&#20110;&#23454;&#20363;&#65292;&#22240;&#27492;&#20381;&#36182;&#20110;&#20174;&#22823;&#22411;&#24322;&#26500;&#22522;&#20934;&#23454;&#20363;&#30340;&#32463;&#39564;&#27979;&#35797;&#25512;&#23548;&#20986;&#30340;&#30828;&#32534;&#30721;&#35268;&#21017;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#21551;&#21457;&#24335;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#24403;&#21069;&#38382;&#39064;&#23454;&#20363;&#12290;&#25105;&#20204;&#29992;&#19968;&#20010;&#36866;&#24212;&#24615;&#26694;&#26550;&#21462;&#20195;&#20102;&#36890;&#24120;&#20351;&#29992;&#30340;&#38745;&#24577;&#21551;&#21457;&#24335;&#22788;&#29702;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20851;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#34892;&#20026;&#30340;&#36807;&#21435;&#35266;&#23519;&#32467;&#26524;&#26469;&#20570;&#20986;&#26410;&#26469;&#20915;&#31574;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#25511;&#21046;&#22823;&#37051;&#22495;&#25628;&#32034;&#21644;&#28508;&#27700; - &#20004;&#20010;&#24191;&#27867;&#19988;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#38382;&#39064;&#24314;&#27169;&#20026;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36229;&#36234;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#36890;&#36807;&#21333;&#20010;&#23398;&#20064;&#20195;&#29702;&#21516;&#26102;&#25511;&#21046;&#20102;&#20004;&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;MIPLIB 2017&#22522;&#20934;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21551;&#21457;&#24335;&#22788;&#29702;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixed Integer Programming (MIP) is NP-hard, and yet modern solvers often solve large real-world problems within minutes. This success can partially be attributed to heuristics. Since their behavior is highly instance-dependent, relying on hard-coded rules derived from empirical testing on a large heterogeneous corpora of benchmark instances might lead to sub-optimal performance. In this work, we propose an online learning approach that adapts the application of heuristics towards the single instance at hand. We replace the commonly used static heuristic handling with an adaptive framework exploiting past observations about the heuristic's behavior to make future decisions. In particular, we model the problem of controlling Large Neighborhood Search and Diving - two broad and complex classes of heuristics as a multi-armed bandit problem. Going beyond existing work in the literature, we control two different classes of heuristics simultaneously by a single learning agent. We verify our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#26550;&#26500;&#21644;&#20849;&#35774;&#35745;&#23545;&#22320;&#29699;&#31995;&#32479;&#21487;&#39044;&#27979;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#21457;&#23637;&#26032;&#30340;&#26550;&#26500;&#21644;&#31574;&#30053;&#20197;&#20419;&#36827;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#22320;&#29699;&#31995;&#32479;&#24314;&#27169;&#21644;&#39044;&#27979;&#39046;&#22495;&#30340;&#21457;&#23637;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.03748</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26550;&#26500;&#21644;&#20849;&#35774;&#35745;&#23545;&#22320;&#29699;&#31995;&#32479;&#21487;&#39044;&#27979;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Perspectives on AI Architectures and Co-design for Earth System Predictability. (arXiv:2304.03748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#26550;&#26500;&#21644;&#20849;&#35774;&#35745;&#23545;&#22320;&#29699;&#31995;&#32479;&#21487;&#39044;&#27979;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#21457;&#23637;&#26032;&#30340;&#26550;&#26500;&#21644;&#31574;&#30053;&#20197;&#20419;&#36827;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#22312;&#22320;&#29699;&#31995;&#32479;&#24314;&#27169;&#21644;&#39044;&#27979;&#39046;&#22495;&#30340;&#21457;&#23637;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#33021;&#28304;&#37096;&#65288;DOE&#65289;&#31185;&#23398;&#21150;&#20844;&#23460;&#29983;&#29289;&#21644;&#29615;&#22659;&#30740;&#31350;&#65288;BER&#65289;&#21644;&#39640;&#32423;&#31185;&#23398;&#35745;&#31639;&#30740;&#31350;&#65288;ASCR&#65289;&#35745;&#21010;&#26368;&#36817;&#32452;&#32455;&#24182;&#20030;&#21150;&#20102;&#8220;&#38754;&#21521;&#22320;&#29699;&#31995;&#32479;&#21487;&#39044;&#27979;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI4ESP&#65289;&#8221;&#30740;&#35752;&#20250;&#31995;&#21015;&#12290;&#20174;&#36825;&#20010;&#30740;&#35752;&#20250;&#20013;&#65292;DOE BER&#21644;ASCR&#31038;&#21306;&#24471;&#20986;&#30340;&#19968;&#20010;&#20851;&#38190;&#32467;&#35770;&#26159;&#38656;&#35201;&#21457;&#23637;&#19968;&#20010;&#26032;&#30340;&#22320;&#29699;&#31995;&#32479;&#21487;&#39044;&#27979;&#24615;&#33539;&#24335;&#65292;&#37325;&#28857;&#26159;&#22312;&#25972;&#20010;&#39046;&#22495;&#12289;&#23454;&#39564;&#23460;&#12289;&#24314;&#27169;&#21644;&#20998;&#26512;&#27963;&#21160;&#20013;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#31216;&#20026;ModEx&#12290;BER&#30340;&#8220;&#27169;&#22411;&#23454;&#39564;&#8221;&#65292;ModEx&#65292;&#26159;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#20351;&#36807;&#31243;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#20551;&#35774;&#12290;&#25152;&#24320;&#21457;&#30340;&#20551;&#35774;&#36890;&#30693;&#37319;&#38598;&#27979;&#37327;&#21644;&#35266;&#27979;&#25968;&#25454;&#30340;&#29616;&#22330;&#21644;&#23454;&#39564;&#23460;&#24037;&#20316;&#65292;&#38543;&#21518;&#29992;&#20110;&#21442;&#25968;&#21270;&#12289;&#39537;&#21160;&#21644;&#27979;&#35797;&#27169;&#22411;&#65288;&#20363;&#22914;&#22522;&#20110;&#36807;&#31243;&#30340;&#65289;&#39044;&#27979;&#12290;&#22312;&#36825;&#20010;AI4ESP&#24037;&#20316;&#22346;&#31995;&#21015;&#20013;&#20849;&#20030;&#34892;&#20102;17&#20010;&#25216;&#26415;&#20250;&#35758;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20854;&#20013;&#19968;&#27425;&#20250;&#35758;&#30340;&#20027;&#39064;&#8220;&#20154;&#24037;&#26234;&#33021;&#26550;&#26500;&#21644;&#20849;&#35774;&#35745;&#23545;&#22320;&#29699;&#31995;&#32479;&#21487;&#39044;&#27979;&#24615;&#30340;&#24433;&#21709;&#8221;&#12290;&#23427;&#25552;&#20379;&#20102;&#21457;&#23637;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#26550;&#26500;&#21644;&#20849;&#35774;&#35745;&#31574;&#30053;&#26469;&#35299;&#20915;&#22320;&#29699;&#31995;&#32479;&#24314;&#27169;&#21644;&#39044;&#27979;&#39046;&#22495;&#29305;&#23450;&#30340;&#31185;&#23398;&#21644;&#25216;&#26415;&#38656;&#27714;&#65292;&#20197;&#25512;&#36827;&#20154;&#24037;&#26234;&#33021;&#22312;&#22320;&#29699;&#31995;&#32479;&#21487;&#39044;&#27979;&#24615;&#20013;&#30340;&#20316;&#29992;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the U.S. Department of Energy (DOE), Office of Science, Biological and Environmental Research (BER), and Advanced Scientific Computing Research (ASCR) programs organized and held the Artificial Intelligence for Earth System Predictability (AI4ESP) workshop series. From this workshop, a critical conclusion that the DOE BER and ASCR community came to is the requirement to develop a new paradigm for Earth system predictability focused on enabling artificial intelligence (AI) across the field, lab, modeling, and analysis activities, called ModEx. The BER's `Model-Experimentation', ModEx, is an iterative approach that enables process models to generate hypotheses. The developed hypotheses inform field and laboratory efforts to collect measurement and observation data, which are subsequently used to parameterize, drive, and test model (e.g., process-based) predictions. A total of 17 technical sessions were held in this AI4ESP workshop series. This paper discusses the topic of the `
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#32773;&#22312;&#20844;&#24179;&#24615;&#20013;&#20248;&#20808;&#32771;&#34385;&#20934;&#30830;&#24615;&#12289;&#20195;&#34920;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#21516;&#26102;&#38754;&#20020;&#25968;&#25454;&#20559;&#35265;&#21644;&#32570;&#20047;&#25351;&#23548;&#26041;&#38024;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.03745</link><description>&lt;p&gt;
&#20174;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#32773;&#30340;&#35282;&#24230;&#35780;&#20272;&#24863;&#30693;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing Perceived Fairness from Machine Learning Developer's Perspective. (arXiv:2304.03745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03745
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#32773;&#22312;&#20844;&#24179;&#24615;&#20013;&#20248;&#20808;&#32771;&#34385;&#20934;&#30830;&#24615;&#12289;&#20195;&#34920;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#21516;&#26102;&#38754;&#20020;&#25968;&#25454;&#20559;&#35265;&#21644;&#32570;&#20047;&#25351;&#23548;&#26041;&#38024;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#20844;&#24179;&#24615;&#23545;&#20110;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#30340;&#24320;&#21457;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#23454;&#36341;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#19981;&#20844;&#24179;&#26159;&#30001;&#20110;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#12289;&#31574;&#21010;&#36807;&#31243;&#20013;&#30340;&#27495;&#35270;&#12289;&#38169;&#35823;&#30340;&#20551;&#35774;&#21644;&#31639;&#27861;&#24320;&#21457;&#36807;&#31243;&#20013;&#21576;&#29616;&#30340;&#38544;&#24615;&#20559;&#35265;&#24341;&#36215;&#30340;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#24320;&#21457;&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25991;&#29486;&#25351;&#20986;&#20102;&#22810;&#31181;&#20851;&#20110;&#29992;&#25143;&#35270;&#35282;&#21644;&#20316;&#20026;&#26410;&#26469;&#24320;&#21457;&#32773;&#30340;&#23398;&#29983;&#35270;&#35282;&#19979;&#65292;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#26159;&#22914;&#20309;&#25551;&#36848;&#30340;&#30475;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#32773;&#30340;&#24863;&#30693;&#20844;&#24179;&#24615;&#36824;&#27809;&#26377;&#24471;&#21040;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#32773;&#24863;&#30693;&#20844;&#24179;&#24615;&#30340;&#21021;&#27493;&#35843;&#26597;&#12290;&#22312;&#25551;&#36848;&#20844;&#24179;&#24615;&#24863;&#30693;&#26041;&#38754;&#65292;&#26412;&#25991;&#20351;&#29992;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#28966;&#28857;&#23567;&#32452;&#26041;&#27861;&#26469;&#35780;&#20272;&#35813;&#27010;&#24565;&#30340;&#23646;&#24615;&#12290;&#22312;&#28966;&#28857;&#23567;&#32452;&#20013;&#65292;&#25105;&#20204;&#35201;&#27714;&#21442;&#19982;&#32773;&#35752;&#35770;&#19977;&#20010;&#38382;&#39064;&#65306;1&#65289;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#29305;&#24449;&#26159;&#20160;&#20040;&#65311;2&#65289;&#24320;&#21457;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#26159;&#20160;&#20040;&#65311;3&#65289;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#32773;&#22914;&#20309;&#32531;&#35299;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#19981;&#20844;&#24179;&#24615;&#65311;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#24320;&#21457;&#32773;&#23558;&#20934;&#30830;&#24615;&#12289;&#20195;&#34920;&#24615;&#21644;&#36879;&#26126;&#24230;&#25918;&#22312;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#20248;&#20808;&#32771;&#34385;&#20301;&#32622;&#65292;&#24182;&#38754;&#20020;&#25968;&#25454;&#20559;&#35265;&#21644;&#32570;&#20047;&#26126;&#30830;&#25351;&#23548;&#26041;&#38024;&#31561;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#31361;&#26174;&#20102;&#24605;&#32771;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#32773;&#35270;&#35282;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#38382;&#39064;&#19978;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness in machine learning (ML) applications is an important practice for developers in research and industry. In ML applications, unfairness is triggered due to bias in the data, curation process, erroneous assumptions, and implicit bias rendered within the algorithmic development process. As ML applications come into broader use developing fair ML applications is critical. Literature suggests multiple views on how fairness in ML is described from the users perspective and students as future developers. In particular, ML developers have not been the focus of research relating to perceived fairness. This paper reports on a pilot investigation of ML developers perception of fairness. In describing the perception of fairness, the paper performs an exploratory pilot study to assess the attributes of this construct using a systematic focus group of developers. In the focus group, we asked participants to discuss three questions- 1) What are the characteristics of fairness in ML? 2) What 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#20840;&#26799;&#24230;DQN&#31639;&#27861;&#20174;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#25193;&#23637;&#33267;&#24179;&#22343;&#22870;&#21169;&#38382;&#39064;&#65292;&#24182;&#22312;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#29615;&#22659;&#20013;&#35777;&#26126;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.03729</link><description>&lt;p&gt;
&#20840;&#26799;&#24230;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#24179;&#22343;&#22870;&#21169;&#26631;&#20934;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Full Gradient Deep Reinforcement Learning for Average-Reward Criterion. (arXiv:2304.03729v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#20840;&#26799;&#24230;DQN&#31639;&#27861;&#20174;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#25193;&#23637;&#33267;&#24179;&#22343;&#22870;&#21169;&#38382;&#39064;&#65292;&#24182;&#22312;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#29615;&#22659;&#20013;&#35777;&#26126;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#24050;&#34987;&#35777;&#26126;&#21487;&#25910;&#25947;&#30340;&#20840;&#26799;&#24230;DQN&#31639;&#27861;&#20174;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#25193;&#23637;&#33267;&#24179;&#22343;&#22870;&#21169;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#29615;&#22659;&#20013;&#65292;&#27604;&#36739;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;RVI Q-learning&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;Differential Q-learning&#20197;&#21450;&#20840;&#26799;&#24230;DQN&#21644;DQN&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#25193;&#23637;&#21040;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#19981;&#38745;&#24577;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;Whittle&#25351;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#20840;&#26799;&#24230;&#21464;&#20307;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend the provably convergent Full Gradient DQN algorithm for discounted reward Markov decision processes from Avrachenkov et al. (2021) to average reward problems. We experimentally compare widely used RVI Q-Learning with recently proposed Differential Q-Learning in the neural function approximation setting with Full Gradient DQN and DQN. We also extend this to learn Whittle indices for Markovian restless multi-armed bandits. We observe a better convergence rate of the proposed Full Gradient variant across different tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#21487;&#20197;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#20197;&#21450;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2304.03724</link><description>&lt;p&gt;
&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Predicting quantum chemical property with easy-to-obtain geometry via positional denoising. (arXiv:2304.03724v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#21487;&#20197;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#20197;&#21450;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#19982;&#20854;&#20960;&#20309;&#32467;&#26500;&#26377;&#37325;&#35201;&#20851;&#32852;&#65292;&#20351;&#29992;3D&#20960;&#20309;&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#39640;&#32423;&#37327;&#23376;&#21147;&#23398;&#35745;&#31639;&#24471;&#20986;&#30340;3D&#20960;&#20309;&#32467;&#26500;&#65292;&#36825;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#29616;&#23454;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65288;&#20363;&#22914;&#26469;&#33258;&#20998;&#23376;&#21147;&#22330;&#30340;&#20248;&#21270;&#20960;&#20309;&#32467;&#26500;&#65289;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#36755;&#20837;&#20960;&#20309;&#32467;&#26500;&#36880;&#28176;&#25509;&#36817;&#27491;&#30830;&#20960;&#20309;&#32467;&#26500;&#65292;&#36890;&#36807;&#22534;&#21472;&#21435;&#22122;&#23618;&#12290;&#25105;&#20204;&#20351;&#29992;3D&#28040;&#24687;&#20256;&#36882;&#20307;&#31995;&#32467;&#26500;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#65288;&#20998;&#23376;&#24615;&#36136;&#21644;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#65289;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21435;&#22122;&#36807;&#31243;&#20943;&#23569;&#20301;&#32622;&#35823;&#24046;&#26377;&#21161;&#20110;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
As quantum chemical properties have a significant dependence on their geometries, graph neural networks (GNNs) using 3D geometric information have achieved high prediction accuracy in many tasks. However, they often require 3D geometries obtained from high-level quantum mechanical calculations, which are practically infeasible, limiting their applicability in real-world problems. To tackle this, we propose a method to accurately predict the properties with relatively easy-to-obtain geometries (e.g., optimized geometries from the molecular force field). In this method, the input geometry, regarded as the corrupted geometry of the correct one, gradually approaches the correct one as it passes through the stacked denoising layers. We investigated the performance of the proposed method using 3D message-passing architectures for two prediction tasks: molecular properties and chemical reaction property. The reduction of positional errors through the denoising process contributed to performan
&lt;/p&gt;</description></item><item><title>&#21512;&#25104;&#25968;&#25454;&#30340;&#28508;&#21147;&#36828;&#38750;&#31169;&#26377;&#21270;&#25968;&#25454;&#65292;&#32780;&#26159;&#21253;&#25324;&#21019;&#24314;&#26356;&#20844;&#24179;&#30340;&#25968;&#25454;&#65292;&#25968;&#25454;&#22686;&#24378;&#65292;&#27169;&#25311;&#21644;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#32780;&#65292;&#21512;&#25104;&#25968;&#25454;&#30340;&#24212;&#29992;&#20173;&#28982;&#38656;&#35201;&#20811;&#26381;&#22522;&#26412;&#25361;&#25112;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#22914;&#20309;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03722</link><description>&lt;p&gt;
&#36229;&#36234;&#38544;&#31169;&#65306;&#21512;&#25104;&#25968;&#25454;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Beyond Privacy: Navigating the Opportunities and Challenges of Synthetic Data. (arXiv:2304.03722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03722
&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#30340;&#28508;&#21147;&#36828;&#38750;&#31169;&#26377;&#21270;&#25968;&#25454;&#65292;&#32780;&#26159;&#21253;&#25324;&#21019;&#24314;&#26356;&#20844;&#24179;&#30340;&#25968;&#25454;&#65292;&#25968;&#25454;&#22686;&#24378;&#65292;&#27169;&#25311;&#21644;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#32780;&#65292;&#21512;&#25104;&#25968;&#25454;&#30340;&#24212;&#29992;&#20173;&#28982;&#38656;&#35201;&#20811;&#26381;&#22522;&#26412;&#25361;&#25112;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#22914;&#20309;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#21644;&#20854;&#20182;&#39046;&#22495;&#24341;&#21457;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36807;&#21435;&#65292;&#21512;&#25104;&#25968;&#25454;&#24448;&#24448;&#34987;&#35270;&#20026;&#20844;&#24320;&#31169;&#26377;&#25968;&#25454;&#30340;&#19968;&#31181;&#26041;&#24335;&#65292;&#20294;&#26368;&#36817;&#19968;&#31995;&#21015;&#30340;&#35770;&#25991;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#28508;&#21147;&#36828;&#38750;&#22914;&#27492;&#8212;&#8212;&#20174;&#21019;&#24314;&#26356;&#20844;&#24179;&#30340;&#25968;&#25454;&#21040;&#25968;&#25454;&#22686;&#24378;&#65292;&#20174;&#27169;&#25311;&#21040;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#26159;&#21542;&#21450;&#22914;&#20309;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#20027;&#23548;&#21147;&#37327;&#65292;&#25215;&#35834;&#26410;&#26469;&#25968;&#25454;&#38598;&#21487;&#20197;&#26681;&#25454;&#20010;&#20154;&#38656;&#27714;&#36827;&#34892;&#23450;&#21046;&#12290;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#31038;&#21306;&#38656;&#35201;&#20811;&#26381;&#30340;&#22522;&#26412;&#25361;&#25112;&#65292;&#20197;&#25193;&#22823;&#21512;&#25104;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#21644;&#24212;&#29992;&#8212;&#8212;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26159;&#37327;&#21270;&#25105;&#20204;&#33021;&#22815;&#20449;&#20219;&#20174;&#21512;&#25104;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#20219;&#20309;&#21457;&#29616;&#25110;&#39044;&#27979;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating synthetic data through generative models is gaining interest in the ML community and beyond. In the past, synthetic data was often regarded as a means to private data release, but a surge of recent papers explore how its potential reaches much further than this -- from creating more fair data to data augmentation, and from simulation to text generated by ChatGPT. In this perspective we explore whether, and how, synthetic data may become a dominant force in the machine learning world, promising a future where datasets can be tailored to individual needs. Just as importantly, we discuss which fundamental challenges the community needs to overcome for wider relevance and application of synthetic data -- the most important of which is quantifying how much we can trust any finding or prediction drawn from synthetic data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#30340;&#26032;&#30340;&#34920;&#29616;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#20197;&#19977;&#20803;&#32452;&#27604;&#36739;&#20026;&#22522;&#30784;&#30340;&#34920;&#29616;&#23450;&#29702;&#38382;&#39064;&#12290;&#36825;&#31181;&#34920;&#29616;&#23450;&#29702;&#21487;&#20197;&#29992;&#20869;&#31215;&#35825;&#23548;&#30340;&#33539;&#25968;&#26469;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.03720</link><description>&lt;p&gt;
&#24230;&#37327;&#23398;&#20064;&#19982;&#20559;&#22909;&#23398;&#20064;&#30340;&#34920;&#29616;&#23450;&#29702;&#65306;&#22522;&#20110;&#20960;&#20309;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Representer Theorems for Metric and Preference Learning: A Geometric Perspective. (arXiv:2304.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03720
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#30340;&#26032;&#30340;&#34920;&#29616;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#20197;&#19977;&#20803;&#32452;&#27604;&#36739;&#20026;&#22522;&#30784;&#30340;&#34920;&#29616;&#23450;&#29702;&#38382;&#39064;&#12290;&#36825;&#31181;&#34920;&#29616;&#23450;&#29702;&#21487;&#20197;&#29992;&#20869;&#31215;&#35825;&#23548;&#30340;&#33539;&#25968;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#23398;&#20064;&#21644;&#20559;&#22909;&#23398;&#20064;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#34920;&#29616;&#23450;&#29702;&#21487;&#20197;&#26681;&#25454;&#38382;&#39064;&#32467;&#26500;&#20869;&#22312;&#30340;&#20869;&#31215;&#25152;&#35825;&#23548;&#30340;&#33539;&#25968;&#26469;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#19977;&#20803;&#32452;&#27604;&#36739;&#30340;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#23427;&#23548;&#33268;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#33258;&#21253;&#21547;&#30340;&#35813;&#20219;&#21153;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#21487;&#20197;&#20351;&#29992;&#31867;&#20284;&#20110;&#32463;&#20856;&#34920;&#29616;&#23450;&#29702;&#30340;&#26680;&#26415;&#35821;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the metric and preference learning problem in Hilbert spaces. We obtain a novel representer theorem for the simultaneous task of metric and preference learning. Our key observation is that the representer theorem can be formulated with respect to the norm induced by the inner product inherent in the problem structure. Additionally, we demonstrate how our framework can be applied to the task of metric learning from triplet comparisons and show that it leads to a simple and self-contained representer theorem for this task. In the case of Reproducing Kernel Hilbert Spaces (RKHS), we demonstrate that the solution to the learning problem can be expressed using kernel terms, akin to classical representer theorems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;(SHM)&#39046;&#22495;&#20013;&#23558;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;&#29992;&#20110;&#23454;&#26102;&#26725;&#26753;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#21830;&#19994;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24179;&#21488;&#26469;&#24320;&#21457;&#21644;&#20998;&#26512;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#35774;&#22791;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03718</link><description>&lt;p&gt;
&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#39046;&#22495;&#20013;&#30340;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Integrating Edge-AI in Structural Health Monitoring domain. (arXiv:2304.03718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;(SHM)&#39046;&#22495;&#20013;&#23558;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;&#29992;&#20110;&#23454;&#26102;&#26725;&#26753;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#21830;&#19994;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24179;&#21488;&#26469;&#24320;&#21457;&#21644;&#20998;&#26512;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#35774;&#22791;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;(SHM)&#20219;&#21153;&#27604;&#22914;&#25439;&#20260;&#26816;&#27979;&#23545;&#20110;&#20851;&#20110;&#32500;&#25252;&#21644;&#21155;&#21270;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#26725;&#26753;&#32500;&#25252;&#26469;&#35828;&#65292;&#35010;&#32441;&#26816;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#35010;&#32441;&#30340;&#36827;&#23637;&#20250;&#23548;&#33268;&#32467;&#26500;&#19981;&#31283;&#23450;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;AI/ML&#27169;&#22411;&#22312;&#23454;&#26102;&#29615;&#22659;&#19979;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#24310;&#36831;&#25512;&#29702;&#26102;&#38388;&#31561;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#23558;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;&#21040;SHM&#39046;&#22495;&#20013;&#65292;&#20197;&#36827;&#34892;&#23454;&#26102;&#26725;&#26753;&#26816;&#27979;&#12290;&#26681;&#25454;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25991;&#29486;&#65292;&#23427;&#30340;&#33021;&#21147;&#23558;&#26159;SHM&#20219;&#21153;&#23454;&#26102;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20013;&#26377;&#20215;&#20540;&#30340;&#38598;&#25104;&#26041;&#24335;&#65292;&#20197;&#20415;&#21487;&#20197;&#22312;&#29289;&#29702;&#29616;&#22330;&#36827;&#34892;&#23454;&#26102;&#25512;&#29702;&#12290;&#26412;&#30740;&#31350;&#23558;&#21033;&#29992;&#21830;&#19994;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24179;&#21488;&#65292;&#20363;&#22914;Google Coral Dev Board &#25110; Kneron KL520&#26469;&#24320;&#21457;&#21644;&#20998;&#26512;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#35774;&#22791;&#30340;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#20010;&#36866;&#29992;&#20110;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#39046;&#22495;&#30340;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structural health monitoring (SHM) tasks like damage detection are crucial for decision-making regarding maintenance and deterioration. For example, crack detection in SHM is crucial for bridge maintenance as crack progression can lead to structural instability. However, most AI/ML models in the literature have low latency and late inference time issues while performing in real-time environments. This study aims to explore the integration of edge-AI in the SHM domain for real-time bridge inspections. Based on edge-AI literature, its capabilities will be valuable integration for a real-time decision support system in SHM tasks such that real-time inferences can be performed on physical sites. This study will utilize commercial edge-AI platforms, such as Google Coral Dev Board or Kneron KL520, to develop and analyze the effectiveness of edge-AI devices. Thus, this study proposes an edge AI framework for the structural health monitoring domain. An edge-AI-compatible deep learning model is
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#25439;&#22833;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#24179;&#34913;&#25152;&#23398;&#34920;&#31034;&#65292;&#27491;&#23545;&#25512;&#21160;&#27169;&#22411;&#23545;&#40784;&#34920;&#31034;&#65292;&#32780;&#36127;&#23545;&#21017;&#20445;&#25345;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03717</link><description>&lt;p&gt;
&#20851;&#20110;&#23545;&#27604;&#25439;&#22833;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Importance of Contrastive Loss in Multimodal Learning. (arXiv:2304.03717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03717
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#24179;&#34913;&#25152;&#23398;&#34920;&#31034;&#65292;&#27491;&#23545;&#25512;&#21160;&#27169;&#22411;&#23545;&#40784;&#34920;&#31034;&#65292;&#32780;&#36127;&#23545;&#21017;&#20445;&#25345;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;&#20363;&#22914; CLIP&#65288;Radford &#31561;&#20154;&#65292;2021&#65289;&#65289;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#27169;&#22411;&#23581;&#35797;&#26368;&#23567;&#21270;&#21516;&#19968;&#25968;&#25454;&#28857;&#30340;&#19981;&#21516;&#35270;&#22270;&#65288;&#20363;&#22914;&#22270;&#20687;&#21644;&#20854;&#26631;&#39064;&#65289;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21516;&#26102;&#20351;&#19981;&#21516;&#25968;&#25454;&#28857;&#30340;&#34920;&#31034;&#24444;&#27492;&#20998;&#31163;&#12290;&#28982;&#32780;&#65292;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#24403;&#25968;&#25454;&#19981;&#26159;&#21508;&#21521;&#21516;&#24615;&#26102;&#65292;&#23545;&#27604;&#23398;&#20064;&#22914;&#20309;&#26377;&#25928;&#22320;&#23398;&#20064;&#26469;&#33258;&#19981;&#21516;&#35270;&#22270;&#30340;&#34920;&#31034;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#24182;&#34920;&#26126;&#23545;&#27604;&#23545;&#26159;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#24179;&#34913;&#25152;&#23398;&#34920;&#31034;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#23588;&#20854;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#27491;&#23545;&#26159;&#33021;&#22815;&#25512;&#21160;&#27169;&#22411;&#22312;&#22686;&#21152;&#26465;&#20214;&#25968;&#30340;&#20195;&#20215;&#19979;&#23545;&#40784;&#34920;&#31034;&#65292;&#32780;&#36127;&#23545;&#21017;&#38477;&#20302;&#26465;&#20214;&#25968;&#65292;&#20445;&#25345;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, contrastive learning approaches (e.g., CLIP (Radford et al., 2021)) have received huge success in multimodal learning, where the model tries to minimize the distance between the representations of different views (e.g., image and its caption) of the same data point while keeping the representations of different data points away from each other. However, from a theoretical perspective, it is unclear how contrastive learning can learn the representations from different views efficiently, especially when the data is not isotropic. In this work, we analyze the training dynamics of a simple multimodal contrastive learning model and show that contrastive pairs are important for the model to efficiently balance the learned representations. In particular, we show that the positive pairs will drive the model to align the representations at the cost of increasing the condition number, while the negative pairs will reduce the condition number, keeping the learned representations balance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#32593;&#32476;&#65292;&#24320;&#21457;&#20986;&#20102;&#20843;&#31181;&#26377;&#21069;&#36884;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#29992;&#20110;&#26816;&#27979;&#28145;&#24230;&#20266;&#36896;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#21333;&#27169;&#22411;&#26816;&#27979;&#22120;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;AUC&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36824;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#20248;&#21155;&#21644;&#36229;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.03698</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Deepfake&#26816;&#27979;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19982;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deepfake Detection with Deep Learning: Convolutional Neural Networks versus Transformers. (arXiv:2304.03698v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#32593;&#32476;&#65292;&#24320;&#21457;&#20986;&#20102;&#20843;&#31181;&#26377;&#21069;&#36884;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#29992;&#20110;&#26816;&#27979;&#28145;&#24230;&#20266;&#36896;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#21333;&#27169;&#22411;&#26816;&#27979;&#22120;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;AUC&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36824;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#20248;&#21155;&#21644;&#36229;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#30340;&#36805;&#36895;&#21457;&#23637;&#27491;&#22312;&#20005;&#37325;&#23041;&#32961;&#30528;&#23186;&#20307;&#20449;&#24687;&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#28436;&#36827;&#65292;&#29305;&#21035;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#32593;&#32476;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20843;&#31181;&#26377;&#21069;&#36884;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#35774;&#35745;&#21644;&#24320;&#21457;&#20102;&#25105;&#20204;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#30693;&#21517;&#30340;&#28145;&#24230;&#20266;&#36896;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#21333;&#27169;&#22411;&#26816;&#27979;&#22120;&#22312;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#21644;&#36328;&#25968;&#25454;&#38598;&#35780;&#20272;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;FF++ 2020&#12289;Google DFD&#12289;Celeb-DF&#12289;Deeper Forensics&#21644;DFDC&#28145;&#24230;&#20266;&#36896;&#30340;&#26816;&#27979;&#20013;&#65292;&#25105;&#20204;&#20998;&#21035;&#36798;&#21040;&#20102;88.74&#65285;&#12289;99.53&#65285;&#12289;97.68&#65285;&#12289;99.73&#65285;&#21644;92.02&#65285;&#30340;&#20934;&#30830;&#29575;&#21644;99.95&#65285;&#12289;100&#65285;&#12289;99.88&#65285;&#12289;99.99&#65285;&#21644;97.61&#65285;&#30340;AUC&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#24182;&#23637;&#31034;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#29420;&#29305;&#20248;&#21183;&#65292;&#24182;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid evolvement of deepfake creation technologies is seriously threating media information trustworthiness. The consequences impacting targeted individuals and institutions can be dire. In this work, we study the evolutions of deep learning architectures, particularly CNNs and Transformers. We identified eight promising deep learning architectures, designed and developed our deepfake detection models and conducted experiments over well-established deepfake datasets. These datasets included the latest second and third generation deepfake datasets. We evaluated the effectiveness of our developed single model detectors in deepfake detection and cross datasets evaluations. We achieved 88.74%, 99.53%, 97.68%, 99.73% and 92.02% accuracy and 99.95%, 100%, 99.88%, 99.99% and 97.61% AUC, in the detection of FF++ 2020, Google DFD, Celeb-DF, Deeper Forensics and DFDC deepfakes, respectively. We also identified and showed the unique strengths of CNNs and Transformers models and analysed the o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HumanLight&#30340;&#31639;&#27861;&#65292;&#37319;&#29992;&#20154;&#24615;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20013;&#28608;&#21169;&#22823;&#23478;&#25340;&#36710;&#65292;&#32531;&#35299;&#20132;&#36890;&#25317;&#22581;&#21644;&#20943;&#23569;&#27745;&#26579;&#12290;&#31639;&#27861;&#36890;&#36807;&#22870;&#21169;&#20056;&#22352;&#22823;&#23481;&#37327;&#36733;&#23458;&#24037;&#20855;&#30340;&#36890;&#21220;&#32773;&#65292;&#23454;&#29616;&#23545;&#32511;&#28783;&#26102;&#38388;&#30340;&#20844;&#24179;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2304.03697</link><description>&lt;p&gt;
&#20154;&#24615;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;&#65306;&#36890;&#36807;&#28608;&#21169;&#25340;&#36710;&#26469;&#32531;&#35299;&#20132;&#36890;&#25317;&#22581;&#21644;&#20943;&#23569;&#27745;&#26579;
&lt;/p&gt;
&lt;p&gt;
HumanLight: Incentivizing Ridesharing via Human-centric Deep Reinforcement Learning in Traffic Signal Control. (arXiv:2304.03697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HumanLight&#30340;&#31639;&#27861;&#65292;&#37319;&#29992;&#20154;&#24615;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20013;&#28608;&#21169;&#22823;&#23478;&#25340;&#36710;&#65292;&#32531;&#35299;&#20132;&#36890;&#25317;&#22581;&#21644;&#20943;&#23569;&#27745;&#26579;&#12290;&#31639;&#27861;&#36890;&#36807;&#22870;&#21169;&#20056;&#22352;&#22823;&#23481;&#37327;&#36733;&#23458;&#24037;&#20855;&#30340;&#36890;&#21220;&#32773;&#65292;&#23454;&#29616;&#23545;&#32511;&#28783;&#26102;&#38388;&#30340;&#20844;&#24179;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#20154;&#39550;&#36710;&#24050;&#25104;&#20026;&#35768;&#22810;&#36890;&#21220;&#32773;&#26368;&#21463;&#38738;&#30544;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#23548;&#33268;&#20132;&#36890;&#25317;&#22581;&#21644;&#31354;&#27668;&#27745;&#26579;&#38382;&#39064;&#21152;&#21095;&#12290;&#20449;&#24687;&#25216;&#26415;&#30340;&#36827;&#27493;&#20026;&#23454;&#29616;&#22478;&#24066;&#8220;&#36731;&#36710;&#21270;&#8221;&#25552;&#20379;&#20102;&#26426;&#36935;&#65292;&#21487;&#20197;&#36890;&#36807;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#28608;&#21169;&#25340;&#36710;&#21644;&#20999;&#25442;&#21040;&#22823;&#23481;&#37327;&#36733;&#23458;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;HumanLight&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#20998;&#25955;&#24335;&#33258;&#36866;&#24212;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31639;&#27861;&#65292;&#26088;&#22312;&#20248;&#21270;&#20132;&#21449;&#21475;&#30340;&#20154;&#21592;&#36890;&#34892;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#22120;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65292;&#22870;&#21169;&#20989;&#25968;&#23884;&#20837;&#20102;&#20197;&#20154;&#20026;&#26412;&#30340;&#20132;&#36890;&#27010;&#24565;&#12290;&#36890;&#36807;&#28608;&#21169;&#22823;&#23481;&#37327;&#36733;&#23458;&#24037;&#20855;&#30340;&#36890;&#21220;&#32773;&#21512;&#24182;&#20056;&#36710;&#20197;&#33410;&#32422;&#26102;&#38388;&#65292;HumanLight&#23454;&#29616;&#20102;&#23545;&#32511;&#28783;&#26102;&#38388;&#30340;&#20844;&#24179;&#20998;&#37197;&#12290;&#38500;&#20102;&#37319;&#29992;FRAP&#20316;&#20026;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#22806;&#65292;HumanLight&#36824;&#24341;&#20837;&#20102;&#8220;&#27963;&#36291;&#36710;&#36742;&#8221;&#36825;&#20010;&#27010;&#24565;&#65292;&#22823;&#33268;&#23450;&#20041;&#26159;&#20020;&#36817;&#20132;&#21449;&#21475;&#19988;&#21487;&#33021;&#24178;&#25200;&#20915;&#31574;&#36807;&#31243;&#30340;&#20219;&#20309;&#36710;&#36742;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single occupancy vehicles are the most attractive transportation alternative for many commuters, leading to increased traffic congestion and air pollution. Advancements in information technologies create opportunities for smart solutions that incentivize ridesharing and mode shift to higher occupancy vehicles (HOVs) to achieve the car lighter vision of cities. In this study, we present HumanLight, a novel decentralized adaptive traffic signal control algorithm designed to optimize people throughput at intersections. Our proposed controller is founded on reinforcement learning with the reward function embedding the transportation-inspired concept of pressure at the person-level. By rewarding HOV commuters with travel time savings for their efforts to merge into a single ride, HumanLight achieves equitable allocation of green times. Apart from adopting FRAP, a state-of-the-art (SOTA) base model, HumanLight introduces the concept of active vehicles, loosely defined as vehicles in proximit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;Monte Carlo Markov Chain&#25277;&#26679;&#31639;&#27861;&#20197;&#35299;&#20915;&#24120;&#29992;&#31639;&#27861;&#26080;&#27861;&#25910;&#25947;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#26032;&#22411;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#31934;&#24230;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20998;&#23376;&#38388;&#21147;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2304.03694</link><description>&lt;p&gt;
&#22522;&#20110;&#31561;&#21464;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#31934;&#24230;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20998;&#23376;&#38388;&#21147;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
High Accuracy Uncertainty-Aware Interatomic Force Modeling with Equivariant Bayesian Neural Networks. (arXiv:2304.03694v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;Monte Carlo Markov Chain&#25277;&#26679;&#31639;&#27861;&#20197;&#35299;&#20915;&#24120;&#29992;&#31639;&#27861;&#26080;&#27861;&#25910;&#25947;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#26032;&#22411;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#31934;&#24230;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20998;&#23376;&#38388;&#21147;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20026;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#12289;&#20027;&#21160;&#23398;&#20064;&#21644;&#32467;&#21512;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#65292;&#20294;&#24456;&#23569;&#26377;&#24212;&#29992;&#20110;&#20998;&#23376;&#38388;&#21147;&#24314;&#27169;&#30340;&#24773;&#20917;&#24471;&#21040;&#21457;&#29616;&#12290;&#20854;&#20013;&#20027;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#36866;&#21512;&#30340;MCMC&#25277;&#26679;&#31639;&#27861;&#26469;&#21462;&#24471;&#21518;&#39564;&#23494;&#24230;&#65292;&#22240;&#20026;&#24120;&#29992;&#31639;&#27861;&#23545;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#22312;&#23454;&#38469;&#26102;&#38388;&#20869;&#26080;&#27861;&#25910;&#25947;&#12290;&#20316;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;MCMC&#25277;&#26679;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#32469;&#36807;&#29616;&#26377;&#25277;&#26679;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;NequIP&#20307;&#31995;&#32467;&#26500;&#30340;&#26032;&#22411;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#24403;&#20854;&#19982;&#25105;&#20204;&#30340;&#26032;&#22411;&#25277;&#26679;&#31639;&#27861;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;&#21644;&#22909;&#30340;&#19981;&#30830;&#23450;&#24230;&#24230;&#37327;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even though Bayesian neural networks offer a promising framework for modeling uncertainty, active learning and incorporating prior physical knowledge, few applications of them can be found in the context of interatomic force modeling. One of the main challenges in their application to learning interatomic forces is the lack of suitable Monte Carlo Markov chain sampling algorithms for the posterior density, as the commonly used algorithms do not converge in a practical amount of time for many of the state-of-the-art architectures. As a response to this challenge, we introduce a new Monte Carlo Markov chain sampling algorithm in this paper which can circumvent the problems of the existing sampling methods. In addition, we introduce a new stochastic neural network model based on the NequIP architecture and demonstrate that, when combined with our novel sampling algorithm, we obtain predictions with state-of-the-art accuracy as well as a good measure of uncertainty.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#24694;&#24847;&#27969;&#37327;&#26816;&#27979;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#21152;&#23494;&#24694;&#24847;&#27969;&#37327;&#30340;&#29305;&#24449;&#21644;&#27010;&#24565;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26816;&#27979;&#26694;&#26550;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#26816;&#27979;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.03691</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21152;&#23494;&#24694;&#24847;&#27969;&#37327;&#26816;&#27979;&#20013;&#30340;&#29305;&#24449;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Feature Mining for Encrypted Malicious Traffic Detection with Deep Learning and Other Machine Learning Algorithms. (arXiv:2304.03691v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#24694;&#24847;&#27969;&#37327;&#26816;&#27979;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#21152;&#23494;&#24694;&#24847;&#27969;&#37327;&#30340;&#29305;&#24449;&#21644;&#27010;&#24565;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26816;&#27979;&#26694;&#26550;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#26816;&#27979;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#23494;&#26426;&#21046;&#30340;&#26222;&#21450;&#23545;&#24694;&#24847;&#27969;&#37327;&#26816;&#27979;&#25552;&#20986;&#20102;&#26497;&#22823;&#30340;&#25361;&#25112;&#65292;&#20256;&#32479;&#26816;&#27979;&#25216;&#26415;&#22312;&#27809;&#26377;&#23545;&#21152;&#23494;&#27969;&#37327;&#35299;&#23494;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#24037;&#20316;&#12290;&#24403;&#21069;&#65292;&#23545;&#20110;&#19981;&#35299;&#23494;&#30340;&#21152;&#23494;&#24694;&#24847;&#27969;&#37327;&#26816;&#27979;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#26426;&#22120;&#23398;&#20064;&#25110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#36873;&#25321;&#19978;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20379;&#27969;&#37327;&#29305;&#24449;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#27969;&#37327;&#29305;&#24449;&#21019;&#24314;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#21152;&#23494;&#24694;&#24847;&#27969;&#37327;&#20998;&#26512;&#35774;&#35745;&#30340;&#26032;&#27010;&#24565;&#21644;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21152;&#23494;&#24694;&#24847;&#27969;&#37327;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#20004;&#23618;&#30340;&#26816;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#65292;&#35813;&#26694;&#26550;&#22312;&#26816;&#27979;&#20934;&#30830;&#24230;&#21644;&#20551;&#38451;&#24615;&#29575;&#26041;&#38754;&#20248;&#20110;&#32463;&#20856;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;ResNet&#21644;&#38543;&#26426;&#26862;&#26519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of encryption mechanisms poses a great challenge to malicious traffic detection. The reason is traditional detection techniques cannot work without the decryption of encrypted traffic. Currently, research on encrypted malicious traffic detection without decryption has focused on feature extraction and the choice of machine learning or deep learning algorithms. In this paper, we first provide an in-depth analysis of traffic features and compare different state-of-the-art traffic feature creation approaches, while proposing a novel concept for encrypted traffic feature which is specifically designed for encrypted malicious traffic analysis. In addition, we propose a framework for encrypted malicious traffic detection. The framework is a two-layer detection framework which consists of both deep learning and traditional machine learning algorithms. Through comparative experiments, it outperforms classical deep learning and traditional machine learning algorithms, such as Res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#24102;&#38656;&#27714;&#30340;&#26426;&#22120;&#23398;&#20064;&#23459;&#35328;&#65292;&#35748;&#20026;&#38656;&#27714;&#23450;&#20041;&#21644;&#28385;&#36275;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#65288;i&#65289;&#38656;&#27714;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#25110;&#21487;&#20197;&#25104;&#21151;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#65288;iii&#65289;&#24573;&#30053;&#38656;&#27714;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37329;&#23383;&#22612;&#24335;&#24320;&#21457;&#27969;&#31243;&#65292;&#22312;&#20854;&#20013;&#65292;&#38656;&#27714;&#23450;&#20041;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#27969;&#31243;&#20013;&#25152;&#26377;&#21518;&#32493;&#38454;&#27573;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;</title><link>http://arxiv.org/abs/2304.03674</link><description>&lt;p&gt;
&#24102;&#38656;&#27714;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#20221;&#23459;&#35328;
&lt;/p&gt;
&lt;p&gt;
Machine Learning with Requirements: a Manifesto. (arXiv:2304.03674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#24102;&#38656;&#27714;&#30340;&#26426;&#22120;&#23398;&#20064;&#23459;&#35328;&#65292;&#35748;&#20026;&#38656;&#27714;&#23450;&#20041;&#21644;&#28385;&#36275;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#65288;i&#65289;&#38656;&#27714;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#25110;&#21487;&#20197;&#25104;&#21151;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#65288;iii&#65289;&#24573;&#30053;&#38656;&#27714;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37329;&#23383;&#22612;&#24335;&#24320;&#21457;&#27969;&#31243;&#65292;&#22312;&#20854;&#20013;&#65292;&#38656;&#27714;&#23450;&#20041;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#27969;&#31243;&#20013;&#25152;&#26377;&#21518;&#32493;&#38454;&#27573;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#38271;&#36275;&#30340;&#36827;&#27493;&#65292;&#25104;&#20026;&#35768;&#22810;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#31361;&#30772;&#30340;&#26681;&#28304;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#23427;&#20204;&#24212;&#29992;&#21040;&#39640;&#39118;&#38505;&#25110;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#23481;&#26131;&#21464;&#24471;&#33030;&#24369;&#21644;&#19981;&#21487;&#38752;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#38656;&#27714;&#23450;&#20041;&#21644;&#28385;&#36275;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#65288;i&#65289;&#38656;&#27714;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#25110;&#21487;&#20197;&#25104;&#21151;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#65288;iii&#65289;&#24573;&#30053;&#38656;&#27714;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#38656;&#27714;&#35268;&#26684;&#35828;&#26126;&#26377;&#30410;&#22320;&#25972;&#21512;&#21040;&#26631;&#20934;&#30340;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#27969;&#31243;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37329;&#23383;&#22612;&#24335;&#24320;&#21457;&#27969;&#31243;&#65292;&#22312;&#20854;&#20013;&#65292;&#38656;&#27714;&#23450;&#20041;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;&#27969;&#31243;&#20013;&#25152;&#26377;&#21518;&#32493;&#38454;&#27573;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the recent years, machine learning has made great advancements that have been at the root of many breakthroughs in different application domains. However, it is still an open issue how make them applicable to high-stakes or safety-critical application domains, as they can often be brittle and unreliable. In this paper, we argue that requirements definition and satisfaction can go a long way to make machine learning models even more fitting to the real world, especially in critical domains. To this end, we present two problems in which (i) requirements arise naturally, (ii) machine learning models are or can be fruitfully deployed, and (iii) neglecting the requirements can have dramatic consequences. We show how the requirements specification can be fruitfully integrated into the standard machine learning development pipeline, proposing a novel pyramid development process in which requirements definition may impact all the subsequent phases in the pipeline, and viceversa.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ARIA&#30340;&#27169;&#22359;&#65292;&#21487;&#22312;&#20219;&#20309;QD&#31639;&#27861;&#20013;&#25552;&#39640;&#23384;&#26723;&#20013;&#23384;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#27010;&#29575;&#21644;&#36866;&#24212;&#24230;&#36827;&#34892;&#21464;&#24322;&#65292;&#20174;&#32780;&#24212;&#23545;&#19981;&#21487;&#39044;&#27979;&#30340;&#22122;&#38899;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2304.03672</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#38752;&#36816;&#27668;&#65306;&#25552;&#39640;&#36136;&#37327;&#22810;&#26679;&#24615;&#35299;&#20915;&#26041;&#26696;&#22312;&#19981;&#21487;&#39044;&#27979;&#39046;&#22495;&#20013;&#30340;&#34892;&#20026;&#37325;&#22797;&#24615;
&lt;/p&gt;
&lt;p&gt;
Don't Bet on Luck Alone: Enhancing Behavioral Reproducibility of Quality-Diversity Solutions in Uncertain Domains. (arXiv:2304.03672v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ARIA&#30340;&#27169;&#22359;&#65292;&#21487;&#22312;&#20219;&#20309;QD&#31639;&#27861;&#20013;&#25552;&#39640;&#23384;&#26723;&#20013;&#23384;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#27010;&#29575;&#21644;&#36866;&#24212;&#24230;&#36827;&#34892;&#21464;&#24322;&#65292;&#20174;&#32780;&#24212;&#23545;&#19981;&#21487;&#39044;&#27979;&#30340;&#22122;&#38899;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#31639;&#27861;&#26088;&#22312;&#22312;&#32473;&#23450;&#25551;&#36848;&#31526;&#31354;&#38388;&#20013;&#29983;&#25104;&#20248;&#31168;&#35299;&#20915;&#26041;&#26696;&#30340;&#38598;&#21512;&#24182;&#26368;&#22823;&#21270;&#23427;&#20204;&#30340;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#19981;&#21487;&#39044;&#27979;&#30340;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#19968;&#35299;&#20915;&#26041;&#26696;&#22312;&#19981;&#21516;&#35780;&#20272;&#20013;&#30340;&#36866;&#24212;&#24230;&#21644;&#25551;&#36848;&#31526;&#21487;&#33021;&#20250;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#23548;&#33268;&#20272;&#35745;&#36825;&#20123;&#20540;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#37492;&#20110;QD&#31639;&#27861;&#30340;&#31934;&#33521;&#20027;&#20041;&#26412;&#36136;&#65292;&#22312;&#36825;&#20123;&#22024;&#26434;&#30340;&#29615;&#22659;&#19979;&#65292;&#23427;&#20204;&#36890;&#24120;&#20250;&#24471;&#21040;&#35768;&#22810;&#36864;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;&#26723;&#26696;&#21487;&#37325;&#29616;&#24615;&#25913;&#36827;&#31639;&#27861;&#8221;(ARIA)&#65307;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#23384;&#26723;&#20013;&#23384;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;&#25105;&#20204;&#23558;&#20854;&#25552;&#35758;&#20026;&#19968;&#31181;&#21333;&#29420;&#30340;&#20248;&#21270;&#27169;&#22359;&#65292;&#20381;&#36182;&#20110;&#33258;&#28982;&#36827;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;QD&#31639;&#27861;&#30340;&#39030;&#37096;&#25191;&#34892;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#23545;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#21464;&#24322;&#65292;&#20197;(1)&#20248;&#21270;&#20854;&#23646;&#20110;&#33258;&#24049;&#30340;&#39046;&#22495;&#30340;&#27010;&#29575;&#65292;&#21644;(2)&#26368;&#22823;&#21270;&#23427;&#20204;&#30340;&#36866;&#24212;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity (QD) algorithms are designed to generate collections of high-performing solutions while maximizing their diversity in a given descriptor space. However, in the presence of unpredictable noise, the fitness and descriptor of the same solution can differ significantly from one evaluation to another, leading to uncertainty in the estimation of such values. Given the elitist nature of QD algorithms, they commonly end up with many degenerate solutions in such noisy settings. In this work, we introduce Archive Reproducibility Improvement Algorithm (ARIA); a plug-and-play approach that improves the reproducibility of the solutions present in an archive. We propose it as a separate optimization module, relying on natural evolution strategies, that can be executed on top of any QD algorithm. Our module mutates solutions to (1) optimize their probability of belonging to their niche, and (2) maximize their fitness. The performance of our method is evaluated on various tasks, incl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#32553;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#20998;&#21306;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21644;&#24178;&#25200;&#30340;&#38750;&#32447;&#24615;&#21453;&#39304;&#22238;&#36335;&#20013;&#21306;&#38388;&#20540;&#40065;&#26834;&#21487;&#36798;&#38598;&#20272;&#35745;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27493;&#39588;&#21644;&#21487;&#36798;&#24615;&#20998;&#21306;&#23618;&#30340;&#35299;&#32806;&#65292;&#21487;&#20197;&#22312;&#24456;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25552;&#20379;&#31934;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.03671</link><description>&lt;p&gt;
&#22522;&#20110;&#25910;&#32553;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#20998;&#21306;&#27861;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#21487;&#36798;&#38598;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Contraction-Guided Adaptive Partitioning for Reachability Analysis of Neural Network Controlled Systems. (arXiv:2304.03671v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#32553;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#20998;&#21306;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21644;&#24178;&#25200;&#30340;&#38750;&#32447;&#24615;&#21453;&#39304;&#22238;&#36335;&#20013;&#21306;&#38388;&#20540;&#40065;&#26834;&#21487;&#36798;&#38598;&#20272;&#35745;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27493;&#39588;&#21644;&#21487;&#36798;&#24615;&#20998;&#21306;&#23618;&#30340;&#35299;&#32806;&#65292;&#21487;&#20197;&#22312;&#24456;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25552;&#20379;&#31934;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#32553;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#20998;&#21306;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#24102;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21644;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#21453;&#39304;&#22238;&#36335;&#20013;&#21306;&#38388;&#20540;&#40065;&#26834;&#21487;&#36798;&#38598;&#20272;&#35745;&#12290;&#31639;&#27861;&#26681;&#25454;&#36229;&#36924;&#36817;&#21306;&#38388;&#30340;&#25910;&#32553;&#36895;&#29575;&#20272;&#35745;&#26469;&#36873;&#25321;&#20309;&#26102;&#20309;&#22320;&#36827;&#34892;&#20998;&#21306;&#12290;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27493;&#39588;&#21644;&#21487;&#36798;&#24615;&#20998;&#21306;&#23618;&#30340;&#35299;&#32806;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#24456;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25552;&#20379;&#31934;&#24230;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#20855;&#26377;&#36275;&#22815;&#31934;&#24230;&#30340;&#24320;&#29615;&#21306;&#38388;&#20540;&#21487;&#36798;&#24615;&#20272;&#35745;&#25216;&#26415;&#21644;&#20219;&#20309;&#29992;&#20110;&#30028;&#23450;&#31070;&#32463;&#32593;&#32476;&#36755;&#20837;&#36755;&#20986;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#22522;&#20110;&#25910;&#32553;&#30340;&#40065;&#26834;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#20026;&#28151;&#21512;&#21333;&#35843;&#21487;&#36798;&#24615;&#31639;&#27861;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#27169;&#25311;&#26469;&#23637;&#31034;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a contraction-guided adaptive partitioning algorithm for improving interval-valued robust reachable set estimates in a nonlinear feedback loop with a neural network controller and disturbances. Based on an estimate of the contraction rate of over-approximated intervals, the algorithm chooses when and where to partition. Then, by leveraging a decoupling of the neural network verification step and reachability partitioning layers, the algorithm can provide accuracy improvements for little computational cost. This approach is applicable with any sufficiently accurate open-loop interval-valued reachability estimation technique and any method for bounding the input-output behavior of a neural network. Using contraction-based robustness analysis, we provide guarantees of the algorithm's performance with mixed monotone reachability. Finally, we demonstrate the algorithm's performance through several numerical simulations and compare it with existing methods in the li
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#19981;&#30830;&#23450;&#24615;&#19982;&#20844;&#24179;&#24615;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#20272;&#31639;&#26679;&#26412;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#21457;&#29616;&#20302;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#26356;&#20934;&#30830;&#21644;&#20844;&#24179;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23450;&#20041;&#30340;&#26032;&#30340;&#20844;&#24179;&#24615;-&#25928;&#29992;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.03646</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#23454;&#29616;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness through Aleatoric Uncertainty. (arXiv:2304.03646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03646
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#19981;&#30830;&#23450;&#24615;&#19982;&#20844;&#24179;&#24615;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#20272;&#31639;&#26679;&#26412;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#21457;&#29616;&#20302;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#26356;&#20934;&#30830;&#21644;&#20844;&#24179;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23450;&#20041;&#30340;&#26032;&#30340;&#20844;&#24179;&#24615;-&#25928;&#29992;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#20013;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#36890;&#24120;&#30456;&#20114;&#31454;&#20105;&#30340;&#30446;&#26631;&#12290; &#20844;&#24179;&#24615;&#30830;&#20445;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#24102;&#20559;&#35265;&#22320;&#38024;&#23545;&#20219;&#20309;&#29305;&#23450;&#32676;&#20307;&#65292;&#32780;&#25928;&#29992;&#21017;&#19987;&#27880;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#19981;&#30830;&#23450;&#24615;&#19982;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#26469;&#20272;&#31639;&#26679;&#26412;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#20272;&#31639;&#19982;&#21463;&#20445;&#25252;&#23646;&#24615;&#30456;&#20851;&#30340;&#28151;&#28102;&#25928;&#24212;&#26080;&#20851;&#12290;&#36890;&#36807;&#23454;&#35777;&#35777;&#25454;&#65292;&#25105;&#20204;&#34920;&#26126;&#20855;&#26377;&#20302;&#20998;&#31867;&#19981;&#30830;&#23450;&#24615;&#30340;&#26679;&#26412;&#27604;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#26679;&#26412;&#26356;&#20934;&#30830;&#21644;&#20844;&#24179;&#22320;&#24314;&#27169;&#65292;&#21487;&#33021;&#20855;&#26377;&#20559;&#24046;&#30340;&#34920;&#31034;&#21644;&#26356;&#39640;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#24179;&#34913;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23450;&#20041;&#30340;&#26032;&#30340;&#20844;&#24179;&#24615;-&#25928;&#29992;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a unique solution to tackle the often-competing goals of fairness and utility in machine learning classification tasks. While fairness ensures that the model's predictions are unbiased and do not discriminate against any particular group, utility focuses on maximizing the accuracy of the model's predictions. Our aim is to investigate the relationship between uncertainty and fairness. Our approach leverages this concept by employing Bayesian learning to estimate the uncertainty in sample predictions where the estimation is independent of confounding effects related to the protected attribute. Through empirical evidence, we show that samples with low classification uncertainty are modeled more accurately and fairly than those with high uncertainty, which may have biased representations and higher prediction errors. To address the challenge of balancing fairness and utility, we propose a novel fairness-utility objective that is defined based on uncertainty quantification. The w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;OBCD&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#27491;&#20132;&#32422;&#26463;&#30340;&#19968;&#33324;&#38750;&#20809;&#28369;&#32452;&#21512;&#38382;&#39064;&#12290; OBCD&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#33719;&#24471;&#20005;&#26684;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.03641</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#27491;&#20132;&#32422;&#26463;&#19979;&#30340;&#38750;&#20809;&#28369;&#32452;&#21512;&#20248;&#21270;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Block Coordinate Descent Method for Nonsmooth Composite Optimization under Orthogonality Constraints. (arXiv:2304.03641v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;OBCD&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#27491;&#20132;&#32422;&#26463;&#30340;&#19968;&#33324;&#38750;&#20809;&#28369;&#32452;&#21512;&#38382;&#39064;&#12290; OBCD&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#33719;&#24471;&#20005;&#26684;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#27491;&#20132;&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#32452;&#21512;&#20248;&#21270;&#22312;&#32479;&#35745;&#23398;&#20064;&#21644;&#25968;&#25454;&#31185;&#23398;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#30001;&#20110;&#20854;&#38750;&#20984;&#24615;&#21644;&#38750;&#20809;&#28369;&#24615;&#36136;&#65292;&#35813;&#38382;&#39064;&#36890;&#24120;&#24456;&#38590;&#27714;&#35299;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#21463;&#21040;&#20197;&#19979;&#19968;&#20010;&#25110;&#22810;&#20010;&#38480;&#21046;&#30340;&#38480;&#21046;&#65306;&#65288;i&#65289;&#23427;&#20204;&#26159;&#38656;&#35201;&#27599;&#27425;&#36845;&#20195;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#20840;&#26799;&#24230;&#26041;&#27861;&#65307;&#65288;ii&#65289;&#23427;&#20204;&#26080;&#27861;&#35299;&#20915;&#19968;&#33324;&#30340;&#38750;&#20809;&#28369;&#32452;&#21512;&#38382;&#39064;&#65307;&#65288;iii&#65289;&#23427;&#20204;&#26159;&#19981;&#21487;&#34892;&#26041;&#27861;&#65292;&#24182;&#19988;&#21482;&#33021;&#22312;&#26497;&#38480;&#28857;&#22788;&#23454;&#29616;&#35299;&#30340;&#21487;&#34892;&#24615;&#65307;&#65288;iv&#65289;&#23427;&#20204;&#32570;&#20047;&#20005;&#26684;&#30340;&#25910;&#25947;&#20445;&#35777;&#65307;&#65288;v&#65289;&#23427;&#20204;&#21482;&#33021;&#33719;&#24471;&#20851;&#38190;&#28857;&#30340;&#24369;&#26368;&#20248;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;OBCD&#65292;&#29992;&#20110;&#35299;&#20915;&#27491;&#20132;&#32422;&#26463;&#19979;&#30340;&#19968;&#33324;&#38750;&#20809;&#28369;&#32452;&#21512;&#38382;&#39064;&#12290;OBCD&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20250;&#26356;&#26032;...
&lt;/p&gt;
&lt;p&gt;
Nonsmooth composite optimization with orthogonality constraints has a broad spectrum of applications in statistical learning and data science. However, this problem is generally challenging to solve due to its non-convex and non-smooth nature. Existing solutions are limited by one or more of the following restrictions: (i) they are full gradient methods that require high computational costs in each iteration; (ii) they are not capable of solving general nonsmooth composite problems; (iii) they are infeasible methods and can only achieve the feasibility of the solution at the limit point; (iv) they lack rigorous convergence guarantees; (v) they only obtain weak optimality of critical points. In this paper, we propose \textit{\textbf{OBCD}}, a new Block Coordinate Descent method for solving general nonsmooth composite problems under Orthogonality constraints. \textit{\textbf{OBCD}} is a feasible method with low computation complexity footprints. In each iteration, our algorithm updates $
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDiSC&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#26465;&#20214;&#19979;&#65292;&#21516;&#26102;&#26816;&#27979;&#30005;&#21147;&#31995;&#32479;&#24178;&#25200;&#21644;&#32593;&#32476;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.03640</link><description>&lt;p&gt;
FedDiSC: &#19968;&#31181;&#38754;&#21521;&#30005;&#21147;&#31995;&#32479;&#24178;&#25200;&#21644;&#32593;&#32476;&#25915;&#20987;&#21306;&#20998;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedDiSC: A Computation-efficient Federated Learning Framework for Power Systems Disturbance and Cyber Attack Discrimination. (arXiv:2304.03640v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDiSC&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#26465;&#20214;&#19979;&#65292;&#21516;&#26102;&#26816;&#27979;&#30005;&#21147;&#31995;&#32479;&#24178;&#25200;&#21644;&#32593;&#32476;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#26234;&#33021;&#30005;&#32593;&#31995;&#32479;&#23433;&#20840;&#21644;&#38544;&#31169;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#65292;&#38024;&#23545;&#20851;&#38190;&#30005;&#32593;&#32452;&#20214;&#65288;&#20363;&#22914;&#29366;&#24577;&#20272;&#35745;&#65289;&#30340;&#32593;&#32476;&#25915;&#20987;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#32593;&#32476;&#23433;&#20840;&#38382;&#39064;&#30340;&#37325;&#28857;&#65292;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#38754;&#20020;&#30528;&#26032;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#38544;&#31169;&#20445;&#25252;&#21644;&#20855;&#26377;&#25112;&#30053;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#20998;&#25955;&#30005;&#21147;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25216;&#26415;&#29942;&#39048;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#25915;&#20987;&#26816;&#27979;&#26694;&#26550;&#65292;&#31216;&#20026; FedDiSC&#65292;&#23427;&#33021;&#22815;&#21306;&#20998;&#30005;&#21147;&#31995;&#32479;&#24178;&#25200;&#21644;&#32593;&#32476;&#25915;&#20987;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20998;&#25955;&#24335;&#30005;&#32593;&#21306;&#22495;&#30340;&#30417;&#35270;&#21644;&#25968;&#25454;&#37319;&#38598;&#23376;&#31995;&#32479;&#33021;&#22815;&#21327;&#20316;&#35757;&#32451;&#19968;&#31181;&#25915;&#20987;&#26816;&#27979;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#25935;&#24863;&#30340;&#30005;&#21147;&#30456;&#20851;&#25968;&#25454;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#25552;&#21462;&#21028;&#21035;&#29305;&#24449;&#65292;&#33021;&#22815;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;&#25915;&#20987;&#26816;&#27979;&#27169;&#22411;&#65292;&#20197;&#20351;&#27599;&#20010;&#26412;&#22320;&#31995;&#32479;&#33021;&#22815;&#36827;&#34892;&#39640;&#25928;&#30340;&#25512;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#23454;&#38469;&#26234;&#33021;&#30005;&#32593;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;FedDiSC&#26694;&#26550;&#65292;&#24182;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#22312;&#38544;&#31169;&#20445;&#25252;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing concern about the security and privacy of smart grid systems, cyberattacks on critical power grid components, such as state estimation, have proven to be one of the top-priority cyber-related issues and have received significant attention in recent years. However, cyberattack detection in smart grids now faces new challenges, including privacy preservation and decentralized power zones with strategic data owners. To address these technical bottlenecks, this paper proposes a novel Federated Learning-based privacy-preserving and communication-efficient attack detection framework, known as FedDiSC, that enables Discrimination between power System disturbances and Cyberattacks. Specifically, we first propose a Federated Learning approach to enable Supervisory Control and Data Acquisition subsystems of decentralized power grid zones to collaboratively train an attack detection model without sharing sensitive power related data. Secondly, we put forward a representation lear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#31616;&#21333;&#30340;&#32447;&#24615;&#21333;&#20803;RNN&#22312;&#38271;&#24207;&#21015;&#35745;&#25968;&#38382;&#39064;&#19978;&#30340;&#26497;&#38480;&#65292;&#29702;&#35770;&#19978;&#20351;&#29992;&#30340;&#26465;&#20214;&#20855;&#26377;&#20805;&#20998;&#24517;&#35201;&#24615;&#65307;&#23454;&#39564;&#25968;&#25454;&#34920;&#26126;&#65292;&#36890;&#36807;&#26631;&#20934;&#30340;&#26041;&#27861;&#65292;&#35813;&#32593;&#32476;&#36890;&#24120;&#26080;&#27861;&#23454;&#29616;&#20934;&#30830;&#30340;&#35745;&#25968;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.03639</link><description>&lt;p&gt;
&#32447;&#24615;&#36882;&#24402;&#32593;&#32476;&#22312;&#38271;&#24207;&#21015;&#35745;&#25968;&#38382;&#39064;&#19978;&#30340;&#29702;&#35770;&#19982;&#23454;&#39564;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
Theoretical Conditions and Empirical Failure of Bracket Counting on Long Sequences with Linear Recurrent Networks. (arXiv:2304.03639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#31616;&#21333;&#30340;&#32447;&#24615;&#21333;&#20803;RNN&#22312;&#38271;&#24207;&#21015;&#35745;&#25968;&#38382;&#39064;&#19978;&#30340;&#26497;&#38480;&#65292;&#29702;&#35770;&#19978;&#20351;&#29992;&#30340;&#26465;&#20214;&#20855;&#26377;&#20805;&#20998;&#24517;&#35201;&#24615;&#65307;&#23454;&#39564;&#25968;&#25454;&#34920;&#26126;&#65292;&#36890;&#36807;&#26631;&#20934;&#30340;&#26041;&#27861;&#65292;&#35813;&#32593;&#32476;&#36890;&#24120;&#26080;&#27861;&#23454;&#29616;&#20934;&#30830;&#30340;&#35745;&#25968;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;&#26080;&#38480;&#28608;&#27963;&#20989;&#25968;&#30340;RNN&#26377;&#35745;&#25968;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;RNN&#30340;&#26377;&#25928;&#35757;&#32451;&#24448;&#24448;&#22256;&#38590;&#65292;&#36890;&#24120;&#26080;&#27861;&#23398;&#20064;&#20934;&#30830;&#30340;&#35745;&#25968;&#34892;&#20026;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26368;&#31616;&#21333;&#30340;&#32447;&#24615;&#21333;&#20803;RNN&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#32447;&#24615;RNN&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20102;&#27169;&#22411;&#23637;&#29616;&#31934;&#30830;&#35745;&#25968;&#34892;&#20026;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#26465;&#20214;&#26159;&#24517;&#35201;&#19988;&#20805;&#20998;&#30340;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#28041;&#21450;&#31867;&#20284;Dyck-1&#24179;&#34913;&#31526;&#21495;&#30340;&#20219;&#21153;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#32447;&#24615;RNN&#36890;&#24120;&#26080;&#27861;&#22312;&#26631;&#20934;&#26041;&#27861;&#35757;&#32451;&#19979;&#28385;&#36275;&#35745;&#25968;&#34892;&#20026;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#24207;&#21015;&#38271;&#24230;&#21644;&#21033;&#29992;&#19981;&#21516;&#30340;&#30446;&#26631;&#31867;&#21035;&#23545;&#27169;&#22411;&#34892;&#20026;&#22312;&#35757;&#32451;&#26399;&#38388;&#21644;&#35745;&#25968;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work has established that RNNs with an unbounded activation function have the capacity to count exactly. However, it has also been shown that RNNs are challenging to train effectively and generally do not learn exact counting behaviour. In this paper, we focus on this problem by studying the simplest possible RNN, a linear single-cell network. We conduct a theoretical analysis of linear RNNs and identify conditions for the models to exhibit exact counting behaviour. We provide a formal proof that these conditions are necessary and sufficient. We also conduct an empirical analysis using tasks involving a Dyck-1-like Balanced Bracket language under two different settings. We observe that linear RNNs generally do not meet the necessary and sufficient conditions for counting behaviour when trained with the standard approach. We investigate how varying the length of training sequences and utilising different target classes impacts model behaviour during training and the ability of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#36848;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#30340;&#32593;&#32476;&#22914;&#20309;&#21512;&#20316;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#36890;&#20449;&#32422;&#26463;&#12289;&#33258;&#36866;&#24212;&#21644;&#21512;&#20316;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#36798;&#21040;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20998;&#24067;&#24335;&#22238;&#24402;&#38382;&#39064;&#30340;&#22522;&#26412;&#23646;&#24615;&#19982;&#26368;&#20248;&#20998;&#37197;&#36890;&#20449;&#36164;&#28304;&#20043;&#38388;&#30340;&#23450;&#37327;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.03638</link><description>&lt;p&gt;
&#21387;&#32553;&#22238;&#24402;&#19982;&#33258;&#36866;&#24212;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Compressed Regression over Adaptive Networks. (arXiv:2304.03638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#30340;&#32593;&#32476;&#22914;&#20309;&#21512;&#20316;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#36890;&#20449;&#32422;&#26463;&#12289;&#33258;&#36866;&#24212;&#21644;&#21512;&#20316;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#36798;&#21040;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20998;&#24067;&#24335;&#22238;&#24402;&#38382;&#39064;&#30340;&#22522;&#26412;&#23646;&#24615;&#19982;&#26368;&#20248;&#20998;&#37197;&#36890;&#20449;&#36164;&#28304;&#20043;&#38388;&#30340;&#23450;&#37327;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#32593;&#32476;&#22312;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#26102;&#65292;&#22312;&#36890;&#20449;&#32422;&#26463;&#12289;&#33258;&#36866;&#24212;&#21644;&#21512;&#20316;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#36798;&#21040;&#30340;&#24615;&#33021;&#12290;&#26234;&#33021;&#20307;&#20351;&#29992;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340; ACTC (adapt-compress-then-combine) &#25193;&#25955;&#31574;&#30053;&#65292;&#22312;&#36825;&#20010;&#31574;&#30053;&#20013;&#65292;&#37051;&#36817;&#26234;&#33021;&#20307;&#20132;&#25442;&#30340;&#20449;&#21495;&#34987;&#38543;&#26426;&#19981;&#21516;&#21387;&#32553;&#31639;&#23376;&#32534;&#30721;&#12290;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#22343;&#26041;&#20272;&#35745;&#35823;&#24046;&#30340;&#29305;&#24449;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#39033;&#19982;&#27809;&#26377;&#36890;&#20449;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#20307;&#23558;&#35201;&#36798;&#21040;&#30340;&#35823;&#24046;&#26377;&#20851;&#30340;&#38169;&#35823;&#39033;&#65292;&#20197;&#21450;&#19968;&#39033;&#30001;&#20110;&#21387;&#32553;&#32780;&#20135;&#29983;&#30340;&#35823;&#24046;&#39033;&#12290;&#20998;&#26512;&#25581;&#31034;&#20102;&#20998;&#24067;&#24335;&#22238;&#24402;&#38382;&#39064;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;Perron&#29305;&#24449;&#21521;&#37327;&#24341;&#36215;&#30340;&#26799;&#24230;&#22122;&#22768;&#21644;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65288;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30693;&#26195;&#36825;&#20123;&#20851;&#31995;&#23545;&#20110;&#26368;&#20248;&#22320;&#20998;&#37197;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36890;&#20449;&#36164;&#28304;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we derive the performance achievable by a network of distributed agents that solve, adaptively and in the presence of communication constraints, a regression problem. Agents employ the recently proposed ACTC (adapt-compress-then-combine) diffusion strategy, where the signals exchanged locally by neighboring agents are encoded with randomized differential compression operators. We provide a detailed characterization of the mean-square estimation error, which is shown to comprise a term related to the error that agents would achieve without communication constraints, plus a term arising from compression. The analysis reveals quantitative relationships between the compression loss and fundamental attributes of the distributed regression problem, in particular, the stochastic approximation error caused by the gradient noise and the network topology (through the Perron eigenvector). We show that knowledge of such relationships is critical to allocate optimally the communication
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#8212;&#8212;&#24322;&#27493;&#32852;&#37030;&#36830;&#32493;&#23398;&#20064;(AFCL)&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#21407;&#22411;&#30340;&#23398;&#20064;&#12289;&#34920;&#31034;&#25439;&#22833;&#12289;&#20998;&#24418;&#39044;&#35757;&#32451;&#20197;&#21450;&#20462;&#25913;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#21517;&#20026;FedSpace&#12290;&#36890;&#36807;&#22312;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#32852;&#37030;&#21010;&#20998;&#19979;&#65292;&#20998;&#21035;&#20351;&#29992;50&#12289;100&#21644;500&#20010;&#23458;&#25143;&#31471;&#65292;&#24471;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03626</link><description>&lt;p&gt;
&#24322;&#27493;&#32852;&#37030;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Federated Continual Learning. (arXiv:2304.03626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03626
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#8212;&#8212;&#24322;&#27493;&#32852;&#37030;&#36830;&#32493;&#23398;&#20064;(AFCL)&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#21407;&#22411;&#30340;&#23398;&#20064;&#12289;&#34920;&#31034;&#25439;&#22833;&#12289;&#20998;&#24418;&#39044;&#35757;&#32451;&#20197;&#21450;&#20462;&#25913;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#21517;&#20026;FedSpace&#12290;&#36890;&#36807;&#22312;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#32852;&#37030;&#21010;&#20998;&#19979;&#65292;&#20998;&#21035;&#20351;&#29992;50&#12289;100&#21644;500&#20010;&#23458;&#25143;&#31471;&#65292;&#24471;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#30340;&#31867;&#21035;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20551;&#23450;&#30475;&#21040;&#19968;&#32452;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#25353;&#29031;&#22266;&#23450;&#30340;&#12289;&#39044;&#23450;&#20041;&#30340;&#39034;&#24207;&#19968;&#20010;&#25509;&#19968;&#20010;&#22320;&#20986;&#29616;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#36825;&#19981;&#22826;&#29616;&#23454;&#65292;&#22240;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#37117;&#26159;&#29420;&#31435;&#22320;&#20197;&#24322;&#27493;&#26041;&#24335;&#24037;&#20316;&#65292;&#26681;&#25454;&#23436;&#20840;&#19981;&#30456;&#20851;&#30340;&#26102;&#38388;&#27573;&#21644;&#39034;&#24207;&#33719;&#21462;&#19981;&#21516;&#20219;&#21153;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65288;AFCL&#65289;&#65292;&#20854;&#20013;&#22810;&#20010;&#20219;&#21153;&#30340;&#36830;&#32493;&#23398;&#20064;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#39034;&#24207;&#21644;&#24322;&#27493;&#26102;&#38388;&#27573;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#21407;&#22411;&#30340;&#23398;&#20064;&#12289;&#34920;&#31034;&#25439;&#22833;&#12289;&#20998;&#24418;&#39044;&#35757;&#32451;&#21644;&#20462;&#25913;&#30340;&#32858;&#21512;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;FedSpace&#65292;&#22312;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#20197;3&#31181;&#19981;&#21516;&#30340;&#32852;&#37030;&#21010;&#20998;&#20026;50&#12289;100&#21644;500&#20010;&#23458;&#25143;&#31471;&#20316;&#20026;&#23454;&#39564;&#65292;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard class-incremental continual learning setting assumes a set of tasks seen one after the other in a fixed and predefined order. This is not very realistic in federated learning environments where each client works independently in an asynchronous manner getting data for the different tasks in time-frames and orders totally uncorrelated with the other ones. We introduce a novel federated learning setting (AFCL) where the continual learning of multiple tasks happens at each client with different orderings and in asynchronous time slots. We tackle this novel task using prototype-based learning, a representation loss, fractal pre-training, and a modified aggregation policy. Our approach, called FedSpace, effectively tackles this task as shown by the results on the CIFAR-100 dataset using 3 different federated splits with 50, 100, and 500 clients, respectively. The code and federated splits are available at https://github.com/LTTM/FedSpace.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#23457;&#33258;&#21160;&#25552;&#31034;&#25216;&#26415;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;K-shot&#23398;&#20064;&#35774;&#32622;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#24182;&#19981;&#33021;&#22987;&#32456;&#20248;&#20110;&#25163;&#21160;&#25552;&#31034;&#65292;&#22240;&#27492;&#25163;&#21160;&#25552;&#31034;&#24212;&#35813;&#20316;&#20026;&#33258;&#21160;&#25552;&#31034;&#30340;&#19968;&#20010;&#22522;&#20934;&#32447;&#12290;</title><link>http://arxiv.org/abs/2304.03609</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#33258;&#21160;&#25552;&#31034;&#65306;&#25105;&#20204;&#30495;&#30340;&#20570;&#24471;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Revisiting Automated Prompting: Are We Actually Doing Better?. (arXiv:2304.03609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#23457;&#33258;&#21160;&#25552;&#31034;&#25216;&#26415;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;K-shot&#23398;&#20064;&#35774;&#32622;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#24182;&#19981;&#33021;&#22987;&#32456;&#20248;&#20110;&#25163;&#21160;&#25552;&#31034;&#65292;&#22240;&#27492;&#25163;&#21160;&#25552;&#31034;&#24212;&#35813;&#20316;&#20026;&#33258;&#21160;&#25552;&#31034;&#30340;&#19968;&#20010;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26159;&#20986;&#33394;&#30340;&#20960;&#20046;&#19981;&#29992;&#23398;&#20064;&#30340;&#23398;&#20064;&#32773;&#65292;&#22312;&#20960;&#20046;&#19981;&#29992;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#31034;&#26174;&#30528;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#38543;&#21518;&#36827;&#34892;&#20102;&#35797;&#22270;&#33258;&#21160;&#21270;&#20154;&#31867;&#25552;&#31034;&#30340;&#23581;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#38543;&#21518;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;K-shot&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#33258;&#21160;&#21270;&#21487;&#20197;&#20248;&#20110;&#24494;&#35843;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#21160;&#25552;&#31034;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#26356;&#22823;&#33539;&#22260;&#30340;K-shot&#23398;&#20064;&#35774;&#32622;&#19978;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#33258;&#21160;&#25552;&#31034;&#19981;&#33021;&#22987;&#32456;&#20248;&#20110;&#31616;&#21333;&#30340;&#25163;&#21160;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#38500;&#20102;&#24494;&#35843;&#20043;&#22806;&#65292;&#25163;&#21160;&#25552;&#31034;&#24212;&#20316;&#20026;&#22522;&#32447;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners, and prompting significantly increases their performance on a range of downstream tasks in a few-shot learning setting. An attempt to automate human-led prompting followed, with some progress achieved. In particular, subsequent work demonstrates automation can outperform fine-tuning in certain K-shot learning scenarios.  In this paper, we revisit techniques for automated prompting on six different downstream tasks and a larger range of K-shot learning settings. We find that automated prompting does not consistently outperform simple manual prompts. Our work suggests that, in addition to fine-tuning, manual prompts should be used as a baseline in this line of research.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30896;&#25758;&#27010;&#29575;&#30340;&#26080;&#22270;Crowd Navigation&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26469;&#24863;&#30693;&#20154;&#32676;&#30340;&#21361;&#38505;&#31243;&#24230;&#65292;&#30830;&#20445;&#26426;&#22120;&#20154;&#22312;&#36890;&#36807;&#25317;&#25380;&#29615;&#22659;&#26102;&#30340;&#23433;&#20840;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03593</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#22270;Crowd Navigation&#19982;&#24863;&#30693;&#39118;&#38505;&#25511;&#21046;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning-Based Mapless Crowd Navigation with Perceived Risk of the Moving Crowd for Mobile Robots. (arXiv:2304.03593v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30896;&#25758;&#27010;&#29575;&#30340;&#26080;&#22270;Crowd Navigation&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26469;&#24863;&#30693;&#20154;&#32676;&#30340;&#21361;&#38505;&#31243;&#24230;&#65292;&#30830;&#20445;&#26426;&#22120;&#20154;&#22312;&#36890;&#36807;&#25317;&#25380;&#29615;&#22659;&#26102;&#30340;&#23433;&#20840;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#22320;&#22270;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#26041;&#27861;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#24448;&#24448;&#20250;&#36935;&#21040;&#8220;&#20923;&#32467;&#26426;&#22120;&#20154;&#38382;&#39064;&#8221;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#27492;&#38382;&#39064;&#65292;&#20294;&#26159;&#23384;&#22312;&#27867;&#21270;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#8220;&#30896;&#25758;&#27010;&#29575;&#8221;&#26469;&#24110;&#21161;&#26426;&#22120;&#20154;&#23433;&#20840;&#36890;&#36807;&#20154;&#32676;&#30340;&#26041;&#27861;&#12290;&#23558;&#8220;&#30896;&#25758;&#27010;&#29575;&#8221;&#21253;&#25324;&#22312;&#35266;&#23519;&#31354;&#38388;&#20013;&#65292;&#32473;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#20010;&#24863;&#30693;&#31227;&#21160;&#20154;&#32676;&#30340;&#21361;&#38505;&#31243;&#24230;&#30340;&#33021;&#21147;&#12290;&#26426;&#22120;&#20154;&#20250;&#22312;&#30475;&#20284;&#23433;&#20840;&#30340;&#24773;&#20917;&#19979;&#31359;&#36807;&#20154;&#32676;&#65292;&#20294;&#22312;&#20154;&#32676;&#31227;&#21160;&#36807;&#20110;&#28608;&#28872;&#26102;&#20250;&#32469;&#36335;&#12290;&#36890;&#36807;&#20851;&#27880;&#26368;&#21361;&#38505;&#30340;&#38556;&#30861;&#29289;&#65292;&#26426;&#22120;&#20154;&#19981;&#20250;&#22312;&#20154;&#32676;&#23494;&#24230;&#36739;&#39640;&#26102;&#28151;&#28102;&#65292;&#30830;&#20445;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24320;&#21457;&#65292;&#24182;&#22312;Gazebo&#27169;&#25311;&#22120;&#20013;&#36827;&#34892;&#20102;&#38750;&#21512;&#20316;&#20154;&#32676;&#29615;&#22659;&#20013;&#30340;&#35757;&#32451;&#65292;&#20854;&#20013;&#30340;&#38556;&#30861;&#29289;&#20197;&#38543;&#26426;&#36895;&#24230;&#31227;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical map-based navigation methods are commonly used for robot navigation, but they often struggle in crowded environments due to the Frozen Robot Problem (FRP). Deep reinforcement learning-based methods address the FRP problem, however, suffer from the issues of generalization and scalability. To overcome these challenges, we propose a method that uses Collision Probability (CP) to help the robot navigate safely through crowds. The inclusion of CP in the observation space gives the robot a sense of the level of danger of the moving crowd. The robot will navigate through the crowd when it appears safe but will take a detour when the crowd is moving aggressively. By focusing on the most dangerous obstacle, the robot will not be confused when the crowd density is high, ensuring scalability of the model. Our approach was developed using deep reinforcement learning (DRL) and trained using the Gazebo simulator in a non cooperative crowd environment with obstacles moving at randomized sp
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#26631;&#31614;&#21487;&#35266;&#27979;&#12289;&#33410;&#28857;&#19981;&#21487;&#35266;&#27979;&#19979;&#30340;&#20108;&#37096;&#22270;&#22270;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#20998;&#27573;&#24120;&#25968;&#21644;H\"older&#36830;&#32493;&#22270;&#35889;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#20102;&#26377;&#38480;&#30340;&#26679;&#26412;&#39118;&#38505;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2304.03590</link><description>&lt;p&gt;
&#26631;&#31614;&#21487;&#35266;&#27979;&#12289;&#33410;&#28857;&#19981;&#21487;&#35266;&#27979;&#19979;&#30340;&#20108;&#37096;&#22270;&#22270;&#20272;&#35745;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graphon Estimation in bipartite graphs with observable edge labels and unobservable node labels. (arXiv:2304.03590v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03590
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#26631;&#31614;&#21487;&#35266;&#27979;&#12289;&#33410;&#28857;&#19981;&#21487;&#35266;&#27979;&#19979;&#30340;&#20108;&#37096;&#22270;&#22270;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#20998;&#27573;&#24120;&#25968;&#21644;H\"older&#36830;&#32493;&#22270;&#35889;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#20102;&#26377;&#38480;&#30340;&#26679;&#26412;&#39118;&#38505;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25968;&#25454;&#38598;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#20010;&#30697;&#38453;&#65292;&#20854;&#26465;&#30446;&#23545;&#24212;&#20110;&#19981;&#21516;&#31867;&#22411;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#65288;&#32593;&#39029;&#29992;&#25143;&#35775;&#38382;&#32593;&#39029;&#30340;&#27425;&#25968;&#12289;&#23398;&#29983;&#26576;&#31185;&#30446;&#30340;&#25104;&#32489;&#12289;&#24739;&#32773;&#23545;&#21307;&#29983;&#30340;&#35780;&#20215;&#31561;&#65289;&#12290;&#26412;&#25991;&#20551;&#35774;&#19978;&#36848;&#20132;&#20114;&#26159;&#30001;&#25551;&#36848;&#27599;&#20010;&#23454;&#20307;&#30340;&#19981;&#21487;&#35266;&#27979;&#28508;&#22312;&#21464;&#37327;&#30830;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20272;&#35745;&#32473;&#23450;&#19981;&#21487;&#35266;&#27979;&#21464;&#37327;&#30340;&#25968;&#25454;&#30697;&#38453;&#30340;&#26465;&#20214;&#26399;&#26395;&#12290;&#36825;&#34987;&#34920;&#31034;&#20026;&#20272;&#35745;&#31216;&#20026;&#22270;&#35889;&#30340;&#21452;&#21464;&#37327;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#27573;&#24120;&#25968;&#21644;H\"older&#36830;&#32493;&#22270;&#35889;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20026;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#21644;&#25351;&#25968;&#21152;&#26435;&#32858;&#21512;&#24314;&#31435;&#20102;&#26377;&#38480;&#26679;&#26412;&#39118;&#38505;&#30028;&#38480;&#12290;&#36825;&#20123;&#30028;&#38480;&#24378;&#35843;&#20102;&#20272;&#35745;&#35823;&#24046;&#19982;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#20132;&#20114;&#24378;&#24230;&#26368;&#22823;&#20540;&#21644;&#22122;&#22768;&#27700;&#24179;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#30001;&#20110;&#20998;&#26512;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#38590;&#20197;&#22788;&#29702;&#65292;
&lt;/p&gt;
&lt;p&gt;
Many real-world data sets can be presented in the form of a matrix whose entries correspond to the interaction between two entities of different natures (number of times a web user visits a web page, a student's grade in a subject, a patient's rating of a doctor, etc.). We assume in this paper that the mentioned interaction is determined by unobservable latent variables describing each entity. Our objective is to estimate the conditional expectation of the data matrix given the unobservable variables. This is presented as a problem of estimation of a bivariate function referred to as graphon. We study the cases of piecewise constant and H\"older-continuous graphons. We establish finite sample risk bounds for the least squares estimator and the exponentially weighted aggregate. These bounds highlight the dependence of the estimation error on the size of the data set, the maximum intensity of the interactions, and the level of noise. As the analyzed least-squares estimator is intractable
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#24050;&#26377;&#19981;&#23569;&#25104;&#26524;&#65292;&#20294;&#23578;&#32570;&#20047;&#20840;&#38754;&#24635;&#32467;&#12290;&#26412;&#25991;&#32508;&#36848;&#20998;&#20026;&#25968;&#25454;&#20013;&#24515;&#21270;&#12289;&#27169;&#22411;&#20013;&#24515;&#21270;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#21644;&#35757;&#32451;&#31574;&#30053;&#31561;&#20116;&#20010;&#26041;&#38754;&#65292;&#27604;&#36739;&#35814;&#23613;&#22320;&#22238;&#39038;&#20102;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#22522;&#26412;&#32452;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.03589</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
On Efficient Training of Large-Scale Deep Learning Models: A Literature Review. (arXiv:2304.03589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03589
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#24050;&#26377;&#19981;&#23569;&#25104;&#26524;&#65292;&#20294;&#23578;&#32570;&#20047;&#20840;&#38754;&#24635;&#32467;&#12290;&#26412;&#25991;&#32508;&#36848;&#20998;&#20026;&#25968;&#25454;&#20013;&#24515;&#21270;&#12289;&#27169;&#22411;&#20013;&#24515;&#21270;&#12289;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#21644;&#35757;&#32451;&#31574;&#30053;&#31561;&#20116;&#20010;&#26041;&#38754;&#65292;&#27604;&#36739;&#35814;&#23613;&#22320;&#22238;&#39038;&#20102;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#22522;&#26412;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#37319;&#29992;&#22823;&#35268;&#27169;&#27169;&#22411;&#24182;&#22312;&#28023;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#22686;&#24378;&#24037;&#19994;&#29983;&#20135;&#21147;&#65292;&#20419;&#36827;&#31038;&#20250;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23545;&#35745;&#31639;&#33021;&#21147;&#35201;&#27714;&#30340;&#22686;&#21152;&#65292;&#23613;&#31649;&#26377;&#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#23545;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21152;&#36895;&#25216;&#26415;&#30340;&#20840;&#38754;&#24635;&#32467;&#20173;&#28982;&#22791;&#21463;&#26399;&#24453;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#22238;&#39038;&#20102;&#35757;&#32451;&#21152;&#36895;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#23558;&#22522;&#26412;&#32452;&#20214;&#20998;&#20026;&#20116;&#20010;&#20027;&#35201;&#35270;&#35282;&#65306;&#65288;1&#65289;&#25968;&#25454;&#20013;&#24515;&#21270;&#65306;&#21253;&#25324;&#25968;&#25454;&#38598;&#27491;&#21017;&#21270;&#12289;&#25968;&#25454;&#37319;&#26679;&#21644;&#25968;&#25454;&#20013;&#24515;&#21270;&#35838;&#31243;&#23398;&#20064;&#25216;&#26415;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25968;&#25454;&#26679;&#26412;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65307;&#65288;2&#65289;&#27169;&#22411;&#20013;&#24515;&#21270;&#65306;&#21253;&#25324;&#22522;&#26412;&#27169;&#22359;&#30340;&#21152;&#36895;&#12289;&#27169;&#22411;&#21387;&#32553;&#21644;&#27169;&#22411;&#33976;&#39311;&#25216;&#26415;&#65307;(&#21097;&#19979;&#21516;&#19978;&#21407;&#25991;)
&lt;/p&gt;
&lt;p&gt;
The field of deep learning has witnessed significant progress, particularly in computer vision (CV), natural language processing (NLP), and speech. The use of large-scale models trained on vast amounts of data holds immense promise for practical applications, enhancing industrial productivity and facilitating social development. With the increasing demands on computational capacity, though numerous studies have explored the efficient training, a comprehensive summarization on acceleration techniques of training deep learning models is still much anticipated. In this survey, we present a detailed review for training acceleration. We consider the fundamental update formulation and split its basic components into five main perspectives: (1) data-centric: including dataset regularization, data sampling, and data-centric curriculum learning techniques, which can significantly reduce the computational complexity of the data samples; (2) model-centric, including acceleration of basic modules,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;ID&#30340;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#34920;&#31034;&#26469;&#36827;&#34892;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;DCASE 2020 Challenge Task2 &#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#25110;&#33258;&#30417;&#30563;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03588</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;ID&#30340;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#34920;&#31034;&#36827;&#34892;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anomalous Sound Detection using Audio Representation with Machine ID based Contrastive Learning Pretraining. (arXiv:2304.03588v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;ID&#30340;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#34920;&#31034;&#26469;&#36827;&#34892;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;DCASE 2020 Challenge Task2 &#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#25110;&#33258;&#30417;&#30563;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#29992;&#20110;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#26679;&#26412;&#22686;&#24378;&#30340;&#23545;&#27604;&#26469;&#31934;&#28860;&#27599;&#20010;&#38899;&#39057;&#26679;&#26412;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26426;&#22120;&#22768;&#38899;&#30340;&#29289;&#29702;&#29305;&#24615;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#21463;&#21040;&#22686;&#24378;&#25968;&#25454;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#38480;&#21046;&#26816;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#31934;&#28860;&#27599;&#20010;&#26426;&#22120;ID&#30340;&#38899;&#39057;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#27599;&#20010;&#38899;&#39057;&#26679;&#26412;&#12290;&#25552;&#20986;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#39044;&#35757;&#32451;&#38899;&#39057;&#34920;&#31034;&#27169;&#22411;&#65292;&#36890;&#36807;&#34701;&#20837;&#26426;&#22120;ID&#21644;&#33258;&#30417;&#30563;ID&#20998;&#31867;&#22120;&#26469;&#23545;&#23398;&#20064;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#22686;&#24378;&#26469;&#33258;&#30456;&#21516;ID&#30340;&#38899;&#39057;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;DCASE 2020&#25361;&#25112;&#20219;&#21153;2&#25968;&#25454;&#38598;&#19978;&#30340;&#25972;&#20307;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#25110;&#33258;&#30417;&#30563;&#20998;&#31867;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing contrastive learning methods for anomalous sound detection refine the audio representation of each audio sample by using the contrast between the samples' augmentations (e.g., with time or frequency masking). However, they might be biased by the augmented data, due to the lack of physical properties of machine sound, thereby limiting the detection performance. This paper uses contrastive learning to refine audio representations for each machine ID, rather than for each audio sample. The proposed two-stage method uses contrastive learning to pretrain the audio representation model by incorporating machine ID and a self-supervised ID classifier to fine-tune the learnt model, while enhancing the relation between audio features from the same ID. Experiments show that our method outperforms the state-of-the-art methods using contrastive learning or self-supervised classification in overall anomaly detection performance and stability on DCASE 2020 Challenge Task2 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#24230;&#21464;&#20998;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#24369;&#30417;&#30563;&#20998;&#21106;&#65292;&#20026;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#27169;&#22411;&#25552;&#20379;&#21487;&#38752;&#34917;&#20805;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2304.03572</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#23545;&#27604;&#24230;&#21464;&#20998;&#27169;&#22411;&#30340;&#28857;&#27880;&#37322;&#23454;&#29616;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#24369;&#30417;&#30563;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Weakly supervised segmentation with point annotations for histopathology images via contrast-based variational model. (arXiv:2304.03572v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#24230;&#21464;&#20998;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#24369;&#30417;&#30563;&#20998;&#21106;&#65292;&#20026;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#27169;&#22411;&#25552;&#20379;&#21487;&#38752;&#34917;&#20805;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#21106;&#26159;&#25104;&#20687;&#21644;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#22312;&#26377;&#36275;&#22815;&#24102;&#27880;&#37322;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#27880;&#37322;&#20247;&#25152;&#21608;&#30693;&#38590;&#20197;&#33719;&#21462;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#65292;&#20854;&#20013;&#30446;&#26631;&#21306;&#22495;&#36890;&#24120;&#20855;&#26377;&#39640;&#24418;&#24577;&#21464;&#24322;&#21644;&#19981;&#35268;&#21017;&#24418;&#29366;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#23569;&#37327;&#28857;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26377;&#26395;&#20943;&#36731;&#27880;&#37322;&#24037;&#20316;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#24230;&#21464;&#20998;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20998;&#21106;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#21487;&#29992;&#20316;&#35757;&#32451;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#28145;&#24230;&#20998;&#21106;&#27169;&#22411;&#30340;&#21487;&#38752;&#34917;&#20805;&#30417;&#30563;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#30446;&#26631;&#21306;&#22495;&#30340;&#20849;&#21516;&#29305;&#24449;&#65292;&#24182;&#21487;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#23427;&#21487;&#20197;&#29983;&#25104;&#26356;&#20855;&#21306;&#22495;&#19968;&#33268;&#24615;&#21644;&#26356;&#24179;&#28369;&#30340;&#36793;&#30028;&#20998;&#21106;&#65292;&#23545;&#26410;&#26631;&#35760;&#30340;&#8220;&#26032;&#22855;&#8221;&#21306;&#22495;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image segmentation is a fundamental task in the field of imaging and vision. Supervised deep learning for segmentation has achieved unparalleled success when sufficient training data with annotated labels are available. However, annotation is known to be expensive to obtain, especially for histopathology images where the target regions are usually with high morphology variations and irregular shapes. Thus, weakly supervised learning with sparse annotations of points is promising to reduce the annotation workload. In this work, we propose a contrast-based variational model to generate segmentation results, which serve as reliable complementary supervision to train a deep segmentation model for histopathology images. The proposed method considers the common characteristics of target regions in histopathology images and can be trained in an end-to-end manner. It can generate more regionally consistent and smoother boundary segmentation, and is more robust to unlabeled `novel' regions. Exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;$\beta$-VAE&#21644;Transformer&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#36817;&#20284;&#27491;&#20132;&#30340;ROMs&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#28151;&#27788;&#27969;&#20307;&#27969;&#21160;&#30340;&#38477;&#38454;&#24314;&#27169;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03571</link><description>&lt;p&gt;
$\beta$-&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;Transformer&#29992;&#20110;&#27969;&#20307;&#27969;&#21160;&#30340;&#38477;&#38454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
$\beta$-Variational autoencoders and transformers for reduced-order modelling of fluid flows. (arXiv:2304.03571v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;$\beta$-VAE&#21644;Transformer&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#36817;&#20284;&#27491;&#20132;&#30340;ROMs&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#28151;&#27788;&#27969;&#20307;&#27969;&#21160;&#30340;&#38477;&#38454;&#24314;&#27169;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26550;&#26500;&#26377;&#28508;&#21147;&#24320;&#21457;&#28151;&#27788;&#27969;&#20307;&#27969;&#21160;&#30340;&#38477;&#38454;&#27169;&#22411;&#65288;ROMs&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;$\beta$-VAE&#21644;Transformer&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#32039;&#20945;&#19988;&#36817;&#20284;&#27491;&#20132;&#30340;ROMs&#65292;&#21516;&#26102;&#22312;&#20108;&#32500;&#31896;&#24615;&#27969;&#20307;&#27969;&#21160;&#30340;&#21608;&#26399;&#21644;&#28151;&#27788;&#29366;&#24577;&#19979;&#36827;&#34892;&#25968;&#20540;&#27979;&#35797;&#12290;$\beta$-VAE&#34987;&#35757;&#32451;&#20026;&#23398;&#20064;&#27969;&#36895;&#30340;&#32039;&#20945;&#28508;&#22312;&#34920;&#31034;&#24418;&#24335;&#65292;&#32780;Transformer&#21017;&#34987;&#35757;&#32451;&#20026;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39044;&#27979;&#26102;&#38388;&#21160;&#24577;&#12290;&#36890;&#36807;&#20351;&#29992;$\beta$-VAE&#26469;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#24418;&#24335;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26356;&#21487;&#35299;&#37322;&#30340;&#27969;&#21160;&#27169;&#22411;&#65292;&#20854;&#29305;&#24449;&#31867;&#20284;&#20110;&#35266;&#23519;&#21040;&#30340;&#36866;&#24403;&#27491;&#20132;&#20998;&#35299;&#65292;&#20294;&#34920;&#31034;&#26356;&#39640;&#25928;&#12290;&#20351;&#29992;Poincar&#233;&#22270;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#27969;&#20307;&#27969;&#21160;&#30340;&#22522;&#26412;&#21160;&#21147;&#23398;&#65292;&#20248;&#20110;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#22825;&#27668;&#39044;&#25253;&#12289;&#22320;&#23618;&#27969;&#21644;&#27668;&#20505;&#27169;&#22411;&#31561;&#20854;&#20182;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoder (VAE) architectures have the potential to develop reduced-order models (ROMs) for chaotic fluid flows. We propose a method for learning compact and near-orthogonal ROMs using a combination of a $\beta$-VAE and a transformer, tested on numerical data from a two-dimensional viscous flow in both periodic and chaotic regimes. The $\beta$-VAE is trained to learn a compact latent representation of the flow velocity, and the transformer is trained to predict the temporal dynamics in latent space. Using the $\beta$-VAE to learn disentangled representations in latent-space, we obtain a more interpretable flow model with features that resemble those observed in the proper orthogonal decomposition, but with a more efficient representation. Using Poincar\'e maps, the results show that our method can capture the underlying dynamics of the flow outperforming other prediction models. The proposed method has potential applications in other fields such as weather forecasting, st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25299;&#23637;&#20102;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;(PINN) &#26469;&#35299;&#20915;&#27714;&#35299;&#38556;&#30861;&#29289;&#30456;&#20851;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#25968;&#20540;&#26041;&#27861;&#30340;&#38590;&#24230;&#36739;&#22823;&#65292;&#20294;&#20316;&#32773;&#36890;&#36807;&#23545;&#22810;&#31181;&#24773;&#20917;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;PINN&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03552</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#27714;&#35299;&#38556;&#30861;&#29289;&#30456;&#20851;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
A physics-informed neural network framework for modeling obstacle-related equations. (arXiv:2304.03552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25299;&#23637;&#20102;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;(PINN) &#26469;&#35299;&#20915;&#27714;&#35299;&#38556;&#30861;&#29289;&#30456;&#20851;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#25968;&#20540;&#26041;&#27861;&#30340;&#38590;&#24230;&#36739;&#22823;&#65292;&#20294;&#20316;&#32773;&#36890;&#36807;&#23545;&#22810;&#31181;&#24773;&#20917;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;PINN&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24456;&#22823;&#25104;&#21151;&#65292;&#20294;&#23558;&#20854;&#29992;&#20110;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#12288;&#30340;&#30740;&#31350;&#21017;&#26159;&#36817;&#24180;&#26469;&#30340;&#28909;&#28857;&#65292;&#23588;&#20854;&#22312;&#30446;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65288;&#22914;TensorFlow&#25110;PyTorch&#65289;&#30340;&#25903;&#25345;&#19979;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#21487;&#36890;&#36807;&#35299;&#26512;&#31232;&#30095;&#19988;&#22122;&#22768;&#25968;&#25454;&#26469;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#23558;&#25299;&#23637;PINN&#26469;&#27714;&#35299;&#38556;&#30861;&#29289;&#30456;&#20851;PDE&#65292;&#36825;&#31867;&#26041;&#31243;&#38590;&#24230;&#36739;&#22823;&#65292;&#38656;&#35201;&#21487;&#20197;&#24471;&#21040;&#20934;&#30830;&#35299;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;&#20316;&#32773;&#22312;&#27491;&#24120;&#21644;&#19981;&#35268;&#21017;&#30340;&#38556;&#30861;&#24773;&#20917;&#19979;&#65292;&#23545;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;PDE&#30340;&#22810;&#20010;&#22330;&#26223;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;PINNs&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been highly successful in some applications. Nevertheless, its use for solving partial differential equations (PDEs) has only been of recent interest with current state-of-the-art machine learning libraries, e.g., TensorFlow or PyTorch. Physics-informed neural networks (PINNs) are an attractive tool for solving partial differential equations based on sparse and noisy data. Here extend PINNs to solve obstacle-related PDEs which present a great computational challenge because they necessitate numerical methods that can yield an accurate approximation of the solution that lies above a given obstacle. The performance of the proposed PINNs is demonstrated in multiple scenarios for linear and nonlinear PDEs subject to regular and irregular obstacles.
&lt;/p&gt;</description></item><item><title>&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#38169;&#35823;&#21464;&#24471;&#26356;&#38590;&#20197;&#20462;&#22797;&#65292;&#27169;&#22411;&#25490;&#38500;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#34987;&#25552;&#20986;&#12290;</title><link>http://arxiv.org/abs/2304.03545</link><description>&lt;p&gt;
AI&#27169;&#22411;&#25490;&#38500;&#65306;&#26041;&#27861;&#19982;&#36873;&#25321;&#12290;(arXiv&#65306;2304.03545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
AI Model Disgorgement: Methods and Choices. (arXiv:2304.03545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03545
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#38169;&#35823;&#21464;&#24471;&#26356;&#38590;&#20197;&#20462;&#22797;&#65292;&#27169;&#22411;&#25490;&#38500;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#34987;&#25552;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#30340;&#36127;&#36131;&#20219;&#20351;&#29992;&#26159;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23454;&#29616;&#19981;&#21487;&#25110;&#32570;&#30340;&#37096;&#20998;&#12290;ML&#24320;&#21457;&#20154;&#21592;&#24517;&#39035;&#20180;&#32454;&#25910;&#38598;&#21644;&#31574;&#21010;&#20182;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#35760;&#24405;&#23427;&#20204;&#30340;&#26469;&#28304;&#12290;&#20182;&#20204;&#36824;&#24517;&#39035;&#30830;&#20445;&#23562;&#37325;&#30693;&#35782;&#20135;&#26435;&#65292;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#65292;&#24182;&#20197;&#21512;&#27861;&#30340;&#26041;&#24335;&#20351;&#29992;&#25968;&#25454;&#12290;&#36817;&#24180;&#26469;&#65292;ML&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#26174;&#30528;&#22686;&#21152;&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#33267;&#20110;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#20219;&#20309;&#32570;&#38519;&#37117;&#19981;&#33021;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#36731;&#26494;&#20462;&#22797;&#12290;&#23613;&#31649;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#26377;&#22797;&#26434;&#30340;&#25511;&#21046;&#65292;&#24182;&#19988;&#33457;&#36153;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#30830;&#20445;&#35757;&#32451;&#35821;&#26009;&#24211;&#34987;&#27491;&#30830;&#32452;&#25104;&#65292;&#20294;&#26159;&#27169;&#22411;&#25152;&#38656;&#25968;&#25454;&#30340;&#22823;&#37327;&#20351;&#24471;&#25163;&#21160;&#26816;&#26597;&#27599;&#20010;&#25968;&#25454;&#37117;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#35299;&#20915;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#25968;&#25454;&#32570;&#38519;&#30340;&#19968;&#20010;&#28508;&#22312;&#26041;&#27861;&#26159;&#27169;&#22411;&#25490;&#38500;&#8212;&#8212;&#19981;&#20165;&#25490;&#38500;&#32570;&#38519;&#25968;&#25454;&#21644;&#26679;&#26412;&#65292;&#32780;&#23558;&#27169;&#22411;&#26435;&#37325;&#21644;&#35757;&#32451;&#20195;&#30721;&#37117;&#25490;&#38500;&#25481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Responsible use of data is an indispensable part of any machine learning (ML) implementation. ML developers must carefully collect and curate their datasets, and document their provenance. They must also make sure to respect intellectual property rights, preserve individual privacy, and use data in an ethical way. Over the past few years, ML models have significantly increased in size and complexity. These models require a very large amount of data and compute capacity to train, to the extent that any defects in the training corpus cannot be trivially remedied by retraining the model from scratch. Despite sophisticated controls on training data and a significant amount of effort dedicated to ensuring that training corpora are properly composed, the sheer volume of data required for the models makes it challenging to manually inspect each datum comprising a training corpus. One potential fix for training corpus data defects is model disgorgement -- the elimination of not just the improp
&lt;/p&gt;</description></item><item><title>HyperTab&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#32467;&#21512;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#20248;&#28857;&#30340;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27599;&#20010;&#29305;&#23450;&#20302;&#32500;&#35270;&#22270;&#22788;&#29702;&#25968;&#25454;&#65292;&#34394;&#25311;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.03543</link><description>&lt;p&gt;
HyperTab: &#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HyperTab: Hypernetwork Approach for Deep Learning on Small Tabular Datasets. (arXiv:2304.03543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03543
&lt;/p&gt;
&lt;p&gt;
HyperTab&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#32467;&#21512;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#20248;&#28857;&#30340;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27599;&#20010;&#29305;&#23450;&#20302;&#32500;&#35270;&#22270;&#22788;&#29702;&#25968;&#25454;&#65292;&#34394;&#25311;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#34920;&#29616;&#65292;&#20363;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20294;&#23427;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20256;&#32479;&#27973;&#23618;&#26041;&#27861;&#30340;&#20248;&#21183;&#20173;&#28982;&#20540;&#24471;&#21830;&#27063;&#12290;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#65288;&#23567;&#20110;1k&#20010;&#26679;&#26412;&#65289;&#19978;&#36229;&#36807;&#26641;&#29366;&#38598;&#25104;&#65288;&#22914;XGBoost&#25110;&#38543;&#26426;&#26862;&#26519;&#65289;&#30340;&#34920;&#29616;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HyperTab&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#35299;&#20915;&#34920;&#26684;&#25968;&#25454;&#38598;&#23567;&#26679;&#26412;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;HyperTab&#29983;&#25104;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#65292;&#20854;&#20013;&#27599;&#20010;&#30446;&#26631;&#27169;&#22411;&#19987;&#38376;&#22788;&#29702;&#25968;&#25454;&#30340;&#29305;&#23450;&#20302;&#32500;&#35270;&#22270;&#12290;&#30001;&#20110;&#27599;&#20010;&#35270;&#22270;&#25198;&#28436;&#25968;&#25454;&#22686;&#24378;&#30340;&#35282;&#33394;&#65292;&#25105;&#20204;&#22312;&#20445;&#25345;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#34394;&#25311;&#22686;&#21152;&#20102;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#23545;40&#22810;&#20010;&#22823;&#23567;&#19981;&#21516;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#23545;HyperTab&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has achieved impressive performance in many domains, such as computer vision and natural language processing, but its advantage over classical shallow methods on tabular datasets remains questionable. It is especially challenging to surpass the performance of tree-like ensembles, such as XGBoost or Random Forests, on small-sized datasets (less than 1k samples). To tackle this challenge, we introduce HyperTab, a hypernetwork-based approach to solving small sample problems on tabular datasets. By combining the advantages of Random Forests and neural networks, HyperTab generates an ensemble of neural networks, where each target model is specialized to process a specific lower-dimensional view of the data. Since each view plays the role of data augmentation, we virtually increase the number of training samples while keeping the number of trainable parameters unchanged, which prevents model overfitting. We evaluated HyperTab on more than 40 tabular datasets of a varying number
&lt;/p&gt;</description></item><item><title>ChatPipe &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#20248;&#21270; ChatGPT &#32534;&#25490; ML &#25968;&#25454;&#20934;&#22791;&#31243;&#24207;&#65292;&#26377;&#25928;&#25512;&#33616;&#19979;&#19968;&#20010;&#25968;&#25454;&#20934;&#22791;&#25805;&#20316;&#65292;&#26041;&#20415;&#29992;&#25143;&#36827;&#34892;&#31243;&#24207;&#30340;&#20462;&#25913;&#21644;&#29256;&#26412;&#20999;&#25442;&#65292;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569; ML &#25968;&#25454;&#20934;&#22791;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.03540</link><description>&lt;p&gt;
ChatPipe&#65306;&#36890;&#36807;&#20248;&#21270;&#20154;-ChatGPT&#20114;&#21160;&#26469;&#32534;&#25490;&#25968;&#25454;&#20934;&#22791;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
ChatPipe: Orchestrating Data Preparation Program by Optimizing Human-ChatGPT Interactions. (arXiv:2304.03540v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03540
&lt;/p&gt;
&lt;p&gt;
ChatPipe &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#20248;&#21270; ChatGPT &#32534;&#25490; ML &#25968;&#25454;&#20934;&#22791;&#31243;&#24207;&#65292;&#26377;&#25928;&#25512;&#33616;&#19979;&#19968;&#20010;&#25968;&#25454;&#20934;&#22791;&#25805;&#20316;&#65292;&#26041;&#20415;&#29992;&#25143;&#36827;&#34892;&#31243;&#24207;&#30340;&#20462;&#25913;&#21644;&#29256;&#26412;&#20999;&#25442;&#65292;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569; ML &#25968;&#25454;&#20934;&#22791;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#25490;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#20934;&#22791;&#31243;&#24207;&#23545;&#20110;&#25104;&#21151;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#26159;&#32791;&#26102;&#36153;&#21147;&#30340;&#12290;&#23613;&#31649;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#19982;&#29992;&#25143;&#20132;&#20114;&#29983;&#25104;&#31243;&#24207;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20173;&#23384;&#22312;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29992;&#25143;&#24517;&#39035;&#25552;&#20379;&#29305;&#23450;&#30340;&#25552;&#31034;&#26469;&#24341;&#23548;ChatGPT&#36845;&#20195;&#22320;&#25913;&#36827;&#25968;&#25454;&#20934;&#22791;&#31243;&#24207;&#65292;&#36825;&#38656;&#35201;&#23545;&#32534;&#31243;&#12289;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;ML&#20219;&#21153;&#26377;&#19968;&#23450;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#19968;&#26086;&#29983;&#25104;&#20102;&#31243;&#24207;&#65292;&#22312;&#19981;&#37325;&#26032;&#24320;&#22987;&#25972;&#20010;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#22238;&#39038;&#20808;&#21069;&#30340;&#29256;&#26412;&#25110;&#23545;&#31243;&#24207;&#36827;&#34892;&#26356;&#25913;&#26159;&#24456;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatPipe&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#20419;&#36827;&#29992;&#25143;&#21644;ChatGPT&#20043;&#38388;&#30340;&#26080;&#32541;&#20132;&#20114;&#12290; ChatPipe&#20026;&#29992;&#25143;&#25552;&#20379;&#20851;&#20110;&#19979;&#19968;&#20010;&#25968;&#25454;&#20934;&#22791;&#25805;&#20316;&#30340;&#26377;&#25928;&#24314;&#35758;&#65292;&#24182;&#25351;&#23548;ChatGPT&#29983;&#25104;&#25805;&#20316;&#30340;&#31243;&#24207;&#12290;&#21478;&#22806;&#65292;Chatpipe&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#20462;&#25913;&#29983;&#25104;&#30340;&#31243;&#24207;&#25110;&#20999;&#25442;&#21040;&#20808;&#21069;&#30340;&#29256;&#26412;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;ChatPipe&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20026;ML&#20219;&#21153;&#20934;&#22791;&#25968;&#25454;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orchestrating a high-quality data preparation program is essential for successful machine learning (ML), but it is known to be time and effort consuming. Despite the impressive capabilities of large language models like ChatGPT in generating programs by interacting with users through natural language prompts, there are still limitations. Specifically, a user must provide specific prompts to iteratively guide ChatGPT in improving data preparation programs, which requires a certain level of expertise in programming, the dataset used and the ML task. Moreover, once a program has been generated, it is non-trivial to revisit a previous version or make changes to the program without starting the process over again. In this paper, we present ChatPipe, a novel system designed to facilitate seamless interaction between users and ChatGPT. ChatPipe provides users with effective recommendation on next data preparation operations, and guides ChatGPT to generate program for the operations. Also, Cha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#23398;&#20064;&#32467;&#26500;&#65292;&#21487;&#20197;&#31649;&#29702;&#25968;&#25454;&#25552;&#20379;&#21830;&#19982;&#25512;&#29702;&#20013;&#24515;&#20043;&#38388;&#30340;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#65292;&#21516;&#26102;&#22312;&#20445;&#25345;&#26426;&#23494;&#29305;&#24449;&#30340;&#38544;&#31169;&#30340;&#21516;&#26102;&#25552;&#20379;&#26377;&#20851;&#25968;&#25454;&#30340;&#38750;&#26426;&#23494;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.03538</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#23398;&#20064;&#32467;&#26500;&#30340;&#21487;&#35843;&#33410;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Adjustable Privacy using Autoencoder-based Learning Structure. (arXiv:2304.03538v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#23398;&#20064;&#32467;&#26500;&#65292;&#21487;&#20197;&#31649;&#29702;&#25968;&#25454;&#25552;&#20379;&#21830;&#19982;&#25512;&#29702;&#20013;&#24515;&#20043;&#38388;&#30340;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#65292;&#21516;&#26102;&#22312;&#20445;&#25345;&#26426;&#23494;&#29305;&#24449;&#30340;&#38544;&#31169;&#30340;&#21516;&#26102;&#25552;&#20379;&#26377;&#20851;&#25968;&#25454;&#30340;&#38750;&#26426;&#23494;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20013;&#24515;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#25165;&#33021;&#25317;&#26377;&#26356;&#20840;&#38754;&#21644;&#26377;&#30410;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#27492;&#65292;&#20182;&#20204;&#38656;&#35201;&#20174;&#25968;&#25454;&#25552;&#20379;&#32773;&#22788;&#25910;&#38598;&#25968;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25968;&#25454;&#25552;&#20379;&#32773;&#22312;&#38544;&#31169;&#26041;&#38754;&#38750;&#24120;&#35880;&#24910;&#65292;&#19981;&#24895;&#23558;&#20854;&#25968;&#25454;&#38598;&#25552;&#20379;&#32473;&#25512;&#29702;&#20013;&#24515;&#12290;&#26412;&#25991;&#36890;&#36807;&#20462;&#25913;&#33258;&#32534;&#30721;&#22120;&#30340;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24456;&#22909;&#22320;&#31649;&#29702;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25968;&#25454;&#39318;&#20808;&#36890;&#36807;&#32534;&#30721;&#22120;&#21387;&#32553;&#65292;&#28982;&#21518;&#20351;&#29992;&#20998;&#31867;&#22120;&#20998;&#31163;&#19982;&#30456;&#20851;&#30340;&#26426;&#23494;&#21644;&#38750;&#26426;&#23494;&#29305;&#24449;&#12290;&#26426;&#23494;&#29305;&#24449;&#19982;&#22122;&#22768;&#36866;&#24403;&#22320;&#32467;&#21512;&#65292;&#38750;&#26426;&#23494;&#29305;&#24449;&#21017;&#24471;&#21040;&#21152;&#24378;&#65292;&#26368;&#21518;&#36890;&#36807;&#35299;&#30721;&#22120;&#20135;&#29983;&#20855;&#26377;&#21407;&#22987;&#25968;&#25454;&#26684;&#24335;&#30340;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#36824;&#20801;&#35768;&#25968;&#25454;&#25552;&#20379;&#32773;&#35774;&#32622;&#26426;&#23494;&#29305;&#24449;&#25152;&#38656;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24050;&#22312;&#22270;&#20687;&#21644;&#20998;&#31867;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference centers need more data to have a more comprehensive and beneficial learning model, and for this purpose, they need to collect data from data providers. On the other hand, data providers are cautious about delivering their datasets to inference centers in terms of privacy considerations. In this paper, by modifying the structure of the autoencoder, we present a method that manages the utility-privacy trade-off well. To be more precise, the data is first compressed using the encoder, then confidential and non-confidential features are separated and uncorrelated using the classifier. The confidential feature is appropriately combined with noise, and the non-confidential feature is enhanced, and at the end, data with the original data format is produced by the decoder. The proposed architecture also allows data providers to set the level of privacy required for confidential features. The proposed method has been examined for both image and categorical databases, and the results s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;43&#31687;&#20351;&#29992;GAN&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#30340;&#25253;&#21578;&#65292;&#21457;&#29616;&#36825;&#20123;&#30740;&#31350;&#23384;&#22312;&#25968;&#25454;&#20559;&#35265;&#12289;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#20197;&#21450;&#32570;&#20047;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;GAN&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#26377;&#26395;&#20026;&#35786;&#26029;COVID-19&#30340;AI&#27169;&#22411;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#65292;&#20294;&#38656;&#35201;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.03536</link><description>&lt;p&gt;
&#20511;&#21161;GAN&#24212;&#23545;COVID-19&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65306;&#21035;&#34987;&#28818;&#20316;&#25152;&#33945;&#34109;
&lt;/p&gt;
&lt;p&gt;
Leveraging GANs for data scarcity of COVID-19: Beyond the hype. (arXiv:2304.03536v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;43&#31687;&#20351;&#29992;GAN&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#30340;&#25253;&#21578;&#65292;&#21457;&#29616;&#36825;&#20123;&#30740;&#31350;&#23384;&#22312;&#25968;&#25454;&#20559;&#35265;&#12289;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#20197;&#21450;&#32570;&#20047;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;GAN&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#26377;&#26395;&#20026;&#35786;&#26029;COVID-19&#30340;AI&#27169;&#22411;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#65292;&#20294;&#38656;&#35201;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20174;&#32954;&#37096;CT&#25195;&#25551;&#21644;X&#23556;&#32447;&#22270;&#20687;&#20013;&#35786;&#26029;COVID-19&#65292;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#26469;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#20135;&#29983;&#21512;&#25104;&#30340;&#32954;&#37096;CT&#25195;&#25551;&#21644;X&#23556;&#32447;&#22270;&#20687;&#65292;&#20197;&#25552;&#39640;AI&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#23578;&#26410;&#25506;&#32034;&#22914;&#20309;&#20351;&#29992;GAN&#29983;&#25104;&#21487;&#38752;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#26412;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;43&#31687;&#21457;&#34920;&#30340;&#25253;&#21578;&#65292;&#25253;&#21578;&#20102;GAN&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#36825;&#20123;&#30740;&#31350;&#20013;&#35768;&#22810;&#23384;&#22312;&#25968;&#25454;&#20559;&#35265;&#12289;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#20197;&#21450;&#32570;&#20047;&#26469;&#33258;&#25918;&#23556;&#23398;&#23478;&#25110;&#20854;&#20182;&#39046;&#22495;&#19987;&#23478;&#30340;&#21453;&#39304;&#12290;&#36825;&#20123;&#30740;&#31350;&#30340;&#19968;&#20010;&#26222;&#36941;&#38382;&#39064;&#26159;&#28304;&#20195;&#30721;&#19981;&#21487;&#29992;&#65292;&#36825;&#22952;&#30861;&#20102;&#21487;&#37325;&#22797;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#20013;&#36824;&#25253;&#21578;&#20102;&#23558;&#36755;&#20837;&#22270;&#20687;&#37325;&#26032;&#32553;&#25918;&#20197;&#35757;&#32451;&#29616;&#26377;&#30340;GAN&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#20294;&#27809;&#26377;&#25552;&#20379;&#22914;&#20309;&#32553;&#25918;&#30340;&#20020;&#24202;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#34429;&#28982;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#26377;&#26395;&#20026;&#35786;&#26029;COVID-19&#30340;AI&#27169;&#22411;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#65292;&#20294;&#26412;&#39033;&#30740;&#31350;&#20998;&#26512;&#30340;&#30740;&#31350;&#20855;&#26377;&#26377;&#38480;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI)-based models can help in diagnosing COVID-19 from lung CT scans and X-ray images; however, these models require large amounts of data for training and validation. Many researchers studied Generative Adversarial Networks (GANs) for producing synthetic lung CT scans and X-Ray images to improve the performance of AI-based models. It is not well explored how good GAN-based methods performed to generate reliable synthetic data. This work analyzes 43 published studies that reported GANs for synthetic data generation. Many of these studies suffered data bias, lack of reproducibility, and lack of feedback from the radiologists or other domain experts. A common issue in these studies is the unavailability of the source code, hindering reproducibility. The included studies reported rescaling of the input images to train the existing GANs architecture without providing clinical insights on how the rescaling was motivated. Finally, even though GAN-based methods have th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#32454;&#35843;BERT&#27169;&#22411;&#21644;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#30340;&#26041;&#27861;&#12290;&#32763;&#36716;&#26174;&#30528;&#38477;&#20302;&#20102;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#32463;&#21382;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.03518</link><description>&lt;p&gt;
SSS&#22312;SemEval-2023&#20219;&#21153;10&#20013;&#30340;&#35770;&#25991;&#65306;&#20351;&#29992;&#25237;&#31080;&#32454;&#35843;&#21464;&#21387;&#22120;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#12290; (arXiv&#65306;2304.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers. (arXiv:2304.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#32454;&#35843;BERT&#27169;&#22411;&#21644;&#22810;&#25968;&#25237;&#31080;&#38598;&#25104;&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#30340;&#26041;&#27861;&#12290;&#32763;&#36716;&#26174;&#30528;&#38477;&#20302;&#20102;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#32463;&#21382;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval 2023&#20219;&#21153;10&#20013;&#25552;&#20132;&#30340;&#20316;&#21697;-&#21487;&#35299;&#37322;&#30340;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;&#65288;EDOS&#65289;&#65292;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#12290;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#19981;&#26029;&#22686;&#38271;&#23548;&#33268;&#22899;&#24615;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#38754;&#20020;&#19981;&#25104;&#27604;&#20363;&#30340;&#24615;&#21035;&#27495;&#35270;&#12290;&#36825;&#20351;&#24471;&#26816;&#27979;&#21644;&#35299;&#37322;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#21464;&#24471;&#27604;&#20197;&#24448;&#26356;&#21152;&#37325;&#35201;&#65292;&#20197;&#20351;&#31038;&#20132;&#23186;&#20307;&#23545;&#22899;&#24615;&#26356;&#21152;&#23433;&#20840;&#21644;&#21487;&#35775;&#38382;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#23454;&#39564;&#21644;&#24494;&#35843;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22810;&#25968;&#25237;&#31080;&#38598;&#21512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#21333;&#20010;&#22522;&#32447;&#27169;&#22411;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20219;&#21153;A&#20013;&#23454;&#29616;&#20102;&#23439;F1&#20998;&#25968;0.8392&#65292;&#22312;&#20219;&#21153;B&#20013;&#20026;0.6092&#65292;&#22312;&#20219;&#21153;C&#20013;&#20026;0.4319&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to Task 10 at SemEval 2023-Explainable Detection of Online Sexism (EDOS), divided into three subtasks. The recent rise in social media platforms has seen an increase in disproportionate levels of sexism experienced by women on social media platforms. This has made detecting and explaining online sexist content more important than ever to make social media safer and more accessible for women. Our approach consists of experimenting and finetuning BERT-based models and using a Majority Voting ensemble model that outperforms individual baseline model scores. Our system achieves a macro F1 score of 0.8392 for Task A, 0.6092 for Task B, and 0.4319 for Task C.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;GNN&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33410;&#28857;&#26631;&#31614;&#30340;&#20998;&#24067;&#26469;&#36827;&#34892;&#32534;&#30721;&#65292;&#25552;&#21319;&#20102;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#22823;&#22810;&#25968;&#22522;&#26412;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03507</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#30340;&#20998;&#24067;&#24335;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Distributional Signals for Node Classification in Graph Neural Networks. (arXiv:2304.03507v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;GNN&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33410;&#28857;&#26631;&#31614;&#30340;&#20998;&#24067;&#26469;&#36827;&#34892;&#32534;&#30721;&#65292;&#25552;&#21319;&#20102;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#22823;&#22810;&#25968;&#22522;&#26412;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#20013;&#65292;&#33410;&#28857;&#29305;&#24449;&#21644;&#26631;&#31614;&#37117;&#26159;&#22270;&#20449;&#21495;&#22788;&#29702;(GSP)&#20013;&#30340;&#37325;&#35201;&#27010;&#24565;&#12290;&#34429;&#28982;&#22312;&#23398;&#20064;&#21644;&#20272;&#35745;&#20219;&#21153;&#20013;&#65292;GSP&#20013;&#36890;&#24120;&#20250;&#26045;&#21152;&#20449;&#21495;&#24179;&#28369;&#24615;&#32422;&#26463;&#65292;&#20294;&#22914;&#20309;&#38024;&#23545;&#31163;&#25955;&#30340;&#33410;&#28857;&#26631;&#31614;&#23454;&#29616;&#36825;&#19968;&#28857;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#24335;&#22270;&#20449;&#21495;&#30340;&#27010;&#24565;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33410;&#28857;&#26631;&#31614;&#30340;&#20998;&#24067;&#32780;&#19981;&#26159;&#20540;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#31181;&#20998;&#24067;&#24335;&#22270;&#20449;&#21495;&#30340;&#24179;&#28369;&#24615;&#21644;&#38750;&#22343;&#21248;&#24615;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;GNN&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20801;&#35768;&#25105;&#20204;&#22312;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#20998;&#24067;&#24335;&#24179;&#28369;&#24615;&#21644;&#38750;&#22343;&#21248;&#24615;&#36827;&#34892;&#32534;&#30721;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19981;&#21516;&#38382;&#39064;&#35774;&#32622;&#20013;&#30340;&#22823;&#22810;&#25968;&#22522;&#26412;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In graph neural networks (GNNs), both node features and labels are examples of graph signals, a key notion in graph signal processing (GSP). While it is common in GSP to impose signal smoothness constraints in learning and estimation tasks, it is unclear how this can be done for discrete node labels. We bridge this gap by introducing the concept of distributional graph signals. In our framework, we work with the distributions of node labels instead of their values and propose notions of smoothness and non-uniformity of such distributional graph signals. We then propose a general regularization method for GNNs that allows us to encode distributional smoothness and non-uniformity of the model output in semi-supervised node classification tasks. Numerical experiments demonstrate that our method can significantly improve the performance of most base GNN models in different problem settings.
&lt;/p&gt;</description></item><item><title>F-RDW&#26159;&#19968;&#31181;&#22522;&#20110;&#26410;&#26469;&#20301;&#32622;&#39044;&#27979;&#30340;&#37325;&#23450;&#21521;&#34892;&#36208;&#26426;&#21046;&#65292;&#21487;&#23558;&#39044;&#27979;&#20449;&#24687;&#19982;&#29616;&#26377;&#30340;&#37325;&#23450;&#21521;&#34892;&#36208;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#26174;&#33879;&#20943;&#23569;&#37325;&#32622;&#27425;&#25968;&#65292;&#21516;&#26102;&#33021;&#22815;&#36866;&#29992;&#20110;&#21508;&#31181;&#34394;&#25311;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2304.03497</link><description>&lt;p&gt;
F-RDW: &#22522;&#20110;&#26410;&#26469;&#20301;&#32622;&#39044;&#27979;&#30340;&#37325;&#23450;&#21521;&#34892;&#36208;
&lt;/p&gt;
&lt;p&gt;
F-RDW: Redirected Walking with Forecasting Future Position. (arXiv:2304.03497v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03497
&lt;/p&gt;
&lt;p&gt;
F-RDW&#26159;&#19968;&#31181;&#22522;&#20110;&#26410;&#26469;&#20301;&#32622;&#39044;&#27979;&#30340;&#37325;&#23450;&#21521;&#34892;&#36208;&#26426;&#21046;&#65292;&#21487;&#23558;&#39044;&#27979;&#20449;&#24687;&#19982;&#29616;&#26377;&#30340;&#37325;&#23450;&#21521;&#34892;&#36208;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#26174;&#33879;&#20943;&#23569;&#37325;&#32622;&#27425;&#25968;&#65292;&#21516;&#26102;&#33021;&#22815;&#36866;&#29992;&#20110;&#21508;&#31181;&#34394;&#25311;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#32473;&#29992;&#25143;&#25552;&#20379;&#26356;&#22909;&#30340;&#34394;&#25311;&#29616;&#23454;&#20307;&#39564;&#65292;&#29616;&#26377;&#30340;&#37325;&#23450;&#21521;&#34892;&#36208;&#39044;&#27979;&#26041;&#27861;&#21033;&#29992;&#26410;&#26469;&#20449;&#24687;&#26469;&#20943;&#23569;&#37325;&#32622;&#27425;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#22312;&#37096;&#32626;&#26102;&#38656;&#35201;&#28385;&#36275;&#19968;&#20010;&#20808;&#20915;&#26465;&#20214;&#65292;&#21487;&#33021;&#38480;&#21046;&#20854;&#26222;&#36866;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#21046; F-RDW&#65292;&#23427;&#20855;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#39044;&#27979;&#34394;&#25311;&#31354;&#38388;&#20013;&#29992;&#25143;&#30340;&#26410;&#26469;&#20301;&#32622;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#20551;&#35774;&#65307;&#65288;2&#65289;&#23558;&#36825;&#20123;&#20449;&#24687;&#19982;&#29616;&#26377;&#30340;&#37325;&#23450;&#21521;&#34892;&#36208;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#31532;&#19968;&#27493;&#30340;&#22522;&#30784;&#26159;&#19968;&#20010;&#22522;&#20110; LSTM &#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#29992;&#25143;&#30340;&#31354;&#38388;&#21644;&#30524;&#21160;&#36319;&#36394;&#25968;&#25454;&#26469;&#39044;&#27979;&#29992;&#25143;&#22312;&#34394;&#25311;&#31354;&#38388;&#20013;&#30340;&#26410;&#26469;&#20301;&#32622;&#65292;&#31532;&#20108;&#27493;&#23558;&#36825;&#20123;&#39044;&#27979;&#20540;&#20197;&#36866;&#29992;&#30340;&#26041;&#24335;&#36755;&#20837;&#21040;&#29616;&#26377;&#30340;&#37325;&#23450;&#21521;&#34892;&#36208;&#26041;&#27861;&#20013;&#65288;&#20363;&#22914; MPCRed&#65292;S2C&#65292;TAPF &#21644; ARC&#65289;&#20013;&#12290;&#25105;&#20204;&#30340;&#20223;&#30495;&#27979;&#35797;&#21644;&#29992;&#25143;&#30740;&#31350;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;F-RDW &#21487;&#26174;&#33879;&#20943;&#23569;&#37325;&#32622;&#27425;&#25968;&#65292;&#21516;&#26102;&#20351;&#34394;&#25311;&#29615;&#22659;&#19982;&#29616;&#23454;&#19990;&#30028;&#20445;&#25345;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;F-RDW &#21487;&#26080;&#32541;&#24212;&#29992;&#20110;&#21508;&#31181;&#34394;&#25311;&#29615;&#22659;&#65292;&#32780;&#19981;&#38656;&#35201;&#28385;&#36275;&#20219;&#20309;&#20808;&#20915;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to serve better VR experiences to users, existing predictive methods of Redirected Walking (RDW) exploit future information to reduce the number of reset occurrences. However, such methods often impose a precondition during deployment, either in the virtual environment's layout or the user's walking direction, which constrains its universal applications. To tackle this challenge, we propose a novel mechanism F-RDW that is twofold: (1) forecasts the future information of a user in the virtual space without any assumptions, and (2) fuse this information while maneuvering existing RDW methods. The backbone of the first step is an LSTM-based model that ingests the user's spatial and eye-tracking data to predict the user's future position in the virtual space, and the following step feeds those predicted values into existing RDW methods (such as MPCRed, S2C, TAPF, and ARC) while respecting their internal mechanism in applicable ways.The results of our simulation test and user study
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#20307;&#31995;&#32467;&#26500; V-&#22810;&#38754;&#20307;&#21487;&#35777;&#26126;&#20462;&#22797;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#20462;&#22797;&#21482;&#20462;&#25913; DNN &#30340;&#21442;&#25968;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#25903;&#25345;&#22810;&#31181;&#31867;&#22411;&#30340;&#23618;&#65292;&#24182;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2304.03496</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20445;&#20307;&#31995;&#32467;&#26500;&#21487;&#35777;&#26126;&#20462;&#22797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Architecture-Preserving Provable Repair of Deep Neural Networks. (arXiv:2304.03496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#20307;&#31995;&#32467;&#26500; V-&#22810;&#38754;&#20307;&#21487;&#35777;&#26126;&#20462;&#22797;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#20462;&#22797;&#21482;&#20462;&#25913; DNN &#30340;&#21442;&#25968;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#25903;&#25345;&#22810;&#31181;&#31867;&#22411;&#30340;&#23618;&#65292;&#24182;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#25104;&#20026;&#20102;&#36719;&#20214;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#35768;&#22810;&#38382;&#39064;&#65288;&#22914;&#22270;&#20687;&#35782;&#21035;&#65289;&#30340;&#26368;&#20808;&#36827;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;DNN &#36828;&#38750;&#19981;&#21487;&#38169;&#35823;&#65292;DNN &#30340;&#19981;&#27491;&#30830;&#34892;&#20026;&#21487;&#33021;&#20250;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36896;&#25104;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20445;&#20307;&#31995;&#32467;&#26500; V-&#22810;&#38754;&#20307;&#21487;&#35777;&#26126;&#20462;&#22797; DNNs &#30340;&#38382;&#39064;&#12290;V-&#22810;&#38754;&#20307;&#20351;&#29992;&#20854;&#39030;&#28857;&#34920;&#31034;&#27861;&#23450;&#20041;&#20102;&#19968;&#20010;&#20984;&#32422;&#26463;&#22810;&#38754;&#20307;&#12290;V-&#22810;&#38754;&#20307;&#21487;&#35777;&#26126;&#20462;&#22797;&#20445;&#35777;&#20462;&#22797;&#21518;&#30340; DNN &#28385;&#36275;&#32473;&#23450; V-&#22810;&#38754;&#20307;&#20013;&#26080;&#38480;&#28857;&#38598;&#19978;&#30340;&#35268;&#33539;&#12290;&#20307;&#31995;&#32467;&#26500;&#20445;&#25345;&#20462;&#22797;&#20165;&#20462;&#25913; DNN &#30340;&#21442;&#25968;&#65292;&#32780;&#19981;&#20462;&#25913;&#20854;&#20307;&#31995;&#32467;&#26500;&#12290;&#20462;&#22797;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#20462;&#25913; DNN &#30340;&#22810;&#20010;&#23618;&#65292;&#24182;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;&#23427;&#25903;&#25345;&#20855;&#26377;&#19968;&#20123;&#32447;&#24615;&#37096;&#20998;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#21450;&#23436;&#20840;&#36830;&#25509;&#30340;&#12289;&#21367;&#31215;&#30340;&#12289;&#27744;&#21270;&#30340;&#21644;&#27531;&#20313;&#23618;&#30340; DNNs&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#25552;&#20379; DNN &#20307;&#31995;&#32467;&#26500;&#20445;&#25345;&#21487;&#35777;&#26126;&#20462;&#22797;&#30340;&#27491;&#24335;&#26694;&#26550;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are becoming increasingly important components of software, and are considered the state-of-the-art solution for a number of problems, such as image recognition. However, DNNs are far from infallible, and incorrect behavior of DNNs can have disastrous real-world consequences. This paper addresses the problem of architecture-preserving V-polytope provable repair of DNNs. A V-polytope defines a convex bounded polytope using its vertex representation. V-polytope provable repair guarantees that the repaired DNN satisfies the given specification on the infinite set of points in the given V-polytope. An architecture-preserving repair only modifies the parameters of the DNN, without modifying its architecture. The repair has the flexibility to modify multiple layers of the DNN, and runs in polynomial time. It supports DNNs with activation functions that have some linear pieces, as well as fully-connected, convolutional, pooling and residual layers. To the best our 
&lt;/p&gt;</description></item><item><title>ParaGraph&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#22270;&#30340;&#31243;&#24207;&#34920;&#31034;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;HPC&#20869;&#26680;&#20195;&#30721;&#65292;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20256;&#36882;&#24212;&#29992;&#31243;&#24207;&#29305;&#23450;&#20449;&#24687;&#20197;&#24110;&#21161;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03487</link><description>&lt;p&gt;
ParaGraph: &#29992;&#20110; HPC Kernel &#24615;&#33021;&#20248;&#21270;&#30340;&#21152;&#26435;&#22270;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
ParaGraph: Weighted Graph Representation for Performance Optimization of HPC Kernels. (arXiv:2304.03487v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03487
&lt;/p&gt;
&lt;p&gt;
ParaGraph&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#22270;&#30340;&#31243;&#24207;&#34920;&#31034;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;HPC&#20869;&#26680;&#20195;&#30721;&#65292;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20256;&#36882;&#24212;&#29992;&#31243;&#24207;&#29305;&#23450;&#20449;&#24687;&#20197;&#24110;&#21161;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110; GPU &#30340; HPC &#38598;&#32676;&#22240;&#20854;&#24191;&#27867;&#30340;&#24182;&#34892;&#24615;&#21644;&#39640;&#33021;&#25928;&#24615;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#31185;&#23398;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#20154;&#21592;&#12290;&#20026;&#20102;&#22312;&#22810;&#31181;&#22810;&#26680;&#26550;&#26500;&#20013;&#23454;&#29616;&#21487;&#31227;&#26893;&#24615;&#65292;&#19968;&#31181;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#20154;&#21592;&#30340;&#27969;&#34892;&#36873;&#25321;&#26159;&#21033;&#29992;&#22522;&#20110;&#25351;&#20196;&#30340;&#24182;&#34892;&#32534;&#31243;&#27169;&#22411;&#65292;&#20363;&#22914; OpenMP&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#20351;&#29992; OpenMP&#65292;&#24320;&#21457;&#20154;&#21592;&#20063;&#24517;&#39035;&#20174;&#35768;&#22810;&#31574;&#30053;&#20013;&#36873;&#25321;&#29992;&#20110;&#21033;&#29992; GPU &#25110; CPU&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064; (ML) &#26041;&#27861;&#22312; HPC &#24212;&#29992;&#31243;&#24207;&#20248;&#21270;&#26041;&#38754;&#24102;&#26469;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110; ML &#27169;&#22411;&#34920;&#31034;&#24212;&#29992;&#31243;&#24207;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#26410;&#33021;&#25429;&#33719;&#23637;&#31034;&#24182;&#34892;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#31243;&#24207;&#34920;&#31034;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#20449;&#24687;&#65292;&#25193;&#23637;&#20102;&#25277;&#35937;&#35821;&#27861;&#26641;&#12290;&#26412;&#25991;&#30340;&#29420;&#21019;&#24615;&#22312;&#20110;&#23545;&#22270;&#20013;&#33410;&#28857;&#21152;&#26435;&#65292;&#20256;&#36882;&#24212;&#29992;&#31243;&#24207;&#29305;&#23450;&#20449;&#24687;&#65292;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20248;&#21270; HPC &#20869;&#26680;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPU-based HPC clusters are attracting more scientific application developers due to their extensive parallelism and energy efficiency. In order to achieve portability among a variety of multi/many core architectures, a popular choice for an application developer is to utilize directive-based parallel programming models, such as OpenMP. However, even with OpenMP, the developer must choose from among many strategies for exploiting a GPU or a CPU. Recently, Machine Learning (ML) approaches have brought significant advances in the optimizations of HPC applications. To this end, several ways have been proposed to represent application characteristics for ML models. However, the available techniques fail to capture features that are crucial for exposing parallelism. In this paper, we introduce a new graph-based program representation for parallel applications that extends the Abstract Syntax Tree to represent control and data flow information. The originality of this work lies in the additio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RED-PSM&#30340;&#26041;&#27861;&#65292;&#23558;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#19982;&#21435;&#22122;&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#25104;&#20687;&#38382;&#39064;&#65292;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03483</link><description>&lt;p&gt;
RED-PSM: &#24102;&#21435;&#22122;&#27491;&#21017;&#21270;&#30340;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#29992;&#20110;&#21160;&#24577;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging. (arXiv:2304.03483v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RED-PSM&#30340;&#26041;&#27861;&#65292;&#23558;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#19982;&#21435;&#22122;&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#25104;&#20687;&#38382;&#39064;&#65292;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#25104;&#20687;&#26159;&#25351;&#21033;&#29992;&#34987;&#27424;&#37319;&#26679;&#30340;&#27979;&#37327;&#25968;&#25454;&#24674;&#22797;&#27599;&#20010;&#26102;&#38388;&#28857;&#19978;&#30340;&#26102;&#21464;&#20108;&#32500;&#25110;&#19977;&#32500;&#29289;&#20307;&#12290;&#23588;&#20854;&#26159;&#22312;&#21160;&#24577;&#26029;&#23618;&#25195;&#25551;&#20013;&#65292;&#27599;&#20010;&#26102;&#38388;&#28857;&#19978;&#21482;&#26377;&#19968;&#20010;&#35270;&#35282;&#19979;&#30340;&#21333;&#20010;&#25237;&#24433;&#21487;&#29992;&#65292;&#20351;&#24471;&#38382;&#39064;&#20005;&#37325;&#31639;&#19981;&#21487;&#36870;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RED-PSM&#30340;&#26041;&#27861;&#65292;&#39318;&#27425;&#23558;&#20004;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25104;&#20687;&#38382;&#39064;&#12290;&#31532;&#19968;&#31181;&#25216;&#26415;&#26159;&#37096;&#20998;&#21487;&#20998;&#27169;&#22411;&#65292;&#24050;&#32463;&#29992;&#20110;&#39640;&#25928;&#22320;&#20026;&#26102;&#31354;&#30446;&#26631;&#24341;&#20837;&#20302;&#31209;&#20808;&#39564;&#12290;&#31532;&#20108;&#31181;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#21435;&#22122;&#27491;&#21017;&#21270;(RED)&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21435;&#22122;&#31639;&#27861;&#22788;&#29702;&#21508;&#31181;&#21453;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#27491;&#21017;&#21270;&#30340;&#37096;&#20998;&#21487;&#20998;&#30446;&#26631;&#65292;&#36890;&#36807;&#21464;&#37327;&#20998;&#35010;&#21644;ADMM&#20248;&#21270;&#26041;&#26696;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30446;&#26631;&#25910;&#25947;&#20110;&#19968;&#20010;&#28385;&#36275;&#20248;&#21270;&#38382;&#39064;&#30340;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21160;&#24577;&#26029;&#23618;&#25195;&#25551;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;RED-PSM&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#21160;&#24577;&#25104;&#20687;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic imaging addresses the recovery of a time-varying 2D or 3D object at each time instant using its undersampled measurements. In particular, in the case of dynamic tomography, only a single projection at a single view angle may be available at a time, making the problem severely ill-posed. In this work, we propose an approach, RED-PSM, which combines for the first time two powerful techniques to address this challenging imaging problem. The first, are partially separable models, which have been used to efficiently introduce a low-rank prior for the spatio-temporal object. The second is the recent Regularization by Denoising (RED), which provides a flexible framework to exploit the impressive performance of state-of-the-art image denoising algorithms, for various inverse problems. We propose a partially separable objective with RED and an optimization scheme with variable splitting and ADMM, and prove convergence of our objective to a value corresponding to a stationary point satis
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#12290;&#20026;&#25506;&#31350;EA&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26356;&#25509;&#36817;&#29616;&#23454;&#30340;&#39640;&#24230;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03468</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#65306;&#26032;&#25968;&#25454;&#38598;&#21644;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking GNN-based Entity Alignment on Heterogeneous Knowledge Graphs: New Datasets and A New Method. (arXiv:2304.03468v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03468
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;GNN&#30340;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#12290;&#20026;&#25506;&#31350;EA&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#26356;&#25509;&#36817;&#29616;&#23454;&#30340;&#39640;&#24230;&#24322;&#26500;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#24212;&#29992;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#38656;&#35201;&#20174;&#21508;&#31181;&#26469;&#28304;&#25552;&#21462;&#30340;&#24322;&#26500;KG&#20043;&#38388;&#30340;&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#12290;&#36817;&#26469;&#65292;&#30001;&#20110;GNN&#30340;&#20986;&#33394;&#32467;&#26500;&#20449;&#24687;&#25429;&#25417;&#33021;&#21147;&#65292;&#22312;EA&#20219;&#21153;&#20013;&#24191;&#27867;&#37319;&#29992;GNN&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#24120;&#35265;EA&#25968;&#25454;&#38598;&#30340;&#36807;&#20110;&#31616;&#21333;&#21270;&#30340;&#35774;&#32622;&#19982;&#29616;&#23454;&#22330;&#26223;&#30456;&#36317;&#29978;&#36828;&#65292;&#36825;&#22952;&#30861;&#20102;&#23545;&#26368;&#36817;&#26041;&#27861;&#25152;&#21462;&#24471;&#36827;&#23637;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#36825;&#31181;&#29616;&#35937;&#20351;&#25105;&#20204;&#28145;&#24605;&#65306;&#29616;&#26377;&#22522;&#20110;GNN&#30340;EA&#26041;&#27861;&#26159;&#21542;&#30495;&#30340;&#21462;&#24471;&#20102;&#20255;&#22823;&#36827;&#23637;&#65311;&#20026;&#20102;&#30740;&#31350;EA&#26041;&#27861;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#32858;&#28966;&#20110;&#39640;&#24230;&#24322;&#26500;&#30340;KG&#65288;HHKG&#65289;&#65288;&#20363;&#22914;&#65292;&#20107;&#20214;KG&#21644;&#36890;&#29992;KG&#65289;&#30340;&#23545;&#40784;&#65292;&#36825;&#20123;KG&#22312;&#35268;&#27169;&#21644;&#32467;&#26500;&#19978;&#19981;&#21516;&#65292;&#24182;&#20849;&#20139;&#26356;&#23569;&#30340;&#37325;&#21472;&#23454;&#20307;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28165;&#29702;&#20102;&#19981;&#21512;&#29702;&#30340;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HHKG&#25968;&#25454;&#38598;&#65292;&#20854;&#23494;&#20999;&#22320;&#27169;&#25311;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of knowledge graph (KG) applications has led to a rising need for entity alignment (EA) between heterogeneous KGs that are extracted from various sources. Recently, graph neural networks (GNNs) have been widely adopted in EA tasks due to GNNs' impressive ability to capture structure information. However, we have observed that the oversimplified settings of the existing common EA datasets are distant from real-world scenarios, which obstructs a full understanding of the advancements achieved by recent methods. This phenomenon makes us ponder: Do existing GNN-based EA methods really make great progress?  In this paper, to study the performance of EA methods in realistic settings, we focus on the alignment of highly heterogeneous KGs (HHKGs) (e.g., event KGs and general KGs) which are different with regard to the scale and structure, and share fewer overlapping entities. First, we sweep the unreasonable settings, and propose two new HHKG datasets that closely mimic real-wo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26089;&#26399;&#24207;&#21015;&#20998;&#31867;&#30340;&#26032;&#31574;&#30053;&#8212;&#8212;&#20998;&#31867;&#22120;&#35825;&#23548;&#20572;&#27490;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#20381;&#36182;&#25506;&#32034;&#23398;&#20064;&#20572;&#27490;&#21644;&#20998;&#31867;&#19981;&#21516;&#65292;&#26412;&#26041;&#27861;&#37319;&#29992;&#30417;&#30563;&#26041;&#27861;&#30452;&#25509;&#36827;&#34892;&#20998;&#31867;&#65292;AUC&#20540;&#21487;&#22686;&#21152;11.8%&#12290;</title><link>http://arxiv.org/abs/2304.03463</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;&#26089;&#26399;&#20998;&#31867;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A Policy for Early Sequence Classification. (arXiv:2304.03463v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26089;&#26399;&#24207;&#21015;&#20998;&#31867;&#30340;&#26032;&#31574;&#30053;&#8212;&#8212;&#20998;&#31867;&#22120;&#35825;&#23548;&#20572;&#27490;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#20381;&#36182;&#25506;&#32034;&#23398;&#20064;&#20572;&#27490;&#21644;&#20998;&#31867;&#19981;&#21516;&#65292;&#26412;&#26041;&#27861;&#37319;&#29992;&#30417;&#30563;&#26041;&#27861;&#30452;&#25509;&#36827;&#34892;&#20998;&#31867;&#65292;AUC&#20540;&#21487;&#22686;&#21152;11.8%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#24448;&#24448;&#19981;&#26159;&#19968;&#27425;&#24615;&#20840;&#37096;&#25509;&#25910;&#30340;&#65292;&#32780;&#26159;&#36880;&#20010;&#20803;&#32032;&#22320;&#36880;&#27493;&#25509;&#25910;&#12290;&#30001;&#20110;&#26089;&#26399;&#39044;&#27979;&#24102;&#26469;&#26356;&#22823;&#30340;&#25910;&#30410;&#65292;&#22240;&#27492;&#20154;&#20204;&#24076;&#26395;&#23613;&#24555;&#12289;&#23613;&#21487;&#33021;&#20934;&#30830;&#22320;&#23545;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#19981;&#24517;&#31561;&#21040;&#26368;&#21518;&#19968;&#20010;&#20803;&#32032;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#26089;&#26399;&#24207;&#21015;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#35825;&#23548;&#20572;&#27490;&#26041;&#27861;&#12290;&#32780;&#20197;&#24448;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#35757;&#32451;&#26399;&#38388;&#30340;&#25506;&#32034;&#26469;&#23398;&#20064;&#20309;&#26102;&#20572;&#27490;&#21644;&#20998;&#31867;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#26356;&#30452;&#25509;&#30340;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#35825;&#23548;&#20572;&#27490;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#24179;&#22343; Pareto &#21069;&#27839; AUC &#22686;&#21152;&#20102; 11.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequences are often not received in their entirety at once, but instead, received incrementally over time, element by element. Early predictions yielding a higher benefit, one aims to classify a sequence as accurately as possible, as soon as possible, without having to wait for the last element. For this early sequence classification, we introduce our novel classifier-induced stopping. While previous methods depend on exploration during training to learn when to stop and classify, ours is a more direct, supervised approach. Our classifier-induced stopping achieves an average Pareto frontier AUC increase of 11.8% over multiple experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35270;&#35273;&#34920;&#31034;&#30340;&#35780;&#20215;&#21327;&#35758;&#65292;&#21457;&#29616;&#23545;&#20110;&#32447;&#24615;&#25506;&#27979;&#26469;&#35828;&#65292;&#36755;&#20837;&#24402;&#19968;&#21270;&#26159;&#28040;&#38500;&#24615;&#33021;&#21464;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.03456</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35270;&#35273;&#34920;&#31034;&#35780;&#20215;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Rethinking Evaluation Protocols of Visual Representations Learned via Self-supervised Learning. (arXiv:2304.03456v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35270;&#35273;&#34920;&#31034;&#30340;&#35780;&#20215;&#21327;&#35758;&#65292;&#21457;&#29616;&#23545;&#20110;&#32447;&#24615;&#25506;&#27979;&#26469;&#35828;&#65292;&#36755;&#20837;&#24402;&#19968;&#21270;&#26159;&#28040;&#38500;&#24615;&#33021;&#21464;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#23398;&#20064;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35270;&#35273;&#34920;&#31034;&#30340;&#36136;&#37327;&#36827;&#34892;&#35780;&#20215;&#24120;&#24120;&#37319;&#29992;&#22312;&#24102;&#26631;&#27880;&#30340;&#19978;&#28216;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#32447;&#24615;&#25506;&#27979;&#21644;K-NN&#26041;&#27861;&#65292;&#20197;&#21450;&#23558;&#20854;&#36801;&#31227;&#21040;&#21508;&#31181;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#22312;&#36825;&#20123;&#35780;&#20215;&#21327;&#35758;&#19979;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#24615;&#33021;&#38750;&#24120;&#25935;&#24863;&#20110;&#32447;&#24615;&#25506;&#27979;&#21644;&#36801;&#31227;&#23398;&#20064;&#20013;&#28041;&#21450;&#30340;&#36229;&#21442;&#25968;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#26469;&#25214;&#20986;&#36825;&#31181;&#24615;&#33021;&#25935;&#24863;&#24615;&#30340;&#21407;&#22240;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#23545;&#20110;&#32447;&#24615;&#25506;&#27979;&#26469;&#35828;&#65292;&#36755;&#20837;&#24402;&#19968;&#21270;&#26159;&#28040;&#38500;&#24615;&#33021;&#21464;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear probing (LP) (and $k$-NN) on the upstream dataset with labels (e.g., ImageNet) and transfer learning (TL) to various downstream datasets are commonly employed to evaluate the quality of visual representations learned via self-supervised learning (SSL). Although existing SSL methods have shown good performances under those evaluation protocols, we observe that the performances are very sensitive to the hyperparameters involved in LP and TL. We argue that this is an undesirable behavior since truly generic representations should be easily adapted to any other visual recognition task, i.e., the learned representations should be robust to the settings of LP and TL hyperparameters. In this work, we try to figure out the cause of performance sensitivity by conducting extensive experiments with state-of-the-art SSL methods. First, we find that input normalization for LP is crucial to eliminate performance variations according to the hyperparameters. Specifically, batch normalization be
&lt;/p&gt;</description></item><item><title>&#31232;&#32570;&#30693;&#35782;&#23545;&#33258;&#21160;&#21270;&#20915;&#31574;&#36896;&#25104;&#20102;&#38556;&#30861;&#65292;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#26159;&#36890;&#36807;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#30693;&#35782;&#24046;&#36317;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03452</link><description>&lt;p&gt;
&#22270;&#32467;&#26500;&#25903;&#25345;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Graph Enabled Cross-Domain Knowledge Transfer. (arXiv:2304.03452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03452
&lt;/p&gt;
&lt;p&gt;
&#31232;&#32570;&#30693;&#35782;&#23545;&#33258;&#21160;&#21270;&#20915;&#31574;&#36896;&#25104;&#20102;&#38556;&#30861;&#65292;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#26159;&#36890;&#36807;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#32531;&#35299;&#19981;&#21516;&#39046;&#22495;&#30693;&#35782;&#24046;&#36317;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20219;&#20309;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#24517;&#39035;&#23558;&#32473;&#23450;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#65292;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#65289;&#36716;&#21270;&#20026;&#21487;&#20197;&#34987;&#20854;&#20860;&#23481;&#35821;&#35328;&#21644;&#25968;&#25454;&#26684;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29702;&#35299;&#21644;&#22788;&#29702;&#30340;&#34920;&#31034;&#21521;&#37327;&#12290;&#28982;&#32780;&#65292;&#32463;&#24120;&#36935;&#21040;&#30340;&#22256;&#38590;&#26159;&#65292;&#39318;&#20808;&#32473;&#23450;&#30340;&#30693;&#35782;&#24182;&#19981;&#20805;&#20998;&#25110;&#21487;&#38752;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#20204;&#20250;&#23547;&#27714;&#34701;&#21512;&#26469;&#33258;&#21333;&#29420;&#39046;&#22495;&#30340;&#36741;&#21161;&#20449;&#24687;&#26469;&#32531;&#35299;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24863;&#20852;&#36259;&#39046;&#22495;&#30340;&#31232;&#32570;&#30693;&#35782;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#31216;&#20026;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#12290;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20174;&#22312;&#32447;&#21307;&#30103;&#24179;&#21488;&#20998;&#26512;&#21040;&#37329;&#34701;&#24066;&#22330;&#39118;&#38505;&#37327;&#21270;&#65292;&#37117;&#23384;&#22312;&#31232;&#32570;&#30693;&#35782;&#30340;&#20849;&#24615;&#65292;&#36825;&#20026;&#25105;&#20204;&#20174;&#33258;&#21160;&#21270;&#20915;&#31574;&#20013;&#21463;&#30410;&#30041;&#19979;&#20102;&#38556;&#30861;&#12290;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#21033;&#29992;&#20102;&#36825;&#31181;&#36328;&#39046;&#22495;&#36716;&#31227;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
To leverage machine learning in any decision-making process, one must convert the given knowledge (for example, natural language, unstructured text) into representation vectors that can be understood and processed by machine learning model in their compatible language and data format. The frequently encountered difficulty is, however, the given knowledge is not rich or reliable enough in the first place. In such cases, one seeks to fuse side information from a separate domain to mitigate the gap between good representation learning and the scarce knowledge in the domain of interest. This approach is named Cross-Domain Knowledge Transfer. It is crucial to study the problem because of the commonality of scarce knowledge in many scenarios, from online healthcare platform analyses to financial market risk quantification, leaving an obstacle in front of us benefiting from automated decision making. From the machine learning perspective, the paradigm of semi-supervised learning takes advanta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#20195;&#29702;&#30340;&#26550;&#26500;&#65292;&#23427;&#33021;&#22815;&#20223;&#30495;&#20986;&#20855;&#26377;&#21487;&#20449;&#24230;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#22635;&#20805;&#20132;&#20114;&#24335;&#27801;&#30418;&#29615;&#22659;&#65292;&#20026;&#21019;&#36896;&#26356;&#21152;&#30495;&#23454;&#30340;&#20154;&#26426;&#20132;&#20114;&#20307;&#39564;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.03442</link><description>&lt;p&gt;
&#29983;&#25104;&#20195;&#29702;: &#20154;&#31867;&#34892;&#20026;&#30340;&#20132;&#20114;&#20223;&#30495;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generative Agents: Interactive Simulacra of Human Behavior. (arXiv:2304.03442v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#20195;&#29702;&#30340;&#26550;&#26500;&#65292;&#23427;&#33021;&#22815;&#20223;&#30495;&#20986;&#20855;&#26377;&#21487;&#20449;&#24230;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#22635;&#20805;&#20132;&#20114;&#24335;&#27801;&#30418;&#29615;&#22659;&#65292;&#20026;&#21019;&#36896;&#26356;&#21152;&#30495;&#23454;&#30340;&#20154;&#26426;&#20132;&#20114;&#20307;&#39564;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#30340;&#20154;&#31867;&#34892;&#20026;&#20223;&#30495;&#21487;&#36171;&#33021;&#20110;&#20174;&#27785;&#28024;&#24335;&#29615;&#22659;&#21040;&#20154;&#38469;&#20132;&#27969;&#25490;&#32451;&#31354;&#38388;&#21040;&#21407;&#22411;&#24037;&#20855;&#30340;&#20132;&#20114;&#24335;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#25104;&#20195;&#29702;&#8212;&#8212;&#20855;&#26377;&#21487;&#20449;&#24230;&#30340;&#20154;&#31867;&#34892;&#20026;&#20223;&#30495;&#30340;&#35745;&#31639;&#26426;&#36719;&#20214;&#20195;&#29702;&#12290;&#29983;&#25104;&#20195;&#29702;&#20250;&#36215;&#24202;&#65292;&#20570;&#26089;&#39184;&#65292;&#21435;&#24037;&#20316;&#65307;&#33402;&#26415;&#23478;&#30011;&#30011;&#65292;&#20316;&#23478;&#20889;&#20316;&#65307;&#20182;&#20204;&#24418;&#25104;&#35266;&#28857;&#65292;&#20114;&#30456;&#27880;&#24847;&#65292;&#24182;&#24320;&#22987;&#20132;&#35848;&#65307;&#20182;&#20204;&#22238;&#24518;&#36807;&#21435;&#30340;&#26085;&#23376;&#24182;&#35745;&#21010;&#26410;&#26469;&#12290;&#20026;&#20102;&#20351;&#29983;&#25104;&#20195;&#29702;&#33021;&#22815;&#23454;&#29616;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23384;&#20648;&#20195;&#29702;&#30340;&#32463;&#21382;&#30340;&#23436;&#25972;&#35760;&#24405;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32508;&#21512;&#36825;&#20123;&#35760;&#24518;&#21040;&#26356;&#39640;&#23618;&#27425;&#30340;&#21453;&#24605;&#65292;&#20197;&#21450;&#21160;&#24577;&#26816;&#32034;&#36825;&#20123;&#35760;&#24518;&#20197;&#35268;&#21010;&#34892;&#20026;&#12290;&#25105;&#20204;&#23454;&#20363;&#21270;&#29983;&#25104;&#20195;&#29702;&#20197;&#22635;&#20805;&#21463;&#12298;&#27169;&#25311;&#20154;&#29983;&#12299;&#21551;&#21457;&#30340;&#20132;&#20114;&#24335;&#27801;&#30418;&#29615;&#22659;&#65292;&#26368;&#32456;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#31995;&#32479;&#19982;25&#20010;&#20195;&#29702;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#24322;&#26500;&#30456;&#20284;&#24615;&#30340;&#26032;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#38450;&#27490;&#36807;&#25311;&#21512;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03440</link><description>&lt;p&gt;
&#24102;&#26377;&#24322;&#26500;&#30456;&#20284;&#24615;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Supervised Contrastive Learning with Heterogeneous Similarity for Distribution Shifts. (arXiv:2304.03440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#24322;&#26500;&#30456;&#20284;&#24615;&#30340;&#26032;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#38450;&#27490;&#36807;&#25311;&#21512;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#30340;&#20998;&#24067;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#21457;&#29983;&#21464;&#21270;&#20250;&#23548;&#33268;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#36827;&#32780;&#20005;&#37325;&#24433;&#21709;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#36807;&#25311;&#21512;&#26159;&#20854;&#21407;&#22240;&#20043;&#19968;&#65292;&#21512;&#36866;&#30340;&#27491;&#21017;&#21270;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#24433;&#21709;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31561;&#39640;&#24230;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#21487;&#20197;&#38450;&#27490;&#36807;&#25311;&#21512;&#65292;&#35757;&#32451;&#27169;&#22411;&#36991;&#20813;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#24615;&#33021;&#36864;&#21270;&#12290;&#20316;&#32773;&#23558;&#23545;&#27604;&#25439;&#22833;&#20013;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#25193;&#23637;&#20026;&#26356;&#36890;&#29992;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#24182;&#24314;&#35758;&#22312;&#27604;&#36739;&#26679;&#26412;&#19982;&#27491;&#26679;&#26412;&#25110;&#36127;&#26679;&#26412;&#26102;&#20351;&#29992;&#19981;&#21516;&#30340;&#21442;&#25968;&#65292;&#22312;&#29702;&#35770;&#19978;&#36825;&#19968;&#24314;&#35758;&#34987;&#35777;&#26126;&#21487;&#20197;&#20316;&#20026;&#23545;&#27604;&#25439;&#22833;&#20013;&#30340;&#19968;&#31181;&#36793;&#32536;&#25928;&#24212;&#12290;&#23454;&#39564;&#22312;&#27169;&#25311;&#20998;&#24067;&#20559;&#31227;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#21253;&#25324;&#23376;&#31181;&#32676;&#20559;&#31227;&#21644;...&#65288;&#21407;&#25991;&#26410;&#23436;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
Distribution shifts are problems where the distribution of data changes between training and testing, which can significantly degrade the performance of a model deployed in the real world. Recent studies suggest that one reason for the degradation is a type of overfitting, and that proper regularization can mitigate the degradation, especially when using highly representative models such as neural networks. In this paper, we propose a new regularization using the supervised contrastive learning to prevent such overfitting and to train models that do not degrade their performance under the distribution shifts. We extend the cosine similarity in contrastive loss to a more general similarity measure and propose to use different parameters in the measure when comparing a sample to a positive or negative example, which is analytically shown to act as a kind of margin in contrastive loss. Experiments on benchmark datasets that emulate distribution shifts, including subpopulation shift and do
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35777;&#26126;&#20855;&#26377;&#19981;&#21464;&#34920;&#31034;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#38750;&#32467;&#26500;&#21270;&#28508;&#22312;&#34920;&#31034;&#65292;&#22240;&#27492;&#20351;&#19981;&#21464;&#24615;&#25104;&#20026;&#22495;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.03431</link><description>&lt;p&gt;
&#40065;&#26834;&#19981;&#21464;&#34920;&#31034;&#20013;&#30340;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization In Robust Invariant Representation. (arXiv:2304.03431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35777;&#26126;&#20855;&#26377;&#19981;&#21464;&#34920;&#31034;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#38750;&#32467;&#26500;&#21270;&#28508;&#22312;&#34920;&#31034;&#65292;&#22240;&#27492;&#20351;&#19981;&#21464;&#24615;&#25104;&#20026;&#22495;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#24120;&#35265;&#21464;&#25442;&#30340;&#19981;&#21464;&#34920;&#31034;&#26041;&#27861;&#24120;&#29992;&#20110;&#30446;&#26631;&#35782;&#21035;&#12290;&#23398;&#20064;&#19981;&#21464;&#24615;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#26356;&#23481;&#26131;&#24212;&#29992;&#12290;&#30001;&#20110;&#19981;&#25913;&#21464;&#23545;&#35937;&#22266;&#26377;&#23646;&#24615;&#30340;&#25968;&#25454;&#21464;&#25442;&#26159;&#35782;&#21035;&#20219;&#21153;&#20013;&#20027;&#35201;&#30340;&#22797;&#26434;&#24615;&#26469;&#28304;&#65292;&#23545;&#36825;&#20123;&#21464;&#25442;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#24182;&#31616;&#21270;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#35797;&#22270;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#20855;&#26377;&#26576;&#20123;&#21464;&#25442;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#22312;&#20808;&#21069;&#26410;&#35265;&#22495;&#20013;&#26159;&#21542;&#20173;&#20855;&#26377;&#19981;&#21464;&#24615;&#65311;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#19981;&#21464;&#34920;&#31034;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#38750;&#32467;&#26500;&#21270;&#28508;&#22312;&#34920;&#31034;&#65292;&#22240;&#27492;&#20351;&#19981;&#21464;&#24615;&#25104;&#20026;&#22495;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised approaches for learning representations invariant to common transformations are used quite often for object recognition. Learning invariances makes models more robust and practical to use in real-world scenarios. Since data transformations that do not change the intrinsic properties of the object cause the majority of the complexity in recognition tasks, models that are invariant to these transformations help reduce the amount of training data required. This further increases the model's efficiency and simplifies training. In this paper, we investigate the generalization of invariant representations on out-of-distribution data and try to answer the question: Do model representations invariant to some transformations in a particular seen domain also remain invariant in previously unseen domains? Through extensive experiments, we demonstrate that the invariant model learns unstructured latent representations that are robust to distribution shifts, thus making invariance a de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#36755;&#20986;&#20013;&#30340;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2304.03427</link><description>&lt;p&gt;
&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cleansing Jewel: A Neural Spelling Correction Model Built On Google OCR-ed Tibetan Manuscripts. (arXiv:2304.03427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#36755;&#20986;&#20013;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#25991;&#23398;&#32773;&#22312;&#30740;&#31350;&#21382;&#21490;&#12289;&#23447;&#25945;&#21644;&#31038;&#20250;&#25919;&#27835;&#32467;&#26500;&#31561;&#26041;&#38754;&#32463;&#24120;&#20381;&#36182;&#20110;&#21476;&#20195;&#25163;&#31295;&#12290;&#34429;&#28982;OCR&#25216;&#26415;&#21487;&#20197;&#23558;&#36825;&#20123;&#23453;&#36149;&#25163;&#31295;&#25968;&#23383;&#21270;&#65292;&#20294;&#22810;&#25968;&#25163;&#31295;&#22240;&#30952;&#25439;&#32780;&#36807;&#26102;&#65292;OCR&#31243;&#24207;&#27809;&#21150;&#27861;&#35782;&#21035;&#32763;&#39029;&#30340;&#34394;&#28129;&#25110;&#27745;&#28173;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#36755;&#20986;&#20013;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#20998;&#20026;&#22235;&#20010;&#37096;&#20998;&#65306;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#35757;&#32451;&#21644;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#21407;&#22987;&#34255;&#25991;&#30005;&#23376;&#25991;&#26412;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#29305;&#24449;&#24037;&#31243;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#20004;&#32452;&#32467;&#26500;&#21270;&#25968;&#25454;&#26694;&#8212;&#8212;&#19968;&#32452;&#21305;&#37197;&#30340;&#29609;&#20855;&#25968;&#25454;&#21644;&#19968;&#32452;&#21305;&#37197;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;Transformer&#26550;&#26500;&#20013;&#23454;&#29616;&#20102;&#32622;&#20449;&#24230;&#24471;&#20998;&#26426;&#21046;&#26469;&#25191;&#34892;&#25340;&#20889;&#26657;&#27491;&#20219;&#21153;&#12290;&#26681;&#25454;&#25439;&#22833;&#21644;&#23383;&#31526;&#38169;&#35823;&#29575;&#65292;&#25105;&#20204;&#30340;Transformer + &#32622;&#20449;&#24230;&#24471;&#20998;&#26426;&#21046;&#27604;&#20854;&#20182;&#24120;&#29992;&#30340;&#25340;&#20889;&#26657;&#27491;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scholars in the humanities rely heavily on ancient manuscripts to study history, religion, and socio-political structures in the past. Many efforts have been devoted to digitizing these precious manuscripts using OCR technology, but most manuscripts were blemished over the centuries so that an Optical Character Recognition (OCR) program cannot be expected to capture faded graphs and stains on pages. This work presents a neural spelling correction model built on Google OCR-ed Tibetan Manuscripts to auto-correct OCR-ed noisy output. This paper is divided into four sections: dataset, model architecture, training and analysis. First, we feature-engineered our raw Tibetan etext corpus into two sets of structured data frames -- a set of paired toy data and a set of paired real data. Then, we implemented a Confidence Score mechanism into the Transformer architecture to perform spelling correction tasks. According to the Loss and Character Error Rate, our Transformer + Confidence score mechani
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;&#39537;&#21160;&#21644;&#26080;&#27169;&#22411;&#20248;&#21183;&#12289;&#20351;&#29992;Youla-Kucera&#21442;&#25968;&#21270;&#23450;&#20041;&#25628;&#32034;&#22495;&#25552;&#20379;&#31283;&#23450;&#24615;&#20445;&#35777;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#20869;&#37096;&#27169;&#22411;&#23454;&#29616;&#26367;&#20195;&#26041;&#27861;&#65292;&#37319;&#29992;&#31070;&#32463;&#32593;&#32476;&#26080;&#32541;&#22320;&#19982;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#24211;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#20004;&#20010;&#27700;&#31665;&#31995;&#32479;&#30340;&#27169;&#25311;&#20013;&#65292;&#36890;&#36807;&#34920;&#36798;&#21442;&#25968;&#21270;&#30340;&#38750;&#32447;&#24615;&#31283;&#23450;&#31639;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.03422</link><description>&lt;p&gt;
&#29992;&#20110;&#31283;&#23450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A modular framework for stabilizing deep reinforcement learning control. (arXiv:2304.03422v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03422
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;&#39537;&#21160;&#21644;&#26080;&#27169;&#22411;&#20248;&#21183;&#12289;&#20351;&#29992;Youla-Kucera&#21442;&#25968;&#21270;&#23450;&#20041;&#25628;&#32034;&#22495;&#25552;&#20379;&#31283;&#23450;&#24615;&#20445;&#35777;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#20869;&#37096;&#27169;&#22411;&#23454;&#29616;&#26367;&#20195;&#26041;&#27861;&#65292;&#37319;&#29992;&#31070;&#32463;&#32593;&#32476;&#26080;&#32541;&#22320;&#19982;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#24211;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#20004;&#20010;&#27700;&#31665;&#31995;&#32479;&#30340;&#27169;&#25311;&#20013;&#65292;&#36890;&#36807;&#34920;&#36798;&#21442;&#25968;&#21270;&#30340;&#38750;&#32447;&#24615;&#31283;&#23450;&#31639;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35774;&#35745;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21270;&#39537;&#21160;&#21644;&#26080;&#27169;&#22411;&#20248;&#21183;&#65292;&#20197;&#21450;&#20351;&#29992;Youla-Kucera&#21442;&#25968;&#21270;&#23450;&#20041;&#25628;&#32034;&#22495;&#25552;&#20379;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;&#26368;&#36817;&#34892;&#20026;&#31995;&#32479;&#30340;&#36827;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#30340;&#20869;&#37096;&#27169;&#22411;&#65307;&#36825;&#20351;&#24471;&#21487;&#20197;&#22522;&#20110;&#36755;&#20837;&#36755;&#20986;&#25506;&#32034;&#25968;&#25454;&#23436;&#20840;&#23454;&#29616;&#20351;&#29992;Youla-Kucera&#21442;&#25968;&#21270;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#34920;&#31034;&#21442;&#25968;&#21270;&#30340;&#38750;&#32447;&#24615;&#31283;&#23450;&#31639;&#23376;&#38598;&#65292;&#21487;&#20197;&#26080;&#32541;&#22320;&#19982;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#24211;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#20004;&#20010;&#27700;&#31665;&#31995;&#32479;&#30340;&#27169;&#25311;&#20013;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for the design of feedback controllers that combines the optimization-driven and model-free advantages of deep reinforcement learning with the stability guarantees provided by using the Youla-Kucera parameterization to define the search domain. Recent advances in behavioral systems allow us to construct a data-driven internal model; this enables an alternative realization of the Youla-Kucera parameterization based entirely on input-output exploration data. Using a neural network to express a parameterized set of nonlinear stable operators enables seamless integration with standard deep learning libraries. We demonstrate the approach on a realistic simulation of a two-tank system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36830;&#32493;&#32454;&#21270;&#26469;&#20943;&#23569;&#20851;&#38190;&#35789;&#35823;&#25253;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#28145;&#24230;&#20851;&#38190;&#35789;&#30417;&#27979;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2304.03416</link><description>&lt;p&gt;
&#35813;&#25991;&#39064;&#30446;&#20026;&#65306;"&#21483;&#37266;&#36824;&#26159;&#19981;&#21483;&#37266;&#65306;&#36890;&#36807;&#36830;&#32493;&#32454;&#21270;&#26469;&#20943;&#23569;&#20851;&#38190;&#35789;&#35823;&#25253;"
&lt;/p&gt;
&lt;p&gt;
To Wake-up or Not to Wake-up: Reducing Keyword False Alarm by Successive Refinement. (arXiv:2304.03416v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36830;&#32493;&#32454;&#21270;&#26469;&#20943;&#23569;&#20851;&#38190;&#35789;&#35823;&#25253;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#28145;&#24230;&#20851;&#38190;&#35789;&#30417;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#30417;&#27979;&#31995;&#32479;&#25345;&#32493;&#22788;&#29702;&#38899;&#39057;&#27969;&#20197;&#26816;&#27979;&#20851;&#38190;&#35789;&#12290;&#35774;&#35745;&#36825;&#31181;&#31995;&#32479;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#38477;&#20302;&#20551;&#35686;&#25253;&#65288;FA&#65289;&#65292;&#21363;&#31995;&#32479;&#34429;&#28982;&#26410;&#34987;&#21796;&#37266;&#20294;&#35823;&#27880;&#20876;&#20851;&#38190;&#35789;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#20248;&#38597;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36981;&#24490;&#20840;&#27010;&#29575;&#27861;&#21017;&#12290;&#25105;&#20204;&#23637;&#31034;&#29616;&#26377;&#30340;&#28145;&#24230;&#20851;&#38190;&#35789;&#30417;&#27979;&#26426;&#21046;&#21487;&#20197;&#36890;&#36807;&#36830;&#32493;&#32454;&#21270;&#24471;&#21040;&#25913;&#36827;&#65292;&#20854;&#20013;&#31995;&#32479;&#39318;&#20808;&#20998;&#31867;&#36755;&#20837;&#38899;&#39057;&#26159;&#21542;&#20026;&#35821;&#38899;&#65292;&#28982;&#21518;&#20998;&#31867;&#36755;&#20837;&#26159;&#21542;&#31867;&#20284;&#20110;&#20851;&#38190;&#35789;&#65292;&#26368;&#21518;&#20998;&#31867;&#21475;&#22836;&#21457;&#20986;&#30340;&#20851;&#38190;&#35789;&#26159;&#21738;&#20010;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36328;&#22810;&#20010;&#27169;&#22411;&#65292;&#20854;&#21442;&#25968;&#33539;&#22260;&#20174;13K&#21040;2.41M&#19981;&#31561;&#65292;&#36830;&#32493;&#32454;&#21270;&#25216;&#26415;&#22312;&#22495;&#20869;&#30041;&#23384;FA&#25968;&#25454;&#19978;&#23558;FA&#20943;&#23569;&#20102;&#39640;&#36798;8&#20493;&#65292;&#22312;&#22495;&#22806;&#65288;OOD&#65289;FA&#25968;&#25454;&#19978;&#21017;&#23558;&#20854;&#20943;&#23569;&#20102;&#39640;&#36798;7&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#30340;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#28145;&#24230;&#20851;&#38190;&#35789;&#30417;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyword spotting systems continuously process audio streams to detect keywords. One of the most challenging tasks in designing such systems is to reduce False Alarm (FA) which happens when the system falsely registers a keyword despite the keyword not being uttered. In this paper, we propose a simple yet elegant solution to this problem that follows from the law of total probability. We show that existing deep keyword spotting mechanisms can be improved by Successive Refinement, where the system first classifies whether the input audio is speech or not, followed by whether the input is keyword-like or not, and finally classifies which keyword was uttered. We show across multiple models with size ranging from 13K parameters to 2.41M parameters, the successive refinement technique reduces FA by up to a factor of 8 on in-domain held-out FA data, and up to a factor of 7 on out-of-domain (OOD) FA data. Further, our proposed approach is "plug-and-play" and can be applied to any deep keyword 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23485;&#20294;&#26377;&#38480;&#30340;&#29305;&#24449;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#38480;&#23485;&#24230;&#25928;&#24212;&#30340;&#21160;&#21147;&#23398;&#65292;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#26435;&#37325;&#38543;&#26426;&#21021;&#22987;&#21270;&#19979;DMFT&#24207;&#21442;&#25968;&#27874;&#21160;&#30340;&#34920;&#24449;&#20197;&#21450;&#29305;&#24449;&#23398;&#20064;&#22914;&#20309;&#21160;&#24577;&#22320;&#20943;&#23569;&#26368;&#32456;NTK&#21644;&#26368;&#32456;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.03408</link><description>&lt;p&gt;
&#26377;&#38480;&#23485;&#24230;&#26680;&#21644;&#24179;&#22343;&#22330;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39044;&#27979;&#27874;&#21160;&#21160;&#21147;&#23398;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks. (arXiv:2304.03408v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23485;&#20294;&#26377;&#38480;&#30340;&#29305;&#24449;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#38480;&#23485;&#24230;&#25928;&#24212;&#30340;&#21160;&#21147;&#23398;&#65292;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#26435;&#37325;&#38543;&#26426;&#21021;&#22987;&#21270;&#19979;DMFT&#24207;&#21442;&#25968;&#27874;&#21160;&#30340;&#34920;&#24449;&#20197;&#21450;&#29305;&#24449;&#23398;&#20064;&#22914;&#20309;&#21160;&#24577;&#22320;&#20943;&#23569;&#26368;&#32456;NTK&#21644;&#26368;&#32456;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#23485;&#20294;&#26377;&#38480;&#30340;&#29305;&#24449;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#38480;&#23485;&#24230;&#25928;&#24212;&#30340;&#21160;&#21147;&#23398;&#12290;&#19982;&#35768;&#22810;&#20808;&#21069;&#30340;&#20998;&#26512;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#38024;&#23545;&#29305;&#24449;&#23398;&#20064;&#24378;&#24230;&#30340;&#38750;&#24494;&#25200;&#26377;&#38480;&#23485;&#24230;&#30340;&#32467;&#26524;&#12290;&#20174;&#26080;&#38480;&#23485;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26680;&#21644;&#39044;&#27979;&#21160;&#21147;&#23398;&#30340;&#21160;&#21147;&#23398;&#24179;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#25551;&#36848;&#24320;&#22987;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#26435;&#37325;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#19979;DMFT&#24207;&#21442;&#25968;$\mathcal{O}(1/\sqrt{\text{width}})$&#27874;&#21160;&#30340;&#34920;&#24449;&#12290;&#22312;&#32593;&#32476;&#35757;&#32451;&#30340;&#25042;&#24816;&#26497;&#38480;&#20013;&#65292;&#25152;&#26377;&#26680;&#37117;&#26159;&#38543;&#26426;&#30340;&#20294;&#22312;&#26102;&#38388;&#19978;&#38745;&#27490;&#30340;&#65292;&#39044;&#27979;&#26041;&#24046;&#20855;&#26377;&#36890;&#29992;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#23500;&#26377;&#29305;&#24449;&#23398;&#20064;&#30340;&#21306;&#22495;&#65292;&#26680;&#21644;&#39044;&#27979;&#30340;&#27874;&#21160;&#26159;&#21160;&#24577;&#32806;&#21512;&#19988;&#26041;&#24046;&#21487;&#20197;&#34987;&#33258;&#27965;&#35745;&#31639;&#12290;&#22312;&#20004;&#23618;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29305;&#24449;&#23398;&#20064;&#22914;&#20309;&#21160;&#24577;&#22320;&#20943;&#23569;&#26368;&#32456;NTK&#21644;&#26368;&#32456;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the dynamics of finite width effects in wide but finite feature learning neural networks. Unlike many prior analyses, our results, while perturbative in width, are non-perturbative in the strength of feature learning. Starting from a dynamical mean field theory (DMFT) description of infinite width deep neural network kernel and prediction dynamics, we provide a characterization of the $\mathcal{O}(1/\sqrt{\text{width}})$ fluctuations of the DMFT order parameters over random initialization of the network weights. In the lazy limit of network training, all kernels are random but static in time and the prediction variance has a universal form. However, in the rich, feature learning regime, the fluctuations of the kernels and predictions are dynamically coupled with variance that can be computed self-consistently. In two layer networks, we show how feature learning can dynamically reduce the variance of the final NTK and final network predictions. We also show how initialization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#21306;&#22495;&#23545;&#27604;&#30340;&#21307;&#23398;&#22270;&#20687;&#33258;&#30417;&#30563;&#23398;&#20064;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#22810;&#22120;&#23448;&#20998;&#21106;&#31561;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03406</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#21306;&#22495;&#23545;&#27604;&#30340;&#21307;&#23398;&#22270;&#20687;&#33258;&#30417;&#30563;&#23398;&#20064;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Localized Region Contrast for Enhancing Self-Supervised Learning in Medical Image Segmentation. (arXiv:2304.03406v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#21306;&#22495;&#23545;&#27604;&#30340;&#21307;&#23398;&#22270;&#20687;&#33258;&#30417;&#30563;&#23398;&#20064;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#22810;&#22120;&#23448;&#20998;&#21106;&#31561;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#34920;&#26126;&#21487;&#20197;&#20174;&#26080;&#26631;&#31614;&#22270;&#20687;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#36825;&#23548;&#33268;&#20154;&#20204;&#23545;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#20852;&#36259;&#22686;&#21152;&#20102;&#65292;&#22240;&#20026;&#26080;&#26631;&#31614;&#22270;&#20687;&#20016;&#23500;&#32780;&#26377;&#26631;&#31614;&#22270;&#20687;&#24456;&#38590;&#33719;&#24471;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#34987;&#24314;&#27169;&#20026;&#22270;&#20687;&#32423;&#21035;&#30340;&#21028;&#21035;&#25110;&#29983;&#25104;&#20195;&#29702;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#22810;&#22120;&#23448;&#20998;&#21106;&#31561;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#25152;&#38656;&#30340;&#26356;&#32454;&#33268;&#32423;&#21035;&#30340;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#38598;&#25104;&#20102;&#23616;&#37096;&#21306;&#22495;&#23545;&#27604;&#65288;LRC&#65289;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;Felzenszwalb&#31639;&#27861;&#35782;&#21035;&#36229;&#20687;&#32032;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#23545;&#27604;&#37319;&#26679;&#25439;&#22833;&#36827;&#34892;&#23616;&#37096;&#23545;&#27604;&#23398;&#20064;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#22810;&#22120;&#23448;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#24182;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in self-supervised learning have demonstrated that effective visual representations can be learned from unlabeled images. This has led to increased interest in applying self-supervised learning to the medical domain, where unlabeled images are abundant and labeled images are difficult to obtain. However, most self-supervised learning approaches are modeled as image level discriminative or generative proxy tasks, which may not capture the finer level representations necessary for dense prediction tasks like multi-organ segmentation. In this paper, we propose a novel contrastive learning framework that integrates Localized Region Contrast (LRC) to enhance existing self-supervised pre-training methods for medical image segmentation. Our approach involves identifying Super-pixels by Felzenszwalb's algorithm and performing local contrastive learning using a novel contrastive sampling loss. Through extensive experiments on three multi-organ segmentation datasets, we demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37327;&#21270;&#37327;&#23376;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26080;&#35770;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12289;&#25293;&#25668;&#27425;&#25968;&#12289;ansatz&#12289;&#35757;&#32451;&#31639;&#27861;&#20197;&#21450;&#37327;&#23376;&#30828;&#20214;&#22122;&#22768;&#30340;&#23384;&#22312;&#22914;&#20309;&#12290;</title><link>http://arxiv.org/abs/2304.03398</link><description>&lt;p&gt;
&#37327;&#23376;&#30456;&#23481;&#39044;&#27979;&#29992;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Quantum Conformal Prediction for Reliable Uncertainty Quantification in Quantum Machine Learning. (arXiv:2304.03398v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37327;&#21270;&#37327;&#23376;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26080;&#35770;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12289;&#25293;&#25668;&#27425;&#25968;&#12289;ansatz&#12289;&#35757;&#32451;&#31639;&#27861;&#20197;&#21450;&#37327;&#23376;&#30828;&#20214;&#22122;&#22768;&#30340;&#23384;&#22312;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26159;&#22312;&#24403;&#21069;&#30340;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;(NISQ)&#35745;&#31639;&#26426;&#26102;&#20195;&#20013;&#20248;&#21270;&#37327;&#23376;&#31639;&#27861;&#30340;&#26377;&#21069;&#36884;&#30340;&#32534;&#31243;&#33539;&#24335;&#12290;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#27867;&#21270;&#24615;&#33021;&#65292;&#22240;&#20026;&#35774;&#35745;&#32773;&#30340;&#30446;&#26631;&#26159;&#22312;&#27979;&#35797;&#26465;&#20214;&#19979;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#29616;&#26377;&#30340;&#27867;&#21270;&#20998;&#26512;&#34429;&#28982;&#33021;&#22815;&#35782;&#21035;&#37325;&#35201;&#30340;&#19968;&#33324;&#36235;&#21183;&#21644;&#35268;&#27169;&#23450;&#24459;&#65292;&#20294;&#19981;&#33021;&#29992;&#20110;&#20026;&#37327;&#23376;&#27169;&#22411;&#25152;&#20316;&#20986;&#30340;&#20915;&#31574;&#20998;&#37197;&#21487;&#38752;&#21644;&#26377;&#20449;&#24687;&#37327;&#30340;&#8220;&#35823;&#24046;&#26465;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37327;&#21270;&#37327;&#23376;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26080;&#35770;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12289;&#25293;&#25668;&#27425;&#25968;&#12289;ansatz&#12289;&#35757;&#32451;&#31639;&#27861;&#20197;&#21450;&#37327;&#23376;&#30828;&#20214;&#22122;&#22768;&#30340;&#23384;&#22312;&#22914;&#20309;&#65292;&#22312;&#27010;&#29575;&#24615;&#30456;&#23481;&#39044;&#27979;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#37327;&#23376;&#27169;&#22411;&#30340;&#20219;&#24847;&#21487;&#33021;&#23567;&#30340;&#25293;&#25668;&#27425;&#25968;&#36716;&#25442;&#20026;&#19968;&#32452;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning is a promising programming paradigm for the optimization of quantum algorithms in the current era of noisy intermediate scale quantum (NISQ) computers. A fundamental challenge in quantum machine learning is generalization, as the designer targets performance under testing conditions, while having access only to limited training data. Existing generalization analyses, while identifying important general trends and scaling laws, cannot be used to assign reliable and informative "error bars" to the decisions made by quantum models. In this article, we propose a general methodology that can reliably quantify the uncertainty of quantum models, irrespective of the amount of training data, of the number of shots, of the ansatz, of the training algorithm, and of the presence of quantum hardware noise. The approach, which builds on probabilistic conformal prediction, turns an arbitrary, possibly small, number of shots from a pre-trained quantum model into a set predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#27604;&#36739;&#20256;&#32479;&#26041;&#27861;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22788;&#29702;&#22823;&#37327;&#35838;&#31243;&#35780;&#35770;&#65292;&#36827;&#34892;&#24773;&#24863;&#26497;&#24615;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.03394</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#35838;&#31243;&#35780;&#35770;&#30340;&#35266;&#28857;&#25366;&#25496;&#21644;&#20027;&#39064;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Opinion Mining and Topic Classification of Course Reviews. (arXiv:2304.03394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#27604;&#36739;&#20256;&#32479;&#26041;&#27861;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22788;&#29702;&#22823;&#37327;&#35838;&#31243;&#35780;&#35770;&#65292;&#36827;&#34892;&#24773;&#24863;&#26497;&#24615;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25945;&#32946;&#24037;&#20316;&#32773;&#21644;&#31649;&#29702;&#32773;&#26469;&#35828;&#65292;&#23398;&#29983;&#23545;&#35838;&#31243;&#30340;&#21453;&#39304;&#24847;&#35265;&#38750;&#24120;&#37325;&#35201;&#65292;&#26080;&#35770;&#35838;&#31243;&#30340;&#31867;&#22411;&#25110;&#26426;&#26500;&#22914;&#20309;&#12290;&#22312;&#26426;&#26500;&#32423;&#21035;&#25110;&#22312;&#32447;&#35770;&#22363;&#19978;&#22788;&#29702;&#22823;&#37327;&#30340;&#24320;&#25918;&#21453;&#39304;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#20102;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#35838;&#31243;&#35780;&#35770;&#12290;&#25105;&#20204;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#30446;&#30340;&#26159;&#20102;&#35299;&#23398;&#29983;&#30340;&#24773;&#24863;&#21644;&#20027;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#24403;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#22914;&#35789;&#23884;&#20837;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;BERT&#65288;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;&#65289;&#12289;RoBERTa&#65288;&#32463;&#36807;&#20248;&#21270;&#30340;BERT&#26041;&#27861;&#65289;&#21644;XLNet&#65288;&#24191;&#20041;&#33258;&#22238;&#24402;&#39044;&#35757;&#32451;&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#36825;&#20123;&#25216;&#26415;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#24046;&#24322;&#12290;&#36825;&#39033;&#27604;&#36739;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#24212;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24773;&#24863;&#26497;&#24615;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Student opinions for a course are important to educators and administrators, regardless of the type of the course or the institution. Reading and manually analyzing open-ended feedback becomes infeasible for massive volumes of comments at institution level or online forums. In this paper, we collected and pre-processed a large number of course reviews publicly available online. We applied machine learning techniques with the goal to gain insight into student sentiments and topics. Specifically, we utilized current Natural Language Processing (NLP) techniques, such as word embeddings and deep neural networks, and state-of-the-art BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly optimized BERT approach) and XLNet (Generalized Auto-regression Pre-training). We performed extensive experimentation to compare these techniques versus traditional approaches. This comparative study demonstrates how to apply modern machine learning approaches for sentiment polari
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03392</link><description>&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge. (arXiv:2304.03392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#20010;&#24615;&#21270;&#25968;&#23383;&#20581;&#24247;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#30340;&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#20197;&#39044;&#27979;&#24178;&#39044;&#25928;&#26524;&#24182;&#20248;&#21270;&#24178;&#39044;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;&#34394;&#25311;&#25945;&#32451;&#31995;&#32479;&#65292;&#24110;&#21161;&#24739;&#32773;&#22362;&#25345;&#34892;&#20026;&#21464;&#38761;&#24178;&#39044;&#65288;BCI&#65289;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#39044;&#27979;&#24739;&#32773;&#26159;&#21542;&#20250;&#25191;&#34892;&#30446;&#26631;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#21453;&#20107;&#23454;&#20363;&#23376;&#36827;&#34892;&#29305;&#24449;&#25511;&#21046;&#65292;&#20197;&#25351;&#23548;&#20010;&#24615;&#21270;BCI&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#21709;&#24212;&#27700;&#24179;&#30340;&#27169;&#25311;&#24739;&#32773;&#25968;&#25454;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are developing a virtual coaching system that helps patients adhere to behavior change interventions (BCI). Our proposed system predicts whether a patient will perform the targeted behavior and uses counterfactual examples with feature control to guide personalizsation of BCI. We evaluated our prediction model using simulated patient data with varying levels of receptivity to intervention.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#19981;&#21516;&#23041;&#32961;&#27169;&#22411;&#19979;&#30340;DNN&#32467;&#26500;&#25552;&#21462;&#25216;&#26415;&#65292;&#20854;&#20013;EZClone&#21033;&#29992;&#32858;&#21512;GPU&#25991;&#20214;&#20316;&#20026;&#20391;&#20449;&#36947;&#26469;&#39044;&#27979;DNN&#32467;&#26500;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03388</link><description>&lt;p&gt;
EZClone&#65306;&#36890;&#36807;GPU&#25191;&#34892;&#25991;&#20214;&#30340;&#24418;&#29366;&#31934;&#28860;&#25552;&#39640;DNN&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
EZClone: Improving DNN Model Extraction Attack via Shape Distillation from GPU Execution Profiles. (arXiv:2304.03388v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#19981;&#21516;&#23041;&#32961;&#27169;&#22411;&#19979;&#30340;DNN&#32467;&#26500;&#25552;&#21462;&#25216;&#26415;&#65292;&#20854;&#20013;EZClone&#21033;&#29992;&#32858;&#21512;GPU&#25991;&#20214;&#20316;&#20026;&#20391;&#20449;&#36947;&#26469;&#39044;&#27979;DNN&#32467;&#26500;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#39044;&#27979;&#21644;&#20998;&#31867;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24050;&#32463;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23427;&#20204;&#30340;&#20351;&#29992;&#25193;&#23637;&#65292;&#23427;&#20204;&#38754;&#20020;&#21508;&#31181;&#23041;&#32961;&#12290;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#31363;&#21462;DNN&#20250;&#21361;&#21450;&#30693;&#35782;&#20135;&#26435;&#12289;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31995;&#32479;&#32423;&#20391;&#20449;&#36947;&#21487;&#29992;&#20110;&#36890;&#36807;&#26292;&#38706;&#21463;&#23475;&#32773;DNN&#30340;&#20307;&#31995;&#32467;&#26500;&#26469;&#27844;&#38706;&#27169;&#22411;&#30340;&#32454;&#33410;&#65292;&#20174;&#32780;&#21152;&#21095;&#36825;&#20123;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#19981;&#21516;&#23041;&#32961;&#27169;&#22411;&#30340;DNN&#32467;&#26500;&#25552;&#21462;&#25216;&#26415;&#12290;&#31532;&#19968;&#31181;&#25216;&#26415;&#20351;&#29992;&#24694;&#24847;&#30340;&#12289;&#21160;&#24577;&#38142;&#25509;&#30340;PyTorch&#29256;&#26412;&#65292;&#22312;&#36890;&#36807;PyTorch&#20998;&#26512;&#22120;&#26292;&#38706;&#21463;&#23475;&#32773;DNN&#32467;&#26500;&#12290;&#31532;&#20108;&#31181;&#25216;&#26415;&#31216;&#20026;EZClone&#65292;&#21033;&#29992;&#32858;&#21512;&#65288;&#32780;&#19981;&#26159;&#26102;&#38388;&#24207;&#21015;&#65289;GPU&#25991;&#20214;&#20316;&#20026;&#20391;&#20449;&#36947;&#26469;&#39044;&#27979;DNN&#32467;&#26500;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20551;&#35774;&#25915;&#20987;&#32773;&#30340;&#33021;&#21147;&#27604;&#20808;&#21069;&#30340;&#30740;&#31350;&#20302;&#12290;&#25105;&#20204;&#22312;&#26368;&#23567;&#21270;&#25915;&#20987;&#22797;&#26434;&#24615;&#30340;&#24773;&#20917;&#19979;&#35843;&#26597;&#20102;EZClone&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have become ubiquitous due to their performance on prediction and classification problems. However, they face a variety of threats as their usage spreads. Model extraction attacks, which steal DNNs, endanger intellectual property, data privacy, and security. Previous research has shown that system-level side-channels can be used to leak the architecture of a victim DNN, exacerbating these risks. We propose two DNN architecture extraction techniques catering to various threat models. The first technique uses a malicious, dynamically linked version of PyTorch to expose a victim DNN architecture through the PyTorch profiler. The second, called EZClone, exploits aggregate (rather than time-series) GPU profiles as a side-channel to predict DNN architecture, employing a simple approach and assuming little adversary capability as compared to previous work. We investigate the effectiveness of EZClone when minimizing the complexity of the attack, when applied to prun
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#22823;&#35268;&#27169;&#20294;&#26377;&#38480;&#30340;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#12290;&#20027;&#35201;&#36129;&#29486;&#20026;&#65306;&#65288;1&#65289;&#35745;&#31639;&#39640;&#26031;&#24615;&#30340;&#20462;&#27491;&#65292;&#31995;&#25968;&#30001;&#21442;&#25968;&#21021;&#22987;&#21270;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#35745;&#23398;&#30830;&#23450;&#12290;&#65288;2&#65289;&#36890;&#36807;&#35745;&#31639;&#32593;&#32476;&#19982;&#26497;&#38480;&#24773;&#20917;&#19979;&#30340;&#20559;&#24046;&#26469;&#25511;&#21046;&#32593;&#32476;&#22312;&#35757;&#32451;&#26102;&#30340;&#36755;&#20986;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03385</link><description>&lt;p&gt;
&#23485;&#31070;&#32463;&#32593;&#32476;&#65306;&#20174;&#21021;&#22987;&#21270;&#30340;&#38750;&#39640;&#26031;&#38543;&#26426;&#22330;&#21040;&#35757;&#32451;&#20013;&#30340;NTK&#20960;&#20309;&#65288;arXiv:2304.03385v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Wide neural networks: From non-gaussian random fields at initialization to the NTK geometry of training. (arXiv:2304.03385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#22823;&#35268;&#27169;&#20294;&#26377;&#38480;&#30340;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#12290;&#20027;&#35201;&#36129;&#29486;&#20026;&#65306;&#65288;1&#65289;&#35745;&#31639;&#39640;&#26031;&#24615;&#30340;&#20462;&#27491;&#65292;&#31995;&#25968;&#30001;&#21442;&#25968;&#21021;&#22987;&#21270;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#35745;&#23398;&#30830;&#23450;&#12290;&#65288;2&#65289;&#36890;&#36807;&#35745;&#31639;&#32593;&#32476;&#19982;&#26497;&#38480;&#24773;&#20917;&#19979;&#30340;&#20559;&#24046;&#26469;&#25511;&#21046;&#32593;&#32476;&#22312;&#35757;&#32451;&#26102;&#30340;&#36755;&#20986;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20013;&#21442;&#25968;&#36798;&#21040;$n=10^{14}$&#65292;&#22240;&#27492;&#30740;&#31350;&#27492;&#31867;&#32593;&#32476;&#30340;&#22823;&#35268;&#27169;&#34892;&#20026;&#21464;&#24471;&#26497;&#20026;&#37325;&#35201;&#12290;&#27492;&#21069;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#32858;&#28966;&#20110;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#23485;&#24230;&#26080;&#38480;&#22823;&#65292;&#21363;$n \to +\infty$&#26102;&#30340;&#26497;&#38480;&#24773;&#20917;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#21021;&#22987;&#21270;&#26102;&#31526;&#21512;&#39640;&#26031;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#23558;&#30740;&#31350;&#22823;&#20294;&#26377;&#38480;&#35268;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#20026;&#65306;&#65288;1&#65289;&#35745;&#31639;&#20197;$n^{-\frac{1}{2}}$&#20026;&#28176;&#36817;&#32423;&#25968;&#30340;&#39640;&#26031;&#24615;&#20462;&#27491;&#65292;&#35813;&#23637;&#24320;&#24335;&#30340;&#31995;&#25968;&#30001;&#21442;&#25968;&#21021;&#22987;&#21270;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#35745;&#23398;&#30830;&#23450;&#12290;(2) &#36890;&#36807;&#35745;&#31639;&#26377;&#38480;&#23485;&#24230;$n$&#32593;&#32476;&#19982;&#26497;&#38480;&#24773;&#20917;&#19979;&#65288;&#22312;&#35813;&#24773;&#20917;&#19979;&#32593;&#32476;&#36890;&#36807;&#32447;&#24615;&#27969;&#28436;&#21270;&#65289;&#30340;&#20559;&#24046;&#26469;&#25511;&#21046;&#32593;&#32476;&#22312;&#35757;&#32451;&#26102;&#30340;&#36755;&#20986;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#36825;&#25552;&#39640;&#20102;&#20197;&#21069;&#30340;&#20272;&#35745;&#65292;&#24471;&#21040;&#20102;&#26356;&#22909;&#30340;&#34928;&#20943;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in applications of artificial neural networks with over $n=10^{14}$ parameters make it extremely important to study the large $n$ behaviour of such networks. Most works studying wide neural networks have focused on the infinite width $n \to +\infty$ limit of such networks and have shown that, at initialization, they correspond to Gaussian processes. In this work we will study their behavior for large, but finite $n$. Our main contributions are the following:  (1) The computation of the corrections to Gaussianity in terms of an asymptotic series in $n^{-\frac{1}{2}}$. The coefficients in this expansion are determined by the statistics of parameter initialization and by the activation function.  (2) Controlling the evolution of the outputs of finite width $n$ networks, during training, by computing deviations from the limiting infinite width case (in which the network evolves through a linear flow). This improves previous estimates and yields sharper decay rates for t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#25968;&#21305;&#37197;&#31639;&#27861;&#23454;&#29616;&#21487;&#25193;&#23637;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20174;&#38750;&#32447;&#24615;&#21487;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#27169;&#22411;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#20013;&#21457;&#29616;&#25972;&#20010;&#22240;&#26524;&#22270;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#26469;&#38477;&#20302;&#20102;&#35745;&#31639;&#38376;&#27099;&#12290;</title><link>http://arxiv.org/abs/2304.03382</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#21305;&#37197;&#30340;&#21487;&#25193;&#23637;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Scalable Causal Discovery with Score Matching. (arXiv:2304.03382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03382
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#25968;&#21305;&#37197;&#31639;&#27861;&#23454;&#29616;&#21487;&#25193;&#23637;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20174;&#38750;&#32447;&#24615;&#21487;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#27169;&#22411;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#20013;&#21457;&#29616;&#25972;&#20010;&#22240;&#26524;&#22270;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#26469;&#38477;&#20302;&#20102;&#35745;&#31639;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#38750;&#32447;&#24615;&#21487;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#27169;&#22411;&#20013;&#21033;&#29992;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#20108;&#38454;&#23548;&#25968;&#26469;&#21457;&#29616;&#25972;&#20010;&#22240;&#26524;&#22270;&#12290;&#20511;&#21161;&#20110;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#36924;&#36817;&#20998;&#25968;&#20989;&#25968; $\nabla \log p(\mathbf{X})$&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;Rolland&#31561;&#20154;&#65288;2022&#65289;&#30340;&#24037;&#20316;&#65292;&#21518;&#32773;&#20165;&#20174;&#20998;&#25968;&#20013;&#24674;&#22797;&#25299;&#25169;&#39034;&#24207;&#65292;&#24182;&#38656;&#35201;&#19968;&#20010;&#26114;&#36149;&#30340;&#20462;&#21098;&#27493;&#39588;&#26469;&#28040;&#38500;&#30001;&#27492;&#39034;&#24207;&#20801;&#35768;&#30340;&#34394;&#20551;&#36793;&#32536;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#23548;&#33268;&#20102;DAS&#65288;&#21363; Discovery At Scale&#65292;&#35268;&#27169;&#21270;&#21457;&#29616;&#65289;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#19982;&#22270;&#24418;&#22823;&#23567;&#25104;&#27604;&#20363;&#30340;&#22240;&#32032;&#20943;&#23569;&#20462;&#21098;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;DAS&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#36895;&#24230;&#25552;&#21319;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#20197;&#19978;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#21407;&#21017;&#24615;&#21644;&#21487;&#25193;&#23637;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper demonstrates how to discover the whole causal graph from the second derivative of the log-likelihood in observational non-linear additive Gaussian noise models. Leveraging scalable machine learning approaches to approximate the score function $\nabla \log p(\mathbf{X})$, we extend the work of Rolland et al. (2022) that only recovers the topological order from the score and requires an expensive pruning step removing spurious edges among those admitted by the ordering. Our analysis leads to DAS (acronym for Discovery At Scale), a practical algorithm that reduces the complexity of the pruning by a factor proportional to the graph size. In practice, DAS achieves competitive accuracy with current state-of-the-art while being over an order of magnitude faster. Overall, our approach enables principled and scalable causal discovery, significantly lowering the compute bar.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#35270;&#39057;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#26041;&#27861;S$^2$VS&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#23454;&#20363;&#21306;&#20998;&#35299;&#20915;&#22810;&#20010;&#26816;&#32034;&#21644;&#26816;&#27979;&#20219;&#21153;&#65292;&#26080;&#38656;&#29992;&#21040;&#26631;&#27880;&#25968;&#25454;&#65292;&#24182;&#22312;&#21508;&#20010;&#20219;&#21153;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03378</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35270;&#39057;&#30456;&#20284;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Video Similarity Learning. (arXiv:2304.03378v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#35270;&#39057;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#26041;&#27861;S$^2$VS&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#23454;&#20363;&#21306;&#20998;&#35299;&#20915;&#22810;&#20010;&#26816;&#32034;&#21644;&#26816;&#27979;&#20219;&#21153;&#65292;&#26080;&#38656;&#29992;&#21040;&#26631;&#27880;&#25968;&#25454;&#65292;&#24182;&#22312;&#21508;&#20010;&#20219;&#21153;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35270;&#39057;&#30456;&#20284;&#24615;&#23398;&#20064;&#26041;&#27861;S$^2$VS&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#23454;&#29616;&#35270;&#39057;&#30456;&#20284;&#24615;&#23398;&#20064;&#65292;&#24182;&#19968;&#27425;&#24615;&#35299;&#20915;&#22810;&#20010;&#26816;&#32034;&#21644;&#26816;&#27979;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#20219;&#21153;&#23450;&#21046;&#30340;&#22686;&#24378;&#21644;InfoNCE&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#22312;&#33258;&#25105;&#30456;&#20284;&#24615;&#21644;&#30828;&#36127;&#30456;&#20284;&#24615;&#19978;&#21516;&#26102;&#25805;&#20316;&#30340;&#38468;&#21152;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#23398;&#20064;&#23454;&#20363;&#21306;&#20998;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31890;&#24230;&#19979;&#23450;&#20041;&#35270;&#39057;&#30456;&#20851;&#24615;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#22797;&#21046;&#35270;&#39057;&#21040;&#25551;&#36848;&#30456;&#21516;&#20107;&#20214;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#26032;&#30340;&#34920;&#29616;&#65292;&#36229;&#36234;&#20102;&#20197;&#21069;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce S$^2$VS, a video similarity learning approach with self-supervision. Self-Supervised Learning (SSL) is typically used to train deep models on a proxy task so as to have strong transferability on target tasks after fine-tuning. Here, in contrast to prior work, SSL is used to perform video similarity learning and address multiple retrieval and detection tasks at once with no use of labeled data. This is achieved by learning via instance-discrimination with task-tailored augmentations and the widely used InfoNCE loss together with an additional loss operating jointly on self-similarity and hard-negative similarity. We benchmark our method on tasks where video relevance is defined with varying granularity, ranging from video copies to videos depicting the same incident or event. We learn a single universal model that achieves state-of-the-art performance on all tasks, surpassing previously proposed methods that use labeled data. The code and pretrained models are publicly avai
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#20998;&#24067;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#31034;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#24863;&#30693;&#25110;&#20960;&#20309;&#26080;&#24863;&#30693;&#34920;&#31034;&#65292;&#20197;&#23545;&#24050;&#27979;&#37327;&#36712;&#36857;&#36827;&#34892;&#26080;&#20559;&#27604;&#36739;&#12290;&#21033;&#29992;&#35813;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#23884;&#20837;&#65292;&#22312;&#28789;&#38271;&#31867;&#20284;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03376</link><description>&lt;p&gt;
&#31070;&#32463;&#32676;&#20307;&#21160;&#24577;&#21644;&#20960;&#20309;&#30340;&#21487;&#35299;&#37322;&#32479;&#35745;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Interpretable statistical representations of neural population dynamics and geometry. (arXiv:2304.03376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#20998;&#24067;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#31034;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#24863;&#30693;&#25110;&#20960;&#20309;&#26080;&#24863;&#30693;&#34920;&#31034;&#65292;&#20197;&#23545;&#24050;&#27979;&#37327;&#36712;&#36857;&#36827;&#34892;&#26080;&#20559;&#27604;&#36739;&#12290;&#21033;&#29992;&#35813;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#23884;&#20837;&#65292;&#22312;&#28789;&#38271;&#31867;&#20284;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;&#31070;&#32463;&#20803;&#32676;&#20307;&#30340;&#21160;&#24577;&#36890;&#24120;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#28436;&#21270;&#12290;&#28982;&#32780;&#65292;&#21306;&#20998;&#20960;&#20309;&#21644;&#21160;&#24577;&#23545;&#32534;&#30721;&#30456;&#20851;&#34892;&#20026;&#21464;&#37327;&#30340;&#36129;&#29486;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#30456;&#36712;&#29305;&#24449;&#30340;&#32479;&#35745;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#20960;&#20309;&#24863;&#30693;&#25110;&#20960;&#20309;&#26080;&#24863;&#30693;&#34920;&#31034;&#65292;&#20197;&#23545;&#24050;&#27979;&#37327;&#36712;&#36857;&#36827;&#34892;&#26080;&#20559;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#32479;&#35745;&#34920;&#31034;&#21487;&#20197;&#27178;&#36328;&#31070;&#32463;&#32593;&#32476;&#23454;&#20363;&#36827;&#34892;&#25512;&#24191;&#65292;&#20197;&#21306;&#20998;&#35745;&#31639;&#26426;&#21046;&#65292;&#22312;&#20855;&#26377;&#20960;&#20309;&#23545;&#24212;&#30340;&#28789;&#38271;&#31867;&#20284;&#20219;&#21153;&#20013;&#35299;&#37322;&#23884;&#20837;&#31070;&#32463;&#21160;&#21147;&#23398;&#65292;&#24182;&#24320;&#21457;&#20855;&#26377;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#20351;&#29992;&#20869;&#22312;&#27969;&#24418;&#32467;&#26500;&#20248;&#20110;&#26102;&#38388;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamics of neuron populations during diverse tasks often evolve on low-dimensional manifolds. However, it remains challenging to discern the contributions of geometry and dynamics for encoding relevant behavioural variables. Here, we introduce an unsupervised geometric deep learning framework for representing non-linear dynamical systems based on statistical distributions of local phase portrait features. Our method provides robust geometry-aware or geometry-agnostic representations for the unbiased comparison of dynamics based on measured trajectories. We demonstrate that our statistical representation can generalise across neural network instances to discriminate computational mechanisms, obtain interpretable embeddings of neural dynamics in a primate reaching task with geometric correspondence to hand kinematics, and develop a decoding algorithm with state-of-the-art accuracy. Our results highlight the importance of using the intrinsic manifold structure over temporal informati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21457;&#29616;&#26356;&#24378;&#22823;&#30340;&#28608;&#27963;&#20989;&#25968;&#21644;&#24314;&#31435;&#26356;&#31283;&#20581;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21021;&#22987;&#21270;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#31168;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2304.03374</link><description>&lt;p&gt;
&#36890;&#36807;&#28608;&#27963;&#20989;&#25968;&#21457;&#29616;&#21644;&#33258;&#21160;&#26435;&#37325;&#21021;&#22987;&#21270;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Optimizing Neural Networks through Activation Function Discovery and Automatic Weight Initialization. (arXiv:2304.03374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21457;&#29616;&#26356;&#24378;&#22823;&#30340;&#28608;&#27963;&#20989;&#25968;&#21644;&#24314;&#31435;&#26356;&#31283;&#20581;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21021;&#22987;&#21270;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#31168;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#24050;&#26377;&#27169;&#22411;&#30340;&#21508;&#20010;&#26041;&#38754;&#26469;&#25913;&#36827;&#12290;&#24403;&#21069;&#26041;&#27861;&#20391;&#37325;&#20110;&#36229;&#21442;&#25968;&#21644;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#20854;&#20182;&#26041;&#38754;&#20063;&#21487;&#20197;&#36827;&#34892;&#20248;&#21270;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;AutoML&#30340;&#29616;&#29366;&#65292;&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21457;&#29616;&#26356;&#24378;&#22823;&#30340;&#28608;&#27963;&#20989;&#25968;&#21644;&#24314;&#31435;&#26356;&#31283;&#20581;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21021;&#22987;&#21270;&#25216;&#26415;&#12290;&#36825;&#20123;&#36129;&#29486;&#19981;&#20165;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#32780;&#19988;&#25552;&#20379;&#20102;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#26032;&#35270;&#35282;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#34920;&#26126;&#21457;&#29616;&#38024;&#23545;&#29305;&#23450;&#26550;&#26500;&#21644;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#27604;&#37325;&#22797;&#20351;&#29992;&#36890;&#29992;&#26041;&#27861;&#24615;&#33021;&#26356;&#22909;&#12290;&#20854;&#27425;&#65292;&#23427;&#34920;&#26126;&#32852;&#21512;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21516;&#32452;&#20214;&#26159;&#21327;&#21516;&#30340;&#65292;&#27604;&#21333;&#29420;&#20248;&#21270;&#32452;&#20214;&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#31532;&#19977;&#65292;&#23427;&#35777;&#26126;&#20102;&#23398;&#20064;&#34920;&#31034;&#26356;&#23481;&#26131;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated machine learning (AutoML) methods improve upon existing models by optimizing various aspects of their design. While present methods focus on hyperparameters and neural network topologies, other aspects of neural network design can be optimized as well. To further the state of the art in AutoML, this dissertation introduces techniques for discovering more powerful activation functions and establishing more robust weight initialization for neural networks. These contributions improve performance, but also provide new perspectives on neural network optimization. First, the dissertation demonstrates that discovering solutions specialized to specific architectures and tasks gives better performance than reusing general approaches. Second, it shows that jointly optimizing different components of neural networks is synergistic, and results in better performance than optimizing individual components alone. Third, it demonstrates that learned representations are easier to optimize tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALARM&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#25903;&#25345;&#20174;&#24322;&#24120;&#26816;&#27979;&#21040;&#20154;&#26426;&#20132;&#20114;&#24335;&#22788;&#29702;&#65292;&#20174;&#32780;&#26368;&#32456;&#23454;&#29616;&#26032;&#35268;&#21017;&#34917;&#20805;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#30417;&#30563;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.03368</link><description>&lt;p&gt;
&#20174;&#35299;&#37322;&#21040;&#34892;&#21160;&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#20154;&#26426;&#21327;&#20316;&#24322;&#24120;&#25512;&#29702;&#21644;&#31649;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From Explanation to Action: An End-to-End Human-in-the-loop Framework for Anomaly Reasoning and Management. (arXiv:2304.03368v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALARM&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#25903;&#25345;&#20174;&#24322;&#24120;&#26816;&#27979;&#21040;&#20154;&#26426;&#20132;&#20114;&#24335;&#22788;&#29702;&#65292;&#20174;&#32780;&#26368;&#32456;&#23454;&#29616;&#26032;&#35268;&#21017;&#34917;&#20805;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#30417;&#30563;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#36890;&#24120;&#26159;&#21046;&#36896;&#12289;&#21307;&#30103;&#12289;&#37329;&#34701;&#12289;&#30417;&#25511;&#31561;&#21508;&#31181;&#31995;&#32479;&#20013;&#25925;&#38556;&#25110;&#20302;&#25928;&#30340;&#25351;&#26631;&#12290;&#23613;&#31649;&#30001;&#20110;&#20854;&#23454;&#38469;&#24847;&#20041;&#32780;&#25991;&#29486;&#20013;&#23384;&#22312;&#30528;&#26377;&#25928;&#30340;&#26816;&#27979;&#31639;&#27861;&#65292;&#20294;&#33258;&#21160;&#24322;&#24120;&#26816;&#27979;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#23569;&#20351;&#29992;&#12290;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#38656;&#35201;&#20154;&#26426;&#21327;&#20316;&#22788;&#29702;&#65292;&#36229;&#20986;&#20102;&#26816;&#27979;&#31561;&#36827;&#31243;&#65292;&#20363;&#22914;&#39564;&#35777;&#21644;&#25925;&#38556;&#25490;&#38500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ALARM&#65288;&#20998;&#26512;&#24072;&#21327;&#20316;&#24322;&#24120;&#25512;&#29702;&#21644;&#31649;&#29702;&#65289;&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;&#65292;&#20840;&#38754;&#25903;&#25345;&#24322;&#24120;&#25366;&#25496;&#21608;&#26399;&#65292;&#20174;&#26816;&#27979;&#21040;&#34892;&#21160;&#12290;&#38500;&#20102;&#38024;&#23545;&#26032;&#20852;&#24322;&#24120;&#30340;&#26080;&#30417;&#30563;&#26816;&#27979;&#65292;&#23427;&#36824;&#25552;&#20379;&#24322;&#24120;&#35299;&#37322;&#21644;&#20132;&#20114;&#24335;GUI&#65292;&#20197;&#36827;&#34892;&#20154;&#26426;&#21327;&#20316;&#36827;&#31243;&#65292;&#21253;&#25324;&#21487;&#35270;&#21270;&#25506;&#32034;&#12289;&#24863;&#30693;&#21644;&#26368;&#32456;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#26816;&#27979;&#35268;&#21017;&#36827;&#34892;&#34892;&#21160;&#65292;&#20174;&#32780;&#24110;&#21161;&#20851;&#38381;&#8220;&#29615;&#36335;&#8221;&#65292;&#26032;&#35268;&#21017;&#34917;&#20805;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#30417;&#30563;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomalies are often indicators of malfunction or inefficiency in various systems such as manufacturing, healthcare, finance, surveillance, to name a few. While the literature is abundant in effective detection algorithms due to this practical relevance, autonomous anomaly detection is rarely used in real-world scenarios. Especially in high-stakes applications, a human-in-the-loop is often involved in processes beyond detection such as verification and troubleshooting. In this work, we introduce ALARM (for Analyst-in-the-Loop Anomaly Reasoning and Management); an end-to-end framework that supports the anomaly mining cycle comprehensively, from detection to action. Besides unsupervised detection of emerging anomalies, it offers anomaly explanations and an interactive GUI for human-in-the-loop processes -- visual exploration, sense-making, and ultimately action-taking via designing new detection rules -- that help close ``the loop'' as the new rules complement rule-based supervised detect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#35782;&#21035;&#24615;&#30340;DF&#35299;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2304.03365</link><description>&lt;p&gt;
&#22870;&#21169;&#36716;&#31227;&#30340;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Decision-Focused Learning for Reward Transfer. (arXiv:2304.03365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#35782;&#21035;&#24615;&#30340;DF&#35299;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20915;&#31574;&#37325;&#28857;&#65288;Decision-focused&#65292;DF&#65289;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#34987;&#20171;&#32461;&#20026;&#19968;&#31181;&#24378;&#26377;&#21147;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#19987;&#27880;&#20110;&#23398;&#20064;&#26368;&#26377;&#21033;&#20110;&#33719;&#24471;&#39640;&#25253;&#37228;&#30340;MDP&#21160;&#24577;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#19987;&#27880;&#20110;&#30452;&#25509;&#20248;&#21270;&#25253;&#37228;&#26469;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#65292;&#20294;&#20174;MLE&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#19981;&#22815;&#20934;&#30830;&#65292;&#22240;&#27492;&#21487;&#33021;&#23545;&#22870;&#21169;&#20989;&#25968;&#30340;&#21464;&#21270;&#24456;&#33030;&#24369;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;DF&#35299;&#30340;&#38750;&#35782;&#21035;&#24615;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#29609;&#20855;&#31034;&#20363;&#21644;&#21307;&#30103;&#27169;&#25311;&#22120;&#19978;&#23637;&#31034;&#20102;RDF&#26174;&#30528;&#22686;&#21152;&#20102;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused (DF) model-based reinforcement learning has recently been introduced as a powerful algorithm which can focus on learning the MDP dynamics which are most relevant for obtaining high rewards. While this approach increases the performance of agents by focusing the learning towards optimizing for the reward directly, it does so by learning less accurate dynamics (from a MLE standpoint), and may thus be brittle to changes in the reward function. In this work, we develop the robust decision-focused (RDF) algorithm which leverages the non-identifiability of DF solutions to learn models which maximize expected returns while simultaneously learning models which are robust to changes in the reward function. We demonstrate on a variety of toy example and healthcare simulators that RDF significantly increases the robustness of DF to changes in the reward function, without decreasing the overall return the agent obtains.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#20197;&#30456;&#23545;&#36739;&#23569;&#30340;&#25968;&#25454;&#37327;&#39044;&#27979;NMR&#21270;&#23398;&#20301;&#31227;&#65292;&#23637;&#31034;&#20102;&#22312;&#29305;&#23450;&#28342;&#21058;&#20013;&#39044;&#27979;&#23567;&#20998;&#23376;&#30340;19F&#21644;13C NMR&#21270;&#23398;&#20301;&#31227;&#26041;&#38754;&#30340;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.03361</link><description>&lt;p&gt;
&#23567;&#25968;&#25454;&#37327;&#19979;&#30340;NMR&#20301;&#31227;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
NMR shift prediction from small data quantities. (arXiv:2304.03361v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03361
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#20197;&#30456;&#23545;&#36739;&#23569;&#30340;&#25968;&#25454;&#37327;&#39044;&#27979;NMR&#21270;&#23398;&#20301;&#31227;&#65292;&#23637;&#31034;&#20102;&#22312;&#29305;&#23450;&#28342;&#21058;&#20013;&#39044;&#27979;&#23567;&#20998;&#23376;&#30340;19F&#21644;13C NMR&#21270;&#23398;&#20301;&#31227;&#26041;&#38754;&#30340;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;NMR&#21270;&#23398;&#20301;&#31227;&#39044;&#27979;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#23613;&#21487;&#33021;&#22810;&#30340;&#25968;&#25454;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#20294;&#26159;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22914;&#24322;&#26680;&#26680;&#32032;&#65292;&#24456;&#38590;&#24471;&#21040;&#22823;&#37327;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;&#30456;&#23545;&#36739;&#23569;&#30340;&#25968;&#25454;&#37327;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#39044;&#27979;&#29305;&#23450;&#28342;&#21058;&#20013;&#23567;&#20998;&#23376;&#30340;19F&#21644;13C NMR&#21270;&#23398;&#20301;&#31227;&#26469;&#23637;&#31034;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction of chemical shift in NMR using machine learning methods is typically done with the maximum amount of data available to achieve the best results. In some cases, such large amounts of data are not available, e.g. for heteronuclei. We demonstrate a novel machine learning model which is able to achieve good results with comparatively low amounts of data. We show this by predicting 19F and 13C NMR chemical shifts of small molecules in specific solvents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#65292;&#23427;&#21253;&#25324;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;top-K&#37319;&#26679;&#22686;&#24378;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25152;&#26377;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.03344</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#22270;&#21327;&#20316;&#20449;&#21495;&#21435;&#22122;&#19982;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Graph Collaborative Signals Denoising and Augmentation for Recommendation. (arXiv:2304.03344v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#65292;&#23427;&#21253;&#25324;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;top-K&#37319;&#26679;&#22686;&#24378;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25152;&#26377;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21327;&#20316;&#36807;&#28388;&#65288;GCF&#65289;&#26159;&#25429;&#25417;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#38454;&#21327;&#21516;&#20449;&#21495;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;GCF&#30340;&#21452;&#21521;&#37051;&#25509;&#30697;&#38453;&#65292;&#20854;&#23450;&#20041;&#20102;&#22522;&#20110;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#36827;&#34892;&#32858;&#21512;&#30340;&#37051;&#23621;&#65292;&#23545;&#20110;&#26377;&#22823;&#37327;&#20132;&#20114;&#20294;&#19981;&#36275;&#30340;&#29992;&#25143;/&#39033;&#30446;&#26469;&#35828;&#21487;&#33021;&#26159;&#22024;&#26434;&#30340;&#12290;&#27492;&#22806;&#65292;&#37051;&#25509;&#30697;&#38453;&#24573;&#30053;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#32858;&#21512;&#30340;&#26377;&#30410;&#37051;&#23621;&#30340;&#33539;&#22260;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#65292;&#23427;&#21253;&#25324;&#20102;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#65292;&#20197;&#24179;&#34913;&#25152;&#26377;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#25968;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#26469;&#33719;&#24471;&#29992;&#25143;/&#39033;&#30446;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;top-K&#37319;&#26679;&#22686;&#24378;&#20102;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#30697;&#38453;&#12290;&#25105;&#20204;&#36824;&#22686;&#24378;&#20102;&#23545;&#31216;&#30340;&#29992;&#25143;-&#29992;&#25143;&#21644;&#39033;&#30446;-&#39033;&#30446;&#30456;&#20851;&#32452;&#20214;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#25152;&#26377;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph collaborative filtering (GCF) is a popular technique for capturing high-order collaborative signals in recommendation systems. However, GCF's bipartite adjacency matrix, which defines the neighbors being aggregated based on user-item interactions, can be noisy for users/items with abundant interactions and insufficient for users/items with scarce interactions. Additionally, the adjacency matrix ignores user-user and item-item correlations, which can limit the scope of beneficial neighbors being aggregated.  In this work, we propose a new graph adjacency matrix that incorporates user-user and item-item correlations, as well as a properly designed user-item interaction matrix that balances the number of interactions across all users. To achieve this, we pre-train a graph-based recommendation method to obtain users/items embeddings, and then enhance the user-item interaction matrix via top-K sampling. We also augment the symmetric user-user and item-item correlation components to th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21033;&#29992;&#33258;&#26059;&#30005;&#23376;&#29289;&#29702;&#27700;&#24211;&#36827;&#34892;&#33258;&#27835;&#22411;&#38271;&#26399;&#39044;&#27979;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#21644;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#26159;&#36866;&#21512;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#23398;&#20064;&#30340;&#12290;&#36825;&#37324;&#25552;&#20986;&#30340;&#22522;&#20110;&#24494;&#26059;&#30913;&#38567;&#31359;&#32467;&#30340;&#28065;&#26059;&#23376;&#21487;&#20197;&#20316;&#20026;&#23454;&#29616;&#27492;&#31181;RC&#30340;&#21407;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03343</link><description>&lt;p&gt;
&#33258;&#20027;&#39044;&#27979;&#21644;&#38271;&#26399;&#23478;&#24237;&#33021;&#28304;&#36127;&#33655;&#39044;&#27979;&#30340;&#33258;&#26059;&#30005;&#23376;&#29289;&#29702;&#27700;&#24211;
&lt;/p&gt;
&lt;p&gt;
Spintronic Physical Reservoir for Autonomous Prediction and Long-Term Household Energy Load Forecasting. (arXiv:2304.03343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21033;&#29992;&#33258;&#26059;&#30005;&#23376;&#29289;&#29702;&#27700;&#24211;&#36827;&#34892;&#33258;&#27835;&#22411;&#38271;&#26399;&#39044;&#27979;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#21644;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#26159;&#36866;&#21512;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#23398;&#20064;&#30340;&#12290;&#36825;&#37324;&#25552;&#20986;&#30340;&#22522;&#20110;&#24494;&#26059;&#30913;&#38567;&#31359;&#32467;&#30340;&#28065;&#26059;&#23376;&#21487;&#20197;&#20316;&#20026;&#23454;&#29616;&#27492;&#31181;RC&#30340;&#21407;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#26059;&#30005;&#23376;&#29289;&#29702;&#27700;&#24211;&#36827;&#34892;&#20102;&#33258;&#27835;&#22411;&#38271;&#26399;&#39044;&#27979;&#12290;&#30001;&#20110;&#30913;&#21270;&#21160;&#21147;&#23398;&#30340;&#30701;&#26399;&#35760;&#24518;&#29305;&#24615;&#65292;&#27700;&#24211;&#29366;&#24577;&#20013;&#20135;&#29983;&#20102;&#38750;&#32447;&#24615;&#65292;&#21487;&#29992;&#20110;&#20351;&#29992;&#31616;&#21333;&#32447;&#24615;&#22238;&#24402;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#30340;&#38271;&#26399;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#39044;&#27979;&#38454;&#27573;&#65292;&#36755;&#20986;&#30452;&#25509;&#39304;&#20837;&#27700;&#24211;&#30340;&#36755;&#20837;&#20013;&#36827;&#34892;&#33258;&#27835;&#22411;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27700;&#24211;&#29992;&#20110;&#24314;&#27169;&#35832;&#22914;Mackey-Glass&#31561;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#21644;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22914;&#23478;&#24237;&#24314;&#31569;&#33021;&#32791;&#12290;&#30001;&#20110;&#21482;&#26377;RC&#30340;&#26368;&#21518;&#19968;&#23618;&#38656;&#35201;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#27492;&#23427;&#38750;&#24120;&#36866;&#21512;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#23398;&#20064;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22522;&#20110;&#24494;&#26059;&#30913;&#38567;&#31359;&#32467;&#30340;&#28065;&#26059;&#23376;&#21487;&#33021;&#29992;&#20316;&#21407;&#22411;RC&#65292;&#20294;&#20219;&#20309;&#20855;&#26377;&#38750;&#32447;&#24615;&#30913;&#21270;&#34892;&#20026;&#30340;&#32435;&#31859;&#30913;&#38567;&#36947;&#32467;&#37117;&#21487;&#20197;&#23454;&#29616;&#27492;&#31181;RC&#12290;&#36890;&#36807;&#27604;&#36739;&#25105;&#20204;&#30340;&#33258;&#26059;&#30005;&#23376;&#29289;&#29702;RC&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In this study, we have shown autonomous long-term prediction with a spintronic physical reservoir. Due to the short-term memory property of the magnetization dynamics, non-linearity arises in the reservoir states which could be used for long-term prediction tasks using simple linear regression for online training. During the prediction stage, the output is directly fed to the input of the reservoir for autonomous prediction. We employ our proposed reservoir for the modeling of the chaotic time series such as Mackey-Glass and dynamic time-series data, such as household building energy loads. Since only the last layer of a RC needs to be trained with linear regression, it is well suited for learning in real time on edge devices. Here we show that a skyrmion based magnetic tunnel junction can potentially be used as a prototypical RC but any nanomagnetic magnetic tunnel junction with nonlinear magnetization behavior can implement such a RC. By comparing our spintronic physical RC approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#22823;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#21028;&#23450;&#26159;&#21542;&#23384;&#22312;&#26159;&#19968;&#20010;NP&#23436;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;&#35745;&#31639;&#26368;&#22823;&#22240;&#23376;&#20998;&#35299;&#30340;&#31639;&#27861;Ord2Factor&#12290;</title><link>http://arxiv.org/abs/2304.03338</link><description>&lt;p&gt;
&#26368;&#22823;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Maximal Ordinal Two-Factorizations. (arXiv:2304.03338v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#22823;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#21028;&#23450;&#26159;&#21542;&#23384;&#22312;&#26159;&#19968;&#20010;NP&#23436;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;&#35745;&#31639;&#26368;&#22823;&#22240;&#23376;&#20998;&#35299;&#30340;&#31639;&#27861;Ord2Factor&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#24418;&#24335;&#32972;&#26223;&#20013;&#65292;&#24207;&#25968;&#22240;&#23376;&#26159;&#20854;&#20851;&#31995;&#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#27010;&#24565;&#26684;&#20013;&#30340;&#38142;&#65292;&#21363;&#23545;&#24212;&#20110;&#32447;&#24615;&#39034;&#24207;&#30340;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#12290;&#20026;&#20102;&#21487;&#35270;&#21270;&#24418;&#24335;&#19978;&#19979;&#25991;&#20013;&#30340;&#25968;&#25454;&#65292;Ganter&#21644;Glodeanu&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#20010;&#24207;&#25968;&#22240;&#23376;&#30340;&#21452;&#22270;&#12290;&#20026;&#20102;&#20351;&#21452;&#22270;&#26377;&#29992;&#65292;&#37325;&#35201;&#30340;&#26159;&#36825;&#20123;&#22240;&#23376;&#23613;&#21487;&#33021;&#21253;&#21547;&#26356;&#22810;&#25968;&#25454;&#28857;&#65292;&#21363;&#35206;&#30422;&#23613;&#21487;&#33021;&#22810;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#30740;&#31350;&#36825;&#26679;&#30340;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30465;&#30053;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#30340;&#24418;&#24335;&#32972;&#26223;&#20013;&#20004;&#20010;&#22240;&#23376;&#30340;&#19981;&#30456;&#20132;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21028;&#23450;&#32473;&#23450;&#22823;&#23567;&#30340;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#26159;&#21542;&#23384;&#22312;&#26159;&#19968;&#20010;NP&#23436;&#20840;&#38382;&#39064;&#65292;&#36825;&#20351;&#24471;&#35745;&#31639;&#26368;&#22823;&#22240;&#23376;&#20998;&#35299;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31639;&#27861;Ord2Factor&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#35745;&#31639;&#22823;&#30340;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a formal context, an ordinal factor is a subset of its incidence relation that forms a chain in the concept lattice, i.e., a part of the dataset that corresponds to a linear order. To visualize the data in a formal context, Ganter and Glodeanu proposed a biplot based on two ordinal factors. For the biplot to be useful, it is important that these factors comprise as much data points as possible, i.e., that they cover a large part of the incidence relation. In this work, we investigate such ordinal two-factorizations. First, we investigate for formal contexts that omit ordinal two-factorizations the disjointness of the two factors. Then, we show that deciding on the existence of two-factorizations of a given size is an NP-complete problem which makes computing maximal factorizations computationally expensive. Finally, we provide the algorithm Ord2Factor that allows us to compute large ordinal two-factorizations.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#19979;&#22810;&#26631;&#31614;&#25490;&#21517;&#38382;&#39064;&#22312;&#25209;&#22788;&#29702;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#24182;&#39318;&#27425;&#32473;&#20986;&#22522;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#30340;&#31561;&#20215;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.03337</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#26631;&#31614;&#25490;&#21517;&#30340;&#21487;&#23398;&#20064;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Learnability of Multilabel Ranking. (arXiv:2304.03337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03337
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#19979;&#22810;&#26631;&#31614;&#25490;&#21517;&#38382;&#39064;&#22312;&#25209;&#22788;&#29702;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#24182;&#39318;&#27425;&#32473;&#20986;&#22522;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#30340;&#31561;&#20215;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#22810;&#26631;&#31614;&#25490;&#21517;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#32593;&#32476;&#25628;&#32034;&#12289;&#26032;&#38395;&#25253;&#36947;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#39046;&#22495;&#12290;&#20294;&#26159;&#65292;&#20851;&#20110;&#22810;&#26631;&#31614;&#25490;&#21517;&#35774;&#32622;&#20013;&#21487;&#23398;&#20064;&#24615;&#30340;&#26368;&#22522;&#26412;&#38382;&#39064;&#20173;&#26410;&#35299;&#31572;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#19979;&#22810;&#26631;&#31614;&#25490;&#21517;&#38382;&#39064;&#22312;&#25209;&#22788;&#29702;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#21516;&#26102;&#20063;&#39318;&#27425;&#32473;&#20986;&#20102;&#22522;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#30340;&#31561;&#20215;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilabel ranking is a central task in machine learning with widespread applications to web search, news stories, recommender systems, etc. However, the most fundamental question of learnability in a multilabel ranking setting remains unanswered. In this paper, we characterize the learnability of multilabel ranking problems in both the batch and online settings for a large family of ranking losses. Along the way, we also give the first equivalence class of ranking losses based on learnability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20174;&#19981;&#21516;&#23545;&#35805;QA&#35821;&#26009;&#24211;&#20013;&#29983;&#25104;&#30340;ChatGPT&#30340;&#21709;&#24212;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#27491;&#30830;&#31572;&#26696;&#30340;&#30456;&#20284;&#24230;&#12290;&#30740;&#31350;&#21457;&#29616;ChatGPT&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#31572;&#26696;&#65292;&#25552;&#20379;&#20102;&#28508;&#22312;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.03325</link><description>&lt;p&gt;
ChatGPT-Crawler&#65306;&#21457;&#29616;ChatGPT&#26159;&#21542;&#30495;&#30340;&#30693;&#36947;&#33258;&#24049;&#22312;&#35828;&#20160;&#20040;&#12290;&#65288;arXiv:2304.03325v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-Crawler: Find out if ChatGPT really knows what it's talking about. (arXiv:2304.03325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20174;&#19981;&#21516;&#23545;&#35805;QA&#35821;&#26009;&#24211;&#20013;&#29983;&#25104;&#30340;ChatGPT&#30340;&#21709;&#24212;&#65292;&#24182;&#27604;&#36739;&#20102;&#20854;&#19982;&#27491;&#30830;&#31572;&#26696;&#30340;&#30456;&#20284;&#24230;&#12290;&#30740;&#31350;&#21457;&#29616;ChatGPT&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#31572;&#26696;&#65292;&#25552;&#20379;&#20102;&#28508;&#22312;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#20854;&#20013;&#65292;OpenAI&#24320;&#21457;&#30340;ChatGPT&#24050;&#32463;&#25104;&#20026;&#26089;&#26399;&#37319;&#29992;&#32773;&#20013;&#38750;&#24120;&#27969;&#34892;&#30340;&#27169;&#22411;&#65292;&#20182;&#20204;&#29978;&#33267;&#23558;&#20854;&#35270;&#20026;&#23458;&#25143;&#26381;&#21153;&#12289;&#25945;&#32946;&#12289;&#21307;&#30103;&#21644;&#37329;&#34701;&#31561;&#35768;&#22810;&#39046;&#22495;&#30340;&#30772;&#22351;&#24615;&#25216;&#26415;&#12290;&#29702;&#35299;&#36825;&#20123;&#21021;&#26399;&#29992;&#25143;&#30340;&#35266;&#28857;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20026;&#19981;&#21516;&#39046;&#22495;&#25216;&#26415;&#30340;&#28508;&#22312;&#20248;&#21183;&#12289;&#21155;&#21183;&#12289;&#25104;&#21151;&#25110;&#22833;&#36133;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;ChatGPT&#20174;&#19981;&#21516;&#23545;&#35805;QA&#35821;&#26009;&#24211;&#20013;&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;&#30740;&#31350;&#20351;&#29992;BERT&#30456;&#20284;&#24230;&#20998;&#25968;&#23558;&#36825;&#20123;&#21709;&#24212;&#19982;&#27491;&#30830;&#31572;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#33719;&#24471;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26631;&#31614;&#12290;&#36824;&#35745;&#31639;&#24182;&#27604;&#36739;&#20102;&#35780;&#20272;&#20998;&#25968;&#65292;&#20197;&#30830;&#23450;GPT-3&#65286;GPT-4&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#30830;&#23450;&#20102;ChatGPT&#25552;&#20379;&#38169;&#35823;&#31572;&#26696;&#30340;&#24773;&#20917;&#65292;&#20026;&#30456;&#20851;&#39046;&#22495;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have gained considerable interest for their impressive performance on various tasks. Among these models, ChatGPT developed by OpenAI has become extremely popular among early adopters who even regard it as a disruptive technology in many fields like customer service, education, healthcare, and finance. It is essential to comprehend the opinions of these initial users as it can provide valuable insights into the potential strengths, weaknesses, and success or failure of the technology in different areas. This research examines the responses generated by ChatGPT from different Conversational QA corpora. The study employed BERT similarity scores to compare these responses with correct answers and obtain Natural Language Inference(NLI) labels. Evaluation scores were also computed and compared to determine the overall performance of GPT-3 \&amp; GPT-4. Additionally, the study identified instances where ChatGPT provided incorrect answers to questions, providing insights into
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Bayesian&#26694;&#26550;&#21644;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#30340;&#26041;&#27861;CO-PAINT&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#20687;&#20462;&#22797;&#20013;&#20462;&#22797;&#21306;&#22495;&#21644;&#26410;&#20462;&#22797;&#21306;&#22495;&#19981;&#21327;&#35843;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.03322</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#30340;&#36830;&#32493;&#22270;&#20687;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Towards Coherent Image Inpainting Using Denoising Diffusion Implicit Models. (arXiv:2304.03322v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Bayesian&#26694;&#26550;&#21644;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#30340;&#26041;&#27861;CO-PAINT&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#20687;&#20462;&#22797;&#20013;&#20462;&#22797;&#21306;&#22495;&#21644;&#26410;&#20462;&#22797;&#21306;&#22495;&#19981;&#21327;&#35843;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20462;&#22797;&#26159;&#25351;&#22312;&#22522;&#20110;&#37096;&#20998;&#21487;&#35265;&#30340;&#21442;&#32771;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#23436;&#25972;&#12289;&#33258;&#28982;&#30340;&#22270;&#20687;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20351;&#29992;&#22266;&#23450;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20462;&#22797;&#24448;&#24448;&#20250;&#23548;&#33268;&#20462;&#22797;&#21644;&#26410;&#20462;&#22797;&#30340;&#21306;&#22495;&#19981;&#21327;&#35843;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CO-PAINT&#31639;&#27861;&#65292;&#20351;&#29992;Bayesian&#26694;&#26550;&#26469;&#21516;&#26102;&#20462;&#25913;&#22270;&#20687;&#20013;&#30340;&#20462;&#22797;&#21644;&#26410;&#20462;&#22797;&#21306;&#22495;&#65292;&#24182;&#37319;&#29992;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#20272;&#35745;&#20687;&#32032;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#20026;&#20102;&#25552;&#39640;&#21518;&#39564;&#20272;&#35745;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#37319;&#21462;&#19968;&#31181;&#21435;&#22122;&#26041;&#27861;&#26469;&#21435;&#38500;&#22312;&#19981;&#21516;&#25193;&#25955;&#38454;&#27573;&#20135;&#29983;&#30340;&#20013;&#38388;&#22270;&#20687;&#22122;&#22768;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#23450;&#37327;&#32467;&#26524;&#26041;&#38754;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image inpainting refers to the task of generating a complete, natural image based on a partially revealed reference image. Recently, many research interests have been focused on addressing this problem using fixed diffusion models. These approaches typically directly replace the revealed region of the intermediate or final generated images with that of the reference image or its variants. However, since the unrevealed regions are not directly modified to match the context, it results in incoherence between revealed and unrevealed regions. To address the incoherence problem, a small number of methods introduce a rigorous Bayesian framework, but they tend to introduce mismatches between the generated and the reference images due to the approximation errors in computing the posterior distributions. In this paper, we propose COPAINT, which can coherently inpaint the whole image without introducing mismatches. COPAINT also uses the Bayesian framework to jointly modify both revealed and unre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#32422;&#26463;&#21644;&#20381;&#36182;&#25439;&#22833;&#30340;&#33258;&#36866;&#24212;&#20915;&#31574;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#22312;&#32447;&#21644;&#38750;&#32447;&#24615;&#35782;&#21035;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.03321</link><description>&lt;p&gt;
&#24102;&#32422;&#26463;&#21644;&#20381;&#36182;&#25439;&#22833;&#30340;&#33258;&#36866;&#24212;&#20915;&#31574;&#65306;&#24615;&#33021;&#20445;&#35777;&#21450;&#20854;&#22312;&#22312;&#32447;&#21644;&#38750;&#32447;&#24615;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive Decision-Making with Constraints and Dependent Losses: Performance Guarantees and Applications to Online and Nonlinear Identification. (arXiv:2304.03321v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#32422;&#26463;&#21644;&#20381;&#36182;&#25439;&#22833;&#30340;&#33258;&#36866;&#24212;&#20915;&#31574;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#22312;&#32447;&#21644;&#38750;&#32447;&#24615;&#35782;&#21035;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#33258;&#36866;&#24212;&#20915;&#31574;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#37325;&#22797;&#36873;&#25321;&#26377;&#38480;&#30340;&#36873;&#39033;&#26469;&#20248;&#21270;&#32047;&#31215;&#24615;&#33021;&#30446;&#26631;&#12290;&#30456;&#23545;&#20110;&#32463;&#20856;&#30340;&#39044;&#27979;&#19982;&#19987;&#23478;&#24314;&#35758;&#65292;&#25105;&#20204;&#32771;&#34385;&#25439;&#22833;&#21463;&#32422;&#26463;&#30340;&#24773;&#20917;&#65292;&#24182;&#25512;&#23548;&#21033;&#29992;&#20248;&#21270;&#21644;&#35745;&#31639;&#19978;&#30340;&#26377;&#25928;&#26041;&#24335;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#26159;&#23454;&#20363;&#30456;&#20851;&#30340;&#65292;&#21363;&#21033;&#29992;&#29615;&#22659;&#30340;&#27425;&#20248;&#36873;&#25321;&#65292;&#24182;&#23558;&#20854;&#21453;&#26144;&#22312;&#25105;&#20204;&#30340;&#36951;&#25022;&#19978;&#38480;&#20013;&#12290;&#32422;&#26463;&#22788;&#29702;&#25439;&#22833;&#20043;&#38388;&#30340;&#19968;&#33324;&#20381;&#36182;&#20851;&#31995;&#65288;&#29978;&#33267;&#36328;&#26102;&#38388;&#65289;&#65292;&#24182;&#19988;&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#29992;&#20110;&#32771;&#34385;&#25439;&#22833;&#39044;&#31639;&#65292;&#29615;&#22659;&#19981;&#20801;&#35768;&#36229;&#36807;&#35813;&#39044;&#31639;&#12290;&#22312;&#20004;&#20010;&#25968;&#20540;&#31034;&#20363;&#20013;&#31361;&#20986;&#20102;&#25152;&#24471;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#21253;&#25324;&#38750;&#32447;&#24615;&#21644;&#22312;&#32447;&#31995;&#32479;&#35782;&#21035;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider adaptive decision-making problems where an agent optimizes a cumulative performance objective by repeatedly choosing among a finite set of options. Compared to the classical prediction-with-expert-advice set-up, we consider situations where losses are constrained and derive algorithms that exploit the additional structure in optimal and computationally efficient ways. Our algorithm and our analysis is instance dependent, that is, suboptimal choices of the environment are exploited and reflected in our regret bounds. The constraints handle general dependencies between losses (even across time), and are flexible enough to also account for a loss budget, which the environment is not allowed to exceed. The performance of the resulting algorithms is highlighted in two numerical examples, which include a nonlinear and online system identification task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23558;&#31070;&#32463;&#25805;&#20316;&#31526;&#23398;&#20064;&#24212;&#29992;&#20110;&#36229;&#22768;&#26029;&#23618;&#25104;&#20687;&#21453;&#28436;&#65292;&#36890;&#36807;&#23398;&#20064;&#26102;&#38388;&#39134;&#34892;&#25968;&#25454;&#21644;&#24322;&#36136;&#22768;&#36895;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#21487;&#36991;&#20813;&#35745;&#31639;&#23494;&#38598;&#22411;&#21453;&#28436;&#36807;&#31243;&#30340;&#39044;&#27979;&#24322;&#36136;&#22768;&#22330;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#28508;&#22312;&#30340;&#22312;&#20083;&#33146;&#25104;&#20687;&#20013;&#36827;&#34892;&#36719;&#32452;&#32455;&#20998;&#24067;&#39044;&#27979;&#21644;&#32959;&#30244;&#35782;&#21035;&#30340;&#23454;&#26102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.03297</link><description>&lt;p&gt;
&#36229;&#22768;&#26029;&#23618;&#25104;&#20687;&#21453;&#28436;&#30340;&#31070;&#32463;&#25805;&#20316;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neural Operator Learning for Ultrasound Tomography Inversion. (arXiv:2304.03297v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23558;&#31070;&#32463;&#25805;&#20316;&#31526;&#23398;&#20064;&#24212;&#29992;&#20110;&#36229;&#22768;&#26029;&#23618;&#25104;&#20687;&#21453;&#28436;&#65292;&#36890;&#36807;&#23398;&#20064;&#26102;&#38388;&#39134;&#34892;&#25968;&#25454;&#21644;&#24322;&#36136;&#22768;&#36895;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#21487;&#36991;&#20813;&#35745;&#31639;&#23494;&#38598;&#22411;&#21453;&#28436;&#36807;&#31243;&#30340;&#39044;&#27979;&#24322;&#36136;&#22768;&#22330;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#28508;&#22312;&#30340;&#22312;&#20083;&#33146;&#25104;&#20687;&#20013;&#36827;&#34892;&#36719;&#32452;&#32455;&#20998;&#24067;&#39044;&#27979;&#21644;&#32959;&#30244;&#35782;&#21035;&#30340;&#23454;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25805;&#20316;&#31526;&#23398;&#20064;&#20316;&#20026;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#36827;&#34892;&#22797;&#26434;&#20989;&#25968;&#31354;&#38388;&#26144;&#23556;&#30340;&#19968;&#31181;&#25163;&#27573;&#24050;&#32463;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#31070;&#32463;&#25805;&#20316;&#31526;&#23398;&#20064;&#24212;&#29992;&#20110;&#39134;&#34892;&#26102;&#38388;&#36229;&#22768;&#35745;&#31639;&#23618;&#26512;&#25104;&#20687;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20840;&#27874;&#27714;&#35299;&#22120;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#23398;&#20064;&#26102;&#38388;&#39134;&#34892;&#65288;TOF&#65289;&#25968;&#25454;&#21644;&#24322;&#36136;&#22768;&#36895;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#35813;&#25805;&#20316;&#31526;&#23398;&#20064;&#30340;&#26032;&#39062;&#24212;&#29992;&#35268;&#36991;&#20102;&#38656;&#35201;&#35299;&#20915;&#35745;&#31639;&#23494;&#38598;&#22411;&#36845;&#20195;&#21453;&#38382;&#39064;&#30340;&#38656;&#27714;&#12290;&#35813;&#25805;&#20316;&#31526;&#23398;&#20064;&#22312;&#31163;&#32447;&#27169;&#24335;&#19979;&#23398;&#20064;&#38750;&#32447;&#24615;&#26144;&#23556;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#30340;&#21333;&#27425;&#21069;&#21521;&#36890;&#36807;&#26469;&#39044;&#27979;&#24322;&#36136;&#22768;&#22330;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#23558;&#25805;&#20316;&#31526;&#23398;&#20064;&#29992;&#20110;&#36229;&#22768;&#26029;&#23618;&#25104;&#20687;&#65292;&#20063;&#26159;&#28508;&#22312;&#30340;&#23454;&#26102;&#39044;&#27979;&#20083;&#33146;&#25104;&#20687;&#20013;&#30340;&#36719;&#32452;&#32455;&#20998;&#24067;&#20197;&#36827;&#34892;&#32959;&#30244;&#35782;&#21035;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operator learning as a means of mapping between complex function spaces has garnered significant attention in the field of computational science and engineering (CS&amp;E). In this paper, we apply Neural operator learning to the time-of-flight ultrasound computed tomography (USCT) problem. We learn the mapping between time-of-flight (TOF) data and the heterogeneous sound speed field using a full-wave solver to generate the training data. This novel application of operator learning circumnavigates the need to solve the computationally intensive iterative inverse problem. The operator learns the non-linear mapping offline and predicts the heterogeneous sound field with a single forward pass through the model. This is the first time operator learning has been used for ultrasound tomography and is the first step in potential real-time predictions of soft tissue distribution for tumor identification in beast imaging.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SS-shapelets&#30340;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#21644;&#20256;&#25773;&#30340;&#20266;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#26469;&#21457;&#29616;&#20195;&#34920;&#24615;&#24418;&#29366;&#23376;&#24207;&#21015;&#65292;&#20174;&#32780;&#25552;&#39640;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03292</link><description>&lt;p&gt;
SS-shapelets: &#20195;&#34920;&#24418;&#29366;&#23376;&#24207;&#21015;&#30340;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SS-shapelets: Semi-supervised Clustering of Time Series Using Representative Shapelets. (arXiv:2304.03292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SS-shapelets&#30340;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#21644;&#20256;&#25773;&#30340;&#20266;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#26469;&#21457;&#29616;&#20195;&#34920;&#24615;&#24418;&#29366;&#23376;&#24207;&#21015;&#65292;&#20174;&#32780;&#25552;&#39640;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#23376;&#24207;&#21015;&#26159;&#20351;&#29992;&#26412;&#22320;&#29305;&#24449;&#65288;&#23376;&#24207;&#21015;&#65289;&#37492;&#21035;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#21069;&#36884;&#30340;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#25429;&#33719;&#20195;&#34920;&#24615;&#24418;&#29366;&#23376;&#24207;&#21015;&#65292;&#22240;&#20026;&#23427;&#20204;&#20174;&#22823;&#37327;&#26080;&#20449;&#24687;&#30340;&#23376;&#24207;&#21015;&#20013;&#21457;&#29616;&#24418;&#29366;&#23376;&#24207;&#21015;&#65292;&#22240;&#27492;&#32858;&#31867;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#21644;&#20256;&#25773;&#30340;&#20266;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#26469;&#24110;&#21161;&#21457;&#29616;&#20195;&#34920;&#24615;&#24418;&#29366;&#23376;&#24207;&#21015;&#65292;&#20174;&#32780;&#25552;&#39640;&#32858;&#31867;&#20934;&#30830;&#24615;&#30340;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;&#65288;SS-Shapelets&#65289;&#12290;&#22312;SS-Shapelets&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25216;&#26415;&#26469;&#21457;&#29616;&#20195;&#34920;&#24615;&#24418;&#29366;&#23376;&#24207;&#21015;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#32858;&#31867;&#26102;&#38388;&#24207;&#21015;&#12290;1&#65289;&#19968;&#20010;&#8220;&#26174;&#33879;&#23376;&#24207;&#21015;&#38142;&#8221;&#65288;$SSC$&#65289;&#65292;&#21487;&#20197;&#20174;&#26631;&#35760;/&#20266;&#26631;&#35760;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#26174;&#33879;&#23376;&#24207;&#21015;&#65288;&#20316;&#20026;&#20505;&#36873;&#24418;&#29366;&#23376;&#24207;&#21015;&#65289;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#20174;&#27744;&#20013;&#21024;&#38500;&#22823;&#37327;&#26080;&#20449;&#24687;&#30340;&#23376;&#24207;&#21015;&#12290;2&#65289;&#19968;&#31181;&#8220;&#32447;&#24615;&#21028;&#21035;&#36873;&#25321;&#24418;&#29366;&#23376;&#24207;&#21015;&#8221;&#65288;$LDSS$&#65289;&#65292;&#23427;&#36873;&#25321;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#20855;&#26377;&#21306;&#21035;&#24615;&#30340;&#24418;&#29366;&#23376;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shapelets that discriminate time series using local features (subsequences) are promising for time series clustering. Existing time series clustering methods may fail to capture representative shapelets because they discover shapelets from a large pool of uninformative subsequences, and thus result in low clustering accuracy. This paper proposes a Semi-supervised Clustering of Time Series Using Representative Shapelets (SS-Shapelets) method, which utilizes a small number of labeled and propagated pseudo-labeled time series to help discover representative shapelets, thereby improving the clustering accuracy. In SS-Shapelets, we propose two techniques to discover representative shapelets for the effective clustering of time series. 1) A \textit{salient subsequence chain} ($SSC$) that can extract salient subsequences (as candidate shapelets) of a labeled/pseudo-labeled time series, which helps remove massive uninformative subsequences from the pool. 2) A \textit{linear discriminant select
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;NARS&#21644;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#24207;&#21015;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;NARS&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#37117;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.03291</link><description>&lt;p&gt;
&#27604;&#36739;NARS&#21644;&#24378;&#21270;&#23398;&#20064;&#65306;&#23545;ONA&#21644;$Q$-Learning&#31639;&#27861;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparing NARS and Reinforcement Learning: An Analysis of ONA and $Q$-Learning Algorithms. (arXiv:2304.03291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;NARS&#21644;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#24207;&#21015;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;NARS&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#37117;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#22522;&#20110;&#24207;&#21015;&#20219;&#21153;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;RL&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#20173;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#21644;&#21019;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20854;&#20013;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#26367;&#20195;&#26041;&#26696;&#26159;&#38750;&#20844;&#29702;&#25512;&#29702;&#31995;&#32479;&#65288;NARS&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35748;&#30693;&#25512;&#29702;&#26694;&#26550;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;NARS&#20316;&#20026;RL&#26367;&#20195;&#26041;&#26696;&#22312;&#35299;&#20915;&#22522;&#20110;&#24207;&#21015;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#20351;&#29992;Open AI gym&#21019;&#24314;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#23545;ONA&#20316;&#20026;NARS&#23454;&#29616;&#21644;$Q$-Learning&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#36825;&#20123;&#29615;&#22659;&#20855;&#26377;&#19981;&#21516;&#30340;&#38590;&#24230;&#32423;&#21035;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#19981;&#31561;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#65292;NARS&#26159;&#19968;&#20010;&#26377;&#31454;&#20105;&#21147;&#30340;RL&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, reinforcement learning (RL) has emerged as a popular approach for solving sequence-based tasks in machine learning. However, finding suitable alternatives to RL remains an exciting and innovative research area. One such alternative that has garnered attention is the Non-Axiomatic Reasoning System (NARS), which is a general-purpose cognitive reasoning framework. In this paper, we delve into the potential of NARS as a substitute for RL in solving sequence-based tasks. To investigate this, we conduct a comparative analysis of the performance of ONA as an implementation of NARS and $Q$-Learning in various environments that were created using the Open AI gym. The environments have different difficulty levels, ranging from simple to complex. Our results demonstrate that NARS is a promising alternative to RL, with competitive performance in diverse environments, particularly in non-deterministic ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28378;&#21160;&#21465;&#36848;(scrollytelling&#65289;&#30340;&#21487;&#35270;&#21270;&#35774;&#35745;&#65292;&#29992;&#20110;&#21521;&#38750;&#25216;&#26415;&#29992;&#25143;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27010;&#24565;&#65292;&#20854;&#20013;&#20197;&#23402;&#29983;&#31070;&#32463;&#32593;&#32476;&#20026;&#20363;&#65292;&#25552;&#20379;&#20102;&#20855;&#26377;&#30452;&#35266;&#35299;&#37322;&#30340;&#20132;&#20114;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.03288</link><description>&lt;p&gt;
VISHIEN-MAAT&#65306;&#22522;&#20110;&#28378;&#21160;&#21465;&#20107;&#30340;&#21487;&#35270;&#21270;&#35774;&#35745;&#65292;&#20026;&#38750;&#25216;&#26415;&#29992;&#25143;&#35299;&#37322;&#23402;&#29983;&#31070;&#32463;&#32593;&#32476;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
VISHIEN-MAAT: Scrollytelling visualization design for explaining Siamese Neural Network concept to non-technical users. (arXiv:2304.03288v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28378;&#21160;&#21465;&#36848;(scrollytelling&#65289;&#30340;&#21487;&#35270;&#21270;&#35774;&#35745;&#65292;&#29992;&#20110;&#21521;&#38750;&#25216;&#26415;&#29992;&#25143;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27010;&#24565;&#65292;&#20854;&#20013;&#20197;&#23402;&#29983;&#31070;&#32463;&#32593;&#32476;&#20026;&#20363;&#65292;&#25552;&#20379;&#20102;&#20855;&#26377;&#30452;&#35266;&#35299;&#37322;&#30340;&#20132;&#20114;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#31361;&#30772;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#39046;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20960;&#20046;&#25152;&#26377;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25216;&#26415;&#21644;&#38750;&#25216;&#26415;&#31471;&#29992;&#25143;&#24517;&#39035;&#20102;&#35299;&#36825;&#20123;&#25216;&#26415;&#20197;&#21033;&#29992;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36164;&#26009;&#37117;&#26159;&#20026;&#19987;&#23478;&#35774;&#35745;&#30340;&#65292;&#20294;&#38750;&#25216;&#26415;&#29992;&#25143;&#38656;&#35201;&#33021;&#22815;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#27493;&#39588;&#21576;&#29616;&#22797;&#26434;&#24605;&#24819;&#30340;&#21560;&#24341;&#20154;&#26448;&#26009;&#12290;&#19968;&#20010;&#36866;&#21512;&#36825;&#31181;&#24773;&#20917;&#30340;&#24037;&#20855;&#26159;&#28378;&#21160;&#21465;&#36848;(scrollytelling&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35762;&#25925;&#20107;&#30340;&#26041;&#27861;&#65292;&#20026;&#35835;&#32773;&#25552;&#20379;&#19968;&#31181;&#33258;&#28982;&#19988;&#20016;&#23500;&#30340;&#38405;&#35835;&#20307;&#39564;&#65292;&#20197;&#35835;&#32773;&#30340;&#33410;&#22863;&#21644;&#28145;&#20837;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#22797;&#26434;&#30340;&#27010;&#24565;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#35774;&#35745;&#65292;&#29992;&#20110;&#21019;&#24314;&#19968;&#20010;&#21487;&#28378;&#21160;&#35762;&#36848;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21521;&#38750;&#25216;&#26415;&#29992;&#25143;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27010;&#24565;&#12290;&#20316;&#20026;&#25105;&#20204;&#35774;&#35745;&#30340;&#19968;&#20010;&#31034;&#20363;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#28378;&#21160;&#21465;&#36848;&#65292;&#20197;&#35299;&#37322;&#23402;&#29983;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23454;&#29616;&#35270;&#35273;&#30456;&#20284;&#24615;&#21305;&#37197;&#38382;&#39064;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#20855;&#26377;&#30452;&#35266;&#35299;&#37322;&#30340;&#20132;&#20114;&#24335;&#30028;&#38754;&#65292;&#26377;&#21161;&#20110;&#21019;&#24314;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#21487;&#35270;&#21270;&#25928;&#26524;&#26469;&#23637;&#31034;&#23402;&#29983;&#31070;&#32463;&#32593;&#32476;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has witnessed rapid progress in AI research since the breakthrough in deep learning. AI technology has been applied in almost every field; therefore, technical and non-technical end-users must understand these technologies to exploit them. However existing materials are designed for experts, but non-technical users need appealing materials that deliver complex ideas in easy-to-follow steps. One notable tool that fits such a profile is scrollytelling, an approach to storytelling that provides readers with a natural and rich experience at the reader's pace, along with in-depth interactive explanations of complex concepts. Hence, this work proposes a novel visualization design for creating a scrollytelling that can effectively explain an AI concept to non-technical users. As a demonstration of our design, we created a scrollytelling to explain the Siamese Neural Network for the visual similarity matching problem. Our approach helps create a visualization valuable for a sho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20851;&#27880;&#20110;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20013;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#21512;&#25104;&#25968;&#23398;&#31243;&#24207;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;CodeT5&#21644;&#20351;&#29992;GPT-3&#26469;&#29983;&#25104;&#38656;&#35201;&#30340;&#31034;&#20363;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2304.03287</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20013;&#30340;&#25968;&#23398;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Synthesis of Mathematical programs from Natural Language Specifications. (arXiv:2304.03287v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20851;&#27880;&#20110;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20013;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#21512;&#25104;&#25968;&#23398;&#31243;&#24207;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;CodeT5&#21644;&#20351;&#29992;GPT-3&#26469;&#29983;&#25104;&#38656;&#35201;&#30340;&#31034;&#20363;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#21830;&#19994;&#39046;&#22495;&#20013;&#36935;&#21040;&#30340;&#20960;&#20010;&#20915;&#31574;&#38382;&#39064;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#25968;&#23398;&#31243;&#24207;&#65292;&#21363;&#20248;&#21270;&#38382;&#39064;&#12290;&#36827;&#34892;&#36825;&#31181;&#24314;&#27169;&#30340;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#28041;&#21450;&#21040;&#21463;&#36807;&#36816;&#31609;&#23398;&#21644;&#39640;&#32423;&#31639;&#27861;&#22521;&#35757;&#30340;&#19987;&#23478;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;&#31243;&#24207;&#21644;&#20195;&#30721;&#21512;&#25104;&#65292;AutoML&#65292;&#23398;&#20064;&#20248;&#21270;&#31561;&#26041;&#38754;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#20154;&#20851;&#27880;&#33258;&#21160;&#21270;&#21512;&#25104;&#25968;&#23398;&#31243;&#24207;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24819;&#35937;&#19968;&#31181;&#24773;&#26223;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24314;&#27169;&#30340;&#35268;&#33539;&#65292;&#21363;&#30446;&#26631;&#21644;&#32422;&#26463;&#20197;&#33258;&#28982;&#35821;&#35328;&#30340;&#24418;&#24335;&#34920;&#36798;&#65292;&#24182;&#19988;&#24517;&#39035;&#20174;&#36825;&#26679;&#30340;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20013;&#21512;&#25104;&#25968;&#23398;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20351;&#29992;&#24102;&#26377;&#25968;&#25454;&#22686;&#24378;&#21644;&#26463;&#21518;&#22788;&#29702;&#30340;CodeT5&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#21033;&#29992;GPT-3&#36827;&#34892;&#32972;&#32763;&#35793;&#20197;&#29983;&#25104;&#21512;&#25104;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#32447;&#24615;&#35268;&#21010;&#35268;&#21017;&#26469;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several decision problems that are encountered in various business domains can be modeled as mathematical programs, i.e. optimization problems. The process of conducting such modeling often requires the involvement of experts trained in operations research and advanced algorithms. Surprisingly, despite the significant advances in the methods for program and code synthesis, AutoML, learning to optimize etc., there has been little or no attention paid to automating the task of synthesizing mathematical programs. We imagine a scenario where the specifications for modeling, i.e. the objective and constraints are expressed in an unstructured form in natural language (NL) and the mathematical program has to be synthesized from such an NL specification. In this work we evaluate the efficacy of employing CodeT5 with data augmentation and post-processing of beams. We utilize GPT-3 with back translation for generation of synthetic examples. Further we apply rules of linear programming to score b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#22270;&#24418;&#25968;&#25454;&#30340;&#21453;&#23398;&#20064;&#65292;&#26088;&#22312;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#65292;&#19982;&#20854;&#20182;&#26694;&#26550;&#30456;&#27604;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24402;&#32435;&#24335;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#35753;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#22788;&#29702;&#21160;&#24577;&#25913;&#21464;&#30340;&#22270;&#24418;&#26102;&#26356;&#20855;&#26377;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03093</link><description>&lt;p&gt;
&#24402;&#32435;&#24335;&#22270;&#24418;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inductive Graph Unlearning. (arXiv:2304.03093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#22270;&#24418;&#25968;&#25454;&#30340;&#21453;&#23398;&#20064;&#65292;&#26088;&#22312;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#65292;&#19982;&#20854;&#20182;&#26694;&#26550;&#30456;&#27604;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24402;&#32435;&#24335;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#35753;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#22788;&#29702;&#21160;&#24577;&#25913;&#21464;&#30340;&#22270;&#24418;&#26102;&#26356;&#20855;&#26377;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#26426;&#22120;&#21453;&#23398;&#20064;&#8221;&#26159;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#23436;&#20840;&#21024;&#38500;&#35201;&#21024;&#38500;&#30340;&#26679;&#26412;&#23545;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#36129;&#29486;&#21644;&#20449;&#24687;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#20854;&#20182;&#26679;&#26412;&#30340;&#36129;&#29486;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#21453;&#23398;&#20064;&#26694;&#26550;&#24050;&#34987;&#25552;&#20986;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#19987;&#27880;&#20110;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#12290;&#20026;&#20102;&#23558;&#21453;&#23398;&#20064;&#25193;&#23637;&#21040;&#22270;&#24418;&#25968;&#25454;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;GraphEraser&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#26159;GraphEraser&#19987;&#38376;&#38024;&#23545;&#36716;&#31227;&#22270;&#35774;&#23450;&#36827;&#34892;&#35774;&#35745;&#65292;&#22312;&#35813;&#35774;&#23450;&#19979;&#65292;&#22270;&#24418;&#26159;&#38745;&#24577;&#30340;&#65292;&#27979;&#35797;&#33410;&#28857;&#30340;&#23646;&#24615;&#21644;&#36793;&#32536;&#22312;&#35757;&#32451;&#26399;&#38388;&#26159;&#21487;&#35265;&#30340;&#12290;&#23545;&#20110;&#24402;&#32435;&#24335;&#30340;&#35774;&#32622;&#26159;&#19981;&#21512;&#36866;&#30340;&#65292;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#22270;&#24418;&#21487;&#20197;&#26159;&#21160;&#24577;&#30340;&#65292;&#27979;&#35797;&#22270;&#24418;&#20449;&#24687;&#20107;&#20808;&#26159;&#19981;&#21487;&#35265;&#30340;&#12290;&#36825;&#31181;&#24402;&#32435;&#33021;&#21147;&#23545;&#20110;&#20855;&#26377;&#19981;&#26029;&#21457;&#23637;&#30340;&#22270;&#24418;&#65288;&#22914;&#31038;&#20132;&#23186;&#20307;&#21644;&#20132;&#26131;&#32593;&#32476;&#65289;&#30340;&#29983;&#20135;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;G...
&lt;/p&gt;
&lt;p&gt;
As a way to implement the "right to be forgotten" in machine learning, \textit{machine unlearning} aims to completely remove the contributions and information of the samples to be deleted from a trained model without affecting the contributions of other samples. Recently, many frameworks for machine unlearning have been proposed, and most of them focus on image and text data. To extend machine unlearning to graph data, \textit{GraphEraser} has been proposed. However, a critical issue is that \textit{GraphEraser} is specifically designed for the transductive graph setting, where the graph is static and attributes and edges of test nodes are visible during training. It is unsuitable for the inductive setting, where the graph could be dynamic and the test graph information is invisible in advance. Such inductive capability is essential for production machine learning systems with evolving graphs like social media and transaction networks. To fill this gap, we propose the \underline{{\bf G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#37325;&#23614;&#37096;&#27491;&#21017;&#21270;&#30340;&#25216;&#26415;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36890;&#36807;&#26126;&#30830;&#25552;&#20513;&#26356;&#37325;&#30340;&#37325;&#23614;&#35889;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#19982;&#26631;&#20934;&#27491;&#21017;&#21270;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.02911</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#37325;&#23614;&#37096;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Heavy-Tailed Regularization of Weight Matrices in Deep Neural Networks. (arXiv:2304.02911v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#37325;&#23614;&#37096;&#27491;&#21017;&#21270;&#30340;&#25216;&#26415;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36890;&#36807;&#26126;&#30830;&#25552;&#20513;&#26356;&#37325;&#30340;&#37325;&#23614;&#35889;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#19982;&#26631;&#20934;&#27491;&#21017;&#21270;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25104;&#21151;&#21644;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#32972;&#21518;&#30340;&#21407;&#22240;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20174;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#24471;&#21040;&#30340;&#26368;&#26032;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26435;&#37325;&#30697;&#38453;&#30340;&#35889;&#20998;&#26512;&#30340;&#20449;&#24687;&#65292;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#32447;&#32034;&#12290;&#19968;&#20010;&#20851;&#38190;&#21457;&#29616;&#26159;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#19982;&#20854;&#26435;&#37325;&#30697;&#38453;&#30340;&#35889;&#30340;&#37325;&#23614;&#31243;&#24230;&#30456;&#20851;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#31216;&#20026;&#37325;&#23614;&#37096;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#26126;&#30830;&#25552;&#20513;&#26435;&#37325;&#30697;&#38453;&#20013;&#26356;&#37325;&#30340;&#37325;&#23614;&#35889;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#21152;&#26435;&#38463;&#23572;&#27861;&#21644;&#31283;&#23450;&#31209;&#20316;&#20026;&#24809;&#32602;&#39033;&#65292;&#20004;&#32773;&#37117;&#21487;&#24494;&#20998;&#65292;&#20174;&#32780;&#21487;&#20197;&#30452;&#25509;&#35745;&#31639;&#23427;&#20204;&#30340;&#26799;&#24230;&#12290;&#20026;&#20102;&#36991;&#20813;&#36807;&#24230;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#24809;&#32602;&#20989;&#25968;&#30340;&#21464;&#20307;&#12290;&#28982;&#21518;&#65292;&#37319;&#29992;&#36125;&#21494;&#26031;&#32479;&#35745;&#35270;&#35282;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#23614;&#37096;&#27491;&#21017;&#21270;&#30340;&#27010;&#29575;&#35299;&#37322;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#20854;&#25928;&#26524;&#29702;&#35299;&#20026;&#26435;&#37325;&#30697;&#38453;&#30340;&#20808;&#39564;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#27491;&#21017;&#21270;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unraveling the reasons behind the remarkable success and exceptional generalization capabilities of deep neural networks presents a formidable challenge. Recent insights from random matrix theory, specifically those concerning the spectral analysis of weight matrices in deep neural networks, offer valuable clues to address this issue. A key finding indicates that the generalization performance of a neural network is associated with the degree of heavy tails in the spectrum of its weight matrices. To capitalize on this discovery, we introduce a novel regularization technique, termed Heavy-Tailed Regularization, which explicitly promotes a more heavy-tailed spectrum in the weight matrix through regularization. Firstly, we employ the Weighted Alpha and Stable Rank as penalty terms, both of which are differentiable, enabling the direct calculation of their gradients. To circumvent over-regularization, we introduce two variations of the penalty function. Then, adopting a Bayesian statistics
&lt;/p&gt;</description></item><item><title>ViralVectors&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#20174;virome&#27979;&#24207;&#25968;&#25454;&#20013;&#29983;&#25104;Minimizers&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#26377;&#25928;&#30340;&#19979;&#28216;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#38750;&#27604;&#23545;&#25216;&#26415;&#26041;&#27861;&#65292;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#30340;&#30149;&#27602;&#23478;&#26063;&#65292;&#29978;&#33267;&#23646;&#65292;&#24182;&#33021;&#22815;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;SARS-CoV-2&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02891</link><description>&lt;p&gt;
ViralVectors&#65306;&#19968;&#31181;&#32039;&#20945;&#19988;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#38750;&#27604;&#23545;&#25216;&#26415;&#29983;&#25104;virome&#29305;&#24449;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ViralVectors: Compact and Scalable Alignment-free Virome Feature Generation. (arXiv:2304.02891v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02891
&lt;/p&gt;
&lt;p&gt;
ViralVectors&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#20174;virome&#27979;&#24207;&#25968;&#25454;&#20013;&#29983;&#25104;Minimizers&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#26377;&#25928;&#30340;&#19979;&#28216;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#38750;&#27604;&#23545;&#25216;&#26415;&#26041;&#27861;&#65292;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#30340;&#30149;&#27602;&#23478;&#26063;&#65292;&#29978;&#33267;&#23646;&#65292;&#24182;&#33021;&#22815;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;SARS-CoV-2&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;SARS-CoV-2&#30340;&#27979;&#24207;&#25968;&#25454;&#37327;&#27604;&#20854;&#20182;&#22823;&#22810;&#25968;&#30149;&#27602;&#37117;&#35201;&#22823;&#33509;&#24178;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19988;SARS-CoV-2&#30340;&#25968;&#25454;&#37327;&#23558;&#32487;&#32493;&#21576;&#20960;&#20309;&#32423;&#25968;&#22686;&#38271;&#65292;&#35768;&#22810;&#22269;&#23478;&#27491;&#22312;&#22823;&#21147;&#25237;&#36164;&#22522;&#22240;&#32452;&#30417;&#27979;&#24037;&#20316;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#22788;&#29702;&#22823;&#37327;&#30340;&#24207;&#21015;&#25968;&#25454;&#20197;&#23454;&#29616;&#26377;&#25928;&#32780;&#21450;&#26102;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#25968;&#25454;&#26469;&#33258;&#21508;&#31181;&#19981;&#21516;&#30340;&#26469;&#28304;&#65306;&#27604;&#23545;&#12289;&#26410;&#27604;&#23545;&#29978;&#33267;&#26410;&#35013;&#37197;&#30340;&#21407;&#22987;&#26680;&#33527;&#37240;&#25110;&#27688;&#22522;&#37240;&#27979;&#24207;reads&#65292;&#28085;&#30422;&#25972;&#20010;&#22522;&#22240;&#32452;&#25110;&#26576;&#20123;&#21306;&#22495;&#65288;&#20363;&#22914;spike&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;ViralVectors&#65292;&#19968;&#31181;&#20174;virome&#27979;&#24207;&#25968;&#25454;&#20013;&#29983;&#25104;&#32039;&#20945;&#29305;&#24449;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#19979;&#28216;&#20998;&#26512;&#12290;&#35813;&#29983;&#25104;&#26041;&#27861;&#22522;&#20110;minimizers&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24207;&#21015;&#8220;&#31614;&#21517;&#8221;&#65292;&#20256;&#32479;&#19978;&#29992;&#20110;&#32452;&#35013;&#21644;&#35835;&#21462;&#26144;&#23556;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#39318;&#27425;&#20351;&#29992;minimizers&#36827;&#34892;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#27979;&#24207;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65306;&#65288;a&#65289;2.5M SARS-CoV-2 nanopore reads&#65292;&#65288;b&#65289;1.5M &#26410;&#27604;&#23545;&#30340;SARS-CoV-2 Illumina reads&#65292;&#20197;&#21450;&#65288;c&#65289;&#22823;&#37327;&#30340;virome reads&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#38750;&#27604;&#23545;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#30340;&#30149;&#27602;&#23478;&#26063;&#65292;&#29978;&#33267;&#23646;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#29305;&#24449;&#21521;&#37327;&#21487;&#20197;&#36731;&#26494;&#22320;&#32858;&#31867;&#21644;&#21487;&#35270;&#21270;&#65292;&#23454;&#29616;&#20102;&#30452;&#35266;&#30340;&#30149;&#27602;&#21457;&#29616;&#21644;&#25506;&#32034;&#21151;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#20844;&#24320;&#30340;SARS-CoV-2&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19968;&#20010;&#24179;&#34913;&#30340;&#20004;&#31867;SARS-CoV-2&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The amount of sequencing data for SARS-CoV-2 is several orders of magnitude larger than any virus. This will continue to grow geometrically for SARS-CoV-2, and other viruses, as many countries heavily finance genomic surveillance efforts. Hence, we need methods for processing large amounts of sequence data to allow for effective yet timely decision-making. Such data will come from heterogeneous sources: aligned, unaligned, or even unassembled raw nucleotide or amino acid sequencing reads pertaining to the whole genome or regions (e.g., spike) of interest. In this work, we propose \emph{ViralVectors}, a compact feature vector generation from virome sequencing data that allows effective downstream analysis. Such generation is based on \emph{minimizers}, a type of lightweight "signature" of a sequence, used traditionally in assembly and read mapping -- to our knowledge, the first use minimizers in this way. We validate our approach on different types of sequencing data: (a) 2.5M SARS-CoV-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;ACTION++&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#35299;&#21078;&#23545;&#27604;&#26469;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2304.02689</link><description>&lt;p&gt;
ACTION++&#65306;&#20351;&#29992;&#33258;&#36866;&#24212;&#35299;&#21078;&#23545;&#27604;&#24230;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast. (arXiv:2304.02689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;ACTION++&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#35299;&#21078;&#23545;&#27604;&#26469;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25968;&#25454;&#36890;&#24120;&#34920;&#29616;&#20026;&#38271;&#23614;&#20998;&#24067;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#36825;&#33258;&#28982;&#23548;&#33268;&#23569;&#25968;&#31867;&#21035;&#65288;&#21363;&#36793;&#30028;&#21306;&#22495;&#25110;&#32597;&#35265;&#29289;&#20307;&#65289;&#30340;&#20998;&#31867;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#37197;&#22791;&#26080;&#30417;&#30563;&#23545;&#27604;&#26631;&#20934;&#65292;&#22312;&#38271;&#23614;&#22330;&#26223;&#20013;&#26174;&#30528;&#25913;&#36827;&#20102;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#22312;&#31867;&#21035;&#20998;&#24067;&#20063;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#26631;&#35760;&#25968;&#25454;&#37096;&#20998;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACTION++&#65292;&#19968;&#31181;&#25913;&#36827;&#30340;&#20855;&#26377;&#33258;&#36866;&#24212;&#35299;&#21078;&#23545;&#27604;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#21307;&#23398;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical data often exhibits long-tail distributions with heavy class imbalance, which naturally leads to difficulty in classifying the minority classes (i.e., boundary regions or rare objects). Recent work has significantly improved semi-supervised medical image segmentation in long-tailed scenarios by equipping them with unsupervised contrastive criteria. However, it remains unclear how well they will perform in the labeled portion of data where class distribution is also highly imbalanced. In this work, we present ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. Specifically, we propose an adaptive supervised contrastive loss, where we first compute the optimal locations of class centers uniformly distributed on the embedding space (i.e., off-line), and then perform online contrastive matching training by encouraging different class features to adaptively match these distinct and uniformly distributed cla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#27979;&#37327;&#29109;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ID-Entropy&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#22810;&#36718;&#25968;&#25454;&#21464;&#25442;&#21644;&#25197;&#26354;&#65292;&#21516;&#26102;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.02223</link><description>&lt;p&gt;
&#26412;&#22320;&#22266;&#26377;&#32500;&#24230;&#29109;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local Intrinsic Dimensional Entropy. (arXiv:2304.02223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#27979;&#37327;&#29109;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ID-Entropy&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#22810;&#36718;&#25968;&#25454;&#21464;&#25442;&#21644;&#25197;&#26354;&#65292;&#21516;&#26102;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29109;&#27979;&#37327;&#20381;&#36182;&#20110;&#27010;&#29575;&#20998;&#24067;&#22312;&#26679;&#26412;&#31354;&#38388;X&#19978;&#30340;&#23637;&#24067;&#24773;&#20917;&#65292;&#26368;&#22823;&#21487;&#23454;&#29616;&#29109;&#19982;&#26679;&#26412;&#31354;&#38388;&#22522;&#25968;|X|&#25104;&#27604;&#20363;&#12290;&#23545;&#20110;&#26377;&#38480;|X|&#65292;&#36825;&#20135;&#29983;&#20102;&#28385;&#36275;&#35768;&#22810;&#37325;&#35201;&#23646;&#24615;&#65288;&#22914;&#23545;&#21452;&#23556;&#30340;&#19981;&#21464;&#24615;&#65289;&#30340;&#24378;&#22823;&#29109;&#27979;&#37327;&#65292;&#32780;&#21516;&#26679;&#19981;&#33021;&#28385;&#36275;&#36830;&#32493;&#31354;&#38388;&#30340;&#35201;&#27714;&#65288;&#20854;&#20013;|X|=&#26080;&#31351;&#22823;&#65289;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;R&#21644;R^d&#65288;d&#22312;Z+&#20013;&#65289;&#20855;&#26377;&#30456;&#21516;&#30340;&#22522;&#25968;&#65288;&#26469;&#33258;Cantor&#30340;&#23545;&#24212;&#35770;&#35777;&#65289;&#65292;&#22522;&#25968;&#20381;&#36182;&#24615;&#29109;&#27979;&#37327;&#26080;&#27861;&#32534;&#30721;&#25968;&#25454;&#32500;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#20102;&#23545;&#36830;&#32493;&#31354;&#38388;&#23450;&#20041;&#29109;&#27979;&#37327;&#20013;&#22522;&#25968;&#21644;&#20998;&#24067;&#23637;&#24067;&#30340;&#20316;&#29992;&#65292;&#36825;&#20123;&#36830;&#32493;&#31354;&#38388;&#21487;&#20197;&#36827;&#34892;&#22810;&#36718;&#21464;&#25442;&#21644;&#25197;&#26354;&#65292;&#20363;&#22914;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#22914;&#26524;&#29992;&#20998;&#24067;&#30340;&#23616;&#37096;&#22266;&#26377;&#32500;&#24230;&#30340;&#24179;&#22343;&#20540;&#26469;&#34920;&#31034;&#27979;&#37327;&#29109;&#65292;&#34987;&#31216;&#20026;ID-Entropy&#65292;&#37027;&#20040;&#21487;&#20197;&#20316;&#20026;&#36830;&#32493;&#31354;&#38388;&#30340;&#24378;&#22823;&#29109;&#27979;&#37327;&#65292;&#21516;&#26102;&#25429;&#25417;&#25968;&#25454;&#30340;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most entropy measures depend on the spread of the probability distribution over the sample space X, and the maximum entropy achievable scales proportionately with the sample space cardinality |X|. For a finite |X|, this yields robust entropy measures which satisfy many important properties, such as invariance to bijections, while the same is not true for continuous spaces (where |X|=infinity). Furthermore, since R and R^d (d in Z+) have the same cardinality (from Cantor's correspondence argument), cardinality-dependent entropy measures cannot encode the data dimensionality. In this work, we question the role of cardinality and distribution spread in defining entropy measures for continuous spaces, which can undergo multiple rounds of transformations and distortions, e.g., in neural networks. We find that the average value of the local intrinsic dimension of a distribution, denoted as ID-Entropy, can serve as a robust entropy measure for continuous spaces, while capturing the data dimen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#19968;&#20010;&#20351;&#29992;&#26368;&#20339;&#32447;&#24615;&#36924;&#36817;(BLA)&#21021;&#22987;&#21270;&#23376;&#31354;&#38388;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#21021;&#22987;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#35782;&#21035;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02119</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#35782;&#21035;&#30340;&#23376;&#31354;&#38388;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Initialization Approach for Nonlinear State-Space Identification via the Subspace Encoder Approach. (arXiv:2304.02119v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#19968;&#20010;&#20351;&#29992;&#26368;&#20339;&#32447;&#24615;&#36924;&#36817;(BLA)&#21021;&#22987;&#21270;&#23376;&#31354;&#38388;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#21021;&#22987;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#35782;&#21035;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SUBNET&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#34987;&#24320;&#21457;&#29992;&#20110;&#20174;&#36755;&#20837;&#36755;&#20986;&#25968;&#25454;&#20013;&#35782;&#21035;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23427;&#23558;&#23637;&#24320;&#30340;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#26041;&#31243;&#21644;&#29366;&#24577;&#32534;&#30721;&#22120;&#20989;&#25968;&#32452;&#21512;&#36215;&#26469;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12290;&#24341;&#20837;&#32534;&#30721;&#22120;&#20989;&#25968;&#26469;&#20174;&#36807;&#21435;&#30340;&#36755;&#20837;&#36755;&#20986;&#25968;&#25454;&#20013;&#37325;&#26500;&#24403;&#21069;&#29366;&#24577;&#65292;&#20174;&#32780;&#20351;&#23637;&#24320;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24471;&#20197;&#21069;&#21521;&#27169;&#25311;&#12290;&#35813;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#25552;&#20379;&#20102;&#39640;&#31934;&#24230;&#21644;&#19968;&#33268;&#30340;&#27169;&#22411;&#20272;&#35745;&#65292;&#20294;&#26159;&#36890;&#36807;&#26377;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#21021;&#22987;&#21270;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#25910;&#25947;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20351;&#29992;&#26368;&#20339;&#32447;&#24615;&#36924;&#36817;(BLA)&#21021;&#22987;&#21270;&#23376;&#31354;&#38388;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#21021;&#22987;&#26041;&#27861;&#12290;&#20351;&#29992;BLA&#25552;&#20379;&#30340;&#29366;&#24577;&#31354;&#38388;&#30697;&#38453;&#21450;&#20854;&#30456;&#20851;&#30340;&#21487;&#37325;&#26500;&#26144;&#23556;&#26469;&#21021;&#22987;&#21270;&#32593;&#32476;&#30340;&#29366;&#24577;&#36716;&#31227;&#37096;&#20998;&#21644;&#32534;&#30721;&#22120;&#12290;&#25913;&#36827;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#30340;&#34920;&#29616;&#22312;Wiener-Hamme&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The SUBNET neural network architecture has been developed to identify nonlinear state-space models from input-output data. To achieve this, it combines the rolled-out nonlinear state-space equations and a state encoder function, both parameterised as a neural network. The encoder function is introduced to reconstruct the current state from past input-output data. Hence it enables the forward simulation of the rolled-out state-space model. While this approach has shown to provide high-accuracy and consistent model estimation, its convergence can be significantly improved by efficient initialization of the training process. This paper focuses on such an initialisation of the subspace encoder approach using the Best Linear Approximation (BLA). Using the BLA provided state-space matrices and its associated reconstructability map both the state-transition part of the network and the encoder are initialized. The performance of the improved initialisation scheme is evaluated on a Wiener-Hamme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#65307;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.01203</link><description>&lt;p&gt;
&#22522;&#20110;&#20934;&#24230;&#37327;&#23398;&#20064;&#30340;&#26368;&#20248;&#30446;&#26631;&#36798;&#25104;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning. (arXiv:2304.01203v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#65307;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30446;&#26631;&#36798;&#25104;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#20855;&#26377;&#29305;&#23450;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31216;&#20026;&#20934;&#24230;&#37327;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;QRL&#30340;&#30446;&#26631;&#26159;&#19987;&#38376;&#20026;&#20934;&#24230;&#37327;&#35774;&#35745;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#24674;&#22797;&#20445;&#35777;&#12290;&#22312;&#31163;&#25955;&#21270;&#30340;MountainCar&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;QRL&#30340;&#24615;&#36136;&#20197;&#21450;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#36824;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In goal-reaching reinforcement learning (RL), the optimal value function has a particular geometry, called quasimetric structure. This paper introduces Quasimetric Reinforcement Learning (QRL), a new RL method that utilizes quasimetric models to learn optimal value functions. Distinct from prior approaches, the QRL objective is specifically designed for quasimetrics, and provides strong theoretical recovery guarantees. Empirically, we conduct thorough analyses on a discretized MountainCar environment, identifying properties of QRL and its advantages over alternatives. On offline and online goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and performance, across both state-based and image-based observations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#21644;&#24402;&#19968;&#21270;&#27969;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21487;&#32852;&#21512;&#39044;&#27979;&#25152;&#26377;&#20301;&#32622;&#21644;&#25552;&#21069;&#26399;&#65292;&#20174;&#32780;&#25918;&#23485;&#20102;&#35768;&#22810;&#20256;&#32479;&#21518;&#22788;&#29702;&#26041;&#27861;&#30340;&#20998;&#24067;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;EUPPBench&#22522;&#20934;&#27979;&#35797;&#35777;&#26126;&#20102;&#20854;&#36229;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17610</link><description>&lt;p&gt;
&#37319;&#29992;&#28789;&#27963;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#38598;&#21512;&#22825;&#27668;&#39044;&#25253;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Ensemble weather forecast post-processing with a flexible probabilistic neural network approach. (arXiv:2303.17610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#21644;&#24402;&#19968;&#21270;&#27969;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21487;&#32852;&#21512;&#39044;&#27979;&#25152;&#26377;&#20301;&#32622;&#21644;&#25552;&#21069;&#26399;&#65292;&#20174;&#32780;&#25918;&#23485;&#20102;&#35768;&#22810;&#20256;&#32479;&#21518;&#22788;&#29702;&#26041;&#27861;&#30340;&#20998;&#24067;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;EUPPBench&#22522;&#20934;&#27979;&#35797;&#35777;&#26126;&#20102;&#20854;&#36229;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#21512;&#39044;&#25253;&#21518;&#22788;&#29702;&#26159;&#29983;&#25104;&#20934;&#30830;&#27010;&#29575;&#39044;&#25253;&#30340;&#24517;&#35201;&#27493;&#39588;&#12290;&#20256;&#32479;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#26159;&#26681;&#25454;&#27599;&#20010;&#20301;&#32622;&#25110;&#27599;&#20010;&#25552;&#21069;&#26399;&#20272;&#35745;&#21442;&#25968;&#32479;&#35745;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#32852;&#21512;&#39044;&#27979;&#25152;&#26377;&#20301;&#32622;&#21644;&#25552;&#21069;&#26399;&#12290;&#20026;&#20102;&#25918;&#23485;&#35768;&#22810;&#21518;&#22788;&#29702;&#26041;&#27861;&#30340;&#20998;&#24067;&#20551;&#35774;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#24402;&#19968;&#21270;&#27969;&#20316;&#20026;&#28789;&#27963;&#30340;&#21442;&#25968;&#20998;&#24067;&#20272;&#35745;&#22120;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#25968;&#23398;&#30830;&#20999;&#30340;&#26041;&#24335;&#27169;&#25311;&#19981;&#21516;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;EUPPBench&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#35813;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#23545;&#35199;&#27431;&#22320;&#21306;&#23376;&#21306;&#22495;&#30340;&#31449;&#28857;&#36827;&#34892;&#20102;&#28201;&#24230;&#39044;&#25253;&#21518;&#22788;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#25105;&#20204;&#20043;&#21069;&#30340;&#34920;&#29616;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#20379;&#35814;&#32454;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble forecast post-processing is a necessary step in producing accurate probabilistic forecasts. Conventional post-processing methods operate by estimating the parameters of a parametric distribution, frequently on a per-location or per-lead-time basis. We propose a novel, neural network-based method, which produces forecasts for all locations and lead times, jointly. To relax the distributional assumption of many post-processing methods, our approach incorporates normalizing flows as flexible parametric distribution estimators. This enables us to model varying forecast distributions in a mathematically exact way. We demonstrate the effectiveness of our method in the context of the EUPPBench benchmark, where we conduct temperature forecast post-processing for stations in a sub-region of western Europe. We show that our novel method exhibits state-of-the-art performance on the benchmark, outclassing our previous, well-performing entry. Additionally, by providing a detailed compariso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#30340;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;EPSL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#12290;EPSL&#24182;&#34892;&#21270;&#23458;&#25143;&#31471;&#27169;&#22411;&#35757;&#32451;&#65292;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#38477;&#20302;&#20102;&#21453;&#21521;&#20256;&#25773;&#30340;&#23616;&#37096;&#26799;&#24230;&#32500;&#24230;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#26381;&#21153;&#22120;&#31471;&#30340;&#35757;&#32451;&#21644;&#36890;&#20449;&#24310;&#36831;&#12290;&#21516;&#26102;&#65292;EPSL&#36824;&#35774;&#35745;&#20102;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2303.15991</link><description>&lt;p&gt;
&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#30340;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Parallel Split Learning over Resource-constrained Wireless Edge Networks. (arXiv:2303.15991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#30340;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;EPSL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#12290;EPSL&#24182;&#34892;&#21270;&#23458;&#25143;&#31471;&#27169;&#22411;&#35757;&#32451;&#65292;&#36890;&#36807;&#32858;&#21512;&#26799;&#24230;&#38477;&#20302;&#20102;&#21453;&#21521;&#20256;&#25773;&#30340;&#23616;&#37096;&#26799;&#24230;&#32500;&#24230;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#26381;&#21153;&#22120;&#31471;&#30340;&#35757;&#32451;&#21644;&#36890;&#20449;&#24310;&#36831;&#12290;&#21516;&#26102;&#65292;EPSL&#36824;&#35774;&#35745;&#20102;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#28145;&#65292;&#36825;&#38459;&#30861;&#20102;&#32852;&#21512;&#23398;&#20064;&#31561;&#38544;&#31169;&#22686;&#24378;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#24335;&#65288;&#22914;&#32852;&#37030;&#23398;&#20064;&#65289;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#30340;&#27665;&#20027;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#20513;&#23548;&#23558;&#36793;&#32536;&#35745;&#31639;&#33539;&#24335;&#21644;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;PSL&#65289;&#30456;&#32467;&#21512;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#36890;&#36807;&#36880;&#23618;&#27169;&#22411;&#20998;&#35010;&#23558;&#22823;&#37327;&#30340;&#35757;&#32451;&#24037;&#20316;&#36127;&#36733;&#21368;&#36733;&#21040;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;PSL&#26041;&#26696;&#20250;&#20135;&#29983;&#36807;&#22810;&#30340;&#35757;&#32451;&#24310;&#36831;&#21644;&#22823;&#37327;&#30340;&#25968;&#25454;&#20256;&#36755;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;PSL&#26694;&#26550;&#8212;&#8212;&#39640;&#25928;&#24182;&#34892;&#20998;&#35010;&#23398;&#20064;&#65288;EPSL&#65289;&#65292;&#20197;&#21152;&#36895;&#27169;&#22411;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EPSL&#23558;&#23458;&#25143;&#31471;&#27169;&#22411;&#35757;&#32451;&#24182;&#34892;&#21270;&#65292;&#24182;&#36890;&#36807;&#26368;&#21518;&#19968;&#23618;&#26799;&#24230;&#32858;&#21512;&#38477;&#20302;&#20102;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#30340;&#23616;&#37096;&#26799;&#24230;&#32500;&#24230;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#26381;&#21153;&#22120;&#31471;&#30340;&#35757;&#32451;&#21644;&#36890;&#20449;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32771;&#34385;&#36793;&#32536;&#35774;&#22791;&#30340;&#24322;&#26500;&#36890;&#36947;&#26465;&#20214;&#21644;&#35745;&#31639;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36164;&#28304;&#20998;&#37197;&#31639;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EPSL&#36890;&#36807;&#23558;&#36890;&#20449;&#25104;&#26412;&#21644;&#35757;&#32451;&#26102;&#38388;&#20998;&#21035;&#38477;&#20302;76&#65285;&#21644;63&#65285;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;PSL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly deeper neural networks hinder the democratization of privacy-enhancing distributed learning, such as federated learning (FL), to resource-constrained devices. To overcome this challenge, in this paper, we advocate the integration of edge computing paradigm and parallel split learning (PSL), allowing multiple client devices to offload substantial training workloads to an edge server via layer-wise model split. By observing that existing PSL schemes incur excessive training latency and large volume of data transmissions, we propose an innovative PSL framework, namely, efficient parallel split learning (EPSL), to accelerate model training. To be specific, EPSL parallelizes client-side model training and reduces the dimension of local gradients for back propagation (BP) via last-layer gradient aggregation, leading to a significant reduction in server-side training and communication latency. Moreover, by considering the heterogeneous channel conditions and computing capabil
&lt;/p&gt;</description></item><item><title>GAS&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;PINNs&#30340;&#25910;&#25947;&#36807;&#31243;&#24182;&#25552;&#39640;&#31934;&#24230;&#65292;&#24050;&#22312;2D&#21040;10D&#38382;&#39064;&#30340;&#25968;&#20540;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#39046;&#20808;&#20110;&#28145;&#23618;&#27714;&#35299;&#22120;&#12289;&#19982;&#20256;&#32479;&#25968;&#20540;&#27714;&#35299;&#22120;&#30456;&#24403;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15849</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#29992;&#20110;PINNs
&lt;/p&gt;
&lt;p&gt;
GAS: A Gaussian Mixture Distribution-Based Adaptive Sampling Method for PINNs. (arXiv:2303.15849v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15849
&lt;/p&gt;
&lt;p&gt;
GAS&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;PINNs&#30340;&#25910;&#25947;&#36807;&#31243;&#24182;&#25552;&#39640;&#31934;&#24230;&#65292;&#24050;&#22312;2D&#21040;10D&#38382;&#39064;&#30340;&#25968;&#20540;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#39046;&#20808;&#20110;&#28145;&#23618;&#27714;&#35299;&#22120;&#12289;&#19982;&#20256;&#32479;&#25968;&#20540;&#27714;&#35299;&#22120;&#30456;&#24403;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#31185;&#23398;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#65292;PINNs&#26041;&#27861;&#22240;&#20854;&#39640;&#32500;&#38382;&#39064;&#22788;&#29702;&#30340;&#39640;&#25928;&#24615;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#20294;&#20854;&#20934;&#30830;&#24615;&#30456;&#23545;&#36739;&#20302;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#39640;&#24230;&#19981;&#35268;&#21017;&#30340;&#38382;&#39064;&#12290;&#21463;&#33258;&#36866;&#24212;&#26377;&#38480;&#20803;&#26041;&#27861;&#21644;&#22686;&#37327;&#23398;&#20064;&#24605;&#24819;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GAS&#65292;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;PINNs&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;GAS&#21033;&#29992;&#24403;&#21069;&#30340;&#27531;&#24046;&#20449;&#24687;&#29983;&#25104;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#20197;&#37319;&#26679;&#20854;&#20182;&#28857;&#65292;&#36825;&#20123;&#25968;&#25454;&#23558;&#19982;&#21382;&#21490;&#25968;&#25454;&#19968;&#36215;&#35757;&#32451;&#65292;&#21152;&#24555;&#25439;&#22833;&#20989;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;2D&#21040;10D&#38382;&#39064;&#30340;&#25968;&#20540;&#27169;&#25311;&#20013;&#65292;GAS&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#28145;&#23618;&#27714;&#35299;&#22120;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#19982;&#20256;&#32479;&#25968;&#20540;&#27714;&#35299;&#22120;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent study of the deep learning in scientific computation, the PINNs method has drawn widespread attention for solving PDEs. Compared with traditional methods, PINNs can efficiently handle high-dimensional problems, while the accuracy is relatively low, especially for highly irregular problems. Inspired by the idea of adaptive finite element methods and incremental learning, we propose GAS, a Gaussian mixture distribution-based adaptive sampling method for PINNs. During the training procedure, GAS uses the current residual information to generate a Gaussian mixture distribution for the sampling of additional points, which are then trained together with history data to speed up the convergence of loss and achieve a higher accuracy. Several numerical simulations on 2d to 10d problems show that GAS is a promising method which achieves the state-of-the-art accuracy among deep solvers, while being comparable with traditional numerical solvers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#20960;&#20309;&#23398;&#20064;&#35270;&#28857;&#31561;&#21464;&#24615;&#20197;&#25552;&#39640;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#23450;&#20301;&#31934;&#24230;&#30340;&#26694;&#26550;VEDet&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;transformer&#26550;&#26500;&#21644;&#35270;&#35282;&#26465;&#20214;&#30340;&#26597;&#35810;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.14548</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#35266;&#28857;&#31561;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Viewpoint Equivariance for Multi-View 3D Object Detection. (arXiv:2303.14548v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#20960;&#20309;&#23398;&#20064;&#35270;&#28857;&#31561;&#21464;&#24615;&#20197;&#25552;&#39640;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#23450;&#20301;&#31934;&#24230;&#30340;&#26694;&#26550;VEDet&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;transformer&#26550;&#26500;&#21644;&#35270;&#35282;&#26465;&#20214;&#30340;&#26597;&#35810;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#26159;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#29616;&#20195;&#26041;&#27861;&#20391;&#37325;&#20110;&#20174;&#22810;&#35270;&#35282;&#30456;&#26426;&#36755;&#20837;&#25512;&#29702;&#21644;&#35299;&#30721;&#29289;&#20307;&#36793;&#30028;&#26694;&#12290;&#26412;&#25991;&#21033;&#29992;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#22312;&#19977;&#32500;&#22330;&#26223;&#29702;&#35299;&#21644;&#20960;&#20309;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#20171;&#32461;&#20102;VEDet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#28857;&#24863;&#30693;&#21644;&#31561;&#21464;&#24615;&#21033;&#29992;&#19977;&#32500;&#22810;&#35270;&#35282;&#20960;&#20309;&#26469;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#12290;VEDet&#21033;&#29992;&#22522;&#20110;&#26597;&#35810;&#30340;transformer&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#23558;&#22270;&#20687;&#29305;&#24449;&#21644;&#26469;&#33258;&#23427;&#20204;&#30340;&#19977;&#32500;&#36879;&#35270;&#20960;&#20309;&#30340;&#20301;&#32622;&#32534;&#30721;&#30456;&#32467;&#21512;&#26469;&#32534;&#30721;&#19977;&#32500;&#22330;&#26223;&#12290;&#25105;&#20204;&#22312;&#36755;&#20986;&#23618;&#35774;&#35745;&#20102;&#35270;&#35282;&#26465;&#20214;&#30340;&#26597;&#35810;&#65292;&#36825;&#20351;&#24471;&#22312;&#35757;&#32451;&#26399;&#38388;&#29983;&#25104;&#22810;&#20010;&#34394;&#25311;&#24103;&#65292;&#36890;&#36807;&#24378;&#21046;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#26469;&#23398;&#20064;&#35270;&#28857;&#31561;&#21464;&#24615;&#12290;&#22312;&#36755;&#20837;&#23618;&#27880;&#20837;&#30340;&#22810;&#35270;&#35282;&#20960;&#20309;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#24182;&#22312;&#25439;&#22833;&#23618;&#20013;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#22320;&#29702;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D object detection from visual sensors is a cornerstone capability of robotic systems. State-of-the-art methods focus on reasoning and decoding object bounding boxes from multi-view camera input. In this work we gain intuition from the integral role of multi-view consistency in 3D scene understanding and geometric learning. To this end, we introduce VEDet, a novel 3D object detection framework that exploits 3D multi-view geometry to improve localization through viewpoint awareness and equivariance. VEDet leverages a query-based transformer architecture and encodes the 3D scene by augmenting image features with positional encodings from their 3D perspective geometry. We design view-conditioned queries at the output level, which enables the generation of multiple virtual frames during training to learn viewpoint equivariance by enforcing multi-view consistency. The multi-view geometry injected at the input level as positional encodings and regularized at the loss level provides rich geo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23398;&#29983;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#26102;&#65292;&#23436;&#32654;&#27867;&#21270;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#36755;&#20837;&#32500;&#24230;&#27604;&#65292;&#23398;&#29983;&#37117;&#26080;&#27861;&#23436;&#32654;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.14083</link><description>&lt;p&gt;
&#23398;&#29983;-&#25945;&#24072;&#26694;&#26550;&#19979;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning for the Random Feature Model in the Student-Teacher Framework. (arXiv:2303.14083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23398;&#29983;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#26102;&#65292;&#23436;&#32654;&#27867;&#21270;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#36755;&#20837;&#32500;&#24230;&#27604;&#65292;&#23398;&#29983;&#37117;&#26080;&#27861;&#23436;&#32654;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#38543;&#30528;&#26435;&#37325;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#20854;&#24615;&#33021;&#36890;&#24120;&#20250;&#25552;&#39640;&#65292;&#23548;&#33268;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#31532;&#19968;&#23618;&#26159;&#20923;&#32467;&#30340;&#65292;&#32780;&#26368;&#21518;&#19968;&#23618;&#26159;&#21487;&#35757;&#32451;&#30340;&#65292;&#31216;&#20026;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#23398;&#29983;-&#25945;&#24072;&#26694;&#26550;&#19979;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#36890;&#36807;&#23548;&#20986;&#19968;&#32452;&#23398;&#20064;&#21160;&#24577;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#36755;&#20837;&#32500;&#24230;&#27604;&#65292;&#23398;&#29983;&#37117;&#26080;&#27861;&#23436;&#32654;&#27867;&#21270;&#65292;&#24182;&#35745;&#31639;&#38750;&#38646;&#28176;&#36817;&#27867;&#21270;&#35823;&#24046;&#12290;&#21482;&#26377;&#24403;&#23398;&#29983;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#26102;&#65292;&#25165;&#26377;&#21487;&#33021;&#23454;&#29616;&#23436;&#32654;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are widely used prediction algorithms whose performance often improves as the number of weights increases, leading to over-parametrization. We consider a two-layered neural network whose first layer is frozen while the last layer is trainable, known as the random feature model. We study over-parametrization in the context of a student-teacher framework by deriving a set of differential equations for the learning dynamics. For any finite ratio of hidden layer size and input dimension, the student cannot generalize perfectly, and we compute the non-zero asymptotic generalization error. Only when the student's hidden layer size is exponentially larger than the input dimension, an approach to perfect generalization is possible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#22312;&#38750;&#20984;&#26465;&#20214;&#19979;&#20351;&#29992;&#28145;&#23618;&#27169;&#22411;&#21644;&#26356;&#22797;&#26434;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12922</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Fragility of Influence Functions. (arXiv:2303.12922v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#22312;&#38750;&#20984;&#26465;&#20214;&#19979;&#20351;&#29992;&#28145;&#23618;&#27169;&#22411;&#21644;&#26356;&#22797;&#26434;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26377;&#24456;&#22810;&#35770;&#25991;&#33268;&#21147;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#39564;&#35777;&#36825;&#20123;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#25110;&#21487;&#20449;&#24230;&#12290;&#26368;&#36817;&#65292;&#24433;&#21709;&#20989;&#25968;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21333;&#20010;&#26679;&#26412;&#19978;&#30340;&#28789;&#25935;&#24230;&#30340;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#24433;&#21709;&#20989;&#25968;&#26131;&#21463;&#22122;&#22768;&#21644;&#25968;&#25454;&#20998;&#24067;&#19981;&#23545;&#31216;&#24615;&#24433;&#21709;&#65292;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25506;&#31350;&#24433;&#21709;&#20989;&#25968;&#32972;&#21518;&#30340;&#26426;&#29702;&#65292;&#20174;&#32780;&#20026;&#22686;&#24378;&#24433;&#21709;&#20989;&#25968;&#30340;&#40065;&#26834;&#24615;&#25552;&#20379;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last few years, many works have tried to explain the predictions of deep learning models. Few methods, however, have been proposed to verify the accuracy or faithfulness of these explanations. Recently, influence functions, which is a method that approximates the effect that leave-one-out training has on the loss function, has been shown to be fragile. The proposed reason for their fragility remains unclear. Although previous work suggests the use of regularization to increase robustness, this does not hold in all cases. In this work, we seek to investigate the experiments performed in the prior work in an effort to understand the underlying mechanisms of influence function fragility. First, we verify influence functions using procedures from the literature under conditions where the convexity assumptions of influence functions are met. Then, we relax these assumptions and study the effects of non-convexity by using deeper models and more complex datasets. Here, we analyze the k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.05737</link><description>&lt;p&gt;
&#20020;&#24202;BERTScore&#65306;&#20020;&#24202;&#29615;&#22659;&#19979;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#25913;&#36827;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a Clinical BERTScore (CBERTScore) metric for ASR in medical contexts, which penalizes clinically-relevant mistakes more than other metrics and aligns more closely with clinician preferences. The authors also collect a benchmark of clinician preferences on medical sentences and release it for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26377;&#28508;&#21147;&#33410;&#30465;&#26102;&#38388;&#65292;&#38477;&#20302;&#25104;&#26412;&#65292;&#25552;&#39640;&#25253;&#21578;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#21307;&#29983;&#30340;&#30130;&#21171;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36991;&#20813;&#21307;&#23398;&#30456;&#20851;&#30340;&#36716;&#24405;&#38169;&#35823;&#30340;&#37325;&#35201;&#24615;&#65292;&#21307;&#30103;&#34892;&#19994;&#37319;&#29992;&#36825;&#31181;&#25216;&#26415;&#30340;&#36895;&#24230;&#36739;&#24930;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;ASR&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#65288;WER&#12289;BLUE&#12289;METEOR&#31561;&#65289;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#24230;&#37327;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#65292;&#26377;&#26102;&#24046;&#36317;&#24456;&#22823;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We demonstrate that this metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins. We collect a benchmark of 13 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP), demonstrate that CBERTScore more closely matches what clinicians prefer, and release the benchmark for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.02468</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;11&#30340;Lon-ea&#65306;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#28608;&#27963;&#20989;&#25968;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction. (arXiv:2303.02468v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#23398;&#20064;&#19981;&#21516;&#24847;&#20219;&#21153;&#30340;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#23618;&#20013;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;&#24433;&#21709;&#12290;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#39044;&#27979;&#36719;&#26631;&#31614;&#26469;&#37327;&#21270;&#19981;&#21516;&#24847;&#37327;&#12290;&#20026;&#20102;&#39044;&#27979;&#36719;&#26631;&#31614;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#39044;&#22788;&#29702;&#22120;&#21644;&#32534;&#30721;&#22120;&#65292;&#24182;&#25913;&#21464;&#36755;&#20986;&#23618;&#20013;&#20351;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#21442;&#25968;&#19981;&#21464;&#12290;&#28982;&#21518;&#23558;&#36719;&#26631;&#31614;&#29992;&#20110;&#30828;&#26631;&#31614;&#39044;&#27979;&#12290;&#32771;&#34385;&#30340;&#28608;&#27963;&#20989;&#25968;&#21253;&#25324;sigmoid&#20989;&#25968;&#20197;&#21450;&#28155;&#21152;&#21040;&#27169;&#22411;&#20013;&#30340;&#38454;&#36291;&#20989;&#25968;&#21644;&#26412;&#25991;&#20013;&#39318;&#27425;&#20171;&#32461;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the influence of different activation functions in the output layer of deep neural network models for soft and hard label prediction in the learning with disagreement task. In this task, the goal is to quantify the amount of disagreement via predicting soft labels. To predict the soft labels, we use BERT-based preprocessors and encoders and vary the activation function used in the output layer, while keeping other parameters constant. The soft labels are then used for the hard label prediction. The activation functions considered are sigmoid as well as a step-function that is added to the model post-training and a sinusoidal activation function, which is introduced for the first time in this paper.
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Safe-DS&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#20197;&#38745;&#24577;&#23433;&#20840;&#30340;&#26041;&#24335;&#36816;&#34892;Python DS&#24211;&#65292;&#24182;&#21487;&#25429;&#33719;&#20986;&#38169;&#30340;&#31867;&#22411;&#65292;&#24182;&#19988;&#25317;&#26377;&#27604;Python Linters&#26356;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.14548</link><description>&lt;p&gt;
Safe-DS: &#19968;&#31181;&#20351;&#25968;&#25454;&#31185;&#23398;&#26356;&#21152;&#23433;&#20840;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Safe-DS: A Domain Specific Language to Make Data Science Safe. (arXiv:2302.14548v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14548
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Safe-DS&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#20197;&#38745;&#24577;&#23433;&#20840;&#30340;&#26041;&#24335;&#36816;&#34892;Python DS&#24211;&#65292;&#24182;&#21487;&#25429;&#33719;&#20986;&#38169;&#30340;&#31867;&#22411;&#65292;&#24182;&#19988;&#25317;&#26377;&#27604;Python Linters&#26356;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#65288;DS&#65289;&#27969;&#27700;&#32447;&#36816;&#34892;&#26102;&#38388;&#36739;&#38271;&#65292;&#21363;&#20351;&#26159;&#23567;&#30340;&#32534;&#31243;&#38169;&#35823;&#65292;&#22914;&#26524;&#19981;&#33021;&#38745;&#24577;&#26816;&#27979;&#21040;&#65292;&#20063;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;DS&#27969;&#27700;&#32447;&#37117;&#26159;&#29992;Python&#32534;&#20889;&#30340;&#65292;&#22240;&#27492;&#21363;&#20351;&#26159;&#22522;&#26412;&#30340;&#38745;&#24577;&#31867;&#22411;&#26816;&#26597;&#20063;&#24456;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;Safe-DS&#65292;&#19968;&#31181;&#38754;&#21521;DS&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;DSL&#65289;&#65292;&#20197;&#38745;&#24577;&#23433;&#20840;&#30340;&#26041;&#24335;&#20351;&#29992;&#20016;&#23500;&#30340;Python DS&#24211;&#12290;Safe-DS&#21487;&#20197;&#25429;&#33719;&#24120;&#35268;&#31867;&#22411;&#38169;&#35823;&#20197;&#21450;&#19982;&#33539;&#22260;&#38480;&#21046;&#12289;&#25968;&#25454;&#22788;&#29702;&#21644;&#20989;&#25968;&#35843;&#29992;&#39034;&#24207;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#36828;&#36828;&#36229;&#20986;&#20102;&#24403;&#21069;Python Linters&#30340;&#33021;&#21147;&#12290;Python&#24211;&#36890;&#36807;&#19968;&#20010;&#23384;&#26681;&#35821;&#35328;&#26469;&#38598;&#25104;&#21040;Safe-DS&#20013;&#65292;&#20197;&#25351;&#23450;&#20854;&#22768;&#26126;&#30340;&#25509;&#21475;&#65292;&#20197;&#21450;&#19968;&#20010;&#33021;&#22815;&#20174;Python&#24211;&#30340;&#20195;&#30721;&#21644;&#25991;&#26723;&#20013;&#25552;&#21462;&#31867;&#22411;&#20449;&#24687;&#30340;API&#32534;&#36753;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the long runtime of Data Science (DS) pipelines, even small programming mistakes can be very costly, if they are not detected statically. However, even basic static type checking of DS pipelines is difficult because most are written in Python. Static typing is available in Python only via external linters. These require static type annotations for parameters or results of functions, which many DS libraries do not provide. In this paper, we show how the wealth of Python DS libraries can be used in a statically safe way via Safe-DS, a domain specific language (DSL) for DS. Safe-DS catches conventional type errors plus errors related to range restrictions, data manipulation, and call order of functions, going well beyond the abilities of current Python linters. Python libraries are integrated into Safe-DS via a stub language for specifying the interface of its declarations, and an API-Editor that is able to extract type information from the code and documentation of Python librarie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;LQR&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#36882;&#25512;-&#35270;&#35282;&#31574;&#30053;&#26799;&#24230;&#65288;RHPG&#65289;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#37319;&#26679;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#36890;&#36807;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;RHPG&#22312;&#32447;&#24615;&#25511;&#21046;&#21644;&#20272;&#35745;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.13144</link><description>&lt;p&gt;
&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;LQR&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Revisiting LQR Control from the Perspective of Receding-Horizon Policy Gradient. (arXiv:2302.13144v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;LQR&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#36882;&#25512;-&#35270;&#35282;&#31574;&#30053;&#26799;&#24230;&#65288;RHPG&#65289;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#37319;&#26679;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#36890;&#36807;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;RHPG&#22312;&#32447;&#24615;&#25511;&#21046;&#21644;&#20272;&#35745;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36882;&#25512;&#35270;&#35282;&#37325;&#26032;&#23457;&#35270;&#20102;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#12290;&#32467;&#21512;&#36882;&#25512;-&#35270;&#35282;&#31574;&#30053;&#26799;&#24230;&#65288;RHPG&#65289;&#27169;&#22411;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#27714;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#37319;&#26679;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#33021;&#22815;&#23398;&#20064;&#21040;&#22312;&#949;-&#33539;&#25968;&#24847;&#20041;&#19979;&#25509;&#36817;LQR&#26368;&#20248;&#35299;&#30340;&#20248;&#21270;&#25511;&#21046;&#31574;&#30053;&#12290;&#22312;&#26368;&#36817;&#23558;RHPG&#24212;&#29992;&#20110;&#23398;&#20064;&#21345;&#23572;&#26364;&#28388;&#27874;&#20013;&#36827;&#34892;&#25299;&#23637;&#20998;&#26512;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RHPG&#22312;&#32447;&#24615;&#25511;&#21046;&#21644;&#20272;&#35745;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit in this paper the discrete-time linear quadratic regulator (LQR) problem from the perspective of receding-horizon policy gradient (RHPG), a newly developed model-free learning framework for control applications. We provide a fine-grained sample complexity analysis for RHPG to learn a control policy that is both stabilizing and $\epsilon$-close to the optimal LQR solution, and our algorithm does not require knowing a stabilizing control policy for initialization. Combined with the recent application of RHPG in learning the Kalman filter, we demonstrate the general applicability of RHPG in linear control and estimation with streamlined analyses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;TROPOMI&#21355;&#26143;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#36873;&#25321;&#19981;&#31526;&#21512;&#33337;&#33334;&#25490;&#25918;&#26631;&#20934;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.12744</link><description>&lt;p&gt;
&#21033;&#29992;TROPOMI&#21355;&#26143;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;NO2&#25490;&#25918;&#33337;&#33334;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anomalous NO2 emitting ship detection with TROPOMI satellite data and machine learning. (arXiv:2302.12744v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;TROPOMI&#21355;&#26143;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#36873;&#25321;&#19981;&#31526;&#21512;&#33337;&#33334;&#25490;&#25918;&#26631;&#20934;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;2021&#24180;&#24320;&#22987;&#65292;&#21271;&#28023;&#21644;&#27874;&#32599;&#30340;&#28023;&#27700;&#22495;&#20013;&#30340;&#33337;&#33334;&#25490;&#25918;$\text{NO}_\text{x}$&#30340;&#38480;&#21046;&#26356;&#21152;&#20005;&#26684;&#12290;&#30001;&#20110;&#30446;&#21069;&#29992;&#20110;&#33337;&#33334;&#36981;&#23432;&#30417;&#30563;&#30340;&#25152;&#26377;&#26041;&#27861;&#37117;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#36164;&#37329;&#65292;&#22240;&#27492;&#24517;&#39035;&#20248;&#20808;&#32771;&#34385;&#26816;&#26597;&#26377;&#21487;&#33021;&#19981;&#31526;&#21512;&#26631;&#20934;&#30340;&#33337;&#33334;&#12290;&#30446;&#21069;&#30340;&#20808;&#36827;&#26041;&#27861;&#26159;&#20351;&#29992;TROPOMI/S5P&#22270;&#20687;&#19978;&#30340;&#22522;&#20110;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#33337;&#25490;&#28895;&#26816;&#27979;&#65292;&#28982;&#32780;&#65292;&#23545;&#20110;&#25968;&#25454;&#26631;&#27880;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#29992;&#20110;&#39564;&#35777;&#30340;&#33337;&#33334;&#25490;&#25918;&#20195;&#29702;&#19981;&#22815;&#22797;&#26434;&#65292;&#38480;&#21046;&#20102;&#35813;&#27169;&#22411;&#29992;&#20110;&#33337;&#33334;&#36981;&#23432;&#30417;&#30563;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;TROPOMI&#21355;&#26143;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32452;&#21512;&#33258;&#21160;&#36873;&#25321;&#21487;&#33021;&#19981;&#31526;&#21512;&#26631;&#20934;&#30340;&#33337;&#33334;&#30340;&#26041;&#27861;&#12290;&#23427;&#22522;&#20110;&#19968;&#20010;&#25552;&#20986;&#30340;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#39044;&#35745;&#20135;&#29983;&#30340;$\text{NO}_\text{2}$&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Starting from 2021, more demanding $\text{NO}_\text{x}$ emission restrictions were introduced for ships operating in the North and Baltic Sea waters. Since all methods currently used for ship compliance monitoring are financially and time demanding, it is important to prioritize the inspection of ships that have high chances of being non-compliant. The current state-of-the-art approach for a large-scale ship $\text{NO}_\text{2}$ estimation is a supervised machine learning-based segmentation of ship plumes on TROPOMI/S5P images. However, challenging data annotation and insufficiently complex ship emission proxy used for the validation limit the applicability of the model for ship compliance monitoring. In this study, we present a method for the automated selection of potentially non-compliant ships using a combination of machine learning models on TROPOMI satellite data. It is based on a proposed regression model predicting the amount of $\text{NO}_\text{2}$ that is expected to be produ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#21644;&#31574;&#30053;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#28151;&#21512;&#25216;&#26415;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.09051</link><description>&lt;p&gt;
&#22797;&#26434;&#38382;&#31572;&#21644;&#35821;&#35328;&#27169;&#22411;&#28151;&#21512;&#26550;&#26500;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Complex QA and language models hybrid architectures, Survey. (arXiv:2302.09051v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#21644;&#31574;&#30053;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#28151;&#21512;&#25216;&#26415;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#21644;&#31574;&#30053;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#28151;&#21512;&#25216;&#26415;&#22312;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#26631;&#20934;&#38382;&#39064;&#19978;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#65292;&#20294;&#22312;&#35299;&#20915;&#26356;&#20855;&#20307;&#30340;&#22797;&#26434;&#38382;&#39064;&#26102;&#65288;&#22914;&#22312;&#19981;&#21516;&#25991;&#21270;&#20013;&#20010;&#20154;&#33258;&#30001;&#27010;&#24565;&#30340;&#21464;&#21270;&#22914;&#20309;&#65311;&#20160;&#20040;&#26159;&#20026;&#20943;&#23569;&#27668;&#20505;&#21464;&#21270;&#32780;&#23454;&#29616;&#30340;&#26368;&#20339;&#21457;&#30005;&#26041;&#27861;&#32452;&#21512;&#65311;&#65289;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#26550;&#26500;&#12289;&#30693;&#35782;&#12289;&#25216;&#33021;&#12289;&#26041;&#27861;&#12289;&#25935;&#24863;&#25968;&#25454;&#20445;&#25252;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#31867;&#23457;&#25209;&#21644;&#22810;&#21151;&#33021;&#21453;&#39304;&#12290;&#26368;&#36817;&#30340;&#39033;&#30446;&#22914;ChatGPT&#21644;GALACTICA&#20801;&#35768;&#38750;&#19987;&#19994;&#20154;&#21592;&#20102;&#35299;LLM&#22312;&#22797;&#26434;QA&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#20197;&#21450;&#21516;&#31561;&#24378;&#22823;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23457;&#26597;&#25152;&#38656;&#30340;&#25216;&#33021;&#21644;&#35780;&#20272;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#28151;&#21512;&#26550;&#26500;&#65292;&#23558;LLM&#19982;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;&#20854;&#20182;AI/ML&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#36825;&#20123;CQA&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#33021;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reviews the state-of-the-art of language models architectures and strategies for "complex" question-answering (QA, CQA, CPS) with a focus on hybridization. Large Language Models (LLM) are good at leveraging public data on standard problems but once you want to tackle more specific complex questions or problems (e.g. How does the concept of personal freedom vary between different cultures ? What is the best mix of power generation methods to reduce climate change ?) you may need specific architecture, knowledge, skills, methods, sensitive data protection, explainability, human approval and versatile feedback... Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA. In this paper, we start by reviewing required skills and evaluation techniques. We integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#24577;&#20559;&#35265;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21464;&#20998;&#21387;&#32553;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#27169;&#24577;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.09479</link><description>&lt;p&gt;
&#26080;&#27169;&#24577;&#20559;&#35265;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21464;&#20998;&#21387;&#32553;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modality-Agnostic Variational Compression of Implicit Neural Representations. (arXiv:2301.09479v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09479
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#24577;&#20559;&#35265;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21464;&#20998;&#21387;&#32553;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#27169;&#24577;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#20989;&#25968;&#35270;&#22270;&#65292;&#24182;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#21442;&#25968;&#21270;&#30340;&#26080;&#27169;&#24577;&#31070;&#32463;&#21387;&#32553;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#36719;&#38376;&#25511;&#26426;&#21046;&#23558;&#38750;&#32447;&#24615;&#26144;&#23556;&#21040;&#32039;&#20945;&#30340;&#28508;&#22312;&#34920;&#31034;&#20013;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#28508;&#22312;&#32534;&#30721;&#21644;&#31232;&#30095;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36825;&#20801;&#35768;&#27599;&#20010;&#25968;&#25454;&#39033;&#36890;&#36807;&#23376;&#32593;&#32476;&#36873;&#25321;&#26469;&#23450;&#21046;&#20849;&#20139;&#30340;INR&#32593;&#32476;&#30340;&#19987;&#19994;&#21270;&#12290;&#22312;&#33719;&#21462;&#36825;&#31181;&#28508;&#22312;&#34920;&#31034;&#30340;&#25968;&#25454;&#38598;&#21518;&#65292;&#25105;&#20204;&#22312;&#26080;&#27169;&#24577;&#31354;&#38388;&#20013;&#30452;&#25509;&#20248;&#21270;&#36895;&#29575;/&#22833;&#30495;&#30340;&#25240;&#34935;&#26041;&#26696;&#65292;&#20351;&#29992;&#31070;&#32463;&#21387;&#32553;&#12290;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#21464;&#20998;&#21387;&#32553;&#65288;VC-INR&#65289;&#22312;&#20855;&#26377;&#30456;&#21516;&#34920;&#31034;&#23481;&#37327;&#30340;&#37327;&#21270;&#20043;&#21069;&#26174;&#31034;&#20986;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20248;&#20110;&#20854;&#20182;INR&#25216;&#26415;&#25152;&#20351;&#29992;&#30340;&#20808;&#21069;&#37327;&#21270;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#31639;&#27861;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#23450;&#20110;&#27169;&#24577;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#19978;&#21462;&#24471;&#21331;&#36234;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22270;&#20687;&#12289;&#27668;&#20505;&#25968;&#25454;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#25968;&#25454;&#19978;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a modality-agnostic neural compression algorithm based on a functional view of data and parameterised as an Implicit Neural Representation (INR). Bridging the gap between latent coding and sparsity, we obtain compact latent representations non-linearly mapped to a soft gating mechanism. This allows the specialisation of a shared INR network to each data item through subnetwork selection. After obtaining a dataset of such latent representations, we directly optimise the rate/distortion trade-off in a modality-agnostic space using neural compression. Variational Compression of Implicit Neural Representations (VC-INR) shows improved performance given the same representational capacity pre quantisation while also outperforming previous quantisation schemes used for other INR techniques. Our experiments demonstrate strong results over a large set of diverse modalities using the same algorithm without any modality-specific inductive biases. We show results on images, climate dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#20132;&#27969;&#30005;&#26368;&#20248;&#28526;&#27969;&#35745;&#31639;&#30340;&#32500;&#24230;&#65292;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#30340;&#36755;&#20986;&#21487;&#20197;&#29992;&#20110;&#28909;&#21551;&#21160;&#31934;&#30830;&#30340;AC&#27714;&#35299;&#22120;&#20197;&#24674;&#22797;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.08840</link><description>&lt;p&gt;
&#21387;&#32553;&#20248;&#21270;&#23398;&#20064;&#29992;&#20110;&#20132;&#27969;&#30005;&#26368;&#20248;&#28526;&#27969;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Compact Optimization Learning for AC Optimal Power Flow. (arXiv:2301.08840v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#20132;&#27969;&#30005;&#26368;&#20248;&#28526;&#27969;&#35745;&#31639;&#30340;&#32500;&#24230;&#65292;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#30340;&#36755;&#20986;&#21487;&#20197;&#29992;&#20110;&#28909;&#21551;&#21160;&#31934;&#30830;&#30340;AC&#27714;&#35299;&#22120;&#20197;&#24674;&#22797;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#26368;&#20248;&#28526;&#27969;&#35745;&#31639;&#65288;OPF&#65289;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#23398;&#20064;&#36755;&#20837;/&#36755;&#20986;&#26144;&#23556;&#30340;&#26041;&#27861;&#30001;&#20110;&#36755;&#20986;&#31354;&#38388;&#30340;&#39640;&#32500;&#24230;&#32780;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#21033;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#26368;&#20248;&#35299;&#30340;&#31354;&#38388;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#21387;&#32553;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20027;&#35201;&#25104;&#20998;&#30340;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#28982;&#21518;&#23558;&#21521;&#37327;&#36716;&#25442;&#20026;&#21407;&#22987;&#36755;&#20986;&#31354;&#38388;&#12290;&#36825;&#31181;&#21387;&#32553;&#22823;&#22823;&#38477;&#20302;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#21387;&#32553;&#23398;&#20064;&#22312;PGLib&#30340;&#21508;&#31181;&#27979;&#35797;&#29992;&#20363;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26368;&#39640;&#21487;&#36798;30,000&#20010;&#24635;&#32447;&#12290;&#26412;&#25991;&#36824;&#34920;&#26126;&#65292;&#21387;&#32553;&#23398;&#20064;&#30340;&#36755;&#20986;&#21487;&#20197;&#29992;&#20110;&#28909;&#21551;&#21160;&#31934;&#30830;&#30340;AC&#27714;&#35299;&#22120;&#20197;&#24674;&#22797;&#21487;&#34892;&#24615;&#65292;&#24182;&#24102;&#26469;&#26174;&#30528;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reconsiders end-to-end learning approaches to the Optimal Power Flow (OPF). Existing methods, which learn the input/output mapping of the OPF, suffer from scalability issues due to the high dimensionality of the output space. This paper first shows that the space of optimal solutions can be significantly compressed using principal component analysis (PCA). It then proposes Compact Learning, a new method that learns in a subspace of the principal components before translating the vectors into the original output space. This compression reduces the number of trainable parameters substantially, improving scalability and effectiveness. Compact Learning is evaluated on a variety of test cases from the PGLib with up to 30,000 buses. The paper also shows that the output of Compact Learning can be used to warm-start an exact AC solver to restore feasibility, while bringing significant speed-ups.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22238;&#24402;&#27169;&#22411;&#19979;&#20998;&#35010;&#23398;&#20064;&#20013;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#21482;&#36866;&#29992;&#20110;&#31163;&#25955;&#26631;&#31614;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#20197;&#25512;&#26029;&#36830;&#32493;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2301.07284</link><description>&lt;p&gt;
&#22522;&#20110;&#22238;&#24402;&#35774;&#32622;&#19979;&#30340;&#20998;&#35010;&#23398;&#20064;&#20013;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Label Inference Attack against Split Learning under Regression Setting. (arXiv:2301.07284v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22238;&#24402;&#27169;&#22411;&#19979;&#20998;&#35010;&#23398;&#20064;&#20013;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#21482;&#36866;&#29992;&#20110;&#31163;&#25955;&#26631;&#31614;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#20197;&#25512;&#26029;&#36830;&#32493;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#26500;&#24314;&#27169;&#22359;&#65292;&#20998;&#35010;&#23398;&#20064;(SL)&#24050;&#32463;&#22312;&#20004;&#26041;&#27169;&#22411;&#35757;&#32451;&#21327;&#20316;&#20013;&#35777;&#26126;&#20102;&#20854;&#23454;&#36341;&#24615;&#65292;&#20854;&#20013;&#19968;&#26041;&#25317;&#26377;&#25968;&#25454;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#21478;&#19968;&#26041;&#25317;&#26377;&#30456;&#24212;&#30340;&#26631;&#31614;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#35748;&#20026;&#26159;&#31169;&#26377;&#30340;&#65292;&#22240;&#20026;&#20849;&#20139;&#20449;&#24687;&#20165;&#26159;&#23884;&#20837;&#21521;&#37327;&#21644;&#26799;&#24230;&#65292;&#32780;&#19981;&#26159;&#31169;&#26377;&#21407;&#22987;&#25968;&#25454;&#21644;&#26631;&#31614;&#12290;&#20294;&#26159;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31169;&#26377;&#26631;&#31614;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#27844;&#28431;&#12290;&#36825;&#20123;&#29616;&#26377;&#30340;&#25915;&#20987;&#20165;&#36866;&#29992;&#20110;&#20998;&#31867;&#35774;&#32622;&#65292;&#20854;&#20013;&#31169;&#26377;&#26631;&#31614;&#26159;&#31163;&#25955;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22238;&#24402;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#27844;&#28431;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#31169;&#26377;&#26631;&#31614;&#26159;&#36830;&#32493;&#30340;&#25968;&#23383;&#65288;&#32780;&#19981;&#26159;&#20998;&#31867;&#20013;&#30340;&#31163;&#25955;&#26631;&#31614;&#65289;&#12290;&#30001;&#20110;&#36755;&#20986;&#33539;&#22260;&#26080;&#38480;&#65292;&#36825;&#20351;&#24471;&#20808;&#21069;&#30340;&#25915;&#20987;&#26356;&#38590;&#25512;&#26029;&#36830;&#32493;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#23427;&#25972;&#21512;&#20102;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a crucial building block in vertical Federated Learning (vFL), Split Learning (SL) has demonstrated its practice in the two-party model training collaboration, where one party holds the features of data samples and another party holds the corresponding labels. Such method is claimed to be private considering the shared information is only the embedding vectors and gradients instead of private raw data and labels. However, some recent works have shown that the private labels could be leaked by the gradients. These existing attack only works under the classification setting where the private labels are discrete. In this work, we step further to study the leakage in the scenario of the regression model, where the private labels are continuous numbers (instead of discrete labels in classification). This makes previous attacks harder to infer the continuous labels due to the unbounded output range. To address the limitation, we propose a novel learning-based attack that integrates gradie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#34507;&#30333;&#36136;&#24037;&#31243;&#37319;&#26679;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#27169;&#25311;&#20013;&#36827;&#21270;&#34507;&#30333;&#36136;&#65292;&#36890;&#36807;&#32452;&#21512;&#26080;&#30417;&#30563;&#27169;&#22411;&#21644;&#30417;&#30563;&#27169;&#22411;&#26469;&#25552;&#39640;&#35780;&#20272;&#26410;&#35265;&#36807;&#30340;&#31361;&#21464;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20351;&#29992;&#26799;&#24230;&#26469;&#25552;&#20986;&#26377;&#21069;&#36884;&#30340;&#31361;&#21464;&#30340;&#24555;&#36895;MCMC&#37319;&#26679;&#22120;&#12290;</title><link>http://arxiv.org/abs/2212.09925</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#31163;&#25955;MCMC&#25554;&#20837;&#21363;&#29992;&#34507;&#30333;&#36136;&#23450;&#21521;&#36827;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plug &amp; Play Directed Evolution of Proteins with Gradient-based Discrete MCMC. (arXiv:2212.09925v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#34507;&#30333;&#36136;&#24037;&#31243;&#37319;&#26679;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#27169;&#25311;&#20013;&#36827;&#21270;&#34507;&#30333;&#36136;&#65292;&#36890;&#36807;&#32452;&#21512;&#26080;&#30417;&#30563;&#27169;&#22411;&#21644;&#30417;&#30563;&#27169;&#22411;&#26469;&#25552;&#39640;&#35780;&#20272;&#26410;&#35265;&#36807;&#30340;&#31361;&#21464;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20351;&#29992;&#26799;&#24230;&#26469;&#25552;&#20986;&#26377;&#21069;&#36884;&#30340;&#31361;&#21464;&#30340;&#24555;&#36895;MCMC&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#34507;&#30333;&#36136;&#24037;&#31243;&#30340;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#26159;&#21152;&#36895;&#21457;&#29616;&#25913;&#21892;&#24050;&#30693;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#26032;&#31361;&#21464;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#37319;&#26679;&#26694;&#26550;&#26469;&#22312;&#27169;&#25311;&#20013;&#36827;&#21270;&#34507;&#30333;&#36136;&#65292;&#25903;&#25345;&#28151;&#21512;&#21644;&#21305;&#37197;&#21508;&#31181;&#26080;&#30417;&#30563;&#27169;&#22411;(&#22914;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;) &#21644;&#20174;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#30417;&#30563;&#27169;&#22411;&#12290;&#36890;&#36807;&#32452;&#21512;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#39640;&#35780;&#20272;&#26410;&#35265;&#36807;&#30340;&#31361;&#21464;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#25628;&#32034;&#38480;&#21046;&#22312;&#21487;&#33021;&#21253;&#21547;&#21151;&#33021;&#34507;&#30333;&#30340;&#24207;&#21015;&#31354;&#38388;&#21306;&#22495;&#20869;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#22312;&#31163;&#25955;&#34507;&#30333;&#36136;&#31354;&#38388;&#20013;&#30452;&#25509;&#26500;&#24314;&#19987;&#23478;&#20998;&#24067;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#27169;&#22411;&#24494;&#35843;&#25110;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24555;&#36895;&#30340;MCMC&#37319;&#26679;&#22120;&#65292;&#20351;&#29992;&#26799;&#24230;&#26469;&#25552;&#20986;&#26377;&#21069;&#36884;&#30340;&#31361;&#21464;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#20856;&#22411;&#30340;&#23450;&#21521;&#36827;&#21270;&#26041;&#27861;&#20013;&#30340;&#26292;&#21147;&#25628;&#32034;&#25110;&#38543;&#26426;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#36866;&#24212;&#24615;&#31354;&#38388;&#19978;&#36827;&#34892;&#20102;&#27169;&#25311;&#23450;&#21521;&#36827;&#21270;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
A long-standing goal of machine-learning-based protein engineering is to accelerate the discovery of novel mutations that improve the function of a known protein. We introduce a sampling framework for evolving proteins in silico that supports mixing and matching a variety of unsupervised models, such as protein language models, and supervised models that predict protein function from sequence. By composing these models, we aim to improve our ability to evaluate unseen mutations and constrain search to regions of sequence space likely to contain functional proteins. Our framework achieves this without any model fine-tuning or re-training by constructing a product of experts distribution directly in discrete protein space. Instead of resorting to brute force search or random sampling, which is typical of classic directed evolution, we introduce a fast MCMC sampler that uses gradients to propose promising mutations. We conduct in silico directed evolution experiments on wide fitness lands
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21644;&#21487;&#35299;&#37322;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#20998;&#31867;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#24573;&#30053;&#36855;&#22240;&#35821;&#20041;&#21644;&#21019;&#24314;&#19978;&#19979;&#25991;&#23548;&#33268;&#20844;&#27491;&#20869;&#23481;&#31649;&#29702;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#37319;&#29992;&#31034;&#20363;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#25512;&#29702;&#24182;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;SOTA&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#25104;&#21151;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#26816;&#27979;&#20102;&#26377;&#23475;&#30340;&#36855;&#22240;&#12290;</title><link>http://arxiv.org/abs/2212.05612</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21644;&#21487;&#35299;&#37322;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multimodal and Explainable Internet Meme Classification. (arXiv:2212.05612v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21644;&#21487;&#35299;&#37322;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#20998;&#31867;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#24573;&#30053;&#36855;&#22240;&#35821;&#20041;&#21644;&#21019;&#24314;&#19978;&#19979;&#25991;&#23548;&#33268;&#20844;&#27491;&#20869;&#23481;&#31649;&#29702;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#37319;&#29992;&#31034;&#20363;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#25512;&#29702;&#24182;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;SOTA&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#25104;&#21151;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#26816;&#27979;&#20102;&#26377;&#23475;&#30340;&#36855;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#29615;&#22659;&#20013;&#65292;&#32593;&#32476;&#24179;&#21488;&#24050;&#32463;&#34987;&#26377;&#25928;&#22320;&#27494;&#22120;&#21270;&#65292;&#34987;&#29992;&#20110;&#21508;&#31181;&#22320;&#32536;&#25919;&#27835;&#20107;&#20214;&#21644;&#31038;&#20250;&#38382;&#39064;&#20013;&#65292;&#20114;&#32852;&#32593;&#36855;&#22240;&#20351;&#24471;&#22823;&#35268;&#27169;&#30340;&#20844;&#27491;&#20869;&#23481;&#31649;&#29702;&#26356;&#21152;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#36855;&#22240;&#20998;&#31867;&#21644;&#36319;&#36394;&#24037;&#20316;&#20027;&#35201;&#37319;&#29992;&#40657;&#30418;&#26041;&#27861;&#65292;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#36855;&#22240;&#30340;&#35821;&#20041;&#25110;&#20854;&#21019;&#24314;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36861;&#27714;&#19968;&#31181;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#20114;&#32852;&#32593;&#36855;&#22240;&#29702;&#35299;&#26550;&#26500;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#20998;&#31867;&#26041;&#27861;&#65292;&#23545;&#35757;&#32451;&#26696;&#20363;&#36827;&#34892;&#31034;&#20363;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#25512;&#29702;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;SOTA&#27169;&#22411;&#26469;&#34920;&#31034;&#21508;&#20010;&#26696;&#20363;&#12290; &#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#27169;&#22411;&#22312;&#26816;&#27979;&#20004;&#20010;&#29616;&#26377;&#20219;&#21153;&#20013;&#26377;&#23475;&#36855;&#22240;&#30340;&#30456;&#20851;&#24615;&#65306;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#21388;&#22899;&#30151;&#20998;&#31867;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#31034;&#20363;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#20197;&#21450;&#25991;&#26412;&#65292;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current context where online platforms have been effectively weaponized in a variety of geo-political events and social issues, Internet memes make fair content moderation at scale even more difficult. Existing work on meme classification and tracking has focused on black-box methods that do not explicitly consider the semantics of the memes or the context of their creation. In this paper, we pursue a modular and explainable architecture for Internet meme understanding. We design and implement multimodal classification methods that perform example- and prototype-based reasoning over training cases, while leveraging both textual and visual SOTA models to represent the individual cases. We study the relevance of our modular and explainable models in detecting harmful memes on two existing tasks: Hate Speech Detection and Misogyny Classification. We compare the performance between example- and prototype-based methods, and between text, vision, and multimodal models, across differen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#21518;&#38376;&#28165;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#26435;&#37325;&#37325;&#26032;&#21021;&#22987;&#21270;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#26377;&#25928;&#28165;&#38500;&#21487;&#30097;&#32593;&#32476;&#30340;&#21518;&#38376;&#34892;&#20026;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#36739;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.12044</link><description>&lt;p&gt;
&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#21518;&#38376;&#28165;&#38500;
&lt;/p&gt;
&lt;p&gt;
Backdoor Cleansing with Unlabeled Data. (arXiv:2211.12044v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#21518;&#38376;&#28165;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#26435;&#37325;&#37325;&#26032;&#21021;&#22987;&#21270;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#26377;&#25928;&#28165;&#38500;&#21487;&#30097;&#32593;&#32476;&#30340;&#21518;&#38376;&#34892;&#20026;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#36739;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#38656;&#27714;&#22686;&#21152;&#65292;&#20844;&#21496;&#21644;&#32452;&#32455;&#24050;&#32463;&#24320;&#22987;&#22806;&#37096;&#21270;&#35757;&#32451;&#36807;&#31243;&#12290;&#20294;&#26159;&#65292;&#22806;&#37096;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20250;&#38754;&#20020;&#21518;&#38376;&#25915;&#20987;&#12290;&#22240;&#27492;&#65292;&#20851;&#38190;&#22312;&#20110;&#38450;&#24481;&#36825;&#31181;&#25915;&#20987;&#65292;&#21363;&#21518;&#22788;&#29702;&#19968;&#20010;&#21487;&#30097;&#27169;&#22411;&#65292;&#20351;&#20854;&#30340;&#21518;&#38376;&#34892;&#20026;&#24471;&#21040;&#32531;&#35299;&#65292;&#21516;&#26102;&#20854;&#23545;&#20110;&#24178;&#20928;&#36755;&#20837;&#30340;&#27491;&#24120;&#39044;&#27979;&#33021;&#21147;&#20173;&#28982;&#20445;&#25345;&#19981;&#21463;&#24433;&#21709;&#12290;&#20026;&#20102;&#28040;&#38500;&#24322;&#24120;&#30340;&#21518;&#38376;&#34892;&#20026;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#26631;&#35760;&#24178;&#20928;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35201;&#27714;&#21487;&#33021;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#23545;&#26368;&#32456;&#29992;&#25143;&#19981;&#21487;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32469;&#36807;&#36825;&#31181;&#38556;&#30861;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#26631;&#31614;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#36880;&#23618;&#26435;&#37325;&#37325;&#26032;&#21021;&#22987;&#21270;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#28165;&#38500;&#21487;&#30097;&#32593;&#32476;&#30340;&#21518;&#38376;&#34892;&#20026;&#65292;&#21516;&#26102;&#23545;&#20854;&#27491;&#24120;&#34892;&#20026;&#30340;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#23427;&#32988;&#36807;&#29616;&#26377;&#30340;&#26080;&#26631;&#31614;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the increasing computational demand of Deep Neural Networks (DNNs), companies and organizations have begun to outsource the training process. However, the externally trained DNNs can potentially be backdoor attacked. It is crucial to defend against such attacks, i.e., to postprocess a suspicious model so that its backdoor behavior is mitigated while its normal prediction power on clean inputs remain uncompromised. To remove the abnormal backdoor behavior, existing methods mostly rely on additional labeled clean samples. However, such requirement may be unrealistic as the training data are often unavailable to end users. In this paper, we investigate the possibility of circumventing such barrier. We propose a novel defense method that does not require training labels. Through a carefully designed layer-wise weight re-initialization and knowledge distillation, our method can effectively cleanse backdoor behaviors of a suspicious network with negligible compromise in its normal beh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#22522;&#20110;&#26377;&#38480;&#20687;&#32032;&#30340;Hessian&#30697;&#38453;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;Limited Pixel BFGS&#65288;LP-BFGS&#65289;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.15446</link><description>&lt;p&gt;
LP-BFGS&#25915;&#20987;&#65306;&#22522;&#20110;Hessian&#30697;&#38453;&#30340;&#26377;&#38480;&#20687;&#32032;&#25932;&#23545;&#26679;&#26412;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
LP-BFGS attack: An adversarial attack based on the Hessian with limited pixels. (arXiv:2210.15446v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#22522;&#20110;&#26377;&#38480;&#20687;&#32032;&#30340;Hessian&#30697;&#38453;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;Limited Pixel BFGS&#65288;LP-BFGS&#65289;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#36973;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;$L_0$-norm&#30340;&#30333;&#30418;&#25915;&#20987;&#26159;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#26799;&#24230;&#26469;&#26500;&#36896;&#25200;&#21160;&#12290;&#30001;&#20110;&#35745;&#31639;Hessian&#30697;&#38453;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#20869;&#23384;&#38480;&#21046;&#65292;Hessian&#25110;&#36817;&#20284;Hessian&#22312;&#30333;&#30418;&#25915;&#20987;&#20013;&#30340;&#24212;&#29992;&#36880;&#28176;&#34987;&#25601;&#32622;&#12290;&#26412;&#25991;&#27880;&#24847;&#21040;$L_0$-norm&#26465;&#20214;&#33258;&#28982;&#20063;&#35201;&#27714;&#25200;&#21160;&#26159;&#31232;&#30095;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;Hessian&#30697;&#38453;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#26377;&#38480;&#25200;&#21160;&#20687;&#32032;&#30340;Hessian&#30697;&#38453;&#25915;&#20987;&#26041;&#27861;&#30340;&#25915;&#20987;&#24615;&#33021;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Limited Pixel BFGS (LP-BFGS)&#25915;&#20987;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#25200;&#21160;&#20687;&#32032;&#36873;&#25321;&#31574;&#30053;&#21644;BFGS&#31639;&#27861;&#65292;&#23558;&#30001;Integrated Gradient&#26041;&#27861;&#35745;&#31639;&#30340;&#21069;k&#20010;&#20687;&#32032;&#30340;&#34920;&#31034;&#20998;&#25968;&#35270;&#20026;&#20248;&#21270;&#21464;&#37327;&#12290;&#22312;&#19981;&#21516;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are vulnerable to adversarial attacks. Most $L_{0}$-norm based white-box attacks craft perturbations by the gradient of models to the input. Since the computation cost and memory limitation of calculating the Hessian matrix, the application of Hessian or approximate Hessian in white-box attacks is gradually shelved. In this work, we note that the sparsity requirement on perturbations naturally lends itself to the usage of Hessian information. We study the attack performance and computation cost of the attack method based on the Hessian with a limited number of perturbation pixels. Specifically, we propose the Limited Pixel BFGS (LP-BFGS) attack method by incorporating the perturbation pixel selection strategy and the BFGS algorithm. Pixels with top-k attribution scores calculated by the Integrated Gradient method are regarded as optimization variables of the LP-BFGS attack. Experimental results across different networks and datasets demonstrate that our approach ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#29983;&#28079;&#36866;&#24212;&#24615;&#36870;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#23398;&#20064;&#30340;&#31574;&#30053;&#20013;&#26500;&#24314;&#22810;&#26679;&#31574;&#30053;&#30340;&#32452;&#21512;&#23454;&#29616;&#20102;&#23545;&#26032;&#30340;&#28436;&#31034;&#30340;&#24555;&#36895;&#36866;&#24212;&#65292;&#21516;&#26102;&#25972;&#21512;&#28436;&#31034;&#20013;&#30340;&#20849;&#24615;&#30693;&#35782;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20219;&#21153;&#25512;&#26029;&#65292;&#36824;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#20013;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#31934;&#31616;&#30340;&#21407;&#22411;&#31574;&#30053;&#38598;&#21512;&#24182;&#36890;&#36807;&#31574;&#30053;&#32452;&#21512;&#26469;&#36924;&#36817;&#25152;&#26377;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2209.11908</link><description>&lt;p&gt;
&#26469;&#33258;&#28436;&#31034;&#30340;&#24555;&#36895;&#29983;&#28079;&#36866;&#24212;&#24615;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast Lifelong Adaptive Inverse Reinforcement Learning from Demonstrations. (arXiv:2209.11908v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#29983;&#28079;&#36866;&#24212;&#24615;&#36870;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#23398;&#20064;&#30340;&#31574;&#30053;&#20013;&#26500;&#24314;&#22810;&#26679;&#31574;&#30053;&#30340;&#32452;&#21512;&#23454;&#29616;&#20102;&#23545;&#26032;&#30340;&#28436;&#31034;&#30340;&#24555;&#36895;&#36866;&#24212;&#65292;&#21516;&#26102;&#25972;&#21512;&#28436;&#31034;&#20013;&#30340;&#20849;&#24615;&#30693;&#35782;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20219;&#21153;&#25512;&#26029;&#65292;&#36824;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#20013;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#31934;&#31616;&#30340;&#21407;&#22411;&#31574;&#30053;&#38598;&#21512;&#24182;&#36890;&#36807;&#31574;&#30053;&#32452;&#21512;&#26469;&#36924;&#36817;&#25152;&#26377;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#31034;&#23398;&#20064;&#65288;LfD&#65289;&#26041;&#27861;&#20351;&#32456;&#31471;&#29992;&#25143;&#36890;&#36807;&#25152;&#38656;&#34892;&#20026;&#30340;&#28436;&#31034;&#26469;&#25945;&#25480;&#26426;&#22120;&#20154;&#26032;&#20219;&#21153;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#20351;&#29992;&#38754;&#26356;&#24191;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;LfD&#26694;&#26550;&#26080;&#27861;&#24555;&#36895;&#36866;&#24212;&#24322;&#26500;&#30340;&#20154;&#31867;&#28436;&#31034;&#65292;&#20063;&#19981;&#33021;&#22312;&#26222;&#36866;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LfD&#26694;&#26550;&#8212;&#8212;&#24555;&#36895;&#29983;&#28079;&#36866;&#24212;&#24615;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;FLAIR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;(1)&#21033;&#29992;&#23398;&#20064;&#31574;&#30053;&#26500;&#24314;&#22810;&#26679;&#31574;&#30053;&#30340;&#32452;&#21512;&#65292;&#20174;&#32780;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#28436;&#31034;&#65292;&#20801;&#35768;&#24555;&#36895;&#30340;&#32456;&#31471;&#29992;&#25143;&#20010;&#24615;&#21270;&#65292;(2)&#25972;&#21512;&#28436;&#31034;&#20013;&#30340;&#20849;&#24615;&#30693;&#35782;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20219;&#21153;&#25512;&#26029;&#65307;(3)&#22312;&#32456;&#36523;&#37096;&#32626;&#20013;&#21482;&#22312;&#38656;&#35201;&#26102;&#25193;&#23637;&#20854;&#27169;&#22411;&#65292;&#36890;&#36807;&#31574;&#30053;&#32452;&#21512;&#32500;&#25252;&#19968;&#20010;&#31934;&#31616;&#30340;&#21407;&#22411;&#31574;&#30053;&#38598;&#21512;&#65292;&#24182;&#33021;&#22815;&#36924;&#36817;&#25152;&#26377;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;FLAIR&#23454;&#29616;&#20102;&#36866;&#24212;&#24615;&#65288;&#21363;&#26426;&#22120;&#20154;&#36866;&#24212;&#20102;&#24322;&#26500;&#30340;&#12289;&#29305;&#23450;&#20110;&#29992;&#25143;&#30340;&#20219;&#21153;&#65289;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#20013;&#33410;&#30465;&#20102;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Demonstration (LfD) approaches empower end-users to teach robots novel tasks via demonstrations of the desired behaviors, democratizing access to robotics. However, current LfD frameworks are not capable of fast adaptation to heterogeneous human demonstrations nor the large-scale deployment in ubiquitous robotics applications. In this paper, we propose a novel LfD framework, Fast Lifelong Adaptive Inverse Reinforcement learning (FLAIR). Our approach (1) leverages learned strategies to construct policy mixtures for fast adaptation to new demonstrations, allowing for quick end-user personalization, (2) distills common knowledge across demonstrations, achieving accurate task inference; and (3) expands its model only when needed in lifelong deployments, maintaining a concise set of prototypical strategies that can approximate all behaviors via policy mixtures. We empirically validate that FLAIR achieves adaptability (i.e., the robot adapts to heterogeneous, user-specific task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#26041;&#27861;&#30340;&#22312;&#32447;&#23454;&#39564;&#25968;&#25454;&#22635;&#34917;&#26041;&#27861;&#65292;&#23558;&#19981;&#23436;&#25972;&#25351;&#26631;&#20540;&#30340;&#29992;&#25143;&#20998;&#20026;&#35775;&#23458;&#21644;&#32570;&#22833;&#36141;&#20080;&#32773;&#20004;&#32452;&#65292;&#20351;&#29992;$k$-&#26368;&#36817;&#37051;&#22635;&#34917;&#26041;&#27861;&#65292;&#24182;&#32771;&#34385;&#23454;&#39564;&#29305;&#23450;&#30340;&#29305;&#24449;&#21644;&#29992;&#25143;&#30340;&#36141;&#29289;&#36335;&#24452;&#27963;&#21160;&#65292;&#21516;&#26102;&#20351;&#29992;&#20998;&#23618;&#21644;&#32858;&#31867;&#32467;&#21512;&#30340;&#26041;&#24335;&#25552;&#39640;&#22635;&#34917;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2209.06125</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#32570;&#22833;&#36141;&#20080;&#32773;&#22312;&#32447;&#23454;&#39564;&#25968;&#25454;&#22635;&#34917;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Clustering-based Imputation for Dropout Buyers in Large-scale Online Experimentation. (arXiv:2209.06125v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#26041;&#27861;&#30340;&#22312;&#32447;&#23454;&#39564;&#25968;&#25454;&#22635;&#34917;&#26041;&#27861;&#65292;&#23558;&#19981;&#23436;&#25972;&#25351;&#26631;&#20540;&#30340;&#29992;&#25143;&#20998;&#20026;&#35775;&#23458;&#21644;&#32570;&#22833;&#36141;&#20080;&#32773;&#20004;&#32452;&#65292;&#20351;&#29992;$k$-&#26368;&#36817;&#37051;&#22635;&#34917;&#26041;&#27861;&#65292;&#24182;&#32771;&#34385;&#23454;&#39564;&#29305;&#23450;&#30340;&#29305;&#24449;&#21644;&#29992;&#25143;&#30340;&#36141;&#29289;&#36335;&#24452;&#27963;&#21160;&#65292;&#21516;&#26102;&#20351;&#29992;&#20998;&#23618;&#21644;&#32858;&#31867;&#32467;&#21512;&#30340;&#26041;&#24335;&#25552;&#39640;&#22635;&#34917;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23454;&#39564;&#20013;&#65292;&#21512;&#36866;&#30340;&#24230;&#37327;&#25351;&#26631;&#65288;&#27604;&#22914;&#36141;&#20080;&#65289;&#21487;&#20197;&#25552;&#20379;&#25903;&#25345;&#20551;&#35774;&#21644;&#22686;&#24378;&#20915;&#31574;&#36807;&#31243;&#30340;&#24378;&#26377;&#21147;&#35777;&#25454;&#12290;&#20294;&#26159;&#65292;&#22312;&#32447;&#23454;&#39564;&#20013;&#32463;&#24120;&#20986;&#29616;&#19981;&#23436;&#25972;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#20351;&#24471;&#21487;&#29992;&#25968;&#25454;&#27604;&#35745;&#21010;&#30340;&#22312;&#32447;&#23454;&#39564;&#65288;&#27604;&#22914;A/B&#27979;&#35797;&#65289;&#35201;&#23569;&#24471;&#22810;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32570;&#22833;&#36141;&#20080;&#32773;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#25351;&#26631;&#20540;&#19981;&#23436;&#25972;&#30340;&#29992;&#25143;&#20998;&#20026;&#20004;&#32452;&#65306;&#35775;&#23458;&#21644;&#32570;&#22833;&#36141;&#20080;&#32773;&#12290;&#20026;&#20102;&#20998;&#26512;&#19981;&#23436;&#25972;&#30340;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;$k$-&#26368;&#36817;&#37051;&#22635;&#34917;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22635;&#34917;&#26041;&#27861;&#32771;&#34385;&#20102;&#23454;&#39564;&#29305;&#23450;&#30340;&#29305;&#24449;&#21644;&#29992;&#25143;&#27839;&#36141;&#29289;&#36335;&#24452;&#30340;&#27963;&#21160;&#65292;&#20801;&#35768;&#19981;&#21516;&#30340;&#29992;&#25143;&#26377;&#19981;&#21516;&#30340;&#22635;&#34917;&#20540;&#12290;&#20026;&#20102;&#26041;&#20415;&#22320;&#22635;&#34917;&#22312;&#32447;&#23454;&#39564;&#20013;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20998;&#23618;&#21644;&#32858;&#31867;&#32467;&#21512;&#30340;&#26041;&#24335;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#29616;&#26377;&#30340;&#27604;&#36739;&#26041;&#27861;&#30456;&#27604;&#36739;&#20026;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online experimentation, appropriate metrics (e.g., purchase) provide strong evidence to support hypotheses and enhance the decision-making process. However, incomplete metrics are frequently occurred in the online experimentation, making the available data to be much fewer than the planned online experiments (e.g., A/B testing). In this work, we introduce the concept of dropout buyers and categorize users with incomplete metric values into two groups: visitors and dropout buyers. For the analysis of incomplete metrics, we propose a clustering-based imputation method using $k$-nearest neighbors. Our proposed imputation method considers both the experiment-specific features and users' activities along their shopping paths, allowing different imputation values for different users. To facilitate efficient imputation of large-scale data sets in online experimentation, the proposed method uses a combination of stratification and clustering. The performance of the proposed method is compar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35889;&#25216;&#26415;&#30340;&#20960;&#31181;&#32858;&#31867;&#21644;&#25490;&#24207;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#24335;&#8212;&#8212;&#26631;&#31614;&#36830;&#32493;&#35823;&#24046;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#25506;&#31350;&#20102;&#25490;&#24207;&#26041;&#27861;&#21644;&#32858;&#31867;&#26041;&#27861;&#20998;&#21035;&#35782;&#21035;&#27169;&#22359;&#32467;&#26500;&#21644;&#24102;&#29366;&#32467;&#26500;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.12933</link><description>&lt;p&gt;
&#22270;&#24418;&#30340;&#25490;&#24207;&#21644;&#32858;&#31867;&#26041;&#27861;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Consistency between ordering and clustering methods for graphs. (arXiv:2208.12933v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35889;&#25216;&#26415;&#30340;&#20960;&#31181;&#32858;&#31867;&#21644;&#25490;&#24207;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#24335;&#8212;&#8212;&#26631;&#31614;&#36830;&#32493;&#35823;&#24046;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#25506;&#31350;&#20102;&#25490;&#24207;&#26041;&#27861;&#21644;&#32858;&#31867;&#26041;&#27861;&#20998;&#21035;&#35782;&#21035;&#27169;&#22359;&#32467;&#26500;&#21644;&#24102;&#29366;&#32467;&#26500;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32858;&#31867;&#25110;&#25490;&#24207;&#30340;&#26041;&#24335;&#32473;&#27599;&#20010;&#20803;&#32032;&#20998;&#37197;&#26631;&#31614;&#26469;&#20998;&#26512;&#20851;&#31995;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#32858;&#31867;&#21644;&#25490;&#24207;&#26041;&#27861;&#37117;&#33021;&#23454;&#29616;&#30456;&#20284;&#30340;&#25968;&#25454;&#38598;&#25551;&#36848;&#65292;&#20294;&#21069;&#32773;&#27604;&#21518;&#32773;&#26356;&#27963;&#36291;&#22320;&#30740;&#31350;&#20102;&#36825;&#19968;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20197;&#22270;&#24418;&#34920;&#31034;&#30340;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#20960;&#31181;&#32858;&#31867;&#21644;&#25490;&#24207;&#26041;&#27861;&#20043;&#38388;&#30340;&#26041;&#27861;&#20851;&#31995;&#65292;&#37325;&#28857;&#20851;&#27880;&#35889;&#25216;&#26415;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#32858;&#31867;&#21644;&#25490;&#24207;&#26041;&#27861;&#30340;&#32467;&#26524;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26631;&#31614;&#36830;&#32493;&#35823;&#24046;&#30340;&#24230;&#37327;&#26041;&#24335;&#65292;&#23427;&#36890;&#24120;&#37327;&#21270;&#20102;&#19968;&#32452;&#20803;&#32032;&#30340;&#24207;&#21015;&#21644;&#20998;&#21306;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#31243;&#24230;&#12290;&#22522;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25490;&#24207;&#26041;&#27861;&#35782;&#21035;&#27169;&#22359;&#32467;&#26500;&#21644;&#32858;&#31867;&#26041;&#27861;&#35782;&#21035;&#24102;&#29366;&#32467;&#26500;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A relational dataset is often analyzed by optimally assigning a label to each element through clustering or ordering. While similar characterizations of a dataset would be achieved by both clustering and ordering methods, the former has been studied much more actively than the latter, particularly for the data represented as graphs. This study fills this gap by investigating methodological relationships between several clustering and ordering methods, focusing on spectral techniques. Furthermore, we evaluate the resulting performance of the clustering and ordering methods. To this end, we propose a measure called the label continuity error, which generically quantifies the degree of consistency between a sequence and partition for a set of elements. Based on synthetic and real-world datasets, we evaluate the extents to which an ordering method identifies a module structure and a clustering method identifies a banded structure.
&lt;/p&gt;</description></item><item><title>Retweet-BERT&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;Twitter&#29992;&#25143;&#30340;&#25919;&#27835;&#20542;&#21521;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#36716;&#21457;&#32593;&#32476;&#32467;&#26500;&#21644;&#29992;&#25143;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#22312;COVID-19&#21644;2020&#24180;&#32654;&#22269;&#24635;&#32479;&#36873;&#20030;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#22312;Twitter&#19978;&#23384;&#22312;&#30528;&#21491;&#20542;&#29992;&#25143;&#20043;&#38388;&#30340;&#25919;&#27835;&#22238;&#38899;&#23460;&#12290;</title><link>http://arxiv.org/abs/2207.08349</link><description>&lt;p&gt;
Retweet-BERT&#65306;&#22522;&#20110;&#35821;&#35328;&#29305;&#24449;&#21644;&#31038;&#20132;&#32593;&#32476;&#20449;&#24687;&#25193;&#25955;&#30340;&#25919;&#27835;&#20542;&#21521;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Retweet-BERT: Political Leaning Detection Using Language Features and Information Diffusion on Social Networks. (arXiv:2207.08349v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08349
&lt;/p&gt;
&lt;p&gt;
Retweet-BERT&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;Twitter&#29992;&#25143;&#30340;&#25919;&#27835;&#20542;&#21521;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#36716;&#21457;&#32593;&#32476;&#32467;&#26500;&#21644;&#29992;&#25143;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#22312;COVID-19&#21644;2020&#24180;&#32654;&#22269;&#24635;&#32479;&#36873;&#20030;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#22312;Twitter&#19978;&#23384;&#22312;&#30528;&#21491;&#20542;&#29992;&#25143;&#20043;&#38388;&#30340;&#25919;&#27835;&#22238;&#38899;&#23460;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#25919;&#27835;&#20542;&#21521;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#36234;&#26469;&#36234;&#32039;&#36843;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#31038;&#20132;&#23186;&#20307;&#28040;&#36153;&#37327;&#30340;&#22686;&#21152;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Retweet-BERT&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;Twitter&#29992;&#25143;&#30340;&#25919;&#27835;&#20542;&#21521;&#12290;Retweet-BERT&#21033;&#29992;&#36716;&#21457;&#32593;&#32476;&#32467;&#26500;&#21644;&#29992;&#25143;&#20010;&#20154;&#36164;&#26009;&#25551;&#36848;&#20013;&#20351;&#29992;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26469;&#33258;&#20110;&#32593;&#32476;&#21644;&#35821;&#35328;&#21516;&#36136;&#24615;&#30340;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#22312;&#37027;&#20123;&#20998;&#20139;&#30456;&#20284;&#24847;&#35782;&#24418;&#24577;&#30340;&#20154;&#20204;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;Retweet-BERT&#22312;&#20004;&#20010;&#26368;&#36817;&#30340;Twitter&#25968;&#25454;&#38598;&#65288;&#19968;&#20010;COVID-19&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;2020&#24180;&#32654;&#22269;&#24635;&#32479;&#36873;&#20030;&#25968;&#25454;&#38598;&#65289;&#19978;&#23637;&#29616;&#20986;&#19982;&#20854;&#20182;&#26368;&#26032;&#22522;&#32447;&#30340;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;96%-97%&#30340;&#23439;F1&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#25163;&#21160;&#39564;&#35777;&#65292;&#20197;&#39564;&#35777;Retweet-BERT&#22312;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#30340;&#29992;&#25143;&#19978;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#22312;COVID-19&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Twitter&#19978;&#25919;&#27835;&#22238;&#38899;&#23460;&#30340;&#23384;&#22312;&#65292;&#24182;&#34920;&#26126;&#36825;&#20027;&#35201;&#23384;&#22312;&#20110;&#21491;&#20542;&#29992;&#25143;&#20043;&#38388;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the political leanings of social media users is a challenging and ever more pressing problem given the increase in social media consumption. We introduce Retweet-BERT, a simple and scalable model to estimate the political leanings of Twitter users. Retweet-BERT leverages the retweet network structure and the language used in users' profile descriptions. Our assumptions stem from patterns of networks and linguistics homophily among people who share similar ideologies. Retweet-BERT demonstrates competitive performance against other state-of-the-art baselines, achieving 96%-97% macro-F1 on two recent Twitter datasets (a COVID-19 dataset and a 2020 United States presidential elections dataset). We also perform manual validation to validate the performance of Retweet-BERT on users not in the training data. Finally, in a case study of COVID-19, we illustrate the presence of political echo chambers on Twitter and show that it exists primarily among right-leaning users. Our code is 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20197;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#26041;&#24335;&#30740;&#31350;&#28183;&#36879;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#19981;&#21516;&#26230;&#26684;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#65292;&#32467;&#21512;&#28151;&#28102;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#28183;&#36879;&#38408;&#20540;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2207.03368</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#28183;&#36879;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine learning of percolation models using graph convolutional neural networks. (arXiv:2207.03368v2 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03368
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20197;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#26041;&#24335;&#30740;&#31350;&#28183;&#36879;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#19981;&#21516;&#26230;&#26684;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#65292;&#32467;&#21512;&#28151;&#28102;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#28183;&#36879;&#38408;&#20540;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28183;&#36879;&#26159;&#27668;&#20505;&#12289;&#29289;&#29702;&#12289;&#26448;&#26009;&#31185;&#23398;&#12289;&#27969;&#34892;&#30149;&#23398;&#12289;&#37329;&#34701;&#31561;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20027;&#39064;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#28183;&#36879;&#38408;&#20540;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20197;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#26041;&#24335;&#30740;&#31350;&#28183;&#36879;&#12290;&#20174;&#30417;&#30563;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21516;&#26102;&#27491;&#30830;&#22320;&#35757;&#32451;&#19981;&#21516;&#26230;&#26684;&#31867;&#22411;&#65288;&#22914;&#27491;&#26041;&#24418;&#21644;&#19977;&#35282;&#24418;&#26230;&#26684;&#65289;&#30340;&#25968;&#25454;&#12290;&#20174;&#38750;&#30417;&#30563;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23558;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19982;&#28151;&#28102;&#27861;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#36890;&#36807;&#8220;W&#8221;&#24418;&#24615;&#33021;&#33719;&#24471;&#28183;&#36879;&#38408;&#20540;&#12290;&#26412;&#25991;&#30340;&#21457;&#29616;&#24320;&#36767;&#20102;&#24314;&#31435;&#19968;&#20010;&#21487;&#20197;&#25506;&#27979;&#28183;&#36879;&#29616;&#35937;&#30340;&#26356;&#24191;&#27867;&#26694;&#26550;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Percolation is an important topic in climate, physics, materials science, epidemiology, finance, and so on. Prediction of percolation thresholds with machine learning methods remains challenging. In this paper, we build a powerful graph convolutional neural network to study the percolation in both supervised and unsupervised ways. From a supervised learning perspective, the graph convolutional neural network simultaneously and correctly trains data of different lattice types, such as the square and triangular lattices. For the unsupervised perspective, combining the graph convolutional neural network and the confusion method, the percolation threshold can be obtained by the "W" shaped performance. The finding of this work opens up the possibility of building a more general framework that can probe the percolation-related phenomenon.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21435;&#22122;MDP&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#29616;&#23454;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#24178;&#25200;&#22240;&#32032;&#21435;&#38500;&#65292;&#23398;&#20064;&#19968;&#20010;&#26356;&#22909;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#21152;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2206.15477</link><description>&lt;p&gt;
&#28040;&#38500;&#22122;&#22768;&#30340;MDPs&#65306;&#23398;&#20064;&#27604;&#29616;&#23454;&#19990;&#30028;&#26412;&#36523;&#26356;&#22909;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoised MDPs: Learning World Models Better Than the World Itself. (arXiv:2206.15477v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21435;&#22122;MDP&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#29616;&#23454;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#24178;&#25200;&#22240;&#32032;&#21435;&#38500;&#65292;&#23398;&#20064;&#19968;&#20010;&#26356;&#22909;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#21152;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31163;&#20449;&#21495;&#19982;&#22122;&#22768;&#65292;&#24182;&#33021;&#29702;&#24615;&#22320;&#25226;&#25569;&#26377;&#25928;&#20449;&#24687;&#23545;&#20110;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20351;&#24471;&#20154;&#31867;&#21487;&#20197;&#39640;&#25928;&#22320;&#23436;&#25104;&#29616;&#23454;&#20219;&#21153;&#32780;&#19981;&#24517;&#32771;&#34385;&#25152;&#26377;&#21487;&#33021;&#30340;&#28902;&#29712;&#22240;&#32032;&#12290;&#37027;&#20040;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22914;&#20309;&#25165;&#33021;&#20570;&#21040;&#36825;&#19968;&#28857;&#65311;&#20195;&#29702;&#21487;&#20197;&#25918;&#24323;&#21738;&#20123;&#20449;&#24687;&#20197;&#36991;&#20813;&#22122;&#22768;&#30340;&#24178;&#25200;&#65311;&#26412;&#25991;&#22522;&#20110;&#21487;&#25511;&#24615;&#21644;&#19982;&#22870;&#21169;&#30340;&#20851;&#31995;&#23558;&#20449;&#24687;&#20998;&#20026;&#22235;&#31867;&#65292;&#24182;&#23558;&#26377;&#29992;&#20449;&#24687;&#23450;&#20041;&#20026;&#26082;&#21487;&#25511;&#21046;&#21448;&#19982;&#22870;&#21169;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#28548;&#28165;&#20102;&#21508;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#30340;&#20808;&#21069;&#24037;&#20316;&#21024;&#38500;&#30340;&#20449;&#24687;&#31867;&#22411;&#65292;&#24182;&#23548;&#33268;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#28040;&#38500;&#26576;&#20123;&#22122;&#22768;&#24178;&#25200;&#30340;&#21435;&#22122;MDP&#30340;&#26041;&#27861;&#12290;&#22312;DeepMind&#25511;&#21046;&#22871;&#20214;&#21644;RoboDesk&#30340;&#21508;&#31181;&#21464;&#20307;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20165;&#20351;&#29992;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20197;&#21450;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#21435;&#22122;&#19990;&#30028;&#27169;&#22411;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to separate signal from noise, and reason with clean abstractions, is critical to intelligence. With this ability, humans can efficiently perform real world tasks without considering all possible nuisance factors.How can artificial agents do the same? What kind of information can agents safely discard as noises?  In this work, we categorize information out in the wild into four types based on controllability and relation with reward, and formulate useful information as that which is both controllable and reward-relevant. This framework clarifies the kinds information removed by various prior work on representation learning in reinforcement learning (RL), and leads to our proposed approach of learning a Denoised MDP that explicitly factors out certain noise distractors. Extensive experiments on variants of DeepMind Control Suite and RoboDesk demonstrate superior performance of our denoised world model over using raw observations alone, and over prior works, across policy opt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20174;TROPOMI&#21355;&#26143;&#25968;&#25454;&#20013;&#20998;&#21106;&#20986;&#21333;&#20010;&#33337;&#21482;NO2&#25490;&#25918;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#20026;&#22522;&#20110;&#36965;&#24863;&#30340;&#20840;&#29699;&#25490;&#25918;&#30417;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.06993</link><description>&lt;p&gt;
&#21033;&#29992;TROPOMI&#21355;&#26143;&#25968;&#25454;&#23545;&#21333;&#20010;&#33337;&#21482;&#30340;NO2&#25490;&#25918;&#36827;&#34892;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Supervised segmentation of NO2 plumes from individual ships using TROPOMI satellite data. (arXiv:2203.06993v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.06993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20174;TROPOMI&#21355;&#26143;&#25968;&#25454;&#20013;&#20998;&#21106;&#20986;&#21333;&#20010;&#33337;&#21482;NO2&#25490;&#25918;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#20026;&#22522;&#20110;&#36965;&#24863;&#30340;&#20840;&#29699;&#25490;&#25918;&#30417;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33322;&#36816;&#19994;&#26159;&#26368;&#24378;&#30340;&#20154;&#31867;&#25490;&#25918;NOx&#29289;&#36136;&#30340;&#34892;&#19994;&#20043;&#19968;&#65292;&#32780;&#36825;&#31181;&#29289;&#36136;&#23545;&#20154;&#31867;&#20581;&#24247;&#21644;&#29615;&#22659;&#37117;&#26377;&#23475;&#12290;&#35813;&#34892;&#19994;&#30340;&#36805;&#36895;&#22686;&#38271;&#23548;&#33268;&#31038;&#20250;&#23545;&#25511;&#21046;&#33337;&#21482;&#25490;&#25918;&#27700;&#24179;&#30340;&#21387;&#21147;&#22686;&#21152;&#12290;&#30446;&#21069;&#29992;&#20110;&#33337;&#33334;&#25490;&#25918;&#30417;&#27979;&#30340;&#25152;&#26377;&#26041;&#27861;&#37117;&#24456;&#26114;&#36149;&#19988;&#38656;&#35201;&#25509;&#36817;&#33337;&#21482;&#65292;&#36825;&#20351;&#24471;&#20840;&#29699;&#21644;&#36830;&#32493;&#30340;&#25490;&#25918;&#30417;&#27979;&#21464;&#24471;&#19981;&#21487;&#33021;&#12290;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#24212;&#29992;&#36965;&#24863;&#25216;&#26415;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;Copernicus Sentinel 5&#20809;&#23398;&#36733;&#33655;&#19978;&#30340;TROPOspheric Monitoring Instrument&#27979;&#37327;&#21487;&#20197;&#35270;&#35273;&#19978;&#21306;&#21035;&#20986;&#21333;&#20010;&#33337;&#21482;&#30340;&#26576;&#20123;NO2&#25490;&#25918;&#12290;&#20026;&#20102;&#37096;&#32626;&#22522;&#20110;&#36965;&#24863;&#30340;&#20840;&#29699;&#25490;&#25918;&#30417;&#27979;&#31995;&#32479;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#31243;&#24207;&#26469;&#20272;&#31639;&#21333;&#20010;&#33337;&#21482;&#30340;NO2&#25490;&#25918;&#12290;&#30001;&#20110;&#21487;&#29992;&#25968;&#25454;&#30340;&#20449;&#22122;&#27604;&#26497;&#20302;&#19988;&#32570;&#20047;&#22320;&#38754;&#30495;&#23454;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#20805;&#28385;&#25361;&#25112;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;TROPOMI&#21355;&#26143;&#25968;&#25454;&#20013;&#20934;&#30830;&#22320;&#35782;&#21035;&#21333;&#20010;&#33337;&#21482;&#30340;NO2&#25490;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
The shipping industry is one of the strongest anthropogenic emitters of $\text{NO}_\text{x}$ -- substance harmful both to human health and the environment. The rapid growth of the industry causes societal pressure on controlling the emission levels produced by ships. All the methods currently used for ship emission monitoring are costly and require proximity to a ship, which makes global and continuous emission monitoring impossible. A promising approach is the application of remote sensing. Studies showed that some of the $\text{NO}_\text{2}$ plumes from individual ships can visually be distinguished using the TROPOspheric Monitoring Instrument on board the Copernicus Sentinel 5 Precursor (TROPOMI/S5P). To deploy a remote sensing-based global emission monitoring system, an automated procedure for the estimation of $\text{NO}_\text{2}$ emissions from individual ships is needed. The extremely low signal-to-noise ratio of the available data as well as the absence of ground truth makes th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#30005;&#21160;&#27773;&#36710;&#30005;&#27744;&#36827;&#34892;&#24555;&#36895;&#35780;&#20272;&#65292;&#20197;&#21028;&#26029;&#20854;&#26159;&#21542;&#36866;&#21512;&#36827;&#34892;&#20108;&#27425;&#21033;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20351;&#29992;&#30005;&#27744;&#30005;&#21387;&#21644;&#30005;&#27969;&#31561;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#36827;&#34892;&#39044;&#27979;&#65292;&#39564;&#35777;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#22343;&#26377;&#24076;&#26395;&#24471;&#21040;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2203.04249</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#30005;&#21160;&#27773;&#36710;&#30005;&#27744;&#20108;&#27425;&#21033;&#29992;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating feasibility of batteries for second-life applications using machine learning. (arXiv:2203.04249v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.04249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#30005;&#21160;&#27773;&#36710;&#30005;&#27744;&#36827;&#34892;&#24555;&#36895;&#35780;&#20272;&#65292;&#20197;&#21028;&#26029;&#20854;&#26159;&#21542;&#36866;&#21512;&#36827;&#34892;&#20108;&#27425;&#21033;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20351;&#29992;&#30005;&#27744;&#30005;&#21387;&#21644;&#30005;&#27969;&#31561;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#36827;&#34892;&#39044;&#27979;&#65292;&#39564;&#35777;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#22343;&#26377;&#24076;&#26395;&#24471;&#21040;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#21450;&#26102;&#35780;&#20272;&#36864;&#24441;&#30005;&#21160;&#27773;&#36710;&#30005;&#27744;&#26159;&#21542;&#36866;&#21512;&#36827;&#34892;&#20108;&#27425;&#21033;&#29992;&#65292;&#20197;&#25193;&#23637;&#20854;&#21407;&#26412;&#30340;&#20351;&#29992;&#23551;&#21629;&#65292;&#25110;&#32773;&#23558;&#20854;&#36865;&#21040;&#22238;&#25910;&#35774;&#26045;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21033;&#29992;&#31616;&#21333;&#30340;&#32479;&#35745;&#23398;&#24212;&#29992;&#20174;&#21487;&#29992;&#30340;&#30005;&#27744;&#30005;&#27969;&#21644;&#30005;&#21387;&#27979;&#37327;&#20013;&#29983;&#25104;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#30456;&#20851;&#24615;&#20998;&#26512;&#36873;&#25321;&#24182;&#25490;&#24207;&#29305;&#24449;&#65292;&#21516;&#26102;&#37319;&#29992;&#22686;&#24378;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;&#21253;&#25324;&#24930;&#20805;&#21644;&#24555;&#20805;&#12289;&#19981;&#21516;&#38452;&#26497;&#21270;&#23398;&#32452;&#25104;&#20197;&#21450;&#22810;&#26679;&#21270;&#25805;&#20316;&#26465;&#20214;&#19979;&#30340;&#36229;&#36807;200&#20010;&#30005;&#27744;&#30340;&#20844;&#24320;&#32769;&#21270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;&#22312;&#22810;&#20010;&#35757;&#32451;-&#27979;&#35797;&#21010;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;&#35266;&#23519;&#21040;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#65292;&#22343;&#26041;&#26681;&#30334;&#20998;&#27604;&#35823;&#24046;&#21644;&#24179;&#22343;&#30334;&#20998;&#27604;&#35823;&#24046;&#24615;&#33021;&#35823;&#24046;&#30340;&#22343;&#20540;&#20998;&#21035;&#23567;&#20110;1.48&#65285;&#21644;1.29&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a combination of machine learning techniques to enable prompt evaluation of retired electric vehicle batteries as to either retain those batteries for a second-life application and extend their operation beyond the original and first intent or send them to recycle facilities. The proposed algorithm generates features from available battery current and voltage measurements with simple statistics, selects and ranks the features using correlation analysis, and employs Gaussian Process Regression enhanced with bagging. This approach is validated over publicly available aging datasets of more than 200 cells with slow and fast charging, with different cathode chemistries, and for diverse operating conditions. Promising results are observed based on multiple training-test partitions, wherein the mean of Root Mean Squared Percent Error and Mean Percent Error performance errors are found to be less than 1.48% and 1.29%, respectively, in the worst-case scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#30002;&#22411;&#27969;&#24863;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#65292;&#20026;&#26089;&#26399;&#21644;&#24555;&#36895;&#25511;&#21046;&#30149;&#27602;&#20256;&#25773;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2201.01140</link><description>&lt;p&gt;
&#21033;&#29992;PSSM&#21644;&#35789;&#23884;&#20837;&#39044;&#27979;&#30002;&#22411;&#27969;&#24863;&#30149;&#27602;&#23487;&#20027;
&lt;/p&gt;
&lt;p&gt;
Predicting Influenza A Viral Host Using PSSM and Word Embeddings. (arXiv:2201.01140v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01140
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#30002;&#22411;&#27969;&#24863;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#65292;&#20026;&#26089;&#26399;&#21644;&#24555;&#36895;&#25511;&#21046;&#30149;&#27602;&#20256;&#25773;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24863;&#30149;&#27602;&#30340;&#24555;&#36895;&#31361;&#21464;&#23041;&#32961;&#20844;&#20849;&#20581;&#24247;&#65292;&#21487;&#33021;&#24341;&#21457;&#33268;&#21629;&#30340;&#22823;&#27969;&#34892;&#30149;&#12290;&#28982;&#32780;&#65292;&#26816;&#27979;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#22312;&#29190;&#21457;&#26399;&#38388;&#25110;&#29190;&#21457;&#21518;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#27969;&#24863;&#30149;&#27602;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#29289;&#31181;&#20043;&#38388;&#24490;&#29615;&#20256;&#25773;&#12290;&#22240;&#27492;&#65292;&#26089;&#26399;&#21644;&#24555;&#36895;&#26816;&#27979;&#30149;&#27602;&#23487;&#20027;&#23558;&#26377;&#21161;&#20110;&#20943;&#23569;&#30149;&#27602;&#30340;&#36827;&#19968;&#27493;&#20256;&#25773;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#20301;&#32622;&#29305;&#24322;&#24615;&#24471;&#20998;&#30697;&#38453;&#65288;PSSM&#65289;&#21644;&#23398;&#20064;&#33258;&#35789;&#23884;&#20837;&#21644;&#35789;&#32534;&#30721;&#30340;&#29305;&#24449;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25512;&#26029;&#30149;&#27602;&#30340;&#21407;&#22987;&#23487;&#20027;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;PSSM&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#36798;&#21040;&#20102;95%&#24038;&#21491;&#30340;MCC&#21644;96%&#24038;&#21491;&#30340;F1&#12290;&#20351;&#29992;&#35789;&#23884;&#20837;&#30340;&#27169;&#22411;&#24471;&#21040;&#30340;MCC&#32422;&#20026;96&#65285;&#65292;F1&#32422;&#20026;97&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid mutation of the influenza virus threatens public health. Reassortment among viruses with different hosts can lead to a fatal pandemic. However, it is difficult to detect the original host of the virus during or after an outbreak as influenza viruses can circulate between different species. Therefore, early and rapid detection of the viral host would help reduce the further spread of the virus. We use various machine learning models with features derived from the position-specific scoring matrix (PSSM) and features learned from word embedding and word encoding to infer the origin host of viruses. The results show that the performance of the PSSM-based model reaches the MCC around 95%, and the F1 around 96%. The MCC obtained using the model with word embedding is around 96%, and the F1 is around 97%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#22810;&#20010;&#23567;&#23545;&#35937;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#38543;&#26426;&#35009;&#21098;&#26367;&#25442;&#20026;&#23545;&#35937;&#25552;&#35758;&#31639;&#27861;&#33719;&#24471;&#30340;&#35009;&#21098;&#12290;</title><link>http://arxiv.org/abs/2112.00319</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#23545;&#35937;&#24863;&#30693;&#35009;&#21098;
&lt;/p&gt;
&lt;p&gt;
Object-Aware Cropping for Self-Supervised Learning. (arXiv:2112.00319v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.00319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#22810;&#20010;&#23567;&#23545;&#35937;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#38543;&#26426;&#35009;&#21098;&#26367;&#25442;&#20026;&#23545;&#35937;&#25552;&#35758;&#31639;&#27861;&#33719;&#24471;&#30340;&#35009;&#21098;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#19968;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26159;&#25968;&#25454;&#22686;&#24378;&#30340;&#35009;&#21098;&#26041;&#27861;&#65292;&#23427;&#36873;&#25321;&#19968;&#20010;&#22270;&#20687;&#30340;&#23376;&#21306;&#22495;&#20316;&#20026;&#33258;&#30417;&#30563;&#25439;&#22833;&#20013;&#30340;&#27491;&#26679;&#26412;&#35270;&#35282;&#12290;&#20854;&#22522;&#26412;&#20551;&#35774;&#26159;&#22312;&#32473;&#23450;&#22270;&#20687;&#30340;&#38543;&#26426;&#35009;&#21098;&#21644;&#32553;&#25918;&#30340;&#21306;&#22495;&#20013;&#65292;&#20851;&#20110;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#26377;&#20849;&#21516;&#20449;&#24687;&#65292;&#19988;&#25152;&#23398;&#30340;&#34920;&#31034;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#20123;&#20449;&#24687;&#12290;&#36825;&#31181;&#20551;&#35774;&#22312;ImageNet&#31561;&#26377;&#24456;&#22823;&#12289;&#20301;&#20110;&#20013;&#22830;&#30340;&#23545;&#35937;&#30340;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#22522;&#26412;&#28385;&#36275;&#65292;&#20294;&#22312;OpenImages&#25110;COCO&#31561;&#26356;&#36148;&#36817;&#30495;&#23454;&#19990;&#30028;&#38750;&#31579;&#36873;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#36890;&#24120;&#26377;&#22810;&#20010;&#23567;&#23545;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#22522;&#20110;&#38543;&#26426;&#35009;&#21098;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#25928;&#26524;&#24456;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#38543;&#26426;&#35009;&#21098;&#20013;&#30340;&#19968;&#20010;&#25110;&#20004;&#20010;&#26367;&#25442;&#20026;&#36890;&#36807;&#23545;&#35937;&#25552;&#35758;&#31639;&#27861;&#33719;&#24471;&#30340;&#35009;&#21098;&#12290;&#36825;&#40723;&#21169;&#27169;&#22411;&#21516;&#26102;&#23398;&#20064;&#23545;&#35937;&#24863;&#30693;&#33021;&#21147;&#21644;&#29305;&#24449;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
A core component of the recent success of self-supervised learning is cropping data augmentation, which selects sub-regions of an image to be used as positive views in the self-supervised loss. The underlying assumption is that randomly cropped and resized regions of a given image share information about the objects of interest, which the learned representation will capture. This assumption is mostly satisfied in datasets such as ImageNet where there is a large, centered object, which is highly likely to be present in random crops of the full image. However, in other datasets such as OpenImages or COCO, which are more representative of real world uncurated data, there are typically multiple small objects in an image. In this work, we show that self-supervised learning based on the usual random cropping performs poorly on such datasets. We propose replacing one or both of the random crops with crops obtained from an object proposal algorithm. This encourages the model to learn both obje
&lt;/p&gt;</description></item><item><title>UBnormal&#26159;&#19968;&#20010;&#26032;&#30340;&#30417;&#30563;&#24335;&#24320;&#25918;&#38598;Benchmark&#65292;&#23427;&#24341;&#20837;&#20102;&#29992;&#20110;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#30340;&#20687;&#32032;&#32423;&#24322;&#24120;&#20107;&#20214;&#27880;&#37322;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#20840;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#12290;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#19968;&#20010;&#26631;&#20934;&#30340;&#24320;&#25918;&#38598;&#38382;&#39064;&#65292;&#19982;&#20854;&#20182;&#22522;&#20934;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2111.08644</link><description>&lt;p&gt;
UBnormal: &#30417;&#30563;&#24335;&#24320;&#25918;&#38598;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#22522;&#20934;(arXiv:2111.08644v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection. (arXiv:2111.08644v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08644
&lt;/p&gt;
&lt;p&gt;
UBnormal&#26159;&#19968;&#20010;&#26032;&#30340;&#30417;&#30563;&#24335;&#24320;&#25918;&#38598;Benchmark&#65292;&#23427;&#24341;&#20837;&#20102;&#29992;&#20110;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#30340;&#20687;&#32032;&#32423;&#24322;&#24120;&#20107;&#20214;&#27880;&#37322;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#20840;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#12290;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#19968;&#20010;&#26631;&#20934;&#30340;&#24320;&#25918;&#38598;&#38382;&#39064;&#65292;&#19982;&#20854;&#20182;&#22522;&#20934;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#20013;&#26816;&#27979;&#24322;&#24120;&#20107;&#20214;&#36890;&#24120;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#21333;&#31867;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#20013;&#35757;&#32451;&#35270;&#39057;&#20165;&#21253;&#21547;&#27491;&#24120;&#20107;&#20214;&#65292;&#32780;&#27979;&#35797;&#35270;&#39057;&#21253;&#21547;&#27491;&#24120;&#21644;&#24322;&#24120;&#20107;&#20214;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#24320;&#25918;&#38598;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#19968;&#20123;&#30740;&#31350;&#23558;&#24322;&#24120;&#26816;&#27979;&#19982;&#21160;&#20316;&#35782;&#21035;&#30456;&#21516;&#12290;&#36825;&#26159;&#19968;&#20010;&#38381;&#38598;&#22330;&#26223;&#65292;&#26080;&#27861;&#27979;&#35797;&#31995;&#32479;&#26816;&#27979;&#26032;&#24322;&#24120;&#31867;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UBnormal&#65292;&#19968;&#20010;&#30001;&#22810;&#20010;&#34394;&#25311;&#22330;&#26223;&#32452;&#25104;&#30340;&#26032;&#30340;&#30417;&#30563;&#24335;&#24320;&#25918;&#38598;Benchmark&#65292;&#29992;&#20110;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#26102;&#24341;&#20837;&#20102;&#20687;&#32032;&#32423;&#24322;&#24120;&#20107;&#20214;&#27880;&#37322;&#65292;&#39318;&#27425;&#20351;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#21487;&#20197;&#20351;&#29992;&#20840;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#20026;&#20102;&#20445;&#25345;&#20856;&#22411;&#30340;&#24320;&#25918;&#38598;&#20844;&#24335;&#65292;&#25105;&#20204;&#30830;&#20445;&#22312;&#35270;&#39057;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#21512;&#20013;&#21253;&#21547;&#19981;&#30456;&#20132;&#30340;&#24322;&#24120;&#31867;&#22411;&#38598;&#21512;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;UBnormal&#26159;&#31532;&#19968;&#20010;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting abnormal events in video is commonly framed as a one-class classification task, where training videos contain only normal events, while test videos encompass both normal and abnormal events. In this scenario, anomaly detection is an open-set problem. However, some studies assimilate anomaly detection to action recognition. This is a closed-set scenario that fails to test the capability of systems at detecting new anomaly types. To this end, we propose UBnormal, a new supervised open-set benchmark composed of multiple virtual scenes for video anomaly detection. Unlike existing data sets, we introduce abnormal events annotated at the pixel level at training time, for the first time enabling the use of fully-supervised learning methods for abnormal event detection. To preserve the typical open-set formulation, we make sure to include disjoint sets of anomaly types in our training and test collections of videos. To our knowledge, UBnormal is the first video anomaly detection benc
&lt;/p&gt;</description></item><item><title>DeLag&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#25628;&#32034;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#20197;&#35786;&#26029;&#26381;&#21153;&#31995;&#32479;&#20013;&#30340;&#24310;&#36831;&#38477;&#32423;&#27169;&#24335;&#65292;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#32858;&#31867;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2110.11155</link><description>&lt;p&gt;
DeLag&#65306;&#20351;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#22686;&#24378;&#26381;&#21153;&#31995;&#32479;&#20013;&#24310;&#36831;&#38477;&#32423;&#27169;&#24335;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeLag: Using Multi-Objective Optimization to Enhance the Detection of Latency Degradation Patterns in Service-based Systems. (arXiv:2110.11155v4 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.11155
&lt;/p&gt;
&lt;p&gt;
DeLag&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#25628;&#32034;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#20197;&#35786;&#26029;&#26381;&#21153;&#31995;&#32479;&#20013;&#30340;&#24310;&#36831;&#38477;&#32423;&#27169;&#24335;&#65292;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#26381;&#21153;&#31995;&#32479;&#20013;&#65292;&#24615;&#33021;&#35843;&#35797;&#26159;&#19968;&#39033;&#22522;&#26412;&#27963;&#21160;&#12290;&#24615;&#33021;&#38382;&#39064;&#30340;&#35786;&#26029;&#36890;&#24120;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#22240;&#20026;&#38656;&#35201;&#35814;&#32454;&#26816;&#26597;&#22823;&#37327;&#30340;&#36319;&#36394;&#21644;&#24615;&#33021;&#25351;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DeLag&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#35786;&#26029;&#26381;&#21153;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;DeLag&#36890;&#36807;&#35782;&#21035;&#26174;&#31034;&#20986;&#28508;&#22312;&#30456;&#20851;&#24615;&#33021;&#38382;&#39064;&#30151;&#29366;&#30340;&#35831;&#27714;&#23376;&#38598;&#26469;&#26816;&#27979;&#28508;&#22312;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#30151;&#29366;&#20026;&#24310;&#36831;&#38477;&#32423;&#27169;&#24335;&#12290;DeLag&#22312;&#21516;&#26102;&#20248;&#21270;&#31934;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#24310;&#36831;&#30456;&#20284;&#24230;&#30340;&#22522;&#30784;&#19978;&#25628;&#32034;&#22810;&#20010;&#24310;&#36831;&#38477;&#32423;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DeLag&#27604;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21644;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#32858;&#31867;&#31639;&#27861;&#25552;&#20379;&#26356;&#22909;&#21644;&#26356;&#31283;&#23450;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance debugging in production is a fundamental activity in modern service-based systems. The diagnosis of performance issues is often time-consuming, since it requires thorough inspection of large volumes of traces and performance indices. In this paper we present DeLag, a novel automated search-based approach for diagnosing performance issues in service-based systems. DeLag identifies subsets of requests that show, in the combination of their Remote Procedure Call execution times, symptoms of potentially relevant performance issues. We call such symptoms Latency Degradation Patterns. DeLag simultaneously searches for multiple latency degradation patterns while optimizing precision, recall and latency dissimilarity. Experimentation on 700 datasets of requests generated from two microservice-based systems shows that our approach provides better and more stable effectiveness than three state-of-the-art approaches and general purpose machine learning clustering algorithms. DeLag is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#29109;&#27491;&#21017;&#21270;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#21452;&#19979;&#38477;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#26368;&#20248;&#38388;&#38553;&#21644;&#32422;&#26463;&#36829;&#21453;&#26041;&#38754;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#20026;$\widetilde{\mathcal {O}}(1/T)$&#12290;</title><link>http://arxiv.org/abs/2110.08923</link><description>&lt;p&gt;
&#24102;&#29109;&#27491;&#21017;&#21270;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#21452;&#37325;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Dual Approach to Constrained Markov Decision Processes with Entropy Regularization. (arXiv:2110.08923v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#29109;&#27491;&#21017;&#21270;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#21452;&#19979;&#38477;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#26368;&#20248;&#38388;&#38553;&#21644;&#32422;&#26463;&#36829;&#21453;&#26041;&#38754;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#20026;$\widetilde{\mathcal {O}}(1/T)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;soft-max&#21442;&#25968;&#21270;&#19979;&#30340;&#24102;&#29109;&#27491;&#21017;&#21270;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(CMDPs)&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#26088;&#22312;&#26368;&#22823;&#21270;&#29109;&#27491;&#21017;&#21270;&#20215;&#20540;&#20989;&#25968;&#30340;&#21516;&#26102;&#28385;&#36275;&#23545;&#26399;&#26395;&#24635;&#25928;&#29992;&#30340;&#32422;&#26463;&#12290;&#36890;&#36807;&#21033;&#29992;&#29109;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#23427;&#30340;Lagrangian&#23545;&#20598;&#20989;&#25968;&#26159;&#24179;&#28369;&#30340;&#65292;Lagrangian&#23545;&#20598;&#38388;&#38553;&#21487;&#20197;&#20998;&#35299;&#20026;&#21407;&#22987;&#26368;&#20248;&#38388;&#38553;&#21644;&#32422;&#26463;&#36829;&#35268;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#21452;&#19979;&#38477;&#26041;&#27861;&#29992;&#20110;&#24102;&#29109;&#27491;&#21017;&#21270;&#30340;CMDPs&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29109;&#27491;&#21017;&#21270;CMDPs&#30340;&#26368;&#20248;&#38388;&#38553;&#21644;&#32422;&#26463;&#36829;&#21453;&#26041;&#38754;&#36798;&#21040;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;$\widetilde{\mathcal {O}}(1/T)$&#12290;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#21333;&#32422;&#26463;CMDPs&#30340;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study entropy-regularized constrained Markov decision processes (CMDPs) under the soft-max parameterization, in which an agent aims to maximize the entropy-regularized value function while satisfying constraints on the expected total utility. By leveraging the entropy regularization, our theoretical analysis shows that its Lagrangian dual function is smooth and the Lagrangian duality gap can be decomposed into the primal optimality gap and the constraint violation. Furthermore, we propose an accelerated dual-descent method for entropy-regularized CMDPs. We prove that our method achieves the global convergence rate $\widetilde{\mathcal{O}}(1/T)$ for both the optimality gap and the constraint violation for entropy-regularized CMDPs. A discussion about a linear convergence rate for CMDPs with a single constraint is also provided.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31639;&#23376;&#65292;&#21487;&#20197;&#23398;&#20064;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#21487;&#20197;&#36924;&#36817;&#32473;&#23450;&#30340;&#38750;&#32447;&#24615;&#36830;&#32493;&#31639;&#23376;&#19988;&#31163;&#25955;&#19981;&#21464;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#22235;&#31867;&#39640;&#25928;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#31639;&#23376;&#12290;&#36825;&#39033;&#25216;&#26415;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#26159;&#23398;&#20064;&#20195;&#29702;&#26144;&#23556;f&#12290;</title><link>http://arxiv.org/abs/2108.08481</link><description>&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#65306;&#23398;&#20064;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Neural Operator: Learning Maps Between Function Spaces. (arXiv:2108.08481v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.08481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31639;&#23376;&#65292;&#21487;&#20197;&#23398;&#20064;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#21487;&#20197;&#36924;&#36817;&#32473;&#23450;&#30340;&#38750;&#32447;&#24615;&#36830;&#32493;&#31639;&#23376;&#19988;&#31163;&#25955;&#19981;&#21464;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#22235;&#31867;&#39640;&#25928;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#31639;&#23376;&#12290;&#36825;&#39033;&#25216;&#26415;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#26159;&#23398;&#20064;&#20195;&#29702;&#26144;&#23556;f&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#26377;&#38480;&#32500;&#27431;&#27663;&#31354;&#38388;&#25110;&#26377;&#38480;&#38598;&#20043;&#38388;&#30340;&#26144;&#23556;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#65292;&#31216;&#20026;&#31070;&#32463;&#31639;&#23376;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#26144;&#23556;&#30340;&#31639;&#23376;&#12290;&#25105;&#20204;&#23558;&#31070;&#32463;&#31639;&#23376;&#26500;&#24314;&#20026;&#32447;&#24615;&#31215;&#20998;&#31639;&#23376;&#21644;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#31639;&#23376;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#34920;&#26126;&#23427;&#21487;&#20197;&#36924;&#36817;&#20219;&#20309;&#32473;&#23450;&#30340;&#38750;&#32447;&#24615;&#36830;&#32493;&#31639;&#23376;&#12290;&#25152;&#25552;&#20986;&#30340;&#31070;&#32463;&#31639;&#23376;&#20063;&#26159;&#31163;&#25955;&#19981;&#21464;&#30340;&#65292;&#21363;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#22522;&#30784;&#20989;&#25968;&#31354;&#38388;&#31163;&#25955;&#21270;&#20043;&#38388;&#20849;&#20139;&#30456;&#21516;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22235;&#31867;&#39640;&#25928;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#21363;&#22270;&#31070;&#32463;&#31639;&#23376;&#12289;&#22810;&#26497;&#22270;&#31070;&#32463;&#31639;&#23376;&#12289;&#20302;&#31209;&#31070;&#32463;&#31639;&#23376;&#21644;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#12290;&#31070;&#32463;&#31639;&#23376;&#30340;&#37325;&#35201;&#24212;&#29992;&#20043;&#19968;&#26159;&#23398;&#20064;&#20195;&#29702;&#26144;&#23556;f&#12290;
&lt;/p&gt;
&lt;p&gt;
The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#20284;&#28982;&#20551;&#35774;&#19979;&#30340;&#39057;&#29575;&#23398;&#27966;&#25512;&#26029;&#65288;LF2I&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#32463;&#20856;&#32479;&#35745;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#26500;&#24314;&#20855;&#26377;&#27491;&#30830;&#26465;&#20214;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;&#30340;&#23454;&#29992;&#31243;&#24207;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#22312;&#21253;&#25324;&#23431;&#23449;&#23398;&#21442;&#25968;&#25512;&#26029;&#22312;&#20869;&#30340;&#22810;&#20010;&#20363;&#23376;&#20013;&#37117;&#23454;&#29616;&#20102;&#35206;&#30422;&#24615;&#36136;&#24471;&#21040;&#22823;&#24133;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2107.03920</link><description>&lt;p&gt;
&#26080;&#20284;&#28982;&#20551;&#35774;&#19979;&#22522;&#20110;&#39057;&#29575;&#23398;&#27966;&#25512;&#26029;&#65306;&#20855;&#26377;&#27491;&#30830;&#26465;&#20214;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Likelihood-Free Frequentist Inference: Confidence Sets with Correct Conditional Coverage. (arXiv:2107.03920v6 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.03920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#20284;&#28982;&#20551;&#35774;&#19979;&#30340;&#39057;&#29575;&#23398;&#27966;&#25512;&#26029;&#65288;LF2I&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#32463;&#20856;&#32479;&#35745;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#26500;&#24314;&#20855;&#26377;&#27491;&#30830;&#26465;&#20214;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;&#30340;&#23454;&#29992;&#31243;&#24207;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#22312;&#21253;&#25324;&#23431;&#23449;&#23398;&#21442;&#25968;&#25512;&#26029;&#22312;&#20869;&#30340;&#22810;&#20010;&#20363;&#23376;&#20013;&#37117;&#23454;&#29616;&#20102;&#35206;&#30422;&#24615;&#36136;&#24471;&#21040;&#22823;&#24133;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#37117;&#24191;&#27867;&#20351;&#29992;&#35745;&#31639;&#26426;&#27169;&#25311;&#22120;&#20197;&#38544;&#21547;&#22797;&#26434;&#31995;&#32479;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;&#20256;&#32479;&#30340;&#32479;&#35745;&#26041;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;&#36825;&#20123;&#31216;&#20026;&#26080;&#20284;&#28982;&#20551;&#35774;&#19979;&#25512;&#26029;&#65288;LFI&#65289;&#30340;&#24773;&#20917;&#65292;&#23588;&#20854;&#26159;&#22312;&#28176;&#36817;&#21644;&#20302;&#32500;&#30340;&#26465;&#20214;&#19979;&#12290;&#34429;&#28982;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#24402;&#19968;&#21270;&#27969;&#65292;&#24050;&#32463;&#38761;&#26032;&#20102;LFI&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#23481;&#37327;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#33021;&#20026;&#23567;&#26679;&#26412;&#22823;&#23567;&#20135;&#29983;&#20855;&#26377;&#27491;&#30830;&#26465;&#20214;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#32463;&#20856;&#32479;&#35745;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#65288;i&#65289;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#21517;&#20041;&#35206;&#30422;&#30340;&#20869;&#26364;&#21306;&#38388;&#24314;&#35774;&#30340;&#23454;&#29992;&#31243;&#24207;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20272;&#35745;&#25972;&#20010;&#21442;&#25968;&#31354;&#38388;&#30340;&#26465;&#20214;&#35206;&#30422;&#30340;&#35786;&#26029;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#31216;&#20026;&#26080;&#20284;&#28982;&#20551;&#35774;&#19979;&#30340;&#39057;&#29575;&#23398;&#27966;&#25512;&#26029;&#65288;LF2I&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20351;&#29992;&#23450;&#20041;&#27979;&#35797;&#32479;&#35745;&#37327;&#30340;&#20219;&#20309;&#26041;&#27861;&#65292;&#22914;&#20284;&#28982;&#27604;&#65292;&#22240;&#27492;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#21512;&#25104;&#21644;&#23454;&#38469;&#30340;&#20363;&#23376;&#65292;&#21253;&#25324;&#23431;&#23449;&#23398;&#21442;&#25968;&#25512;&#26029;&#65292;&#24182;&#35777;&#26126;&#19982;&#29616;&#26377;&#30340;LFI&#26041;&#27861;&#30456;&#27604;&#65292;&#35206;&#30422;&#24615;&#36136;&#24471;&#21040;&#20102;&#22823;&#24133;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many areas of science make extensive use of computer simulators that implicitly encode likelihood functions of complex systems. Classical statistical methods are poorly suited for these so-called likelihood-free inference (LFI) settings, particularly outside asymptotic and low-dimensional regimes. Although new machine learning methods, such as normalizing flows, have revolutionized the sample efficiency and capacity of LFI methods, it remains an open question whether they produce confidence sets with correct conditional coverage for small sample sizes. This paper unifies classical statistics with modern machine learning to present (i) a practical procedure for the Neyman construction of confidence sets with finite-sample guarantees of nominal coverage, and (ii) diagnostics that estimate conditional coverage over the entire parameter space. We refer to our framework as likelihood-free frequentist inference (LF2I). Any method that defines a test statistic, like the likelihood ratio, can 
&lt;/p&gt;</description></item></channel></rss>