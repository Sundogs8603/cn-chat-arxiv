<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#30340;&#24179;&#34913;K-Means&#32858;&#31867;&#30340;&#27010;&#29575;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#26368;&#20248;&#35299;&#26469;&#35745;&#31639;&#26657;&#20934;&#21518;&#39564;&#27010;&#29575;&#65292;&#23454;&#29616;&#22312;D-Wave AQC&#19978;&#35782;&#21035;&#27169;&#31946;&#35299;&#20915;&#26041;&#26696;&#21644;&#25968;&#25454;&#28857;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.12153</link><description>&lt;p&gt;
&#20351;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#30340;&#24179;&#34913;K-Means&#30340;&#27010;&#29575;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing. (arXiv:2310.12153v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12153
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#30340;&#24179;&#34913;K-Means&#32858;&#31867;&#30340;&#27010;&#29575;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#26368;&#20248;&#35299;&#26469;&#35745;&#31639;&#26657;&#20934;&#21518;&#39564;&#27010;&#29575;&#65292;&#23454;&#29616;&#22312;D-Wave AQC&#19978;&#35782;&#21035;&#27169;&#31946;&#35299;&#20915;&#26041;&#26696;&#21644;&#25968;&#25454;&#28857;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#65288;AQC&#65289;&#26159;&#19968;&#31181;&#26377;&#26395;&#29992;&#20110;&#31163;&#25955;&#19988;&#36890;&#24120;&#20026;NP&#22256;&#38590;&#20248;&#21270;&#38382;&#39064;&#30340;&#37327;&#23376;&#35745;&#31639;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;AQC&#20801;&#35768;&#23454;&#29616;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#65292;&#36825;&#20419;&#20351;&#20102;&#20026;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#24320;&#21457;&#37327;&#23376;&#34920;&#31034;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#38656;&#35201;&#20174;&#22122;&#22768;AQC&#36827;&#34892;&#22810;&#27425;&#27979;&#37327;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#20165;&#21033;&#29992;&#26368;&#20339;&#27979;&#37327;&#65292;&#20002;&#24323;&#20102;&#20854;&#20182;&#27979;&#37327;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#36827;&#34892;&#27010;&#29575;&#24179;&#34913;k-means&#32858;&#31867;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#38750;&#26368;&#20248;&#35299;&#26469;&#35745;&#31639;&#26657;&#20934;&#21518;&#39564;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#35745;&#31639;&#25104;&#26412;&#24456;&#20302;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#27169;&#31946;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#25968;&#25454;&#28857;&#65292;&#25105;&#20204;&#22312;D-Wave AQC&#19978;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adiabatic quantum computing (AQC) is a promising quantum computing approach for discrete and often NP-hard optimization problems. Current AQCs allow to implement problems of research interest, which has sparked the development of quantum representations for many machine learning and computer vision tasks. Despite requiring multiple measurements from the noisy AQC, current approaches only utilize the best measurement, discarding information contained in the remaining ones. In this work, we explore the potential of using this information for probabilistic balanced k-means clustering. Instead of discarding non-optimal solutions, we propose to use them to compute calibrated posterior probabilities with little additional compute cost. This allows us to identify ambiguous solutions and data points, which we demonstrate on a D-Wave AQC on synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;(HPO)&#65292;&#25105;&#20204;&#22312;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26032;&#27169;&#22411;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#30340;&#31574;&#30053;&#65292;&#20197;&#23547;&#25214;&#26356;&#20844;&#24179;&#21644;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#38024;&#23545;&#20934;&#30830;&#24615;&#36827;&#34892;&#20248;&#21270;&#21487;&#33021;&#20250;&#23548;&#33268;&#20844;&#24179;&#24615;&#30340;&#38477;&#20302;&#65292;&#22240;&#27492;&#38656;&#35201;&#21516;&#26102;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12145</link><description>&lt;p&gt;
&#36890;&#36807;NAS&#23454;&#29616;&#26356;&#20844;&#24179;&#21644;&#20934;&#30830;&#30340;&#34920;&#26684;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fairer and More Accurate Tabular Models Through NAS. (arXiv:2310.12145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12145
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;(HPO)&#65292;&#25105;&#20204;&#22312;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26032;&#27169;&#22411;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#30340;&#31574;&#30053;&#65292;&#20197;&#23547;&#25214;&#26356;&#20844;&#24179;&#21644;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#38024;&#23545;&#20934;&#30830;&#24615;&#36827;&#34892;&#20248;&#21270;&#21487;&#33021;&#20250;&#23548;&#33268;&#20844;&#24179;&#24615;&#30340;&#38477;&#20302;&#65292;&#22240;&#27492;&#38656;&#35201;&#21516;&#26102;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#36890;&#36807;&#31639;&#27861;&#20351;&#24471;&#34920;&#26684;&#25968;&#25454;&#30340;&#27169;&#22411;&#26356;&#21152;&#20844;&#24179;&#19968;&#30452;&#26159;&#30740;&#31350;&#30340;&#35838;&#39064;&#12290;&#29616;&#26377;&#30340;&#25216;&#26415;&#36890;&#24120;&#38024;&#23545;&#23384;&#22312;&#19981;&#21487;&#21462;&#30340;&#32467;&#26524;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#21464;&#25968;&#25454;&#30340;&#25668;&#20837;&#26041;&#24335;&#12289;&#27169;&#22411;&#26435;&#37325;&#25110;&#36755;&#20986;&#22788;&#29702;&#26041;&#24335;&#26469;&#20462;&#22797;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26032;&#30340;&#31574;&#30053;&#65292;&#22312;&#21435;&#20559;&#36807;&#31243;&#20013;&#32771;&#34385;&#26356;&#26032;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#36229;&#21442;&#25968;&#65292;&#20197;&#25214;&#21040;&#19968;&#20010;&#20174;&#19968;&#24320;&#22987;&#22312;&#39044;&#27979;&#32467;&#26524;&#19978;&#26356;&#22909;&#30340;&#26032;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;(HPO)&#24212;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;MLP&#12289;ResNet&#21644;FT-Transformer&#31561;&#19981;&#21516;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#31354;&#38388;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25506;&#32034;&#65292;&#23637;&#31034;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#25351;&#26631;&#23545;&#36229;&#21442;&#25968;&#32452;&#21512;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20165;&#38024;&#23545;&#20934;&#30830;&#24615;&#36827;&#34892;&#20248;&#21270;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#20844;&#24179;&#24615;&#30340;&#38477;&#20302;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#21516;&#26102;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making models algorithmically fairer in tabular data has been long studied, with techniques typically oriented towards fixes which usually take a neural model with an undesirable outcome and make changes to how the data are ingested, what the model weights are, or how outputs are processed. We employ an emergent and different strategy where we consider updating the model's architecture and training hyperparameters to find an entirely new model with better outcomes from the beginning of the debiasing procedure. In this work, we propose using multi-objective Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO) in the first application to the very challenging domain of tabular data. We conduct extensive exploration of architectural and hyperparameter spaces (MLP, ResNet, and FT-Transformer) across diverse datasets, demonstrating the dependence of accuracy and fairness metrics of model predictions on hyperparameter combinations. We show that models optimized solely for ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#31232;&#30095;&#22238;&#24402;&#20648;&#23618;&#35745;&#31639;&#26426;&#26469;&#35782;&#21035;&#21160;&#24577;&#37329;&#34701;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#26500;&#21270;&#30697;&#38453;&#36924;&#36817;&#21644;&#31232;&#30095;&#26368;&#23567;&#20108;&#20056;&#26041;&#27861;&#30830;&#23450;&#36755;&#20986;&#32806;&#21512;&#30697;&#38453;&#30340;&#36817;&#20284;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#34920;&#31034;&#24314;&#31435;&#23545;&#24212;&#20110;&#32473;&#23450;&#37329;&#34701;&#31995;&#32479;&#20013;&#36882;&#24402;&#32467;&#26500;&#30340;&#22238;&#24402;&#27169;&#22411;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#21160;&#24577;&#37329;&#34701;&#21644;&#32463;&#27982;&#36807;&#31243;&#30340;&#36817;&#20284;&#35782;&#21035;&#21644;&#39044;&#27979;&#27169;&#25311;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12144</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#22238;&#24402;&#20648;&#23618;&#35745;&#31639;&#26426;&#35782;&#21035;&#21160;&#24577;&#37329;&#34701;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Dynamic financial processes identification using sparse regressive reservoir computers. (arXiv:2310.12144v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#31232;&#30095;&#22238;&#24402;&#20648;&#23618;&#35745;&#31639;&#26426;&#26469;&#35782;&#21035;&#21160;&#24577;&#37329;&#34701;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#26500;&#21270;&#30697;&#38453;&#36924;&#36817;&#21644;&#31232;&#30095;&#26368;&#23567;&#20108;&#20056;&#26041;&#27861;&#30830;&#23450;&#36755;&#20986;&#32806;&#21512;&#30697;&#38453;&#30340;&#36817;&#20284;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#34920;&#31034;&#24314;&#31435;&#23545;&#24212;&#20110;&#32473;&#23450;&#37329;&#34701;&#31995;&#32479;&#20013;&#36882;&#24402;&#32467;&#26500;&#30340;&#22238;&#24402;&#27169;&#22411;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#21160;&#24577;&#37329;&#34701;&#21644;&#32463;&#27982;&#36807;&#31243;&#30340;&#36817;&#20284;&#35782;&#21035;&#21644;&#39044;&#27979;&#27169;&#25311;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32467;&#26500;&#21270;&#30697;&#38453;&#36924;&#36817;&#29702;&#35770;&#30340;&#20851;&#38190;&#21457;&#29616;&#65292;&#20197;&#21450;&#20854;&#22312;&#21160;&#24577;&#37329;&#34701;&#36807;&#31243;&#30340;&#22238;&#24402;&#34920;&#31034;&#20013;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#28041;&#21450;&#20174;&#37329;&#34701;&#25110;&#32463;&#27982;&#31995;&#32479;&#20013;&#25552;&#21462;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36890;&#29992;&#38750;&#32447;&#24615;&#26102;&#24310;&#23884;&#20837;&#30340;&#20840;&#38754;&#26041;&#27861;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#31232;&#30095;&#26368;&#23567;&#20108;&#20056;&#21644;&#32467;&#26500;&#21270;&#30697;&#38453;&#36924;&#36817;&#26041;&#27861;&#65292;&#26469;&#35782;&#21035;&#36755;&#20986;&#32806;&#21512;&#30697;&#38453;&#30340;&#36817;&#20284;&#34920;&#31034;&#12290;&#36825;&#20123;&#34920;&#31034;&#22312;&#24314;&#31435;&#23545;&#24212;&#20110;&#32473;&#23450;&#37329;&#34701;&#31995;&#32479;&#20013;&#36882;&#24402;&#32467;&#26500;&#30340;&#22238;&#24402;&#27169;&#22411;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#21033;&#29992;&#19978;&#36848;&#25216;&#26415;&#30340;&#21407;&#22411;&#31639;&#27861;&#12290;&#36890;&#36807;&#22312;&#21160;&#24577;&#37329;&#34701;&#21644;&#32463;&#27982;&#36807;&#31243;&#30340;&#36817;&#20284;&#35782;&#21035;&#21644;&#39044;&#27979;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#65292;&#21253;&#25324;&#21487;&#33021;&#34920;&#29616;&#20986;&#28151;&#27788;&#34892;&#20026;&#30340;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this document, we present key findings in structured matrix approximation theory, with applications to the regressive representation of dynamic financial processes. Initially, we explore a comprehensive approach involving generic nonlinear time delay embedding for time series data extracted from a financial or economic system under examination. Subsequently, we employ sparse least-squares and structured matrix approximation methods to discern approximate representations of the output coupling matrices. These representations play a pivotal role in establishing the regressive models corresponding to the recursive structures inherent in a given financial system. The document further introduces prototypical algorithms that leverage the aforementioned techniques. These algorithms are demonstrated through applications in approximate identification and predictive simulation of dynamic financial and economic processes, encompassing scenarios that may or may not exhibit chaotic behavior.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26597;&#30475;&#27010;&#24565;&#30340;&#30697;&#38453;&#32479;&#35745;&#37327;&#65292;&#29983;&#25104;&#19968;&#20010;&#27010;&#24565;&#30340;&#20855;&#20307;&#34920;&#31034;&#25110;&#31614;&#21517;&#65292;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#27010;&#24565;&#20043;&#38388;&#30340;&#32467;&#26500;&#24182;&#36882;&#24402;&#20135;&#29983;&#26356;&#39640;&#32423;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#21487;&#20197;&#36890;&#36807;&#27010;&#24565;&#30340;&#31614;&#21517;&#26469;&#25214;&#21040;&#30456;&#20851;&#30340;&#20849;&#21516;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12143</link><description>&lt;p&gt;
&#31616;&#21333;&#26426;&#21046;&#29992;&#20110;&#34920;&#31034;&#12289;&#32034;&#24341;&#21644;&#25805;&#20316;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Simple Mechanisms for Representing, Indexing and Manipulating Concepts. (arXiv:2310.12143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12143
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26597;&#30475;&#27010;&#24565;&#30340;&#30697;&#38453;&#32479;&#35745;&#37327;&#65292;&#29983;&#25104;&#19968;&#20010;&#27010;&#24565;&#30340;&#20855;&#20307;&#34920;&#31034;&#25110;&#31614;&#21517;&#65292;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#27010;&#24565;&#20043;&#38388;&#30340;&#32467;&#26500;&#24182;&#36882;&#24402;&#20135;&#29983;&#26356;&#39640;&#32423;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#21487;&#20197;&#36890;&#36807;&#27010;&#24565;&#30340;&#31614;&#21517;&#26469;&#25214;&#21040;&#30456;&#20851;&#30340;&#20849;&#21516;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#36890;&#24120;&#36890;&#36807;&#20998;&#31867;&#22120;&#23398;&#20064;&#27010;&#24565;&#65292;&#36825;&#28041;&#21450;&#35774;&#32622;&#27169;&#22411;&#24182;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#23427;&#20197;&#36866;&#24212;&#20855;&#26377;&#26631;&#35760;&#27010;&#24565;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#19968;&#20010;&#19981;&#21516;&#30340;&#35266;&#28857;&#65292;&#21363;&#21487;&#20197;&#36890;&#36807;&#26597;&#30475;&#27010;&#24565;&#30340;&#30697;&#38453;&#30697;&#38453;&#32479;&#35745;&#37327;&#26469;&#29983;&#25104;&#27010;&#24565;&#30340;&#20855;&#20307;&#34920;&#31034;&#25110;&#31614;&#21517;&#12290;&#36825;&#20123;&#31614;&#21517;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#19968;&#32452;&#27010;&#24565;&#30340;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20174;&#36825;&#20123;&#31614;&#21517;&#20013;&#23398;&#20064;&#35813;&#32467;&#26500;&#26469;&#36882;&#24402;&#22320;&#20135;&#29983;&#26356;&#39640;&#32423;&#30340;&#27010;&#24565;&#12290;&#24403;&#27010;&#24565;"&#30456;&#20132;"&#26102;&#65292;&#27010;&#24565;&#30340;&#31614;&#21517;&#21487;&#20197;&#29992;&#20110;&#22312;&#19968;&#20123;&#30456;&#20851;&#30340;"&#30456;&#20132;"&#27010;&#24565;&#20013;&#25214;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#20027;&#39064;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#29992;&#20110;&#20445;&#25345;&#19968;&#20010;&#27010;&#24565;&#23383;&#20856;&#65292;&#20197;&#20415;&#36755;&#20837;&#33021;&#22815;&#27491;&#30830;&#35782;&#21035;&#24182;&#34987;&#36335;&#30001;&#21040;&#19982;&#36755;&#20837;&#30340;(&#28508;&#22312;)&#29983;&#25104;&#30456;&#20851;&#30340;&#27010;&#24565;&#38598;&#21512;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep networks typically learn concepts via classifiers, which involves setting up a model and training it via gradient descent to fit the concept-labeled data. We will argue instead that learning a concept could be done by looking at its moment statistics matrix to generate a concrete representation or signature of that concept. These signatures can be used to discover structure across the set of concepts and could recursively produce higher-level concepts by learning this structure from those signatures. When the concepts are `intersected', signatures of the concepts can be used to find a common theme across a number of related `intersected' concepts. This process could be used to keep a dictionary of concepts so that inputs could correctly identify and be routed to the set of concepts involved in the (latent) generation of the input.
&lt;/p&gt;</description></item><item><title>DiagrammerGPT&#26159;&#19968;&#20010;&#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;T2I&#27169;&#22411;&#22312;&#22270;&#34920;&#29983;&#25104;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.12128</link><description>&lt;p&gt;
DiagrammerGPT: &#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning. (arXiv:2310.12128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12128
&lt;/p&gt;
&lt;p&gt;
DiagrammerGPT&#26159;&#19968;&#20010;&#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;T2I&#27169;&#22411;&#22312;&#22270;&#34920;&#29983;&#25104;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#20351;&#29992;T2I&#27169;&#22411;&#29983;&#25104;&#22270;&#34920;&#26041;&#38754;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#22270;&#34920;&#26159;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#20016;&#23500;&#21644;&#31354;&#38388;&#22797;&#26434;&#30340;&#21487;&#35270;&#21270;&#26469;&#35299;&#37322;&#20449;&#24687;&#30340;&#31526;&#21495;/&#31034;&#24847;&#24615;&#34920;&#31034;&#65288;&#20363;&#22914;&#65292;&#19968;&#31181;&#23494;&#38598;&#30340;&#30456;&#20851;&#23545;&#35937;&#12289;&#25991;&#26412;&#26631;&#31614;&#12289;&#26041;&#21521;&#31661;&#22836;&#12289;&#36830;&#25509;&#32447;&#31561;&#32452;&#21512;&#65289;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;T2I&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#34920;&#26102;&#32463;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35768;&#22810;&#23545;&#35937;&#36890;&#36807;&#22797;&#26434;&#30340;&#20851;&#31995;&#65288;&#22914;&#31661;&#22836;/&#32447;&#65289;&#23494;&#38598;&#36830;&#25509;&#26102;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#23545;&#35937;&#24067;&#23616;&#25511;&#21046;&#65292;&#24182;&#19988;&#32463;&#24120;&#19981;&#33021;&#28210;&#26579;&#20986;&#21487;&#29702;&#35299;&#30340;&#25991;&#26412;&#26631;&#31614;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiagrammerGPT&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;&#22270;&#34920;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;LLM&#65288;&#22914;GPT-4&#65289;&#30340;&#24067;&#23616;&#24341;&#23548;&#33021;&#21147;&#26469;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#8220;&#22270;&#34920;&#35268;&#21010;&#8221;&#65288;&#22312;&#19968;&#20010;&#35268;&#21010;&#26041;&#26696;&#20013;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) generation has seen significant growth over the past few years. Despite this, there has been little work on generating diagrams with T2I models. A diagram is a symbolic/schematic representation that explains information using structurally rich and spatially complex visualizations (e.g., a dense combination of related objects, text labels, directional arrows, connection lines, etc.). Existing state-of-the-art T2I models often fail at diagram generation because they lack fine-grained object layout control when many objects are densely connected via complex relations such as arrows/lines and also often fail to render comprehensible text labels. To address this gap, we present DiagrammerGPT, a novel two-stage text-to-diagram generation framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4) to generate more accurate open-domain, open-platform diagrams. In the first stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a planne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#20197;&#21450;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#30740;&#31350;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#22312;&#40664;&#35748;&#20026;&#30007;&#24615;&#32763;&#35793;&#19978;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65292;&#21516;&#26102;&#24573;&#35270;&#20102;&#25351;&#31034;&#32844;&#19994;&#24615;&#21035;&#30340;&#20195;&#35789;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#34892;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.12127</link><description>&lt;p&gt;
&#20195;&#35789;&#25925;&#20107;&#65306;&#21487;&#35299;&#37322;&#24615;&#25351;&#23548;&#19979;&#30340;&#20844;&#24179;&#25351;&#23548;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation. (arXiv:2310.12127v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#20197;&#21450;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#30740;&#31350;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#22312;&#40664;&#35748;&#20026;&#30007;&#24615;&#32763;&#35793;&#19978;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65292;&#21516;&#26102;&#24573;&#35270;&#20102;&#25351;&#31034;&#32844;&#19994;&#24615;&#21035;&#30340;&#20195;&#35789;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#34892;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#21487;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#35299;&#20915;&#38382;&#39064;&#65292;&#20854;&#20013;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#26159;&#19968;&#20010;&#31361;&#20986;&#30340;&#29992;&#20363;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#22312;&#26631;&#20934;&#24615;&#33021;&#22522;&#20934;&#19978;&#65292;&#24573;&#35270;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#20844;&#24179;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;&#22312;MT&#20013;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24615;&#21035;&#38169;&#35823;&#30340;&#32763;&#35793;&#65292;&#20174;&#32780;&#23548;&#33268;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#30340;&#25345;&#32493;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#36825;&#20123;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#26159;&#21542;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#20197;&#21450;&#22914;&#20309;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#20174;&#33521;&#25991;&#21040;&#24503;&#25991;&#21644;&#35199;&#29677;&#29273;&#25991;&#30340;WinoMT&#35821;&#26009;&#24211;&#19978;&#35745;&#31639;&#24050;&#24314;&#31435;&#30340;&#24615;&#21035;&#20559;&#35265;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#40664;&#35748;&#20026;&#30007;&#24615;&#23624;&#20174;&#32763;&#35793;&#65292;&#29978;&#33267;&#24573;&#35270;&#22899;&#24615;&#32844;&#19994;&#21051;&#26495;&#21360;&#35937;&#12290;&#25509;&#19979;&#26469;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#27169;&#22411;&#31995;&#32479;&#24615;&#22320;&#24573;&#35270;&#25351;&#31034;&#30446;&#26631;&#32844;&#19994;&#24615;&#21035;&#30340;&#20195;&#35789;&#22312;&#21516;&#26102;&#24615;&#21035;&#38169;&#35823;&#30340;&#32763;&#35793;&#20013;&#12290;&#26368;&#21518;&#65292;&#26681;&#25454;&#21487;&#35299;&#37322;&#24615;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24615;&#21035;&#20559;&#35265;&#32531;&#35299;&#30340;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;MT&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent instruction fine-tuned models can solve multiple NLP tasks when prompted to do so, with machine translation (MT) being a prominent use case. However, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. In MT, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. In this work, we address this gap by investigating whether and to what extent such models exhibit gender bias in machine translation and how we can mitigate it. Concretely, we compute established gender bias metrics on the WinoMT corpus from English to German and Spanish. We discover that IFT models default to male-inflected translations, even disregarding female occupational stereotypes. Next, using interpretability methods, we unveil that models systematically overlook the pronoun indicating the gender of a target occupation in misgendered translations. Finally, based on 
&lt;/p&gt;</description></item><item><title>SHARCS&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#23485;&#24230;&#23376;&#32593;&#32476;&#36827;&#34892;&#36335;&#30001;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#25512;&#29702;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#24182;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12126</link><description>&lt;p&gt;
SHARCS&#65306;&#36890;&#36807;&#21160;&#24577;&#23485;&#24230;&#23376;&#32593;&#32476;&#36827;&#34892;&#36335;&#30001;&#30340;&#39640;&#25928;Transformer
&lt;/p&gt;
&lt;p&gt;
SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks. (arXiv:2310.12126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12126
&lt;/p&gt;
&lt;p&gt;
SHARCS&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#23485;&#24230;&#23376;&#32593;&#32476;&#36827;&#34892;&#36335;&#30001;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#25512;&#29702;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#24182;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SHARCS&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#25512;&#29702;&#65292;&#32771;&#34385;&#21040;&#36755;&#20837;&#26679;&#26412;&#30340;&#38590;&#24230;&#12290;SHARCS&#21487;&#20197;&#22312;&#20219;&#20309;Transformer&#32593;&#32476;&#19978;&#35757;&#32451;&#19968;&#20010;&#36335;&#30001;&#22120;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23558;&#19981;&#21516;&#26679;&#26412;&#25351;&#21521;&#20855;&#26377;&#19981;&#21516;&#23485;&#24230;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;1&#65289;&#22312;&#20934;&#30830;&#24615;&#19982;FLOPs&#20043;&#38388;&#65292;SHARCS&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#25110;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#27599;&#20010;&#26679;&#26412;&#33258;&#36866;&#24212;&#25512;&#29702;&#26041;&#27861;&#65307;&#65288;2&#65289;SHARCS&#22312;&#19981;&#21516;&#26550;&#26500;&#20043;&#38388;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#20197;&#24212;&#29992;&#20110;&#21387;&#32553;&#21644;&#39640;&#25928;&#30340;Transformer&#32534;&#30721;&#22120;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#25928;&#29575;&#65307;&#65288;3&#65289;SHARCS&#21487;&#20197;&#25552;&#20379;2&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SHARCS for adaptive inference that takes into account the hardness of input samples. SHARCS can train a router on any transformer network, enabling the model to direct different samples to sub-networks with varying widths. Our experiments demonstrate that: (1) SHARCS outperforms or complements existing per-sample adaptive inference methods across various classification tasks in terms of accuracy vs. FLOPs; (2) SHARCS generalizes across different architectures and can be even applied to compressed and efficient transformer encoders to further improve their efficiency; (3) SHARCS can provide a 2 times inference speed up at an insignificant drop in accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#39044;&#27979;&#24739;&#26377;&#31934;&#31070;&#30142;&#30149;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#26426;&#26862;&#26519;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33647;&#29289;&#22788;&#26041;&#23588;&#20854;&#26159;&#30827;&#37240;&#21527;&#21857;&#23545;&#39044;&#27979;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.12121</link><description>&lt;p&gt;
&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#33258;&#21160;&#39044;&#27979;&#24739;&#26377;&#31934;&#31070;&#30142;&#30149;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;
&lt;/p&gt;
&lt;p&gt;
Automatic prediction of mortality in patients with mental illness using electronic health records. (arXiv:2310.12121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#39044;&#27979;&#24739;&#26377;&#31934;&#31070;&#30142;&#30149;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#26426;&#26862;&#26519;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33647;&#29289;&#22788;&#26041;&#23588;&#20854;&#26159;&#30827;&#37240;&#21527;&#21857;&#23545;&#39044;&#27979;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#31070;&#38556;&#30861;&#24433;&#21709;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#30340;&#29983;&#27963;&#65292;&#19981;&#20165;&#22952;&#30861;&#20182;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#65292;&#36824;&#26126;&#26174;&#32553;&#30701;&#20102;&#39044;&#26399;&#23551;&#21629;&#12290;&#26412;&#25991;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405; (EHR) &#20013;&#30340;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#24739;&#26377;&#31934;&#31070;&#35786;&#26029;&#30340;&#24739;&#32773;&#20013;&#39044;&#27979;&#27515;&#20129;&#29575;&#30340;&#25345;&#20037;&#25361;&#25112;&#12290;&#20174;&#20247;&#25152;&#21608;&#30693;&#30340;&#20020;&#24202; MIMIC-III &#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20102;&#24739;&#26377;&#31934;&#31070;&#30142;&#30149;&#35786;&#26029;&#30340;&#24739;&#32773;&#30340;&#25968;&#25454;&#65292;&#21033;&#29992;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#22788;&#26041;&#20449;&#24687;&#21644;&#31243;&#24207;&#20449;&#24687;&#12290;&#20351;&#29992;&#20102;&#22235;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861; (&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;K&#36817;&#37051;) &#65292;&#32467;&#26524;&#34920;&#26126;&#38543;&#26426;&#26862;&#26519;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;AUC&#24471;&#20998;&#20026;0.911&#12290;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#26174;&#31034;&#33647;&#29289;&#22788;&#26041;&#65292;&#29305;&#21035;&#26159;&#30827;&#37240;&#21527;&#21857;&#65292;&#22312;&#39044;&#27979;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#39044;&#27979;30&#22825;&#20869;&#30340;&#27515;&#20129;&#29575;&#65292;&#38543;&#21518;&#36827;&#34892;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mental disorders impact the lives of millions of people globally, not only impeding their day-to-day lives but also markedly reducing life expectancy. This paper addresses the persistent challenge of predicting mortality in patients with mental diagnoses using predictive machine-learning models with electronic health records (EHR). Data from patients with mental disease diagnoses were extracted from the well-known clinical MIMIC-III data set utilizing demographic, prescription, and procedural information. Four machine learning algorithms (Logistic Regression, Random Forest, Support Vector Machine, and K-Nearest Neighbors) were used, with results indicating that Random Forest and Support Vector Machine models outperformed others, with AUC scores of 0.911. Feature importance analysis revealed that drug prescriptions, particularly Morphine Sulfate, play a pivotal role in prediction. We applied a variety of machine learning algorithms to predict 30-day mortality followed by feature importa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;MMD&#36317;&#31163;&#21644;&#32463;&#20856;&#30340;drop and relearn&#21407;&#29702;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#38543;&#26426;&#26862;&#26519;&#20013;&#26816;&#27979;&#24433;&#21709;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#37327;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#31454;&#20105;&#23545;&#25163;&#12290;</title><link>http://arxiv.org/abs/2310.12115</link><description>&lt;p&gt;
&#22522;&#20110;MMD&#30340;&#20998;&#24067;&#38543;&#26426;&#26862;&#26519;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
MMD-based Variable Importance for Distributional Random Forest. (arXiv:2310.12115v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;MMD&#36317;&#31163;&#21644;&#32463;&#20856;&#30340;drop and relearn&#21407;&#29702;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#38543;&#26426;&#26862;&#26519;&#20013;&#26816;&#27979;&#24433;&#21709;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#37327;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#31454;&#20105;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#38543;&#26426;&#26862;&#26519;&#65288;DRF&#65289;&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#22522;&#20110;&#26862;&#26519;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#32473;&#23450;&#36755;&#20837;&#21464;&#37327;&#30340;&#22810;&#20803;&#36755;&#20986;&#30340;&#20840;&#26465;&#20214;&#20998;&#24067;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#30340;drop and relearn&#21407;&#29702;&#21644;MMD&#36317;&#31163;&#30340;DRF&#21464;&#37327;&#37325;&#35201;&#24615;&#31639;&#27861;&#12290;&#20256;&#32479;&#30340;&#37325;&#35201;&#24615;&#24230;&#37327;&#21482;&#33021;&#21457;&#29616;&#23545;&#36755;&#20986;&#22343;&#20540;&#26377;&#24433;&#21709;&#30340;&#21464;&#37327;&#65292;&#32780;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#26356;&#26222;&#36941;&#22320;&#21457;&#29616;&#24433;&#21709;&#36755;&#20986;&#20998;&#24067;&#30340;&#21464;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24341;&#20837;&#30340;&#37325;&#35201;&#24615;&#24230;&#37327;&#26159;&#19968;&#33268;&#30340;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#27169;&#25311;&#25968;&#25454;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#24182;&#19988;&#36229;&#36234;&#20102;&#31454;&#20105;&#23545;&#25163;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#36882;&#24402;&#29305;&#24449;&#28040;&#38500;&#39640;&#25928;&#22320;&#36873;&#25321;&#21464;&#37327;&#65292;&#22240;&#27492;&#21487;&#20197;&#25552;&#20379;&#23567;&#22411;&#21464;&#37327;&#38598;&#21512;&#26469;&#26500;&#24314;&#20934;&#30830;&#30340;&#26465;&#20214;&#36755;&#20986;&#20998;&#24067;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional Random Forest (DRF) is a flexible forest-based method to estimate the full conditional distribution of a multivariate output of interest given input variables. In this article, we introduce a variable importance algorithm for DRFs, based on the well-established drop and relearn principle and MMD distance. While traditional importance measures only detect variables with an influence on the output mean, our algorithm detects variables impacting the output distribution more generally. We show that the introduced importance measure is consistent, exhibits high empirical performance on both real and simulated data, and outperforms competitors. In particular, our algorithm is highly efficient to select variables through recursive feature elimination, and can therefore provide small sets of variables to build accurate estimates of conditional output distributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23454;&#35777;&#38544;&#31169;&#38450;&#24481;&#20013;&#21442;&#32771;&#25968;&#25454;&#30340;&#20316;&#29992;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#38450;&#24481;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#25928;&#29992;&#21644;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.12112</link><description>&lt;p&gt;
&#19968;&#21017;&#35686;&#31034;&#25925;&#20107;&#65306;&#20851;&#20110;&#21442;&#32771;&#25968;&#25454;&#22312;&#23454;&#35777;&#38544;&#31169;&#38450;&#24481;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Cautionary Tale: On the Role of Reference Data in Empirical Privacy Defenses. (arXiv:2310.12112v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23454;&#35777;&#38544;&#31169;&#38450;&#24481;&#20013;&#21442;&#32771;&#25968;&#25454;&#30340;&#20316;&#29992;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#38450;&#24481;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#25928;&#29992;&#21644;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#23454;&#35777;&#38544;&#31169;&#38450;&#24481;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#22312;&#19981;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#27700;&#24179;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38450;&#24481;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#26041;&#27861;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#21442;&#32771;&#25968;&#25454;&#65292;&#21442;&#32771;&#25968;&#25454;&#25351;&#30340;&#26159;&#26469;&#33258;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#65288;&#25110;&#31867;&#20284;&#65289;&#22522;&#30784;&#20998;&#24067;&#30340;&#38468;&#21152;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#21442;&#32771;&#25968;&#25454;&#30340;&#20351;&#29992;&#24456;&#26222;&#36941;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#23545;&#20110;&#23450;&#20041;&#21644;&#35780;&#20272;&#21442;&#32771;&#25968;&#25454;&#38544;&#31169;&#30456;&#24403;&#20445;&#23432;&#12290;&#30001;&#20110;&#27169;&#22411;&#25928;&#29992;&#21644;/&#25110;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#25552;&#21319;&#21487;&#33021;&#20197;&#29306;&#29298;&#21442;&#32771;&#25968;&#25454;&#38544;&#31169;&#20026;&#20195;&#20215;&#65292;&#22240;&#27492;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#36825;&#19977;&#20010;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#20043;&#21069;&#30340;&#20316;&#21697;&#20013;&#21442;&#32771;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21450;&#20854;&#38544;&#31169;&#22788;&#29702;&#24773;&#20917;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#20844;&#24179;&#27604;&#36739;&#38450;&#24481;&#26041;&#27861;&#26469;&#35828;&#21442;&#32771;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#38450;&#24481;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#27169;&#22411;&#25928;&#29992;&#21644;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the realm of privacy-preserving machine learning, empirical privacy defenses have been proposed as a solution to achieve satisfactory levels of training data privacy without a significant drop in model utility. Most existing defenses against membership inference attacks assume access to reference data, defined as an additional dataset coming from the same (or a similar) underlying distribution as training data. Despite the common use of reference data, previous works are notably reticent about defining and evaluating reference data privacy. As gains in model utility and/or training data privacy may come at the expense of reference data privacy, it is essential that all three aspects are duly considered. In this paper, we first examine the availability of reference data and its privacy treatment in previous works and demonstrate its necessity for fairly comparing defenses. Second, we propose a baseline defense that enables the utility-privacy tradeoff with respect to both trainin
&lt;/p&gt;</description></item><item><title>Monarch Mixer (M2) is a new architecture that uses sub-quadratic primitive to scale along both sequence length and model dimension, achieving high hardware efficiency and matching the performance of existing models with fewer parameters.</title><link>http://arxiv.org/abs/2310.12109</link><description>&lt;p&gt;
Monarch Mixer: &#19968;&#20010;&#22522;&#20110;&#23376;&#20108;&#27425;&#24191;&#20041;&#30697;&#38453;&#30456;&#20056;&#30340;&#31616;&#21333;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture. (arXiv:2310.12109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12109
&lt;/p&gt;
&lt;p&gt;
Monarch Mixer (M2) is a new architecture that uses sub-quadratic primitive to scale along both sequence length and model dimension, achieving high hardware efficiency and matching the performance of existing models with fewer parameters.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#24207;&#21015;&#38271;&#24230;&#21644;&#27169;&#22411;&#32500;&#24230;&#19978;&#30340;&#25193;&#23637;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20197;&#36798;&#21040;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26550;&#26500;&#22914;Transformer&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#30340;&#25193;&#23637;&#37117;&#26159;&#20108;&#27425;&#30340;&#12290;&#25105;&#20204;&#38382;&#65306;&#26159;&#21542;&#26377;&#19968;&#31181;&#24615;&#33021;&#33391;&#22909;&#30340;&#26550;&#26500;&#21487;&#20197;&#22312;&#24207;&#21015;&#38271;&#24230;&#21644;&#27169;&#22411;&#32500;&#24230;&#19978;&#20855;&#26377;&#23376;&#20108;&#27425;&#30340;&#25193;&#23637;&#24615;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;Monarch Mixer (M2)&#65292;&#19968;&#31181;&#20351;&#29992;&#30456;&#21516;&#23376;&#20108;&#27425;&#21407;&#35821;&#30340;&#26032;&#26550;&#26500;&#65292;&#35813;&#21407;&#35821;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#20855;&#26377;&#34920;&#36798;&#21147;&#30340;&#32467;&#26500;&#21270;&#30697;&#38453;&#65292;&#21487;&#20197;&#25429;&#25417;&#35768;&#22810;&#32447;&#24615;&#21464;&#25442;&#65292;&#22312;GPU&#19978;&#20855;&#26377;&#39640;&#30828;&#20214;&#25928;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#23376;&#20108;&#27425;&#30340;&#25193;&#23637;&#24615;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#39046;&#22495;&#25506;&#32034;&#20102;M2&#30340;&#24615;&#33021;&#65306;&#38750;&#22240;&#26524;BERT&#27169;&#24335;&#30340;&#35821;&#35328;&#24314;&#27169;&#65292;ViT&#27169;&#24335;&#30340;&#22270;&#20687;&#20998;&#31867;&#21644;&#22240;&#26524;GPT&#27169;&#24335;&#30340;&#35821;&#35328;&#24314;&#27169;&#12290;&#23545;&#20110;&#38750;&#22240;&#26524;BERT&#27169;&#24335;&#30340;&#24314;&#27169;&#65292;M2&#22312;GLUE&#36136;&#37327;&#19978;&#19982;BERT-base&#21644;BERT-large&#30456;&#21305;&#37197;&#65292;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;&#22810;&#36798;27%
&lt;/p&gt;
&lt;p&gt;
Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#20102;&#20132;&#26131;&#32773;&#38388;&#30340;&#32463;&#32426;&#20132;&#26131;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#20449;&#24687;&#25259;&#38706;&#24773;&#20917;&#19979;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12107</link><description>&lt;p&gt;
&#12298;&#19968;&#20010;&#20851;&#20110;&#32463;&#32426;&#20132;&#26131;&#30340;&#22312;&#32447;&#23398;&#20064;&#29702;&#35770;&#12299;
&lt;/p&gt;
&lt;p&gt;
An Online Learning Theory of Brokerage. (arXiv:2310.12107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12107
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#20102;&#20132;&#26131;&#32773;&#38388;&#30340;&#32463;&#32426;&#20132;&#26131;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#20449;&#24687;&#25259;&#38706;&#24773;&#20917;&#19979;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20132;&#26131;&#32773;&#38388;&#30340;&#32463;&#32426;&#20132;&#26131;&#12290;&#22312;&#27599;&#19968;&#36718;t&#65292;&#20004;&#20010;&#20132;&#26131;&#32773;&#24102;&#30528;&#20182;&#20204;&#30340;&#31169;&#20154;&#20272;&#20540;&#21040;&#36798;&#65292;&#32463;&#32426;&#20154;&#25552;&#20986;&#19968;&#20010;&#20132;&#26131;&#20215;&#26684;&#12290;&#19982;&#22312;&#32447;&#23398;&#20064;&#25991;&#29486;&#20013;&#24050;&#32463;&#30740;&#31350;&#30340;&#20854;&#20182;&#21452;&#36793;&#20132;&#26131;&#38382;&#39064;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#27809;&#26377;&#25351;&#23450;&#20080;&#26041;&#21644;&#21334;&#26041;&#35282;&#33394;&#30340;&#24773;&#20917;&#65306;&#27599;&#20010;&#20132;&#26131;&#32773;&#23558;&#26681;&#25454;&#21830;&#21697;&#30340;&#24403;&#21069;&#20215;&#26684;&#23581;&#35797;&#20080;&#20837;&#25110;&#21334;&#20986;&#12290;&#25105;&#20204;&#20551;&#35774;&#20132;&#26131;&#32773;&#30340;&#20272;&#20540;&#26159;&#20174;&#19968;&#20010;&#22266;&#23450;&#20294;&#26410;&#30693;&#30340;&#20998;&#24067;&#20013;&#29420;&#31435;&#21516;&#20998;&#24067;&#22320;&#25277;&#21462;&#30340;&#12290;&#22914;&#26524;&#20998;&#24067;&#30340;&#23494;&#24230;&#21463;&#21040;&#26576;&#20010;&#24120;&#25968;M&#30340;&#38480;&#21046;&#65292;&#21017;&#23545;&#20110;&#20219;&#20309;&#26102;&#38388;&#27573;T&#65306;$\bullet$&#22914;&#26524;&#22312;&#27599;&#27425;&#20132;&#20114;&#20043;&#21518;&#25581;&#31034;&#20132;&#26131;&#32773;&#30340;&#20272;&#20540;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36798;&#21040;&#36951;&#25022;$M \log T$&#30340;&#31639;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#20010;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#65292;&#38500;&#20102;&#24120;&#25968;&#22240;&#23376;&#12290;$\bullet$&#22914;&#26524;&#20165;&#22312;&#27599;&#27425;&#20132;&#20114;&#20043;&#21518;&#25581;&#31034;&#20182;&#20204;&#30340;&#24895;&#24847;&#25353;&#29031;&#25552;&#35758;&#30340;&#20215;&#26684;&#21334;&#20986;&#25110;&#20080;&#20837;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36798;&#21040;&#36951;&#25022;$\sqrt{M T}$&#30340;&#31639;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#20010;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate brokerage between traders from an online learning perspective. At any round $t$, two traders arrive with their private valuations, and the broker proposes a trading price. Unlike other bilateral trade problems already studied in the online learning literature, we focus on the case where there are no designated buyer and seller roles: each trader will attempt to either buy or sell depending on the current price of the good.  We assume the agents' valuations are drawn i.i.d. from a fixed but unknown distribution. If the distribution admits a density bounded by some constant $M$, then, for any time horizon $T$:  $\bullet$ If the agents' valuations are revealed after each interaction, we provide an algorithm achieving regret $M \log T$ and show this rate is optimal, up to constant factors.  $\bullet$ If only their willingness to sell or buy at the proposed price is revealed after each interaction, we provide an algorithm achieving regret $\sqrt{M T}$ and show this rate is op
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65288;AdaLink&#65289;&#65292;&#36890;&#36807;&#21482;&#35843;&#25972;&#27169;&#22411;&#30340;&#22806;&#37096;&#21442;&#25968;&#32780;&#20445;&#25345;&#20869;&#37096;&#32467;&#26500;&#19981;&#21464;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#24314;&#27169;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#21644;&#37096;&#32626;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.12100</link><description>&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#33258;&#36866;&#24212;&#65306;&#38754;&#21521;&#22810;&#27169;&#24577;&#24314;&#27169;&#30340;&#36755;&#20837;&#20013;&#24515;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling. (arXiv:2310.12100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12100
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65288;AdaLink&#65289;&#65292;&#36890;&#36807;&#21482;&#35843;&#25972;&#27169;&#22411;&#30340;&#22806;&#37096;&#21442;&#25968;&#32780;&#20445;&#25345;&#20869;&#37096;&#32467;&#26500;&#19981;&#21464;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#24314;&#27169;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#21644;&#37096;&#32626;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#36890;&#36807;&#23558;&#21442;&#25968;&#25968;&#37327;&#20174;O&#65288;10^9&#65289;&#25193;&#23637;&#21040;O&#65288;10^{12}&#65289;&#29978;&#33267;&#26356;&#39640;&#27700;&#24179;&#65292;&#23637;&#29616;&#20986;&#22312;&#24191;&#27867;&#20219;&#21153;&#19978;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#36825;&#26679;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#20351;&#24471;&#22312;&#32473;&#23450;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#23436;&#20840;&#19987;&#19994;&#21270;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#21644;&#37096;&#32626;&#25104;&#20026;&#19981;&#21487;&#33021;&#12290;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25104;&#20026;&#24212;&#23545;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#36866;&#24212;&#21644;&#26381;&#21153;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#23558;PEFT&#25216;&#26415;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;&#20405;&#20837;&#24335;&#21644;&#38750;&#20405;&#20837;&#24335;&#12290;&#20405;&#20837;&#24335;PEFT&#25216;&#26415;&#30452;&#25509;&#25913;&#21464;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#12290;&#34429;&#28982;&#26356;&#28789;&#27963;&#65292;&#20294;&#22312;&#35757;&#32451;&#21644;&#26381;&#21153;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#22797;&#26434;&#24615;&#12290;&#38750;&#20405;&#20837;&#24335;PEFT&#25216;&#26415;&#20445;&#25345;&#20869;&#37096;&#32467;&#26500;&#19981;&#21464;&#65292;&#21482;&#35843;&#25972;&#27169;&#22411;&#30340;&#22806;&#37096;&#21442;&#25968;&#65292;&#22914;&#36755;&#20837;&#30340;&#23884;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;AdaLink&#25551;&#36848;&#20026;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;PEFT&#25216;&#26415;&#65292;&#19982;SoTA&#20405;&#20837;&#24335;PEFT&#65288;LoRA&#65289;&#21644;&#23436;&#25972;&#27169;&#22411;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) and vision language models (VLMs) demonstrate excellent performance on a wide range of tasks by scaling up parameter counts from O(10^9) to O(10^{12}) levels and further beyond. These large scales make it impossible to adapt and deploy fully specialized models given a task of interest. Parameter-efficient fine-tuning (PEFT) emerges as a promising direction to tackle the adaptation and serving challenges for such large models. We categorize PEFT techniques into two types: intrusive and non-intrusive. Intrusive PEFT techniques directly change a model's internal architecture. Though more flexible, they introduce significant complexities for training and serving. Non-intrusive PEFT techniques leave the internal architecture unchanged and only adapt model-external parameters, such as embeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT technique that achieves competitive performance compared to SoTA intrusive PEFT (LoRA) and full model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#28145;&#24230;&#23398;&#20064;&#38477;&#38454;&#27169;&#22411;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25193;&#23637;&#23545;&#30001;&#38543;&#26426;&#22330;&#21442;&#25968;&#21270;&#30340;PDE&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.12095</link><description>&lt;p&gt;
&#20851;&#20110;&#30001;&#38543;&#26426;&#22330;&#21442;&#25968;&#21270;&#30340;PDE&#30340;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#32500;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the latent dimension of deep autoencoders for reduced order modeling of PDEs parametrized by random fields. (arXiv:2310.12095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#28145;&#24230;&#23398;&#20064;&#38477;&#38454;&#27169;&#22411;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25193;&#23637;&#23545;&#30001;&#38543;&#26426;&#22330;&#21442;&#25968;&#21270;&#30340;PDE&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35774;&#35745;&#37096;&#20998;&#24494;&#20998;&#26041;&#31243;&#30340;&#38477;&#38454;&#27169;&#22411;&#20013;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#38590;&#20197;&#22788;&#29702;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#28145;&#24230;&#33258;&#21160;&#32534;&#30721;&#22120;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26497;&#20026;&#28789;&#27963;&#30340;&#24037;&#20855;&#26469;&#38477;&#20302;&#32473;&#23450;&#38382;&#39064;&#30340;&#32500;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#38543;&#26426;&#23383;&#27573;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#38382;&#39064;&#20013;&#65292;&#24403;&#21069;&#23545;&#28145;&#24230;&#23398;&#20064;&#38477;&#38454;&#27169;&#22411;&#30340;&#29702;&#35299;&#20027;&#35201;&#22522;&#20110;&#32463;&#39564;&#35777;&#25454;&#65292;&#20854;&#29702;&#35770;&#20998;&#26512;&#30446;&#21069;&#20165;&#38480;&#20110;&#20381;&#36182;&#26377;&#38480;&#20010;&#65288;&#30830;&#23450;&#24615;&#65289;&#21442;&#25968;&#30340;PDE&#24773;&#20917;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#25552;&#20379;&#19968;&#20123;&#29702;&#35770;&#35770;&#35777;&#65292;&#25193;&#23637;&#29616;&#26377;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning is having a remarkable impact on the design of Reduced Order Models (ROMs) for Partial Differential Equations (PDEs), where it is exploited as a powerful tool for tackling complex problems for which classical methods might fail. In this respect, deep autoencoders play a fundamental role, as they provide an extremely flexible tool for reducing the dimensionality of a given problem by leveraging on the nonlinear capabilities of neural networks. Indeed, starting from this paradigm, several successful approaches have already been developed, which are here referred to as Deep Learning-based ROMs (DL-ROMs). Nevertheless, when it comes to stochastic problems parameterized by random fields, the current understanding of DL-ROMs is mostly based on empirical evidence: in fact, their theoretical analysis is currently limited to the case of PDEs depending on a finite number of (deterministic) parameters. The purpose of this work is to extend the existing literature by providing some t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.12086</link><description>&lt;p&gt;
&#21457;&#29616;&#22622;&#22764;&#20043;&#27468;&#65306;&#21487;&#38752;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT/GPT-4&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#22312;&#32593;&#32476;&#24179;&#21488;&#19978;&#23384;&#22312;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#30340;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#37319;&#29992;&#12290;&#23545;&#30001;LLMs&#20135;&#29983;&#30340;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#19981;&#20165;&#28041;&#21450;&#23545;&#22522;&#26412;&#20107;&#23454;&#30340;&#21028;&#26029;&#65292;&#36824;&#21253;&#25324;&#23545;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#22810;&#36339;&#31561;&#65289;&#20013;&#20986;&#29616;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FactCHD&#65292;&#19968;&#31181;&#20026;LLMs&#31934;&#24515;&#35774;&#35745;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#12290;&#20316;&#20026;&#22312;&#8220;&#26597;&#35810;-&#21709;&#24212;&#8221;&#19978;&#19979;&#25991;&#20013;&#35780;&#20272;&#20107;&#23454;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#37319;&#29992;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20107;&#23454;&#27169;&#24335;&#65292;&#22914;&#22522;&#26412;&#20107;&#23454;&#65292;&#22810;&#36339;&#65292;&#27604;&#36739;&#21644;&#38598;&#21512;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20934;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#65292;&#20174;&#32780;&#20415;&#20110;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#25935;&#24863;&#24615;&#20998;&#26512;&#30830;&#23450;&#20102;&#20195;&#35874;&#25104;&#26412;&#20272;&#31639;&#30340;&#20851;&#38190;&#21442;&#25968;&#21644;&#36755;&#20837;&#21464;&#37327;&#12290;&#36890;&#36807;&#20998;&#26512;&#21151;&#29575;&#30456;&#20851;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#21644;&#20351;&#29992;&#19981;&#21516;&#36755;&#20837;&#29305;&#24449;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#25581;&#31034;&#20102;&#20195;&#35874;&#33021;&#37327;&#27169;&#22411;&#30340;&#36129;&#29486;&#25104;&#20998;&#12290;</title><link>http://arxiv.org/abs/2310.12083</link><description>&lt;p&gt;
&#22312;&#27493;&#24577;&#20013;&#20272;&#35745;&#20195;&#35874;&#25104;&#26412;&#30340;&#20195;&#35874;&#33021;&#37327;&#27169;&#22411;&#30340;&#36129;&#29486;&#25104;&#20998;
&lt;/p&gt;
&lt;p&gt;
Contributing Components of Metabolic Energy Models to Metabolic Cost Estimations in Gait. (arXiv:2310.12083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#25935;&#24863;&#24615;&#20998;&#26512;&#30830;&#23450;&#20102;&#20195;&#35874;&#25104;&#26412;&#20272;&#31639;&#30340;&#20851;&#38190;&#21442;&#25968;&#21644;&#36755;&#20837;&#21464;&#37327;&#12290;&#36890;&#36807;&#20998;&#26512;&#21151;&#29575;&#30456;&#20851;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#21644;&#20351;&#29992;&#19981;&#21516;&#36755;&#20837;&#29305;&#24449;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#25581;&#31034;&#20102;&#20195;&#35874;&#33021;&#37327;&#27169;&#22411;&#30340;&#36129;&#29486;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20195;&#35874;&#25104;&#26412;&#26159;&#24433;&#21709;&#20154;&#31867;&#27493;&#24577;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#65292;&#25105;&#20204;&#24076;&#26395;&#21152;&#28145;&#23545;&#20195;&#35874;&#33021;&#37327;&#28040;&#32791;&#27169;&#22411;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30830;&#23450;&#20102;&#24433;&#21709;&#20934;&#30830;&#20272;&#35745;&#20195;&#35874;&#25104;&#26412;&#30340;&#21442;&#25968;&#21644;&#36755;&#20837;&#21464;&#37327;&#65292;&#20363;&#22914;&#32908;&#32905;&#25110;&#20851;&#33410;&#29366;&#24577;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#22312;&#33945;&#29305;&#21345;&#27931;&#25935;&#24863;&#24615;&#20998;&#26512;&#20013;&#25506;&#35752;&#20102;&#22235;&#20010;&#20195;&#35874;&#33021;&#37327;&#28040;&#32791;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#30340;&#25935;&#24863;&#24615;&#25351;&#26631;&#12289;&#29983;&#29702;&#29615;&#22659;&#21644;&#27493;&#24577;&#21608;&#26399;&#20013;&#24471;&#21040;&#30340;&#20195;&#35874;&#29575;&#20998;&#26512;&#27169;&#22411;&#21442;&#25968;&#12290;&#22312;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#20013;&#20855;&#26377;&#26368;&#39640;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#32452;&#21512;&#20195;&#34920;&#20102;&#19968;&#20010;&#20934;&#20248;&#21270;&#27169;&#22411;&#12290;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#20351;&#29992;&#19981;&#21516;&#36755;&#20837;&#29305;&#24449;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#26469;&#30740;&#31350;&#36755;&#20837;&#21442;&#25968;&#21644;&#21464;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;&#32467;&#26524;&#65306;&#21151;&#29575;&#30456;&#20851;&#21442;&#25968;&#22312;&#25935;&#24863;&#24615;&#20998;&#26512;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#36873;&#25321;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: As metabolic cost is a primary factor influencing humans' gait, we want to deepen our understanding of metabolic energy expenditure models. Therefore, this paper identifies the parameters and input variables, such as muscle or joint states, that contribute to accurate metabolic cost estimations. Methods: We explored the parameters of four metabolic energy expenditure models in a Monte Carlo sensitivity analysis. Then, we analysed the model parameters by their calculated sensitivity indices, physiological context, and the resulting metabolic rates during the gait cycle. The parameter combination with the highest accuracy in the Monte Carlo simulations represented a quasi-optimized model. In the second step, we investigated the importance of input parameters and variables by analysing the accuracy of neural networks trained with different input features. Results: Power-related parameters were most influential in the sensitivity analysis and the neural network-based feature sel
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#20855;&#26377;&#24418;&#29366;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#32553;&#25918;&#26497;&#38480;&#21487;&#20197;&#30001;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#26410;&#32463;&#24418;&#29366;&#22788;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20449;&#24687;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#26410;&#32463;&#24418;&#29366;&#22788;&#29702;&#30340;&#32593;&#32476;&#65292;&#21457;&#29616;&#23427;&#20204;&#20063;&#21487;&#20197;&#30001;&#31867;&#20284;&#30340;&#24494;&#20998;&#26041;&#31243;&#26469;&#25551;&#36848;&#65292;&#24182;&#32473;&#20986;&#20102;&#23427;&#20204;&#30340;&#19968;&#20123;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.12079</link><description>&lt;p&gt;
&#24418;&#29366;&#21644;&#38750;&#24418;&#29366;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#20998;&#26041;&#31243;&#32553;&#25918;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks. (arXiv:2310.12079v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12079
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#20855;&#26377;&#24418;&#29366;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#32553;&#25918;&#26497;&#38480;&#21487;&#20197;&#30001;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#26410;&#32463;&#24418;&#29366;&#22788;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20449;&#24687;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#26410;&#32463;&#24418;&#29366;&#22788;&#29702;&#30340;&#32593;&#32476;&#65292;&#21457;&#29616;&#23427;&#20204;&#20063;&#21487;&#20197;&#30001;&#31867;&#20284;&#30340;&#24494;&#20998;&#26041;&#31243;&#26469;&#25551;&#36848;&#65292;&#24182;&#32473;&#20986;&#20102;&#23427;&#20204;&#30340;&#19968;&#20123;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20855;&#26377;&#24418;&#29366;&#28608;&#27963;&#20989;&#25968;&#65288;&#21363;&#38543;&#30528;&#32593;&#32476;&#35268;&#27169;&#22686;&#22823;&#32780;&#32553;&#25918;&#30340;&#28608;&#27963;&#20989;&#25968;&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#20204;&#20855;&#26377;&#30001;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#30340;&#32553;&#25918;&#26497;&#38480;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#19981;&#39044;&#20808;&#21578;&#35785;&#25105;&#20204;&#20851;&#20110;&#8220;&#26222;&#36890;&#8221;&#38750;&#24418;&#29366;&#32593;&#32476;&#30340;&#20219;&#20309;&#20449;&#24687;&#65292;&#20854;&#20013;&#28608;&#27963;&#20989;&#25968;&#22312;&#32593;&#32476;&#35268;&#27169;&#22686;&#22823;&#26102;&#20445;&#25345;&#19981;&#21464;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#38750;&#24418;&#29366;&#32593;&#32476;&#25214;&#21040;&#20102;&#31867;&#20284;&#30340;&#22522;&#20110;&#24494;&#20998;&#26041;&#31243;&#30340;&#28176;&#36817;&#29305;&#24449;&#25551;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20197;&#19979;&#20004;&#31181;&#26550;&#26500;&#22312;&#21021;&#22987;&#21270;&#26102;&#20250;&#25910;&#25947;&#21040;&#30456;&#21516;&#30340;&#26080;&#38480;&#28145;&#24230;&#21644;&#23485;&#24230;&#26497;&#38480;&#65306;&#65288;i&#65289;&#24102;&#26377;&#27531;&#24046;&#20998;&#25903;&#19978;&#30340; $d^{-1/2}$ &#22240;&#23376;&#30340;&#20840;&#36830;&#25509; ResNet&#65292;&#20854;&#20013; $d$ &#26159;&#32593;&#32476;&#30340;&#28145;&#24230;&#65307;&#65288;ii&#65289;&#24102;&#26377;&#28145;&#24230; $d \ll$ &#23485;&#24230; $n$ &#21644;&#24418;&#29366; ReLU &#28608;&#27963;&#20989;&#25968; (activation) &#30340;&#22810;&#23618;&#24863;&#30693;&#26426; (MLP)&#65292;&#20197; $d^{-1/2}$ &#30340;&#36895;&#29575;&#12290;&#20854;&#27425;&#65292;&#23545;&#20110;&#21021;&#22987;&#21270;&#30340;&#38750;&#24418;&#29366; MLP&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#23618;&#38388;&#30456;&#20851;&#24615;&#30340;&#19968;&#38454;&#28176;&#36817;&#20462;&#27491;&#12290;&#29305;&#21035;&#22320;&#65292;&#22914;&#26524; $\rho_\ell$ &#26159;&#31532; $\ell$ &#23618;&#30340;&#30456;&#20851;&#24615;&#65292;&#21017;...
&lt;/p&gt;
&lt;p&gt;
Recent analyses of neural networks with shaped activations (i.e. the activation function is scaled as the network size grows) have led to scaling limits described by differential equations. However, these results do not a priori tell us anything about "ordinary" unshaped networks, where the activation is unchanged as the network size grows. In this article, we find similar differential equation based asymptotic characterization for two types of unshaped networks.  Firstly, we show that the following two architectures converge to the same infinite-depth-and-width limit at initialization: (i) a fully connected ResNet with a $d^{-1/2}$ factor on the residual branch, where $d$ is the network depth. (ii) a multilayer perceptron (MLP) with depth $d \ll$ width $n$ and shaped ReLU activation at rate $d^{-1/2}$.  Secondly, for an unshaped MLP at initialization, we derive the first order asymptotic correction to the layerwise correlation. In particular, if $\rho_\ell$ is the correlation at layer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#21482;&#26377;&#19968;&#27425;&#28436;&#31034;&#12289;&#19981;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#21644;&#27809;&#26377;&#20808;&#21069;&#30340;&#20219;&#21153;&#25110;&#23545;&#35937;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#27169;&#20223;&#23398;&#20064;&#24418;&#24335;&#21270;&#20026;&#36712;&#36857;&#20256;&#36755;&#21644;&#26410;&#30693;&#23545;&#35937;&#23039;&#24577;&#20272;&#35745;&#30340;&#32452;&#21512;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#20219;&#21153;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#26368;&#20808;&#36827;&#30340;&#26410;&#30693;&#23545;&#35937;&#23039;&#24577;&#20272;&#35745;&#22120;&#22312;&#19968;&#27425;&#24615;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#28145;&#20837;&#20998;&#26512;&#20102;&#30456;&#26426;&#26657;&#20934;&#12289;&#23039;&#24577;&#20272;&#35745;&#35823;&#24046;&#21644;&#31354;&#38388;&#27867;&#21270;&#23545;&#20219;&#21153;&#25104;&#21151;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.12077</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#27169;&#20223;&#23398;&#20064;&#65306;&#23039;&#24577;&#20272;&#35745;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
One-Shot Imitation Learning: A Pose Estimation Perspective. (arXiv:2310.12077v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#21482;&#26377;&#19968;&#27425;&#28436;&#31034;&#12289;&#19981;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#21644;&#27809;&#26377;&#20808;&#21069;&#30340;&#20219;&#21153;&#25110;&#23545;&#35937;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#27169;&#20223;&#23398;&#20064;&#24418;&#24335;&#21270;&#20026;&#36712;&#36857;&#20256;&#36755;&#21644;&#26410;&#30693;&#23545;&#35937;&#23039;&#24577;&#20272;&#35745;&#30340;&#32452;&#21512;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#20219;&#21153;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#26368;&#20808;&#36827;&#30340;&#26410;&#30693;&#23545;&#35937;&#23039;&#24577;&#20272;&#35745;&#22120;&#22312;&#19968;&#27425;&#24615;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#28145;&#20837;&#20998;&#26512;&#20102;&#30456;&#26426;&#26657;&#20934;&#12289;&#23039;&#24577;&#20272;&#35745;&#35823;&#24046;&#21644;&#31354;&#38388;&#27867;&#21270;&#23545;&#20219;&#21153;&#25104;&#21151;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65306;&#65288;1&#65289;&#20165;&#19968;&#27425;&#28436;&#31034;&#65292;&#65288;2&#65289;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#65288;3&#65289;&#27809;&#26377;&#20808;&#21069;&#30340;&#20219;&#21153;&#25110;&#23545;&#35937;&#30693;&#35782;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#22914;&#20309;&#23558;&#27169;&#20223;&#23398;&#20064;&#24418;&#24335;&#21270;&#20026;&#36712;&#36857;&#20256;&#36755;&#21644;&#26410;&#30693;&#23545;&#35937;&#23039;&#24577;&#20272;&#35745;&#30340;&#32452;&#21512;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;&#26410;&#30693;&#23545;&#35937;&#23039;&#24577;&#20272;&#35745;&#22120;&#22312;&#21313;&#20010;&#30495;&#23454;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;&#30456;&#26426;&#26657;&#20934;&#12289;&#23039;&#24577;&#20272;&#35745;&#35823;&#24046;&#21644;&#31354;&#38388;&#27867;&#21270;&#23545;&#20219;&#21153;&#25104;&#21151;&#29575;&#30340;&#24433;&#21709;&#12290;&#26377;&#20851;&#35270;&#39057;&#65292;&#35831;&#35775;&#38382;https://www.robot-learning.uk/pose-estimation-perspective&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study imitation learning under the challenging setting of: (1) only a single demonstration, (2) no further data collection, and (3) no prior task or object knowledge. We show how, with these constraints, imitation learning can be formulated as a combination of trajectory transfer and unseen object pose estimation. To explore this idea, we provide an in-depth study on how state-of-the-art unseen object pose estimators perform for one-shot imitation learning on ten real-world tasks, and we take a deep dive into the effects that camera calibration, pose estimation error, and spatial generalisation have on task success rates. For videos, please visit https://www.robot-learning.uk/pose-estimation-perspective.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#26088;&#22312;&#21521;&#31185;&#23398;&#23478;&#20171;&#32461;Transformer&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#22312;&#22825;&#25991;&#23398;&#20013;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#25104;&#20687;&#25968;&#25454;&#30340;&#20855;&#20307;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2310.12069</link><description>&lt;p&gt;
&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#30340;Transformer&#65306;&#22825;&#25991;&#23398;&#23478;&#30340;&#25945;&#23398;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Transformers for scientific data: a pedagogical review for astronomers. (arXiv:2310.12069v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#26088;&#22312;&#21521;&#31185;&#23398;&#23478;&#20171;&#32461;Transformer&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#22312;&#22825;&#25991;&#23398;&#20013;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#25104;&#20687;&#25968;&#25454;&#30340;&#20855;&#20307;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;ChatGPT&#21644;&#30456;&#20851;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#30456;&#20851;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#34987;&#31216;&#20026;Transformer&#12290;&#26368;&#21021;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;Transformer&#21644;&#23427;&#20204;&#21033;&#29992;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#33258;&#28982;&#31185;&#23398;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25945;&#23398;&#21644;&#38750;&#27491;&#24335;&#32508;&#36848;&#30340;&#30446;&#26631;&#26159;&#21521;&#31185;&#23398;&#23478;&#20171;&#32461;Transformer&#12290;&#25105;&#20204;&#30340;&#25945;&#23398;&#21644;&#38750;&#27491;&#24335;&#32508;&#36848;&#21253;&#25324;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#23545;&#21407;&#22987;Transformer&#26550;&#26500;&#30340;&#25551;&#36848;&#65292;&#20197;&#21450;&#22312;&#22825;&#25991;&#23398;&#20013;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#25104;&#20687;&#25968;&#25454;&#30340;&#19968;&#33410;&#12290;&#25105;&#20204;&#36824;&#21253;&#25324;&#20102;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#37096;&#20998;&#65292;&#20379;&#37027;&#20123;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24863;&#20852;&#36259;&#24182;&#24076;&#26395;&#24320;&#22987;&#20351;&#29992;Transformer&#36827;&#34892;&#30740;&#31350;&#30340;&#35835;&#32773;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deep learning architecture associated with ChatGPT and related generative AI products is known as transformers. Initially applied to Natural Language Processing, transformers and the self-attention mechanism they exploit have gained widespread interest across the natural sciences. The goal of this pedagogical and informal review is to introduce transformers to scientists. Our pedagogical and informal review includes the mathematics underlying the attention mechanism, a description of the original transformer architecture, and a section on applications to time series and imaging data in astronomy. We include with a Frequently Asked Questions section for readers who are curious about generative AI and interested in getting started with transformers for their research problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#20351;&#29992;GAN&#26102;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#25915;&#20987;&#26469;&#35782;&#21035;&#35757;&#32451;&#25968;&#25454;&#25104;&#21592;&#36523;&#20221;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#29256;&#26435;&#21644;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#37325;&#35201;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.12063</link><description>&lt;p&gt;
GAN&#20013;&#40657;&#30418;&#35757;&#32451;&#25968;&#25454;&#35782;&#21035;&#30340;&#25506;&#27979;&#22120;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Black-Box Training Data Identification in GANs via Detector Networks. (arXiv:2310.12063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#20351;&#29992;GAN&#26102;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#25915;&#20987;&#26469;&#35782;&#21035;&#35757;&#32451;&#25968;&#25454;&#25104;&#21592;&#36523;&#20221;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#29256;&#26435;&#21644;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#37325;&#35201;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23427;&#20204;&#38382;&#19990;&#20197;&#26469;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#19968;&#30452;&#26159;&#27969;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#34920;&#26684;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#35757;&#32451;&#22909;&#30340;GAN&#20197;&#21450;&#26469;&#33258;&#22522;&#30784;&#20998;&#24067;&#30340;&#26032;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#25915;&#20987;&#32773;&#33021;&#21542;&#26377;&#25928;&#22320;&#35782;&#21035;&#32473;&#23450;&#28857;&#26159;&#21542;&#23646;&#20110;GAN&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#23545;&#20110;&#29256;&#26435;&#30456;&#20851;&#30340;&#21407;&#22240;&#24456;&#26377;&#24847;&#20041;&#65292;&#29992;&#25143;&#21487;&#33021;&#24819;&#30830;&#23450;&#20182;&#20204;&#30340;&#29256;&#26435;&#25968;&#25454;&#26159;&#21542;&#34987;&#29992;&#26469;&#35757;&#32451;GAN&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#30740;&#31350;&#20063;&#24456;&#26377;&#24847;&#20041;&#65292;&#20854;&#20013;&#26816;&#27979;&#35757;&#32451;&#38598;&#25104;&#21592;&#36523;&#20221;&#30340;&#33021;&#21147;&#34987;&#31216;&#20026;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#20351;&#29992;GAN&#30340;&#38544;&#31169;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#21482;&#33021;&#35775;&#38382;&#29983;&#25104;&#22120;&#30340;&#26679;&#26412;&#65292;&#32780;&#19981;&#33021;&#35775;&#38382;&#37492;&#21035;&#22120;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#38024;&#23545;&#40657;&#30418;&#35774;&#32622;&#20013;GAN&#30340;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#65292;&#24182;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their inception Generative Adversarial Networks (GANs) have been popular generative models across images, audio, video, and tabular data. In this paper we study whether given access to a trained GAN, as well as fresh samples from the underlying distribution, if it is possible for an attacker to efficiently identify if a given point is a member of the GAN's training data. This is of interest for both reasons related to copyright, where a user may want to determine if their copyrighted data has been used to train a GAN, and in the study of data privacy, where the ability to detect training set membership is known as a membership inference attack. Unlike the majority of prior work this paper investigates the privacy implications of using GANs in black-box settings, where the attack only has access to samples from the generator, rather than access to the discriminator as well. We introduce a suite of membership inference attacks against GANs in the black-box setting and evaluate our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21644;&#20960;&#20309;&#34920;&#31034;&#26041;&#27861;&#26469;&#35299;&#20915;&#21453;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#37327;&#21270;&#22870;&#21169;&#19981;&#30830;&#23450;&#24615;&#21644;&#35782;&#21035;&#22870;&#21169;&#20989;&#25968;&#30340;&#20013;&#24515;&#34920;&#31034;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12055</link><description>&lt;p&gt;
&#22312;&#21453;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#29702;&#35299;&#22870;&#21169;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Reward Ambiguity Through Optimal Transport Theory in Inverse Reinforcement Learning. (arXiv:2310.12055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21644;&#20960;&#20309;&#34920;&#31034;&#26041;&#27861;&#26469;&#35299;&#20915;&#21453;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#37327;&#21270;&#22870;&#21169;&#19981;&#30830;&#23450;&#24615;&#21644;&#35782;&#21035;&#22870;&#21169;&#20989;&#25968;&#30340;&#20013;&#24515;&#34920;&#31034;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21453;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#20013;&#65292;&#20027;&#35201;&#30446;&#26631;&#26159;&#20174;&#35266;&#23519;&#21040;&#30340;&#19987;&#23478;&#34892;&#20026;&#20013;&#25512;&#26029;&#20986;&#28508;&#22312;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#19981;&#20165;&#35201;&#35299;&#37322;&#32473;&#23450;&#30340;&#25968;&#25454;&#65292;&#36824;&#35201;&#27867;&#21270;&#21040;&#26410;&#30693;&#22330;&#26223;&#12290;&#36825;&#30830;&#20445;&#20102;&#23545;&#22870;&#21169;&#19981;&#30830;&#23450;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#21516;&#26679;&#35299;&#37322;&#30456;&#21516;&#30340;&#19987;&#23478;&#34892;&#20026;&#12290;&#34429;&#28982;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#19978;&#24050;&#32463;&#20570;&#20986;&#20102;&#37325;&#22823;&#21162;&#21147;&#65292;&#20294;&#26159;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#39640;&#32500;&#38382;&#39064;&#19978;&#38754;&#20020;&#25361;&#25112;&#65292;&#24182;&#19988;&#32570;&#20047;&#20960;&#20309;&#22522;&#30784;&#12290;&#26412;&#25991;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#29702;&#35770;&#65292;&#20026;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;&#36890;&#36807;&#21033;&#29992;OT&#30340;Wasserstein&#36317;&#31163;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20960;&#20309;&#26694;&#26550;&#65292;&#21487;&#20197;&#37327;&#21270;&#22870;&#21169;&#19981;&#30830;&#23450;&#24615;&#24182;&#35782;&#21035;&#22870;&#21169;&#20989;&#25968;&#30340;&#20013;&#24515;&#34920;&#31034;&#25110;&#36136;&#24515;&#12290;&#36825;&#20123;&#35266;&#28857;&#20026;&#20197;&#20960;&#20309;&#35299;&#37322;&#20026;&#22522;&#30784;&#30340;&#40065;&#26834;IRL&#26041;&#27861;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#39640;&#32500;&#22870;&#21169;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In inverse reinforcement learning (IRL), the central objective is to infer underlying reward functions from observed expert behaviors in a way that not only explains the given data but also generalizes to unseen scenarios. This ensures robustness against reward ambiguity where multiple reward functions can equally explain the same expert behaviors. While significant efforts have been made in addressing this issue, current methods often face challenges with high-dimensional problems and lack a geometric foundation. This paper harnesses the optimal transport (OT) theory to provide a fresh perspective on these challenges. By utilizing the Wasserstein distance from OT, we establish a geometric framework that allows for quantifying reward ambiguity and identifying a central representation or centroid of reward functions. These insights pave the way for robust IRL methodologies anchored in geometric interpretations, offering a structured approach to tackle reward ambiguity in high-dimensiona
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#20892;&#19994;&#33829;&#20859;&#24212;&#29992;&#26102;&#38388;&#25512;&#33616;&#26041;&#27861;&#12290;&#36890;&#36807;&#39044;&#27979;&#25972;&#20010;&#23395;&#33410;&#25152;&#38656;&#30340;&#32933;&#26009;&#25968;&#37327;&#65292;&#24182;&#26681;&#25454;&#22825;&#27668;&#26465;&#20214;&#21644;&#22303;&#22756;&#29305;&#24615;&#35843;&#25972;&#32933;&#26009;&#37327;&#65292;&#20197;&#20419;&#36827;&#32463;&#27982;&#39640;&#25928;&#21644;&#29615;&#22659;&#21451;&#22909;&#30340;&#20892;&#19994;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#26045;&#32933;&#24212;&#29992;&#19982;&#22825;&#27668;&#25968;&#25454;&#23545;&#20316;&#29289;&#20135;&#37327;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.12052</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#20892;&#19994;&#33829;&#20859;&#24212;&#29992;&#26102;&#38388;&#25512;&#33616;&#65306;&#19968;&#31181;&#22823;&#35268;&#27169;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-based Nutrient Application's Timeline Recommendation for Smart Agriculture: A Large-Scale Data Mining Approach. (arXiv:2310.12052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#20892;&#19994;&#33829;&#20859;&#24212;&#29992;&#26102;&#38388;&#25512;&#33616;&#26041;&#27861;&#12290;&#36890;&#36807;&#39044;&#27979;&#25972;&#20010;&#23395;&#33410;&#25152;&#38656;&#30340;&#32933;&#26009;&#25968;&#37327;&#65292;&#24182;&#26681;&#25454;&#22825;&#27668;&#26465;&#20214;&#21644;&#22303;&#22756;&#29305;&#24615;&#35843;&#25972;&#32933;&#26009;&#37327;&#65292;&#20197;&#20419;&#36827;&#32463;&#27982;&#39640;&#25928;&#21644;&#29615;&#22659;&#21451;&#22909;&#30340;&#20892;&#19994;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#26045;&#32933;&#24212;&#29992;&#19982;&#22825;&#27668;&#25968;&#25454;&#23545;&#20316;&#29289;&#20135;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#25968;&#25454;&#20998;&#26512;&#22312;&#30417;&#27979;&#20892;&#20316;&#29289;&#26045;&#32933;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#19981;&#20934;&#30830;&#30340;&#26045;&#32933;&#20915;&#31574;&#20250;&#23548;&#33268;&#26114;&#36149;&#30340;&#21518;&#26524;&#65292;&#38459;&#30861;&#31918;&#39135;&#29983;&#20135;&#65292;&#24182;&#36896;&#25104;&#29615;&#22659;&#21361;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#30830;&#23450;&#25972;&#20010;&#23395;&#33410;&#25152;&#38656;&#30340;&#32933;&#26009;&#25968;&#37327;&#26469;&#39044;&#27979;&#33829;&#20859;&#24212;&#29992;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#24314;&#35758;&#26681;&#25454;&#22825;&#27668;&#26465;&#20214;&#21644;&#22303;&#22756;&#29305;&#24615;&#35843;&#25972;&#32933;&#26009;&#37327;&#65292;&#20197;&#20419;&#36827;&#32463;&#27982;&#39640;&#25928;&#21644;&#29615;&#22659;&#21451;&#22909;&#30340;&#20892;&#19994;&#12290;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#26159;&#39640;&#32500;&#24230;&#21644;&#24322;&#26500;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#22659;&#20013;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#24322;&#26500;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#20197;&#20908;&#23567;&#40614;&#20316;&#29289;&#20026;&#26696;&#20363;&#30740;&#31350;&#20102;&#26045;&#32933;&#24212;&#29992;&#19982;&#22825;&#27668;&#25968;&#25454;&#23545;&#20316;&#29289;&#20135;&#37327;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#29702;&#35299;&#26412;&#22320;&#32972;&#26223;&#21644;&#22320;&#29702;&#22240;&#32032;&#65292;&#25105;&#20204;&#24076;&#26395;&#31283;&#23450;&#29978;&#33267;&#20943;&#23569;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study addresses the vital role of data analytics in monitoring fertiliser applications in crop cultivation. Inaccurate fertiliser application decisions can lead to costly consequences, hinder food production, and cause environmental harm. We propose a solution to predict nutrient application by determining required fertiliser quantities for an entire season. The proposed solution recommends adjusting fertiliser amounts based on weather conditions and soil characteristics to promote cost-effective and environmentally friendly agriculture. The collected dataset is high-dimensional and heterogeneous. Our research examines large-scale heterogeneous datasets in the context of the decision-making process, encompassing data collection and analysis. We also study the impact of fertiliser applications combined with weather data on crop yield, using the winter wheat crop as a case study. By understanding local contextual and geographic factors, we aspire to stabilise or even reduce the dema
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#22788;&#29702;&#25968;&#20540;&#25361;&#25112;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12046</link><description>&lt;p&gt;
&#20197;&#26426;&#22120;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20195;&#29702;&#27169;&#22411;&#22312;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#26041;&#27861;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems. (arXiv:2310.12046v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#22788;&#29702;&#25968;&#20540;&#25361;&#25112;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#22312;&#22686;&#21152;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#20026;&#31185;&#23398;&#38382;&#39064;&#25552;&#20379;&#25968;&#20540;&#35299;&#12290;&#36825;&#31181;&#25928;&#29575;&#22312;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#30340;&#25968;&#20540;&#25361;&#25112;&#38382;&#39064;&#25110;&#38656;&#35201;&#35780;&#20272;&#35768;&#22810;&#31867;&#20284;&#20998;&#26512;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#20248;&#21183;&#12290;&#19968;&#20010;&#29305;&#23450;&#30340;&#31185;&#23398;&#20852;&#36259;&#39046;&#22495;&#26159;&#36870;&#38382;&#39064;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#25105;&#20204;&#30693;&#36947;&#31995;&#32479;&#30340;&#27491;&#21521;&#21160;&#24577;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#65292;&#20219;&#21153;&#26159;&#26681;&#25454;&#36825;&#20123;&#21160;&#24577;&#30340;&#65288;&#28508;&#22312;&#26377;&#22122;&#22768;&#30340;&#65289;&#35266;&#27979;&#26469;&#25512;&#26029;&#31995;&#32479;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#32771;&#34385;&#25512;&#26029;&#32473;&#23450;2D&#22768;&#27874;&#26041;&#31243;&#30340;&#22024;&#26434;&#35299;&#30340;&#26041;&#22359;&#22495;&#20013;&#27874;&#28304;&#30340;&#20301;&#32622;&#30340;&#36870;&#38382;&#39064;&#12290;&#22312;&#20551;&#35774;&#20026;&#39640;&#26031;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26500;&#36896;&#28304;&#20301;&#32622;&#30340;&#20284;&#28982;&#20989;&#25968;&#65292;&#27599;&#20010;&#35780;&#20272;&#37117;&#38656;&#35201;&#23545;&#31995;&#32479;&#36827;&#34892;&#19968;&#27425;&#27491;&#21521;&#27169;&#25311;&#12290;&#20351;&#29992;&#26631;&#20934;&#30340;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural networks have become a powerful tool as surrogate models to provide numerical solutions for scientific problems with increased computational efficiency. This efficiency can be advantageous for numerically challenging problems where time to solution is important or when evaluation of many similar analysis scenarios is required. One particular area of scientific interest is the setting of inverse problems, where one knows the forward dynamics of a system are described by a partial differential equation and the task is to infer properties of the system given (potentially noisy) observations of these dynamics. We consider the inverse problem of inferring the location of a wave source on a square domain, given a noisy solution to the 2-D acoustic wave equation. Under the assumption of Gaussian noise, a likelihood function for source location can be formulated, which requires one forward simulation of the system per evaluation. Using a standard neural network as a surrogate model make
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#23454;&#38469;&#31639;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25512;&#23548;&#20986;&#19968;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#32469;&#36807;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#36817;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20174;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#32780;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.12036</link><description>&lt;p&gt;
&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#29702;&#35299;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#19968;&#33324;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A General Theoretical Paradigm to Understand Learning from Human Preferences. (arXiv:2310.12036v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#23454;&#38469;&#31639;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25512;&#23548;&#20986;&#19968;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#32469;&#36807;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#36817;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20174;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#32780;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#27969;&#34892;&#26041;&#27861;&#20381;&#36182;&#20110;&#20004;&#20010;&#37325;&#35201;&#30340;&#36817;&#20284;&#65306;&#31532;&#19968;&#20551;&#35774;&#21487;&#20197;&#29992;&#36880;&#28857;&#22870;&#21169;&#26367;&#20195;&#25104;&#23545;&#20559;&#22909;&#12290;&#31532;&#20108;&#20010;&#20551;&#35774;&#26159;&#22312;&#36825;&#20123;&#36880;&#28857;&#22870;&#21169;&#19978;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#20174;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#27867;&#21270;&#21040;&#31574;&#30053;&#37319;&#26679;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32469;&#36807;&#20102;&#31532;&#20108;&#20010;&#36817;&#20284;&#65292;&#24182;&#30452;&#25509;&#20174;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#32780;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#31532;&#19968;&#20010;&#36817;&#20284;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23545;&#36825;&#20123;&#23454;&#38469;&#31639;&#27861;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#31216;&#20026;&#936;PO&#65292;&#29992;&#20110;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#65292;&#35813;&#30446;&#26631;&#20197;&#25104;&#23545;&#20559;&#22909;&#30340;&#24418;&#24335;&#34920;&#36798;&#65292;&#22240;&#27492;&#32469;&#36807;&#20102;&#36825;&#20004;&#20010;&#36817;&#20284;&#12290;&#36825;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#19968;&#31181;&#26032;&#30340;&#20174;&#35757;&#32451;&#25968;&#25454;&#30452;&#25509;&#23398;&#20064;&#31574;&#30053;&#30340;&#26041;&#27861;&#32780;&#26080;&#38656;&#36827;&#34892;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.  In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\Psi$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoDrug&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#21644;&#26680;&#23494;&#24230;&#20272;&#35745;&#26469;&#35299;&#20915;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#19968;&#33268;&#24615;&#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.12033</link><description>&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#19979;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#36827;&#34892;&#19968;&#33268;&#24615;&#33647;&#29289;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Drug Property Prediction with Density Estimation under Covariate Shift. (arXiv:2310.12033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12033
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoDrug&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#21644;&#26680;&#23494;&#24230;&#20272;&#35745;&#26469;&#35299;&#20915;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#19968;&#33268;&#24615;&#33647;&#29289;&#23646;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#30830;&#35748;&#33647;&#21697;&#24615;&#36136;&#30340;&#39044;&#27979;&#38656;&#35201;&#36827;&#34892;&#26114;&#36149;&#30340;&#28287;&#23454;&#39564;&#12290;&#22240;&#27492;&#65292;&#33719;&#21462;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#20248;&#20808;&#36873;&#25321;&#33647;&#29289;&#20998;&#23376;&#36827;&#34892;&#21518;&#32493;&#23454;&#39564;&#39564;&#35777;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#33268;&#24615;&#39044;&#27979;&#26159;&#19968;&#31181;&#26377;&#20445;&#35777;&#35206;&#30422;&#29575;&#30340;&#20026;&#20998;&#23376;&#24615;&#36136;&#21019;&#24314;&#39044;&#27979;&#38598;&#30340;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#20219;&#21153;&#20013;&#65292;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#20132;&#25442;&#24615;&#20551;&#35774;&#24448;&#24448;&#20250;&#21463;&#21040;&#21327;&#21464;&#37327;&#20559;&#31227;&#30340;&#25361;&#25112;&#65306;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#21253;&#21547;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#19981;&#36275;&#20197;&#20195;&#34920;&#20174;&#20013;&#25552;&#21462;&#20998;&#23376;&#30340;&#24222;&#22823;&#21270;&#23398;&#31354;&#38388;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;CoDrug&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#21644;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#26680;&#23494;&#24230;&#20272;&#35745;&#26469;&#35780;&#20272;&#20998;&#23376;&#38598;&#21512;&#30340;&#23494;&#24230;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#20272;&#35745;&#30340;&#23494;&#24230;&#26469;&#21152;&#26435;&#20998;&#23376;&#26679;&#26412;&#20197;&#26500;&#24314;&#39044;&#27979;&#38598;&#24182;&#20462;&#27491;&#20197;&#19979;&#30340;
&lt;/p&gt;
&lt;p&gt;
In drug discovery, it is vital to confirm the predictions of pharmaceutical properties from computational models using costly wet-lab experiments. Hence, obtaining reliable uncertainty estimates is crucial for prioritizing drug molecules for subsequent experimental validation. Conformal Prediction (CP) is a promising tool for creating such prediction sets for molecular properties with a coverage guarantee. However, the exchangeability assumption of CP is often challenged with covariate shift in drug discovery tasks: Most datasets contain limited labeled data, which may not be representative of the vast chemical space from which molecules are drawn. To address this limitation, we propose a method called CoDrug that employs an energy-based model leveraging both training data and unlabelled data, and Kernel Density Estimation (KDE) to assess the densities of a molecule set. The estimated densities are then used to weigh the molecule samples while building prediction sets and rectifying fo
&lt;/p&gt;</description></item><item><title>LMC&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#34920;&#26126;&#65292;&#21482;&#38656;&#23545;&#22122;&#22768;&#27169;&#22411;&#36827;&#34892;&#28201;&#21644;&#20551;&#35774;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#25928;&#35745;&#31639;&#12290;&#36890;&#36807;&#24341;&#20837;&#23436;&#25972;&#21442;&#25968;&#21270;&#30340;&#8220;&#25237;&#24433;LMC&#8221;&#27169;&#22411;&#21644;&#36793;&#32536;&#20284;&#28982;&#20989;&#25968;&#34920;&#36798;&#24335;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#26410;&#32463;&#22788;&#29702;&#30340;&#26041;&#27861;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12032</link><description>&lt;p&gt;
LMC&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#31934;&#30830;&#21644;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Exact and efficient solutions of the LMC Multitask Gaussian Process model. (arXiv:2310.12032v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12032
&lt;/p&gt;
&lt;p&gt;
LMC&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#34920;&#26126;&#65292;&#21482;&#38656;&#23545;&#22122;&#22768;&#27169;&#22411;&#36827;&#34892;&#28201;&#21644;&#20551;&#35774;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#25928;&#35745;&#31639;&#12290;&#36890;&#36807;&#24341;&#20837;&#23436;&#25972;&#21442;&#25968;&#21270;&#30340;&#8220;&#25237;&#24433;LMC&#8221;&#27169;&#22411;&#21644;&#36793;&#32536;&#20284;&#28982;&#20989;&#25968;&#34920;&#36798;&#24335;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#26410;&#32463;&#22788;&#29702;&#30340;&#26041;&#27861;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#20849;&#21516;&#20851;&#32852;&#27169;&#22411;&#65288;LMC&#65289;&#26159;&#19968;&#31181;&#38750;&#24120;&#36890;&#29992;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;&#22238;&#24402;&#25110;&#20998;&#31867;&#12290;&#34429;&#28982;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#27010;&#24565;&#31616;&#21333;&#24615;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#26420;&#32032;&#23454;&#29616;&#22312;&#25968;&#25454;&#28857;&#25968;&#37327;&#21644;&#20219;&#21153;&#25968;&#37327;&#26041;&#38754;&#20855;&#26377;&#31435;&#26041;&#22797;&#26434;&#24230;&#65292;&#20351;&#24471;&#23545;&#22823;&#22810;&#25968;&#24212;&#29992;&#26469;&#35828;&#65292;&#24517;&#39035;&#36827;&#34892;&#36817;&#20284;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#35813;&#27169;&#22411;&#30340;&#28508;&#22312;&#36807;&#31243;&#21487;&#20197;&#35299;&#32806;&#65292;&#23548;&#33268;&#20165;&#19982;&#25152;&#36848;&#36807;&#31243;&#25968;&#37327;&#21576;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25193;&#23637;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#20174;&#26368;&#19968;&#33324;&#30340;&#20551;&#35774;&#20013;&#23637;&#31034;&#20102;&#22312;LMC&#30340;&#39640;&#25928;&#31934;&#30830;&#35745;&#31639;&#25152;&#38656;&#30340;&#21807;&#19968;&#26465;&#20214;&#26159;&#23545;&#22122;&#22768;&#27169;&#22411;&#36827;&#34892;&#28201;&#21644;&#20551;&#35774;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32467;&#26524;&#30340;&#23436;&#25972;&#21442;&#25968;&#21270;&#8220;&#25237;&#24433;LMC&#8221;&#27169;&#22411;&#65292;&#24182;&#32473;&#20986;&#20102;&#36793;&#32536;&#20284;&#28982;&#20989;&#25968;&#30340;&#34920;&#36798;&#24335;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20102;&#21442;&#25968;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30456;&#23545;&#20110;&#26410;&#32463;&#22788;&#29702;&#30340;&#26041;&#27861;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Linear Model of Co-regionalization (LMC) is a very general model of multitask gaussian process for regression or classification. While its expressivity and conceptual simplicity are appealing, naive implementations have cubic complexity in the number of datapoints and number of tasks, making approximations mandatory for most applications. However, recent work has shown that under some conditions the latent processes of the model can be decoupled, leading to a complexity that is only linear in the number of said processes. We here extend these results, showing from the most general assumptions that the only condition necessary to an efficient exact computation of the LMC is a mild hypothesis on the noise model. We introduce a full parametrization of the resulting \emph{projected LMC} model, and an expression of the marginal likelihood enabling efficient optimization. We perform a parametric study on synthetic data to show the excellent performance of our approach, compared to an unr
&lt;/p&gt;</description></item><item><title>SegmATRon&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21464;&#25442;&#27169;&#22411;&#65292;&#29992;&#20110;&#23460;&#20869;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#36890;&#36807;&#22312;&#22810;&#24352;&#22270;&#20687;&#19978;&#36827;&#34892;&#25512;&#26029;&#26102;&#26435;&#37325;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#30340;&#36136;&#37327;&#12290;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20351;&#29992;&#20195;&#29702;&#30340;&#34892;&#20026;&#33719;&#21462;&#39069;&#22806;&#22270;&#20687;&#30340;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.12031</link><description>&lt;p&gt;
SegmATRon: &#29992;&#20110;&#23460;&#20869;&#29615;&#22659;&#30340;&#20855;&#26377;&#33258;&#36866;&#24212;&#35821;&#20041;&#20998;&#21106;&#30340;&#20307;&#39564;&#24863;&#30693;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SegmATRon: Embodied Adaptive Semantic Segmentation for Indoor Environment. (arXiv:2310.12031v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12031
&lt;/p&gt;
&lt;p&gt;
SegmATRon&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21464;&#25442;&#27169;&#22411;&#65292;&#29992;&#20110;&#23460;&#20869;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#36890;&#36807;&#22312;&#22810;&#24352;&#22270;&#20687;&#19978;&#36827;&#34892;&#25512;&#26029;&#26102;&#26435;&#37325;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#30340;&#36136;&#37327;&#12290;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20351;&#29992;&#20195;&#29702;&#30340;&#34892;&#20026;&#33719;&#21462;&#39069;&#22806;&#22270;&#20687;&#30340;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SegmATRon&#30340;&#33258;&#36866;&#24212;&#21464;&#25442;&#27169;&#22411;&#65292;&#29992;&#20110;&#23460;&#20869;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#20854;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#20351;&#29992;&#28151;&#21512;&#22810;&#32452;&#20998;&#25439;&#22833;&#20989;&#25968;&#22312;&#22810;&#24352;&#22270;&#20687;&#19978;&#36827;&#34892;&#25512;&#26029;&#30340;&#27169;&#22411;&#26435;&#37325;&#36866;&#24212;&#24615;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#36924;&#30495;&#30340;Habitat&#21644;AI2-THOR&#21512;&#25104;&#27169;&#25311;&#22120;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#35813;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#21033;&#29992;&#20195;&#29702;&#30340;&#34892;&#20026;&#33719;&#21462;&#39069;&#22806;&#22270;&#20687;&#21487;&#20197;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#30340;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/wingrune/SegmATRon&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an adaptive transformer model named SegmATRon for embodied image semantic segmentation. Its distinctive feature is the adaptation of model weights during inference on several images using a hybrid multicomponent loss function. We studied this model on datasets collected in the photorealistic Habitat and the synthetic AI2-THOR Simulators. We showed that obtaining additional images using the agent's actions in an indoor environment can improve the quality of semantic segmentation. The code of the proposed approach and datasets are publicly available at https://github.com/wingrune/SegmATRon.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26080;&#21442;&#25968;&#31163;&#25955;&#36873;&#25321;&#23454;&#39564;&#19982;&#26426;&#22120;&#23398;&#20064;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#35774;&#35745;&#26041;&#27861;GBS&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26500;&#24314;&#37197;&#23545;&#27604;&#36739;&#38382;&#39064;&#26469;&#28385;&#36275;&#28040;&#36153;&#32773;&#30340;&#20559;&#22909;&#65292;&#19981;&#38656;&#35201;&#21442;&#25968;&#21270;&#25928;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#20855;&#26377;&#25968;&#30334;&#20010;&#23646;&#24615;&#30340;&#20135;&#21697;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.12026</link><description>&lt;p&gt;
&#26080;&#21442;&#25968;&#31163;&#25955;&#36873;&#25321;&#23454;&#39564;&#19982;&#26426;&#22120;&#23398;&#20064;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Discrete Choice Experiments with Machine Learning Guided Adaptive Design. (arXiv:2310.12026v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12026
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26080;&#21442;&#25968;&#31163;&#25955;&#36873;&#25321;&#23454;&#39564;&#19982;&#26426;&#22120;&#23398;&#20064;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#35774;&#35745;&#26041;&#27861;GBS&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26500;&#24314;&#37197;&#23545;&#27604;&#36739;&#38382;&#39064;&#26469;&#28385;&#36275;&#28040;&#36153;&#32773;&#30340;&#20559;&#22909;&#65292;&#19981;&#38656;&#35201;&#21442;&#25968;&#21270;&#25928;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#20855;&#26377;&#25968;&#30334;&#20010;&#23646;&#24615;&#30340;&#20135;&#21697;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#28385;&#36275;&#28040;&#36153;&#32773;&#30340;&#20559;&#22909;&#65292;&#35774;&#35745;&#20135;&#21697;&#23545;&#20110;&#20225;&#19994;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#35843;&#26597;&#65288;GBS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#23646;&#24615;&#20135;&#21697;&#35774;&#35745;&#30340;&#31163;&#25955;&#36873;&#25321;&#23454;&#39564;&#12290;&#35813;&#23454;&#39564;&#36890;&#36807;&#19968;&#31995;&#21015;&#38024;&#23545;&#37096;&#20998;&#37197;&#32622;&#36827;&#34892;&#30340;&#37197;&#23545;&#27604;&#36739;&#26469;&#33719;&#21462;&#28040;&#36153;&#32773;&#30340;&#20559;&#22909;&#12290;GBS&#26681;&#25454;&#21463;&#35775;&#32773;&#30340;&#20808;&#21069;&#36873;&#25321;&#33258;&#36866;&#24212;&#22320;&#26500;&#24314;&#37197;&#23545;&#27604;&#36739;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#30340;&#38543;&#26426;&#25928;&#29992;&#26368;&#22823;&#21270;&#33539;&#24335;&#19981;&#21516;&#65292;GBS&#19981;&#38656;&#35201;&#21442;&#25968;&#21270;&#25928;&#29992;&#27169;&#22411;&#65292;&#22240;&#27492;&#23545;&#27169;&#22411;&#35268;&#33539;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#23454;&#39564;&#35774;&#35745;&#30456;&#32467;&#21512;&#65292;GBS&#21487;&#25193;&#23637;&#21040;&#20855;&#26377;&#25968;&#30334;&#20010;&#23646;&#24615;&#30340;&#20135;&#21697;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#24322;&#36136;&#28040;&#36153;&#32773;&#35774;&#35745;&#20010;&#24615;&#21270;&#20135;&#21697;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;GBS&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21442;&#25968;&#21270;&#21644;&#38750;&#21442;&#25968;&#21270;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing products to meet consumers' preferences is essential for a business's success. We propose the Gradient-based Survey (GBS), a discrete choice experiment for multiattribute product design. The experiment elicits consumer preferences through a sequence of paired comparisons for partial profiles. GBS adaptively constructs paired comparison questions based on the respondents' previous choices. Unlike the traditional random utility maximization paradigm, GBS is robust to model misspecification by not requiring a parametric utility model. Cross-pollinating the machine learning and experiment design, GBS is scalable to products with hundreds of attributes and can design personalized products for heterogeneous consumers. We demonstrate the advantage of GBS in accuracy and sample efficiency compared to the existing parametric and nonparametric methods in simulations.
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#36890;&#29992;&#29983;&#25104;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#22312;&#38750;&#31283;&#24577;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.12001</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Flow Networks in Continual Learning. (arXiv:2310.12001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12001
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#36890;&#29992;&#29983;&#25104;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#22312;&#38750;&#31283;&#24577;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#36890;&#29992;&#29983;&#25104;&#24314;&#27169;&#20013;&#38750;&#24120;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#20043;&#19968;&#65292;&#20855;&#26377;&#23398;&#20064;&#20219;&#20309;&#25968;&#25454;&#31867;&#22411;&#30340;&#33021;&#21147;&#12290;&#20182;&#20204;&#30340;&#24378;&#22823;&#20043;&#22788;&#22312;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#36866;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;BFNs&#30340;&#26426;&#21046;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#23427;&#22312;&#38750;&#31283;&#24577;&#25968;&#25454;&#19978;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Flow Networks (BFNs) has been recently proposed as one of the most promising direction to universal generative modelling, having ability to learn any of the data type. Their power comes from the expressiveness of neural networks and Bayesian inference which make them suitable in the context of continual learning. We delve into the mechanics behind BFNs and conduct the experiments to empirically verify the generative capabilities on non-stationary data.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#29992;&#20110;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;Vecchia-Laplace&#36817;&#20284;&#27861;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;Cholesky&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.12000</link><description>&lt;p&gt;
Vecchia-Laplace&#36817;&#20284;&#27861;&#22312;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#36845;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models. (arXiv:2310.12000v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12000
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#29992;&#20110;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;Vecchia-Laplace&#36817;&#20284;&#27861;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;Cholesky&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#26159;&#28789;&#27963;&#30340;&#27010;&#29575;&#38750;&#21442;&#25968;&#20989;&#25968;&#27169;&#22411;&#12290;Vecchia&#36817;&#20284;&#26159;&#29992;&#20110;&#20811;&#26381;&#22823;&#25968;&#25454;&#35745;&#31639;&#29942;&#39048;&#30340;&#20934;&#30830;&#36817;&#20284;&#26041;&#27861;&#65292;Laplace&#36817;&#20284;&#26159;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#38750;&#39640;&#26031;&#20284;&#28982;&#20989;&#25968;&#30340;&#36793;&#32536;&#20284;&#28982;&#21644;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#28176;&#36817;&#25910;&#25947;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#24403;&#19982;&#30452;&#25509;&#27714;&#35299;&#26041;&#27861;&#65288;&#22914;Cholesky&#20998;&#35299;&#65289;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;Vecchia-Laplace&#36817;&#20284;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#38271;&#36229;&#32447;&#24615;&#22320;&#38543;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#19982;Vecchia-Laplace&#36817;&#20284;&#35745;&#31639;&#30456;&#20851;&#30340;&#36816;&#31639;&#22312;&#36890;&#24120;&#24773;&#20917;&#19979;&#26159;&#26368;&#20934;&#30830;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#20250;&#21464;&#24471;&#38750;&#24120;&#32531;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;Vecchia-Laplace&#36817;&#20284;&#25512;&#26029;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;Cholesky&#30340;&#35745;&#31639;&#65292;&#21487;&#20197;&#22823;&#22823;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Gaussian process (GP) models are flexible probabilistic non-parametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations thus become prohibitively slow precisely when the approximations are usually the most accurate, i.e., on large data sets. In this article, we present several iterative methods for inference with Vecchia-Laplace approximations which make computations considerably faster compared to Cholesky-based calculations. We analyze our propo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32852;&#21512;&#23376;&#31354;&#38388;&#20272;&#35745;&#20174;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#28040;&#38500;&#38169;&#35823;&#27010;&#24565;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11991</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#23376;&#31354;&#38388;&#20272;&#35745;&#28040;&#38500;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#30340;&#38169;&#35823;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation. (arXiv:2310.11991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11991
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32852;&#21512;&#23376;&#31354;&#38388;&#20272;&#35745;&#20174;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#28040;&#38500;&#38169;&#35823;&#27010;&#24565;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#32463;&#24120;&#20250;&#24433;&#21709;&#21040;&#27169;&#22411;&#22312;&#26679;&#26412;&#22806;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#24120;&#35265;&#30340;&#31574;&#30053;&#26159;&#36890;&#36807;&#20174;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#28040;&#38500;&#38169;&#35823;&#27010;&#24565;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#38169;&#35823;&#27010;&#24565;&#28040;&#38500;&#26041;&#27861;&#24448;&#24448;&#36807;&#20110;&#28608;&#36827;&#65292;&#19981;&#32463;&#24847;&#38388;&#20250;&#28040;&#38500;&#19982;&#27169;&#22411;&#20027;&#35201;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#65292;&#36890;&#36807;&#20849;&#21516;&#35782;&#21035;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#30340;&#20004;&#20010;&#20302;&#32500;&#27491;&#20132;&#23376;&#31354;&#38388;&#26469;&#20998;&#31163;&#38169;&#35823;&#21644;&#20027;&#35201;&#20219;&#21153;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;Waterbirds&#65292;CelebA&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;MultiNLI&#65289;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#31639;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#20248;&#20110;&#29616;&#26377;&#30340;&#27010;&#24565;&#28040;&#38500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution generalization in neural networks is often hampered by spurious correlations. A common strategy is to mitigate this by removing spurious concepts from the neural network representation of the data. Existing concept-removal methods tend to be overzealous by inadvertently eliminating features associated with the main task of the model, thereby harming model performance. We propose an iterative algorithm that separates spurious from main-task concepts by jointly identifying two low-dimensional orthogonal subspaces in the neural network representation. We evaluate the algorithm on benchmark datasets for computer vision (Waterbirds, CelebA) and natural language processing (MultiNLI), and show that it outperforms existing concept removal methods
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24341;&#23548;&#32858;&#31867;&#12290;&#36890;&#36807;&#21033;&#29992;WordNet&#30340;&#25991;&#26412;&#35821;&#20041;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#32858;&#31867;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11989</link><description>&lt;p&gt;
&#24102;&#26377;&#22806;&#37096;&#24341;&#23548;&#30340;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Image Clustering with External Guidance. (arXiv:2310.11989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11989
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24341;&#23548;&#32858;&#31867;&#12290;&#36890;&#36807;&#21033;&#29992;WordNet&#30340;&#25991;&#26412;&#35821;&#20041;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#32858;&#31867;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#30340;&#26680;&#24515;&#26159;&#34701;&#20837;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#26500;&#24314;&#30417;&#30563;&#20449;&#21495;&#12290;&#20174;&#22522;&#20110;&#25968;&#25454;&#32039;&#23494;&#24615;&#30340;&#32463;&#20856;k&#22343;&#20540;&#21040;&#26368;&#36817;&#30340;&#22522;&#20110;&#33258;&#30417;&#30563;&#24341;&#23548;&#30340;&#23545;&#27604;&#32858;&#31867;&#65292;&#32858;&#31867;&#26041;&#27861;&#30340;&#36827;&#27493;&#19982;&#30417;&#30563;&#20449;&#21495;&#30340;&#21457;&#23637;&#20869;&#22312;&#22320;&#30456;&#23545;&#24212;&#12290;&#30446;&#21069;&#65292;&#24456;&#22810;&#24037;&#20316;&#24050;&#32463;&#33268;&#21147;&#20110;&#20174;&#25968;&#25454;&#20013;&#25366;&#25496;&#20869;&#37096;&#30417;&#30563;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#20016;&#23500;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#20363;&#22914;&#35821;&#20041;&#25551;&#36848;&#65292;&#33258;&#28982;&#22320;&#20419;&#36827;&#20102;&#32858;&#31867;&#65292;&#21364;&#34987;&#36951;&#25022;&#22320;&#24573;&#35270;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#20316;&#20026;&#26032;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24341;&#23548;&#32858;&#31867;&#65292;&#21363;&#20351;&#23427;&#20284;&#20046;&#19982;&#32473;&#23450;&#30340;&#25968;&#25454;&#26080;&#20851;&#12290;&#20026;&#20102;&#23454;&#29616;&#21644;&#39564;&#35777;&#25105;&#20204;&#30340;&#24819;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22806;&#37096;&#24341;&#23548;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;&#25991;&#26412;&#36741;&#21161;&#32858;&#31867;&#65292;TAC&#65289;&#65292;&#23427;&#21033;&#29992;WordNet&#30340;&#25991;&#26412;&#35821;&#20041;&#26469;&#20419;&#36827;&#22270;&#20687;&#32858;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TAC&#39318;&#20808;&#36873;&#25321;&#24182;&#26816;&#32034;&#26368;&#33021;&#21306;&#20998;&#22270;&#20687;&#30340;WordNet&#21517;&#35789;&#20197;&#22686;&#24378;&#32858;&#31867;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core of clustering is incorporating prior knowledge to construct supervision signals. From classic k-means based on data compactness to recent contrastive clustering guided by self-supervision, the evolution of clustering methods intrinsically corresponds to the progression of supervision signals. At present, substantial efforts have been devoted to mining internal supervision signals from data. Nevertheless, the abundant external knowledge such as semantic descriptions, which naturally conduces to clustering, is regrettably overlooked. In this work, we propose leveraging external knowledge as a new supervision signal to guide clustering, even though it seems irrelevant to the given data. To implement and validate our idea, we design an externally guided clustering method (Text-Aided Clustering, TAC), which leverages the textual semantics of WordNet to facilitate image clustering. Specifically, TAC first selects and retrieves WordNet nouns that best distinguish images to enhance th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#20027;&#21160;&#27700;&#24179;&#38598;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;&#19968;&#32500;&#21644;&#39640;&#32500;&#24773;&#20917;&#19979;&#22343;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#25928;&#26524;&#65292;&#33021;&#22815;&#24179;&#34913;&#20272;&#35745;&#35823;&#24046;&#21644;&#24050;&#31227;&#21160;&#36317;&#31163;&#65292;&#20855;&#26377;&#25512;&#24191;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11985</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#20027;&#21160;&#27700;&#24179;&#38598;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Finite-Horizon Approach to Active Level Set Estimation. (arXiv:2310.11985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11985
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#20027;&#21160;&#27700;&#24179;&#38598;&#20272;&#35745;&#26041;&#27861;&#65292;&#22312;&#19968;&#32500;&#21644;&#39640;&#32500;&#24773;&#20917;&#19979;&#22343;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#25928;&#26524;&#65292;&#33021;&#22815;&#24179;&#34913;&#20272;&#35745;&#35823;&#24046;&#21644;&#24050;&#31227;&#21160;&#36317;&#31163;&#65292;&#20855;&#26377;&#25512;&#24191;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#31354;&#38388;&#37319;&#26679;&#20013;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#29992;&#20110;&#27700;&#24179;&#38598;&#20272;&#35745;&#65288;LSE&#65289;&#12290;&#35813;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#23613;&#24555;&#23450;&#20301;&#25152;&#26377;&#31526;&#21512;&#32473;&#23450;&#38408;&#20540;&#19978;/&#19979;&#30340;&#24863;&#20852;&#36259;&#20989;&#25968;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#38480;&#26102;&#38388;&#30340;&#25628;&#32034;&#36807;&#31243;&#65292;&#20197;&#22312;&#19968;&#32500;&#20013;&#25191;&#34892;LSE&#65292;&#21516;&#26102;&#22312;&#22266;&#23450;&#25968;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#22320;&#24179;&#34913;&#26368;&#32456;&#20272;&#35745;&#35823;&#24046;&#21644;&#24050;&#31227;&#21160;&#30340;&#36317;&#31163;&#12290;&#36890;&#36807;&#35843;&#25972;&#21442;&#25968;&#26469;&#26435;&#34913;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#24050;&#31227;&#21160;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24471;&#20986;&#30340;&#20248;&#21270;&#38382;&#39064;&#21487;&#20197;&#38381;&#24335;&#27714;&#35299;&#65292;&#24182;&#19988;&#25152;&#24471;&#31574;&#30053;&#25512;&#24191;&#20102;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#27969;&#34892;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#19979;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#26469;&#36827;&#34892;&#26356;&#39640;&#32500;&#24230;&#30340;&#27700;&#24179;&#38598;&#20272;&#35745;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#26053;&#34892;&#25104;&#26412;&#30340;&#22686;&#21152;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#38750;&#36845;&#20195;&#36317;&#31163;&#22788;&#29702;&#30340;&#33021;&#21147;&#20351;&#20854;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#24050;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of active learning in the context of spatial sampling for level set estimation (LSE), where the goal is to localize all regions where a function of interest lies above/below a given threshold as quickly as possible. We present a finite-horizon search procedure to perform LSE in one dimension while optimally balancing both the final estimation error and the distance traveled for a fixed number of samples. A tuning parameter is used to trade off between the estimation accuracy and distance traveled. We show that the resulting optimization problem can be solved in closed form and that the resulting policy generalizes existing approaches to this problem. We then show how this approach can be used to perform level set estimation in higher dimensions under the popular Gaussian process model. Empirical results on synthetic data indicate that as the cost of travel increases, our method's ability to treat distance nonmyopically allows it to significantly improve on the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#31639;&#26415;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#20559;&#32622;&#20197;&#21450;Attention Bias Calibration&#65288;ABC&#65289;&#26469;&#23454;&#29616;&#23545;&#20110;&#38271;&#38271;&#24230;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.11984</link><description>&lt;p&gt;
&#20174;&#25554;&#20540;&#21040;&#22806;&#25512;&#65306;&#31639;&#26415;Transformer&#30340;&#23436;&#25972;&#38271;&#24230;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers. (arXiv:2310.11984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#31639;&#26415;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#20559;&#32622;&#20197;&#21450;Attention Bias Calibration&#65288;ABC&#65289;&#26469;&#23454;&#29616;&#23545;&#20110;&#38271;&#38271;&#24230;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#25552;&#20986;&#20197;&#26469;&#65292;Transformer&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#31639;&#27861;&#20219;&#21153;&#20013;&#65292;&#38271;&#24230;&#27867;&#21270;&#20173;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#31639;&#26415;&#31639;&#27861;&#65288;&#22914;&#21152;&#27861;&#21644;&#20056;&#27861;&#65289;&#26041;&#38754;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#21644;&#27880;&#24847;&#21147;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23454;&#29616;&#26368;&#20339;&#38271;&#24230;&#27867;&#21270;&#30340;&#20960;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#30446;&#26631;&#25351;&#21521;&#20559;&#32622;&#26469;&#27867;&#21270;&#21040;&#38271;&#38271;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Attention Bias Calibration&#65288;ABC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26657;&#20934;&#38454;&#27573;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#36866;&#24403;&#30340;&#27880;&#24847;&#21147;&#20559;&#32622;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#26426;&#21046;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;ABC&#65292;Transformer&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#20123;&#31639;&#26415;&#20219;&#21153;&#19978;&#23454;&#29616;&#21069;&#25152;&#26410;&#26377;&#30340;&#23436;&#32654;&#38271;&#24230;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its introduction, the transformer model has demonstrated outstanding performance across various tasks. However, there are still unresolved issues regarding length generalization, particularly in algorithmic tasks. In this paper, we investigate the inherent capabilities of transformer models in learning arithmetic algorithms, such as addition and multiplication. Through experiments and attention analysis, we identify a number of crucial factors for achieving optimal length generalization. We show that transformer models are able to generalize to long lengths with the help of targeted attention biasing. We then introduce Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which we link to mechanisms in relative position encoding. We demonstrate that using ABC, the transformer model can achieve unprecedented perfect length generalization on certain arithmetic tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#32452;&#32553;&#25918;&#26041;&#27861;&#65288;BVS&#65289;&#30340;&#20960;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#21644;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#30340;&#20998;&#32452;&#26041;&#26696;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11978</link><description>&lt;p&gt;
&#21487;&#20197;&#36890;&#36807;&#20998;&#32452;&#32553;&#25918;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?. (arXiv:2310.11978v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#32452;&#32553;&#25918;&#26041;&#27861;&#65288;BVS&#65289;&#30340;&#20960;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#21644;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#30340;&#20998;&#32452;&#26041;&#26696;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20998;&#32452;&#26041;&#24046;&#32553;&#25918;&#65288;BVS&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#38382;&#39064;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#65292;&#33021;&#22815;&#27604;&#32479;&#19968;&#26041;&#24046;&#65288;&#25110;&#28201;&#24230;&#65289;&#32553;&#25918;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#26657;&#27491;&#12290;&#21407;&#22987;&#29256;&#26412;&#30340;BVS&#20351;&#29992;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20998;&#32452;&#65292;&#26088;&#22312;&#25552;&#39640;&#26465;&#20214;&#19978;&#30340;&#26657;&#20934;&#24615;&#65292;&#21363;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;BVS&#30340;&#20960;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#25439;&#22833;&#20989;&#25968;&#21644;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#65288;X&#65289;&#30340;&#20998;&#32452;&#26041;&#26696;&#19978;&#36827;&#34892;&#25913;&#36827;&#65292;&#20197;&#25552;&#39640;&#36866;&#24212;&#24615;&#65292;&#21363;&#22312;&#32473;&#23450;X&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#26657;&#20934;&#24615;&#12290;&#23558;BVS&#21450;&#20854;&#25913;&#36827;&#26041;&#26696;&#22312;&#39044;&#27979;&#21407;&#23376;&#21270;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#27979;&#35797;&#65292;&#24182;&#19982;&#20445;&#24207;&#22238;&#24402;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binwise Variance Scaling (BVS) has recently been proposed as a post hoc recalibration method for prediction uncertainties of machine learning regression problems that is able of more efficient corrections than uniform variance (or temperature) scaling. The original version of BVS uses uncertainty-based binning, which is aimed to improve calibration conditionally on uncertainty, i.e. consistency. I explore here several adaptations of BVS, in particular with alternative loss functions and a binning scheme based on an input-feature (X) in order to improve adaptivity, i.e. calibration conditional on X. The performances of BVS and its proposed variants are tested on a benchmark dataset for the prediction of atomization energies and compared to the results of isotonic regression.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;AI&#21161;&#25163;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.11971</link><description>&lt;p&gt;
&#36890;&#36807;&#32676;&#20307;&#19981;&#21464;&#24615;&#23398;&#20064;&#25552;&#39640;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization of Alignment with Human Preferences through Group Invariant Learning. (arXiv:2310.11971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;AI&#21161;&#25163;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;AI&#21161;&#25163;&#30340;&#25104;&#21151;&#22312;&#20110;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;, &#20351;&#29983;&#25104;&#30340;&#22238;&#31572;&#26356;&#21152;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;. &#20316;&#20026;&#36890;&#29992;AI&#21161;&#25163;, &#20154;&#20204;&#36234;&#26469;&#36234;&#26399;&#26395;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#34920;&#29616;&#19968;&#33268;. &#28982;&#32780;, &#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;,&#24378;&#21270;&#23398;&#20064;(RL)&#32463;&#24120;&#21033;&#29992;&#25463;&#24452;&#20197;&#33719;&#24471;&#36739;&#39640;&#30340;&#22870;&#21169;, &#24573;&#30053;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;. &#36825;&#31181;&#23545;&#24555;&#36895;&#22870;&#21169;&#25910;&#30410;&#30340;&#20851;&#27880;&#19981;&#20165;&#21066;&#24369;&#20102;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;, &#20063;&#21066;&#24369;&#20102;&#27169;&#22411;&#23545;&#26032;&#30340;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;. &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;, &#21487;&#20197;&#36890;&#36807;RL&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#30340;&#31574;&#30053;. &#37492;&#20110;&#33719;&#24471;&#32676;&#20307;&#26631;&#27880;&#30340;&#25361;&#25112;, &#25105;&#20204;&#30340;&#26041;&#27861;&#20250;&#33258;&#21160;&#23558;&#25968;&#25454;&#20998;&#31867;&#21040;&#19981;&#21516;&#30340;&#32452;&#20013;, &#26377;&#24847;&#22320;&#26368;&#22823;&#21270;&#24615;&#33021;&#24046;&#24322;. &#28982;&#21518;, &#25105;&#20204;&#20248;&#21270;&#31574;&#30053;&#20197;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#20013;&#34920;&#29616;&#33391;&#22909;. &#26368;&#21518;, &#21033;&#29992;&#24050;&#24314;&#31435;&#30340;
&lt;/p&gt;
&lt;p&gt;
The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the estab
&lt;/p&gt;</description></item><item><title>aTrain&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#31163;&#32447;&#38899;&#39057;&#36716;&#24405;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#35821;&#35328;&#36716;&#24405;&#24182;&#20855;&#26377;CPU&#21644;NVIDIA GPU&#25903;&#25345;&#12290;&#23427;&#36866;&#29992;&#20110;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#23450;&#24615;&#25968;&#25454;&#65292;&#25552;&#20379;&#31616;&#21333;&#26131;&#29992;&#30340;&#30028;&#38754;&#65292;&#24182;&#33021;&#19982;&#24120;&#29992;&#30340;&#23450;&#24615;&#25968;&#25454;&#20998;&#26512;&#36719;&#20214;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.11967</link><description>&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#35775;&#38382;&#24615;&#35775;&#35848;&#36716;&#24405;&#30340;&#30028;&#38754;&#8212;&#8212;aTrain
&lt;/p&gt;
&lt;p&gt;
Take the aTrain. Introducing an Interface for the Accessible Transcription of Interviews. (arXiv:2310.11967v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11967
&lt;/p&gt;
&lt;p&gt;
aTrain&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#31163;&#32447;&#38899;&#39057;&#36716;&#24405;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#35821;&#35328;&#36716;&#24405;&#24182;&#20855;&#26377;CPU&#21644;NVIDIA GPU&#25903;&#25345;&#12290;&#23427;&#36866;&#29992;&#20110;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#23450;&#24615;&#25968;&#25454;&#65292;&#25552;&#20379;&#31616;&#21333;&#26131;&#29992;&#30340;&#30028;&#38754;&#65292;&#24182;&#33021;&#19982;&#24120;&#29992;&#30340;&#23450;&#24615;&#25968;&#25454;&#20998;&#26512;&#36719;&#20214;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
aTrain&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#31163;&#32447;&#24037;&#20855;&#65292;&#21487;&#29992;&#20110;&#20197;&#22810;&#31181;&#35821;&#35328;&#36827;&#34892;&#38899;&#39057;&#25968;&#25454;&#36716;&#24405;&#65292;&#24182;&#25903;&#25345;CPU&#21644;NVIDIA GPU&#12290;&#23427;&#19987;&#38376;&#20026;&#20351;&#29992;&#20174;&#21508;&#31181;&#24418;&#24335;&#30340;&#35821;&#38899;&#20114;&#21160;&#20013;&#29983;&#25104;&#30340;&#23450;&#24615;&#25968;&#25454;&#30340;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#12290;aTrain&#19981;&#38656;&#35201;&#32534;&#31243;&#25216;&#33021;&#65292;&#21487;&#22312;&#22823;&#22810;&#25968;&#35745;&#31639;&#26426;&#19978;&#36816;&#34892;&#65292;&#19981;&#38656;&#35201;&#20114;&#32852;&#32593;&#36830;&#25509;&#65292;&#24182;&#32463;&#36807;&#39564;&#35777;&#19981;&#20250;&#19978;&#20256;&#25968;&#25454;&#21040;&#20219;&#20309;&#26381;&#21153;&#22120;&#12290;aTrain&#23558;OpenAI&#30340;Whisper&#27169;&#22411;&#19982;&#35828;&#35805;&#20154;&#35782;&#21035;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#19982;&#24120;&#29992;&#30340;&#23450;&#24615;&#25968;&#25454;&#20998;&#26512;&#36719;&#20214;&#24037;&#20855;MAXQDA&#21644;ATLAS.ti&#38598;&#25104;&#30340;&#36755;&#20986;&#12290;&#23427;&#26377;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#22270;&#24418;&#30028;&#38754;&#65292;&#24182;&#36890;&#36807;Microsoft Store&#25552;&#20379;&#20316;&#20026;Windows&#24212;&#29992;&#31243;&#24207;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26041;&#20415;&#22320;&#36827;&#34892;&#23433;&#35013;&#12290;&#28304;&#20195;&#30721;&#22312;GitHub&#19978;&#20813;&#36153;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
aTrain is an open-source and offline tool for transcribing audio data in multiple languages with CPU and NVIDIA GPU support. It is specifically designed for researchers using qualitative data generated from various forms of speech interactions with research participants. aTrain requires no programming skills, runs on most computers, does not require an internet connection, and was verified not to upload data to any server. aTrain combines OpenAI's Whisper model with speaker recognition to provide output that integrates with the popular qualitative data analysis software tools MAXQDA and ATLAS.ti. It has an easy-to-use graphical interface and is provided as a Windows-App through the Microsoft Store allowing for simple installation by researchers. The source code is freely available on GitHub. Having developed aTrain with a focus on speed on local computers, we show that the transcription time on current mobile CPUs is around 2 to 3 times the duration of the audio file using the highest-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#21355;&#26143;&#36890;&#20449;&#20013;&#30340;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#65292;&#22312;&#22788;&#29702;&#24322;&#26500;&#27969;&#37327;&#22330;&#26223;&#26102;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#21644;&#32422;&#26463;&#38598;&#25104;&#21040;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#36164;&#28304;&#20998;&#37197;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#24230;&#37327;&#32508;&#21512;&#32771;&#34385;&#20102;&#36890;&#20449;&#31995;&#32479;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11966</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#21355;&#26143;&#26580;&#24615;&#26377;&#25928;&#36733;&#33655;&#37197;&#32622;
&lt;/p&gt;
&lt;p&gt;
Flexible Payload Configuration for Satellites using Machine Learning. (arXiv:2310.11966v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#21355;&#26143;&#36890;&#20449;&#20013;&#30340;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#65292;&#22312;&#22788;&#29702;&#24322;&#26500;&#27969;&#37327;&#22330;&#26223;&#26102;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#21644;&#32422;&#26463;&#38598;&#25104;&#21040;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#36164;&#28304;&#20998;&#37197;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#24230;&#37327;&#32508;&#21512;&#32771;&#34385;&#20102;&#36890;&#20449;&#31995;&#32479;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21355;&#26143;&#36890;&#20449;&#23545;&#20110;&#29616;&#20195;&#36830;&#25509;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#25193;&#22823;&#20102;&#38470;&#22320;&#32593;&#32476;&#26080;&#27861;&#35206;&#30422;&#30340;&#28023;&#19978;&#12289;&#33322;&#31354;&#21644;&#20559;&#36828;&#22320;&#21306;&#30340;&#25509;&#20837;&#33539;&#22260;&#12290;&#24403;&#21069;&#30340;&#22320;&#29699;&#21516;&#27493;&#36712;&#36947;&#21355;&#26143;&#31995;&#32479;&#20351;&#29992;&#22810;&#27874;&#26463;&#35206;&#30422;&#24182;&#36890;&#36807;&#20998;&#39057;&#37325;&#29992;&#26469;&#22343;&#21248;&#20998;&#37197;&#21151;&#29575;&#21644;&#24102;&#23485;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#24322;&#26500;&#27969;&#37327;&#22330;&#26223;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#23548;&#33268;&#20102;&#20302;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#65288;RRM&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;RRM&#20219;&#21153;&#35270;&#20026;&#22238;&#24402;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#23558;RRM&#30446;&#26631;&#21644;&#32422;&#26463;&#38598;&#25104;&#21040;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#30446;&#26631;&#26159;&#23558;&#20854;&#26368;&#23567;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#24230;&#37327;&#26631;&#20934;&#65292;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#32771;&#34385;&#20854;&#36164;&#28304;&#20998;&#37197;&#20915;&#31574;&#23545;&#36890;&#20449;&#31995;&#32479;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Satellite communications, essential for modern connectivity, extend access to maritime, aeronautical, and remote areas where terrestrial networks are unfeasible. Current GEO systems distribute power and bandwidth uniformly across beams using multi-beam footprints with fractional frequency reuse. However, recent research reveals the limitations of this approach in heterogeneous traffic scenarios, leading to inefficiencies. To address this, this paper presents a machine learning (ML)-based approach to Radio Resource Management (RRM).  We treat the RRM task as a regression ML problem, integrating RRM objectives and constraints into the loss function that the ML algorithm aims at minimizing. Moreover, we introduce a context-aware ML metric that evaluates the ML model's performance but also considers the impact of its resource allocation decisions on the overall performance of the communication system.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#20351;&#29992;&#20998;&#27835;&#31574;&#30053;&#23558;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#20302;&#21040;O(n log n)&#25110;O(n)&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20840;&#23616;&#24863;&#30693;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2310.11960</link><description>&lt;p&gt;
&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#65306;&#19968;&#31181;&#29992;&#20110;&#38271;&#24207;&#21015;&#30340;&#20998;&#27835;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences. (arXiv:2310.11960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11960
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#20351;&#29992;&#20998;&#27835;&#31574;&#30053;&#23558;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#20302;&#21040;O(n log n)&#25110;O(n)&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20840;&#23616;&#24863;&#30693;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#33258;&#27880;&#24847;&#21147;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;Transformer&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#65292;&#19968;&#31181;&#20351;&#29992;&#20998;&#27835;&#31574;&#30053;&#26469;&#20943;&#23569;&#27880;&#24847;&#21147;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23558;&#38271;&#24230;&#20026;n&#30340;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#20302;&#21040;O(n log n)&#25110;O(n)&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20840;&#23616;&#24863;&#30693;&#33539;&#22260;&#12290;&#36825;&#31181;&#20998;&#23618;&#26041;&#27861;&#23558;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#20998;&#20026;O(log n)&#32423;&#30340;&#20998;&#36776;&#29575;&#65292;&#36739;&#36828;&#36317;&#31163;&#30340;&#32452;&#32676;&#36234;&#26469;&#36234;&#22823;&#65292;&#24182;&#23398;&#20064;&#35745;&#31639;&#32452;&#32676;&#25968;&#37327;&#30340;&#26435;&#37325;&#12290;&#22240;&#27492;&#65292;&#20197;&#39640;&#25928;&#20998;&#23618;&#30340;&#26041;&#24335;&#22312;&#36739;&#20302;&#30340;&#20998;&#36776;&#29575;&#20013;&#32771;&#34385;&#36828;&#31163;&#24444;&#27492;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#30340;&#24635;&#20307;&#22797;&#26434;&#24230;&#20026;O(n)&#25110;O(n log n)&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have achieved state-of-the-art performance in many areas. However, the quadratic complexity of self-attention with respect to the input length hinders the applicability of Transformer-based models to long sequences. To address this, we present Fast Multipole Attention, a new attention mechanism that uses a divide-and-conquer strategy to reduce the time and memory complexity of attention for sequences of length $n$ from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a global receptive field. The hierarchical approach groups queries, keys, and values into $\mathcal{O}( \log n)$ levels of resolution, where groups at greater distances are increasingly larger in size and the weights to compute group quantities are learned. As such, the interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$ or $\mathcal{O}(n \
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MSD-Mixer&#30340;&#22810;&#23610;&#24230;&#20998;&#35299;MLP-Mixer&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23618;&#27425;&#20013;&#34920;&#31034;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25340;&#25509;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#22810;&#23610;&#24230;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#36890;&#36947;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.11959</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#22810;&#23610;&#24230;&#20998;&#35299;MLP-Mixer
&lt;/p&gt;
&lt;p&gt;
A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis. (arXiv:2310.11959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11959
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MSD-Mixer&#30340;&#22810;&#23610;&#24230;&#20998;&#35299;MLP-Mixer&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23618;&#27425;&#20013;&#34920;&#31034;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25340;&#25509;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#22810;&#23610;&#24230;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#36890;&#36947;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#29420;&#29305;&#30340;&#32452;&#25104;&#21644;&#22797;&#26434;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#21464;&#21270;&#65292;&#38656;&#35201;&#22312;&#20854;&#20998;&#26512;&#20013;&#29305;&#21035;&#32771;&#34385;&#20998;&#35299;&#21644;&#22810;&#23610;&#24230;&#24314;&#27169;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#19988;&#23545;&#23376;&#24207;&#21015;&#32423;&#21035;&#30340;&#24314;&#27169;&#21644;&#20998;&#35299;&#19981;&#22815;&#20805;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MSD-Mixer&#65292;&#19968;&#31181;&#22810;&#23610;&#24230;&#20998;&#35299;&#30340;MLP-Mixer&#65292;&#23427;&#23398;&#20250;&#20102;&#23558;&#36755;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#26126;&#30830;&#22320;&#20998;&#35299;&#25104;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23618;&#27425;&#20013;&#34920;&#31034;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#23610;&#24230;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#36890;&#36947;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25340;&#25509;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20026;&#22810;&#23610;&#24230;&#23376;&#24207;&#21015;&#65292;&#21363;patches&#65292;&#24182;&#20351;&#29992;MLPs&#26469;&#32452;&#21512;patches&#20869;&#37096;&#21644;patches&#38388;&#30340;&#21464;&#21270;&#20197;&#21450;&#36890;&#36947;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#26469;&#32422;&#26463;&#20998;&#35299;&#27531;&#24046;&#30340;&#24133;&#24230;&#21644;&#33258;&#30456;&#20851;&#24615;&#65292;&#20197;&#23454;&#29616;&#23436;&#25972;&#30340;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series data, often characterized by unique composition and complex multi-scale temporal variations, requires special consideration of decomposition and multi-scale modeling in its analysis. Existing deep learning methods on this best fit to only univariate time series, and have not sufficiently accounted for sub-series level modeling and decomposition completeness. To address this, we propose MSD-Mixer, a Multi-Scale Decomposition MLP-Mixer which learns to explicitly decompose the input time series into different components, and represents the components in different layers. To handle multi-scale temporal patterns and inter-channel dependencies, we propose a novel temporal patching approach to model the time series as multi-scale sub-series, i.e., patches, and employ MLPs to mix intra- and inter-patch variations and channel-wise correlations. In addition, we propose a loss function to constrain both the magnitude and autocorrelation of the decomposition residual for decomposition 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36136;&#30097;&#20102;&#30452;&#25509;&#27169;&#22411;&#32534;&#36753;&#20316;&#20026;&#32416;&#27491;LLM&#29983;&#25104;&#20013;&#20107;&#23454;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20043;&#31867;&#20284;&#20294;&#26356;&#20026;&#26126;&#30830;&#30340;&#19977;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#34429;&#28982;&#27169;&#22411;&#32534;&#36753;&#22312;&#25552;&#21319;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#19981;&#33021;&#34987;&#35270;&#20026;&#35299;&#20915;LLMs&#22266;&#26377;&#32570;&#28857;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#23384;&#22312;&#21152;&#24378;&#27169;&#22411;&#21487;&#20449;&#24615;&#35266;&#24565;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.11958</link><description>&lt;p&gt;
&#29992;&#21242;&#23376;&#33280;&#31354;&#28023;&#27915;&#65306;&#25105;&#20204;&#24212;&#35813;&#32534;&#36753;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Emptying the Ocean with a Spoon: Should We Edit Models?. (arXiv:2310.11958v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11958
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36136;&#30097;&#20102;&#30452;&#25509;&#27169;&#22411;&#32534;&#36753;&#20316;&#20026;&#32416;&#27491;LLM&#29983;&#25104;&#20013;&#20107;&#23454;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20043;&#31867;&#20284;&#20294;&#26356;&#20026;&#26126;&#30830;&#30340;&#19977;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#34429;&#28982;&#27169;&#22411;&#32534;&#36753;&#22312;&#25552;&#21319;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#19981;&#33021;&#34987;&#35270;&#20026;&#35299;&#20915;LLMs&#22266;&#26377;&#32570;&#28857;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#23384;&#22312;&#21152;&#24378;&#27169;&#22411;&#21487;&#20449;&#24615;&#35266;&#24565;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#30452;&#25509;&#27169;&#22411;&#32534;&#36753;&#20316;&#20026;&#32416;&#27491;LLM&#29983;&#25104;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#32534;&#36753;&#19982;&#36861;&#27714;&#26356;&#26126;&#30830;&#30446;&#26631;&#30340;&#19977;&#31181;&#31867;&#20284;&#20294;&#19981;&#21516;&#30340;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65306;&#65288;1&#65289;&#22522;&#20110;&#26816;&#32034;&#30340;&#26550;&#26500;&#65292;&#23558;&#20107;&#23454;&#35760;&#24518;&#19982;LLMs&#25152;&#20307;&#29616;&#30340;&#25512;&#29702;&#21644;&#35821;&#35328;&#33021;&#21147;&#35299;&#32806;&#65307;&#65288;2&#65289;&#27010;&#24565;&#25830;&#38500;&#26041;&#27861;&#65292;&#26088;&#22312;&#38450;&#27490;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#31995;&#32479;&#20559;&#35265;&#65307;&#65288;3&#65289;&#24402;&#23646;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#29983;&#25104;&#32467;&#26524;&#19982;&#24050;&#30830;&#23450;&#30340;&#25991;&#26412;&#26469;&#28304;&#36830;&#25509;&#36215;&#26469;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19981;&#33021;&#23558;&#30452;&#25509;&#27169;&#22411;&#32534;&#36753;&#20316;&#20026;&#35299;&#20915;LLMs&#22266;&#26377;&#32570;&#28857;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#65292;&#24182;&#19988;&#34429;&#28982;&#23427;&#22312;&#25913;&#36827;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#36890;&#36807;&#21152;&#24378;&#27169;&#22411;&#21487;&#20449;&#24615;&#30340;&#35266;&#24565;&#32780;&#23384;&#22312;&#39118;&#38505;&#12290;&#25105;&#20204;&#21628;&#21505;&#22312;LLM&#37096;&#32626;&#36807;&#31243;&#20013;&#35880;&#24910;&#25512;&#24191;&#21644;&#24212;&#29992;&#27169;&#22411;&#32534;&#36753;&#65292;&#24182;&#36127;&#36131;&#20219;&#22320;&#38480;&#21046;LLMs&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#20197;&#19981;&#20381;&#36182;....
&lt;/p&gt;
&lt;p&gt;
We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations. We contrast model editing with three similar but distinct approaches that pursue better defined objectives: (1) retrieval-based architectures, which decouple factual memory from inference and linguistic capabilities embodied in LLMs; (2) concept erasure methods, which aim at preventing systemic bias in generated text; and (3) attribution methods, which aim at grounding generations into identified textual sources. We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality. We call for cautious promotion and application of model editing as part of the LLM deployment process, and for responsibly limiting the use cases of LLMs to those not relying o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#36830;&#32493;&#23398;&#20064;&#37325;&#22609;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37319;&#29992;&#20803;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#38656;&#35201;&#23545;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#36830;&#32493;&#23398;&#20064;episode&#19978;&#30340;&#20803;&#32423;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#36890;&#29992;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.11952</link><description>&lt;p&gt;
&#23558;&#36830;&#32493;&#23398;&#20064;&#37325;&#22609;&#20026;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Recasting Continual Learning as Sequence Modeling. (arXiv:2310.11952v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#36830;&#32493;&#23398;&#20064;&#37325;&#22609;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37319;&#29992;&#20803;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#38656;&#35201;&#23545;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#36830;&#32493;&#23398;&#20064;episode&#19978;&#30340;&#20803;&#32423;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#36890;&#29992;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#20004;&#20010;&#37325;&#35201;&#39046;&#22495;&#20043;&#38388;&#30340;&#24378;&#36830;&#25509;&#65306;&#36830;&#32493;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36830;&#32493;&#23398;&#20064;&#20316;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#36827;&#34892;&#34920;&#36848;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#20808;&#36827;&#30340;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#25104;&#20026;&#24207;&#21015;&#27169;&#22411;&#30340;&#21069;&#21521;&#20256;&#25773;&#12290;&#36890;&#36807;&#37319;&#29992;&#20803;&#36830;&#32493;&#23398;&#20064;(MCL)&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#22810;&#20010;&#36830;&#32493;&#23398;&#20064;episode&#19978;&#23545;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#20803;&#32423;&#35757;&#32451;&#12290;&#20316;&#20026;&#25105;&#20204;&#26032;&#26694;&#26550;&#30340;&#19968;&#20010;&#20855;&#20307;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;Transformer&#21450;&#20854;&#39640;&#25928;&#21464;&#20307;&#24212;&#29992;&#20110;MCL&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#19971;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#36890;&#29992;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling. That is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning. Under this formulation, the continual learning process becomes the forward pass of a sequence model. By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes. As a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods. Our experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#23384;&#22312;&#24615;&#33021;&#35780;&#20272;&#36807;&#39640;&#30340;&#38382;&#39064;&#65292;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#25968;&#25454;&#20998;&#21106;&#21644;&#20132;&#21449;&#39564;&#35777;&#23548;&#33268;&#20102;&#32467;&#26524;&#30340;&#20559;&#35265;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#26368;&#26032;&#30340;&#30740;&#31350;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#19981;&#27491;&#30830;&#30340;&#32467;&#26524;&#20250;&#23548;&#33268;&#25253;&#21578;&#36739;&#20302;&#20934;&#30830;&#24230;&#30340;&#35770;&#25991;&#26356;&#38590;&#21457;&#34920;&#12290;</title><link>http://arxiv.org/abs/2310.11950</link><description>&lt;p&gt;
&#22826;&#22909;&#19981;&#20687;&#26159;&#30495;&#30340;&#65306;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#35780;&#20272;&#36807;&#39640;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Too Good To Be True: performance overestimation in (re)current practices for Human Activity Recognition. (arXiv:2310.11950v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#23384;&#22312;&#24615;&#33021;&#35780;&#20272;&#36807;&#39640;&#30340;&#38382;&#39064;&#65292;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#25968;&#25454;&#20998;&#21106;&#21644;&#20132;&#21449;&#39564;&#35777;&#23548;&#33268;&#20102;&#32467;&#26524;&#30340;&#20559;&#35265;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#26368;&#26032;&#30340;&#30740;&#31350;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#19981;&#27491;&#30830;&#30340;&#32467;&#26524;&#20250;&#23548;&#33268;&#25253;&#21578;&#36739;&#20302;&#20934;&#30830;&#24230;&#30340;&#35770;&#25991;&#26356;&#38590;&#21457;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#65292;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#39046;&#22495;&#26377;&#19968;&#20123;&#26631;&#20934;&#21644;&#26082;&#23450;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20256;&#32479;&#26041;&#27861;&#20250;&#23548;&#33268;&#31934;&#24230;&#34987;&#39640;&#20272;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#36827;&#34892;&#25968;&#25454;&#20998;&#21106;&#30340;&#26041;&#27861;&#20197;&#21450;&#26631;&#20934;&#30340;&#38543;&#26426;k&#25240;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#20250;&#20135;&#29983;&#20559;&#35265;&#32467;&#26524;&#12290;&#23545;&#36807;&#21435;&#30340;&#25991;&#29486;&#21644;&#29616;&#20195;&#30740;&#31350;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;HAR&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#20013;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#26377;&#24517;&#35201;&#24341;&#36215;&#31185;&#23398;&#30028;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#30340;&#36127;&#38754;&#24433;&#21709;&#34987;&#24573;&#35270;&#20102;&#12290;&#21542;&#21017;&#65292;&#21457;&#34920;&#20559;&#35265;&#32467;&#26524;&#30340;&#35770;&#25991;&#23558;&#20250;&#25253;&#21578;&#36739;&#20302;&#30340;&#20934;&#30830;&#24230;&#65292;&#27491;&#30830;&#30340;&#26080;&#20559;&#26041;&#27861;&#26356;&#38590;&#20197;&#21457;&#34920;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#23454;&#39564;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#36825;&#20010;&#38382;&#39064;&#30340;&#23384;&#22312;&#65292;&#24182;&#19988;&#26080;&#35770;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#22914;&#20309;&#65292;&#36825;&#20010;&#38382;&#39064;&#37117;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, there are standard and well established procedures within the Human Activity Recognition (HAR) pipeline. However, some of these conventional approaches lead to accuracy overestimation. In particular, sliding windows for data segmentation followed by standard random k-fold cross validation, produce biased results. An analysis of previous literature and present-day studies, surprisingly, shows that these are common approaches in state-of-the-art studies on HAR. It is important to raise awareness in the scientific community about this problem, whose negative effects are being overlooked. Otherwise, publications of biased results lead to papers that report lower accuracies, with correct unbiased methods, harder to publish. Several experiments with different types of datasets and different types of classification models allow us to exhibit the problem and show it persists independently of the method or dataset.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#29942;&#39048;&#65292;&#31216;&#20026;&#28388;&#27874;&#22120;&#32452;&#65288;FB&#65289;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#30340;&#20809;&#35889;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;ISVAE&#65289;&#12290;&#36890;&#36807;&#32422;&#26463;VAE&#30340;&#36776;&#35782;&#33021;&#21147;&#65292;&#36825;&#20010;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#20855;&#26377;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#21644;&#32858;&#31867;&#33021;&#21147;&#30340;&#26032;&#32534;&#30721;f_0&#65292;&#24182;&#21576;&#29616;&#20026;&#19968;&#20010;&#21160;&#24577;&#30340;&#20998;&#23618;&#26641;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19982;&#28388;&#27874;&#22120;&#32452;&#32467;&#26500;&#23545;&#31216;&#23545;&#40784;&#30340;&#23450;&#21046;&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#25968;&#25454;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2310.11940</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20809;&#35889;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;ISVAE&#65289;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Interpretable Spectral Variational AutoEncoder (ISVAE) for time series clustering. (arXiv:2310.11940v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11940
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#29942;&#39048;&#65292;&#31216;&#20026;&#28388;&#27874;&#22120;&#32452;&#65288;FB&#65289;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#30340;&#20809;&#35889;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;ISVAE&#65289;&#12290;&#36890;&#36807;&#32422;&#26463;VAE&#30340;&#36776;&#35782;&#33021;&#21147;&#65292;&#36825;&#20010;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#20855;&#26377;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#21644;&#32858;&#31867;&#33021;&#21147;&#30340;&#26032;&#32534;&#30721;f_0&#65292;&#24182;&#21576;&#29616;&#20026;&#19968;&#20010;&#21160;&#24577;&#30340;&#20998;&#23618;&#26641;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19982;&#28388;&#27874;&#22120;&#32452;&#32467;&#26500;&#23545;&#31216;&#23545;&#40784;&#30340;&#23450;&#21046;&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#25968;&#25454;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22909;&#30340;&#32534;&#30721;&#26159;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#21021;&#26399;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#29942;&#39048;&#65292;&#31216;&#20026;&#28388;&#27874;&#22120;&#32452;&#65288;FB&#65289;&#12290;&#36825;&#31181;&#23433;&#25490;&#36843;&#20351;VAE&#20851;&#27880;&#36755;&#20837;&#20449;&#21495;&#20013;&#26368;&#26377;&#20449;&#24687;&#30340;&#29255;&#27573;&#65292;&#20419;&#36827;&#20102;&#26032;&#32534;&#30721;f_0&#30340;&#23398;&#20064;&#65292;&#20351;&#20854;&#22312;&#20256;&#32479;&#30340;&#28508;&#21464;&#37327;&#31354;&#38388;&#19978;&#20855;&#26377;&#22686;&#24378;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#32858;&#31867;&#33021;&#21147;&#12290;&#36890;&#36807;&#26377;&#24847;&#22320;&#32422;&#26463;VAE&#20351;&#29992;&#36825;&#20010;&#28388;&#27874;&#22120;&#32452;&#65292;&#25105;&#20204;&#26377;&#24847;&#22320;&#38480;&#21046;&#20102;&#23427;&#35775;&#38382;&#24191;&#27867;&#36755;&#20837;&#22495;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#19968;&#20010;&#21487;&#36776;&#35782;&#12289;&#21487;&#20998;&#31163;&#19988;&#38477;&#20302;&#32500;&#24230;&#30340;&#32534;&#30721;&#30340;&#21457;&#23637;&#12290;f_0&#30340;&#36827;&#21270;&#23398;&#20064;&#36712;&#36857;&#36827;&#19968;&#27493;&#34920;&#29616;&#20026;&#19968;&#20010;&#21160;&#24577;&#30340;&#20998;&#23618;&#26641;&#65292;&#25552;&#20379;&#20102;&#23545;&#32858;&#31867;&#30456;&#20284;&#24615;&#30340;&#28145;&#21051;&#27934;&#23519;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22788;&#29702;&#22797;&#26434;&#30340;&#25968;&#25454;&#37197;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#28388;&#27874;&#22120;&#32452;&#32467;&#26500;&#23545;&#31216;&#23545;&#40784;&#30340;&#23450;&#21046;&#35299;&#30721;&#22120;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The best encoding is the one that is interpretable in nature. In this work, we introduce a novel model that incorporates an interpretable bottleneck-termed the Filter Bank (FB)-at the outset of a Variational Autoencoder (VAE). This arrangement compels the VAE to attend on the most informative segments of the input signal, fostering the learning of a novel encoding ${f_0}$ which boasts enhanced interpretability and clusterability over traditional latent spaces. By deliberately constraining the VAE with this FB, we intentionally constrict its capacity to access broad input domain information, promoting the development of an encoding that is discernible, separable, and of reduced dimensionality. The evolutionary learning trajectory of ${f_0}$ further manifests as a dynamic hierarchical tree, offering profound insights into cluster similarities. Additionally, for handling intricate data configurations, we propose a tailored decoder structure that is symmetrically aligned with FB's architec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#20013;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#65292;&#22522;&#20110;Wikidata5M&#36827;&#34892;&#25193;&#23637;&#12290;&#36890;&#36807;&#21508;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#20449;&#24687;&#32452;&#21512;&#65292;&#35813;&#22522;&#20934;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#19978;&#19979;&#25991;&#21644;&#25991;&#26412;&#20449;&#24687;&#22312;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#25972;&#21512;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2310.11917</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs. (arXiv:2310.11917v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#20013;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#65292;&#22522;&#20110;Wikidata5M&#36827;&#34892;&#25193;&#23637;&#12290;&#36890;&#36807;&#21508;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#20449;&#24687;&#32452;&#21512;&#65292;&#35813;&#22522;&#20934;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#19978;&#19979;&#25991;&#21644;&#25991;&#26412;&#20449;&#24687;&#22312;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#25972;&#21512;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#26159;&#26681;&#25454;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#39044;&#27979;&#26032;&#30340;&#12289;&#20043;&#21069;&#26410;&#35265;&#30340;&#23454;&#20307;&#30340;&#20107;&#23454;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#21644;&#25551;&#36848;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22522;&#20934;&#26469;&#35780;&#20272;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#22522;&#20934;&#22522;&#20110;&#24182;&#25193;&#23637;&#20102;Wikidata5M&#65306;&#23427;&#25552;&#20379;&#20102;&#36716;&#23548;&#24335;&#12289;k-shot&#21644;0-shot&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#20250;&#26681;&#25454;&#21487;&#29992;&#30340;&#20449;&#24687;&#24773;&#20917;&#20174;&#65288;i&#65289;&#20165;&#26377;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#65292;&#21040;&#65288;ii&#65289;&#21253;&#21547;&#25991;&#26412;&#25552;&#21450;&#65292;&#20877;&#21040;&#65288;iii&#65289;&#23454;&#20307;&#30340;&#35814;&#32454;&#25551;&#36848;&#36827;&#34892;&#21464;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#20110;&#26368;&#36817;&#26041;&#27861;&#30340;&#23567;&#22411;&#30740;&#31350;&#65292;&#21457;&#29616;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#30340;&#24615;&#33021;&#36828;&#36828;&#20302;&#20110;&#36716;&#23548;&#24335;&#24615;&#33021;&#65292;&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#37117;&#34920;&#29616;&#20986;&#23545;&#20110;&#38271;&#23614;&#23454;&#20307;&#30340;&#19981;&#36275;&#12290;&#35813;&#22522;&#20934;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#23558;&#19978;&#19979;&#25991;&#21644;&#25991;&#26412;&#20449;&#24687;&#25972;&#21512;&#21040;&#38142;&#25509;&#39044;&#27979;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-inductive link prediction (LP) in knowledge graphs (KG) is the task of predicting facts for new, previously unseen entities based on context information. Although new entities can be integrated by retraining the model from scratch in principle, such an approach is infeasible for large-scale KGs, where retraining is expensive and new entities may arise frequently. In this paper, we propose and describe a large-scale benchmark to evaluate semi-inductive LP models. The benchmark is based on and extends Wikidata5M: It provides transductive, k-shot, and 0-shot LP tasks, each varying the available information from (i) only KG structure, to (ii) including textual mentions, and (iii) detailed descriptions of the entities. We report on a small study of recent approaches and found that semi-inductive LP performance is far from transductive performance on long-tail entities throughout all experiments. The benchmark provides a test bed for further research into integrating context and textual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#27874;&#27744;&#21270;&#36793;&#32536;&#20445;&#30041;&#33258;&#32534;&#30721;&#22120;&#30340;&#26080;&#30417;&#30563;&#22810;&#27169;&#24335;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#29305;&#24449;&#25552;&#21462;&#21644;&#20449;&#24687;&#20445;&#30041;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11910</link><description>&lt;p&gt;
&#20351;&#29992;&#23567;&#27874;&#27744;&#21270;&#36793;&#32536;&#20445;&#30041;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#22810;&#27169;&#24335;&#21307;&#23398;&#31070;&#32463;&#22270;&#20687;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Medical Neurological Image Fusion using Wavelet Pooled Edge Preserving Autoencoder. (arXiv:2310.11910v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#27874;&#27744;&#21270;&#36793;&#32536;&#20445;&#30041;&#33258;&#32534;&#30721;&#22120;&#30340;&#26080;&#30417;&#30563;&#22810;&#27169;&#24335;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#29305;&#24449;&#25552;&#21462;&#21644;&#20449;&#24687;&#20445;&#30041;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#25972;&#21512;&#20102;&#28304;&#22270;&#20687;&#27169;&#24577;&#30340;&#20114;&#34917;&#35786;&#26029;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;&#23545;&#28508;&#22312;&#24322;&#24120;&#30340;&#21487;&#35270;&#21270;&#21644;&#20998;&#26512;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#36890;&#36807;&#21516;&#26102;&#25191;&#34892;&#29305;&#24449;&#25552;&#21462;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#29305;&#24449;&#34701;&#21512;&#20219;&#21153;&#65292;&#32988;&#36807;&#20256;&#32479;&#30340;&#34701;&#21512;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#20351;&#29992;&#20256;&#32479;&#30340;&#27744;&#21270;&#25110;&#36328;&#27493;&#21367;&#31215;&#31574;&#30053;&#23545;&#29305;&#24449;&#22270;&#36827;&#34892;&#19979;&#37319;&#26679;&#12290;&#36825;&#20250;&#23548;&#33268;&#28304;&#22270;&#20687;&#20013;&#30340;&#37325;&#35201;&#35786;&#26029;&#20449;&#24687;&#21644;&#36793;&#32536;&#32454;&#33410;&#30340;&#27169;&#31946;&#25110;&#20002;&#22833;&#65292;&#24182;&#31232;&#37322;&#20102;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#30340;&#25928;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20445;&#36793;&#31264;&#23494;&#33258;&#32534;&#30721;&#22120;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#26080;&#30417;&#30563;&#34701;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#27169;&#24335;&#21307;&#23398;&#22270;&#20687;&#12290;&#22312;&#25552;&#20986;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#27874;&#20998;&#35299;&#22522;&#20110;&#27880;&#24847;&#26426;&#21046;&#27744;&#21270;&#29305;&#24449;&#22270;&#26469;&#25913;&#21892;&#29305;&#24449;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image fusion integrates the complementary diagnostic information of the source image modalities for improved visualization and analysis of underlying anomalies. Recently, deep learning-based models have excelled the conventional fusion methods by executing feature extraction, feature selection, and feature fusion tasks, simultaneously. However, most of the existing convolutional neural network (CNN) architectures use conventional pooling or strided convolutional strategies to downsample the feature maps. It causes the blurring or loss of important diagnostic information and edge details available in the source images and dilutes the efficacy of the feature extraction process. Therefore, this paper presents an end-to-end unsupervised fusion model for multimodal medical images based on an edge-preserving dense autoencoder network. In the proposed model, feature extraction is improved by using wavelet decomposition-based attention pooling of feature maps. This helps in preserving 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#36895;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;Nesterov&#21160;&#37327;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#26356;&#24555;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#22312;softmax&#31574;&#30053;&#21442;&#25968;&#21270;&#20013;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#23427;&#20197; $\tilde{O}(1/t^2)$ &#30340;&#36895;&#29575;&#25910;&#25947;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;Nesterov&#21152;&#36895;&#26799;&#24230;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.11897</link><description>&lt;p&gt;
&#21152;&#36895;&#31574;&#30053;&#26799;&#24230;&#65306;&#20851;&#20110;&#24212;&#29992;Nesterov&#21160;&#37327;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning. (arXiv:2310.11897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#36895;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;Nesterov&#21160;&#37327;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#26356;&#24555;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#22312;softmax&#31574;&#30053;&#21442;&#25968;&#21270;&#20013;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#23427;&#20197; $\tilde{O}(1/t^2)$ &#30340;&#36895;&#29575;&#25910;&#25947;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;Nesterov&#21152;&#36895;&#26799;&#24230;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#34920;&#26126;&#65292;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22312;&#38750;&#27491;&#21017;&#21270;&#34920;&#26684;softmax&#35774;&#32622;&#20013;&#20197;&#920;(1/t)&#30340;&#36895;&#29575;&#20840;&#23616;&#25910;&#25947;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20165;&#20351;&#29992;&#19968;&#38454;&#26356;&#26032;&#36827;&#19968;&#27493;&#25913;&#36827;&#36825;&#31181;&#25910;&#25947;&#36895;&#24230;&#12290;&#26412;&#25991;&#20174;&#21160;&#37327;&#30340;&#35282;&#24230;&#22238;&#31572;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#33879;&#21517;&#30340;Nesterov&#21152;&#36895;&#26799;&#24230;&#65288;NAG&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#31216;&#20043;&#20026; \textit{&#21152;&#36895;&#31574;&#30053;&#26799;&#24230;}&#65288;APG&#65289;&#12290;&#20026;&#20102;&#23637;&#31034;APG&#22312;&#23454;&#29616;&#26356;&#24555;&#20840;&#23616;&#25910;&#25947;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#20351;&#29992;&#30495;&#23454;&#26799;&#24230;&#26102;&#65292;&#20855;&#26377; softmax &#31574;&#30053;&#21442;&#25968;&#21270;&#30340;APG&#20197; $\tilde{O}(1/t^2)$ &#30340;&#36895;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;NAG&#22312;RL&#39046;&#22495;&#20013;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#30340;&#31532;&#19968;&#20010;&#34920;&#24449;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#20381;&#36182;&#20110;&#19968;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;&#19981;&#35770;&#21021;&#22987;&#21270;&#22914;&#20309;&#65292;APG&#26368;&#32456;&#21487;&#20197;&#36798;&#21040;&#36817;&#20046;&#23616;&#37096;&#25910;&#25947;&#30340;&#22320;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy gradient methods have recently been shown to enjoy global convergence at a $\Theta(1/t)$ rate in the non-regularized tabular softmax setting. Accordingly, one important research question is whether this convergence rate can be further improved, with only first-order updates. In this paper, we answer the above question from the perspective of momentum by adapting the celebrated Nesterov's accelerated gradient (NAG) method to reinforcement learning (RL), termed \textit{Accelerated Policy Gradient} (APG). To demonstrate the potential of APG in achieving faster global convergence, we formally show that with the true gradient, APG with softmax policy parametrization converges to an optimal policy at a $\tilde{O}(1/t^2)$ rate. To the best of our knowledge, this is the first characterization of the global convergence rate of NAG in the context of RL. Notably, our analysis relies on one interesting finding: Regardless of the initialization, APG could end up reaching a locally nearly-con
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#36947;&#27880;&#24847;&#21147;&#30340;&#25289;&#26222;&#25289;&#26031;&#33258;&#32534;&#30721;&#22120;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20445;&#30041;&#34917;&#20805;&#20449;&#24687;&#21644;&#37325;&#35201;&#30340;&#32452;&#32455;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.11896</link><description>&lt;p&gt;
&#22522;&#20110;&#36890;&#36947;&#27880;&#24847;&#21147;&#30340;&#25289;&#26222;&#25289;&#26031;&#33258;&#32534;&#30721;&#22120;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
A New Multimodal Medical Image Fusion based on Laplacian Autoencoder with Channel Attention. (arXiv:2310.11896v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#36947;&#27880;&#24847;&#21147;&#30340;&#25289;&#26222;&#25289;&#26031;&#33258;&#32534;&#30721;&#22120;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20445;&#30041;&#34917;&#20805;&#20449;&#24687;&#21644;&#37325;&#35201;&#30340;&#32452;&#32455;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#32467;&#21512;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#20197;&#21327;&#21161;&#21307;&#23398;&#19987;&#19994;&#20154;&#21592;&#22312;&#20020;&#24202;&#35786;&#26029;&#20013;&#30830;&#35786;&#24739;&#32773;&#30142;&#30149;&#65292;&#24182;&#22312;&#26415;&#21069;&#21644;&#26415;&#20013;&#25552;&#20379;&#25351;&#23548;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#20855;&#26377;&#39640;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#31471;&#21040;&#31471;&#22270;&#20687;&#34701;&#21512;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34701;&#21512;&#27169;&#22411;&#23545;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#19979;&#37319;&#26679;&#65292;&#20197;&#26368;&#23567;&#21270;&#21487;&#23398;&#20064;&#21442;&#25968;&#21644;&#35745;&#31639;&#37327;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#28304;&#22270;&#20687;&#30340;&#26174;&#33879;&#29305;&#24449;&#21464;&#24471;&#26080;&#27861;&#24674;&#22797;&#65292;&#23548;&#33268;&#20002;&#22833;&#20851;&#38190;&#30340;&#35786;&#26029;&#36793;&#32536;&#32454;&#33410;&#21644;&#21508;&#31181;&#33041;&#32452;&#32455;&#30340;&#23545;&#27604;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25972;&#21512;&#30340;&#25289;&#26222;&#25289;&#26031;-&#39640;&#26031;&#32423;&#32852;&#21644;&#27880;&#24847;&#21147;&#27744;&#21270;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#34917;&#20805;&#20449;&#24687;&#21644;&#37325;&#35201;&#30340;&#32452;&#32455;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image fusion combines the complementary information of multimodal medical images to assist medical professionals in the clinical diagnosis of patients' disorders and provide guidance during preoperative and intra-operative procedures. Deep learning (DL) models have achieved end-to-end image fusion with highly robust and accurate fusion performance. However, most DL-based fusion models perform down-sampling on the input images to minimize the number of learnable parameters and computations. During this process, salient features of the source images become irretrievable leading to the loss of crucial diagnostic edge details and contrast of various brain tissues. In this paper, we propose a new multimodal medical image fusion model is proposed that is based on integrated Laplacian-Gaussian concatenation with attention pooling (LGCA). We prove that our model preserves effectively complementary information and important tissue structures.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#32463;&#20856;&#26680;&#19982;&#37327;&#23376;&#26680;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.11891</link><description>&lt;p&gt;
&#37327;&#23376;&#26680;&#26041;&#27861;&#30340;&#36229;&#21442;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Hyperparameter Study for Quantum Kernel Methods. (arXiv:2310.11891v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#32463;&#20856;&#26680;&#19982;&#37327;&#23376;&#26680;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26680;&#26041;&#27861;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#22240;&#20026;&#19982;&#20043;&#30456;&#20851;&#30340;&#20445;&#35777;&#12290;&#23427;&#20204;&#30340;&#21487;&#35775;&#38382;&#24615;&#20063;&#25171;&#24320;&#20102;&#22522;&#20110;&#28508;&#22312;&#37327;&#23376;&#20248;&#21183;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#20808;&#31579;&#36873;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#26089;&#26399;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#20960;&#20309;&#24046;&#24322;&#65292;&#23427;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#20004;&#31181;&#22522;&#20110;&#26680;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#25509;&#36817;&#24230;&#24230;&#37327;&#65292;&#29305;&#21035;&#26159;&#37327;&#23376;&#26680;&#21644;&#32463;&#20856;&#26680;&#20043;&#38388;&#30340;&#25509;&#36817;&#24230;&#12290;&#35813;&#24230;&#37327;&#25351;&#31034;&#20102;&#37327;&#23376;&#21644;&#32463;&#20856;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22240;&#27492;&#65292;&#23427;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#19982;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#20851;&#31995;&#65292;&#20960;&#20309;&#24046;&#24322;&#26159;&#21542;&#21487;&#20197;&#25104;&#20026;&#38500;&#20102;&#28508;&#22312;&#30340;&#37327;&#23376;&#20248;&#21183;&#20043;&#22806;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#32463;&#20856;&#26680;&#19982;&#37327;&#23376;&#26680;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35828;&#65292;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#37325;&#35201;&#24615;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum kernel methods are a promising method in quantum machine learning thanks to the guarantees connected to them. Their accessibility for analytic considerations also opens up the possibility of prescreening datasets based on their potential for a quantum advantage. To do so, earlier works developed the geometric difference, which can be understood as a closeness measure between two kernel-based machine learning approaches, most importantly between a quantum kernel and classical kernel. This metric links the quantum and classical model complexities. Therefore, it raises the question of whether the geometric difference, based on its relation to model complexity, can be a useful tool in evaluations other than for the potential for quantum advantage. In this work, we investigate the effects of hyperparameter choice on the model performance and the generalization gap between classical and quantum kernels. The importance of hyperparameter optimization is well known also for classical ma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20174;&#25429;&#25417;&#30340;&#27969;&#37327;&#36319;&#36394;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#26032;&#39062;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#30495;&#23454;&#32593;&#32476;&#22330;&#26223;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11889</link><description>&lt;p&gt;
&#20174;&#25429;&#25417;&#30340;&#27969;&#37327;&#36319;&#36394;&#26500;&#24314;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Building a Graph-based Deep Learning network model from captured traffic traces. (arXiv:2310.11889v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20174;&#25429;&#25417;&#30340;&#27969;&#37327;&#36319;&#36394;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#26032;&#39062;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#30495;&#23454;&#32593;&#32476;&#22330;&#26223;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#27169;&#22411;&#22522;&#20110;&#31163;&#25955;&#20107;&#20214;&#20223;&#30495;&#65288;DES&#65289;&#12290;&#34429;&#28982;DES&#38750;&#24120;&#31934;&#30830;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#19988;&#38590;&#20197;&#24182;&#34892;&#21270;&#65292;&#20351;&#24471;&#27169;&#25311;&#39640;&#24615;&#33021;&#32593;&#32476;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#27492;&#22806;&#65292;&#27169;&#25311;&#22330;&#26223;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#30495;&#23454;&#32593;&#32476;&#22330;&#26223;&#20013;&#30340;&#25152;&#26377;&#22797;&#26434;&#24615;&#12290;&#34429;&#28982;&#23384;&#22312;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#30340;&#32593;&#32476;&#27169;&#22411;&#20197;&#20943;&#23569;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20063;&#26159;&#21033;&#29992;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#30456;&#21516;&#30340;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#25361;&#25112;&#36187;2023&#24341;&#20837;&#20102;&#19968;&#32452;&#25429;&#25417;&#30340;&#27969;&#37327;&#36319;&#36394;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#29992;&#26469;&#26500;&#24314;&#27809;&#26377;&#36825;&#20123;&#38480;&#21046;&#30340;&#22522;&#20110;ML&#30340;&#32593;&#32476;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#26356;&#22909;&#22320;&#25429;&#25417;&#30495;&#23454;&#32593;&#32476;&#22330;&#26223;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#26041;&#27861;&#26469;&#20174;&#25429;&#25417;&#30340;&#25968;&#25454;&#21253;&#24207;&#21015;&#20013;&#25429;&#25417;&#20449;&#24687;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently the state of the art network models are based or depend on Discrete Event Simulation (DES). While DES is highly accurate, it is also computationally costly and cumbersome to parallelize, making it unpractical to simulate high performance networks. Additionally, simulated scenarios fail to capture all of the complexities present in real network scenarios. While there exists network models based on Machine Learning (ML) techniques to minimize these issues, these models are also trained with simulated data and hence vulnerable to the same pitfalls. Consequently, the Graph Neural Networking Challenge 2023 introduces a dataset of captured traffic traces that can be used to build a ML-based network model without these limitations. In this paper we propose a Graph Neural Network (GNN)-based solution specifically designed to better capture the complexities of real network scenarios. This is done through a novel encoding method to capture information from the sequence of captured pack
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20998;&#26512;&#36136;&#35889;&#25968;&#25454;&#20197;&#26816;&#27979;&#21476;&#20195;&#28779;&#26143;&#36866;&#23621;&#24615;&#28508;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#22806;&#26143;&#29289;&#36136;&#20998;&#26512;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20851;&#38190;&#25216;&#26415;&#21253;&#25324;&#36136;&#35889;&#20540;&#30340;&#36716;&#25442;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.11888</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#26234;&#33021;&#20998;&#26512;&#36136;&#35889;&#25968;&#25454;&#20197;&#36741;&#21161;&#29702;&#35299;&#28779;&#26143;&#30340;&#36807;&#21435;&#36866;&#23621;&#24615;&#24182;&#25552;&#20379;&#26410;&#26469;&#20219;&#21153;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Analyze Mass Spectrometry data with Artificial Intelligence to assist the understanding of past habitability of Mars and provide insights for future missions. (arXiv:2310.11888v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20998;&#26512;&#36136;&#35889;&#25968;&#25454;&#20197;&#26816;&#27979;&#21476;&#20195;&#28779;&#26143;&#36866;&#23621;&#24615;&#28508;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#22806;&#26143;&#29289;&#36136;&#20998;&#26512;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20851;&#38190;&#25216;&#26415;&#21253;&#25324;&#36136;&#35889;&#20540;&#30340;&#36716;&#25442;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#36136;&#35889;&#25968;&#25454;&#20197;&#26816;&#27979;&#28779;&#26143;&#21476;&#20195;&#36866;&#23621;&#24615;&#28508;&#21147;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#25968;&#25454;&#26159;&#38024;&#23545;&#28779;&#26143;&#25910;&#38598;&#30340;&#65292;&#20294;&#21516;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22826;&#38451;&#31995;&#20013;&#30340;&#20219;&#20309;&#22320;&#29699;&#23545;&#35937;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#20219;&#20309;&#20351;&#29992;&#36136;&#35889;&#30340;&#39046;&#22495;&#12290;&#30740;&#31350;&#38598;&#20013;&#20110;&#20004;&#31181;&#36136;&#35889;&#25216;&#26415;&#65288;&#36827;&#21270;&#27668;&#20307;&#20998;&#26512;-&#36136;&#35889;&#21644;&#27668;&#30456;&#33394;&#35889;-&#36136;&#35889;&#65289;&#30340;&#25968;&#25454;&#20998;&#26512;&#65292;&#36825;&#20123;&#25216;&#26415;&#29992;&#20110;&#35782;&#21035;&#22320;&#36136;&#26679;&#21697;&#20013;&#30340;&#29305;&#23450;&#21270;&#23398;&#21270;&#21512;&#29289;&#12290;&#30740;&#31350;&#35777;&#26126;&#20102;&#36827;&#21270;&#27668;&#20307;&#20998;&#26512;-&#36136;&#35889;&#21644;&#27668;&#30456;&#33394;&#35889;-&#36136;&#35889;&#25968;&#25454;&#22312;&#22806;&#26143;&#29289;&#36136;&#20998;&#26512;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#21253;&#25324;&#36136;&#35889;&#20540;&#30340;&#24179;&#26041;&#26681;&#36716;&#25442;&#65292;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#20108;&#32500;&#20809;&#35889;&#22270;&#65292;&#24182;&#21033;&#29992;&#29305;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25216;&#26415;&#20197;&#36991;&#20813;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#36807;&#24230;&#25311;&#21512;&#12290;EGA-MS&#21644;GC-MS&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
This paper presents an application of artificial intelligence on mass spectrometry data for detecting habitability potential of ancient Mars. Although data was collected for planet Mars the same approach can be replicated for any terrestrial object of our solar system. Furthermore, proposed methodology can be adapted to any domain that uses mass spectrometry. This research is focused in data analysis of two mass spectrometry techniques, evolved gas analysis (EGA-MS) and gas chromatography (GC-MS), which are used to identify specific chemical compounds in geological material samples. The study demonstrates the applicability of EGA-MS and GC-MS data to extra-terrestrial material analysis. Most important features of proposed methodology includes square root transformation of mass spectrometry values, conversion of raw data to 2D sprectrograms and utilization of specific machine learning models and techniques to avoid overfitting on relative small datasets. Both EGA-MS and GC-MS datasets c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#22522;&#20110;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2310.11884</link><description>&lt;p&gt;
&#20174;&#31070;&#32463;&#28608;&#27963;&#21040;&#27010;&#24565;: &#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27010;&#24565;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks. (arXiv:2310.11884v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#22522;&#20110;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#27010;&#24565;&#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#21644;&#25512;&#29702;&#20043;&#38388;&#30340;&#33258;&#28982;&#26725;&#26753;&#65306;&#19968;&#26086;&#30830;&#23450;&#20102;&#31070;&#32463;&#23398;&#20064;&#31995;&#32479;&#20351;&#29992;&#30340;&#27010;&#24565;&#65292;&#23601;&#21487;&#20197;&#23558;&#36825;&#20123;&#27010;&#24565;&#19982;&#25512;&#29702;&#31995;&#32479;&#25972;&#21512;&#65292;&#29992;&#20110;&#25512;&#29702;&#25110;&#20351;&#29992;&#25512;&#29702;&#31995;&#32479;&#23545;&#20854;&#36827;&#34892;&#25913;&#36827;&#25110;&#22686;&#24378;&#20197;&#25913;&#21892;&#23398;&#20064;&#31995;&#32479;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19981;&#20165;&#21487;&#20197;&#20174;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#36824;&#21487;&#20197;&#23558;&#27010;&#24565;&#30693;&#35782;&#25554;&#20837;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20013;&#12290;&#30001;&#20110;&#25972;&#21512;&#23398;&#20064;&#21644;&#25512;&#29702;&#26159;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#65292;&#25152;&#20197;&#36890;&#36807;&#36825;&#39033;&#35843;&#26597;&#33719;&#24471;&#30340;&#35265;&#35299;&#21487;&#20197;&#25104;&#20026;&#23454;&#29616;&#22522;&#20110;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we review recent approaches for explaining concepts in neural networks. Concepts can act as a natural link between learning and reasoning: once the concepts are identified that a neural learning system uses, one can integrate those concepts with a reasoning system for inference or use a reasoning system to act upon them to improve or enhance the learning system. On the other hand, knowledge can not only be extracted from neural networks but concept knowledge can also be inserted into neural network architectures. Since integrating learning and reasoning is at the core of neuro-symbolic AI, the insights gained from this survey can serve as an important step towards realizing neuro-symbolic AI based on explainable concepts.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#22810;&#26799;&#24230;&#19979;&#38477;&#65288;OMGD&#65289;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#20108;&#27425;&#21644;&#32447;&#24615;&#24320;&#20851;&#25104;&#26412;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#31454;&#20105;&#27604;&#29575;&#19978;&#30028;&#65292;&#24182;&#22312;&#26377;&#38480;&#20449;&#24687;&#35774;&#32622;&#19979;&#36798;&#21040;&#20102;&#26368;&#20248;&#65288;&#25353;&#39034;&#24207;&#65289;&#30340;&#21160;&#24577;&#21518;&#24724;&#12290;</title><link>http://arxiv.org/abs/2310.11880</link><description>&lt;p&gt;
&#20855;&#26377;&#24320;&#20851;&#25104;&#26412;&#21644;&#24310;&#36831;&#26799;&#24230;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Convex Optimization with Switching Cost and Delayed Gradients. (arXiv:2310.11880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11880
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#22810;&#26799;&#24230;&#19979;&#38477;&#65288;OMGD&#65289;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#20108;&#27425;&#21644;&#32447;&#24615;&#24320;&#20851;&#25104;&#26412;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#31454;&#20105;&#27604;&#29575;&#19978;&#30028;&#65292;&#24182;&#22312;&#26377;&#38480;&#20449;&#24687;&#35774;&#32622;&#19979;&#36798;&#21040;&#20102;&#26368;&#20248;&#65288;&#25353;&#39034;&#24207;&#65289;&#30340;&#21160;&#24577;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#26377;&#38480;&#20449;&#24687;&#35774;&#32622;&#19979;&#20855;&#26377;&#20108;&#27425;&#21644;&#32447;&#24615;&#24320;&#20851;&#25104;&#26412;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#36825;&#37324;&#22312;&#32447;&#31639;&#27861;&#20165;&#33021;&#21033;&#29992;&#20808;&#21069;&#30446;&#26631;&#20989;&#25968;&#30340;&#26799;&#24230;&#20449;&#24687;&#36827;&#34892;&#21160;&#20316;&#36873;&#25321;&#12290;&#23545;&#20110;$L$-&#20809;&#28369;&#21644;$\mu$-&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#22810;&#26799;&#24230;&#19979;&#38477;&#65288;OMGD&#65289;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#20855;&#26377;&#20108;&#27425;&#24320;&#20851;&#25104;&#26412;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#31454;&#20105;&#27604;&#29575;&#26368;&#22810;&#20026;$4(L+5)+\frac{16(L+5)}{\mu}$&#12290;&#23545;&#20110;OMGD&#30340;&#31454;&#20105;&#27604;&#29575;&#19978;&#30028;&#20063;&#34987;&#35777;&#26126;&#22312;$L$&#21644;$\mu$&#26041;&#38754;&#26159;&#32039;&#33268;&#30340;&#12290;&#27492;&#22806;&#65292;&#24403;&#24320;&#20851;&#25104;&#26412;&#20026;&#20108;&#27425;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#20309;&#22312;&#32447;&#31639;&#27861;&#30340;&#31454;&#20105;&#27604;&#29575;&#26159;$\max\{\Omega(L), \Omega(\frac{L}{\sqrt{\mu}})\}$&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#20449;&#24687;&#35774;&#32622;&#19979;&#65292;OMGD&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#20248;&#65288;&#25353;&#39034;&#24207;&#65289;&#30340;&#21160;&#24577;&#21518;&#24724;&#12290;&#23545;&#20110;&#32447;&#24615;&#24320;&#20851;&#25104;&#26412;&#65292;
&lt;/p&gt;
&lt;p&gt;
We consider the online convex optimization (OCO) problem with quadratic and linear switching cost in the limited information setting, where an online algorithm can choose its action using only gradient information about the previous objective function. For $L$-smooth and $\mu$-strongly convex objective functions, we propose an online multiple gradient descent (OMGD) algorithm and show that its competitive ratio for the OCO problem with quadratic switching cost is at most $4(L + 5) + \frac{16(L + 5)}{\mu}$. The competitive ratio upper bound for OMGD is also shown to be order-wise tight in terms of $L,\mu$. In addition, we show that the competitive ratio of any online algorithm is $\max\{\Omega(L), \Omega(\frac{L}{\sqrt{\mu}})\}$ in the limited information setting when the switching cost is quadratic. We also show that the OMGD algorithm achieves the optimal (order-wise) dynamic regret in the limited information setting. For the linear switching cost, the competitive ratio upper bound of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#32447;&#24615;&#20998;&#31867;&#22120;&#28151;&#21512;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#30340;&#32479;&#35745;&#26597;&#35810;&#65288;SQ&#65289;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#19979;&#30028;&#26159;$n^{\mathrm{poly}(1/\Delta) \log(r)}$&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#20852;&#36259;&#30340;&#26032;&#29699;&#38754;&#35774;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11876</link><description>&lt;p&gt;
&#23398;&#20064;&#32447;&#24615;&#20998;&#31867;&#22120;&#28151;&#21512;&#30340;SQ&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
SQ Lower Bounds for Learning Mixtures of Linear Classifiers. (arXiv:2310.11876v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#32447;&#24615;&#20998;&#31867;&#22120;&#28151;&#21512;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#30340;&#32479;&#35745;&#26597;&#35810;&#65288;SQ&#65289;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#19979;&#30028;&#26159;$n^{\mathrm{poly}(1/\Delta) \log(r)}$&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#20852;&#36259;&#30340;&#26032;&#29699;&#38754;&#35774;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39640;&#26031;&#21327;&#21464;&#37327;&#19979;&#23398;&#20064;&#32447;&#24615;&#20998;&#31867;&#22120;&#28151;&#21512;&#30340;&#38382;&#39064;&#12290;&#32473;&#23450;&#23545;&#24418;&#24335;&#20026;$(\mathbf{x},y_{\ell})$&#30340;$n$&#32500;&#39640;&#26031;&#20998;&#24067;&#30340;$r$&#20010;&#28151;&#21512;&#20998;&#24067;&#26679;&#26412;&#35775;&#38382;&#26435;&#38480;&#65292;&#20854;&#20013;$\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_n)$&#65292;$y_\ell=\mathrm{sign}(\langle\mathbf{v}_\ell,\mathbf{x}\rangle)$&#65292;&#30446;&#26631;&#26159;&#20197;&#24635;&#21464;&#24322;&#36317;&#31163;&#23398;&#20064;&#28508;&#22312;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#32479;&#35745;&#26597;&#35810;&#65288;SQ&#65289;&#30340;&#19979;&#30028;&#65292;&#34920;&#26126;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#30340;&#24050;&#30693;&#31639;&#27861;&#23454;&#38469;&#19978;&#26159;&#26368;&#22909;&#30340;&#65292;&#21363;&#20351;&#23545;&#20110;&#22343;&#21248;&#28151;&#21512;&#30340;&#29305;&#27530;&#24773;&#20917;&#20063;&#26159;&#22914;&#27492;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#35813;&#38382;&#39064;&#30340;&#20219;&#20309;SQ&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#37117;&#26159;$n^{\mathrm{poly}(1/\Delta) \log(r)}$&#65292;&#20854;&#20013;$\Delta$&#26159;$\mathbf{v}_\ell$&#20043;&#38388;&#30340;&#20004;&#20004;$\ell_2$-&#20998;&#31163;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#32467;&#26524;&#30340;&#20851;&#38190;&#25216;&#26415;&#26500;&#24314;&#26159;&#19968;&#31181;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#20852;&#36259;&#30340;&#26032;&#29699;&#38754;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning mixtures of linear classifiers under Gaussian covariates. Given sample access to a mixture of $r$ distributions on $\mathbb{R}^n$ of the form $(\mathbf{x},y_{\ell})$, $\ell\in [r]$, where $\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_n)$ and $y_\ell=\mathrm{sign}(\langle\mathbf{v}_\ell,\mathbf{x}\rangle)$ for an unknown unit vector $\mathbf{v}_\ell$, the goal is to learn the underlying distribution in total variation distance. Our main result is a Statistical Query (SQ) lower bound suggesting that known algorithms for this problem are essentially best possible, even for the special case of uniform mixtures. In particular, we show that the complexity of any SQ algorithm for the problem is $n^{\mathrm{poly}(1/\Delta) \log(r)}$, where $\Delta$ is a lower bound on the pairwise $\ell_2$-separation between the $\mathbf{v}_\ell$'s. The key technical ingredient underlying our result is a new construction of spherical designs that may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#25968;&#27010;&#24565;&#20462;&#25913;&#28608;&#27963;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#24182;&#25552;&#39640;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11875</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20998;&#25968;&#27010;&#24565;&#65306;&#22686;&#36827;&#28608;&#27963;&#21644;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Fractional Concepts in Neural Networks: Enhancing Activation and Loss Functions. (arXiv:2310.11875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#25968;&#27010;&#24565;&#20462;&#25913;&#28608;&#27963;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#24182;&#25552;&#39640;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#20998;&#25968;&#27010;&#24565;&#20462;&#25913;&#28608;&#27963;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#30830;&#23450;&#35757;&#32451;&#36807;&#31243;&#30340;&#20998;&#25968;&#23548;&#25968;&#38454;&#25968;&#20316;&#20026;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#26469;&#23450;&#20041;&#21644;&#20248;&#21270;&#20854;&#28608;&#27963;&#20989;&#25968;&#12290;&#36825;&#23558;&#20351;&#24471;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#20803;&#33021;&#22815;&#35843;&#25972;&#20854;&#28608;&#27963;&#20989;&#25968;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#24182;&#20943;&#23569;&#36755;&#20986;&#38169;&#35823;&#65292;&#26377;&#21487;&#33021;&#25552;&#39640;&#32593;&#32476;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents a method for using fractional concepts in a neural network to modify the activation and loss functions. The methodology allows the neural network to define and optimize its activation functions by determining the fractional derivative order of the training process as an additional hyperparameter. This will enable neurons in the network to adjust their activation functions to match input data better and reduce output errors, potentially improving the network's overall performance.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21028;&#21035;&#24615;&#22522;&#30784;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#20559;&#35265;&#30340;&#20998;&#31867;&#27861;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#35780;&#20272;&#29616;&#26377;&#30340;&#20943;&#23569;&#27169;&#22411;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#22312;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;&#26681;&#25454;&#20219;&#21153;&#28041;&#21450;&#20154;&#31867;&#31243;&#24230;&#12289;&#20027;&#35266;&#24615;&#31243;&#24230;&#21644;&#39044;&#26399;&#30446;&#30340;&#23545;&#26399;&#26395;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#23545;&#21463;&#20445;&#25252;&#23646;&#24615;&#36827;&#34892;&#20102;&#23450;&#37327;&#30340;&#20844;&#24179;&#24615;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.11867</link><description>&lt;p&gt;
&#35780;&#20272;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21028;&#21035;&#24615;&#22522;&#30784;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Fairness of Discriminative Foundation Models in Computer Vision. (arXiv:2310.11867v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11867
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21028;&#21035;&#24615;&#22522;&#30784;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#20559;&#35265;&#30340;&#20998;&#31867;&#27861;&#12290;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#35780;&#20272;&#29616;&#26377;&#30340;&#20943;&#23569;&#27169;&#22411;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#22312;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;&#26681;&#25454;&#20219;&#21153;&#28041;&#21450;&#20154;&#31867;&#31243;&#24230;&#12289;&#20027;&#35266;&#24615;&#31243;&#24230;&#21644;&#39044;&#26399;&#30446;&#30340;&#23545;&#26399;&#26395;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#23545;&#21463;&#20445;&#25252;&#23646;&#24615;&#36827;&#34892;&#20102;&#23450;&#37327;&#30340;&#20844;&#24179;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#35780;&#20272;&#21028;&#21035;&#24615;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;Contrastive Language-Pretraining (CLIP)&#65289;&#20559;&#35265;&#30340;&#20998;&#31867;&#27861;&#65292;&#35813;&#20998;&#31867;&#27861;&#29992;&#20110;&#26631;&#35760;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#29616;&#26377;&#20943;&#23569;&#36825;&#20123;&#27169;&#22411;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;OpenAI&#30340;CLIP&#21644;OpenCLIP&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#12289;&#22270;&#20687;&#26816;&#32034;&#21644;&#22270;&#20687;&#23383;&#24149;&#31561;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26681;&#25454;&#19977;&#20010;&#32500;&#24230;&#23545;&#26399;&#26395;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#20998;&#31867;&#65306;&#65288;i&#65289;&#20219;&#21153;&#26159;&#21542;&#28041;&#21450;&#20154;&#31867;&#65307;&#65288;ii&#65289;&#20219;&#21153;&#30340;&#20027;&#35266;&#24615;&#31243;&#24230;&#65288;&#21363;&#65292;&#26469;&#33258;&#19981;&#21516;&#32972;&#26223;&#30340;&#20154;&#20204;&#26159;&#21542;&#20250;&#23545;&#26631;&#35760;&#36798;&#25104;&#19968;&#33268;&#65289;&#65307;&#65288;iii&#65289;&#20219;&#21153;&#30340;&#39044;&#26399;&#30446;&#30340;&#65292;&#20844;&#24179;&#24615;&#26159;&#36890;&#36807;&#20844;&#27491;&#65288;&#21363;&#65292;&#29420;&#31435;&#20110;&#21463;&#20445;&#25252;&#23646;&#24615;&#36827;&#34892;&#20915;&#31574;&#65289;&#36824;&#26159;&#34920;&#36798;&#65288;&#21363;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22810;&#26679;&#24615;&#36827;&#34892;&#20915;&#31574;&#65289;&#26356;&#22909;&#22320;&#23454;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#20108;&#20540;&#21644;&#22810;&#20540;&#21463;&#20445;&#25252;&#23646;&#24615;&#36827;&#34892;&#20102;&#23450;&#37327;&#30340;&#20844;&#24179;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel taxonomy for bias evaluation of discriminative foundation models, such as Contrastive Language-Pretraining (CLIP), that are used for labeling tasks. We then systematically evaluate existing methods for mitigating bias in these models with respect to our taxonomy. Specifically, we evaluate OpenAI's CLIP and OpenCLIP models for key applications, such as zero-shot classification, image retrieval and image captioning. We categorize desired behaviors based around three axes: (i) if the task concerns humans; (ii) how subjective the task is (i.e., how likely it is that people from a diverse range of backgrounds would agree on a labeling); and (iii) the intended purpose of the task and if fairness is better served by impartiality (i.e., making decisions independent of the protected attributes) or representation (i.e., making decisions to maximize diversity). Finally, we provide quantitative fairness evaluations for both binary-valued and multi-valued protected attributes ove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#25552;&#20379;Hessian&#30697;&#38453;&#12289;&#26799;&#24230;&#21644;&#20989;&#25968;&#20540;&#30340;&#19981;&#31934;&#30830;&#35745;&#31639;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#20256;&#25773;&#24320;&#38144;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36798;&#21040;&#949;-&#36817;&#20284;&#20108;&#38454;&#20248;&#21270;&#26102;&#19982;&#31934;&#30830;&#35745;&#31639;&#20855;&#26377;&#30456;&#21516;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.11866</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#31934;&#30830;&#30340;Hessian&#30697;&#38453;&#12289;&#26799;&#24230;&#21644;&#20989;&#25968;&#30340;&#38750;&#20984;&#38382;&#39064;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Optimization for Non-convex Problem with Inexact Hessian Matrix, Gradient, and Function. (arXiv:2310.11866v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#25552;&#20379;Hessian&#30697;&#38453;&#12289;&#26799;&#24230;&#21644;&#20989;&#25968;&#20540;&#30340;&#19981;&#31934;&#30830;&#35745;&#31639;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#20256;&#25773;&#24320;&#38144;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36798;&#21040;&#949;-&#36817;&#20284;&#20108;&#38454;&#20248;&#21270;&#26102;&#19982;&#31934;&#30830;&#35745;&#31639;&#20855;&#26377;&#30456;&#21516;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#21306;&#22495;(TR)&#21644;&#20351;&#29992;&#19977;&#27425;&#26041;(ARC)&#36827;&#34892;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#35777;&#26126;&#23545;&#20110;&#38750;&#20984;&#20248;&#21270;&#20855;&#26377;&#19968;&#20123;&#38750;&#24120;&#21560;&#24341;&#20154;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#36890;&#36807;&#21516;&#26102;&#35745;&#31639;&#20989;&#25968;&#20540;&#12289;&#26799;&#24230;&#21644;Hessian&#30697;&#38453;&#26469;&#33719;&#24471;&#19979;&#19968;&#20010;&#25628;&#32034;&#26041;&#21521;&#21644;&#35843;&#25972;&#21442;&#25968;&#12290;&#23613;&#31649;&#38543;&#26426;&#36817;&#20284;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#25910;&#25947;&#36895;&#24230;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#26063;&#33021;&#22815;&#21516;&#26102;&#25552;&#20379;Hessian&#30697;&#38453;&#12289;&#26799;&#24230;&#21644;&#20989;&#25968;&#20540;&#30340;&#19981;&#31934;&#30830;&#35745;&#31639;&#30340;&#38543;&#26426;TR&#21644;ARC&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#27599;&#27425;&#36845;&#20195;&#25152;&#38656;&#30340;&#20256;&#25773;&#24320;&#38144;&#27604;TR&#21644;ARC&#35201;&#23569;&#24471;&#22810;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36798;&#21040;&#949;-&#36817;&#20284;&#20108;&#38454;&#20248;&#21270;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#19982;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#31934;&#30830;&#35745;&#31639;&#30340;&#25968;&#37327;&#32423;&#30456;&#21516;&#12290;&#27492;&#22806;&#65292;&#23545;&#19981;&#31934;&#30830;&#24615;&#30340;&#28201;&#21644;&#26465;&#20214;&#21487;&#20197;&#36890;&#36807;&#22312;&#26377;&#38480;&#21644;&#24635;&#21644;m&#20013;&#21033;&#29992;&#38543;&#26426;&#37319;&#26679;&#25216;&#26415;&#26469;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust-region (TR) and adaptive regularization using cubics (ARC) have proven to have some very appealing theoretical properties for non-convex optimization by concurrently computing function value, gradient, and Hessian matrix to obtain the next search direction and the adjusted parameters. Although stochastic approximations help largely reduce the computational cost, it is challenging to theoretically guarantee the convergence rate. In this paper, we explore a family of stochastic TR and ARC methods that can simultaneously provide inexact computations of the Hessian matrix, gradient, and function values. Our algorithms require much fewer propagations overhead per iteration than TR and ARC. We prove that the iteration complexity to achieve $\epsilon$-approximate second-order optimality is of the same order as the exact computations demonstrated in previous studies. Additionally, the mild conditions on inexactness can be met by leveraging a random sampling technology in the finite-sum m
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HybridTree&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#32852;&#37030;&#26641;&#23398;&#20064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26641;&#20013;&#30340;&#19968;&#33268;&#20998;&#21106;&#35268;&#21017;&#65292;&#21442;&#19982;&#26041;&#30340;&#30693;&#35782;&#21487;&#20197;&#34987;&#32435;&#20837;&#26641;&#30340;&#36739;&#20302;&#23618;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.11865</link><description>&lt;p&gt;
&#28151;&#21512;&#25968;&#25454;&#19978;&#26377;&#25928;&#39640;&#25928;&#30340;&#32852;&#37030;&#26641;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Effective and Efficient Federated Tree Learning on Hybrid Data. (arXiv:2310.11865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11865
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HybridTree&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#32852;&#37030;&#26641;&#23398;&#20064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26641;&#20013;&#30340;&#19968;&#33268;&#20998;&#21106;&#35268;&#21017;&#65292;&#21442;&#19982;&#26041;&#30340;&#30693;&#35782;&#21487;&#20197;&#34987;&#32435;&#20837;&#26641;&#30340;&#36739;&#20302;&#23618;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#20419;&#36827;&#22810;&#20010;&#21442;&#19982;&#26041;&#22312;&#19981;&#20256;&#36755;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#38598;&#20013;&#22312;&#27700;&#24179;&#25968;&#25454;&#25110;&#22402;&#30452;&#25968;&#25454;&#35774;&#32622;&#19978;&#65292;&#20854;&#20013;&#19981;&#21516;&#21442;&#19982;&#26041;&#30340;&#25968;&#25454;&#34987;&#35748;&#20026;&#26469;&#33258;&#30456;&#21516;&#30340;&#29305;&#24449;&#25110;&#26679;&#26412;&#31354;&#38388;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24120;&#35265;&#30340;&#24773;&#20917;&#26159;&#28151;&#21512;&#25968;&#25454;&#35774;&#32622;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#21442;&#19982;&#26041;&#30340;&#25968;&#25454;&#22312;&#29305;&#24449;&#21644;&#26679;&#26412;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HybridTree&#65292;&#19968;&#31181;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#32852;&#37030;&#26641;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26641;&#20013;&#23384;&#22312;&#19968;&#33268;&#30340;&#20998;&#21106;&#35268;&#21017;&#12290;&#20511;&#21161;&#36825;&#20123;&#20998;&#21106;&#35268;&#21017;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#21442;&#19982;&#26041;&#30340;&#30693;&#35782;&#21487;&#20197;&#34987;&#32435;&#20837;&#26641;&#30340;&#36739;&#20302;&#23618;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#38656;&#35201;&#39057;&#32321;&#30340;&#36890;&#20449;&#27969;&#37327;&#26469;&#35757;&#32451;&#19968;&#26869;&#26641;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#28151;&#21512;&#25968;&#25454;&#26102;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has emerged as a promising distributed learning paradigm that facilitates collaborative learning among multiple parties without transferring raw data. However, most existing federated learning studies focus on either horizontal or vertical data settings, where the data of different parties are assumed to be from the same feature or sample space. In practice, a common scenario is the hybrid data setting, where data from different parties may differ both in the features and samples. To address this, we propose HybridTree, a novel federated learning approach that enables federated tree learning on hybrid data. We observe the existence of consistent split rules in trees. With the help of these split rules, we theoretically show that the knowledge of parties can be incorporated into the lower layers of a tree. Based on our theoretical analysis, we propose a layer-level solution that does not need frequent communication traffic to train a tree. Our experiments demonstrate 
&lt;/p&gt;</description></item><item><title>VQ-NeRF&#26159;&#19968;&#20010;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#35299;&#21644;&#32534;&#36753;3D&#22330;&#26223;&#20013;&#30340;&#21453;&#23556;&#22330;&#12290;&#36890;&#36807;&#23558;&#36830;&#32493;&#26448;&#26009;&#31163;&#25955;&#21270;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#22122;&#22768;&#24182;&#29983;&#25104;&#31163;&#25955;&#26448;&#26009;&#30340;&#20998;&#21106;&#22320;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#31616;&#21270;&#30340;&#26448;&#26009;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2310.11864</link><description>&lt;p&gt;
VQ-NeRF: &#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#30340;&#31070;&#32463;&#21453;&#23556;&#20998;&#35299;&#19982;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector Quantization. (arXiv:2310.11864v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11864
&lt;/p&gt;
&lt;p&gt;
VQ-NeRF&#26159;&#19968;&#20010;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#35299;&#21644;&#32534;&#36753;3D&#22330;&#26223;&#20013;&#30340;&#21453;&#23556;&#22330;&#12290;&#36890;&#36807;&#23558;&#36830;&#32493;&#26448;&#26009;&#31163;&#25955;&#21270;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#22122;&#22768;&#24182;&#29983;&#25104;&#31163;&#25955;&#26448;&#26009;&#30340;&#20998;&#21106;&#22320;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#31616;&#21270;&#30340;&#26448;&#26009;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VQ-NeRF&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#30340;&#21452;&#20998;&#25903;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;3D&#22330;&#26223;&#20013;&#30340;&#21453;&#23556;&#22330;&#36827;&#34892;&#20998;&#35299;&#21644;&#32534;&#36753;&#12290;&#20256;&#32479;&#30340;&#31070;&#32463;&#21453;&#23556;&#22330;&#20165;&#20351;&#29992;&#36830;&#32493;&#34920;&#31034;&#26469;&#24314;&#27169;3D&#22330;&#26223;&#65292;&#23613;&#31649;&#29616;&#23454;&#20013;&#30340;&#29289;&#20307;&#36890;&#24120;&#30001;&#31163;&#25955;&#26448;&#26009;&#32452;&#25104;&#12290;&#36825;&#31181;&#32570;&#20047;&#31163;&#25955;&#21270;&#21487;&#33021;&#23548;&#33268;&#26448;&#26009;&#20998;&#35299;&#22122;&#22768;&#21644;&#22797;&#26434;&#30340;&#26448;&#26009;&#32534;&#36753;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#36830;&#32493;&#20998;&#25903;&#21644;&#19968;&#20010;&#31163;&#25955;&#20998;&#25903;&#12290;&#36830;&#32493;&#20998;&#25903;&#25353;&#29031;&#20256;&#32479;&#27969;&#31243;&#39044;&#27979;&#20998;&#35299;&#30340;&#26448;&#26009;&#65292;&#32780;&#31163;&#25955;&#20998;&#25903;&#20351;&#29992;VQ&#26426;&#21046;&#23558;&#36830;&#32493;&#26448;&#26009;&#37327;&#21270;&#20026;&#21333;&#29420;&#30340;&#26448;&#26009;&#12290;&#36890;&#36807;&#31163;&#25955;&#21270;&#26448;&#26009;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#20998;&#35299;&#36807;&#31243;&#20013;&#30340;&#22122;&#22768;&#65292;&#24182;&#29983;&#25104;&#31163;&#25955;&#26448;&#26009;&#30340;&#20998;&#21106;&#22320;&#22270;&#12290;&#21487;&#20197;&#36890;&#36807;&#28857;&#20987;&#20998;&#21106;&#32467;&#26524;&#30340;&#30456;&#24212;&#21306;&#22495;&#26469;&#36731;&#26494;&#36873;&#25321;&#29305;&#23450;&#26448;&#26009;&#36827;&#34892;&#36827;&#19968;&#27493;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose VQ-NeRF, a two-branch neural network model that incorporates Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes. Conventional neural reflectance fields use only continuous representations to model 3D scenes, despite the fact that objects are typically composed of discrete materials in reality. This lack of discretization can result in noisy material decomposition and complicated material editing. To address these limitations, our model consists of a continuous branch and a discrete branch. The continuous branch follows the conventional pipeline to predict decomposed materials, while the discrete branch uses the VQ mechanism to quantize continuous materials into individual ones. By discretizing the materials, our model can reduce noise in the decomposition process and generate a segmentation map of discrete materials. Specific materials can be easily selected for further editing by clicking on the corresponding area of the segmentation outcomes. Ad
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#31034;&#20363;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#25915;&#20987;&#20998;&#31867;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#35780;&#20272;&#25581;&#31034;&#20102;&#19968;&#20123;&#26032;&#30340;&#35265;&#35299;&#21644;&#20849;&#35782;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.11850</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#31034;&#20363;&#65306;&#25915;&#20987;&#20998;&#31867;&#65292;&#35780;&#20272;&#25351;&#21335;&#21644;&#26032;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Revisiting Transferable Adversarial Image Examples: Attack Categorization, Evaluation Guidelines, and New Insights. (arXiv:2310.11850v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#31034;&#20363;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#25915;&#20987;&#20998;&#31867;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#35780;&#20272;&#25581;&#31034;&#20102;&#19968;&#20123;&#26032;&#30340;&#35265;&#35299;&#21644;&#20849;&#35782;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#40657;&#30418;&#25915;&#20987;&#22330;&#26223;&#20013;&#24341;&#21457;&#20102;&#20851;&#38190;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#24120;&#35265;&#35780;&#20272;&#23454;&#36341;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;(1) &#23545;&#20110;&#25915;&#20987;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#32570;&#20047;&#31995;&#32479;&#21270;&#30340;&#65292;&#19968;&#23545;&#19968;&#30340;&#25915;&#20987;&#27604;&#36739;&#21644;&#20844;&#24179;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#12290;(2) &#23545;&#20110;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#31616;&#21333;&#22320;&#27809;&#26377;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;(1) &#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#20998;&#31867;&#31574;&#30053;&#65292;&#24182;&#22312;&#21487;&#36716;&#31227;&#24615;&#26041;&#38754;&#36827;&#34892;&#31995;&#32479;&#21270;&#21644;&#20844;&#24179;&#30340;&#21516;&#31867;&#21035;&#20998;&#26512;&#65292;&#20197;&#21450;(2) &#20174;&#25915;&#20987;&#22238;&#28335;&#30340;&#35282;&#24230;&#32771;&#34385;&#22810;&#26679;&#30340;&#38590;&#20197;&#23519;&#35273;&#30340;&#24230;&#37327;&#21644;&#26356;&#32454;&#31890;&#24230;&#30340;&#38544;&#34109;&#29305;&#24615;&#26469;&#24314;&#31435;&#26032;&#30340;&#35780;&#20272;&#25351;&#21335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;ImageNet&#19978;&#30340;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#35780;&#20272;&#65292;&#28041;&#21450;&#23545;9&#31181;&#20195;&#34920;&#24615;&#38450;&#24481;&#30340;23&#31181;&#20195;&#34920;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#19968;&#20123;&#26032;&#30340;&#35265;&#35299;&#65292;&#21253;&#25324;&#25361;&#25112;&#20849;&#35782;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transferable adversarial examples raise critical security concerns in real-world, black-box attack scenarios. However, in this work, we identify two main problems in common evaluation practices: (1) For attack transferability, lack of systematic, one-to-one attack comparison and fair hyperparameter settings. (2) For attack stealthiness, simply no comparisons. To address these problems, we establish new evaluation guidelines by (1) proposing a novel attack categorization strategy and conducting systematic and fair intra-category analyses on transferability, and (2) considering diverse imperceptibility metrics and finer-grained stealthiness characteristics from the perspective of attack traceback. To this end, we provide the first large-scale evaluation of transferable adversarial examples on ImageNet, involving 23 representative attacks against 9 representative defenses. Our evaluation leads to a number of new insights, including consensus-challenging ones: (1) Under a fair attack hyper
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65288;RL4Presolve&#65289;&#65292;&#36890;&#36807;&#23558;&#31639;&#27861;&#35774;&#35745;&#20219;&#21153;&#36716;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#32447;&#24615;&#35268;&#21010;&#20013;&#21069;&#22788;&#29702;&#31243;&#24207;&#30340;&#36873;&#25321;&#12289;&#39034;&#24207;&#21644;&#20572;&#27490;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.11845</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21152;&#36895;&#22823;&#35268;&#27169;&#32447;&#24615;&#35268;&#21010;&#20013;&#30340;&#21069;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Accelerate Presolve in Large-Scale Linear Programming via Reinforcement Learning. (arXiv:2310.11845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65288;RL4Presolve&#65289;&#65292;&#36890;&#36807;&#23558;&#31639;&#27861;&#35774;&#35745;&#20219;&#21153;&#36716;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#32447;&#24615;&#35268;&#21010;&#20013;&#21069;&#22788;&#29702;&#31243;&#24207;&#30340;&#36873;&#25321;&#12289;&#39034;&#24207;&#21644;&#20572;&#27490;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#24037;&#19994;&#30028;&#30340;&#22823;&#35268;&#27169;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#20887;&#20313;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#35299;&#20915;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#30340;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#65292;&#20351;&#24471;&#21069;&#22788;&#29702;&#65288;&#21363;&#38382;&#39064;&#31616;&#21270;&#27169;&#22359;&#65289;&#25104;&#20026;&#29616;&#20195;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#20013;&#26368;&#20851;&#38190;&#30340;&#32452;&#20214;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#39640;&#36136;&#37327;&#30340;&#21069;&#22788;&#29702;&#31243;&#24207;&#65288;&#21363;&#30830;&#23450;&#65288;P1&#65289;&#36873;&#25321;&#21738;&#20123;&#21069;&#22788;&#29702;&#22120;&#65292;&#65288;P2&#65289;&#20197;&#20309;&#31181;&#39034;&#24207;&#25191;&#34892;&#65292;&#65288;P3&#65289;&#20309;&#26102;&#20572;&#27490;&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#19987;&#23478;&#30693;&#35782;&#30340;&#24191;&#27867;&#35201;&#27714;&#21644;&#24222;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#30001;&#20110;&#20219;&#21153;&#20855;&#26377;&#39034;&#24207;&#20915;&#31574;&#23646;&#24615;&#21644;&#32570;&#20047;&#19987;&#23478;&#31034;&#33539;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#8212;&#8212;&#21363;&#21069;&#22788;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL4Presolve&#65289;&#65292;&#20197;&#21516;&#26102;&#35299;&#20915;&#65288;P1&#65289;-&#65288;P3&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#31639;&#27861;&#35774;&#35745;&#20219;&#21153;&#36716;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#21160;&#20316;&#24207;&#21015;&#30340;RL&#26694;&#26550;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21069;&#22788;&#29702;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale LP problems from industry usually contain much redundancy that severely hurts the efficiency and reliability of solving LPs, making presolve (i.e., the problem simplification module) one of the most critical components in modern LP solvers. However, how to design high-quality presolve routines -that is, the program determining (P1) which presolvers to select, (P2) in what order to execute, and (P3) when to stop -- remains a highly challenging task due to the extensive requirements on expert knowledge and the large search space. Due to the sequential decision property of the task and the lack of expert demonstrations, we propose a simple and efficient reinforcement learning (RL) framework -- namely, reinforcement learning for presolve (RL4Presolve) -to tackle (P1)-(P3) simultaneously. Specifically, we formulate the routine design task as a Markov decision process and propose an RL framework with adaptive action sequences to generate high-quality presolve routines efficie
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;17&#31181;&#30446;&#26631;&#35268;&#33539;&#24418;&#24335;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#24418;&#24335;&#21270;&#26041;&#27861;&#36827;&#34892;&#39044;&#25490;&#24207;&#65292;&#24182;&#21576;&#29616;&#20026;&#21704;&#26031;&#22270;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#24418;&#24335;&#21270;&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#24182;&#19988;&#27809;&#26377;&#19968;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26082;&#20855;&#26377;&#20027;&#23548;&#24615;&#30340;&#34920;&#36798;&#33021;&#21147;&#21448;&#23481;&#26131;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.11840</link><description>&lt;p&gt;
&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30446;&#26631;&#35268;&#33539;&#24418;&#24335;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning. (arXiv:2310.11840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;17&#31181;&#30446;&#26631;&#35268;&#33539;&#24418;&#24335;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#24418;&#24335;&#21270;&#26041;&#27861;&#36827;&#34892;&#39044;&#25490;&#24207;&#65292;&#24182;&#21576;&#29616;&#20026;&#21704;&#26031;&#22270;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#24418;&#24335;&#21270;&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#24182;&#19988;&#27809;&#26377;&#19968;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26082;&#20855;&#26377;&#20027;&#23548;&#24615;&#30340;&#34920;&#36798;&#33021;&#21147;&#21448;&#23481;&#26131;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#24517;&#39035;&#23545;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#36827;&#34892;&#24418;&#24335;&#21270;&#35268;&#23450;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35201;&#27714;&#23558;&#30446;&#26631;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#65288;&#22914;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#21644;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65289;&#12290;&#27492;&#22806;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#20854;&#20013;&#19968;&#20123;&#24418;&#24335;&#21270;&#26041;&#27861;&#33021;&#22815;&#34920;&#36798;&#20854;&#20182;&#24418;&#24335;&#21270;&#26041;&#27861;&#26080;&#27861;&#34920;&#36798;&#30340;&#29305;&#23450;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23545;&#36825;&#20123;&#24418;&#24335;&#21270;&#26041;&#27861;&#22312;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#22914;&#20309;&#30456;&#20114;&#20851;&#32852;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;17&#31181;&#30446;&#26631;&#35268;&#33539;&#24418;&#24335;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24418;&#24335;&#21270;&#26041;&#27861;&#26681;&#25454;&#20854;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#39044;&#25490;&#24207;&#65292;&#24182;&#23558;&#35813;&#39044;&#25490;&#24207;&#21576;&#29616;&#20026;&#21704;&#26031;&#22270;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#24418;&#24335;&#21270;&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#24182;&#19988;&#27809;&#26377;&#19968;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26082;&#20855;&#26377;&#20027;&#23548;&#24615;&#30340;&#34920;&#36798;&#33021;&#21147;&#21448;&#23481;&#26131;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
To solve a task with reinforcement learning (RL), it is necessary to formally specify the goal of that task. Although most RL algorithms require that the goal is formalised as a Markovian reward function, alternatives have been developed (such as Linear Temporal Logic and Multi-Objective Reinforcement Learning). Moreover, it is well known that some of these formalisms are able to express certain tasks that other formalisms cannot express. However, there has not yet been any thorough analysis of how these formalisms relate to each other in terms of expressivity. In this work, we fill this gap in the existing literature by providing a comprehensive comparison of the expressivities of 17 objective-specification formalisms in RL. We place these formalisms in a preorder based on their expressive power, and present this preorder as a Hasse diagram. We find a variety of limitations for the different formalisms, and that no formalism is both dominantly expressive and straightforward to optimis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21442;&#25968;&#24341;&#23548;&#31639;&#27861;&#30340;&#31561;&#21464;&#24418;&#24335;&#65292;&#21487;&#20197;&#22312;&#25104;&#20687;&#21453;&#38382;&#39064;&#20013;&#37327;&#21270;&#37325;&#26500;&#22270;&#20687;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20219;&#20309;&#22270;&#20687;&#37325;&#24314;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.11838</link><description>&lt;p&gt;
&#31561;&#21464;&#24341;&#23548;&#27861;&#22312;&#25104;&#20687;&#21453;&#38382;&#39064;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Equivariant Bootstrapping for Uncertainty Quantification in Imaging Inverse Problems. (arXiv:2310.11838v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21442;&#25968;&#24341;&#23548;&#31639;&#27861;&#30340;&#31561;&#21464;&#24418;&#24335;&#65292;&#21487;&#20197;&#22312;&#25104;&#20687;&#21453;&#38382;&#39064;&#20013;&#37327;&#21270;&#37325;&#26500;&#22270;&#20687;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20219;&#20309;&#22270;&#20687;&#37325;&#24314;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25104;&#20687;&#38382;&#39064;&#36890;&#24120;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#36866;&#23450;&#24615;&#65292;&#22240;&#27492;&#20855;&#26377;&#37325;&#35201;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#20934;&#30830;&#37327;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#20005;&#26684;&#35299;&#37322;&#23454;&#39564;&#32467;&#26524;&#20197;&#21450;&#21487;&#38752;&#22320;&#20351;&#29992;&#37325;&#26500;&#22270;&#20687;&#20316;&#20026;&#31185;&#23398;&#35777;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#25104;&#20687;&#26041;&#27861;&#26080;&#27861;&#20197;&#23545;&#23454;&#39564;&#37325;&#22797;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26041;&#24335;&#37327;&#21270;&#37325;&#26500;&#22270;&#20687;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#22522;&#20110;&#21442;&#25968;&#24341;&#23548;&#31639;&#27861;&#30340;&#31561;&#21464;&#24418;&#24335;&#65292;&#21033;&#29992;&#22312;&#25104;&#20687;&#38382;&#39064;&#20013;&#24120;&#35265;&#30340;&#23545;&#31216;&#24615;&#21644;&#19981;&#21464;&#24615;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#19982;&#20219;&#20309;&#22270;&#20687;&#37325;&#24314;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#65292;&#21253;&#25324;&#21482;&#33021;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22312;&#21482;&#26377;&#35266;&#27979;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific imaging problems are often severely ill-posed, and hence have significant intrinsic uncertainty. Accurately quantifying the uncertainty in the solutions to such problems is therefore critical for the rigorous interpretation of experimental results as well as for reliably using the reconstructed images as scientific evidence. Unfortunately, existing imaging methods are unable to quantify the uncertainty in the reconstructed images in a manner that is robust to experiment replications. This paper presents a new uncertainty quantification methodology based on an equivariant formulation of the parametric bootstrap algorithm that leverages symmetries and invariance properties commonly encountered in imaging problems. Additionally, the proposed methodology is general and can be easily applied with any image reconstruction technique, including unsupervised training strategies that can be trained from observed data alone, thus enabling uncertainty quantification in situations where 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#20248;&#21270;&#36807;&#31243;&#20026;&#38024;&#23545;&#26131;&#20110;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#30340;&#26367;&#20195;&#20998;&#24067;&#30340;&#21442;&#25968;&#20248;&#21270;&#26469;&#35299;&#20915;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25193;&#23637;&#21487;&#24212;&#29992;&#33258;&#28982;&#26799;&#24230;&#30340;&#20998;&#24067;&#33539;&#22260;&#65292;&#36895;&#24230;&#24555;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.11837</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#26367;&#20195;&#21697;&#20248;&#21270;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Optimising Distributions with Natural Gradient Surrogates. (arXiv:2310.11837v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#20248;&#21270;&#36807;&#31243;&#20026;&#38024;&#23545;&#26131;&#20110;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#30340;&#26367;&#20195;&#20998;&#24067;&#30340;&#21442;&#25968;&#20248;&#21270;&#26469;&#35299;&#20915;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25193;&#23637;&#21487;&#24212;&#29992;&#33258;&#28982;&#26799;&#24230;&#30340;&#20998;&#24067;&#33539;&#22260;&#65292;&#36895;&#24230;&#24555;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#26799;&#24230;&#26041;&#27861;&#24050;&#32463;&#34987;&#29992;&#20110;&#20248;&#21270;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#27010;&#29575;&#20998;&#24067;&#21442;&#25968;&#65292;&#36890;&#24120;&#33021;&#24471;&#21040;&#24555;&#36895;&#25910;&#25947;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#20998;&#24067;&#65292;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#28041;&#21450;&#23558;&#20248;&#21270;&#37325;&#26032;&#23450;&#20041;&#20026;&#20851;&#20110;&#26367;&#20195;&#20998;&#24067;&#21442;&#25968;&#30340;&#20248;&#21270;&#65292;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#24456;&#23481;&#26131;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20960;&#20010;&#21487;&#20197;&#35299;&#37322;&#20026;&#24212;&#29992;&#36825;&#31181;&#25216;&#26415;&#30340;&#29616;&#26377;&#26041;&#27861;&#30340;&#20363;&#23376;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#21487;&#20197;&#26377;&#25928;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#30340;&#20998;&#24067;&#38598;&#21512;&#12290;&#27492;&#22806;&#65292;&#23427;&#24555;&#36895;&#12289;&#26131;&#20110;&#29702;&#35299;&#65292;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#30340;&#33258;&#21160;&#24494;&#20998;&#36719;&#20214;&#36827;&#34892;&#31616;&#21333;&#23454;&#29616;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20887;&#38271;&#30340;&#27169;&#22411;&#29305;&#23450;&#23548;&#25968;&#35745;&#31639;&#12290;&#25105;&#20204;&#22312;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21644;&#21464;&#20998;&#25512;&#26029;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural gradient methods have been used to optimise the parameters of probability distributions in a variety of settings, often resulting in fast-converging procedures. Unfortunately, for many distributions of interest, computing the natural gradient has a number of challenges. In this work we propose a novel technique for tackling such issues, which involves reframing the optimisation as one with respect to the parameters of a surrogate distribution, for which computing the natural gradient is easy. We give several examples of existing methods that can be interpreted as applying this technique, and propose a new method for applying it to a wide variety of problems. Our method expands the set of distributions that can be efficiently targeted with natural gradients. Furthermore, it is fast, easy to understand, simple to implement using standard autodiff software, and does not require lengthy model-specific derivations. We demonstrate our method on maximum likelihood estimation and varia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#33719;&#21462;&#38899;&#39057;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#21644;&#24773;&#24863;&#32500;&#24230;&#30340;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.11830</link><description>&lt;p&gt;
CLARA: &#22810;&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#38899;&#39057;&#34920;&#31034;&#33719;&#21462;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
CLARA: Multilingual Contrastive Learning for Audio Representation Acquisition. (arXiv:2310.11830v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#33719;&#21462;&#38899;&#39057;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#21644;&#24773;&#24863;&#32500;&#24230;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#21644;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#12290;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#21046;&#32422;&#20102;&#36328;&#35821;&#35328;&#35821;&#38899;&#22788;&#29702;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;&#26368;&#36817;&#23545;&#27604;&#23398;&#20064;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#33258;&#30417;&#30563;&#25216;&#26415;&#26469;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#20943;&#23569;&#25968;&#25454;&#20381;&#36182;&#24615;&#21644;&#25913;&#21892;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#26465;&#20214;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#35821;&#35328;&#20013;&#33719;&#24471;&#20849;&#20139;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#20351;&#29992;&#26377;&#38480;&#30340;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20027;&#35266;&#24863;&#30693;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#25429;&#25417;&#35821;&#38899;&#20013;&#30340;&#24773;&#24863;&#32447;&#32034;&#26159;&#22256;&#38590;&#30340;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#20174;&#22810;&#26679;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#36798;&#24615;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#24320;&#21457;&#32534;&#30721;&#24773;&#24863;&#32500;&#24230;&#30340;&#35821;&#38899;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#38899;&#39057;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel framework for multilingual speech and sound representation learning using contrastive learning. The lack of sizeable labelled datasets hinders speech-processing research across languages. Recent advances in contrastive learning provide self-supervised techniques to learn from unlabelled data. Motivated by reducing data dependence and improving generalisation across diverse languages and conditions, we develop a multilingual contrastive framework. This framework enables models to acquire shared representations across languages, facilitating cross-lingual transfer with limited target language data.  Additionally, capturing emotional cues within speech is challenging due to subjective perceptual assessments. By learning expressive representations from diverse, multilingual data in a self-supervised manner, our approach aims to develop speech representations that encode emotive dimensions.  Our method trains encoders on a large corpus of multi-lingual audio data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22270;&#22522;&#30784;&#27169;&#22411;&#65288;GFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#23545;&#20854;&#20851;&#38190;&#29305;&#24449;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#38416;&#36848;&#12290;&#21516;&#26102;&#65292;&#23558;&#29616;&#26377;GFMs&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#22270;&#23398;&#20064;&#33539;&#24335;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2310.11829</link><description>&lt;p&gt;
&#36208;&#21521;&#22270;&#22522;&#30784;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;&#19982;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Towards Graph Foundation Models: A Survey and Beyond. (arXiv:2310.11829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22270;&#22522;&#30784;&#27169;&#22411;&#65288;GFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#23545;&#20854;&#20851;&#38190;&#29305;&#24449;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#38416;&#36848;&#12290;&#21516;&#26102;&#65292;&#23558;&#29616;&#26377;GFMs&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#22270;&#23398;&#20064;&#33539;&#24335;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#25104;&#21151;&#65292;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#32463;&#21382;&#20102;&#30001;&#27973;&#23618;&#26041;&#27861;&#21521;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#36716;&#21464;&#12290;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#21644;&#21516;&#21270;&#33021;&#21147;&#24341;&#36215;&#20102;&#22270;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#30340;&#20852;&#36259;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#24320;&#21457;&#19979;&#19968;&#20010;&#39044;&#35757;&#32451;&#20110;&#24191;&#27867;&#22270;&#25968;&#25454;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#22270;&#20219;&#21153;&#30340;&#22270;&#23398;&#20064;&#33539;&#24335;&#30340;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#36825;&#31867;&#24037;&#20316;&#23578;&#26080;&#26126;&#30830;&#30340;&#23450;&#20041;&#21644;&#31995;&#32479;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#22522;&#30784;&#27169;&#22411;(GFMs)&#30340;&#27010;&#24565;&#65292;&#24182;&#39318;&#27425;&#23545;&#20854;&#20851;&#38190;&#29305;&#24449;&#21644;&#25216;&#26415;&#36827;&#34892;&#20840;&#38754;&#38416;&#36848;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26681;&#25454;&#20854;&#21487;&#38752;&#24615;&#23558;&#29616;&#26377;GFMs&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging as fundamental building blocks for diverse artificial intelligence applications, foundation models have achieved notable success across natural language processing and many other domains. Parallelly, graph machine learning has witnessed a transformative shift, with shallow methods giving way to deep learning approaches. The emergence and homogenization capabilities of foundation models have piqued the interest of graph machine learning researchers, sparking discussions about developing the next graph learning paradigm that is pre-trained on broad graph data and can be adapted to a wide range of downstream graph tasks. However, there is currently no clear definition and systematic analysis for this type of work. In this article, we propose the concept of graph foundation models (GFMs), and provide the first comprehensive elucidation on their key characteristics and technologies. Following that, we categorize existing works towards GFMs into three categories based on their relia
&lt;/p&gt;</description></item><item><title>&#22312;&#22024;&#26434;&#30340;&#37329;&#34701;&#25968;&#25454;&#19978;&#36827;&#34892;&#20445;&#23432;&#39044;&#27979;&#65292;&#36890;&#36807;&#23545;&#19981;&#30830;&#23450;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#21098;&#26525;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11815</link><description>&lt;p&gt;
&#22312;&#22024;&#26434;&#30340;&#37329;&#34701;&#25968;&#25454;&#19978;&#36827;&#34892;&#20445;&#23432;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conservative Predictions on Noisy Financial Data. (arXiv:2310.11815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11815
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22024;&#26434;&#30340;&#37329;&#34701;&#25968;&#25454;&#19978;&#36827;&#34892;&#20445;&#23432;&#39044;&#27979;&#65292;&#36890;&#36807;&#23545;&#19981;&#30830;&#23450;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#21098;&#26525;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#24066;&#22330;&#30340;&#20215;&#26684;&#27874;&#21160;&#34987;&#35748;&#20026;&#26159;&#38750;&#24120;&#22024;&#26434;&#30340;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20598;&#23572;&#33021;&#22815;&#25429;&#25417;&#21040;&#21487;&#21033;&#29992;&#30340;&#27169;&#24335;&#65292;&#30001;&#20110;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#22122;&#22768;&#65292;&#36825;&#20123;&#27169;&#24335;&#24448;&#24448;&#34987;&#25513;&#30422;&#65292;&#20351;&#24471;&#39044;&#27979;&#21464;&#24471;&#19981;&#22826;&#26377;&#29992;&#19988;&#23384;&#22312;&#39118;&#38505;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#31867;&#20284;&#30340;&#26041;&#27861;&#65292;&#21363;&#27169;&#22411;&#22312;&#23545;&#19981;&#30830;&#23450;&#30340;&#25968;&#25454;&#28857;&#19978;&#19981;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#19968;&#31995;&#21015;&#36825;&#26679;&#30340;&#27169;&#22411;&#25353;&#24207;&#36827;&#34892;&#23398;&#20064;&#65292;&#31867;&#20284;&#20110;&#35268;&#21017;&#21015;&#34920;&#65292;&#27599;&#20010;&#27169;&#22411;&#20165;&#22312;&#21069;&#38754;&#30340;&#27169;&#22411;&#23545;&#20854;&#19981;&#30830;&#23450;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#27979;&#35797;&#26102;&#20063;&#20250;&#36827;&#34892;&#31867;&#20284;&#30340;&#25968;&#25454;&#21098;&#26525;&#65292;&#21482;&#23545;&#27979;&#35797;&#25968;&#25454;&#30340;&#19968;&#37096;&#20998;&#65288;&#25903;&#25345;&#38598;&#65289;&#36827;&#34892;&#39044;&#27979;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Price movements in financial markets are well known to be very noisy. As a result, even if there are, on occasion, exploitable patterns that could be picked up by machine-learning algorithms, these are obscured by feature and label noise rendering the predictions less useful, and risky in practice. Traditional rule-learning techniques developed for noisy data, such as CN2, would seek only high precision rules and refrain from making predictions where their antecedents did not apply. We apply a similar approach, where a model abstains from making a prediction on data points that it is uncertain on. During training, a cascade of such models are learned in sequence, similar to rule lists, with each model being trained only on data on which the previous model(s) were uncertain. Similar pruning of data takes place at test-time, with (higher accuracy) predictions being made albeit only on a fraction (support) of test-time data. In a financial prediction setting, such an approach allows decis
&lt;/p&gt;</description></item><item><title>&#21019;&#26032;&#28857;&#26159;&#24341;&#20837;&#20102;Vector Field Network (VFN)&#26469;&#25552;&#39640;&#34507;&#30333;&#36136;&#32467;&#26500;&#32534;&#30721;&#22120;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#22312;&#20840;&#26032;&#34507;&#30333;&#36136;&#35774;&#35745;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.11802</link><description>&lt;p&gt;
&#20351;&#29992;&#20960;&#20309;&#30690;&#37327;&#22330;&#32593;&#32476;&#36827;&#34892;&#20840;&#26032;&#34507;&#30333;&#36136;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
De novo protein design using geometric vector field networks. (arXiv:2310.11802v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11802
&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#28857;&#26159;&#24341;&#20837;&#20102;Vector Field Network (VFN)&#26469;&#25552;&#39640;&#34507;&#30333;&#36136;&#32467;&#26500;&#32534;&#30721;&#22120;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#22312;&#20840;&#26032;&#34507;&#30333;&#36136;&#35774;&#35745;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#34507;&#30333;&#36136;&#25193;&#25955;&#31561;&#21019;&#26032;&#25216;&#26415;&#65292;&#22312;&#20840;&#26032;&#34507;&#30333;&#36136;&#35774;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#26159;&#29983;&#21629;&#31185;&#23398;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#34507;&#30333;&#36136;&#32467;&#26500;&#32534;&#30721;&#22120;&#26469;&#24314;&#27169;&#27531;&#22522;&#39592;&#26550;&#24103;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19981;&#23384;&#22312;&#21407;&#23376;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#32534;&#30721;&#22120;&#20381;&#36182;&#21407;&#23376;&#32423;&#29305;&#24449;&#65292;&#22914;&#21407;&#23376;&#20043;&#38388;&#30340;&#35282;&#24230;&#21644;&#36317;&#31163;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#21482;&#26377;&#20960;&#20010;&#31616;&#21333;&#30340;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;IPA&#65289;&#34987;&#25552;&#20986;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#20351;&#24471;&#24103;&#27169;&#22411;&#25104;&#20026;&#19968;&#20010;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30690;&#37327;&#22330;&#32593;&#32476;&#65288;VFN&#65289;&#65292;&#23427;&#20351;&#24471;&#32593;&#32476;&#23618;&#33021;&#22815;&#22312;&#24103;&#38170;&#23450;&#30340;&#34394;&#25311;&#21407;&#23376;&#22352;&#26631;&#20043;&#38388;&#36827;&#34892;&#21487;&#23398;&#20064;&#30340;&#30690;&#37327;&#35745;&#31639;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#30340;&#24103;&#27169;&#22411;&#33021;&#21147;&#12290;&#30690;&#37327;&#35745;&#31639;&#30340;&#25805;&#20316;&#26041;&#24335;&#31867;&#20284;&#20110;&#32447;&#24615;&#23618;&#65292;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#25509;&#25910;3D&#34394;&#25311;&#21407;&#23376;&#22352;&#26631;&#32780;&#19981;&#26159;&#26631;&#37327;&#20540;&#12290;&#30690;&#37327;&#35745;&#31639;&#36755;&#20986;&#30340;&#22810;&#20010;&#29305;&#24449;&#21521;&#37327;&#28982;&#21518;&#34987;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Innovations like protein diffusion have enabled significant progress in de novo protein design, which is a vital topic in life science. These methods typically depend on protein structure encoders to model residue backbone frames, where atoms do not exist. Most prior encoders rely on atom-wise features, such as angles and distances between atoms, which are not available in this context. Thus far, only several simple encoders, such as IPA, have been proposed for this scenario, exposing the frame modeling as a bottleneck. In this work, we proffer the Vector Field Network (VFN), which enables network layers to perform learnable vector computations between coordinates of frame-anchored virtual atoms, thus achieving a higher capability for modeling frames. The vector computation operates in a manner similar to a linear layer, with each input channel receiving 3D virtual atom coordinates instead of scalar values. The multiple feature vectors output by the vector computation are then used to 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AT-PINNs&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#23545;&#25239;&#26679;&#26412;&#30340;&#24494;&#35843;&#26469;&#22686;&#24378;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#34892;&#20855;&#26377;&#26102;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2310.11789</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training for Physics-Informed Neural Networks. (arXiv:2310.11789v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11789
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AT-PINNs&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#23545;&#25239;&#26679;&#26412;&#30340;&#24494;&#35843;&#26469;&#22686;&#24378;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#34892;&#20855;&#26377;&#26102;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#36275;&#30340;&#40065;&#26834;&#24615;&#65292;&#26222;&#36890;&#30340;PINNs&#22312;&#35299;&#20915;&#28041;&#21450;&#22810;&#23610;&#24230;&#34892;&#20026;&#25110;&#20855;&#26377;&#23574;&#38160;&#25110;&#25391;&#33633;&#29305;&#24449;&#30340;&#22797;&#26434;PDE&#26102;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#23545;&#25239;&#25915;&#20987;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#65292;&#34987;&#31216;&#20026;AT-PINNs&#12290;AT-PINNs&#36890;&#36807;&#23545;&#25239;&#26679;&#26412;&#30340;&#24494;&#35843;&#26469;&#22686;&#24378;PINNs&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#22833;&#25928;&#20301;&#32622;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#23548;&#27169;&#22411;&#19987;&#27880;&#20110;&#36825;&#20123;&#21306;&#22495;&#12290;AT-PINNs&#36824;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#22260;&#32469;&#26102;&#38388;&#21021;&#22987;&#20540;&#30340;&#21021;&#22987;&#25311;&#21512;&#28857;&#26469;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#25105;&#20204;&#23558;AT-PINNs&#24212;&#29992;&#20110;&#20855;&#26377;&#22810;&#23610;&#24230;&#31995;&#25968;&#30340;&#26925;&#22278;&#26041;&#31243;&#12289;&#20855;&#26377;&#22810;&#23792;&#35299;&#30340;&#27850;&#26494;&#26041;&#31243;&#12289;&#20855;&#26377;&#23574;&#38160;&#35299;&#30340;Burgers&#26041;&#31243;&#20197;&#21450;Allen-Cahn&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks have shown great promise in solving partial differential equations. However, due to insufficient robustness, vanilla PINNs often face challenges when solving complex PDEs, especially those involving multi-scale behaviors or solutions with sharp or oscillatory characteristics. To address these issues, based on the projected gradient descent adversarial attack, we proposed an adversarial training strategy for PINNs termed by AT-PINNs. AT-PINNs enhance the robustness of PINNs by fine-tuning the model with adversarial samples, which can accurately identify model failure locations and drive the model to focus on those regions during training. AT-PINNs can also perform inference with temporal causality by selecting the initial collocation points around temporal initial values. We implement AT-PINNs to the elliptic equation with multi-scale coefficients, Poisson equation with multi-peak solutions, Burgers equation with sharp solutions and the Allen-Cahn equati
&lt;/p&gt;</description></item><item><title>NeuroCUT&#26159;&#19968;&#31181;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#30340;&#22270;&#20998;&#21306;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#65292;&#21363;&#23545;&#22270;&#25299;&#25169;&#21644;&#20998;&#21306;&#35745;&#25968;&#20855;&#26377;&#24402;&#32435;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11787</link><description>&lt;p&gt;
NeuroCUT&#65306;&#19968;&#31181;&#29992;&#20110;&#40065;&#26834;&#22270;&#20998;&#21306;&#30340;&#31070;&#32463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NeuroCUT: A Neural Approach for Robust Graph Partitioning. (arXiv:2310.11787v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11787
&lt;/p&gt;
&lt;p&gt;
NeuroCUT&#26159;&#19968;&#31181;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#30340;&#22270;&#20998;&#21306;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#65292;&#21363;&#23545;&#22270;&#25299;&#25169;&#21644;&#20998;&#21306;&#35745;&#25968;&#20855;&#26377;&#24402;&#32435;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#21306;&#26088;&#22312;&#23558;&#22270;&#20998;&#21106;&#20026;k&#20010;&#19981;&#30456;&#20132;&#30340;&#23376;&#38598;&#65292;&#21516;&#26102;&#20248;&#21270;&#29305;&#23450;&#30340;&#20998;&#21306;&#30446;&#26631;&#12290;&#30001;&#20110;&#20854;&#32452;&#21512;&#24615;&#36136;&#65292;&#22823;&#37096;&#20998;&#19982;&#22270;&#20998;&#21306;&#30456;&#20851;&#30340;&#38382;&#39064;&#37117;&#21576;&#29616;&#20986;NP&#38590;&#24230;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#36817;&#20284;&#31639;&#27861;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#26377;&#26102;&#24102;&#26377;&#36817;&#20284;&#20445;&#35777;&#65292;&#26377;&#26102;&#21017;&#27809;&#26377;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20256;&#32479;&#26041;&#27861;&#38024;&#23545;&#29305;&#23450;&#30340;&#20998;&#21306;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#19981;&#36866;&#29992;&#20110;&#20854;&#20182;&#24050;&#30693;&#30340;&#25991;&#29486;&#20013;&#30340;&#20998;&#21306;&#30446;&#26631;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#31070;&#32463;&#26041;&#27861;&#24212;&#36816;&#32780;&#29983;&#65292;&#24182;&#23637;&#31034;&#20986;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;NeuroCut&#25193;&#23637;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#24037;&#20316;&#12290;NeuroCut&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#23427;&#23545;&#22270;&#25299;&#25169;&#21644;&#20998;&#21306;&#35745;&#25968;&#20855;&#26377;&#24402;&#32435;&#24615;&#65292;&#36825;&#20123;&#20449;&#24687;&#22312;&#26597;&#35810;&#26102;&#25552;&#20379;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Graph partitioning aims to divide a graph into $k$ disjoint subsets while optimizing a specific partitioning objective. The majority of formulations related to graph partitioning exhibit NP-hardness due to their combinatorial nature. As a result, conventional approximation algorithms rely on heuristic methods, sometimes with approximation guarantees and sometimes without. Unfortunately, traditional approaches are tailored for specific partitioning objectives and do not generalize well across other known partitioning objectives from the literature. To overcome this limitation, and learn heuristics from the data directly, neural approaches have emerged, demonstrating promising outcomes. In this study, we extend this line of work through a novel framework, NeuroCut. NeuroCut introduces two key innovations over prevailing methodologies. First, it is inductive to both graph topology and the partition count, which is provided at query time. Second, by leveraging a reinforcement learning base
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#19978;&#30340;&#26368;&#20248;&#20256;&#36755;&#26469;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#22312;&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11762</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
A Quasi-Wasserstein Loss for Learning Graph Neural Networks. (arXiv:2310.11762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11762
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#19978;&#30340;&#26368;&#20248;&#20256;&#36755;&#26469;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#22312;&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22312;&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20219;&#21153;&#20013;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26102;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#29420;&#31435;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#33410;&#28857;&#30340;&#65292;&#21363;&#20351;&#33410;&#28857;&#23884;&#20837;&#21644;&#23427;&#20204;&#30340;&#26631;&#31614;&#30001;&#20110;&#22270;&#32467;&#26500;&#30340;&#23384;&#22312;&#32780;&#19981;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#65288;QW&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#20511;&#21161;&#20110;&#22312;&#22270;&#19978;&#23450;&#20041;&#30340;&#26368;&#20248;&#20256;&#36755;&#65292;&#20174;&#32780;&#24341;&#23548;GNN&#30340;&#26032;&#23398;&#20064;&#21644;&#39044;&#27979;&#33539;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#8220;&#20934;&#29926;&#29380;&#26031;&#22374;&#8221;&#36317;&#31163;&#65292;&#29992;&#20110;&#35266;&#27979;&#21040;&#30340;&#22810;&#32500;&#33410;&#28857;&#26631;&#31614;&#21644;&#23427;&#20204;&#30340;&#20272;&#35745;&#20043;&#38388;&#65292;&#36890;&#36807;&#20248;&#21270;&#22312;&#22270;&#36793;&#19978;&#23450;&#20041;&#30340;&#26631;&#31614;&#20256;&#36755;&#12290;&#36825;&#20123;&#20272;&#35745;&#26159;&#30001;&#19968;&#20010;GNN&#21442;&#25968;&#21270;&#30340;&#65292;&#20854;&#20013;&#26368;&#20248;&#26631;&#31614;&#20256;&#36755;&#21487;&#20197;&#36873;&#25321;&#24615;&#22320;&#30830;&#23450;&#22270;&#36793;&#30340;&#26435;&#37325;&#12290;&#36890;&#36807;&#23558;&#26631;&#31614;&#20256;&#36755;&#30340;&#20005;&#26684;&#32422;&#26463;&#37325;&#26032;&#34920;&#36798;&#20026;&#22522;&#20110;Bregman&#25955;&#24230;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25152;&#25552;&#20986;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;&#65292;&#20851;&#32852;&#20004;&#20010;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#23398;&#20064;GNN&#20197;&#21450;&#26368;&#20248;&#26631;&#31614;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
When learning graph neural networks (GNNs) in node-level prediction tasks, most existing loss functions are applied for each node independently, even if node embeddings and their labels are non-i.i.d. because of their graph structures. To eliminate such inconsistency, in this study we propose a novel Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on graphs, leading to new learning and prediction paradigms of GNNs. In particular, we design a "Quasi-Wasserstein" distance between the observed multi-dimensional node labels and their estimations, optimizing the label transport defined on graph edges. The estimations are parameterized by a GNN in which the optimal label transport may determine the graph edge weights optionally. By reformulating the strict constraint of the label transport to a Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein loss associated with two efficient solvers learning the GNN together with optimal label transp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#36890;&#29992;&#26410;&#30693;&#25915;&#20987;&#30340;&#20154;&#33080;&#21453;&#27450;&#35784;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Transformer-based&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#21512;&#25104;&#26410;&#30693;&#25915;&#20987;&#26679;&#26412;&#29983;&#25104;&#22120;&#65288;SUASG&#65289;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#24050;&#30693;&#21644;&#26410;&#30693;&#25915;&#20987;&#30340;&#39046;&#22495;&#36890;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11758</link><description>&lt;p&gt;
&#38754;&#21521;&#26410;&#30693;&#25915;&#20987;&#30340;&#39046;&#22495;&#36890;&#29992;&#20154;&#33080;&#21453;&#27450;&#35784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain-Generalized Face Anti-Spoofing with Unknown Attacks. (arXiv:2310.11758v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11758
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#36890;&#29992;&#26410;&#30693;&#25915;&#20987;&#30340;&#20154;&#33080;&#21453;&#27450;&#35784;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Transformer-based&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#21512;&#25104;&#26410;&#30693;&#25915;&#20987;&#26679;&#26412;&#29983;&#25104;&#22120;&#65288;SUASG&#65289;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#24050;&#30693;&#21644;&#26410;&#30693;&#25915;&#20987;&#30340;&#39046;&#22495;&#36890;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#33080;&#21453;&#27450;&#35784;&#65288;FAS&#65289;&#26041;&#27861;&#22312;&#29305;&#23450;&#39046;&#22495;&#25110;&#25915;&#20987;&#31867;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#39046;&#22495;&#25913;&#21464;&#21644;&#26410;&#30693;&#25915;&#20987;&#21516;&#26102;&#23384;&#22312;&#30340;&#24773;&#20917;&#65292;&#36825;&#26356;&#25509;&#36817;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;&#20026;&#20102;&#22788;&#29702;&#39046;&#22495;&#36890;&#29992;&#30340;&#26410;&#30693;&#25915;&#20987;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292; DGUA-FAS&#65292;&#23427;&#30001;&#22522;&#20110;Transformer&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#19968;&#20010;&#21512;&#25104;&#26410;&#30693;&#25915;&#20987;&#26679;&#26412;&#29983;&#25104;&#22120;&#65288;SUASG&#65289;&#32452;&#25104;&#12290;SUASG&#32593;&#32476;&#27169;&#25311;&#26410;&#30693;&#25915;&#20987;&#26679;&#26412;&#26469;&#36741;&#21161;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24050;&#30693;&#25110;&#26410;&#30693;&#25915;&#20987;&#30340;&#39046;&#22495;&#36890;&#29992;&#21453;&#27450;&#35784;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although face anti-spoofing (FAS) methods have achieved remarkable performance on specific domains or attack types, few studies have focused on the simultaneous presence of domain changes and unknown attacks, which is closer to real application scenarios. To handle domain-generalized unknown attacks, we introduce a new method, DGUA-FAS, which consists of a Transformer-based feature extractor and a synthetic unknown attack sample generator (SUASG). The SUASG network simulates unknown attack samples to assist the training of the feature extractor. Experimental results show that our method achieves superior performance on domain generalization FAS with known or unknown attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-GP-UCB&#26041;&#27861;&#20272;&#35745;&#20114;&#21160;&#29289;&#20307;&#26448;&#26009;&#24615;&#36136;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#32452;&#20114;&#21160;&#29289;&#20307;&#22330;&#26223;&#30340;&#35266;&#23519;&#19979;&#65292;&#36890;&#36807;&#24314;&#27169;&#22870;&#21169;&#20989;&#25968;&#32467;&#26500;&#21644;&#37096;&#20998;&#35780;&#20272;&#26469;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.11749</link><description>&lt;p&gt;
&#20351;&#29992;Sum-GP-UCB&#26041;&#27861;&#20272;&#35745;&#20114;&#21160;&#29289;&#20307;&#30340;&#26448;&#26009;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Estimating Material Properties of Interacting Objects Using Sum-GP-UCB. (arXiv:2310.11749v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-GP-UCB&#26041;&#27861;&#20272;&#35745;&#20114;&#21160;&#29289;&#20307;&#26448;&#26009;&#24615;&#36136;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#32452;&#20114;&#21160;&#29289;&#20307;&#22330;&#26223;&#30340;&#35266;&#23519;&#19979;&#65292;&#36890;&#36807;&#24314;&#27169;&#22870;&#21169;&#20989;&#25968;&#32467;&#26500;&#21644;&#37096;&#20998;&#35780;&#20272;&#26469;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#38656;&#35201;&#36890;&#36807;&#35266;&#23519;&#26469;&#20934;&#30830;&#27169;&#25311;&#29289;&#20307;&#30340;&#26448;&#26009;&#21644;&#21160;&#24577;&#29305;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#19968;&#32452;&#35266;&#23519;&#26469;&#35782;&#21035;&#29289;&#20307;&#30340;&#26448;&#26009;&#23646;&#24615;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22522;&#20110;&#19981;&#21516;&#32452;&#20114;&#21160;&#29289;&#20307;&#22330;&#26223;&#30340;&#35266;&#23519;&#26469;&#20272;&#35745;&#36825;&#20123;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22870;&#21169;&#20989;&#25968;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21035;&#24314;&#27169;&#27599;&#20010;&#35266;&#23519;&#30340;&#22870;&#21169;&#65292;&#24182;&#20165;&#20351;&#29992;&#35813;&#22330;&#26223;&#20013;&#29289;&#20307;&#30340;&#21442;&#25968;&#20316;&#20026;&#36755;&#20837;&#12290;&#24471;&#21040;&#30340;&#20302;&#32500;&#27169;&#22411;&#22312;&#21442;&#25968;&#31354;&#38388;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21152;&#24555;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#20943;&#23569;&#23547;&#25214;&#20248;&#31168;&#21442;&#25968;&#20540;&#25152;&#38656;&#30340;&#20223;&#30495;&#27425;&#25968;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#37096;&#20998;&#35780;&#20272;&#26041;&#27861;&#65292;&#20854;&#20013;&#36873;&#25321;&#30340;&#21442;&#25968;&#20165;&#22312;&#19968;&#37096;&#20998;&#30495;&#23454;&#19990;&#30028;&#35780;&#20272;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots need to estimate the material and dynamic properties of objects from observations in order to simulate them accurately. We present a Bayesian optimization approach to identifying the material property parameters of objects based on a set of observations. Our focus is on estimating these properties based on observations of scenes with different sets of interacting objects. We propose an approach that exploits the structure of the reward function by modeling the reward for each observation separately and using only the parameters of the objects in that scene as inputs. The resulting lower-dimensional models generalize better over the parameter space, which in turn results in a faster optimization. To speed up the optimization process further, and reduce the number of simulation runs needed to find good parameter values, we also propose partial evaluations of the reward function, wherein the selected parameters are only evaluated on a subset of real world evaluations. The approach 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;ASR&#27169;&#22411;&#20013;&#30340;&#24847;&#22806;&#35760;&#24518;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23457;&#35745;&#26041;&#27861;&#26469;&#27979;&#37327;&#35760;&#24518;&#25928;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;ASR&#27169;&#22411;&#23384;&#22312;&#35760;&#24518;&#29616;&#35937;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#20351;&#29992;&#28176;&#21464;&#35009;&#21098;&#36827;&#34892;&#35757;&#32451;&#26469;&#32531;&#35299;&#35760;&#24518;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.11739</link><description>&lt;p&gt;
&#22823;&#22411;ASR&#27169;&#22411;&#20013;&#30340;&#24847;&#22806;&#35760;&#24518;&#21450;&#20854;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unintended Memorization in Large ASR Models, and How to Mitigate It. (arXiv:2310.11739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11739
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;ASR&#27169;&#22411;&#20013;&#30340;&#24847;&#22806;&#35760;&#24518;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23457;&#35745;&#26041;&#27861;&#26469;&#27979;&#37327;&#35760;&#24518;&#25928;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;ASR&#27169;&#22411;&#23384;&#22312;&#35760;&#24518;&#29616;&#35937;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#20351;&#29992;&#28176;&#21464;&#35009;&#21098;&#36827;&#34892;&#35757;&#32451;&#26469;&#32531;&#35299;&#35760;&#24518;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#35760;&#20303;&#35757;&#32451;&#26679;&#26412;&#65292;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22411;&#38750;&#33258;&#22238;&#24402;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#20013;&#23457;&#35745;&#35760;&#24518;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;&#30828;&#24230;&#26657;&#20934;&#65289;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23457;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;ASR&#27169;&#22411;&#20013;&#27979;&#37327;&#35760;&#24518;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21152;&#36895;&#38543;&#26426;&#29983;&#25104;&#30340;&#35805;&#35821;&#65292;&#21019;&#24314;&#19968;&#20010;&#35821;&#38899;&#21644;&#25991;&#26412;&#20449;&#24687;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#36825;&#22312;&#20856;&#22411;&#30340;&#35757;&#32451;&#26679;&#26412;&#20013;&#24456;&#38590;&#23398;&#20064;&#21040;&#12290;&#22240;&#27492;&#65292;&#20165;&#23545;&#21152;&#36895;&#35757;&#32451;&#26679;&#26412;&#30340;&#20934;&#30830;&#39044;&#27979;&#21487;&#20197;&#20316;&#20026;&#35760;&#24518;&#30340;&#26126;&#30830;&#35777;&#25454;&#65292;&#24182;&#19988;&#30456;&#24212;&#30340;&#20934;&#30830;&#24615;&#21487;&#20197;&#29992;&#26469;&#34913;&#37327;&#35760;&#24518;&#12290;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;ASR&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#29616;&#35937;&#12290;&#20026;&#20102;&#32531;&#35299;&#35760;&#24518;&#65292;&#25105;&#20204;&#23581;&#35797;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#28176;&#21464;&#35009;&#21098;&#65292;&#20197;&#38480;&#21046;&#20219;&#20309;&#21333;&#20010;&#26679;&#26412;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well-known that neural networks can unintentionally memorize their training examples, causing privacy concerns. However, auditing memorization in large non-auto-regressive automatic speech recognition (ASR) models has been challenging due to the high compute cost of existing methods such as hardness calibration. In this work, we design a simple auditing method to measure memorization in large ASR models without the extra compute overhead. Concretely, we speed up randomly-generated utterances to create a mapping between vocal and text information that is difficult to learn from typical training examples. Hence, accurate predictions only for sped-up training examples can serve as clear evidence for memorization, and the corresponding accuracy can be used to measure memorization. Using the proposed method, we showcase memorization in the state-of-the-art ASR models. To mitigate memorization, we tried gradient clipping during training to bound the influence of any individual example 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#23384;&#22312;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#23545;&#31572;&#26696;&#20915;&#31574;&#21644;&#26684;&#24335;&#20559;&#22909;&#36127;&#36131;&#12290;&#23545;&#40784;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#28151;&#28102;&#12290;</title><link>http://arxiv.org/abs/2310.11732</link><description>&lt;p&gt;
&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#30740;&#31350;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting. (arXiv:2310.11732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#23384;&#22312;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#23545;&#31572;&#26696;&#20915;&#31574;&#21644;&#26684;&#24335;&#20559;&#22909;&#36127;&#36131;&#12290;&#23545;&#40784;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20542;&#21521;&#20110;&#19982;&#39044;&#35757;&#32451;&#30340;LM&#30456;&#27604;&#65292;&#22312;&#36755;&#20986;&#31572;&#26696;&#26102;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#23545;&#40784;&#36807;&#31243;&#23545;&#22810;&#36873;&#35774;&#32622;&#19979;LM&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#23545;&#40784;LM&#22312;&#26657;&#20934;&#26041;&#38754;&#19982;&#20854;&#39044;&#35757;&#32451;&#23545;&#24212;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#36827;&#34892;&#20102;&#35748;&#30495;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#22810;&#36873;&#35774;&#32622;&#19979;&#65292;LM&#23384;&#22312;&#20004;&#31181;&#26126;&#26174;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#36127;&#36131;&#31572;&#26696;&#20915;&#31574;&#21644;LM&#30340;&#26684;&#24335;&#20559;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#31616;&#21333;&#30340;&#21512;&#25104;&#23545;&#40784;&#26041;&#26696;&#20013;&#36827;&#34892;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#22312;&#23545;&#40784;LM&#30340;&#26657;&#20934;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#23545;&#40784;LM&#36807;&#24230;&#33258;&#20449;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#28151;&#28102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;&#24120;&#35265;&#30340;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant progress made in practical applications of aligned language models (LMs), they tend to be overconfident in output answers compared to the corresponding pre-trained LMs. In this work, we systematically evaluate the impact of the alignment process on logit-based uncertainty calibration of LMs under the multiple-choice setting. We first conduct a thoughtful empirical study on how aligned LMs differ in calibration from their pre-trained counterparts. Experimental results reveal that there are two distinct uncertainties in LMs under the multiple-choice setting, which are responsible for the answer decision and the format preference of the LMs, respectively. Then, we investigate the role of these two uncertainties on aligned LM's calibration through fine-tuning in simple synthetic alignment schemes and conclude that one reason for aligned LMs' overconfidence is the conflation of these two types of uncertainty. Furthermore, we examine the utility of common post-hoc cal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#21327;&#21516;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.11730</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#30340;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation. (arXiv:2310.11730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#21327;&#21516;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#36890;&#36807;&#20803;&#36335;&#24452;&#25551;&#36848;&#20016;&#23500;&#30340;&#35821;&#20041;&#65292;&#24050;&#25104;&#20026;&#32531;&#35299;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;HIN&#30340;&#25512;&#33616;&#31995;&#32479;&#25345;&#26377;&#25968;&#25454;&#30340;&#38598;&#20013;&#23384;&#20648;&#20551;&#35774;&#65292;&#24182;&#36827;&#34892;&#38598;&#20013;&#24335;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24448;&#24448;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#23384;&#20648;&#65292;&#23548;&#33268;&#38598;&#20013;&#24335;HIN&#25512;&#33616;&#26080;&#27861;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;HIN&#20998;&#20026;&#23458;&#25143;&#31471;&#23384;&#20648;&#30340;&#31169;&#26377;HIN&#21644;&#26381;&#21153;&#22120;&#31471;&#30340;&#20849;&#20139;HIN&#12290;&#22312;&#27492;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;HIN&#19978;&#21327;&#20316;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#27844;&#38706;&#29992;&#25143;&#38544;&#31169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#22522;&#20110;HIN&#30340;&#32852;&#21512;&#25512;&#33616;&#65292;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#20809;&#19979;&#30830;&#23450;&#20102;&#38544;&#31169;&#23450;&#20041;&#65292;&#26088;&#22312;&#20445;&#25252;&#31169;&#26377;HIN&#30340;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#65292;&#20197;&#21450;&#29992;&#25143;&#30340;&#38544;&#31169;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has become a powerful tool to alleviate data sparsity in recommender systems. Existing HIN-based recommendations hold the data centralized storage assumption and conduct centralized model training. However, the real-world data is often stored in a distributed manner for privacy concerns, resulting in the failure of centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored in the client side and shared HINs in the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which can collaboratively train a recommendation model on distributed HINs without leaking user privacy. Specifically, we first formalize the privacy definition in the light of differential privacy for HIN-based federated recommendation, which aims to protect user-item interactions of private HIN as well as user's 
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>http://arxiv.org/abs/2310.11721</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding. (arXiv:2310.11721v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11721
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) is a technique that guides Large Language Models (LLMs) to decompose complex tasks into multi-step reasoning through intermediate steps in natural language form. Briefly, CoT enables LLMs to think step by step. However, although many Natural Language Understanding (NLU) tasks also require thinking step by step, LLMs perform less well than small-scale Masked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose Chain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt tuning, to implement step-by-step thinking for MLMs on NLU tasks. From the perspective of CoT, CoTT's two-step framework enables MLMs to implement task decomposition; CoTT's prompt tuning allows intermediate steps to be used in natural language form. Thereby, the success of CoT can be extended to NLU tasks through MLMs. To verify the effectiveness of CoTT, we conduct experiments on two NLU tasks: hierarchical classification and relation extraction, and the results 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;Fr\'echet inception&#36317;&#31163;&#65288;FID&#65289;&#65292;&#24182;&#32771;&#34385;&#19981;&#21516;&#32858;&#21512;&#20998;&#25968;&#65292;&#21457;&#29616;FID-all&#21644;FID-avg&#20998;&#25968;&#30340;&#27169;&#22411;&#25490;&#21517;&#21487;&#33021;&#19981;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.11714</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On the Evaluation of Generative Models in Distributed Learning Tasks. (arXiv:2310.11714v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;Fr\'echet inception&#36317;&#31163;&#65288;FID&#65289;&#65292;&#24182;&#32771;&#34385;&#19981;&#21516;&#32858;&#21512;&#20998;&#25968;&#65292;&#21457;&#29616;FID-all&#21644;FID-avg&#20998;&#25968;&#30340;&#27169;&#22411;&#25490;&#21517;&#21487;&#33021;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#23545;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#22312;&#20869;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#21333;&#20010;&#23458;&#25143;&#31471;&#23384;&#20648;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#38598;&#20013;&#24335;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#29983;&#25104;&#27169;&#22411;&#30340;&#35768;&#22810;&#24212;&#29992;&#28041;&#21450;&#21040;&#20998;&#24067;&#24335;&#23398;&#20064;&#29615;&#22659;&#65292;&#20363;&#22914;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#30001;&#22810;&#20010;&#23458;&#25143;&#31471;&#25910;&#38598;&#24182;&#20998;&#21457;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20851;&#27880;Fr\'echet inception&#36317;&#31163;&#65288;FID&#65289;&#65292;&#24182;&#32771;&#34385;&#20197;&#19979;&#22522;&#20110;FID&#30340;&#32858;&#21512;&#20998;&#25968;&#65306;1&#65289;FID-avg&#20316;&#20026;&#23458;&#25143;&#31471;&#20010;&#20307;FID&#20998;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;2&#65289;FID-all&#20316;&#20026;&#35757;&#32451;&#27169;&#22411;&#19982;&#21253;&#21547;&#25152;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#38598;&#20307;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;FID&#36317;&#31163;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26681;&#25454;FID-all&#21644;FID-avg&#20998;&#25968;&#30340;&#27169;&#22411;&#25490;&#21517;&#21487;&#33021;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of deep generative models including generative adversarial networks (GANs) and diffusion models has been extensively studied in the literature. While the existing evaluation methods mainly target a centralized learning problem with training data stored by a single client, many applications of generative models concern distributed learning settings, e.g. the federated learning scenario, where training data are collected by and distributed among several clients. In this paper, we study the evaluation of generative models in distributed learning tasks with heterogeneous data distributions. First, we focus on the Fr\'echet inception distance (FID) and consider the following FID-based aggregate scores over the clients: 1) FID-avg as the mean of clients' individual FID scores, 2) FID-all as the FID distance of the trained model to the collective dataset containing all clients' data. We prove that the model rankings according to the FID-all and FID-avg scores could be inconsist
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26631;&#31614;&#27604;&#20363;(LLP)&#30340;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#32447;&#25216;&#26415;&#36827;&#34892;&#25913;&#36827;&#65292;&#32467;&#21512;&#33258;&#30417;&#30563;&#30446;&#26631;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11707</link><description>&lt;p&gt;
&#23398;&#20064;&#25991;&#26412;&#20998;&#31867;&#30340;&#26631;&#31614;&#27604;&#20363;
&lt;/p&gt;
&lt;p&gt;
Learning under Label Proportions for Text Classification. (arXiv:2310.11707v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11707
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26631;&#31614;&#27604;&#20363;(LLP)&#30340;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#32447;&#25216;&#26415;&#36827;&#34892;&#25913;&#36827;&#65292;&#32467;&#21512;&#33258;&#30417;&#30563;&#30446;&#26631;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#23398;&#20064;&#20174;&#26631;&#31614;&#27604;&#20363;(LLP)&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20316;&#65292;&#20854;&#20013;&#25968;&#25454;&#20197;&#34955;&#29366;&#32858;&#21512;&#24418;&#24335;&#25552;&#20379;&#65292;&#20165;&#29992;&#27599;&#20010;&#31867;&#21035;&#20013;&#26679;&#26412;&#30340;&#27604;&#20363;&#20316;&#20026;&#30495;&#20540;&#12290;&#36825;&#20010;&#35774;&#32622;&#19982;&#22312;&#38544;&#31169;&#35774;&#32622;&#21644;&#24369;&#30417;&#30563;&#19979;&#35757;&#32451;&#27169;&#22411;&#30340;&#26399;&#26395;&#29305;&#24615;&#30456;&#21563;&#21512;&#12290;&#36890;&#36807;&#23545;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#32447;&#25216;&#26415;DLLP&#30340;&#19968;&#20123;&#19981;&#35268;&#21017;&#36827;&#34892;&#34920;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#20844;&#24335;&#12290;&#36825;&#20276;&#38543;&#30528;&#19968;&#20010;&#23398;&#20064;&#32467;&#26524;&#65292;&#22312;LLP&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#27867;&#21270;&#30028;&#38480;&#12290;&#23558;&#36825;&#20010;&#20844;&#24335;&#19982;&#19968;&#20010;&#33258;&#30417;&#30563;&#30446;&#26631;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#38271;&#25991;&#26412;&#21644;&#30701;&#25991;&#26412;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#20197;&#21450;&#22810;&#20010;&#25351;&#26631;&#30340;&#22810;&#20010;&#23454;&#39564;&#37197;&#32622;&#20013;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#65292;&#36798;&#21040;&#20102;&#36817;87%&#30340;&#23454;&#39564;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present one of the preliminary NLP works under the challenging setup of Learning from Label Proportions (LLP), where the data is provided in an aggregate form called bags and only the proportion of samples in each class as the ground truth. This setup is inline with the desired characteristics of training models under Privacy settings and Weakly supervision. By characterizing some irregularities of the most widely used baseline technique DLLP, we propose a novel formulation that is also robust. This is accompanied with a learnability result that provides a generalization bound under LLP. Combining this formulation with a self-supervised objective, our method achieves better results as compared to the baselines in almost 87% of the experimental configurations which include large scale models for both long and short range texts across multiple metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#30452;&#25509;&#22788;&#29702;&#21333;&#35270;&#35282;&#35270;&#39057;&#30340;&#36305;&#32773;&#20877;&#35782;&#21035;&#31995;&#32479;&#12290;&#36890;&#36807;&#33258;&#21160;&#22788;&#29702;&#21407;&#22987;&#35270;&#39057;&#20316;&#20026;&#36755;&#20837;&#26469;&#35782;&#21035;&#36305;&#32773;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#36305;&#32773;&#34987;&#26694;&#36873;&#20986;&#22810;&#27425;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.11700</link><description>&lt;p&gt;
&#21333;&#35270;&#35282;&#35270;&#39057;&#20013;&#30340;&#36305;&#32773;&#20877;&#35782;&#21035;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;
&lt;/p&gt;
&lt;p&gt;
Runner re-identification from single-view video in the open-world setting. (arXiv:2310.11700v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#30452;&#25509;&#22788;&#29702;&#21333;&#35270;&#35282;&#35270;&#39057;&#30340;&#36305;&#32773;&#20877;&#35782;&#21035;&#31995;&#32479;&#12290;&#36890;&#36807;&#33258;&#21160;&#22788;&#29702;&#21407;&#22987;&#35270;&#39057;&#20316;&#20026;&#36755;&#20837;&#26469;&#35782;&#21035;&#36305;&#32773;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#36305;&#32773;&#34987;&#26694;&#36873;&#20986;&#22810;&#27425;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20307;&#32946;&#36816;&#21160;&#20013;&#65292;&#36305;&#32773;&#30340;&#20877;&#35782;&#21035;&#23545;&#20110;&#33258;&#21160;&#35270;&#39057;&#22788;&#29702;&#21644;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#22810;&#35270;&#35282;&#25110;&#21333;&#35270;&#35282;&#20307;&#32946;&#35270;&#39057;&#20013;&#36305;&#32773;&#20877;&#35782;&#21035;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20351;&#29992;&#26631;&#35760;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#23553;&#38381;&#19990;&#30028;&#35774;&#23450;&#30340;&#20877;&#35782;&#21035;&#19978;&#65292;&#32780;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#19979;&#36827;&#34892;&#33258;&#21160;&#35270;&#39057;&#20998;&#26512;&#30340;&#36305;&#32773;&#20877;&#35782;&#21035;&#24182;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#22788;&#29702;&#21333;&#35270;&#35282;&#35270;&#39057;&#20197;&#35299;&#20915;&#24320;&#25918;&#19990;&#30028;&#35774;&#32622;&#30340;&#36305;&#32773;&#20877;&#35782;&#21035;&#31995;&#32479;&#12290;&#22312;&#24320;&#25918;&#19990;&#30028;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#26080;&#27861;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#24517;&#39035;&#30452;&#25509;&#22788;&#29702;&#35270;&#39057;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#33258;&#21160;&#22788;&#29702;&#21407;&#22987;&#35270;&#39057;&#20316;&#20026;&#36755;&#20837;&#26469;&#35782;&#21035;&#36305;&#32773;&#65292;&#21363;&#20351;&#36305;&#32773;&#20986;&#29616;&#22810;&#27425;&#34987;&#26694;&#36873;&#20986;&#65292;&#31995;&#32479;&#20063;&#33021;&#36827;&#34892;&#35782;&#21035;&#12290;&#23545;&#20110;&#33258;&#21160;&#22788;&#29702;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;YOLOv8&#21644;&#24494;&#35843;&#30340;EfficientNet&#26469;&#26816;&#27979;&#35270;&#39057;&#20013;&#30340;&#36305;&#32773;&#12290;&#28982;&#21518;&#20351;&#29992;ByteTrack&#26469;&#36319;&#36394;&#36305;&#32773;&#65292;&#24182;&#20351;&#29992;&#24494;&#35843;&#30340;YOLO&#26469;&#26816;&#27979;&#20182;&#20204;&#30340;&#38795;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many sports, player re-identification is crucial for automatic video processing and analysis. However, most of the current studies on player re-identification in multi- or single-view sports videos focus on re-identification in the closed-world setting using labeled image dataset, and player re-identification in the open-world setting for automatic video analysis is not well developed. In this paper, we propose a runner re-identification system that directly processes single-view video to address the open-world setting. In the open-world setting, we cannot use labeled dataset and have to process video directly. The proposed system automatically processes raw video as input to identify runners, and it can identify runners even when they are framed out multiple times. For the automatic processing, we first detect the runners in the video using the pre-trained YOLOv8 and the fine-tuned EfficientNet. We then track the runners using ByteTrack and detect their shoes with the fine-tuned YO
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;mixup&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#21892;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#26041;&#27861;&#65288;DAM&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#35299;&#20915;&#22312;&#24212;&#29992;&#20110;&#23567;&#25968;&#25454;&#38598;&#26102;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.11693</link><description>&lt;p&gt;
AUC-mixup: &#32467;&#21512;mixup&#30340;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AUC-mixup: Deep AUC Maximization with Mixup. (arXiv:2310.11693v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;mixup&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#21892;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#26041;&#27861;&#65288;DAM&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#35299;&#20915;&#22312;&#24212;&#29992;&#20110;&#23567;&#25968;&#25454;&#38598;&#26102;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#22312;&#19981;&#24179;&#34913;&#21307;&#23398;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#33016;&#37096;X&#20809;&#20998;&#31867;&#21644;&#30382;&#32932;&#25439;&#20260;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#23567;&#25968;&#25454;&#38598;&#26102;&#65292;&#30001;&#20110;&#20854;&#23558;&#27491;&#26679;&#26412;&#30340;&#39044;&#27979;&#20998;&#25968;&#19982;&#36127;&#26679;&#26412;&#30340;&#20998;&#25968;&#36828;&#31163;&#30340;&#28608;&#36827;&#24615;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;mixup&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#21892;DAM&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;mixup&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#25552;&#39640;&#20132;&#21449;&#29109;&#25439;&#22833;&#22522;&#30784;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep AUC maximization (DAM) has shown remarkable success on imbalanced medical tasks, e.g., chest X-rays classification and skin lesions classification, it could suffer from severe overfitting when applied to small datasets due to its aggressive nature of pushing prediction scores of positive data away from that of negative data. This paper studies how to improve generalization of DAM by mixup data augmentation -- an approach that is widely used for improving generalization of the cross-entropy loss based deep learning methods. %For overfitting issues arising from limited data, the common approach is to employ mixup data augmentation to boost the models' generalization performance by enriching the training data. However, AUC is defined over positive and negative pairs, which makes it challenging to incorporate mixup data augmentation into DAM algorithms. To tackle this challenge, we employ the AUC margin loss and incorporate soft labels into the formulation to effectively learn f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#30340;&#30005;&#21147;&#31995;&#32479;&#30701;&#26399;&#30005;&#21387;&#31283;&#23450;&#24615;&#35780;&#20272;&#12290;&#20351;&#29992;&#26465;&#20214;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#31283;&#23450;&#24615;&#35780;&#20272;Transformer&#20316;&#20026;&#20998;&#31867;&#27169;&#22411;&#65292;&#21453;&#26144;&#31995;&#32479;&#36816;&#34892;&#29366;&#24577;&#19982;&#31283;&#23450;&#24615;&#32467;&#26524;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11690</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#30340;&#30005;&#21147;&#31995;&#32479;&#30701;&#26399;&#30005;&#21387;&#31283;&#23450;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Deep learning based on Transformer architecture for power system short-term voltage stability assessment with class imbalance. (arXiv:2310.11690v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11690
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#30340;&#30005;&#21147;&#31995;&#32479;&#30701;&#26399;&#30005;&#21387;&#31283;&#23450;&#24615;&#35780;&#20272;&#12290;&#20351;&#29992;&#26465;&#20214;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#31283;&#23450;&#24615;&#35780;&#20272;Transformer&#20316;&#20026;&#20998;&#31867;&#27169;&#22411;&#65292;&#21453;&#26144;&#31995;&#32479;&#36816;&#34892;&#29366;&#24577;&#19982;&#31283;&#23450;&#24615;&#32467;&#26524;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#30005;&#21147;&#31995;&#32479;&#30701;&#26399;&#30005;&#21387;&#31283;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#20551;&#23450;&#36755;&#20837;&#25968;&#25454;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#24178;&#25200;&#21518;&#30701;&#26399;&#30005;&#21387;&#19981;&#31283;&#23450;&#24615;&#30340;&#21457;&#29983;&#24456;&#23569;&#65292;&#23548;&#33268;&#26174;&#33879;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#21644;&#20998;&#31867;&#22120;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#30701;&#26399;&#30005;&#21387;&#31283;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#26412;&#30340;Transformer&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#31283;&#23450;&#24615;&#35780;&#20272;Transformer (StaaT)&#20316;&#20026;&#19968;&#20010;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#21453;&#26144;&#31995;&#32479;&#36816;&#34892;&#29366;&#24577;&#19982;&#31283;&#23450;&#24615;&#32467;&#26524;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#24102;&#26377;&#26799;&#24230;&#24809;&#32602;&#30340;&#26465;&#20214;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (CWGAN-GP)&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#24110;&#21161;&#21019;&#24314;&#19968;&#20010;&#24179;&#34913;&#12289;&#20195;&#34920;&#24615;&#30340;&#35757;&#32451;&#38598;&#29992;&#20110;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing data-driven power system short-term voltage stability assessment (STVSA) approaches presume class-balanced input data. However, in practical applications, the occurrence of short-term voltage instability following a disturbance is minimal, leading to a significant class imbalance problem and a consequent decline in classifier performance. This work proposes a Transformer-based STVSA method to address this challenge. By utilizing the basic Transformer architecture, a stability assessment Transformer (StaaT) is developed {as a classification model to reflect the correlation between the operational states of the system and the resulting stability outcomes}. To combat the negative impact of imbalanced datasets, this work employs a conditional Wasserstein generative adversarial network with gradient penalty (CWGAN-GP) for synthetic data generation, aiding in the creation of a balanced, representative training set for the classifier. Semi-supervised clustering learning is imple
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#35780;&#20272;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#35843;&#25972;&#65292;&#33021;&#22815;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#24182;&#25552;&#39640;&#20854;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11689</link><description>&lt;p&gt;
&#33258;&#25105;&#35780;&#20272;&#30340;&#33258;&#36866;&#24212;&#25913;&#36827;LLMs&#20013;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs. (arXiv:2310.11689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#35780;&#20272;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#35843;&#25972;&#65292;&#33021;&#22815;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#24182;&#25552;&#39640;&#20854;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#22330;&#26223;&#20013;&#20173;&#28982;&#38480;&#20110;&#20854;&#28508;&#22312;&#30340;&#38169;&#35823;&#12290;&#36873;&#25321;&#24615;&#39044;&#27979;&#26159;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#22312;LLMs&#19981;&#30830;&#23450;&#26102;&#20351;&#20854;&#36991;&#20813;&#39044;&#27979;&#32780;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#30340;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#35780;&#20272;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;LLMs&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#20351;&#29992;&#21442;&#25968;&#25928;&#29575;&#35843;&#25972;&#26469;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#24182;&#25913;&#36827;&#20854;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#30340;&#24605;&#24819;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;&#22312;CoQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;AUACC&#20174;91.23%&#25552;&#39640;&#21040;92.63%&#65292;&#24182;&#23558;AURO
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. Selective prediction is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the AURO
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;softmax&#21644;&#32447;&#24615;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;softmax&#27880;&#24847;&#21147;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2310.11685</link><description>&lt;p&gt;
Softmax&#30340;&#20248;&#36234;&#24615;&#65306;&#25581;&#31034;&#20854;&#30456;&#23545;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#24615;&#33021;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention. (arXiv:2310.11685v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11685
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;softmax&#21644;&#32447;&#24615;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#65292;&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;softmax&#27880;&#24847;&#21147;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;Transformer&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#22312;Transformer&#26550;&#26500;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#20013;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#36890;&#36807;&#21033;&#29992;softmax&#20989;&#25968;&#25429;&#25417;&#24207;&#21015;&#20013;&#30340;&#26631;&#35760;&#20132;&#20114;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30456;&#21453;&#65292;&#32447;&#24615;&#27880;&#24847;&#21147;&#36890;&#36807;&#32447;&#24615;&#22797;&#26434;&#24230;&#36817;&#20284;softmax&#25805;&#20316;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;softmax&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#27604;&#65292;&#23427;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#38477;&#32423;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20004;&#31181;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;softmax&#27880;&#24847;&#21147;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large transformer models have achieved state-of-the-art results in numerous natural language processing tasks. Among the pivotal components of the transformer architecture, the attention mechanism plays a crucial role in capturing token interactions within sequences through the utilization of softmax function.  Conversely, linear attention presents a more computationally efficient alternative by approximating the softmax operation with linear complexity. However, it exhibits substantial performance degradation when compared to the traditional softmax attention mechanism.  In this paper, we bridge the gap in our theoretical understanding of the reasons behind the practical performance gap between softmax and linear attention. By conducting a comprehensive comparative analysis of these two attention mechanisms, we shed light on the underlying reasons for why softmax attention outperforms linear attention in most scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#37327;&#23376;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#30456;&#36739;&#20110;&#32463;&#20856;&#31639;&#27861;&#65292;&#22312;&#36951;&#25022;&#30028;&#38480;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.11684</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#37327;&#23376;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning. (arXiv:2310.11684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#37327;&#23376;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#30456;&#36739;&#20110;&#32463;&#20856;&#31639;&#27861;&#65292;&#22312;&#36951;&#25022;&#30028;&#38480;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#37327;&#23376;&#21152;&#36895;&#22312;&#35299;&#20915;&#26080;&#38480;&#26102;&#22495;Markov&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#25552;&#39640;&#24179;&#22343;&#22870;&#21169;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#29992;&#20110;&#20195;&#29702;&#19982;&#26410;&#30693;MDP&#30340;&#20114;&#21160;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#20132;&#20114;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#20048;&#35266;&#20027;&#23548;&#30340;&#20855;&#26377;&#37327;&#23376;&#20449;&#21495;&#30340;&#34920;&#26684;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#33719;&#21462;&#20195;&#29702;&#33719;&#21462;&#30340;&#37327;&#23376;&#20449;&#21495;&#12290;&#36890;&#36807;&#28145;&#20837;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#30340;&#20248;&#21183;&#33021;&#22815;&#22312;&#26080;&#38480;&#26102;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#23548;&#33268;&#36951;&#25022;&#20445;&#35777;&#30340;&#25351;&#25968;&#36827;&#23637;&#12290;&#20855;&#20307;&#22320;&#65292;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#36951;&#25022;&#30028;&#20026;$\tilde{\mathcal{O}}(1)$&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#30456;&#23545;&#20110;&#32463;&#20856;&#23545;&#24212;&#31639;&#27861;&#25152;&#23637;&#31034;&#30340;$\tilde{\mathcal{O}}(\sqrt{T})$&#30028;&#38480;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the potential of quantum acceleration in addressing infinite horizon Markov Decision Processes (MDPs) to enhance average reward outcomes. We introduce an innovative quantum framework for the agent's engagement with an unknown MDP, extending the conventional interaction paradigm. Our approach involves the design of an optimism-driven tabular Reinforcement Learning algorithm that harnesses quantum signals acquired by the agent through efficient quantum mean estimation techniques. Through thorough theoretical analysis, we demonstrate that the quantum advantage in mean estimation leads to exponential advancements in regret guarantees for infinite horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm achieves a regret bound of $\tilde{\mathcal{O}}(1)$, a significant improvement over the $\tilde{\mathcal{O}}(\sqrt{T})$ bound exhibited by classical counterparts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#39564;&#20998;&#31867;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#36716;&#21270;&#20026;&#26377;&#38480;&#36712;&#36857;&#19978;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#34920;&#36798;&#65292;&#24182;&#21033;&#29992;&#20248;&#20808;&#21270;&#32463;&#39564;&#22238;&#25918;&#25216;&#26415;&#25913;&#21892;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#30446;&#26631;&#36923;&#36753;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11678</link><description>&lt;p&gt;
&#20351;&#29992;&#32463;&#39564;&#20998;&#31867;&#35757;&#32451;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Using Experience Classification for Training Non-Markovian Tasks. (arXiv:2310.11678v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11678
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#39564;&#20998;&#31867;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#36716;&#21270;&#20026;&#26377;&#38480;&#36712;&#36857;&#19978;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#34920;&#36798;&#65292;&#24182;&#21033;&#29992;&#20248;&#20808;&#21270;&#32463;&#39564;&#22238;&#25918;&#25216;&#26415;&#25913;&#21892;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#30446;&#26631;&#36923;&#36753;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#20110;&#26631;&#20934;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#35768;&#22810;&#23454;&#38469;&#20219;&#21153;&#26159;&#38750;&#39532;&#23572;&#21487;&#22827;&#30340;&#65292;&#20854;&#22870;&#21169;&#26159;&#22522;&#20110;&#29366;&#24577;&#21382;&#21490;&#32780;&#19981;&#20165;&#20165;&#26159;&#24403;&#21069;&#29366;&#24577;&#12290;&#35299;&#20915;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#37329;&#34701;&#20132;&#26131;&#21644;&#21307;&#23398;&#35786;&#26029;&#65289;&#20013;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#26377;&#38480;&#36712;&#36857;&#19978;&#34920;&#36798;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#30446;&#26631;&#36923;&#36753;LTL$_f$&#65288;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20174;LTL$_f$&#21040;MDPs&#65288;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65289;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#32534;&#30721;&#65292;&#20197;&#21033;&#29992;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#33258;&#21160;&#26426;&#32467;&#26500;&#65288;&#35821;&#20041;&#31561;&#20215;&#20110;LTL$_f$&#35268;&#33539;&#65289;&#30340;&#20248;&#20808;&#21270;&#32463;&#39564;&#22238;&#25918;&#25216;&#26415;&#26469;&#25913;&#21892;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#20010;&#24102;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#30340;&#22522;&#20934;&#38382;&#39064;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlike the standard Reinforcement Learning (RL) model, many real-world tasks are non-Markovian, whose rewards are predicated on state history rather than solely on the current state. Solving a non-Markovian task, frequently applied in practical applications such as autonomous driving, financial trading, and medical diagnosis, can be quite challenging. We propose a novel RL approach to achieve non-Markovian rewards expressed in temporal logic LTL$_f$ (Linear Temporal Logic over Finite Traces). To this end, an encoding of linear complexity from LTL$_f$ into MDPs (Markov Decision Processes) is introduced to take advantage of advanced RL algorithms. Then, a prioritized experience replay technique based on the automata structure (semantics equivalent to LTL$_f$ specification) is utilized to improve the training process. We empirically evaluate several benchmark problems augmented with non-Markovian tasks to demonstrate the feasibility and effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;ANPG&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;ANPG&#23454;&#29616;&#20102;&#26679;&#26412;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11677</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes. (arXiv:2310.11677v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;ANPG&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;ANPG&#23454;&#29616;&#20102;&#26679;&#26412;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#35774;&#35745;&#26679;&#26412;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21152;&#36895;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;ANPG&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#21152;&#36895;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#26469;&#33719;&#21462;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#12290;ANPG&#31639;&#27861;&#22312;&#19968;&#33324;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;O(&#949;^{-2})&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;O(&#949;^{-1})&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#949;&#23450;&#20041;&#20102;&#26368;&#20248;&#24615;&#35823;&#24046;&#12290;&#36825;&#23558;&#26679;&#26412;&#22797;&#26434;&#24230;&#25552;&#39640;&#20102;&#19968;&#20010;log(1/&#949;)&#30340;&#22240;&#23376;&#12290;ANPG&#26159;&#19968;&#20010;&#19968;&#38454;&#31639;&#27861;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#29616;&#26377;&#25991;&#29486;&#20013;&#21487;&#33021;&#26080;&#27861;&#39564;&#35777;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;(IS)&#26435;&#37325;&#26041;&#24046;&#19978;&#30028;&#30340;&#20551;&#35774;&#12290;&#22312;&#26080;Hessian&#21644;&#26080;IS&#31639;&#27861;&#31867;&#20013;&#65292;ANPG&#36229;&#36807;&#20102;&#24050;&#30693;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19968;&#20010;O(&#949;^{-\frac{1}{2}})&#30340;&#22240;&#23376;&#65292;&#24182;&#21516;&#26102;&#36798;&#21040;&#20102;&#23427;&#20204;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of designing sample efficient learning algorithms for infinite horizon discounted reward Markov Decision Process. Specifically, we propose the Accelerated Natural Policy Gradient (ANPG) algorithm that utilizes an accelerated stochastic gradient descent process to obtain the natural policy gradient. ANPG achieves $\mathcal{O}({\epsilon^{-2}})$ sample complexity and $\mathcal{O}(\epsilon^{-1})$ iteration complexity with general parameterization where $\epsilon$ defines the optimality error. This improves the state-of-the-art sample complexity by a $\log(\frac{1}{\epsilon})$ factor. ANPG is a first-order algorithm and unlike some existing literature, does not require the unverifiable assumption that the variance of importance sampling (IS) weights is upper bounded. In the class of Hessian-free and IS-free algorithms, ANPG beats the best-known sample complexity by a factor of $\mathcal{O}(\epsilon^{-\frac{1}{2}})$ and simultaneously matches their state-of-the-art it
&lt;/p&gt;</description></item><item><title>PREM&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31616;&#21270;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#36807;&#31243;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11676</link><description>&lt;p&gt;
PREM:&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection. (arXiv:2310.11676v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11676
&lt;/p&gt;
&lt;p&gt;
PREM&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31616;&#21270;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#36807;&#31243;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#22312;&#35782;&#21035;&#21307;&#23398;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#24120;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#26631;&#27880;&#25968;&#25454;&#30340;&#21294;&#20047;&#65292;&#24050;&#26377;&#30340;&#22522;&#20110;&#37325;&#26500;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#24448;&#24448;&#22312;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#36825;&#28304;&#20110;&#23427;&#20204;&#22797;&#26434;&#30340;&#30446;&#26631;&#21644;&#32321;&#29712;&#30340;&#27169;&#22359;&#12290;&#20026;&#20102;&#25552;&#39640;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;PREprocessing and Matching&#65288;&#31616;&#31216;PREM&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#22270;&#24322;&#24120;&#26816;&#27979;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;PREM&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#39044;&#22788;&#29702;&#27169;&#22359;&#21644;&#37051;&#23621;&#21305;&#37197;&#27169;&#22359;&#12290;PREM&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28040;&#38500;&#20102;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31616;&#21333;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Node-level graph anomaly detection (GAD) plays a critical role in identifying anomalous nodes from graph-structured data in various domains such as medicine, social networks, and e-commerce. However, challenges have arisen due to the diversity of anomalies and the dearth of labeled data. Existing methodologies reconstruction-based and contrastive learning - while effective, often suffer from efficiency issues, stemming from their complex objectives and elaborate modules. To improve the efficiency of GAD, we introduce a simple method termed PREprocessing and Matching (PREM for short). Our approach streamlines GAD, reducing time and memory consumption while maintaining powerful anomaly detection capabilities. Comprising two modules - a pre-processing module and an ego-neighbor matching module - PREM eliminates the necessity for message-passing propagation during training, and employs a simple contrastive loss, leading to considerable reductions in training time and memory usage. Moreov
&lt;/p&gt;</description></item><item><title>SOTOPIA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#26234;&#33021;&#20013;&#30340;&#31038;&#20132;&#26234;&#33021;&#30340;&#20132;&#20114;&#24335;&#29615;&#22659;&#12290;&#36890;&#36807;&#27169;&#25311;&#22797;&#26434;&#30340;&#31038;&#20132;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#31038;&#20132;&#26234;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;SOTOPIA-hard&#24773;&#26223;&#19979;&#12290;GPT-4&#22312;&#36825;&#20010;&#23376;&#38598;&#19978;&#30340;&#30446;&#26631;&#23436;&#25104;&#29575;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2310.11667</link><description>&lt;p&gt;
SOTOPIA: &#20132;&#20114;&#24335;&#35780;&#20272;&#35821;&#35328;&#26234;&#33021;&#20013;&#30340;&#31038;&#20132;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents. (arXiv:2310.11667v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11667
&lt;/p&gt;
&lt;p&gt;
SOTOPIA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#26234;&#33021;&#20013;&#30340;&#31038;&#20132;&#26234;&#33021;&#30340;&#20132;&#20114;&#24335;&#29615;&#22659;&#12290;&#36890;&#36807;&#27169;&#25311;&#22797;&#26434;&#30340;&#31038;&#20132;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#31038;&#20132;&#26234;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;SOTOPIA-hard&#24773;&#26223;&#19979;&#12290;GPT-4&#22312;&#36825;&#20010;&#23376;&#38598;&#19978;&#30340;&#30446;&#26631;&#23436;&#25104;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26159;&#31038;&#20132;&#30340;&#23384;&#22312;&#65307;&#25105;&#20204;&#22312;&#26085;&#24120;&#20114;&#21160;&#20013;&#36861;&#27714;&#31038;&#20132;&#30446;&#26631;&#65292;&#36825;&#26159;&#31038;&#20132;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SOTOPIA&#65292;&#19968;&#20010;&#24320;&#25918;&#24335;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#22797;&#26434;&#31038;&#20132;&#20114;&#21160;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#31038;&#20132;&#26234;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#20013;&#65292;&#20195;&#29702;&#20154;&#25198;&#28436;&#35282;&#33394;&#65292;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#30456;&#20114;&#21327;&#20316;&#12289;&#21512;&#20316;&#12289;&#20132;&#27969;&#21644;&#31454;&#20105;&#65292;&#20197;&#23454;&#29616;&#22797;&#26434;&#30340;&#31038;&#20132;&#30446;&#26631;&#12290;&#25105;&#20204;&#27169;&#25311;&#20102;LLM-based&#20195;&#29702;&#20154;&#19982;&#20154;&#31867;&#20043;&#38388;&#22312;&#36825;&#20010;&#20219;&#21153;&#31354;&#38388;&#20869;&#30340;&#35282;&#33394;&#25198;&#28436;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;SOTOPIA-Eval&#30340;&#25972;&#20307;&#35780;&#20272;&#26694;&#26550;&#23545;&#23427;&#20204;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;SOTOPIA&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#31038;&#20132;&#26234;&#33021;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#30830;&#23450;&#20102;SOTOPIA&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#21363;SOTOPIA-hard&#65292;&#23545;&#25152;&#26377;&#27169;&#22411;&#26469;&#35828;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#20010;&#23376;&#38598;&#19978;&#65292;GPT-4&#30340;&#30446;&#26631;&#23436;&#25104;&#29575;&#26174;&#33879;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completio
&lt;/p&gt;</description></item><item><title>Hetero$^2$Net&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#22270;&#30340;&#24322;&#36136;&#23646;&#24615;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20803;&#36335;&#24452;&#35782;&#21035;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#24230;&#37327;&#25351;&#26631;&#26469;&#25551;&#36848;&#24322;&#36136;&#24615;&#27700;&#24179;&#65292;&#20197;&#35299;&#20915;&#24120;&#35265;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.11664</link><description>&lt;p&gt;
Hetero$^2$Net: &#38754;&#21521;&#24322;&#26500;&#22270;&#30340;&#24322;&#36136;&#23646;&#24615;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hetero$^2$Net: Heterophily-aware Representation Learning on Heterogenerous Graphs. (arXiv:2310.11664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11664
&lt;/p&gt;
&lt;p&gt;
Hetero$^2$Net&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#22270;&#30340;&#24322;&#36136;&#23646;&#24615;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20803;&#36335;&#24452;&#35782;&#21035;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#24230;&#37327;&#25351;&#26631;&#26469;&#25551;&#36848;&#24322;&#36136;&#24615;&#27700;&#24179;&#65292;&#20197;&#35299;&#20915;&#24120;&#35265;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#30340;&#22270;&#36890;&#24120;&#38750;&#24120;&#22797;&#26434;&#65292;&#20840;&#23616;&#32467;&#26500;&#20013;&#23384;&#22312;&#24322;&#26500;&#24615;&#65292;&#32780;&#23616;&#37096;&#37051;&#22495;&#20869;&#21017;&#34920;&#29616;&#20986;&#24378;&#28872;&#30340;&#24322;&#36136;&#24615;&#12290;&#23613;&#31649;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#25581;&#31034;&#20102;&#24120;&#35265;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22788;&#29702;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#21516;&#26500;&#22270;&#26102;&#30340;&#23616;&#38480;&#24615;&#65292;&#20294;&#22312;&#30740;&#31350;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#36136;&#24615;&#23646;&#24615;&#26041;&#38754;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#20351;&#29992;&#20803;&#36335;&#24452;&#35782;&#21035;&#20102;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#23454;&#29992;&#30340;&#24230;&#37327;&#25351;&#26631;&#26469;&#23450;&#37327;&#25551;&#36848;&#24322;&#36136;&#24615;&#27700;&#24179;&#12290;&#36890;&#36807;&#23545;&#20960;&#20010;&#23637;&#31034;&#19981;&#21516;&#24322;&#36136;&#24615;&#27700;&#24179;&#30340;&#30495;&#23454;&#19990;&#30028;&#24322;&#26500;&#22270;&#30340;&#28145;&#20837;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#32487;&#25215;&#20102;&#24456;&#22810;&#35774;&#35745;&#29992;&#20110;&#21516;&#26500;&#22270;&#30340;GNNs&#26426;&#21046;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#22312;&#22788;&#29702;&#20855;&#26377;&#24322;&#36136;&#24615;&#25110;&#20302;&#24230;&#21516;&#36136;&#24615;&#30340;&#24322;&#26500;&#22270;&#26102;&#26080;&#27861;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hetero$^2$Net&#65292;&#19968;&#20010;&#38754;&#21521;&#24322;&#26500;&#22270;&#30340;&#24322;&#36136;&#23646;&#24615;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world graphs are typically complex, exhibiting heterogeneity in the global structure, as well as strong heterophily within local neighborhoods. While a growing body of literature has revealed the limitations of common graph neural networks (GNNs) in handling homogeneous graphs with heterophily, little work has been conducted on investigating the heterophily properties in the context of heterogeneous graphs. To bridge this research gap, we identify the heterophily in heterogeneous graphs using metapaths and propose two practical metrics to quantitatively describe the levels of heterophily. Through in-depth investigations on several real-world heterogeneous graphs exhibiting varying levels of heterophily, we have observed that heterogeneous graph neural networks (HGNNs), which inherit many mechanisms from GNNs designed for homogeneous graphs, fail to generalize to heterogeneous graphs with heterophily or low level of homophily. To address the challenge, we present Hetero$^2$Net, a h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35745;&#25968;&#25968;&#25454;&#30340;&#20027;&#39064;&#19987;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#20285;&#29595;&#38543;&#26426;&#25928;&#24212;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#21516;&#26102;&#33719;&#24471;&#20102;&#22266;&#23450;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21644;&#38543;&#26426;&#25928;&#24212;&#30340;&#26368;&#20339;&#26080;&#20559;&#39044;&#27979;&#22120;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#22788;&#29702;&#39640;&#22522;&#25968;&#20998;&#31867;&#29305;&#24449;&#30340;&#32858;&#31867;&#35745;&#25968;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.11654</link><description>&lt;p&gt;
&#38024;&#23545;&#20855;&#26377;&#39640;&#22522;&#25968;&#20998;&#31867;&#29305;&#24449;&#30340;&#35745;&#25968;&#25968;&#25454;&#30340;&#20027;&#39064;&#19987;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features. (arXiv:2310.11654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35745;&#25968;&#25968;&#25454;&#30340;&#20027;&#39064;&#19987;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#20285;&#29595;&#38543;&#26426;&#25928;&#24212;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#21516;&#26102;&#33719;&#24471;&#20102;&#22266;&#23450;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21644;&#38543;&#26426;&#25928;&#24212;&#30340;&#26368;&#20339;&#26080;&#20559;&#39044;&#27979;&#22120;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#22788;&#29702;&#39640;&#22522;&#25968;&#20998;&#31867;&#29305;&#24449;&#30340;&#32858;&#31867;&#35745;&#25968;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23454;&#38469;&#25968;&#25454;&#36890;&#24120;&#21576;&#29616;&#20986;&#30456;&#20851;&#24615;&#65292;&#38024;&#23545;&#20027;&#39064;&#29305;&#23450;&#30340;&#39044;&#27979;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20294;&#20256;&#32479;DNN&#26694;&#26550;&#36890;&#24120;&#24573;&#35270;&#20102;&#36825;&#19968;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23618;&#27425;&#20284;&#28982;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#20285;&#29595;&#38543;&#26426;&#25928;&#24212;&#24341;&#20837;&#21040;&#27850;&#26494;DNN&#20013;&#65292;&#20197;&#36890;&#36807;&#25429;&#25417;&#36755;&#20837;&#21464;&#37327;&#30340;&#38750;&#32447;&#24615;&#25928;&#24212;&#21644;&#20027;&#39064;&#29305;&#23450;&#30340;&#32858;&#31867;&#25928;&#24212;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#21333;&#20010;&#30446;&#26631;&#20989;&#25968;&#21516;&#26102;&#33719;&#24471;&#20102;&#22266;&#23450;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21644;&#38543;&#26426;&#25928;&#24212;&#30340;&#26368;&#20339;&#26080;&#20559;&#39044;&#27979;&#22120;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#24555;&#36895;&#22788;&#29702;&#28041;&#21450;&#39640;&#22522;&#25968;&#20998;&#31867;&#29305;&#24449;&#30340;&#32858;&#31867;&#35745;&#25968;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#26550;&#26500;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#21040;&#25152;&#25552;&#20986;&#30340;h-likelihood&#26694;&#26550;&#24403;&#20013;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#21644;&#19968;&#20010;&#31232;&#30095;&#23618;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in subject-specific predictions using deep neural networks (DNNs) because real-world data often exhibit correlations, which has been typically overlooked in traditional DNN frameworks. In this paper, we propose a novel hierarchical likelihood learning framework for introducing gamma random effects into the Poisson DNN, so as to improve the prediction performance by capturing both nonlinear effects of input variables and subject-specific cluster effects. The proposed method simultaneously yields maximum likelihood estimators for fixed parameters and best unbiased predictors for random effects by optimizing a single objective function. This approach enables a fast end-to-end algorithm for handling clustered count data, which often involve high-cardinality categorical features. Furthermore, state-of-the-art network architectures can be easily implemented into the proposed h-likelihood framework. As an example, we introduce multi-head attention layer and a spars
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#33258;&#27880;&#24847;&#26426;&#21046;&#20174;&#25353;&#38190;&#24207;&#21015;&#20013;&#25552;&#21462;&#20449;&#24687;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#25353;&#38190;&#36523;&#20221;&#39564;&#35777;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;Aalto&#26700;&#38754;&#25353;&#38190;&#25968;&#25454;&#38598;&#19978;&#65292;&#37319;&#29992;&#25209;&#37327;-&#20840;&#37096;&#19977;&#37325;&#25439;&#22833;&#21644;&#20313;&#24358;&#36317;&#31163;&#30340;&#21452;&#37325;&#32534;&#30721;&#22120;&#26550;&#26500;&#36798;&#21040;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11640</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#30340;&#33258;&#30001;&#25991;&#26412;&#25353;&#38190;&#36523;&#20221;&#39564;&#35777;&#65306;&#26550;&#26500;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Free-text Keystroke Authentication using Transformers: A Comparative Study of Architectures and Loss Functions. (arXiv:2310.11640v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#33258;&#27880;&#24847;&#26426;&#21046;&#20174;&#25353;&#38190;&#24207;&#21015;&#20013;&#25552;&#21462;&#20449;&#24687;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#25353;&#38190;&#36523;&#20221;&#39564;&#35777;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;Aalto&#26700;&#38754;&#25353;&#38190;&#25968;&#25454;&#38598;&#19978;&#65292;&#37319;&#29992;&#25209;&#37327;-&#20840;&#37096;&#19977;&#37325;&#25439;&#22833;&#21644;&#20313;&#24358;&#36317;&#31163;&#30340;&#21452;&#37325;&#32534;&#30721;&#22120;&#26550;&#26500;&#36798;&#21040;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25353;&#38190;&#29983;&#29289;&#29305;&#24449;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#29992;&#25143;&#35782;&#21035;&#21644;&#39564;&#35777;&#26041;&#27861;&#65292;&#21033;&#29992;&#20010;&#20307;&#30340;&#36755;&#20837;&#34892;&#20026;&#20013;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#65292;&#21033;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#20174;&#25353;&#38190;&#24207;&#21015;&#20013;&#25552;&#21462;&#20449;&#24687;&#29305;&#24449;&#65292;&#36229;&#36807;&#20256;&#32479;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26550;&#26500;&#65292;&#21363;&#21452;&#37325;&#32534;&#30721;&#22120;&#21644;&#20132;&#21449;&#32534;&#30721;&#22120;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#25353;&#38190;&#36523;&#20221;&#39564;&#35777;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#19977;&#37325;&#65292;&#25209;&#37327;-&#20840;&#37096;&#19977;&#37325;&#21644;WDCL&#25439;&#22833;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#36317;&#31163;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;&#27431;&#27663;&#36317;&#31163;&#65292;&#26364;&#21704;&#39039;&#36317;&#31163;&#21644;&#20313;&#24358;&#36317;&#31163;&#12290;&#36825;&#20123;&#23454;&#39564;&#20351;&#25105;&#20204;&#33021;&#22815;&#20248;&#21270;&#35757;&#32451;&#36807;&#31243;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;Aalto&#26700;&#38754;&#25353;&#38190;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25209;&#37327;-&#20840;&#37096;&#19977;&#37325;&#25439;&#22833;&#21644;&#20313;&#24358;&#36317;&#31163;&#30340;&#21452;&#37325;&#32534;&#30721;&#22120;&#26550;&#26500;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keystroke biometrics is a promising approach for user identification and verification, leveraging the unique patterns in individuals' typing behavior. In this paper, we propose a Transformer-based network that employs self-attention to extract informative features from keystroke sequences, surpassing the performance of traditional Recurrent Neural Networks. We explore two distinct architectures, namely bi-encoder and cross-encoder, and compare their effectiveness in keystroke authentication. Furthermore, we investigate different loss functions, including triplet, batch-all triplet, and WDCL loss, along with various distance metrics such as Euclidean, Manhattan, and cosine distances. These experiments allow us to optimize the training process and enhance the performance of our model. To evaluate our proposed model, we employ the Aalto desktop keystroke dataset. The results demonstrate that the bi-encoder architecture with batch-all triplet loss and cosine distance achieves the best perf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#20013;&#24515;&#31243;&#24230;&#38382;&#39064;&#30340;&#21518;&#22788;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#26597;&#35810;&#21644;&#30011;&#24266;&#26679;&#26412;&#26500;&#24314;&#30340;&#20004;&#20010;&#24211;&#65292;&#20197;&#21450;&#22522;&#20110;&#36825;&#20004;&#20010;&#24211;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#20013;&#24515;&#28857;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.11612</link><description>&lt;p&gt;
&#24179;&#34913;&#34892;&#20026;&#65306;&#36890;&#36807;&#26597;&#35810;&#21644;&#30011;&#24266;&#24211;&#20943;&#23569;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#20013;&#24515;&#31243;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Balance Act: Mitigating Hubness in Cross-Modal Retrieval with Query and Gallery Banks. (arXiv:2310.11612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#20013;&#24515;&#31243;&#24230;&#38382;&#39064;&#30340;&#21518;&#22788;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#26597;&#35810;&#21644;&#30011;&#24266;&#26679;&#26412;&#26500;&#24314;&#30340;&#20004;&#20010;&#24211;&#65292;&#20197;&#21450;&#22522;&#20110;&#36825;&#20004;&#20010;&#24211;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#20013;&#24515;&#28857;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#22788;&#29702;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36328;&#27169;&#24577;&#26816;&#32034;&#20013;&#30340;&#20013;&#24515;&#31243;&#24230;&#38382;&#39064;&#12290;&#36825;&#26159;&#19968;&#20010;&#29616;&#35937;&#65292;&#20854;&#20013;&#19968;&#20010;&#23567;&#37096;&#20998;&#30011;&#24266;&#25968;&#25454;&#28857;&#32463;&#24120;&#34987;&#26816;&#32034;&#65292;&#23548;&#33268;&#26816;&#32034;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23558;&#30011;&#24266;&#21644;&#26597;&#35810;&#25968;&#25454;&#37117;&#32435;&#20837;&#32771;&#34385;&#35299;&#20915;&#20013;&#24515;&#31243;&#24230;&#38382;&#39064;&#30340;&#24517;&#35201;&#24615;&#65292;&#22240;&#20026;&#20013;&#24515;&#28857;&#22987;&#32456;&#19982;&#30011;&#24266;&#21644;&#26597;&#35810;&#25968;&#25454;&#20855;&#26377;&#39640;&#30456;&#20284;&#24615;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21452;&#24211;&#24402;&#19968;&#21270;&#65288;DBNorm&#65289;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#20165;&#21033;&#29992;&#26597;&#35810;&#26679;&#26412;&#26469;&#20943;&#36731;&#20013;&#24515;&#31243;&#24230;&#38382;&#39064;&#65292;&#32780;DBNorm&#21033;&#29992;&#20174;&#26597;&#35810;&#26679;&#26412;&#21644;&#30011;&#24266;&#26679;&#26412;&#26500;&#24314;&#30340;&#20004;&#20010;&#24211;&#26469;&#20943;&#23569;&#25512;&#29702;&#36807;&#31243;&#20013;&#20013;&#24515;&#28857;&#30340;&#20986;&#29616;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#34917;&#20805;DBNorm&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21452;&#21453;softmax&#21644;&#21452;&#21160;&#24577;&#21453;softmax&#65292;&#29992;&#20110;&#22522;&#20110;&#36825;&#20004;&#20010;&#24211;&#36827;&#34892;&#30456;&#20284;&#24230;&#24402;&#19968;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;&#20013;&#24515;&#28857;&#21644;&#26597;&#35810;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a post-processing solution to address the hubness problem in cross-modal retrieval, a phenomenon where a small number of gallery data points are frequently retrieved, resulting in a decline in retrieval performance. We first theoretically demonstrate the necessity of incorporating both the gallery and query data for addressing hubness as hubs always exhibit high similarity with gallery and query data. Second, building on our theoretical results, we propose a novel framework, Dual Bank Normalization (DBNorm). While previous work has attempted to alleviate hubness by only utilizing the query samples, DBNorm leverages two banks constructed from the query and gallery samples to reduce the occurrence of hubs during inference. Next, to complement DBNorm, we introduce two novel methods, dual inverted softmax and dual dynamic inverted softmax, for normalizing similarity based on the two banks. Specifically, our proposed methods reduce the similarity between hubs and qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#20351;&#29992;RPS&#12289;&#21098;&#26525;&#25216;&#26415;&#21644;&#26500;&#24314;&#36739;&#23567;&#27169;&#22411;&#31561;&#26041;&#27861;&#22312;&#20869;&#23384;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21457;&#29616;RPS&#22312;&#25972;&#20010;&#21387;&#32553;&#33539;&#22260;&#20869;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#22312;&#39640;&#21387;&#32553;&#22330;&#26223;&#20013;&#34920;&#29616;&#23588;&#20026;&#31361;&#20986;&#12290;</title><link>http://arxiv.org/abs/2310.11611</link><description>&lt;p&gt;
&#25903;&#25345;&#21442;&#25968;&#20849;&#20139;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#30340;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
In defense of parameter sharing for model-compression. (arXiv:2310.11611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#20351;&#29992;RPS&#12289;&#21098;&#26525;&#25216;&#26415;&#21644;&#26500;&#24314;&#36739;&#23567;&#27169;&#22411;&#31561;&#26041;&#27861;&#22312;&#20869;&#23384;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21457;&#29616;RPS&#22312;&#25972;&#20010;&#21387;&#32553;&#33539;&#22260;&#20869;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#22312;&#39640;&#21387;&#32553;&#22330;&#26223;&#20013;&#34920;&#29616;&#23588;&#20026;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#27169;&#22411;&#26550;&#26500;&#26102;&#65292;&#26377;&#22810;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#20854;&#20869;&#23384;&#21344;&#29992;&#12290;&#21382;&#21490;&#19978;&#65292;&#27969;&#34892;&#30340;&#26041;&#27861;&#21253;&#25324;&#36873;&#25321;&#36739;&#23567;&#30340;&#26550;&#26500;&#21644;&#36890;&#36807;&#21098;&#26525;&#21019;&#24314;&#31232;&#30095;&#32593;&#32476;&#12290;&#26368;&#36817;&#65292;&#38543;&#26426;&#21442;&#25968;&#20849;&#20139;&#65288;RPS&#65289;&#26041;&#27861;&#22312;&#35757;&#32451;&#24320;&#22987;&#26102;&#30340;&#27169;&#22411;&#21387;&#32553;&#20013;&#21463;&#21040;&#20102;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#22312;RPS&#12289;&#21098;&#26525;&#25216;&#26415;&#21644;&#26500;&#24314;&#36739;&#23567;&#27169;&#22411;&#20043;&#38388;&#30340;&#20869;&#23384;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25972;&#20010;&#21387;&#32553;&#33539;&#22260;&#20869;&#65292;&#26080;&#35770;&#26159;&#25968;&#25454;&#39537;&#21160;&#36824;&#26159;&#27169;&#22411;&#26080;&#20851;&#65292;RPS&#22987;&#32456;&#20248;&#20110;/&#19982;&#36739;&#23567;&#27169;&#22411;&#21644;&#25152;&#26377;&#20449;&#24687;&#31245;&#24494;&#20805;&#36275;&#30340;&#21098;&#26525;&#31574;&#30053;&#22914;MAG&#12289;SNIP&#12289;SYNFLOW&#21644;GRASP&#30456;&#21305;&#37197;&#12290;&#36825;&#31181;&#20248;&#21183;&#22312;&#26356;&#39640;&#30340;&#21387;&#32553;&#22330;&#26223;&#20013;&#23588;&#20026;&#26126;&#26174;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#19982;&#39640;&#24230;&#20449;&#24687;&#20805;&#36275;&#30340;&#21098;&#26525;&#25216;&#26415;&#22914;Lottery Ticket Rewinding&#65288;LTR&#65289;&#30456;&#27604;&#65292;RPS&#22312;&#39640;&#21387;&#32553;&#35774;&#32622;&#20013;&#20063;&#23637;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#36825;&#25351;&#20986;&#20102;&#20854;&#20869;&#22312;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When considering a model architecture, there are several ways to reduce its memory footprint. Historically, popular approaches included selecting smaller architectures and creating sparse networks through pruning. More recently, randomized parameter-sharing (RPS) methods have gained traction for model compression at start of training. In this paper, we comprehensively assess the trade-off between memory and accuracy across RPS, pruning techniques, and building smaller models. Our findings demonstrate that RPS, which is both data and model-agnostic, consistently outperforms/matches smaller models and all moderately informed pruning strategies, such as MAG, SNIP, SYNFLOW, and GRASP, across the entire compression range. This advantage becomes particularly pronounced in higher compression scenarios. Notably, even when compared to highly informed pruning techniques like Lottery Ticket Rewinding (LTR), RPS exhibits superior performance in high compression settings. This points out inherent c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#21453;&#23556;&#31561;&#21464;&#24615;&#30340;&#25193;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21516;&#20301;&#32032;&#26059;&#36716;&#20809;&#35889;&#20013;&#27979;&#23450;&#26377;&#26426;&#20998;&#23376;&#30340;&#19977;&#32500;&#32467;&#26500;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#30001;&#20110;&#32570;&#22833;&#27491;&#36127;&#31526;&#21495;&#32780;&#38590;&#20197;&#30830;&#23450;&#23454;&#38469;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.11609</link><description>&lt;p&gt;
&#20174;&#22825;&#28982;&#23384;&#22312;&#30340;&#21516;&#20301;&#32032;&#26059;&#36716;&#20809;&#35889;&#20013;&#30340;&#19977;&#32500;&#32467;&#26500;&#27979;&#23450;&#26469;&#30475;&#65292;&#20855;&#26377;&#21453;&#23556;&#31561;&#21464;&#24615;&#30340;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reflection-Equivariant Diffusion for 3D Structure Determination from Isotopologue Rotational Spectra in Natural Abundance. (arXiv:2310.11609v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#21453;&#23556;&#31561;&#21464;&#24615;&#30340;&#25193;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21516;&#20301;&#32032;&#26059;&#36716;&#20809;&#35889;&#20013;&#27979;&#23450;&#26377;&#26426;&#20998;&#23376;&#30340;&#19977;&#32500;&#32467;&#26500;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#30001;&#20110;&#32570;&#22833;&#27491;&#36127;&#31526;&#21495;&#32780;&#38590;&#20197;&#30830;&#23450;&#23454;&#38469;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#27979;&#23450;&#23545;&#20110;&#35782;&#21035;&#26410;&#30693;&#30340;&#26377;&#26426;&#20998;&#23376;&#26159;&#24517;&#35201;&#30340;&#65292;&#20363;&#22914;&#22825;&#28982;&#20135;&#29289;&#12289;&#27861;&#21307;&#26679;&#26412;&#12289;&#26143;&#38469;&#20171;&#36136;&#21644;&#23454;&#39564;&#23460;&#21512;&#25104;&#29289;&#31561;&#12290;&#26059;&#36716;&#20809;&#35889;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#23567;&#26377;&#26426;&#20998;&#23376;&#30340;&#24815;&#37327;&#30697;&#26469;&#36827;&#34892;&#32467;&#26500;&#27979;&#23450;&#65292;&#20174;&#32780;&#25552;&#20379;&#31934;&#30830;&#30340;&#19977;&#32500;&#20449;&#24687;&#12290;&#21033;&#29992;&#36825;&#20123;&#24815;&#37327;&#30697;&#65292;Kraitchman&#20998;&#26512;&#30830;&#23450;&#21516;&#20301;&#32032;&#32622;&#25442;&#22352;&#26631;&#65292;&#36825;&#20123;&#22352;&#26631;&#26159;&#20855;&#26377;&#22825;&#28982;&#21516;&#20301;&#32032;&#20016;&#24230;&#30340;&#25152;&#26377;&#21407;&#23376;&#30340;&#26080;&#31526;&#21495;|x|&#12289;|y|&#21644;|z|&#22352;&#26631;&#65292;&#21253;&#25324;&#30899;&#12289;&#27694;&#21644;&#27687;&#12290;&#34429;&#28982;&#26080;&#31526;&#21495;&#30340;&#32622;&#25442;&#22352;&#26631;&#21487;&#20197;&#39564;&#35777;&#32467;&#26500;&#30340;&#29468;&#27979;&#65292;&#20294;&#26159;&#32570;&#22833;&#30340;&#27491;&#36127;&#31526;&#21495;&#20351;&#24471;&#20165;&#20973;&#32622;&#25442;&#22352;&#26631;&#38590;&#20197;&#30830;&#23450;&#23454;&#38469;&#30340;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#36870;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;KREED&#65288;Kraitchman&#20855;&#26377;&#21453;&#23556;&#31561;&#21464;&#24615;&#30340;&#25193;&#25955;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20998;&#23376;&#30340;&#20998;&#23376;&#24335;&#12289;&#24815;&#37327;&#30697;&#21644;&#26080;&#31526;&#21495;&#30340;&#32622;&#25442;&#22352;&#26631;&#20013;&#25512;&#26029;&#20986;&#20998;&#23376;&#30340;&#23436;&#25972;&#19977;&#32500;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure determination is necessary to identify unknown organic molecules, such as those in natural products, forensic samples, the interstellar medium, and laboratory syntheses. Rotational spectroscopy enables structure determination by providing accurate 3D information about small organic molecules via their moments of inertia. Using these moments, Kraitchman analysis determines isotopic substitution coordinates, which are the unsigned $|x|,|y|,|z|$ coordinates of all atoms with natural isotopic abundance, including carbon, nitrogen, and oxygen. While unsigned substitution coordinates can verify guesses of structures, the missing $+/-$ signs make it challenging to determine the actual structure from the substitution coordinates alone. To tackle this inverse problem, we develop KREED (Kraitchman REflection-Equivariant Diffusion), a generative diffusion model that infers a molecule's complete 3D structure from its molecular formula, moments of inertia, and unsigned substitution coordi
&lt;/p&gt;</description></item><item><title>TK-KNN&#26159;&#19968;&#31181;&#24179;&#34913;&#30340;&#36317;&#31163;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#20351;&#29992;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#25345;&#31867;&#21035;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TK-KNN&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.11607</link><description>&lt;p&gt;
TK-KNN&#65306;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#24847;&#22270;&#20998;&#31867;&#30340;&#24179;&#34913;&#36317;&#31163;&#20266;&#26631;&#31614;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised Intent Classification. (arXiv:2310.11607v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11607
&lt;/p&gt;
&lt;p&gt;
TK-KNN&#26159;&#19968;&#31181;&#24179;&#34913;&#30340;&#36317;&#31163;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#20351;&#29992;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#25345;&#31867;&#21035;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TK-KNN&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25216;&#26415;&#20013;&#65292;&#26816;&#27979;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24847;&#22270;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36825;&#20123;&#31995;&#32479;&#24448;&#24448;&#20135;&#29983;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#25163;&#21160;&#26631;&#35760;&#36825;&#20123;&#25968;&#25454;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#12290;&#21322;&#30417;&#30563;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#20351;&#29992;&#22312;&#23569;&#37327;&#26631;&#35760;&#31034;&#20363;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#20026;&#39044;&#27979;&#32622;&#20449;&#24230;&#39640;&#20110;&#26576;&#20010;&#38408;&#20540;&#30340;&#26410;&#26631;&#35760;&#31034;&#20363;&#20998;&#37197;&#20266;&#26631;&#31614;&#26469;&#20943;&#23569;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#29305;&#21035;&#21361;&#38505;&#30340;&#21518;&#26524;&#26159;&#22312;&#21508;&#31867;&#21035;&#20043;&#38388;&#36873;&#25321;&#19981;&#24179;&#34913;&#30340;&#31034;&#20363;&#38598;&#65292;&#21487;&#33021;&#23548;&#33268;&#26631;&#31614;&#36136;&#37327;&#36739;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#31354;&#38388;&#36317;&#31163;&#30340;&#26356;&#31283;&#20581;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;Top-K K-Nearest Neighbor (TK-KNN)&#65292;&#36890;&#36807;&#22522;&#20110;&#25490;&#21517;&#30340;&#26041;&#27861;&#22312;&#31867;&#21035;&#20043;&#38388;&#32500;&#25345;&#19968;&#20010;&#24179;&#34913;&#30340;&#20266;&#26631;&#31614;&#31034;&#20363;&#38598;&#12290;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TK-KNN&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to detect intent in dialogue systems has become increasingly important in modern technology. These systems often generate a large amount of unlabeled data, and manually labeling this data requires substantial human effort. Semi-supervised methods attempt to remedy this cost by using a model trained on a few labeled examples and then by assigning pseudo-labels to further a subset of unlabeled examples that has a model prediction confidence higher than a certain threshold. However, one particularly perilous consequence of these methods is the risk of picking an imbalanced set of examples across classes, which could lead to poor labels. In the present work, we describe Top-K K-Nearest Neighbor (TK-KNN), which uses a more robust pseudo-labeling approach based on distance in the embedding space while maintaining a balanced set of pseudo-labeled examples across classes through a ranking-based approach. Experiments on several datasets show that TK-KNN outperforms existing models, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DIAR&#26041;&#27861;&#65292;&#21033;&#29992;Swin Transformers&#23545;&#19968;&#31995;&#21015;&#22833;&#30495;&#22270;&#20687;&#36827;&#34892;&#23545;&#40784;&#21644;&#37325;&#24314;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20998;&#26512;&#24207;&#21015;&#22270;&#20687;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#22270;&#26816;&#27979;&#30456;&#20851;&#20869;&#23481;&#24182;&#21306;&#20998;&#24322;&#24120;&#20540;&#21644;&#20266;&#20687;&#12290;&#21516;&#26102;&#65292;&#36824;&#25506;&#32034;&#20102;&#20351;&#29992;&#31070;&#32463;&#29305;&#24449;&#22270;&#20316;&#20026;&#20256;&#32479;&#20851;&#38190;&#28857;&#26816;&#27979;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11605</link><description>&lt;p&gt;
DIAR: &#20351;&#29992;Swin Transformers&#36827;&#34892;&#28145;&#24230;&#22270;&#20687;&#23545;&#40784;&#21644;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
DIAR: Deep Image Alignment and Reconstruction using Swin Transformers. (arXiv:2310.11605v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DIAR&#26041;&#27861;&#65292;&#21033;&#29992;Swin Transformers&#23545;&#19968;&#31995;&#21015;&#22833;&#30495;&#22270;&#20687;&#36827;&#34892;&#23545;&#40784;&#21644;&#37325;&#24314;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20998;&#26512;&#24207;&#21015;&#22270;&#20687;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#22270;&#26816;&#27979;&#30456;&#20851;&#20869;&#23481;&#24182;&#21306;&#20998;&#24322;&#24120;&#20540;&#21644;&#20266;&#20687;&#12290;&#21516;&#26102;&#65292;&#36824;&#25506;&#32034;&#20102;&#20351;&#29992;&#31070;&#32463;&#29305;&#24449;&#22270;&#20316;&#20026;&#20256;&#32479;&#20851;&#38190;&#28857;&#26816;&#27979;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25293;&#25668;&#19968;&#20123;&#34987;&#36974;&#25377;&#20869;&#23481;&#30340;&#22270;&#20687;&#26102;&#65292;&#36890;&#24120;&#20250;&#38754;&#20020;&#30340;&#38382;&#39064;&#26159;&#27599;&#20010;&#21333;&#29420;&#30340;&#22270;&#20687;&#24103;&#37117;&#21253;&#21547;&#19981;&#38656;&#35201;&#30340;&#20266;&#20687;&#65292;&#20294;&#26159;&#22914;&#26524;&#27491;&#30830;&#23545;&#40784;&#21644;&#32858;&#21512;&#19968;&#31995;&#21015;&#22270;&#20687;&#65292;&#23601;&#21487;&#20197;&#33719;&#24471;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#26500;&#24314;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#21516;&#26102;&#23545;&#19968;&#31995;&#21015;&#22833;&#30495;&#22270;&#20687;&#36827;&#34892;&#23545;&#40784;&#21644;&#37325;&#24314;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#22270;&#20687;&#22833;&#30495;&#65288;&#22914;&#20809;&#29031;&#12289;&#38236;&#38754;&#21453;&#23556;&#12289;&#38452;&#24433;&#21644;&#36974;&#25377;&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#20026;&#27599;&#20010;&#22270;&#20687;&#21019;&#24314;&#20102;&#30456;&#24212;&#30340;&#22320;&#38754;&#30495;&#20540;&#21333;&#24212;&#24615;&#20316;&#20026;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;Swin Transformer&#27169;&#22411;&#26469;&#20998;&#26512;&#24207;&#21015;&#22270;&#20687;&#25968;&#25454;&#12290;&#27880;&#24847;&#21147;&#22270;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26816;&#27979;&#21040;&#30456;&#20851;&#30340;&#22270;&#20687;&#20869;&#23481;&#24182;&#23558;&#20854;&#21306;&#20998;&#20986;&#26469;&#20197;&#19982;&#24322;&#24120;&#20540;&#21644;&#20266;&#20687;&#21306;&#20998;&#24320;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20351;&#29992;&#31070;&#32463;&#29305;&#24449;&#22270;&#20316;&#20026;&#20256;&#32479;&#20851;&#38190;&#28857;&#26816;&#27979;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;&#21367;&#31215;&#23618;&#30340;&#29305;&#24449;&#22270;&#25552;&#20379;&#20102;&#21487;&#20197;&#29992;&#26469;&#23547;&#25214;&#22270;&#20687;&#30340;&#23494;&#38598;&#25551;&#36848;&#31526;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When taking images of some occluded content, one is often faced with the problem that every individual image frame contains unwanted artifacts, but a collection of images contains all relevant information if properly aligned and aggregated. In this paper, we attempt to build a deep learning pipeline that simultaneously aligns a sequence of distorted images and reconstructs them. We create a dataset that contains images with image distortions, such as lighting, specularities, shadows, and occlusion. We create perspective distortions with corresponding ground-truth homographies as labels. We use our dataset to train Swin transformer models to analyze sequential image data. The attention maps enable the model to detect relevant image content and differentiate it from outliers and artifacts. We further explore using neural feature maps as alternatives to classical key point detectors. The feature maps of trained convolutional layers provide dense image descriptors that can be used to find 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#32473;&#20104;LLM&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;LLMs&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#25805;&#20316;&#25216;&#33021;&#20013;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#24207;&#21015;&#65292;&#24182;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#65292;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11604</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Zero-Shot Trajectory Generators. (arXiv:2310.11604v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#32473;&#20104;LLM&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;LLMs&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#25805;&#20316;&#25216;&#33021;&#20013;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#24207;&#21015;&#65292;&#24182;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#65292;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20104;&#20302;&#32423;&#25216;&#33021;&#36873;&#25321;&#26102;&#33021;&#22815;&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#39640;&#32423;&#35268;&#21010;&#22120;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#35748;&#20026;LLMs&#19981;&#20855;&#22791;&#36275;&#22815;&#30340;&#30693;&#35782;&#26469;&#29992;&#20110;&#20302;&#32423;&#36712;&#36857;&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;&#36825;&#31181;&#20551;&#35774;&#65292;&#24182;&#35843;&#26597;&#20102;&#24403;&#32473;&#20104;LLM&#65288;GPT-4&#65289;&#20165;&#33021;&#35775;&#38382;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#26102;&#65292;&#23427;&#33021;&#21542;&#30452;&#25509;&#39044;&#27979;&#19968;&#31995;&#21015;&#23494;&#38598;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#29992;&#20110;&#25805;&#20316;&#25216;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#25552;&#31034;&#65292;&#27809;&#26377;&#20219;&#20309;&#19978;&#19979;&#25991;&#31034;&#20363;&#12289;&#36816;&#21160;&#21407;&#35821;&#25110;&#22806;&#37096;&#36712;&#36857;&#20248;&#21270;&#22120;&#65292;&#23427;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#22914;&#8220;&#25171;&#24320;&#29942;&#30422;&#8221;&#21644;&#8220;&#29992;&#28023;&#32501;&#25830;&#25325;&#30424;&#23376;&#8221;&#65292;&#20197;&#21450;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20010;&#25552;&#31034;&#20013;&#21738;&#20123;&#35774;&#35745;&#36873;&#25321;&#26368;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#32467;&#35770;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#39318;&#27425;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#20132;&#21449;&#28857;&#65292;&#24341;&#20837;&#20102;Adversarial Robustness Unhardening&#65288;ARU&#65289;&#65292;&#36890;&#36807;&#26377;&#24847;&#20171;&#20837;&#20998;&#25955;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.11594</link><description>&lt;p&gt;
Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning. (arXiv:2310.11594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning. (arXiv:2310.11594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#20132;&#21449;&#28857;&#65292;&#24341;&#20837;&#20102;Adversarial Robustness Unhardening&#65288;ARU&#65289;&#65292;&#36890;&#36807;&#26377;&#24847;&#20171;&#20837;&#20998;&#25955;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#25968;&#25454;&#39537;&#21160;&#29615;&#22659;&#20013;&#65292;&#32500;&#25252;&#29992;&#25143;&#38544;&#31169;&#21644;&#37322;&#25918;&#25968;&#25454;&#28508;&#21147;&#20043;&#38388;&#24494;&#22937;&#30340;&#24179;&#34913;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20197;&#38544;&#31169;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#23454;&#29616;&#20102;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#36825;&#31181;&#20998;&#25955;&#24335;&#26041;&#27861;&#24102;&#26469;&#20102;&#23433;&#20840;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24694;&#24847;&#23454;&#20307;&#27880;&#20837;&#25439;&#22351;&#25968;&#25454;&#30340;&#20013;&#27602;&#21644;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26368;&#21021;&#21463;&#21040;&#27979;&#35797;&#26102;&#38388;&#36867;&#36991;&#25915;&#20987;&#30340;&#21551;&#21457;&#65292;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#20132;&#21449;&#28857;&#65292;&#24341;&#20837;&#20102;Adversarial Robustness Unhardening&#65288;ARU&#65289;&#12290;ARU&#34987;&#19968;&#37096;&#20998;&#23545;&#25163;&#20351;&#29992;&#65292;&#20197;&#26377;&#24847;&#20171;&#20837;&#20998;&#25955;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;ARU&#23545;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#29616;&#26377;&#30340;&#40065;&#26834;&#32858;&#21512;&#38450;&#24481;&#31574;&#30053;&#23545;&#20013;&#27602;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's data-driven landscape, the delicate equilibrium between safeguarding user privacy and unleashing data potential stands as a paramount concern. Federated learning, which enables collaborative model training without necessitating data sharing, has emerged as a privacy-centric solution. This decentralized approach brings forth security challenges, notably poisoning and backdoor attacks where malicious entities inject corrupted data. Our research, initially spurred by test-time evasion attacks, investigates the intersection of adversarial training and backdoor attacks within federated learning, introducing Adversarial Robustness Unhardening (ARU). ARU is employed by a subset of adversaries to intentionally undermine model robustness during decentralized training, rendering models susceptible to a broader range of evasion attacks. We present extensive empirical experiments evaluating ARU's impact on adversarial training and existing robust aggregation defenses against poisoning a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35780;&#20215;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#26080;&#27861;&#25429;&#25417;&#20010;&#24615;&#21270;&#36136;&#37327;&#30340;&#24494;&#22937;&#24046;&#21035;&#65292;&#32780;&#20154;&#24037;&#21028;&#26029;&#21448;&#26114;&#36149;&#19988;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#27979;&#37327;&#20010;&#24615;&#21270;&#12289;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#36825;&#19977;&#20010;&#37325;&#35201;&#35821;&#20041;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.11593</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35780;&#20215;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Automated Evaluation of Personalized Text Generation using Large Language Models. (arXiv:2310.11593v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11593
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35780;&#20215;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#26080;&#27861;&#25429;&#25417;&#20010;&#24615;&#21270;&#36136;&#37327;&#30340;&#24494;&#22937;&#24046;&#21035;&#65292;&#32780;&#20154;&#24037;&#21028;&#26029;&#21448;&#26114;&#36149;&#19988;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#27979;&#37327;&#20010;&#24615;&#21270;&#12289;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#36825;&#19977;&#20010;&#37325;&#35201;&#35821;&#20041;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#29992;&#25143;&#20010;&#20154;&#32972;&#26223;&#20132;&#20184;&#20869;&#23481;&#30340;&#19987;&#38376;&#26426;&#21046;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#36805;&#36895;&#65292;&#20294;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#65288;&#22914;BLEU&#21644;ROUGE&#65289;&#20027;&#35201;&#34913;&#37327;&#19982;&#20154;&#24037;&#21442;&#32771;&#25991;&#26412;&#30340;&#35789;&#27719;&#30456;&#20284;&#24230;&#65292;&#24182;&#19981;&#33021;&#21306;&#20998;&#20010;&#24615;&#21270;&#19982;&#20854;&#20182;&#24494;&#22937;&#30340;&#35821;&#20041;&#26041;&#38754;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#20010;&#24615;&#21270;&#29983;&#25104;&#20869;&#23481;&#36136;&#37327;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20154;&#24037;&#21028;&#26029;&#26159;&#26114;&#36149;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20010;&#24615;&#21270;&#35780;&#20272;&#39046;&#22495;&#12290;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35780;&#20272;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#65292;&#24182;&#26816;&#39564;&#23427;&#20204;&#29702;&#35299;&#32454;&#33268;&#30340;&#29992;&#25143;&#32972;&#26223;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AuPEL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#25991;&#26412;&#30340;&#20010;&#24615;&#21270;&#12289;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#19977;&#20010;&#20027;&#35201;&#35821;&#20041;&#26041;&#38754;&#25552;&#21462;&#24182;&#33258;&#21160;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized text generation presents a specialized mechanism for delivering content that is specific to a user's personal context. While the research progress in this area has been rapid, evaluation still presents a challenge. Traditional automated metrics such as BLEU and ROUGE primarily measure lexical similarity to human-written references, and are not able to distinguish personalization from other subtle semantic aspects, thus falling short of capturing the nuances of personalized generated content quality. On the other hand, human judgments are costly to obtain, especially in the realm of personalized evaluation. Inspired by these challenges, we explore the use of large language models (LLMs) for evaluating personalized text generation, and examine their ability to understand nuanced user context. We present AuPEL, a novel evaluation method that distills three major semantic aspects of the generated text: personalization, quality and relevance, and automatically measures these as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25311;&#36890;&#36807;&#38750;&#35821;&#35328;&#34892;&#20026;&#25552;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#21360;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#20998;&#26512;&#32467;&#26524;&#65292;&#21457;&#29616;&#22312;&#23548;&#33322;&#22330;&#26223;&#20013;&#65292;&#31354;&#38388;&#29305;&#24449;&#26159;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.11590</link><description>&lt;p&gt;
&#25506;&#32034;&#22312;&#23548;&#33322;&#22330;&#26223;&#19979;&#25512;&#26029;&#29992;&#25143;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Towards Inferring Users' Impressions of Robot Performance in Navigation Scenarios. (arXiv:2310.11590v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25311;&#36890;&#36807;&#38750;&#35821;&#35328;&#34892;&#20026;&#25552;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#21360;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#20998;&#26512;&#32467;&#26524;&#65292;&#21457;&#29616;&#22312;&#23548;&#33322;&#22330;&#26223;&#20013;&#65292;&#31354;&#38388;&#29305;&#24449;&#26159;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;&#21360;&#35937;&#36890;&#24120;&#36890;&#36807;&#35843;&#26597;&#38382;&#21367;&#26469;&#34913;&#37327;&#12290;&#20316;&#20026;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#19988;&#25104;&#26412;&#25928;&#30410;&#26356;&#39640;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#35821;&#35328;&#34892;&#20026;&#25552;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#21360;&#35937;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;SEAN TOGETHER&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#22312;&#34394;&#25311;&#29616;&#23454;&#27169;&#25311;&#20013;&#20154;&#19982;&#31227;&#21160;&#26426;&#22120;&#20154;&#30456;&#20114;&#20316;&#29992;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#20197;&#21450;&#29992;&#25143;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;5&#28857;&#37327;&#34920;&#35780;&#20215;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#20154;&#31867;&#21644;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#22914;&#20309;&#22522;&#20110;&#19981;&#21516;&#30340;&#35266;&#23519;&#31867;&#22411;&#65288;&#20363;&#22914;&#38754;&#37096;&#12289;&#31354;&#38388;&#21644;&#22320;&#22270;&#29305;&#24449;&#65289;&#26469;&#39044;&#27979;&#24863;&#30693;&#21040;&#30340;&#26426;&#22120;&#20154;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20165;&#38754;&#37096;&#34920;&#24773;&#23601;&#33021;&#25552;&#20379;&#20851;&#20110;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#21360;&#35937;&#30340;&#26377;&#29992;&#20449;&#24687;&#65307;&#20294;&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;&#23548;&#33322;&#22330;&#26223;&#20013;&#65292;&#31354;&#38388;&#29305;&#24449;&#26159;&#36825;&#31181;&#25512;&#26029;&#20219;&#21153;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human impressions of robot performance are often measured through surveys. As a more scalable and cost-effective alternative, we study the possibility of predicting people's impressions of robot behavior using non-verbal behavioral cues and machine learning techniques. To this end, we first contribute the SEAN TOGETHER Dataset consisting of observations of an interaction between a person and a mobile robot in a Virtual Reality simulation, together with impressions of robot performance provided by users on a 5-point scale. Second, we contribute analyses of how well humans and supervised learning techniques can predict perceived robot performance based on different combinations of observation types (e.g., facial, spatial, and map features). Our results show that facial expressions alone provide useful information about human impressions of robot performance; but in the navigation scenarios we tested, spatial features are the most critical piece of information for this inference task. Als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20027;&#21160;&#20219;&#21153;&#24341;&#23548;&#65288;GATE&#65289;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19982;&#29992;&#25143;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#30340;&#12289;&#22522;&#20110;&#35821;&#35328;&#30340;&#20132;&#20114;&#26469;&#24341;&#23548;&#21644;&#25512;&#26029;&#39044;&#26399;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#65292;&#36890;&#36807;GATE&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#27604;&#29992;&#25143;&#32534;&#20889;&#30340;&#25552;&#31034;&#25110;&#26631;&#31614;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.11589</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#33719;&#21462;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Eliciting Human Preferences with Language Models. (arXiv:2310.11589v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20027;&#21160;&#20219;&#21153;&#24341;&#23548;&#65288;GATE&#65289;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19982;&#29992;&#25143;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#30340;&#12289;&#22522;&#20110;&#35821;&#35328;&#30340;&#20132;&#20114;&#26469;&#24341;&#23548;&#21644;&#25512;&#26029;&#39044;&#26399;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#65292;&#36890;&#36807;GATE&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#27604;&#29992;&#25143;&#32534;&#20889;&#30340;&#25552;&#31034;&#25110;&#26631;&#31614;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26631;&#27880;&#31034;&#20363;&#25110;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#25191;&#34892;&#30446;&#26631;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#22312;&#36873;&#25321;&#31034;&#20363;&#25110;&#25776;&#20889;&#25552;&#31034;&#26102;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#8212;&#8212;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#24322;&#24120;&#24773;&#20917;&#12289;&#35201;&#27714;&#31934;&#30830;&#34920;&#36798;&#27169;&#31946;&#20559;&#22909;&#25110;&#38656;&#35201;&#20934;&#30830;&#30340;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#35748;&#30693;&#30340;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;*&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;*&#26469;&#24341;&#23548;&#20219;&#21153;&#35268;&#33539;&#30340;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;**&#29983;&#25104;&#24335;&#20027;&#21160;&#20219;&#21153;&#24341;&#23548;&#65288;GATE&#65289;**&#65306;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#27169;&#22411;&#36890;&#36807;&#19982;&#29992;&#25143;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#30340;&#12289;&#22522;&#20110;&#35821;&#35328;&#30340;&#20132;&#20114;&#26469;&#24341;&#23548;&#24182;&#25512;&#26029;&#39044;&#26399;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#39046;&#22495;&#30740;&#31350;&#20102;GATE&#65306;&#30005;&#23376;&#37038;&#20214;&#39564;&#35777;&#12289;&#20869;&#23481;&#25512;&#33616;&#21644;&#36947;&#24503;&#25512;&#29702;&#12290;&#22312;&#39044;&#20808;&#27880;&#20876;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#31034;&#25191;&#34892;GATE&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;&#36890;&#36807;&#29983;&#25104;&#24320;&#25918;&#24335;&#38382;&#39064;&#25110;&#21512;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#36793;&#30028;&#26696;&#20363;&#65289;&#25152;&#24341;&#21457;&#30340;&#21709;&#24212;&#36890;&#24120;&#27604;&#29992;&#25143;&#32534;&#20889;&#30340;&#25552;&#31034;&#25110;&#26631;&#31614;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;&#29992;&#25143;&#25253;&#21578;&#31216;&#65292;&#20132;&#20114;&#24335;&#20219;&#21153;&#24341;&#23548;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#24110;&#21161;&#20182;&#20204;&#34920;&#36798;&#20559;&#22909;&#21644;&#25351;&#23548;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) can be directed to perform target tasks by using labeled examples or natural language prompts. But selecting examples or writing prompts for can be challenging--especially in tasks that involve unusual edge cases, demand precise articulation of nebulous preferences, or require an accurate mental model of LM behavior. We propose to use *LMs themselves* to guide the task specification process. In this paper, we introduce **Generative Active Task Elicitation (GATE)**: a learning framework in which models elicit and infer intended behavior through free-form, language-based interaction with users. We study GATE in three domains: email validation, content recommendation, and moral reasoning. In preregistered experiments, we show that LMs prompted to perform GATE (e.g., by generating open-ended questions or synthesizing informative edge cases) elicit responses that are often more informative than user-written prompts or labels. Users report that interactive task elicitat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#65288;TOA&#65289;&#30340;&#27010;&#24565;&#21644;&#26694;&#26550;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#23545;&#25512;&#29702;&#20219;&#21153;&#26377;&#29992;&#31572;&#26696;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#23454;&#32423;&#36974;&#34109;&#65288;FLM&#65289;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#25105;&#30417;&#30563;&#30340;TOA&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.11571</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#19968;&#20010;&#22909;&#38382;&#39064;&#65311;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#19982;&#20107;&#23454;&#32423;&#36974;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is a good question? Task-oriented asking with fact-level masking. (arXiv:2310.11571v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#65288;TOA&#65289;&#30340;&#27010;&#24565;&#21644;&#26694;&#26550;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#23545;&#25512;&#29702;&#20219;&#21153;&#26377;&#29992;&#31572;&#26696;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#23454;&#32423;&#36974;&#34109;&#65288;FLM&#65289;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#25105;&#30417;&#30563;&#30340;TOA&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#38382;&#26159;&#29616;&#23454;&#29983;&#27963;&#20013;&#21512;&#20316;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#38382;&#31572;&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#27861;&#24459;&#21161;&#25163;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#27809;&#26377;&#29992;&#25143;&#24773;&#20917;&#30340;&#20855;&#20307;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#27861;&#25552;&#20379;&#20934;&#30830;&#30340;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#20250;&#30452;&#25509;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#25512;&#29702;&#20219;&#21153;&#65292;&#32780;&#19981;&#20250;&#21521;&#29992;&#25143;&#25110;&#31532;&#19977;&#26041;&#25552;&#20986;&#21518;&#32493;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31216;&#20026;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#65288;TOA&#65289;&#12290;&#38646;-shot&#32842;&#22825;&#27169;&#22411;&#21487;&#20197;&#25191;&#34892;TOA&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#20027;&#35201;&#22522;&#20110;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#65292;&#32780;&#19981;&#26159;&#38382;&#39064;&#26159;&#21542;&#23545;&#25104;&#21151;&#30340;&#21512;&#20316;&#26377;&#24110;&#21161;&#12290;&#20026;&#20102;&#33021;&#22815;&#35757;&#32451;&#21644;&#35780;&#20272;TOA&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#23548;&#21521;&#35810;&#38382;&#30340;&#23450;&#20041;&#21644;&#26694;&#26550;&#65292;&#21363;&#29983;&#25104;&#33021;&#22815;&#20026;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#31572;&#26696;&#30340;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20107;&#23454;&#32423;&#36974;&#34109;&#65288;FLM&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30465;&#30053;&#29305;&#23450;&#30340;&#37096;&#20998;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#25105;&#30417;&#30563;&#30340;TOA&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Asking questions is an important element of real-life collaboration on reasoning tasks like question answering. For example, a legal assistant chatbot may be unable to make accurate recommendations without specific information on the user's circumstances. However, large language models are usually deployed to solve reasoning tasks directly without asking follow-up questions to the user or third parties. We term this problem task-oriented asking (TOA). Zero-shot chat models can perform TOA, but their training is primarily based on next-token prediction rather than whether questions contribute to successful collaboration. To enable the training and evaluation of TOA models, we present a definition and framework for natural language task-oriented asking, the problem of generating questions that result in answers useful for a reasoning task. We also present fact-level masking (FLM), a procedure for converting natural language datasets into self-supervised TOA datasets by omitting particula
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#23618;&#27425;&#21270;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#32771;&#34385;&#28857;&#39044;&#27979;&#65292;&#36824;&#33021;&#25552;&#20379;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11569</link><description>&lt;p&gt;
&#24403;&#21018;&#24615;&#25104;&#20026;&#38382;&#39064;&#65306;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#29992;&#20110;&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting. (arXiv:2310.11569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#23618;&#27425;&#21270;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#32771;&#34385;&#28857;&#39044;&#27979;&#65292;&#36824;&#33021;&#25552;&#20379;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#37325;&#35201;&#21464;&#20307;&#65292;&#20854;&#30446;&#26631;&#26159;&#23545;&#20855;&#26377;&#23618;&#27425;&#21270;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20851;&#27880;&#28857;&#39044;&#27979;&#65292;&#24182;&#26410;&#25552;&#20379;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#20063;&#22312;&#28857;&#39044;&#27979;&#21644;&#20998;&#24067;&#26679;&#26412;&#20013;&#26045;&#21152;&#23618;&#27425;&#20851;&#31995;&#65292;&#20294;&#26410;&#32771;&#34385;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20063;&#40664;&#35748;&#25968;&#25454;&#38598;&#24635;&#26159;&#19982;&#32473;&#23450;&#30340;&#23618;&#27425;&#20851;&#31995;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#26410;&#36866;&#24212;&#26174;&#31034;&#20986;&#20559;&#31163;&#27492;&#20551;&#35774;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20004;&#20010;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;PROFHiT&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#23436;&#20840;&#27010;&#29575;&#30340;&#20998;&#23618;&#39044;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#23545;&#25972;&#20010;&#23618;&#27425;&#30340;&#39044;&#27979;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;PROFHiT&#20351;&#29992;&#28789;&#27963;&#30340;&#27010;&#29575;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gap and propose PROFHiT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHiT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regulariz
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#65288;NS-POSGs&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#34701;&#21512;&#24863;&#30693;&#26426;&#21046;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#24207;&#21015;&#20915;&#31574;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#21482;&#26377;&#37096;&#20998;&#35266;&#27979;&#20449;&#24687;&#30340;&#26234;&#33021;&#20307;&#21644;&#19968;&#31181;&#23436;&#20840;&#35266;&#27979;&#30340;&#26234;&#33021;&#20307;&#30340;&#21333;&#26041;&#38754;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;NS-POSGs&#20540;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11566</link><description>&lt;p&gt;
&#20855;&#26377;&#31070;&#32463;&#24863;&#30693;&#26426;&#21046;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Stochastic Games with Neural Perception Mechanisms. (arXiv:2310.11566v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#65288;NS-POSGs&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#34701;&#21512;&#24863;&#30693;&#26426;&#21046;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#24207;&#21015;&#20915;&#31574;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#21482;&#26377;&#37096;&#20998;&#35266;&#27979;&#20449;&#24687;&#30340;&#26234;&#33021;&#20307;&#21644;&#19968;&#31181;&#23436;&#20840;&#35266;&#27979;&#30340;&#26234;&#33021;&#20307;&#30340;&#21333;&#26041;&#38754;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;NS-POSGs&#20540;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21338;&#24328;&#26159;&#19968;&#20010;&#20026;&#22810;&#26234;&#33021;&#20307;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#24207;&#21015;&#20915;&#31574;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#22312;&#29616;&#23454;&#20013;&#65292;&#26234;&#33021;&#20307;&#23545;&#29615;&#22659;&#21482;&#26377;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#36825;&#20351;&#24471;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#21333;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#26234;&#33021;&#20307;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#22312;&#36830;&#32493;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#24863;&#30693;&#29615;&#22659;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#65288;NS-POSGs&#65289;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#36830;&#32493;&#31354;&#38388;&#24182;&#21457;&#38543;&#26426;&#21338;&#24328;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#26126;&#30830;&#22320;&#34701;&#20837;&#20102;&#24863;&#30693;&#26426;&#21046;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21333;&#26041;&#38754;&#30340;&#35774;&#32622;&#65292;&#21253;&#21547;&#20102;&#19968;&#20010;&#20855;&#26377;&#31163;&#25955;&#12289;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#35266;&#27979;&#21644;&#19968;&#20010;&#20855;&#26377;&#36830;&#32493;&#35266;&#27979;&#30340;&#20805;&#20998;&#20102;&#35299;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#36793;NS-HSVI&#30340;&#22522;&#20110;&#28857;&#30340;&#26041;&#27861;&#65292;&#29992;&#26469;&#36817;&#20284;&#35745;&#31639;&#21333;&#26041;&#38754;NS-POSGs&#30340;&#20540;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic games are a well established model for multi-agent sequential decision making under uncertainty. In reality, though, agents have only partial observability of their environment, which makes the problem computationally challenging, even in the single-agent setting of partially observable Markov decision processes. Furthermore, in practice, agents increasingly perceive their environment using data-driven approaches such as neural networks trained on continuous data. To tackle this problem, we propose the model of neuro-symbolic partially-observable stochastic games (NS-POSGs), a variant of continuous-space concurrent stochastic games that explicitly incorporates perception mechanisms. We focus on a one-sided setting, comprising a partially-informed agent with discrete, data-driven observations and a fully-informed agent with continuous observations. We present a new point-based method, called one-sided NS-HSVI, for approximating values of one-sided NS-POSGs and implement it ba
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#35774;&#35745;&#22312;&#32447;&#31639;&#27861;&#26102;&#26368;&#20339;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#39044;&#27979;&#27010;&#29575;&#24615;&#30340;&#22312;&#32447;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11558</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#39044;&#27979;&#30340;&#22312;&#32447;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Algorithms with Uncertainty-Quantified Predictions. (arXiv:2310.11558v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11558
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#35774;&#35745;&#22312;&#32447;&#31639;&#27861;&#26102;&#26368;&#20339;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#39044;&#27979;&#27010;&#29575;&#24615;&#30340;&#22312;&#32447;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#39044;&#27979;&#30340;&#22312;&#32447;&#31639;&#27861;&#24050;&#25104;&#20026;&#31639;&#27861;&#30340;&#36229;&#36234;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#39046;&#22495;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#36825;&#20123;&#31639;&#27861;&#21033;&#29992;&#23545;&#26410;&#26469;&#30340;&#39044;&#27979;&#26469;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#24403;&#39044;&#27979;&#33391;&#22909;&#26102;&#65292;&#21516;&#26102;&#22312;&#39044;&#27979;&#20219;&#24847;&#24046;&#30340;&#24773;&#20917;&#19979;&#20173;&#20445;&#25345;&#30028;&#38480;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#31639;&#27861;&#34987;&#35748;&#20026;&#23545;&#39044;&#27979;&#36136;&#37327;&#19981;&#30693;&#24773;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#26368;&#26032;&#21457;&#23637;&#24050;&#30740;&#31350;&#20102;&#25552;&#20379;&#23545;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#36827;&#34892;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#25216;&#26415;&#65292;&#21363;&#25551;&#36848;&#27169;&#22411;&#23545;&#20854;&#36136;&#37327;&#30340;&#30830;&#23450;&#31243;&#24230;&#12290;&#26412;&#25991;&#32771;&#23519;&#20102;&#22914;&#20309;&#22312;&#22312;&#32447;&#31639;&#27861;&#35774;&#35745;&#20013;&#26368;&#20339;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#29992;&#25551;&#36848;&#22522;&#26412;&#20107;&#23454;&#33853;&#22312;&#26576;&#20010;&#33539;&#22260;&#20869;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22686;&#24378;&#39044;&#27979;&#30340;&#24773;&#20917;&#65292;&#24182;&#35774;&#35745;&#20102;&#20855;&#26377;&#36825;&#20123;&#27010;&#29575;&#24615;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online algorithms with predictions have become a trending topic in the field of beyond worst-case analysis of algorithms. These algorithms incorporate predictions about the future to obtain performance guarantees that are of high quality when the predictions are good, while still maintaining bounded worst-case guarantees when predictions are arbitrarily poor. In general, the algorithm is assumed to be unaware of the prediction's quality. However, recent developments in the machine learning literature have studied techniques for providing uncertainty quantification on machine-learned predictions, which describes how certain a model is about its quality. This paper examines the question of how to optimally utilize uncertainty-quantified predictions in the design of online algorithms. In particular, we consider predictions augmented with uncertainty quantification describing the likelihood of the ground truth falling in a certain range, designing online algorithms with these probabilistic
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24102;&#26377;&#23545;&#25239;&#25439;&#22833;&#21644;&#24378;&#30423;&#21453;&#39304;&#30340;&#32447;&#24615;MDPs&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;$\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$&#21644;$\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$&#30340;&#36951;&#25022;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11550</link><description>&lt;p&gt;
&#38754;&#21521;&#24102;&#26377;&#24378;&#23545;&#25239;&#25439;&#22833;&#21644;&#24378;&#30423;&#21453;&#39304;&#30340;&#23545;&#25239;&#24615;&#32447;&#24615;MDPs&#30340;&#26368;&#20248;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback. (arXiv:2310.11550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11550
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24102;&#26377;&#23545;&#25239;&#25439;&#22833;&#21644;&#24378;&#30423;&#21453;&#39304;&#30340;&#32447;&#24615;MDPs&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;$\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$&#21644;$\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$&#30340;&#36951;&#25022;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#32771;&#34385;&#20102;&#23545;&#25239;&#24615;&#25439;&#22833;&#21644;&#24378;&#30423;&#21453;&#39304;&#65292;&#27809;&#26377;&#20107;&#20808;&#20102;&#35299;&#36716;&#25442;&#25110;&#35775;&#38382;&#27169;&#25311;&#22120;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#20204;&#37117;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#36951;&#25022;&#24615;&#33021;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;&#34429;&#28982;&#35745;&#31639;&#25928;&#29575;&#20302;&#65292;&#20294;&#33021;&#20445;&#35777;$\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$&#30340;&#36951;&#25022;&#24615;&#33021;&#65292;&#20854;&#20013;$K$&#26159;&#22238;&#21512;&#25968;&#12290;&#36825;&#26159;&#35813;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;$K$&#20381;&#36182;&#24615;&#30340;&#32467;&#26524;&#12290;&#31532;&#20108;&#31181;&#31639;&#27861;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#20445;&#35777;$\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$&#30340;&#36951;&#25022;&#24615;&#33021;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;&#25105;&#20204;&#30340;&#20004;&#20010;&#32467;&#26524;&#37117;&#26174;&#33879;&#25913;&#21892;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65306;Kong&#31561;&#20154;[2023]&#30340;&#35745;&#31639;&#25928;&#29575;&#20302;&#30340;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#24615;&#33021;&#20026;$\widetilde{\mathcal{O}}\left(K^{\frac{4}{5}}+poly\left(\frac{1}{\lambda_{\min}}\right) \right)$&#65292;&#20854;&#20013;$\lambda_{\min}$&#26159;&#38382;&#39064;&#30456;&#20851;&#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback, without prior knowledge on transitions or access to simulators. We introduce two algorithms that achieve improved regret performance compared to existing approaches. The first algorithm, although computationally inefficient, ensures a regret of $\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$, where $K$ is the number of episodes. This is the first result with the optimal $K$ dependence in the considered setting. The second algorithm, which is based on the policy optimization framework, guarantees a regret of $\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$ and is computationally efficient. Both our results significantly improve over the state-of-the-art: a computationally inefficient algorithm by Kong et al. [2023] with $\widetilde{\mathcal{O}}\left(K^{\frac{4}{5}}+poly\left(\frac{1}{\lambda_{\min}}\right) \right)$ regret, for some problem-dependent constant $\lam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32423;&#25628;&#32034;&#21644;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#20195;&#30721;&#27169;&#22411;&#26469;&#20943;&#36731;&#36719;&#20214;&#29983;&#25104;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#21644;&#35823;&#24046;&#12290;&#36890;&#36807;&#32416;&#27491;&#20043;&#21069;&#29256;&#26412;&#20013;&#30340;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25913;&#21892;&#25968;&#25454;&#20998;&#26512;&#21644;&#29983;&#25104;&#36719;&#20214;&#31995;&#32479;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.11546</link><description>&lt;p&gt;
&#36719;&#20214;&#29983;&#25104;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#19982;&#35823;&#24046;&#20943;&#36731;&#65306;&#36816;&#29992;&#29983;&#25104;&#20195;&#30721;&#27169;&#22411;&#30340;&#39640;&#32423;&#25628;&#32034;&#19982;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Bias and Error Mitigation in Software-Generated Data: An Advanced Search and Optimization Framework Leveraging Generative Code Models. (arXiv:2310.11546v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32423;&#25628;&#32034;&#21644;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#20195;&#30721;&#27169;&#22411;&#26469;&#20943;&#36731;&#36719;&#20214;&#29983;&#25104;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#21644;&#35823;&#24046;&#12290;&#36890;&#36807;&#32416;&#27491;&#20043;&#21069;&#29256;&#26412;&#20013;&#30340;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25913;&#21892;&#25968;&#25454;&#20998;&#26512;&#21644;&#29983;&#25104;&#36719;&#20214;&#31995;&#32479;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#29983;&#25104;&#21644;&#20998;&#26512;&#26159;&#35768;&#22810;&#34892;&#19994;&#21644;&#23398;&#31185;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#20174;&#20225;&#19994;&#30340;&#25112;&#30053;&#20915;&#31574;&#21040;&#29289;&#29702;&#21644;&#31038;&#20250;&#31185;&#23398;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36719;&#20214;&#21644;&#31639;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#33021;&#20250;&#21463;&#21040;&#20559;&#24046;&#21644;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#20986;&#29616;&#22312;&#21407;&#22987;&#36719;&#20214;&#20013;&#65292;&#21487;&#33021;&#30001;&#20110;&#40664;&#35748;&#35774;&#32622;&#19982;&#20855;&#20307;&#38656;&#27714;&#19981;&#19968;&#33268;&#65292;&#29978;&#33267;&#21487;&#33021;&#20986;&#29616;&#22312;&#24213;&#23618;&#29702;&#35770;&#21644;&#27169;&#22411;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32423;&#25628;&#32034;&#21644;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#29983;&#25104;&#21644;&#36873;&#25321;&#33021;&#22815;&#32416;&#27491;&#21069;&#29256;&#26412;&#20013;&#30340;&#38169;&#35823;&#21644;&#20559;&#24046;&#30340;&#26368;&#20339;&#28304;&#20195;&#30721;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20998;&#26512;&#21644;&#29983;&#25104;&#30340;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#20856;&#22411;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#20225;&#19994;&#21644;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#30340;&#36719;&#20214;&#31995;&#32479;&#12290;&#22312;&#21516;&#19968;&#20010;&#36719;&#20214;&#31995;&#32479;&#19978;&#22810;&#27425;&#24212;&#29992;&#35813;&#26694;&#26550;&#23558;&#36880;&#27493;&#25552;&#39640;&#36755;&#20986;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#23427;&#20197;Solomonoff&#24402;&#32435;&#20316;&#20026;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#36827;&#34892;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data generation and analysis is a fundamental aspect of many industries and disciplines, from strategic decision making in business to research in the physical and social sciences. However, data generated using software and algorithms can be subject to biases and errors. These can be due to problems with the original software, default settings that do not align with the specific needs of the situation, or even deeper problems with the underlying theories and models. This paper proposes an advanced search and optimization framework aimed at generating and choosing optimal source code capable of correcting errors and biases from previous versions to address typical problems in software systems specializing in data analysis and generation, especially those in the corporate and data science world. Applying this framework multiple times on the same software system would incrementally improve the quality of the output results. It uses Solomonoff Induction as a sound theoretical basis, extend
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#21644;&#32479;&#19968;&#38899;&#33410;&#26631;&#35760;&#30340;&#25991;&#26412;&#21644;&#38899;&#38901;&#39046;&#22495;&#20013;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#38899;&#33410;&#21270;&#21333;&#35789;&#24182;&#29983;&#25104;&#23453;&#36149;&#27880;&#37322;&#65292;&#36866;&#29992;&#20110;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12289;&#35821;&#38899;&#21333;&#20803;&#21457;&#29616;&#21644;&#35821;&#38899;&#22240;&#32032;&#35299;&#32544;&#12290;</title><link>http://arxiv.org/abs/2310.11541</link><description>&lt;p&gt;
MUST&amp;P-SRL: &#22810;&#35821;&#35328;&#21644;&#32479;&#19968;&#38899;&#33410;&#26631;&#35760;&#30340;&#25991;&#26412;&#21644;&#38899;&#38901;&#39046;&#22495;&#20013;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MUST&amp;P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning. (arXiv:2310.11541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#21644;&#32479;&#19968;&#38899;&#33410;&#26631;&#35760;&#30340;&#25991;&#26412;&#21644;&#38899;&#38901;&#39046;&#22495;&#20013;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#38899;&#33410;&#21270;&#21333;&#35789;&#24182;&#29983;&#25104;&#23453;&#36149;&#27880;&#37322;&#65292;&#36866;&#29992;&#20110;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12289;&#35821;&#38899;&#21333;&#20803;&#21457;&#29616;&#21644;&#35821;&#38899;&#22240;&#32032;&#35299;&#32544;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#22810;&#31181;&#35821;&#35328;&#20013;&#33258;&#21160;&#38899;&#33410;&#21270;&#21333;&#35789;&#65292;&#24182;&#35774;&#35745;&#19982;&#24378;&#21046;&#23545;&#40784;&#24037;&#20855;Montreal Forced Aligner&#65288;MFA&#65289;&#20860;&#23481;&#12290;&#22312;&#25991;&#26412;&#21644;&#38899;&#38901;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#38899;&#26631;&#36716;&#24405;&#12289;&#37325;&#38899;&#26631;&#35760;&#21644;&#32479;&#19968;&#30340;&#33258;&#21160;&#38899;&#33410;&#21270;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#20102;&#24320;&#28304;&#32452;&#20214;&#21644;&#36164;&#28304;&#26500;&#24314;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#38899;&#33410;&#21270;&#22810;&#31181;&#35821;&#35328;&#65288;&#33521;&#35821;&#12289;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65289;&#30340;&#21333;&#35789;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35813;&#25216;&#26415;&#24212;&#29992;&#20110;CMU ARCTIC&#25968;&#25454;&#38598;&#30340;&#36716;&#24405;&#20013;&#65292;&#29983;&#25104;&#20102;&#26377;&#21161;&#20110;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12289;&#35821;&#38899;&#21333;&#20803;&#21457;&#29616;&#21644;&#35821;&#38899;&#22240;&#32032;&#35299;&#32544;&#30340;&#23453;&#36149;&#27880;&#37322;&#65292;&#22312;&#32447;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online\footnote{\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#26080;&#38480;&#26102;&#22495;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#20195;&#29702;&#27169;&#25311;&#19987;&#23478;&#30340;&#34892;&#20026;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23567;&#32047;&#31215;&#36951;&#25022;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#30340;&#20808;&#39564;&#30456;&#20851;&#36951;&#25022;&#20998;&#26512;&#25552;&#20379;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#19978;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#26469;&#32467;&#21512;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#22312;&#32447;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.11531</link><description>&lt;p&gt;
&#22312;&#26080;&#38480;&#26102;&#22495;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#39640;&#25928;&#22312;&#32447;&#23398;&#20064;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach. (arXiv:2310.11531v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#26080;&#38480;&#26102;&#22495;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#20195;&#29702;&#27169;&#25311;&#19987;&#23478;&#30340;&#34892;&#20026;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23567;&#32047;&#31215;&#36951;&#25022;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#30340;&#20808;&#39564;&#30456;&#20851;&#36951;&#25022;&#20998;&#26512;&#25552;&#20379;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#19978;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#26469;&#32467;&#21512;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#22312;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#23384;&#22312;&#19968;&#20010;&#31163;&#32447;&#25968;&#25454;&#38598;&#26102;&#65292;&#22914;&#20309;&#22312;&#26080;&#38480;&#26102;&#22495;&#35774;&#32622;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#31163;&#32447;&#25968;&#25454;&#38598;&#26159;&#30001;&#19968;&#20010;&#19987;&#23478;&#29983;&#25104;&#30340;&#65292;&#20294;&#20854;&#33021;&#21147;&#27700;&#24179;&#26410;&#30693;&#65292;&#21363;&#23427;&#19981;&#26159;&#23436;&#32654;&#30340;&#65292;&#20063;&#19981;&#19968;&#23450;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#23398;&#20064;&#20195;&#29702;&#27169;&#25311;&#19987;&#23478;&#20351;&#29992;&#30340;&#34892;&#20026;&#31574;&#30053;&#65288;&#30001;&#33021;&#21147;&#21442;&#25968;&#21442;&#25968;&#21270;&#65289;&#65292;&#22312;&#32047;&#31215;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#38754;&#33021;&#21462;&#24471;&#26126;&#26174;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20197; $\tilde{O}(\sqrt{T})$ &#20026;&#32553;&#25918;&#30340;&#31934;&#30830;&#26377;&#29992;PSRL&#31639;&#27861;&#36951;&#25022;&#30340;&#19978;&#30028;&#12290;&#36825;&#38656;&#35201;&#23545;&#36125;&#21494;&#26031;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#26080;&#38480;&#26102;&#22495;&#35774;&#32622;&#19979;&#36827;&#34892;&#26032;&#39062;&#30340;&#20808;&#39564;&#30456;&#20851;&#36951;&#25022;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;Informed RLSVI&#31639;&#27861;&#65292;&#21487;&#20197;&#29702;&#35299;&#20026;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#28982;&#21518;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of efficient online reinforcement learning in the infinite horizon setting when there is an offline dataset to start with. We assume that the offline dataset is generated by an expert but with unknown level of competence, i.e., it is not perfect and not necessarily using the optimal policy. We show that if the learning agent models the behavioral policy (parameterized by a competence parameter) used by the expert, it can do substantially better in terms of minimizing cumulative regret, than if it doesn't do that. We establish an upper bound on regret of the exact informed PSRL algorithm that scales as $\tilde{O}(\sqrt{T})$. This requires a novel prior-dependent regret analysis of Bayesian online learning algorithms for the infinite horizon setting. We then propose an approximate Informed RLSVI algorithm that we can interpret as performing imitation learning with the offline dataset, and then performing online learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34180;&#32780;&#28145;&#30340;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#23398;&#20064;&#20302;&#32500;&#23884;&#20837;&#21644;&#35299;&#37322;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.11527</link><description>&lt;p&gt;
&#34180;&#32780;&#28145;&#30340;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Thin and Deep Gaussian Processes. (arXiv:2310.11527v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34180;&#32780;&#28145;&#30340;&#39640;&#26031;&#36807;&#31243;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#23398;&#20064;&#20302;&#32500;&#23884;&#20837;&#21644;&#35299;&#37322;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20855;&#26377;&#26131;&#20110;&#35299;&#37322;&#30340;&#20869;&#26680;&#36229;&#21442;&#25968;&#65292;&#22914;&#38271;&#24230;&#23610;&#24230;&#65292;&#21487;&#20197;&#25511;&#21046;&#20989;&#25968;&#20540;&#30340;&#30456;&#20851;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#21512;&#36866;&#30340;&#20869;&#26680;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#36890;&#36807;&#36880;&#23618;&#21442;&#25968;&#21270;GP&#23618;&#30340;&#20869;&#26680;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#20869;&#26680;&#24037;&#31243;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#35299;&#37322;&#36755;&#20986;&#25968;&#25454;&#30340;&#20302;&#32500;&#23884;&#20837;&#26041;&#27861;&#12290;&#27839;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#26368;&#24120;&#35265;&#30340;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#36880;&#23618;&#21464;&#24418;&#36755;&#20837;&#31354;&#38388;&#65292;&#20294;&#22833;&#21435;&#20102;&#27973;&#23618;&#39640;&#26031;&#36807;&#31243;&#30340;&#25152;&#26377;&#35299;&#37322;&#24615;&#12290;&#21478;&#19968;&#31181;&#26500;&#24314;&#26041;&#27861;&#26159;&#36880;&#23618;&#21442;&#25968;&#21270;&#20869;&#26680;&#30340;&#38271;&#24230;&#23610;&#24230;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#65292;&#20294;&#26368;&#32456;&#25918;&#24323;&#20102;&#23398;&#20064;&#20302;&#32500;&#23884;&#20837;&#30340;&#27010;&#24565;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23481;&#26131;&#21463;&#21040;&#29305;&#23450;&#30340;&#30149;&#24577;&#24433;&#21709;&#65292;&#21487;&#33021;&#20250;&#38459;&#30861;&#25311;&#21512;&#24182;&#38480;&#21046;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32508;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes (GPs) can provide a principled approach to uncertainty quantification with easy-to-interpret kernel hyperparameters, such as the lengthscale, which controls the correlation distance of function values. However, selecting an appropriate kernel can be challenging. Deep GPs avoid manual kernel engineering by successively parameterizing kernels with GP layers, allowing them to learn low-dimensional embeddings of the inputs that explain the output data. Following the architecture of deep neural networks, the most common deep GPs warp the input space layer-by-layer but lose all the interpretability of shallow GPs. An alternative construction is to successively parameterize the lengthscale of a kernel, improving the interpretability but ultimately giving away the notion of learning lower-dimensional embeddings. Unfortunately, both methods are susceptible to particular pathologies which may hinder fitting and limit their interpretability. This work proposes a novel synthesis
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#30340;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#23548;&#21040;&#20010;&#21035;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#22522;&#26412;LLM&#19978;&#21152;&#20837;&#29420;&#31435;&#30340;transformer&#27169;&#22359;&#26469;&#39044;&#27979;&#32676;&#20307;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;GPO&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11523</link><description>&lt;p&gt;
&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Group Preference Optimization: Few-Shot Alignment of Large Language Models. (arXiv:2310.11523v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11523
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#30340;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#23548;&#21040;&#20010;&#21035;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#22522;&#26412;LLM&#19978;&#21152;&#20837;&#29420;&#31435;&#30340;transformer&#27169;&#22359;&#26469;&#39044;&#27979;&#32676;&#20307;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;GPO&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35768;&#22810;&#24212;&#29992;&#65292;&#20174;&#32842;&#22825;&#26426;&#22120;&#20154;&#21040;&#21019;&#24847;&#20889;&#20316;&#65292;&#37117;&#38656;&#35201;&#32454;&#33268;&#20837;&#24494;&#30340;&#20027;&#35266;&#21028;&#26029;&#65292;&#36825;&#20123;&#21028;&#26029;&#22312;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#31639;&#27861;&#22312;&#27599;&#20010;&#32676;&#20307;&#19978;&#23545;&#40784;&#30340;&#25104;&#26412;&#24456;&#39640;&#65292;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#32780;&#35328;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#32676;&#20307;&#29305;&#23450;&#20559;&#22909;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#21040;&#20010;&#21035;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#22312;GPO&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29420;&#31435;&#30340;transformer&#27169;&#22359;&#26469;&#25193;&#20805;&#22522;&#26412;LLM&#65292;&#29992;&#20110;&#39044;&#27979;&#32676;&#20307;&#23545;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20559;&#22909;&#12290;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#27169;&#22359;&#21442;&#25968;&#21270;&#20026;&#19968;&#20010;&#19978;&#19979;&#25991;&#33258;&#22238;&#24402;&#30340;transformer&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#22810;&#20010;&#32676;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#19981;&#21516;&#35268;&#27169;&#30340;LLM&#22312;&#19977;&#20010;&#20154;&#31867;&#24847;&#35265;&#36866;&#24212;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;GPO&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations. For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#21644;&#35780;&#20272;&#20102;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#22312;&#26032;&#38395;&#25991;&#26412;&#25688;&#35201;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#20351;&#29992;ROUGE&#24471;&#20998;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12290;&#26368;&#20339;&#34920;&#29616;&#27169;&#22411;&#34987;&#38598;&#25104;&#21040;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33021;&#21147;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2310.11520</link><description>&lt;p&gt;
&#33258;&#21160;&#26032;&#38395;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Automatic News Summerization. (arXiv:2310.11520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#21644;&#35780;&#20272;&#20102;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#22312;&#26032;&#38395;&#25991;&#26412;&#25688;&#35201;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#20351;&#29992;ROUGE&#24471;&#20998;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12290;&#26368;&#20339;&#34920;&#29616;&#27169;&#22411;&#34987;&#38598;&#25104;&#21040;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33021;&#21147;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#27491;&#22312;&#34028;&#21187;&#21457;&#23637;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#38024;&#23545;&#21253;&#25324;&#26032;&#38395;&#25991;&#31456;&#22312;&#20869;&#30340;&#22823;&#22411;&#25991;&#26412;&#30340;&#25991;&#26412;&#25688;&#35201;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#23545;&#26032;&#38395;&#25991;&#26412;&#25688;&#35201;&#30340;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#35780;&#20272;&#65292;&#24182;&#37325;&#28857;&#20998;&#26512;&#20102;ROUGE&#24471;&#20998;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;CNN-Daily Mail&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26032;&#38395;&#25991;&#31456;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;&#21442;&#32771;&#25688;&#35201;&#12290;&#35780;&#20272;&#20351;&#29992;ROUGE&#24471;&#20998;&#26469;&#35780;&#20272;&#29983;&#25104;&#25688;&#35201;&#30340;&#25928;&#26524;&#21644;&#36136;&#37327;&#12290;&#22312;&#35780;&#20272;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#26368;&#20339;&#34920;&#29616;&#27169;&#22411;&#38598;&#25104;&#21040;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#35780;&#20272;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33021;&#21147;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing is booming with its applications in the real world, one of which is Text Summarization for large texts including news articles. This research paper provides an extensive comparative evaluation of extractive and abstractive approaches for news text summarization, with an emphasis on the ROUGE score analysis. The study employs the CNN-Daily Mail dataset, which consists of news articles and human-generated reference summaries. The evaluation employs ROUGE scores to assess the efficacy and quality of generated summaries. After Evaluation, we integrate the best-performing models on a web application to assess their real-world capabilities and user experience.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.11518</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#22312;&#22810;&#20154;&#28216;&#25103;&#20013;&#23545;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability. (arXiv:2310.11518v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11518
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#23545;&#25239;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#21103;&#26412;&#20132;&#20114;&#26469;&#23398;&#20064;&#12290;&#33258;&#25105;&#23545;&#25239;&#23545;&#20110;&#29983;&#25104;&#22823;&#37327;&#30340;&#23398;&#20064;&#25968;&#25454;&#24456;&#26377;&#29992;&#65292;&#20294;&#23427;&#30340;&#32570;&#28857;&#26159;&#35757;&#32451;&#21518;&#23398;&#20064;&#32773;&#23558;&#38754;&#23545;&#30340;&#26234;&#33021;&#20307;&#21487;&#33021;&#19982;&#36890;&#36807;&#19982;&#33258;&#36523;&#20132;&#20114;&#26102;&#25152;&#26399;&#26395;&#30340;&#26234;&#33021;&#20307;&#34892;&#20026;&#25130;&#28982;&#19981;&#21516;&#12290;&#23545;&#20110;&#20004;&#20154;&#24120;&#21644;&#28216;&#25103;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#36798;&#21040;&#32435;&#20160;&#22343;&#34913;&#30340;&#33258;&#25105;&#23545;&#25239;&#33021;&#22815;&#20445;&#35777;&#20135;&#29983;&#23545;&#20219;&#20309;&#35757;&#32451;&#21518;&#23545;&#25163;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#20154;&#28216;&#25103;&#26469;&#35828;&#27809;&#26377;&#36825;&#26679;&#30340;&#20445;&#35777;&#23384;&#22312;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36817;&#20284;&#20998;&#35299;&#20026;&#19968;&#32452;&#20004;&#20154;&#24120;&#21644;&#28216;&#25103;&#65288;&#31216;&#20026;&#22810;&#30697;&#38453;&#28216;&#25103;&#65289;&#30340;&#28216;&#25103;&#20013;&#65292;&#20854;&#20013;&#20840;&#23616; $\epsilon$-&#32435;&#20160;&#22343;&#34913;&#22312;&#27599;&#20010;&#23376;&#28216;&#25103;&#20013;&#37117;&#19982;&#32435;&#20160;&#22343;&#34913;&#26377;&#26377;&#30028;&#36317;&#31163;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#23558;&#20135;&#29983;&#19968;&#20010;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39318;&#27425;&#30830;&#23450;&#20102;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Self-play is a technique for machine learning in multi-agent systems where a learning algorithm learns by interacting with copies of itself. Self-play is useful for generating large quantities of data for learning, but has the drawback that the agents the learner will face post-training may have dramatically different behavior than the learner came to expect by interacting with itself. For the special case of two-player constant-sum games, self-play that reaches Nash equilibrium is guaranteed to produce strategies that perform well against any post-training opponent; however, no such guarantee exists for multi-player games. We show that in games that approximately decompose into a set of two-player constant-sum games (called polymatrix games) where global $\epsilon$-Nash equilibria are boundedly far from Nash-equilibria in each subgame, any no-external-regret algorithm that learns by self-play will produce a strategy with bounded vulnerability. For the first time, our results identify 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20215;&#20540;&#20559;&#35265;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#21518;&#24724;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11515</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#20559;&#35265;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22312;&#25240;&#25187;&#32447;&#24615;MDPs&#20013;&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Value-Biased Maximum Likelihood Estimation for Model-based Reinforcement Learning in Discounted Linear MDPs. (arXiv:2310.11515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11515
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20215;&#20540;&#20559;&#35265;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#21518;&#24724;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#26080;&#38480;&#26102;&#27573;&#30340;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#65292;&#20854;&#20013;&#21160;&#24577;&#27169;&#22411;&#30340;&#36716;&#31227;&#27010;&#29575;&#21487;&#20197;&#36890;&#36807;&#39044;&#23450;&#20041;&#30340;&#20302;&#32500;&#29305;&#24449;&#26144;&#23556;&#36827;&#34892;&#32447;&#24615;&#21442;&#25968;&#21270;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22522;&#20110;&#22238;&#24402;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#36798;&#21040;&#20960;&#20046;&#26368;&#20248;&#30340;&#21518;&#24724;&#65292;&#20294;&#30001;&#20110;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#20248;&#21270;&#36816;&#34892;&#65292;&#29305;&#21035;&#26159;&#24403;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#36739;&#22823;&#26102;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#19978;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22522;&#20110;&#20215;&#20540;&#20559;&#35265;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;VBMLE&#65289;&#26469;&#35299;&#20915;&#32447;&#24615;MDPs&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#35299;&#20915;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#20013;&#24050;&#30693;&#38381;&#29615;&#35782;&#21035;&#38382;&#39064;&#30340;&#32463;&#20856;&#27169;&#22411;&#39537;&#21160;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#65288;i&#65289;VBMLE&#20139;&#26377;$\widetilde{O}(d\sqrt{T})$&#30340;&#21518;&#24724;&#65292;&#20854;&#20013;$T$&#26159;&#26102;&#38388;&#27573;&#65292;$d$&#26159;&#27169;&#22411;&#21442;&#25968;&#30340;&#32500;&#24230;&#65292;&#20197;&#21450;&#65288;ii&#65289;VBMLE&#22312;&#35745;&#31639;&#19978;&#26159;...
&lt;/p&gt;
&lt;p&gt;
We consider the infinite-horizon linear Markov Decision Processes (MDPs), where the transition probabilities of the dynamic model can be linearly parameterized with the help of a predefined low-dimensional feature mapping. While the existing regression-based approaches have been theoretically shown to achieve nearly-optimal regret, they are computationally rather inefficient due to the need for a large number of optimization runs in each time step, especially when the state and action spaces are large. To address this issue, we propose to solve linear MDPs through the lens of Value-Biased Maximum Likelihood Estimation (VBMLE), which is a classic model-based exploration principle in the adaptive control literature for resolving the well-known closed-loop identification problem of Maximum Likelihood Estimation. We formally show that (i) VBMLE enjoys $\widetilde{O}(d\sqrt{T})$ regret, where $T$ is the time horizon and $d$ is the dimension of the model parameter, and (ii) VBMLE is computat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;GenEval&#65292;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#32452;&#25104;&#23646;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#24403;&#21069;&#30340;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20854;&#20182;&#35270;&#35273;&#21028;&#21035;&#27169;&#22411;&#36827;&#19968;&#27493;&#39564;&#35777;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11513</link><description>&lt;p&gt;
GenEval&#65306;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#30340;&#38754;&#21521;&#23545;&#35937;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment. (arXiv:2310.11513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;GenEval&#65292;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#32452;&#25104;&#23646;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#24403;&#21069;&#30340;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20854;&#20182;&#35270;&#35273;&#21028;&#21035;&#27169;&#22411;&#36827;&#19968;&#27493;&#39564;&#35777;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#39640;&#25928;&#24494;&#35843;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#23548;&#33268;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#12290;&#30001;&#20110;&#20154;&#24037;&#35780;&#20272;&#26114;&#36149;&#19988;&#38590;&#20197;&#25193;&#23637;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#23545;&#20110;&#35780;&#20272;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#33258;&#21160;&#21270;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;FID&#25110;CLIPScore&#65289;&#20165;&#25552;&#20379;&#22270;&#20687;&#36136;&#37327;&#25110;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#30340;&#25972;&#20307;&#24230;&#37327;&#65292;&#24182;&#19981;&#36866;&#29992;&#20110;&#32454;&#31890;&#24230;&#25110;&#23454;&#20363;&#32423;&#21035;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;GenEval&#30340;&#38754;&#21521;&#23545;&#35937;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#30340;&#32452;&#25104;&#23646;&#24615;&#65292;&#22914;&#23545;&#35937;&#20849;&#29616;&#12289;&#20301;&#32622;&#12289;&#35745;&#25968;&#21644;&#39068;&#33394;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#30340;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#19982;&#20154;&#31867;&#30340;&#24378;&#19968;&#33268;&#24615;&#65292;&#20854;&#20313;&#30340;&#35270;&#35273;&#21028;&#21035;&#27169;&#22411;&#21487;&#20197;&#38142;&#25509;&#21040;&#36825;&#20010;&#27969;&#31243;&#20013;&#65292;&#36827;&#19968;&#27493;&#39564;&#35777;&#20687;&#23545;&#35937;&#39068;&#33394;&#36825;&#26679;&#30340;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#20960;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in diffusion models, multimodal pretraining, and efficient finetuning have led to an explosion of text-to-image generative models. Given human evaluation is expensive and difficult to scale, automated methods are critical for evaluating the increasingly large number of new models. However, most current automated evaluation metrics like FID or CLIPScore only offer a holistic measure of image quality or image-text alignment, and are unsuited for fine-grained or instance-level analysis. In this paper, we introduce GenEval, an object-focused framework to evaluate compositional image properties such as object co-occurrence, position, count, and color. We show that current object detection models can be leveraged to evaluate text-to-image models on a variety of generation tasks with strong human agreement, and that other discriminative vision models can be linked to this pipeline to further verify properties like object color. We then evaluate several open-source text-to
&lt;/p&gt;</description></item><item><title>Self-RAG&#26159;&#19968;&#31181;&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#21644;&#20107;&#23454;&#24615;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.11511</link><description>&lt;p&gt;
Self-RAG: &#36890;&#36807;&#33258;&#25105;&#21453;&#24605;&#23398;&#20064;&#26816;&#32034;&#12289;&#29983;&#25104;&#21644;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. (arXiv:2310.11511v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11511
&lt;/p&gt;
&lt;p&gt;
Self-RAG&#26159;&#19968;&#31181;&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#21644;&#20107;&#23454;&#24615;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#23436;&#20840;&#20381;&#36182;&#20110;&#23427;&#20204;&#25152;&#21253;&#21547;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#65292;&#22240;&#27492;&#24448;&#24448;&#20250;&#20135;&#29983;&#21547;&#26377;&#20107;&#23454;&#19981;&#20934;&#30830;&#24615;&#30340;&#21709;&#24212;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#22686;&#24378;LM&#30340;&#20020;&#26102;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19981;&#21152;&#36873;&#25321;&#22320;&#26816;&#32034;&#24182;&#32467;&#21512;&#19968;&#23450;&#25968;&#37327;&#30340;&#26816;&#32034;&#27573;&#33853;&#65292;&#32780;&#19981;&#32771;&#34385;&#26816;&#32034;&#26159;&#21542;&#24517;&#35201;&#25110;&#27573;&#33853;&#26159;&#21542;&#30456;&#20851;&#65292;&#20250;&#38477;&#20302;LM&#30340;&#22810;&#21151;&#33021;&#24615;&#25110;&#23548;&#33268;&#26080;&#25928;&#30340;&#21709;&#24212;&#29983;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Self-Reflective Retrieval-Augmented Generation &#65288;Self-RAG&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25552;&#39640;LM&#30340;&#36136;&#37327;&#21644;&#20107;&#23454;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#29420;&#30340;&#20219;&#24847;LM&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#38656;&#27714;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#27573;&#33853;&#65292;&#24182;&#20351;&#29992;&#29305;&#27530;&#30340;&#26631;&#35760;&#65292;&#31216;&#20026;&#21453;&#24605;&#26631;&#35760;&#65292;&#29983;&#25104;&#21644;&#21453;&#24605;&#26816;&#32034;&#30340;&#27573;&#33853;&#21644;&#33258;&#36523;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20799;&#31461;&#38405;&#35835;&#30340;&#23454;&#26102;&#36319;&#36394;&#22120;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#36319;&#36394;&#30340;&#24310;&#36831;&#26041;&#38754;&#20855;&#26377;&#36739;&#20302;&#30340;&#25935;&#24863;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#38024;&#32593;&#32476;&#21644;&#24378;&#21046;&#23545;&#40784;&#29983;&#25104;&#35757;&#32451;&#20449;&#21495;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#36319;&#36394;&#25104;&#20154;&#35821;&#38899;&#65292;&#24182;&#22312;&#20799;&#31461;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11486</link><description>&lt;p&gt;
&#20799;&#31461;&#38405;&#35835;&#23454;&#26102;&#36319;&#36394;&#30340;&#31471;&#21040;&#31471;&#25351;&#38024;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
End-to-End real time tracking of children's reading with pointer network. (arXiv:2310.11486v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20799;&#31461;&#38405;&#35835;&#30340;&#23454;&#26102;&#36319;&#36394;&#22120;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#36319;&#36394;&#30340;&#24310;&#36831;&#26041;&#38754;&#20855;&#26377;&#36739;&#20302;&#30340;&#25935;&#24863;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#38024;&#32593;&#32476;&#21644;&#24378;&#21046;&#23545;&#40784;&#29983;&#25104;&#35757;&#32451;&#20449;&#21495;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#36319;&#36394;&#25104;&#20154;&#35821;&#38899;&#65292;&#24182;&#22312;&#20799;&#31461;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#26500;&#24314;&#19968;&#20010;&#29992;&#20110;&#20799;&#31461;&#35821;&#38899;&#30340;&#23454;&#26102;&#38405;&#35835;&#36319;&#36394;&#22120;&#12290;&#20043;&#21069;&#25552;&#20986;&#30340;&#38405;&#35835;&#36319;&#36394;&#22120;&#20027;&#35201;&#22522;&#20110;ASR&#30340;&#32423;&#32852;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#23545;&#35821;&#38899;&#36319;&#36394;&#30340;&#24310;&#36831;&#26356;&#23569;&#12290;&#25105;&#20204;&#37319;&#29992;&#25351;&#38024;&#32593;&#32476;&#65292;&#30452;&#25509;&#23398;&#20064;&#22312;&#27969;&#24335;&#35821;&#38899;&#20013;&#39044;&#27979;&#19982;&#30495;&#23454;&#25991;&#26412;&#20301;&#32622;&#23545;&#24212;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#20010;&#25351;&#38024;&#32593;&#32476;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#21046;&#23545;&#40784;&#22312;&#35757;&#32451;&#38598;&#19978;&#29983;&#25104;&#30495;&#23454;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#20174;&#32780;&#23545;&#35835;&#20986;&#30340;&#35821;&#38899;&#21644;&#34987;&#35835;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;&#22312;&#25506;&#32034;&#19981;&#21516;&#30340;&#24378;&#21046;&#23545;&#40784;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#31070;&#32463;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#22312;&#23545;&#40784;&#20934;&#30830;&#24615;&#19978;&#33267;&#23569;&#19982;&#33945;&#29305;&#21033;&#23572;&#24378;&#21046;&#23545;&#40784;&#22120;&#30456;&#24403;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#26356;&#36866;&#21512;&#29992;&#20316;&#25351;&#38024;&#32593;&#32476;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25253;&#21578;&#20102;&#19968;&#20010;&#25104;&#20154;&#35821;&#38899;&#25968;&#25454;&#38598;(TIMIT)&#21644;&#20004;&#20010;&#20799;&#31461;&#35821;&#38899;&#25968;&#25454;&#38598;(CMU Kids&#21644;Reading Races)&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#33021;&#22815;&#20197;87.8%&#30340;&#20934;&#30830;&#29575;&#36319;&#36394;&#25104;&#20154;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore how a real time reading tracker can be built efficiently for children's voices. While previously proposed reading trackers focused on ASR-based cascaded approaches, we propose a fully end-to-end model making it less prone to lags in voice tracking. We employ a pointer network that directly learns to predict positions in the ground truth text conditioned on the streaming speech. To train this pointer network, we generate ground truth training signals by using forced alignment between the read speech and the text being read on the training set. Exploring different forced alignment models, we find a neural attention based model is at least as close in alignment accuracy to the Montreal Forced Aligner, but surprisingly is a better training signal for the pointer network. Our results are reported on one adult speech data (TIMIT) and two children's speech datasets (CMU Kids and Reading Races). Our best model can accurately track adult speech with 87.8% accuracy and t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#38598;&#32676;&#32852;&#37030;&#20010;&#24615;&#21270;&#30340;&#20840;&#33041;&#24433;&#20687;&#32452;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#26426;&#26500;&#38388;&#21644;&#26426;&#26500;&#20869;&#29305;&#24449;&#36716;&#31227;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#24102;&#26469;&#30340;&#25910;&#25947;&#38382;&#39064;&#21644;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.11480</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#32676;&#32852;&#37030;&#20010;&#24615;&#21270;&#30340;&#20840;&#33041;&#24433;&#20687;&#32452;&#23398;&#29992;&#20110;&#33041;&#32959;&#30244;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Whole-brain radiomics for clustered federated personalization in brain tumor segmentation. (arXiv:2310.11480v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11480
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#38598;&#32676;&#32852;&#37030;&#20010;&#24615;&#21270;&#30340;&#20840;&#33041;&#24433;&#20687;&#32452;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#26426;&#26500;&#38388;&#21644;&#26426;&#26500;&#20869;&#29305;&#24449;&#36716;&#31227;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#24102;&#26469;&#30340;&#25910;&#25947;&#38382;&#39064;&#21644;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#21450;&#20854;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#26368;&#36817;&#25104;&#20026;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#36825;&#31181;&#35757;&#32451;&#33539;&#24335;&#22312;&#21442;&#19982;&#26426;&#26500;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#20943;&#24930;&#65292;&#20197;&#21450;&#19982;&#32463;&#20856;&#35757;&#32451;&#30456;&#27604;&#28508;&#22312;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24433;&#21709;&#65292;&#32852;&#37030;&#20010;&#24615;&#21270;&#20316;&#20026;&#27599;&#20010;&#26426;&#26500;&#30340;&#32852;&#37030;&#20248;&#21270;&#36880;&#28176;&#23853;&#38706;&#22836;&#35282;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20010;&#24615;&#21270;&#31639;&#27861;&#65292;&#38024;&#23545;&#19981;&#21516;&#26426;&#26500;&#20351;&#29992;&#19981;&#21516;&#25195;&#25551;&#20202;&#21644;&#37319;&#38598;&#21442;&#25968;&#24341;&#36215;&#30340;&#29305;&#24449;&#36716;&#31227;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#32771;&#34385;&#20102;&#26426;&#26500;&#38388;&#21644;&#26426;&#26500;&#20869;&#29305;&#24449;&#36716;&#31227;&#38382;&#39064;&#65288;&#21516;&#19968;&#20010;&#26426;&#26500;&#20351;&#29992;&#22810;&#20010;&#25195;&#25551;&#20202;&#65289;&#30340;&#26041;&#27861;&#12290;&#23427;&#22522;&#20110;&#22312;&#27599;&#20010;&#20013;&#24515;&#35745;&#31639;&#19968;&#31995;&#21015;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#65292;&#25429;&#25417;&#27599;&#20010;3D&#22270;&#20687;&#20307;&#32032;&#30340;&#20840;&#23616;&#32441;&#29702;&#65292;&#28982;&#21518;&#36890;&#36807;&#32858;&#31867;&#20998;&#26512;&#27719;&#24635;&#20174;&#26412;&#22320;&#26426;&#26500;&#36716;&#31227;&#30340;&#25152;&#26377;&#29305;&#24449;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning and its application to medical image segmentation have recently become a popular research topic. This training paradigm suffers from statistical heterogeneity between participating institutions' local datasets, incurring convergence slowdown as well as potential accuracy loss compared to classical training. To mitigate this effect, federated personalization emerged as the federated optimization of one model per institution. We propose a novel personalization algorithm tailored to the feature shift induced by the usage of different scanners and acquisition parameters by different institutions. This method is the first to account for both inter and intra-institution feature shift (multiple scanners used in a single institution). It is based on the computation, within each centre, of a series of radiomic features capturing the global texture of each 3D image volume, followed by a clustering analysis pooling all feature vectors transferred from the local institutions to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#28201;&#24230;&#21442;&#25968;&#32435;&#20837;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#33268;&#39044;&#27979;&#20013;&#30340;&#20248;&#21183;&#65292;&#20197;&#25552;&#20379;&#26377;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.11479</link><description>&lt;p&gt;
&#20851;&#20110;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#33268;&#39044;&#27979;&#20013;&#30340;&#28201;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction. (arXiv:2310.11479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11479
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#28201;&#24230;&#21442;&#25968;&#32435;&#20837;&#36125;&#21494;&#26031;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#33268;&#39044;&#27979;&#20013;&#30340;&#20248;&#21183;&#65292;&#20197;&#25552;&#20379;&#26377;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#32463;&#24120;&#20351;&#29992;GNNs&#30340;&#24773;&#20917;&#19979;&#12290;&#19968;&#33268;&#39044;&#27979;(CP)&#20026;&#20219;&#20309;&#40657;&#30418;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#12290;CP&#20445;&#35777;&#20102;&#19968;&#20010;&#39044;&#27979;&#38598;&#20197;&#25152;&#38656;&#30340;&#27010;&#29575;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#30340;&#24418;&#24335;&#30340;&#23448;&#26041;&#27010;&#29575;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#38598;&#30340;&#22823;&#23567;&#65292;&#21363;"&#20302;&#25928;&#29575;"&#65292;&#21463;&#21040;&#24213;&#23618;&#27169;&#22411;&#21644;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36125;&#21494;&#26031;&#23398;&#20064;&#36824;&#22522;&#20110;&#20272;&#35745;&#30340;&#21518;&#39564;&#20998;&#24067;&#25552;&#20379;&#19968;&#20010;&#21487;&#20449;&#21306;&#22495;&#65292;&#20294;&#21482;&#26377;&#22312;&#27169;&#22411;&#27491;&#30830;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#21306;&#22495;&#25165;&#26159;"&#33391;&#22909;&#26657;&#20934;"&#30340;&#12290;&#22312;&#19968;&#20010;&#26368;&#36817;&#30340;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#35813;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#32553;&#25918;&#21442;&#25968;&#65292;&#29992;&#20110;&#20174;&#21518;&#39564;&#20272;&#35745;&#20013;&#26500;&#24314;&#26377;&#25928;&#30340;&#21487;&#20449;&#21306;&#22495;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;CP&#26694;&#26550;&#20013;&#23558;&#19968;&#20010;&#28201;&#24230;&#21442;&#25968;&#32435;&#20837;&#36125;&#21494;&#26031;GNNs&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty quantification in graph neural networks (GNNs) is essential, especially in high-stakes domains where GNNs are frequently employed. Conformal prediction (CP) offers a promising framework for quantifying uncertainty by providing $\textit{valid}$ prediction sets for any black-box model. CP ensures formal probabilistic guarantees that a prediction set contains a true label with a desired probability. However, the size of prediction sets, known as $\textit{inefficiency}$, is influenced by the underlying model and data generating process. On the other hand, Bayesian learning also provides a credible region based on the estimated posterior distribution, but this region is $\textit{well-calibrated}$ only when the model is correctly specified. Building on a recent work that introduced a scaling parameter for constructing valid credible regions from posterior estimate, our study explores the advantages of incorporating a temperature parameter into Bayesian GNNs within CP fra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#36873;&#25321;&#20195;&#29702;&#25968;&#25454;&#38598;&#26694;&#26550; (ASP)&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#25214;&#21040;&#20449;&#24687;&#20016;&#23500;&#30340;&#20195;&#29702;&#23376;&#38598;&#26469;&#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#24182;&#33410;&#30465;AutoML&#22788;&#29702;&#26102;&#38388;&#65292;&#23454;&#39564;&#35777;&#26126;ASP&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#19978;&#33719;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11478</link><description>&lt;p&gt;
ASP: &#29992;&#20110;&#39640;&#25928;AutoML&#30340;&#33258;&#21160;&#36873;&#25321;&#20195;&#29702;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ASP: Automatic Selection of Proxy dataset for efficient AutoML. (arXiv:2310.11478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#36873;&#25321;&#20195;&#29702;&#25968;&#25454;&#38598;&#26694;&#26550; (ASP)&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#25214;&#21040;&#20449;&#24687;&#20016;&#23500;&#30340;&#20195;&#29702;&#23376;&#38598;&#26469;&#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#24182;&#33410;&#30465;AutoML&#22788;&#29702;&#26102;&#38388;&#65292;&#23454;&#39564;&#35777;&#26126;ASP&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#19978;&#33719;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#21644;&#22810;&#26679;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;, &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;, &#36825;&#20063;&#32473;&#35745;&#31639;&#24102;&#26469;&#20102;&#27785;&#37325;&#30340;&#36127;&#25285;, &#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#37327;&#19982;&#35757;&#32451;&#26102;&#38388;&#25104;&#27491;&#27604;&#12290;&#27492;&#22806;, &#19968;&#20010;&#33391;&#22909;&#30340;&#27169;&#22411;&#38656;&#35201;&#37325;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#32467;&#26500;&#35774;&#35745;&#21644;&#36229;&#21442;&#25968;, &#21363;&#20351;&#20351;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#31639;&#27861;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;, &#36825;&#21487;&#33021;&#20063;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#36873;&#25321;&#20195;&#29702;&#25968;&#25454;&#38598;&#26694;&#26550; (ASP), &#26088;&#22312;&#22312;&#27599;&#20010;epoch&#21160;&#24577;&#22320;&#25214;&#21040;&#20449;&#24687;&#20016;&#23500;&#30340;&#20195;&#29702;&#23376;&#38598;, &#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#24182;&#33410;&#30465;AutoML&#22788;&#29702;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;CIFAR10&#12289;CIFAR100&#12289;ImageNet16-120&#21644;ImageNet-1k&#19978;&#39564;&#35777;&#20102;ASP&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;, &#36890;&#36807;&#23545;&#19981;&#21516;&#20844;&#20849;&#27169;&#22411;&#22522;&#20934;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ASP&#21487;&#20197;&#33719;&#24471;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have gained great success due to the increasing amounts of data, and diverse effective neural network designs. However, it also brings a heavy computing burden as the amount of training data is proportional to the training time. In addition, a well-behaved model requires repeated trials of different structure designs and hyper-parameters, which may take a large amount of time even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms and neural architecture search (NAS) algorithms. In this paper, we propose an Automatic Selection of Proxy dataset framework (ASP) aimed to dynamically find the informative proxy subsets of training data at each epoch, reducing the training data size as well as saving the AutoML processing time. We verify the effectiveness and generalization of ASP on CIFAR10, CIFAR100, ImageNet16-120, and ImageNet-1k, across various public model benchmarks. The experiment results show that ASP can obtain better results than other 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#29992;&#20110;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#65292;&#37319;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#21644;&#19968;&#31181;&#26032;&#30340;&#21452;&#25439;&#22833;&#20989;&#25968;&#12290;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#19981;&#21516;&#31995;&#32479;&#24182;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#23545;&#21508;&#31181;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11477</link><description>&lt;p&gt;
Robust-MBFD&#65306;&#20351;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#21644;&#19968;&#31181;&#26032;&#30340;&#21452;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#30340;&#31283;&#20581;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Robust-MBFD: A Robust Deep Learning System for Motor Bearing Faults Detection Using Multiple Deep Learning Training Strategies and A Novel Double Loss Function. (arXiv:2310.11477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#29992;&#20110;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#65292;&#37319;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#21644;&#19968;&#31181;&#26032;&#30340;&#21452;&#25439;&#22833;&#20989;&#25968;&#12290;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#19981;&#21516;&#31995;&#32479;&#24182;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#23545;&#21508;&#31181;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#65288;MBFD&#65289;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25391;&#21160;&#20449;&#21495;&#35782;&#21035;&#30005;&#26426;&#36724;&#25215;&#30340;&#25925;&#38556;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;MBFD&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;MBFD&#31995;&#32479;&#65292;&#20998;&#21035;&#25506;&#32034;&#20102;&#30417;&#30563;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#36825;&#19977;&#31181;&#35757;&#32451;&#31574;&#30053;&#12290;&#23545;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#24182;&#25214;&#20986;&#20102;&#36866;&#29992;&#20110;MBFD&#20219;&#21153;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#32654;&#22269;&#26426;&#26800;&#25925;&#38556;&#39044;&#38450;&#25216;&#26415;&#21327;&#20250;&#65288;MFPT&#65289;&#12289;&#20975;&#26031;&#35199;&#20648;&#22823;&#23398;&#36724;&#25215;&#20013;&#24515;&#65288;CWRU&#65289;&#21644;&#24085;&#24503;&#21338;&#24681;&#22823;&#23398;&#30340;&#30005;&#26426;&#39537;&#21160;&#31995;&#32479;&#36724;&#25215;&#25439;&#20260;&#29366;&#24577;&#30417;&#27979;&#31561;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive analysis of motor bearing fault detection (MBFD), which involves the task of identifying faults in a motor bearing based on its vibration. To this end, we first propose and evaluate various machine learning based systems for the MBFD task. Furthermore, we propose three deep learning based systems for the MBFD task, each of which explores one of the following training strategies: supervised learning, semi-supervised learning, and unsupervised learning. The proposed machine learning based systems and deep learning based systems are evaluated, compared, and then they are used to identify the best model for the MBFD task. We conducted extensive experiments on various benchmark datasets of motor bearing faults, including those from the American Society for Mechanical Failure Prevention Technology (MFPT), Case Western Reserve University Bearing Center (CWRU), and the Condition Monitoring of Bearing Damage in Electromechanical Drive Systems from Paderborn U
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Code Distillation&#65288;CoDist&#65289;&#30340;&#21019;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19968;&#31181;&#35821;&#35328;&#26080;&#20851;&#30340;&#20013;&#38388;&#34920;&#31034;&#20013;&#25429;&#25417;&#20195;&#30721;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#31561;&#20215;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#31243;&#24207;&#32763;&#35793;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26426;&#22120;&#32763;&#35793;&#21644;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22312;&#25968;&#25454;&#23545;&#40784;&#21644;IR&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.11476</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#31934;&#28860;&#23454;&#29616;&#31243;&#24207;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Program Translation via Code Distillation. (arXiv:2310.11476v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Code Distillation&#65288;CoDist&#65289;&#30340;&#21019;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19968;&#31181;&#35821;&#35328;&#26080;&#20851;&#30340;&#20013;&#38388;&#34920;&#31034;&#20013;&#25429;&#25417;&#20195;&#30721;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#31561;&#20215;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#31243;&#24207;&#32763;&#35793;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26426;&#22120;&#32763;&#35793;&#21644;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22312;&#25968;&#25454;&#23545;&#40784;&#21644;IR&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#29256;&#26412;&#36801;&#31227;&#21644;&#31243;&#24207;&#32763;&#35793;&#26159;&#22823;&#22411;&#20195;&#30721;&#24211;&#29983;&#21629;&#21608;&#26399;&#20013;&#37325;&#35201;&#19988;&#26114;&#36149;&#30340;&#37096;&#20998;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#32763;&#35793;&#20381;&#36182;&#20110;&#24182;&#34892;&#35821;&#26009;&#24211;&#36827;&#34892;&#30417;&#30563;&#32763;&#35793;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#23545;&#40784;&#25968;&#25454;&#65292;&#36825;&#22312;&#31243;&#24207;&#32763;&#35793;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26368;&#36817;&#30340;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#25216;&#26415;&#36890;&#36807;&#21253;&#25324;&#21453;&#21521;&#32763;&#35793;&#21644;&#20302;&#32423;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;(IR)&#31561;&#25216;&#26415;&#20811;&#26381;&#20102;&#25968;&#25454;&#38480;&#21046;&#12290;&#36825;&#20123;&#26041;&#27861;&#20998;&#21035;&#38754;&#20020;&#30528;&#20195;&#30721;&#29255;&#27573;&#23545;&#40784;&#20013;&#30340;&#22122;&#22768;&#21644;IR&#30340;&#22810;&#26679;&#24615;&#25152;&#24102;&#26469;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Code Distillation&#65288;CoDist&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19968;&#31181;&#35821;&#35328;&#26080;&#20851;&#30340;&#20013;&#38388;&#34920;&#31034;&#20013;&#25429;&#25417;&#20195;&#30721;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#31561;&#20215;&#20851;&#31995;&#12290;&#31934;&#28860;&#30340;&#20195;&#30721;&#20316;&#20026;&#20219;&#20309;&#32534;&#31243;&#35821;&#35328;&#30340;&#32763;&#35793;&#26530;&#32445;&#65292;&#22312;&#26500;&#24314;&#20013;&#36890;&#36807;&#31616;&#21333;&#24212;&#29992;&#31934;&#28860;&#32534;&#35793;&#23454;&#29616;&#23545;&#25152;&#26377;&#21487;&#29992;&#28304;&#20195;&#30721;&#30340;&#24182;&#34892;&#35821;&#26009;&#24211;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software version migration and program translation are an important and costly part of the lifecycle of large codebases. Traditional machine translation relies on parallel corpora for supervised translation, which is not feasible for program translation due to a dearth of aligned data. Recent unsupervised neural machine translation techniques have overcome data limitations by included techniques such as back translation and low level compiler intermediate representations (IR). These methods face significant challenges due to the noise in code snippet alignment and the diversity of IRs respectively. In this paper we propose a novel model called Code Distillation (CoDist) whereby we capture the semantic and structural equivalence of code in a language agnostic intermediate representation. Distilled code serves as a translation pivot for any programming language, leading by construction to parallel corpora which scale to all available source code by simply applying the distillation compil
&lt;/p&gt;</description></item><item><title>&#26412;&#31456;&#20027;&#35201;&#20171;&#32461;&#20102;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#12290;&#20854;&#20013;&#20171;&#32461;&#20102;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20197;&#21450;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.11470</link><description>&lt;p&gt;
&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Classic machine learning methods. (arXiv:2310.11470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#20027;&#35201;&#20171;&#32461;&#20102;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#12290;&#20854;&#20013;&#20171;&#32461;&#20102;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20197;&#21450;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#19968;&#31456;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20027;&#35201;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#31456;&#30340;&#22823;&#37096;&#20998;&#20869;&#23481;&#37117;&#29992;&#20110;&#20171;&#32461;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#26368;&#36817;&#37051;&#26041;&#27861;&#12289;&#32447;&#24615;&#22238;&#24402;&#21644;&#36923;&#36753;&#22238;&#24402;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#22522;&#20110;&#26641;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#36807;&#25311;&#21512;&#38382;&#39064;&#20197;&#21450;&#20811;&#26381;&#36807;&#25311;&#21512;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#27010;&#36848;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#29992;&#20110;&#32858;&#31867;&#21644;&#38477;&#32500;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this chapter, we present the main classic machine learning methods. A large part of the chapter is devoted to supervised learning techniques for classification and regression, including nearest-neighbor methods, linear and logistic regressions, support vector machines and tree-based algorithms. We also describe the problem of overfitting as well as strategies to overcome it. We finally provide a brief overview of unsupervised learning methods, namely for clustering and dimensionality reduction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25253;&#21578;&#36890;&#36807;&#25972;&#21512;&#29983;&#25104;&#30340;&#20195;&#30721;&#21644;&#27880;&#37322;&#23545;&#65292;&#25913;&#36827;&#20108;&#36827;&#21046;&#20195;&#30721;&#27880;&#37322;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11467</link><description>&lt;p&gt;
&#25552;&#21319;&#20108;&#36827;&#21046;&#20195;&#30721;&#27880;&#37322;&#36136;&#37327;&#20998;&#31867;&#65306;&#25972;&#21512;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Binary Code Comment Quality Classification: Integrating Generative AI for Improved Accuracy. (arXiv:2310.11467v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#36890;&#36807;&#25972;&#21512;&#29983;&#25104;&#30340;&#20195;&#30721;&#21644;&#27880;&#37322;&#23545;&#65292;&#25913;&#36827;&#20108;&#36827;&#21046;&#20195;&#30721;&#27880;&#37322;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#29983;&#25104;&#30340;&#20195;&#30721;&#21644;&#27880;&#37322;&#23545;&#65292;&#25913;&#36827;&#20108;&#36827;&#21046;&#20195;&#30721;&#27880;&#37322;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;9048&#23545;&#29992;C&#35821;&#35328;&#32534;&#20889;&#30340;&#20195;&#30721;&#21644;&#27880;&#37322;&#65292;&#27599;&#23545;&#37117;&#34987;&#27880;&#37322;&#20026;&#8220;&#26377;&#29992;&#8221;&#25110;&#8220;&#26080;&#29992;&#8221;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#29983;&#25104;&#20195;&#30721;&#21644;&#27880;&#37322;&#23545;&#65292;&#24182;&#23545;&#36825;&#20123;&#29983;&#25104;&#30340;&#23545;&#36827;&#34892;&#26631;&#35760;&#20197;&#25351;&#31034;&#20854;&#23454;&#29992;&#24615;&#12290;&#27492;&#39033;&#24037;&#20316;&#30340;&#25104;&#26524;&#21253;&#25324;&#20004;&#20010;&#20998;&#31867;&#27169;&#22411;&#65306;&#19968;&#20010;&#21033;&#29992;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#21478;&#19968;&#20010;&#21033;&#29992;&#26032;&#29983;&#25104;&#30340;&#20195;&#30721;&#27880;&#37322;&#23545;&#21644;&#26631;&#35760;&#30340;&#25193;&#20805;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report focuses on enhancing a binary code comment quality classification model by integrating generated code and comment pairs, to improve model accuracy. The dataset comprises 9048 pairs of code and comments written in the C programming language, each annotated as "Useful" or "Not Useful." Additionally, code and comment pairs are generated using a Large Language Model Architecture, and these generated pairs are labeled to indicate their utility. The outcome of this effort consists of two classification models: one utilizing the original dataset and another incorporating the augmented dataset with the newly generated code comment pairs and labels.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#34507;&#30333;&#36136;&#22522;&#20110;&#32467;&#26500;&#30340;&#24615;&#36136;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#27979;&#32467;&#26500;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#24182;&#23558;&#20854;&#24402;&#22240;&#20026;&#32467;&#26500;&#23884;&#20837;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.11466</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#19977;&#32500;&#22270;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#31283;&#20581;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#34507;&#30333;&#36136;&#24615;&#36136;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Protein 3D Graph Structure Learning for Robust Structure-based Protein Property Prediction. (arXiv:2310.11466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#34507;&#30333;&#36136;&#22522;&#20110;&#32467;&#26500;&#30340;&#24615;&#36136;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#27979;&#32467;&#26500;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#24182;&#23558;&#20854;&#24402;&#22240;&#20026;&#32467;&#26500;&#23884;&#20837;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#22522;&#20110;&#32467;&#26500;&#30340;&#24615;&#36136;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#29983;&#29289;&#23398;&#20219;&#21153;&#65288;&#22914;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#21644;&#20122;&#32454;&#32990;&#23450;&#20301;&#20272;&#35745;&#65289;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#23454;&#39564;&#34507;&#30333;&#36136;&#32467;&#26500;&#25968;&#25454;&#65292;&#22312;&#36825;&#20123;&#25968;&#25454;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#22833;&#36133;&#12290;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65288;&#22914;AlphaFold2&#65289;&#39044;&#27979;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20570;&#27861;&#65292;&#21363;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20165;&#20351;&#29992;&#20934;&#30830;&#39044;&#27979;&#30340;&#32467;&#26500;&#65292;&#20250;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#24615;&#26126;&#26174;&#19979;&#38477;&#12290;&#34429;&#28982;&#31867;&#20284;&#29616;&#35937;&#24050;&#32463;&#22312;&#19968;&#33324;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#20316;&#20026;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#23427;&#20204;&#23545;&#34507;&#30333;&#36136;&#24615;&#36136;&#39044;&#27979;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#32467;&#26500;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#21033;&#29992;&#39044;&#27979;&#30340;&#32467;&#26500;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#23558;&#20854;&#24402;&#22240;&#20026;&#32467;&#26500;&#23884;&#20837;&#20559;&#24046;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Protein structure-based property prediction has emerged as a promising approach for various biological tasks, such as protein function prediction and sub-cellular location estimation. The existing methods highly rely on experimental protein structure data and fail in scenarios where these data are unavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were utilized as alternatives. However, we observed that current practices, which simply employ accurately predicted structures during inference, suffer from notable degradation in prediction accuracy. While similar phenomena have been extensively studied in general fields (e.g., Computer Vision) as model robustness, their impact on protein property prediction remains unexplored. In this paper, we first investigate the reason behind the performance decrease when utilizing predicted structures, attributing it to the structure embedding bias from the perspective of structure representation learning. To study this problem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;YouTube clickbait&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#24314;&#27169;clickbait&#29616;&#35937;&#30340;&#37325;&#35201;&#20215;&#20540;&#65292;&#24182;&#19988;&#21487;&#20197;&#24320;&#21457;&#20986;&#26356;&#22797;&#26434;&#30340;&#36328;&#35821;&#35328;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11465</link><description>&lt;p&gt;
BaitBuster-Bangla:&#19968;&#20010;&#21253;&#21547;&#22810;&#29305;&#24449;&#21644;&#22810;&#27169;&#24577;&#20998;&#26512;&#30340;&#29992;&#20110;&#23391;&#21152;&#25289;&#35821;Clickbait&#26816;&#27979;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in Bangla with Multi-Feature and Multi-Modal Analysis. (arXiv:2310.11465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;YouTube clickbait&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#24314;&#27169;clickbait&#29616;&#35937;&#30340;&#37325;&#35201;&#20215;&#20540;&#65292;&#24182;&#19988;&#21487;&#20197;&#24320;&#21457;&#20986;&#26356;&#22797;&#26434;&#30340;&#36328;&#35821;&#35328;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;YouTube clickbait&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20351;&#29992;YouTube API&#21644;Python&#32593;&#32476;&#33258;&#21160;&#21270;&#26694;&#26550;&#33258;&#21160;&#25910;&#38598;&#20102;253,070&#20010;&#25968;&#25454;&#28857;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;58&#20010;&#23391;&#21152;&#25289;&#35821;YouTube&#39057;&#36947;&#30340;&#21333;&#20010;&#35270;&#39057;&#30340;18&#20010;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#20998;&#31867;&#20026;&#20803;&#25968;&#25454;&#12289;&#20027;&#35201;&#20869;&#23481;&#12289;&#21442;&#19982;&#32479;&#35745;&#21644;&#26631;&#31614;&#12290;&#23545;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#39044;&#22788;&#29702;&#65292;&#21435;&#22122;&#22768;&#12289;&#21435;&#37325;&#22797;&#21644;&#21435;&#20559;&#24046;&#65292;&#30830;&#20445;&#20102;&#26080;&#20559;&#20506;&#21644;&#21487;&#38752;&#30340;&#20998;&#26512;&#12290;&#20316;&#20026;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#19988;&#26368;&#24378;&#22823;&#30340;&#23391;&#21152;&#25289;&#35821;clickbait&#35821;&#26009;&#24211;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25968;&#25454;&#31185;&#23398;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#20182;&#20204;&#24076;&#26395;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25512;&#36827;clickbait&#29616;&#35937;&#30340;&#24314;&#27169;&#12290;&#23427;&#30340;&#22810;&#27169;&#24577;&#24615;&#36136;&#20351;&#24471;&#21487;&#20197;&#23545;clickbait&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#28085;&#30422;&#20869;&#23481;&#12289;&#29992;&#25143;&#20132;&#20114;&#21644;&#35821;&#35328;&#32500;&#24230;&#65292;&#20197;&#24320;&#21457;&#20855;&#26377;&#36328;&#35821;&#35328;&#24212;&#29992;&#30340;&#26356;&#22797;&#26434;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a large multi-modal Bangla YouTube clickbait dataset consisting of 253,070 data points collected through an automated process using the YouTube API and Python web automation frameworks. The dataset contains 18 diverse features categorized into metadata, primary content, engagement statistics, and labels for individual videos from 58 Bangla YouTube channels. A rigorous preprocessing step has been applied to denoise, deduplicate, and remove bias from the features, ensuring unbiased and reliable analysis. As the largest and most robust clickbait corpus in Bangla to date, this dataset provides significant value for natural language processing and data science researchers seeking to advance modeling of clickbait phenomena in low-resource languages. Its multi-modal nature allows for comprehensive analyses of clickbait across content, user interactions, and linguistic dimensions to develop more sophisticated detection methods with cross-linguistic applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#35270;&#35273;&#21487;&#35299;&#37322;&#24615;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#24182;&#25214;&#21040;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#24847;&#20041;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.11431</link><description>&lt;p&gt;
&#22312;&#20154;&#24037;&#21644;&#29983;&#29289;&#31070;&#32463;&#31995;&#32479;&#20013;&#35782;&#21035;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Identifying Interpretable Visual Features in Artificial and Biological Neural Systems. (arXiv:2310.11431v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#35270;&#35273;&#21487;&#35299;&#37322;&#24615;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#24182;&#25214;&#21040;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#24847;&#20041;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#36890;&#24120;&#26159;&#8220;&#21487;&#35299;&#37322;&#30340;&#8221;&#65292;&#22240;&#20026;&#23427;&#20204;&#20195;&#34920;&#20010;&#21035;&#30452;&#35266;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#31070;&#32463;&#20803;&#34920;&#29616;&#20986;&#8220;&#28151;&#21512;&#36873;&#25321;&#24615;&#8221;&#65292;&#21363;&#23427;&#20204;&#20195;&#34920;&#22810;&#20010;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#20551;&#35774;&#35748;&#20026;&#65292;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#21487;&#33021;&#20197;&#8220;&#21472;&#21152;&#8221;&#30340;&#26041;&#24335;&#34920;&#31034;&#65292;&#21363;&#30001;&#22810;&#20010;&#31070;&#32463;&#20803;&#27839;&#38750;&#27491;&#20132;&#36724;&#34920;&#31034;&#65292;&#22240;&#20026;&#33258;&#28982;&#25968;&#25454;&#20013;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#25968;&#36890;&#24120;&#22823;&#20110;&#32473;&#23450;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#20803;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24212;&#35813;&#33021;&#22815;&#22312;&#28608;&#27963;&#31354;&#38388;&#20013;&#25214;&#21040;&#19982;&#20010;&#21035;&#31070;&#32463;&#20803;&#19981;&#23545;&#40784;&#30340;&#26377;&#24847;&#20041;&#30340;&#26041;&#21521;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#35270;&#35273;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#19982;&#22823;&#37327;&#20154;&#31867;&#24515;&#29702;&#29289;&#29702;&#23398;&#23545;&#31070;&#32463;&#20803;&#21487;&#35299;&#37322;&#24615;&#30340;&#21028;&#26029;&#36827;&#34892;&#39564;&#35777;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#31181;&#22312;&#32593;&#32476;&#28608;&#27963;&#31354;&#38388;&#20013;&#23547;&#25214;&#26377;&#24847;&#20041;&#26041;&#21521;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#26469;&#21457;&#29616;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single neurons in neural networks are often ``interpretable'' in that they represent individual, intuitively meaningful features. However, many neurons exhibit $\textit{mixed selectivity}$, i.e., they represent multiple unrelated features. A recent hypothesis proposes that features in deep networks may be represented in $\textit{superposition}$, i.e., on non-orthogonal axes by multiple neurons, since the number of possible interpretable features in natural data is generally larger than the number of neurons in a given network. Accordingly, we should be able to find meaningful directions in activation space that are not aligned with individual neurons. Here, we propose (1) an automated method for quantifying visual interpretability that is validated against a large database of human psychophysics judgments of neuron interpretability, and (2) an approach for finding meaningful directions in network activation space. We leverage these methods to discover directions in convolutional neural
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#20195;&#29702;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#24046;&#36317;&#12290;&#36825;&#20010;&#24046;&#36317;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#33021;&#21542;&#36866;&#24403;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.11211</link><description>&lt;p&gt;
&#29702;&#35299;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Understanding Fairness Surrogate Functions in Algorithmic Fairness. (arXiv:2310.11211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#20195;&#29702;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#24046;&#36317;&#12290;&#36825;&#20010;&#24046;&#36317;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#33021;&#21542;&#36866;&#24403;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#35266;&#23519;&#21040;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26576;&#20123;&#20154;&#32676;&#20135;&#29983;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#20559;&#35265;&#24182;&#23454;&#29616;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#65292;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#24341;&#20837;&#28041;&#21450;&#20844;&#24179;&#24615;&#23450;&#20041;&#30340;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#65292;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#36825;&#31181;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#28145;&#20837;&#29702;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20197;&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#8212;&#8212;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#8212;&#8212;&#20026;&#20363;&#65292;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#20844;&#24179;&#24615;&#23450;&#20041;&#21644;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#20195;&#29702;-&#20844;&#24179;&#24615;&#24046;&#36317;&#12290;&#36825;&#20010;"&#24046;&#36317;"&#30452;&#25509;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#26159;&#21542;&#36866;&#21512;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#20851;&#20110;&#36825;&#20010;"&#24046;&#36317;"&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#28608;&#21457;&#20102;&#25105;&#20204;&#30340;&#20852;&#36259;&#65292;&#34920;&#26126;&#26080;&#38480;&#21046;&#30340;&#20195;&#29702;&#20989;&#25968;&#23558;&#21463;&#21040;&#20915;&#31574;&#36793;&#30028;&#36828;&#31163;&#30340;&#28857;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been observed that machine learning algorithms exhibit biased predictions against certain population groups. To mitigate such bias while achieving comparable accuracy, a promising approach is to introduce surrogate functions of the concerned fairness definition and solve a constrained optimization problem. However, an intriguing issue in previous work is that such fairness surrogate functions may yield unfair results. In this work, in order to deeply understand this issue, taking a widely used fairness definition, demographic parity as an example, we both theoretically and empirically show that there is a surrogate-fairness gap between the fairness definition and the fairness surrogate function. The "gap" directly determines whether a surrogate function is an appropriate substitute for a fairness definition. Also, the theoretical analysis and experimental results about the "gap" motivate us that the unbounded surrogate functions will be affected by the points far from the decisi
&lt;/p&gt;</description></item><item><title>HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.11102</link><description>&lt;p&gt;
HGCVAE: &#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning. (arXiv:2310.11102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11102
&lt;/p&gt;
&lt;p&gt;
HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#29983;&#25104;&#24335;SSL&#22312;&#24322;&#26500;&#22270;&#23398;&#20064;&#65288;HGL&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#20851;&#20110;&#24322;&#26500;&#22270;&#30340;SSL&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#65292;&#38656;&#35201;&#35774;&#35745;&#22797;&#26434;&#30340;&#35270;&#22270;&#26469;&#25429;&#25417;&#24322;&#36136;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;SSL&#26041;&#27861;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#35299;&#20915;HGL&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HGCVAE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#20351;HGL&#25670;&#33073;&#20102;&#22797;&#26434;&#24322;&#36136;&#24615;&#30340;&#36127;&#25285;&#12290;HGCVAE&#19981;&#20877;&#19987;&#27880;&#20110;&#22797;&#26434;&#30340;&#24322;&#36136;&#24615;&#65292;&#32780;&#26159;&#20805;&#20998;&#21033;&#29992;&#20102;&#29983;&#25104;&#24335;SSL&#30340;&#28508;&#21147;&#12290;HGCVAE&#21019;&#26032;&#22320;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#29983;&#25104;&#24335;SSL&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#20960;&#20010;&#20851;&#38190;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#28176;&#36827;&#26426;&#21046;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;hard&#26679;&#26412;&#65292;
&lt;/p&gt;
&lt;p&gt;
Generative self-supervised learning (SSL) has exhibited significant potential and garnered increasing interest in graph learning. In this study, we aim to explore the problem of generative SSL in the context of heterogeneous graph learning (HGL). The previous SSL approaches for heterogeneous graphs have primarily relied on contrastive learning, necessitating the design of complex views to capture heterogeneity. However, existing generative SSL methods have not fully leveraged the capabilities of generative models to address the challenges of HGL. In this paper, we present HGCVAE, a novel contrastive variational graph auto-encoder that liberates HGL from the burden of intricate heterogeneity capturing. Instead of focusing on complicated heterogeneity, HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#26368;&#20248;&#25511;&#21046; (OC)&#21644;&#24378;&#21270;&#23398;&#20064; (RL)&#26041;&#27861;&#22312;&#33258;&#20027;&#26080;&#20154;&#26426;&#36187;&#36710;&#20013;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#30452;&#25509;&#20248;&#21270;&#20219;&#21153;&#23618;&#38754;&#30340;&#30446;&#26631;&#65292;&#24182;&#21033;&#29992;&#39046;&#22495;&#30340;&#38543;&#26426;&#22240;&#32032;&#65292;&#32780;&#26368;&#20248;&#25511;&#21046;&#30340;&#20998;&#35299;&#38480;&#21046;&#20102;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2310.10943</link><description>&lt;p&gt;
&#22312;&#33258;&#20027;&#36187;&#36710;&#20013;&#36798;&#21040;&#26497;&#38480;: &#26368;&#20248;&#25511;&#21046;&#19982;&#24378;&#21270;&#23398;&#20064;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Reaching the Limit in Autonomous Racing: Optimal Control versus Reinforcement Learning. (arXiv:2310.10943v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10943
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#26368;&#20248;&#25511;&#21046; (OC)&#21644;&#24378;&#21270;&#23398;&#20064; (RL)&#26041;&#27861;&#22312;&#33258;&#20027;&#26080;&#20154;&#26426;&#36187;&#36710;&#20013;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#30452;&#25509;&#20248;&#21270;&#20219;&#21153;&#23618;&#38754;&#30340;&#30446;&#26631;&#65292;&#24182;&#21033;&#29992;&#39046;&#22495;&#30340;&#38543;&#26426;&#22240;&#32032;&#65292;&#32780;&#26368;&#20248;&#25511;&#21046;&#30340;&#20998;&#35299;&#38480;&#21046;&#20102;&#25511;&#21046;&#22120;&#30340;&#34892;&#20026;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20013;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#20026;&#25935;&#25463;&#31227;&#21160;&#26426;&#22120;&#20154;&#35774;&#35745;&#25511;&#21046;&#31995;&#32479;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#26159;&#33258;&#20027;&#26080;&#20154;&#26426;&#36187;&#36710;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;(RL)&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#32988;&#36807;&#26368;&#20248;&#25511;&#21046;(OC)&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#35843;&#26597;&#20102;&#21738;&#20123;&#22522;&#26412;&#22240;&#32032;&#23545;RL&#30340;&#25104;&#21151;&#25110;OC&#30340;&#38480;&#21046;&#26377;&#25152;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;RL&#30456;&#23545;&#20110;OC&#30340;&#26681;&#26412;&#20248;&#21183;&#19981;&#22312;&#20110;&#20248;&#21270;&#30446;&#26631;&#30340;&#25928;&#26524;&#26356;&#22909;&#65292;&#32780;&#26159;&#22312;&#20110;&#23427;&#20248;&#21270;&#20102;&#19968;&#20010;&#26356;&#22909;&#30340;&#30446;&#26631;&#12290;OC&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#35268;&#21010;&#21644;&#25511;&#21046;&#65292;&#20351;&#29992;&#19968;&#20010;&#26126;&#30830;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#22914;&#36712;&#36857;&#65292;&#20316;&#20026;&#25509;&#21475;&#12290;&#36825;&#31181;&#20998;&#35299;&#38480;&#21046;&#20102;&#25511;&#21046;&#22120;&#21487;&#20197;&#34920;&#36798;&#30340;&#34892;&#20026;&#33539;&#22260;&#65292;&#24403;&#38754;&#20020;&#26410;&#24314;&#27169;&#30340;&#24433;&#21709;&#26102;&#65292;&#23548;&#33268;&#25511;&#21046;&#24615;&#33021;&#36739;&#24046;&#12290;&#30456;&#21453;&#65292;RL&#21487;&#20197;&#30452;&#25509;&#20248;&#21270;&#20219;&#21153;&#23618;&#38754;&#30340;&#30446;&#26631;&#65292;&#24182;&#19988;&#21487;&#20197;&#21033;&#29992;&#39046;&#22495;&#38543;&#26426;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central question in robotics is how to design a control system for an agile mobile robot. This paper studies this question systematically, focusing on a challenging setting: autonomous drone racing. We show that a neural network controller trained with reinforcement learning (RL) outperformed optimal control (OC) methods in this setting. We then investigated which fundamental factors have contributed to the success of RL or have limited OC. Our study indicates that the fundamental advantage of RL over OC is not that it optimizes its objective better but that it optimizes a better objective. OC decomposes the problem into planning and control with an explicit intermediate representation, such as a trajectory, that serves as an interface. This decomposition limits the range of behaviors that can be expressed by the controller, leading to inferior control performance when facing unmodeled effects. In contrast, RL can directly optimize a task-level objective and can leverage domain rando
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#19981;&#36830;&#32493;&#20989;&#25968;&#30340;&#26367;&#20195;&#20027;&#21160;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#27963;&#36291;&#23376;&#31354;&#38388;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10907</link><description>&lt;p&gt;
&#38024;&#23545;&#36339;&#36291;&#19981;&#36830;&#32493;&#20989;&#25968;&#30340;&#26367;&#20195;&#20027;&#21160;&#23376;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Surrogate Active Subspaces for Jump-Discontinuous Functions. (arXiv:2310.10907v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10907
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#19981;&#36830;&#32493;&#20989;&#25968;&#30340;&#26367;&#20195;&#20027;&#21160;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#27963;&#36291;&#23376;&#31354;&#38388;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26367;&#20195;&#24314;&#27169;&#21644;&#27963;&#36291;&#23376;&#31354;&#38388;&#24050;&#32463;&#25104;&#20026;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#24378;&#22823;&#33539;&#20363;&#12290;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#31163;&#25955;&#36755;&#20986;&#30340;Agent-Based&#27169;&#22411;&#31561;&#19981;&#36830;&#32493;&#27169;&#25311;&#22120;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#24212;&#29992;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#23545;&#20110;&#36825;&#31867;&#20272;&#35745;&#22120;&#65292;&#26367;&#20195;&#35745;&#31639;&#30340;&#27963;&#36291;&#23376;&#31354;&#38388;&#21487;&#20197;&#20135;&#29983;&#26377;&#36259;&#30340;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#27963;&#36291;&#23376;&#31354;&#38388;&#26159;&#36890;&#36807;&#26799;&#24230;&#23450;&#20041;&#30340;&#65292;&#24403;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#36830;&#32493;&#27169;&#25311;&#22120;&#26102;&#65292;&#20272;&#35745;&#30340;&#26159;&#20160;&#20040;&#37327;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#36827;&#34892;&#27492;&#31867;&#20998;&#26512;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#19968;&#20123;&#30149;&#24577;&#24773;&#20917;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#23558;&#27963;&#36291;&#23376;&#31354;&#38388;&#25193;&#23637;&#21040;&#19981;&#36830;&#32493;&#20989;&#25968;&#19978;&#65292;&#28548;&#28165;&#20102;&#22312;&#27492;&#31867;&#20998;&#26512;&#20013;&#23454;&#38469;&#20272;&#35745;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#36824;&#23545;&#21512;&#25104;&#27979;&#35797;&#20989;&#25968;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#27963;&#36291;&#23376;&#31354;&#38388;&#30340;&#39640;&#26031;&#36807;&#31243;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surrogate modeling and active subspaces have emerged as powerful paradigms in computational science and engineering. Porting such techniques to computational models in the social sciences brings into sharp relief their limitations in dealing with discontinuous simulators, such as Agent-Based Models, which have discrete outputs. Nevertheless, prior applied work has shown that surrogate estimates of active subspaces for such estimators can yield interesting results. But given that active subspaces are defined by way of gradients, it is not clear what quantity is being estimated when this methodology is applied to a discontinuous simulator. We begin this article by showing some pathologies that can arise when conducting such an analysis. This motivates an extension of active subspaces to discontinuous functions, clarifying what is actually being estimated in such analyses. We also conduct numerical experiments on synthetic test functions to compare Gaussian process estimates of active sub
&lt;/p&gt;</description></item><item><title>&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#25581;&#24320;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31070;&#31192;&#38754;&#32433;&#65292;&#36890;&#36807;&#35780;&#20272;&#20219;&#20309;&#21253;&#21547;&#24658;&#23450;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#30830;&#23450;&#20102;&#21518;&#38376;&#25915;&#20987;&#25104;&#21151;&#30340;&#20915;&#23450;&#22240;&#32032;&#12289;&#26368;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#21521;&#20197;&#21450;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#20154;&#31867;&#35302;&#21457;&#22120;&#20309;&#26102;&#20250;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2310.10780</link><description>&lt;p&gt;
&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#25581;&#24320;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31070;&#31192;&#38754;&#32433;
&lt;/p&gt;
&lt;p&gt;
Demystifying Poisoning Backdoor Attacks from a Statistical Perspective. (arXiv:2310.10780v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10780
&lt;/p&gt;
&lt;p&gt;
&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#25581;&#24320;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31070;&#31192;&#38754;&#32433;&#65292;&#36890;&#36807;&#35780;&#20272;&#20219;&#20309;&#21253;&#21547;&#24658;&#23450;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#30830;&#23450;&#20102;&#21518;&#38376;&#25915;&#20987;&#25104;&#21151;&#30340;&#20915;&#23450;&#22240;&#32032;&#12289;&#26368;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#21521;&#20197;&#21450;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#20154;&#31867;&#35302;&#21457;&#22120;&#20309;&#26102;&#20250;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#20381;&#36182;&#26085;&#30410;&#22686;&#38271;&#65292;&#24378;&#35843;&#20102;&#29702;&#35299;&#21644;&#30830;&#20445;&#20854;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30001;&#20110;&#20854;&#38544;&#34109;&#24615;&#21644;&#28508;&#22312;&#30340;&#20005;&#37325;&#21518;&#26524;&#32780;&#26500;&#25104;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#36825;&#31867;&#25915;&#20987;&#28041;&#21450;&#23558;&#35302;&#21457;&#22120;&#23884;&#20837;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20197;&#22312;&#23384;&#22312;&#27963;&#21160;&#35302;&#21457;&#22120;&#26102;&#24341;&#36215;&#24694;&#24847;&#34892;&#20026;&#65292;&#21516;&#26102;&#22312;&#27809;&#26377;&#35302;&#21457;&#22120;&#30340;&#24773;&#20917;&#19979;&#32500;&#25345;&#27491;&#24120;&#21151;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#20026;&#21463;&#25439;&#27169;&#22411;&#22312;&#28165;&#27905;&#21644;&#21518;&#38376;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#24314;&#31435;&#20005;&#26684;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#65292;&#35780;&#20272;&#20102;&#20219;&#20309;&#21253;&#21547;&#24658;&#23450;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;&#25152;&#24320;&#21457;&#30340;&#29702;&#35770;&#22238;&#31572;&#20102;&#19968;&#31995;&#21015;&#22522;&#26412;&#20294;&#20197;&#21069;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#65288;1&#65289;&#21518;&#38376;&#25915;&#20987;&#25104;&#21151;&#30340;&#20915;&#23450;&#22240;&#32032;&#26159;&#20160;&#20040;&#65292;&#65288;2&#65289;&#26368;&#26377;&#25928;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#21521;&#26159;&#20160;&#20040;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#20154;&#31867;&#35302;&#21457;&#22120;&#20309;&#26102;&#20250;&#25104;&#21151;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#29702;&#35299;...
&lt;/p&gt;
&lt;p&gt;
The growing dependence on machine learning in real-world applications emphasizes the importance of understanding and ensuring its safety. Backdoor attacks pose a significant security risk due to their stealthy nature and potentially serious consequences. Such attacks involve embedding triggers within a learning model with the intention of causing malicious behavior when an active trigger is present while maintaining regular functionality without it. This paper evaluates the effectiveness of any backdoor attack incorporating a constant trigger, by establishing tight lower and upper boundaries for the performance of the compromised model on both clean and backdoor test data. The developed theory answers a series of fundamental but previously underexplored problems, including (1) what are the determining factors for a backdoor attack's success, (2) what is the direction of the most effective backdoor attack, and (3) when will a human-imperceptible trigger succeed. Our derived understandin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;Microscaling&#65288;MX&#65289;&#25968;&#25454;&#26684;&#24335;&#22312;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;MX&#25968;&#25454;&#26684;&#24335;&#21487;&#20197;&#20316;&#20026;&#22522;&#32447;FP32&#30340;&#26367;&#20195;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#29992;&#25143;&#25705;&#25830;&#65292;&#24182;&#19988;&#25104;&#21151;&#22312;&#36229;&#36807;&#20004;&#25171;&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23567;&#20110;8&#20301;&#30340;&#25968;&#25454;&#26684;&#24335;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.10537</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#25193;&#23637;&#25968;&#25454;&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
Microscaling Data Formats for Deep Learning. (arXiv:2310.10537v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;Microscaling&#65288;MX&#65289;&#25968;&#25454;&#26684;&#24335;&#22312;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;MX&#25968;&#25454;&#26684;&#24335;&#21487;&#20197;&#20316;&#20026;&#22522;&#32447;FP32&#30340;&#26367;&#20195;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#29992;&#25143;&#25705;&#25830;&#65292;&#24182;&#19988;&#25104;&#21151;&#22312;&#36229;&#36807;&#20004;&#25171;&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23567;&#20110;8&#20301;&#30340;&#25968;&#25454;&#26684;&#24335;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31364;&#20301;&#23485;&#25968;&#25454;&#26684;&#24335;&#23545;&#20110;&#38477;&#20302;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#23558;&#27599;&#20010;&#22359;&#30340;&#32553;&#25918;&#22240;&#23376;&#19982;&#31364;&#28014;&#28857;&#21644;&#25972;&#25968;&#31867;&#22411;&#30456;&#32467;&#21512;&#30340;&#24494;&#25193;&#23637;&#65288;MX&#65289;&#25968;&#25454;&#26684;&#24335;&#65292;&#20197;&#28385;&#36275;&#30828;&#20214;&#25928;&#29575;&#12289;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#29992;&#25143;&#25705;&#25830;&#20043;&#38388;&#30340;&#31454;&#20105;&#38656;&#27714;&#12290;&#23545;&#20110;AI&#25512;&#29702;&#21644;&#35757;&#32451;&#65292;MX&#25968;&#25454;&#26684;&#24335;&#22312;&#36229;&#36807;&#20004;&#25171;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#20316;&#20026;&#22522;&#32447;FP32&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;&#26102;&#29992;&#25143;&#25705;&#25830;&#23567;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26368;&#23567;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#21644;&#26080;&#38656;&#20462;&#25913;&#35757;&#32451;&#37197;&#26041;&#30340;&#24773;&#20917;&#19979;&#65292;&#39318;&#27425;&#35757;&#32451;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#23567;&#20110;8&#20301;&#30340;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;&#28176;&#21464;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Narrow bit-width data formats are key to reducing the computational and storage costs of modern deep learning applications. This paper evaluates Microscaling (MX) data formats that combine a per-block scaling factor with narrow floating-point and integer types for individual elements. MX formats balance the competing needs of hardware efficiency, model accuracy, and user friction. Empirical results on over two dozen benchmarks demonstrate practicality of MX data formats as a drop-in replacement for baseline FP32 for AI inference and training with low user friction. We also show the first instance of training generative language models at sub-8-bit weights, activations, and gradients with minimal accuracy loss and no modifications to the training recipe.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ParsingDST&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#35299;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.10520</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#35299;&#26512;&#65292;&#29992;&#20110;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking. (arXiv:2310.10520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ParsingDST&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#35299;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#35299;&#20915;&#20102;&#33719;&#21462;&#21644;&#27880;&#37322;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#36153;&#21147;&#12290;&#28982;&#32780;&#65292;DST&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#22635;&#27133;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#26356;&#26032;&#31574;&#30053;&#26469;&#36319;&#36394;&#23545;&#35805;&#29366;&#24577;&#38543;&#30528;&#23545;&#35805;&#30340;&#36827;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ParsingDST&#65292;&#19968;&#31181;&#26032;&#30340;In-Context Learning&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#20197;&#24341;&#20837;&#39069;&#22806;&#30340;&#22797;&#26434;&#26356;&#26032;&#31574;&#30053;&#29992;&#20110;&#38646;&#26679;&#26412;DST&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24182;&#36890;&#36807;&#35821;&#20041;&#35299;&#26512;&#23558;&#21407;&#22987;&#23545;&#35805;&#25991;&#26412;&#36716;&#25442;&#20026;JSON&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#29366;&#24577;&#26469;&#37325;&#26032;&#23450;&#20041;DST&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#26356;&#22810;&#30340;&#27169;&#22359;&#26469;&#30830;&#20445;&#25991;&#26412;&#21040;JSON&#36807;&#31243;&#20013;&#26356;&#26032;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MultiWOZ&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;DST&#26041;&#27861;&#65292;&#22312;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#65288;JGA&#65289;&#21644;&#27133;&#20934;&#30830;&#24230;&#26041;&#38754;&#19982;&#29616;&#26377;&#30340;ICL&#26041;&#27861;&#30456;&#27604;&#21576;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;MPT-7b-instruct, Falcon-7b-instruct&#21644;OpenAI Chat-GPT&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#36827;&#34892;&#20102;&#25991;&#26412;&#25688;&#35201;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;text-davinci-003&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24615;&#33021;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.10449</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#25688;&#35201;: MPT-7b-instruct&#12289;Falcon-7b-instruct&#21644;OpenAI Chat-GPT&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models. (arXiv:2310.10449v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;MPT-7b-instruct, Falcon-7b-instruct&#21644;OpenAI Chat-GPT&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#36827;&#34892;&#20102;&#25991;&#26412;&#25688;&#35201;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;text-davinci-003&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24615;&#33021;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24212;&#29992;&#33539;&#22260;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#21644;&#20869;&#23481;&#29983;&#25104;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21319;&#25688;&#35201;&#25216;&#26415;&#26041;&#38754;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20351;&#29992;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21253;&#25324;MPT-7b-instruct&#65292;falcon-7b-instruct&#21644;OpenAI ChatGPT text-davinci-003&#27169;&#22411;&#65289;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#30340;&#25506;&#32034;&#12290;&#23454;&#39564;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#20351;&#29992;&#35832;&#22914;&#21452;&#35821;&#35780;&#20272;&#34913;&#37327;&#65288;BLEU&#65289;&#20998;&#25968;&#65292;&#38754;&#21521;&#22238;&#24518;&#30340;&#35270;&#35282;&#35780;&#20272;&#65288;ROUGE&#65289;&#20998;&#25968;&#21644;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#22120;&#65288;BERT&#65289;&#20998;&#25968;&#31561;&#24191;&#27867;&#25509;&#21463;&#30340;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#30340;&#25688;&#35201;&#12290;&#26681;&#25454;&#23454;&#39564;&#65292;text-davinci-003&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#26412;&#27425;&#30740;&#31350;&#28041;&#21450;CNN Daily Mail&#21644;XSum&#36825;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#30446;&#26631;&#26159;&#20840;&#38754;&#20102;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text summarization is a critical Natural Language Processing (NLP) task with applications ranging from information retrieval to content generation. Leveraging Large Language Models (LLMs) has shown remarkable promise in enhancing summarization techniques. This paper embarks on an exploration of text summarization with a diverse set of LLMs, including MPT-7b-instruct, falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment was performed with different hyperparameters and evaluated the generated summaries using widely accepted metrics such as the Bilingual Evaluation Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score, and Bidirectional Encoder Representations from Transformers (BERT) Score. According to the experiment, text-davinci-003 outperformed the others. This investigation involved two distinct datasets: CNN Daily Mail and XSum. Its primary objective was to provide a comprehensive understanding of the performance of Large
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10378</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26174;&#31034;&#23384;&#20648;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#30830;&#20445;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#30340;&#29992;&#25143;&#20174;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#33719;&#24471;&#19968;&#33268;&#30340;&#21453;&#39304;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#22810;&#35821;&#35328;PLM&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65288;CLC&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#19968;&#33268;&#24615;&#65288;RankC&#65289;&#24230;&#37327;&#65292;&#29992;&#20110;&#29420;&#31435;&#20110;&#20934;&#30830;&#24615;&#35780;&#20272;&#36328;&#35821;&#35328;&#38388;&#30340;&#30693;&#35782;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20915;&#23450;CLC&#30340;&#22240;&#32032;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#21253;&#25324;&#27169;&#22411;&#23618;&#38754;&#21644;&#35821;&#35328;&#23545;&#23618;&#38754;&#12290;&#22312;&#20854;&#20182;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#22823;&#22810;&#25968;&#35821;&#35328;&#20013;&#30340;&#20107;&#23454;&#25506;&#27979;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#33021;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#32534;&#36753;&#22312;PLMs&#20013;&#25554;&#20837;&#26032;&#30340;&#20107;&#23454;&#20851;&#32852;&#36827;&#34892;&#20102;&#19968;&#20010;CLC&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#23545;&#19968;&#23567;&#37096;&#20998;&#20107;&#23454;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21644;&#21033;&#29992;&#19981;&#21516;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20013;&#22269;&#32472;&#30011;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22810;&#31181;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#38754;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09978</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20013;&#22269;&#32472;&#30011;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Chinese Painting Style Transfer Using Deep Generative Models. (arXiv:2310.09978v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21644;&#21033;&#29992;&#19981;&#21516;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20013;&#22269;&#32472;&#30011;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22810;&#31181;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#38754;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#39118;&#26684;&#36716;&#25442;&#26088;&#22312;&#22312;&#20445;&#30041;&#22270;&#20687;&#20869;&#23481;&#30340;&#21516;&#26102;&#20462;&#25913;&#20854;&#39118;&#26684;&#12290;&#33258;2015&#24180;&#20197;&#26469;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39118;&#26684;&#36716;&#25442;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#22823;&#22810;&#25968;&#24212;&#29992;&#37117;&#38598;&#20013;&#22312;&#20687;&#26805;&#39640;&#12289;&#33707;&#22856;&#12289;&#22622;&#23578;&#36825;&#26679;&#30340;&#29305;&#23450;&#33402;&#26415;&#23478;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#20256;&#32479;&#20013;&#22269;&#32472;&#30011;&#39118;&#26684;&#36716;&#25442;&#26041;&#38754;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#36739;&#23569;&#12290;&#26412;&#25991;&#23558;&#30740;&#31350;&#21644;&#21033;&#29992;&#19981;&#21516;&#30340;&#26368;&#26032;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20013;&#22269;&#32472;&#30011;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#38754;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#20960;&#31181;&#39118;&#26684;&#36716;&#25442;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25226;&#20256;&#32479;&#20013;&#22269;&#32472;&#30011;&#30340;&#20004;&#31181;&#20027;&#35201;&#39118;&#26684;&#65292;&#21363;&#8220;&#24037;&#31508;&#8221;&#21644;&#8220;&#27700;&#22696;&#8221;&#65292;&#24212;&#29992;&#21040;&#29616;&#20195;&#22270;&#20687;&#20013;&#65292;&#22914;&#33258;&#28982;&#23545;&#35937;&#12289;&#32918;&#20687;&#21644;&#39118;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artistic style transfer aims to modify the style of the image while preserving its content. Style transfer using deep learning models has been widely studied since 2015, and most of the applications are focused on specific artists like Van Gogh, Monet, Cezanne. There are few researches and applications on traditional Chinese painting style transfer. In this paper, we will study and leverage different state-of-the-art deep generative models for Chinese painting style transfer and evaluate the performance both qualitatively and quantitatively. In addition, we propose our own algorithm that combines several style transfer models for our task. Specifically, we will transfer two main types of traditional Chinese painting style, known as "Gong-bi" and "Shui-mo" (to modern images like nature objects, portraits and landscapes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#32929;&#24066;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#30340;&#25216;&#26415;&#25351;&#26631;&#32452;&#21512;&#26469;&#23454;&#29616;&#26368;&#23569;&#35823;&#24046;&#30340;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#21253;&#35013;&#22120;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.09903</link><description>&lt;p&gt;
&#35780;&#20272;&#29305;&#24449;&#36873;&#25321;&#22312;&#32929;&#24066;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#20197;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#25216;&#26415;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Evaluation of feature selection performance for identification of best effective technical indicators on stock market price prediction. (arXiv:2310.09903v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#32929;&#24066;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#30340;&#25216;&#26415;&#25351;&#26631;&#32452;&#21512;&#26469;&#23454;&#29616;&#26368;&#23569;&#35823;&#24046;&#30340;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#21253;&#35013;&#22120;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25216;&#26415;&#25351;&#26631;&#23545;&#32929;&#24066;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#29305;&#24449;&#36873;&#25321;&#23545;&#36873;&#25321;&#26368;&#20339;&#25351;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#31181;&#32771;&#34385;&#22312;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20013;&#27169;&#22411;&#24615;&#33021;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#26159;&#21253;&#35013;&#22120;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#37492;&#23450;&#20986;&#26368;&#23569;&#35823;&#24046;&#30340;&#39044;&#27979;&#32929;&#24066;&#20215;&#26684;&#30340;&#26368;&#20339;&#32929;&#24066;&#25351;&#26631;&#32452;&#21512;&#12290;&#20026;&#35780;&#20272;&#21253;&#35013;&#22120;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#23545;&#32929;&#24066;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#26412;&#25991;&#22312;&#36807;&#21435;10&#24180;&#33529;&#26524;&#20844;&#21496;&#30340;&#25968;&#25454;&#19978;&#20351;&#29992;&#20102;10&#20010;&#35780;&#20272;&#22120;&#21644;123&#20010;&#25216;&#26415;&#25351;&#26631;&#36827;&#34892;&#20102;SFS&#21644;SBS&#30340;&#32771;&#23519;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#23558;&#30001;3&#22825;&#26102;&#38388;&#31383;&#21475;&#21019;&#24314;&#30340;&#25968;&#25454;&#36716;&#21270;&#20026;&#36866;&#29992;&#20110;&#22238;&#24402;&#26041;&#27861;&#30340;&#36755;&#20837;&#12290;&#20174;&#35266;&#23519;&#32467;&#26524;&#21487;&#20197;&#24471;&#20986;&#65306;&#65288;1&#65289;&#27599;&#31181;&#21253;&#35013;&#22120;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#27599;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#20063;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the influence of many factors, including technical indicators on stock market prediction, feature selection is important to choose the best indicators. One of the feature selection methods that consider the performance of models during feature selection is the wrapper feature selection method. The aim of this research is to identify a combination of the best stock market indicators through feature selection to predict the stock market price with the least error. In order to evaluate the impact of wrapper feature selection techniques on stock market prediction, in this paper SFS and SBS with 10 estimators and 123 technical indicators have been examined on the last 10 years of Apple Company. Also, by the proposed method, the data created by the 3-day time window were converted to the appropriate input for regression methods. Based on the results observed: (1) Each wrapper feature selection method has different results with different machine learning methods, and each method is mor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Edge-InversionNet&#65292;&#36890;&#36807;&#37319;&#29992;&#32467;&#26500;&#21270;&#20462;&#21098;&#31639;&#27861;&#24471;&#21040;&#20102;InversionNet&#30340;&#36731;&#37327;&#21270;&#29256;&#26412;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20462;&#21098;&#21518;&#30340;InversionNet&#22312;&#24615;&#33021;&#30053;&#26377;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;98.2%&#30340;&#35745;&#31639;&#36164;&#28304;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2310.09667</link><description>&lt;p&gt;
Edge-InversionNet&#65306;&#20351;InversionNet&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Edge-InversionNet: Enabling Efficient Inference of InversionNet on Edge Devices. (arXiv:2310.09667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Edge-InversionNet&#65292;&#36890;&#36807;&#37319;&#29992;&#32467;&#26500;&#21270;&#20462;&#21098;&#31639;&#27861;&#24471;&#21040;&#20102;InversionNet&#30340;&#36731;&#37327;&#21270;&#29256;&#26412;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20462;&#21098;&#21518;&#30340;InversionNet&#22312;&#24615;&#33021;&#30053;&#26377;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;98.2%&#30340;&#35745;&#31639;&#36164;&#28304;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#38663;&#20840;&#27874;&#24418;&#21453;&#28436;(FWI)&#26159;&#22320;&#29699;&#29289;&#29702;&#23398;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#22320;&#38663;&#25968;&#25454;&#20013;&#25512;&#26029;&#22320;&#19979;&#32467;&#26500;&#12290;&#32780;InversionNet&#26159;&#26368;&#25104;&#21151;&#30340;&#25968;&#25454;&#39537;&#21160;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#65292;&#24212;&#29992;&#20110;&#22320;&#38663;FWI&#12290;&#28982;&#32780;&#65292;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;InversionNet&#38590;&#20197;&#26377;&#25928;&#37096;&#32626;&#21040;&#36890;&#24120;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#32467;&#26500;&#21270;&#20462;&#21098;&#31639;&#27861;&#33719;&#24471;InversionNet&#30340;&#36731;&#37327;&#21270;&#29256;&#26412;&#65292;&#20197;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#39640;&#25928;&#25512;&#29702;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#26641;&#33683;&#27966;&#21046;&#20316;&#20102;&#19968;&#20010;&#36816;&#34892;&#36731;&#37327;&#21270;InversionNet&#30340;&#21407;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#21098;&#21518;&#30340;InversionNet&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;98.2%&#30340;&#35745;&#31639;&#36164;&#28304;&#20943;&#23569;&#65292;&#32780;&#27169;&#22411;&#24615;&#33021;&#30053;&#26377;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seismic full waveform inversion (FWI) is a widely used technique in geophysics for inferring subsurface structures from seismic data. And InversionNet is one of the most successful data-driven machine learning models that is applied to seismic FWI. However, the high computing costs to run InversionNet have made it challenging to be efficiently deployed on edge devices that are usually resource-constrained. Therefore, we propose to employ the structured pruning algorithm to get a lightweight version of InversionNet, which can make an efficient inference on edge devices. And we also made a prototype with Raspberry Pi to run the lightweight InversionNet. Experimental results show that the pruned InversionNet can achieve up to 98.2 % reduction in computing resources with moderate model performance degradation.
&lt;/p&gt;</description></item><item><title>&#22312;LLM&#35757;&#32451;&#20013;&#65292;&#20998;&#35789;&#22120;&#30340;&#36873;&#25321;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#25104;&#26412;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#65292;&#24120;&#35265;&#30340;&#20998;&#35789;&#22120;&#35780;&#20272;&#25351;&#26631;&#19981;&#19968;&#23450;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08754</link><description>&lt;p&gt;
LLM&#35757;&#32451;&#20013;&#30340;&#20998;&#35789;&#36873;&#25321;&#65306;&#24494;&#19981;&#36275;&#36947;&#36824;&#26159;&#33267;&#20851;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Tokenizer Choice For LLM Training: Negligible or Crucial?. (arXiv:2310.08754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08754
&lt;/p&gt;
&lt;p&gt;
&#22312;LLM&#35757;&#32451;&#20013;&#65292;&#20998;&#35789;&#22120;&#30340;&#36873;&#25321;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#25104;&#26412;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#65292;&#24120;&#35265;&#30340;&#20998;&#35789;&#22120;&#35780;&#20272;&#25351;&#26631;&#19981;&#19968;&#23450;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;LLM&#30340;&#25104;&#21151;&#20027;&#35201;&#26159;&#30001;&#20110;&#31574;&#21010;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#25193;&#23637;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#36827;&#27493;&#65292;&#32780;&#20998;&#35789;&#22120;&#30340;&#24433;&#21709;&#21017;&#26159;&#19968;&#20010;&#30450;&#28857;&#12290;&#36890;&#36807;&#23545;24&#20010;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23545;&#19981;&#21516;&#30340;&#20998;&#35789;&#22120;&#31639;&#27861;&#21644;&#21442;&#25968;&#36827;&#34892;&#22823;&#33539;&#22260;&#23454;&#39564;&#65292;&#25105;&#20204;&#23545;&#20998;&#35789;&#22120;&#36873;&#25321;&#23545;LLM&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20998;&#35789;&#22120;&#36873;&#25321;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24120;&#35265;&#30340;&#20998;&#35789;&#22120;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;&#20016;&#23500;&#24230;&#21644;&#24179;&#31561;&#24615;&#65289;&#24182;&#19981;&#24635;&#26159;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#65292;&#36825;&#20351;&#24471;&#36825;&#20123;&#25351;&#26631;&#25104;&#20026;&#23545;&#20998;&#35789;&#22120;&#35780;&#20272;&#30340;&#21487;&#30097;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38024;&#23545;&#20116;&#31181;&#26368;&#24120;&#35265;&#30340;&#27431;&#27954;&#35821;&#35328;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#38656;&#35201;&#35789;&#27719;&#34920;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of LLMs has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot. Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model's downstream performance, training and inference costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable choice for tokenizer evaluation. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary si
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07923</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Expresssive Power of Transformers with Chain of Thought. (arXiv:2310.07923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#20123;&#20986;&#20154;&#24847;&#26009;&#22320;&#31616;&#21333;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#20363;&#22914;&#26816;&#26597;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#36830;&#25509;&#30340;&#20004;&#20010;&#33410;&#28857;&#65292;&#25110;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#26426;&#65292;&#36825;&#20123;&#38382;&#39064;&#34987;&#35777;&#26126;&#26080;&#27861;&#30001;&#31435;&#21363;&#35835;&#21462;&#36755;&#20837;&#21518;&#22238;&#31572;&#30340;&#26631;&#20934;Transformer&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20801;&#35768;Transformer&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25110;&#8220;&#33609;&#31295;&#32440;&#8221;&#65292;&#21363;&#22312;&#22238;&#31572;&#20043;&#21069;&#29983;&#25104;&#24182;&#20381;&#36182;&#19968;&#31995;&#21015;&#20013;&#38388;token&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#38382;&#65306;&#36825;&#31181;&#20013;&#38388;&#29983;&#25104;&#26159;&#21542;&#20174;&#26681;&#26412;&#19978;&#25193;&#23637;&#20102;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;Transformer&#30340;&#35745;&#31639;&#33021;&#21147;&#65311;&#25105;&#20204;&#34920;&#26126;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#20294;&#22686;&#21152;&#30340;&#31243;&#24230;&#20851;&#38190;&#21462;&#20915;&#20110;&#20013;&#38388;&#29983;&#25104;&#30340;&#25968;&#37327;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#26469;&#35828;&#65292;&#20855;&#26377;&#23545;&#25968;&#32423;&#35299;&#30721;&#27493;&#39588;&#30340;Transformer&#35299;&#30721;&#22120;&#20165;&#30053;&#24494;&#25512;&#21160;&#20102;&#26631;&#20934;Transformer&#30340;&#26497;&#38480;&#65292;&#32780;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#21017;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#65288;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard compl
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07177</link><description>&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07177
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#26469;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#22810;&#26679;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#33609;&#31295;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#33021;&#21147;&#24046;&#36317;&#26102;&#65292;&#20854;&#26377;&#25928;&#24615;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#65288;OSD&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#20016;&#23500;&#30340;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#29992;&#25143;&#26597;&#35810;&#25968;&#25454;&#25345;&#32493;&#26356;&#26032;&#65288;&#22810;&#20010;&#65289;&#33609;&#31295;&#27169;&#22411;&#12290;&#30001;&#20110;LLM&#25512;&#29702;&#21463;&#20869;&#23384;&#38480;&#21046;&#65292;&#20856;&#22411;&#30340;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#30340;&#21097;&#20313;&#35745;&#31639;&#33021;&#21147;&#21487;&#20197;&#29992;&#20110;&#22312;&#32447;&#37325;&#26032;&#35757;&#32451;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#25104;&#26412;&#20445;&#25345;&#20013;&#24615;&#12290;&#30001;&#20110;LLM&#26381;&#21153;&#30340;&#26597;&#35810;&#20998;&#24067;&#30456;&#23545;&#31616;&#21333;&#65292;&#26681;&#25454;&#26597;&#35810;&#20998;&#24067;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#20351;&#33609;&#31295;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crystal&#30340;&#20869;&#30465;&#22411;&#24120;&#35782;&#25512;&#29702;&#22120;&#65292;&#36890;&#36807;&#20869;&#30465;&#30693;&#35782;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#24120;&#35782;&#25512;&#29702;&#30340;&#24615;&#33021;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04921</link><description>&lt;p&gt;
Crystal: &#20197;&#33258;&#25105;&#21453;&#39304;&#20026;&#22686;&#24378;&#30340;&#20869;&#30465;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Crystal: Introspective Reasoners Reinforced with Self-Feedback. (arXiv:2310.04921v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04921
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crystal&#30340;&#20869;&#30465;&#22411;&#24120;&#35782;&#25512;&#29702;&#22120;&#65292;&#36890;&#36807;&#20869;&#30465;&#30693;&#35782;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#24120;&#35782;&#25512;&#29702;&#30340;&#24615;&#33021;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;&#25512;&#29702;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24120;&#35782;&#25512;&#29702;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20854;&#20013;&#25512;&#29702;&#36807;&#31243;&#30340;&#22522;&#30784;&#30693;&#35782;&#26126;&#30830;&#34920;&#36798;&#21644;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23454;&#29616;&#65292;&#21253;&#25324;"&#24605;&#32500;&#38142;"&#21450;&#20854;&#21464;&#31181;&#65292;&#26410;&#33021;&#25429;&#25417;&#21040;&#24120;&#35782;&#25512;&#29702;&#20013;&#25152;&#38656;&#30340;&#20869;&#30465;&#24615;&#36136;&#65292;&#20063;&#26410;&#33021;&#35299;&#37322;&#30693;&#35782;&#29983;&#25104;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#30456;&#20114;&#36866;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#20869;&#30465;&#22411;&#24120;&#35782;&#25512;&#29702;&#22120; Crystal&#12290;&#20026;&#20102;&#35299;&#20915;&#24120;&#35782;&#38382;&#39064;&#65292;&#23427;&#39318;&#20808;&#20869;&#30465;&#19982;&#32473;&#23450;&#38382;&#39064;&#30456;&#20851;&#30340;&#30693;&#35782;&#38472;&#36848;&#65292;&#28982;&#21518;&#22522;&#20110;&#20808;&#21069;&#20869;&#30465;&#30340;&#30693;&#35782;&#36827;&#34892;&#30693;&#24773;&#39044;&#27979;&#12290;&#27169;&#22411;&#30340;&#30693;&#35782;&#20869;&#30465;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#27169;&#24335;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35843;&#25972;&#65292;&#20854;&#20013;&#22870;&#21169;&#26469;&#33258;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extensive work has shown that the performance and interpretability of commonsense reasoning can be improved via knowledge-augmented reasoning methods, where the knowledge that underpins the reasoning process is explicitly verbalized and utilized. However, existing implementations, including "chain-of-thought" and its variants, fall short in capturing the introspective nature of knowledge required in commonsense reasoning, and in accounting for the mutual adaptation between the generation and utilization of knowledge. We propose a novel method to develop an introspective commonsense reasoner, Crystal. To tackle commonsense problems, it first introspects for knowledge statements related to the given question, and subsequently makes an informed prediction that is grounded in the previously introspected knowledge. The knowledge introspection and knowledge-grounded reasoning modes of the model are tuned via reinforcement learning to mutually adapt, where the reward derives from the feedback
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19971;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;ToyMix&#12289;LargeMix&#21644;UltraLarge&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#26377;&#30417;&#30563;&#26631;&#31614;&#30340;&#22810;&#26679;&#24615;&#26041;&#38754;&#31361;&#30772;&#20102;&#30028;&#38480;&#65292;&#28085;&#30422;&#20102;&#36817;1&#20159;&#20010;&#20998;&#23376;&#21644;3000&#22810;&#20010;&#31232;&#30095;&#23450;&#20041;&#30340;&#20219;&#21153;&#65292;&#24635;&#35745;&#36229;&#36807;130&#20159;&#20010;&#20851;&#20110;&#37327;&#23376;&#21644;&#29983;&#29289;&#24615;&#36136;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2310.04292</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#20998;&#23376;&#23398;&#20064;&#22522;&#30784;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets. (arXiv:2310.04292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19971;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;ToyMix&#12289;LargeMix&#21644;UltraLarge&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#26377;&#30417;&#30563;&#26631;&#31614;&#30340;&#22810;&#26679;&#24615;&#26041;&#38754;&#31361;&#30772;&#20102;&#30028;&#38480;&#65292;&#28085;&#30422;&#20102;&#36817;1&#20159;&#20010;&#20998;&#23376;&#21644;3000&#22810;&#20010;&#31232;&#30095;&#23450;&#20041;&#30340;&#20219;&#21153;&#65292;&#24635;&#35745;&#36229;&#36807;130&#20159;&#20010;&#20851;&#20110;&#37327;&#23376;&#21644;&#29983;&#29289;&#24615;&#36136;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#25163;&#24037;&#31574;&#21010;&#30340;&#65292;&#22240;&#27492;&#35268;&#27169;&#36739;&#23567;&#65292;&#32570;&#20047;&#24102;&#26377;&#26631;&#35760;&#29305;&#24449;&#21644;&#31649;&#29702;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#20195;&#30721;&#24211;&#65292;&#21046;&#32422;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19971;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;ToyMix&#12289;LargeMix&#21644;UltraLarge&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#26377;&#30417;&#30563;&#26631;&#31614;&#30340;&#22810;&#26679;&#24615;&#26041;&#38754;&#31361;&#30772;&#20102;&#30028;&#38480;&#12290;&#23427;&#20204;&#28085;&#30422;&#20102;&#36817;1&#20159;&#20010;&#20998;&#23376;&#21644;3000&#22810;&#20010;&#31232;&#30095;&#23450;&#20041;&#30340;&#20219;&#21153;&#65292;&#24635;&#35745;&#36229;&#36807;130&#20159;&#20010;&#20851;&#20110;&#37327;&#23376;&#21644;&#29983;&#29289;&#24615;&#36136;&#30340;&#26631;&#31614;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;OGB-LSC PCQM4Mv2&#25968;&#25454;&#38598;&#30340;300&#20493;&#65292;&#20063;&#26159;&#20165;&#21253;&#21547;&#37327;&#23376;&#25968;&#25454;&#30340;QM1B&#25968;&#25454;&#38598;&#30340;13&#20493;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25903;&#25345;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24615;&#33021;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#19978;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21152;&#36895;&#65292;&#33719;&#24471;&#20102;2.24&#20493;&#21644;5.27&#20493;&#30340;&#21534;&#21520;&#37327;&#25552;&#21319;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02784</link><description>&lt;p&gt;
&#36229;&#36234;&#21333;&#33410;&#28857;&#65306;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#19978;&#23454;&#29616;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
MAD Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems. (arXiv:2310.02784v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02784
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24615;&#33021;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#19978;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21152;&#36895;&#65292;&#33719;&#24471;&#20102;2.24&#20493;&#21644;5.27&#20493;&#30340;&#21534;&#21520;&#37327;&#25552;&#21319;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#26159;&#32791;&#26102;&#19988;&#38656;&#35201;&#22823;&#37327;&#20998;&#24067;&#24335;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#12290;&#26681;&#25454;&#23454;&#38469;&#24773;&#20917;&#22312;&#25968;&#25454;&#20013;&#24515;&#35268;&#27169;&#22522;&#30784;&#35774;&#26045;&#19978;&#36827;&#34892;&#22823;&#27169;&#22411;&#35757;&#32451;&#65292;&#25105;&#20204;&#21457;&#29616;14~32%&#30340;GPU&#23567;&#26102;&#29992;&#20110;&#36890;&#20449;&#65292;&#27809;&#26377;&#37325;&#21472;&#35745;&#31639;&#12290;&#20026;&#20102;&#23613;&#37327;&#20943;&#23569;&#31561;&#24453;&#36890;&#20449;&#24310;&#36831;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#24615;&#33021;&#24314;&#27169;&#26694;&#26550;&#65292;&#25351;&#23548;&#24182;&#34892;&#21270;&#21644;&#30828;&#20214;&#36719;&#20214;&#20849;&#21516;&#35774;&#35745;&#31574;&#30053;&#12290;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;GPU&#35757;&#32451;&#30828;&#20214;&#19978;&#30340;&#19968;&#22871;&#23454;&#38469;&#22823;&#35268;&#27169;ML&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#21644;&#25512;&#26029;&#22330;&#26223;&#20998;&#21035;&#21487;&#20197;&#25552;&#39640;2.24&#20493;&#21644;5.27&#20493;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training and deploying large machine learning (ML) models is time-consuming and requires significant distributed computing infrastructures. Based on real-world large model training on datacenter-scale infrastructures, we show 14~32% of all GPU hours are spent on communication with no overlapping computation. To minimize the outstanding communication latency, in this work, we develop an agile performance modeling framework to guide parallelization and hardware-software co-design strategies. Using the suite of real-world large ML models on state-of-the-art GPU training hardware, we demonstrate 2.24x and 5.27x throughput improvement potential for pre-training and inference scenarios, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#32990;&#33258;&#21160;&#26426;&#30340;&#26234;&#33021;&#23458;&#25143;&#31471;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#20256;&#24863;&#22120;&#25968;&#37327;&#22686;&#21152;&#24102;&#26469;&#30340;&#36890;&#20449;&#21644;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00627</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#32990;&#33258;&#21160;&#26426;&#30340;&#26234;&#33021;&#23458;&#25143;&#31471;&#36873;&#25321;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Intelligent Client Selection for Federated Learning using Cellular Automata. (arXiv:2310.00627v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#32990;&#33258;&#21160;&#26426;&#30340;&#26234;&#33021;&#23458;&#25143;&#31471;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#20256;&#24863;&#22120;&#25968;&#37327;&#22686;&#21152;&#24102;&#26469;&#30340;&#36890;&#20449;&#21644;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#38544;&#31169;&#22686;&#24378;&#21644;&#24310;&#36831;&#26368;&#23567;&#21270;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#65288;&#22914;&#20132;&#36890;&#12289;&#36890;&#20449;&#21644;&#21307;&#30103;&#65289;&#20013;&#30340;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;FL&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#25968;&#30334;&#19975;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24341;&#20837;&#36793;&#32536;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#24555;&#36895;&#21709;&#24212;&#65292;&#24182;&#20135;&#29983;&#39640;&#24230;&#20010;&#24615;&#21270;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#24212;&#29992;&#20013;&#20256;&#24863;&#22120;&#25968;&#37327;&#30340;&#22686;&#21152;&#22312;&#36890;&#20449;&#21644;&#36164;&#28304;&#20998;&#37197;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#25152;&#26377;&#35774;&#22791;&#21442;&#19982;&#32852;&#37030;&#36807;&#31243;&#30340;&#33021;&#21147;&#65292;&#36827;&#32780;&#38656;&#35201;&#26377;&#25928;&#30340;FL&#23458;&#25143;&#31471;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#32990;&#33258;&#21160;&#26426;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#65288;CA-CS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#20803;&#32990;&#33258;&#21160;&#26426;&#65288;CA&#65289;&#20316;&#20026;&#27169;&#22411;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#24555;&#36895;&#28436;&#21464;&#29615;&#22659;&#20013;&#30340;&#26102;&#31354;&#21464;&#21270;&#12290;CA-CS&#32771;&#34385;&#20102;&#35745;&#31639;&#36164;&#28304;&#21644;&#36890;&#20449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a promising solution for privacy-enhancement and latency minimization in various real-world applications, such as transportation, communications, and healthcare. FL endeavors to bring Machine Learning (ML) down to the edge by harnessing data from million of devices and IoT sensors, thus enabling rapid responses to dynamic environments and yielding highly personalized results. However, the increased amount of sensors across diverse applications poses challenges in terms of communication and resource allocation, hindering the participation of all devices in the federated process and prompting the need for effective FL client selection. To address this issue, we propose Cellular Automaton-based Client Selection (CA-CS), a novel client selection algorithm, which leverages Cellular Automata (CA) as models to effectively capture spatio-temporal changes in a fast-evolving environment. CA-CS considers the computational resources and communication capacity
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#25968;&#25454;&#23545;&#20110;&#32447;&#24615;&#27169;&#22411;&#20013;&#20302;&#32500;&#21442;&#25968;&#20272;&#35745;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#20102;&#26465;&#20214;&#20351;&#24471;&#20272;&#35745;&#35823;&#24046;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#24773;&#20917;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2310.00532</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32447;&#24615;&#27169;&#22411;&#30340;&#32479;&#35745;&#26497;&#38480;&#65306;&#20302;&#32500;&#20272;&#35745;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Statistical Limits of Adaptive Linear Models: Low-Dimensional Estimation and Inference. (arXiv:2310.00532v1 [math.ST] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00532
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#25968;&#25454;&#23545;&#20110;&#32447;&#24615;&#27169;&#22411;&#20013;&#20302;&#32500;&#21442;&#25968;&#20272;&#35745;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#20102;&#26465;&#20214;&#20351;&#24471;&#20272;&#35745;&#35823;&#24046;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#24773;&#20917;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25968;&#25454;&#34987;&#33258;&#36866;&#24212;&#22320;&#25910;&#38598;&#26102;&#65292;&#32479;&#35745;&#23398;&#20013;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#21363;&#20351;&#22312;&#32447;&#24615;&#27169;&#22411;&#20013;&#65292;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#65288;OLS&#65289;&#22312;&#21333;&#20010;&#22352;&#26631;&#20272;&#35745;&#21644;&#35823;&#24046;&#33192;&#32960;&#26041;&#38754;&#21487;&#33021;&#26080;&#27861;&#23637;&#29616;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#26368;&#36817;&#30340;&#26497;&#23567;&#26497;&#38480;&#26174;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#25351;&#20986;&#24403;&#25968;&#25454;&#20801;&#35768;&#20219;&#24847;&#33258;&#36866;&#24212;&#24615;&#26102;&#65292;&#20272;&#35745;&#21333;&#20010;&#22352;&#26631;&#30340;&#35823;&#24046;&#21487;&#20197;&#22686;&#21152;&#19968;&#20010;$\sqrt{d}$&#20493;&#65292;&#19982;i.i.d.&#24773;&#20917;&#30456;&#27604;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#22312;&#21033;&#29992;i.i.d.&#21644;&#33258;&#36866;&#24212;&#25968;&#25454;&#26102;&#20272;&#35745;&#24615;&#33021;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#33258;&#36866;&#24212;&#31243;&#24230;&#22914;&#20309;&#24433;&#21709;&#39640;&#32500;&#32447;&#24615;&#27169;&#22411;&#20013;&#20272;&#35745;&#20302;&#32500;&#21442;&#25968;&#32452;&#25104;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#25968;&#25454;&#25910;&#38598;&#26426;&#21046;&#26465;&#20214;&#65292;&#20351;&#24471;&#20302;&#32500;&#21442;&#25968;&#32452;&#25104;&#30340;&#20272;&#35745;&#35823;&#24046;&#22312;i.i.d.&#24773;&#20917;&#19979;&#19982;&#20854;&#30456;&#21305;&#37197;&#65292;&#21482;&#24046;&#19968;&#20010;f&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimation and inference in statistics pose significant challenges when data are collected adaptively. Even in linear models, the Ordinary Least Squares (OLS) estimator may fail to exhibit asymptotic normality for single coordinate estimation and have inflated error. This issue is highlighted by a recent minimax lower bound, which shows that the error of estimating a single coordinate can be enlarged by a multiple of $\sqrt{d}$ when data are allowed to be arbitrarily adaptive, compared with the case when they are i.i.d. Our work explores this striking difference in estimation performance between utilizing i.i.d. and adaptive data. We investigate how the degree of adaptivity in data collection impacts the performance of estimating a low-dimensional parameter component in high-dimensional linear models. We identify conditions on the data collection mechanism under which the estimation error for a low-dimensional parameter component matches its counterpart in the i.i.d. setting, up to a f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;</title><link>http://arxiv.org/abs/2309.17264</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#19968;&#33324;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26088;&#22312;&#25551;&#32472;&#24863;&#20852;&#36259;&#30340;&#35299;&#21078;&#25110;&#30149;&#29702;&#32467;&#26500;&#65292;&#22312;&#20020;&#24202;&#35786;&#26029;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26500;&#24314;&#39640;&#31934;&#24230;&#30340;&#28145;&#24230;&#20998;&#21106;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#27880;&#37322;&#38750;&#24120;&#32321;&#29712;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#23398;&#35270;&#39057;&#25110;3D&#20307;&#31215;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#21644;&#24046;&#30340;&#24103;&#38388;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#65292;&#19968;&#20010;&#21517;&#20026;Moving Object Segmentation (MOS)&#30340;&#22522;&#26412;&#20219;&#21153;&#22312;&#25216;&#26415;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#23427;&#30340;&#30446;&#26631;&#26159;&#22312;&#22270;&#20687;&#24207;&#21015;&#20013;&#20174;&#32972;&#26223;&#20013;&#25551;&#32472;&#31227;&#21160;&#29289;&#20307;&#65292;&#21482;&#38656;&#35201;&#26368;&#23567;&#30340;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;MOS&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21517;&#20026;iMOS&#12290;&#23545;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;iMOS&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21482;&#38656;&#23545;&#24207;&#21015;&#20013;&#23569;&#37327;&#30340;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;iMOS&#23601;&#21487;&#20197;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
&lt;/p&gt;</description></item><item><title>SEPT&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#25429;&#25417;&#36712;&#36857;&#30340;&#36816;&#21160;&#23398;&#29305;&#24449;&#12289;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#20197;&#21450;&#36947;&#36335;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#21160;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15289</link><description>&lt;p&gt;
SEPT: &#20026;&#36816;&#21160;&#39044;&#27979;&#30340;&#39640;&#25928;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SEPT: Towards Efficient Scene Representation Learning for Motion Prediction. (arXiv:2309.15289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15289
&lt;/p&gt;
&lt;p&gt;
SEPT&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#25429;&#25417;&#36712;&#36857;&#30340;&#36816;&#21160;&#23398;&#29305;&#24449;&#12289;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#20197;&#21450;&#36947;&#36335;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#21160;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#39044;&#27979;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#22797;&#26434;&#20132;&#36890;&#29615;&#22659;&#20013;&#23433;&#20840;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#25552;&#21462;&#20132;&#36890;&#20803;&#32032;&#20043;&#38388;&#30340;&#26377;&#25928;&#26102;&#31354;&#20851;&#31995;&#26159;&#20934;&#30830;&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#21463;&#21040;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;SEPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#24320;&#21457;&#22797;&#26434;&#20132;&#36890;&#22330;&#26223;&#20013;&#24378;&#22823;&#30340;&#26102;&#31354;&#29702;&#35299;&#33021;&#21147;&#30340;&#24314;&#27169;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21040;&#22312;&#22330;&#26223;&#36755;&#20837;&#19978;&#36827;&#34892;&#19977;&#20010;&#25513;&#30721;&#37325;&#26500;&#24314;&#27169;&#20219;&#21153;&#65292;&#21253;&#25324;&#20195;&#29702;&#36335;&#24452;&#21644;&#36947;&#36335;&#32593;&#32476;&#65292;&#39044;&#35757;&#32451;&#22330;&#26223;&#32534;&#30721;&#22120;&#20197;&#25429;&#25417;&#36712;&#36857;&#30340;&#36816;&#21160;&#23398;&#29305;&#24449;&#65292;&#36947;&#36335;&#32593;&#32476;&#30340;&#31354;&#38388;&#32467;&#26500;&#20197;&#21450;&#36947;&#36335;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#28982;&#21518;&#22312;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;SEPT&#22312;Argoverse 1&#21644;Argoverse&#19978;&#26080;&#38656;&#31934;&#24515;&#35774;&#35745;&#30340;&#26550;&#26500;&#25110;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion prediction is crucial for autonomous vehicles to operate safely in complex traffic environments. Extracting effective spatiotemporal relationships among traffic elements is key to accurate forecasting. Inspired by the successful practice of pretrained large language models, this paper presents SEPT, a modeling framework that leverages self-supervised learning to develop powerful spatiotemporal understanding for complex traffic scenes. Specifically, our approach involves three masking-reconstruction modeling tasks on scene inputs including agents' trajectories and road network, pretraining the scene encoder to capture kinematics within trajectory, spatial structure of road network, and interactions among roads and agents. The pretrained encoder is then finetuned on the downstream forecasting task. Extensive experiments demonstrate that SEPT, without elaborate architectural design or manual feature engineering, achieves state-of-the-art performance on the Argoverse 1 and Argoverse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15028</link><description>&lt;p&gt;
&#35753;PPO&#21464;&#24471;&#26356;&#22909;&#65306;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26102;&#65292;&#20351;&#29992;&#26368;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;Proximal Policy Optimization (PPO)&#65292;&#22240;&#27492;&#21487;&#20197;&#35748;&#20026;&#25512;&#29702;&#26102;&#38388;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#22914;Monte-Carlo Tree Search (MCTS) &#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;PPO&#30340;&#24615;&#33021;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#35299;&#30721;&#25991;&#26412;&#26102;&#65292;&#19981;&#35201;&#20002;&#24323;&#20540;&#32593;&#32476;&#65292;&#21363;PPO&#35757;&#32451;&#26102;&#29992;&#20110;&#35780;&#20272;&#37096;&#20998;&#36755;&#20986;&#24207;&#21015;&#30340;&#21103;&#20135;&#21697;&#65292;&#32780;&#26159;&#23558;&#20854;&#19982;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PPO-MCTS&#30340;&#26032;&#39062;&#30340;&#20540;&#23548;&#21521;&#35299;&#30721;&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;PPO&#30340;&#20540;&#32593;&#32476;&#19982;&#25512;&#29702;&#26102;&#38388;&#20135;&#29983;&#30340;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#19982;&#22522;&#20110;MCTS&#30340;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#20943;&#23569;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#30340;&#35780;&#20998;&#26426;&#21046;&#30340;&#22522;&#26412;&#19981;&#21305;&#37197;&#12290;&#22312;&#22235;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;PPO-MCTS&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS grea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;IBMDP&#20013;&#20351;&#29992;Actor-Critic&#31639;&#27861;&#23398;&#20064;&#20915;&#31574;&#26641;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22312;&#31616;&#21333;&#30340;&#29609;&#20855;&#20219;&#21153;&#19978;&#65292;&#28145;&#24230;RL&#20063;&#21487;&#33021;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2309.13365</link><description>&lt;p&gt;
&#20915;&#31574;&#26641;&#31574;&#30053;&#22312;IBMDP&#20013;&#30340;Actor-Critic&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65288;arXiv:2309.13365v2 [cs.LG] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Limits of Actor-Critic Algorithms for Decision Tree Policies Learning in IBMDPs. (arXiv:2309.13365v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13365
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;IBMDP&#20013;&#20351;&#29992;Actor-Critic&#31639;&#27861;&#23398;&#20064;&#20915;&#31574;&#26641;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22312;&#31616;&#21333;&#30340;&#29609;&#20855;&#20219;&#21153;&#19978;&#65292;&#28145;&#24230;RL&#20063;&#21487;&#33021;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#23433;&#20840;&#26816;&#26597;&#26469;&#24314;&#31435;&#23545;&#36825;&#20123;AI&#30340;&#20449;&#20219;&#12290;&#29305;&#21035;&#26159;&#65292;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#25552;&#20379;&#20102;&#23545;&#23398;&#20064;&#27169;&#22411;&#30340;&#25972;&#20307;&#35270;&#35282;&#65292;&#24182;&#36879;&#26126;&#22320;&#25581;&#31034;&#20102;&#21738;&#20123;&#36755;&#20837;&#29305;&#24449;&#23545;&#20110;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#20915;&#31574;&#26641;&#36807;&#22823;&#65292;&#21487;&#35299;&#37322;&#24615;&#23601;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#23398;&#20064;&#32039;&#20945;&#30340;&#20915;&#31574;&#26641;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#28145;&#24230;RL&#25506;&#32034;DT&#30340;&#31354;&#38388;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22686;&#21152;&#21160;&#20316;&#26469;&#25910;&#38598;&#20851;&#20110;&#38544;&#34255;&#36755;&#20837;&#29305;&#24449;&#30340;&#20449;&#24687;&#65292;&#36890;&#36807;&#36866;&#24403;&#22320;&#23545;&#36825;&#20123;&#21160;&#20316;&#36827;&#34892;&#24809;&#32602;&#65292;&#20195;&#29702;&#23398;&#20064;&#22914;&#20309;&#22312;&#26641;&#30340;&#22823;&#23567;&#21644;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26368;&#20248;&#26435;&#34913;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#38656;&#35201;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#21453;&#24212;&#24615;&#31574;&#30053;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#36825;&#19968;&#31867;&#31616;&#21333;&#30340;&#29609;&#20855;&#20219;&#21153;&#19978;&#65292;&#28145;&#24230;RL&#20063;&#21487;&#33021;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of AI models allows for user safety checks to build trust in such AIs. In particular, Decision Trees (DTs) provide a global look at the learned model and transparently reveal which features of the input are critical for making a decision. However, interpretability is hindered if the DT is too large. To learn compact trees, a recent Reinforcement Learning (RL) framework has been proposed to explore the space of DTs using deep RL. This framework augments a decision problem (e.g. a supervised classification task) with additional actions that gather information about the features of an otherwise hidden input. By appropriately penalizing these actions, the agent learns to optimally trade-off size and performance of DTs. In practice, a reactive policy for a partially observable Markov decision process (MDP) needs to be learned, which is still an open problem. We show in this paper that deep RL can fail even on simple toy tasks of this class. However, when the underlying deci
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32437;&#21521;&#24352;&#37327;&#21709;&#24212;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#27719;&#38598;&#31354;&#38388;&#20998;&#24067;&#30340;&#20307;&#32032;&#20449;&#24687;&#26469;&#25512;&#26029;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#24182;&#23545;&#21327;&#21464;&#37327;&#36827;&#34892;&#35843;&#25972;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#65292;&#21033;&#29992;&#20302;&#31209;&#20998;&#35299;&#20943;&#23569;&#32500;&#24230;&#24182;&#20445;&#25345;&#32500;&#24230;&#30340;&#31354;&#38388;&#37197;&#32622;&#65292;&#36890;&#36807;&#28385;&#36275;&#21518;&#39564;&#20998;&#24067;&#24418;&#29366;&#30340;&#32852;&#21512;&#21487;&#20449;&#21306;&#22495;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2309.10065</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#32437;&#21521;&#24352;&#37327;&#21709;&#24212;&#22238;&#24402;&#30340;&#31070;&#32463;&#21487;&#22609;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Bayesian longitudinal tensor response regression for modeling neuroplasticity. (arXiv:2309.10065v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10065
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32437;&#21521;&#24352;&#37327;&#21709;&#24212;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#27719;&#38598;&#31354;&#38388;&#20998;&#24067;&#30340;&#20307;&#32032;&#20449;&#24687;&#26469;&#25512;&#26029;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#24182;&#23545;&#21327;&#21464;&#37327;&#36827;&#34892;&#35843;&#25972;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#65292;&#21033;&#29992;&#20302;&#31209;&#20998;&#35299;&#20943;&#23569;&#32500;&#24230;&#24182;&#20445;&#25345;&#32500;&#24230;&#30340;&#31354;&#38388;&#37197;&#32622;&#65292;&#36890;&#36807;&#28385;&#36275;&#21518;&#39564;&#20998;&#24067;&#24418;&#29366;&#30340;&#32852;&#21512;&#21487;&#20449;&#21306;&#22495;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32437;&#21521;&#31070;&#32463;&#24433;&#20687;&#30740;&#31350;&#20013;&#23545;&#20110;&#30001;&#27835;&#30103;&#21644;&#20854;&#20182;&#22240;&#32032;&#24341;&#36215;&#30340;&#20307;&#32032;&#32423;&#31070;&#32463;&#21487;&#22609;&#24615;&#30340;&#35843;&#26597;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20307;&#32032;&#32423;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#38519;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#36825;&#20123;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36125;&#21494;&#26031;&#24352;&#37327;&#21709;&#24212;&#22238;&#24402;&#30340;&#32437;&#21521;&#24433;&#20687;&#25968;&#25454;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27719;&#38598;&#31354;&#38388;&#20998;&#24067;&#30340;&#20307;&#32032;&#20449;&#24687;&#26469;&#25512;&#26029;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#24182;&#23545;&#21327;&#21464;&#37327;&#36827;&#34892;&#35843;&#25972;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#37319;&#26679;&#23454;&#29616;&#65292;&#21033;&#29992;&#20302;&#31209;&#20998;&#35299;&#26469;&#20943;&#23569;&#32500;&#24230;&#24182;&#20445;&#25345;&#20272;&#35745;&#31995;&#25968;&#20013;&#20307;&#32032;&#30340;&#31354;&#38388;&#37197;&#32622;&#12290;&#23427;&#36824;&#36890;&#36807;&#28385;&#36275;&#21518;&#39564;&#20998;&#24067;&#24418;&#29366;&#30340;&#32852;&#21512;&#21487;&#20449;&#21306;&#22495;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#25512;&#26029;&#12290;&#38500;&#20102;&#32676;&#20307;&#27700;&#24179;&#30340;&#25512;&#26029;&#65292;&#35813;&#26041;&#27861;&#36824;&#33021;&#22815;&#25512;&#26029;&#20010;&#20307;&#27700;&#24179;&#30340;&#31070;&#32463;&#21487;&#22609;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#36827;&#34892;&#20010;&#20307;&#27700;&#24179;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major interest in longitudinal neuroimaging studies involves investigating voxel-level neuroplasticity due to treatment and other factors across visits. However, traditional voxel-wise methods are beset with several pitfalls, which can compromise the accuracy of these approaches. We propose a novel Bayesian tensor response regression approach for longitudinal imaging data, which pools information across spatially-distributed voxels to infer significant changes while adjusting for covariates. The proposed method, which is implemented using Markov chain Monte Carlo (MCMC) sampling, utilizes low-rank decomposition to reduce dimensionality and preserve spatial configurations of voxels when estimating coefficients. It also enables feature selection via joint credible regions which respect the shape of the posterior distributions for more accurate inference. In addition to group level inferences, the method is able to infer individual-level neuroplasticity, allowing for examination of pers
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06800</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#65292;&#22240;&#20026;&#23427;&#22312;&#20132;&#36890;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36817;&#26399;&#65292;&#35768;&#22810;&#30740;&#31350;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#39044;&#27979;&#20301;&#32622;&#26377;&#23436;&#25972;&#25110;&#33267;&#23569;&#37096;&#20998;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#19981;&#33021;&#25193;&#23637;&#21040;&#26080;&#21382;&#21490;&#35760;&#24405;&#30340;&#20301;&#32622;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#39044;&#31639;&#38480;&#21046;&#21644;&#23433;&#35013;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#21487;&#33021;&#21463;&#38480;&#65292;&#36825;&#20351;&#24471;&#22823;&#22810;&#25968;&#24403;&#21069;&#27169;&#22411;&#19981;&#36866;&#29992;&#12290;&#34429;&#28982;&#23569;&#25968;&#25991;&#29486;&#23581;&#35797;&#22312;&#32570;&#22833;&#20301;&#32622;&#19978;&#25554;&#34917;&#20132;&#36890;&#29366;&#24577;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19982;&#20256;&#24863;&#22120;&#20301;&#32622;&#21516;&#26102;&#35266;&#27979;&#30340;&#25968;&#25454;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#39044;&#27979;&#20219;&#21153;&#12290;&#21478;&#19968;&#20010;&#32570;&#28857;&#26159;&#32570;&#20047;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#24471;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#20219;&#21153;&#25110;&#28041;&#21450;&#20915;&#31574;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#21463;&#21040;&#20808;&#21069;&#30340;&#24402;&#32435;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-awa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#22806;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;HAct&#28608;&#27963;&#30452;&#26041;&#22270;&#25551;&#36848;&#31526;&#23545;OOD&#36827;&#34892;&#26816;&#27979;&#65292;&#20855;&#26377;&#31616;&#21333;&#39640;&#25928;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;OOD&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.04837</link><description>&lt;p&gt;
HAct&#65306;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#30452;&#26041;&#22270;&#30340;&#25968;&#25454;&#38598;&#22806;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
HAct: Out-of-Distribution Detection with Neural Net Activation Histograms. (arXiv:2309.04837v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#22806;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;HAct&#28608;&#27963;&#30452;&#26041;&#22270;&#25551;&#36848;&#31526;&#23545;OOD&#36827;&#34892;&#26816;&#27979;&#65292;&#20855;&#26377;&#31616;&#21333;&#39640;&#25928;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;OOD&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#20934;&#30830;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#25968;&#25454;&#38598;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#30340;&#26816;&#27979;&#65292;&#36825;&#26159;&#29992;&#20110;OOD&#27867;&#21270;&#26041;&#27861;&#30340;&#28508;&#22312;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25551;&#36848;&#31526;&#65292;&#21363;HAct&#28608;&#27963;&#30452;&#26041;&#22270;&#65292;&#29992;&#20110;OOD&#26816;&#27979;&#65292;&#21363;&#36890;&#36807;&#30452;&#26041;&#22270;&#26469;&#36817;&#20284;&#34920;&#31034;&#31070;&#32463;&#32593;&#32476;&#23618;&#36755;&#20986;&#20540;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;HAct&#22312;&#22810;&#31181;OOD&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#20934;&#30830;&#12290;&#20363;&#22914;&#65292;&#22312;&#26631;&#20934;&#30340;OOD&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;Resnet-50&#23454;&#29616;&#20102;95%&#30340;&#30495;&#27491;&#20363;&#29575;&#65288;TPR&#65289;&#65292;&#32780;&#21482;&#26377;0.05%&#30340;&#35823;&#25253;&#29575;&#65292;&#20351;&#24471;&#20854;&#22312;&#35823;&#25253;&#29575;&#19978;&#30456;&#36739;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;20.66%&#65288;&#22312;&#30456;&#21516;&#30340;95%TPR&#19979;&#65289;&#12290;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#26131;&#20110;&#23454;&#29616;&#20351;&#24471;HAct&#36866;&#21512;&#22823;&#35268;&#27169;&#23454;&#36341;&#20013;&#22312;&#32447;&#30417;&#27979;&#37096;&#32626;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple, efficient, and accurate method for detecting out-of-distribution (OOD) data for trained neural networks, a potential first step in methods for OOD generalization. We propose a novel descriptor, HAct activation histograms, for OOD detection, that is, probability distributions (approximated by histograms) of output values of neural network layers under the influence of incoming data. We demonstrate that HAct is significantly more accurate than state-of-the-art on multiple OOD image classification benchmarks. For instance, our approach achieves a true positive rate (TPR) of 95% with only 0.05% false-positives using Resnet-50 on standard OOD benchmarks, outperforming previous state-of-the-art by 20.66% in the false positive rate (at the same TPR of 95%). The low computational complexity and the ease of implementation make HAct suitable for online implementation in monitoring deployed neural networks in practice at scale.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#24046;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#31163;&#25955;&#20998;&#24067;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#31163;&#25955;&#19988;&#20445;&#25345;&#24230;&#37327;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#38656;&#35201;&#36830;&#32493;&#23884;&#20837;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#36830;&#32493;&#23884;&#20837;&#27969;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2308.15613</link><description>&lt;p&gt;
&#28151;&#21512;&#26041;&#24046;&#27969;&#29992;&#20110;&#31163;&#25955;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Mixed Variational Flows for Discrete Variables. (arXiv:2308.15613v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#24046;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#31163;&#25955;&#20998;&#24067;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#31163;&#25955;&#19988;&#20445;&#25345;&#24230;&#37327;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#38656;&#35201;&#36830;&#32493;&#23884;&#20837;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#36830;&#32493;&#23884;&#20837;&#27969;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#27969;&#20801;&#35768;&#20174;&#20107;&#32773;&#23398;&#20064;&#22797;&#26434;&#30340;&#36830;&#32493;&#20998;&#24067;&#65292;&#20294;&#26159;&#36817;&#20284;&#31163;&#25955;&#20998;&#24067;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#31163;&#25955;&#30446;&#26631;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#20013;-&#36890;&#24120;&#26159;&#36890;&#36807;&#36830;&#32493;&#26494;&#24347;&#25110;&#21435;&#37327;&#21270;-&#28982;&#21518;&#24212;&#29992;&#36830;&#32493;&#27969;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#28041;&#21450;&#19968;&#20010;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#21040;&#21407;&#22987;&#31163;&#25955;&#30446;&#26631;&#30340;&#26367;&#20195;&#30446;&#26631;&#65292;&#21487;&#33021;&#20855;&#26377;&#20559;&#20506;&#25110;&#19981;&#31283;&#23450;&#30340;&#26799;&#24230;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#21019;&#24314;&#19968;&#20010;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;&#31163;&#25955;&#20998;&#24067;&#30340;&#21464;&#20998;&#27969;&#26063;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#36830;&#32493;&#23884;&#20837;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20445;&#25345;&#24230;&#37327;&#30340;&#31163;&#25955;&#21487;&#36870;&#26144;&#23556;&#65292;&#20351;&#31163;&#25955;&#30446;&#26631;&#20445;&#25345;&#19981;&#21464;&#65292;&#28982;&#21518;&#22522;&#20110;&#35813;&#26144;&#23556;&#21019;&#24314;&#20102;&#19968;&#20010;&#28151;&#21512;&#21464;&#20998;&#27969;(MAD Mix)&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#25193;&#23637;&#65292;&#29992;&#20110;&#22788;&#29702;&#32852;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MAD Mix&#20135;&#29983;&#20102;&#27604;&#36830;&#32493;&#23884;&#20837;&#27969;&#26356;&#21487;&#38752;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational flows allow practitioners to learn complex continuous distributions, but approximating discrete distributions remains a challenge. Current methodologies typically embed the discrete target in a continuous space - usually via continuous relaxation or dequantization - and then apply a continuous flow. These approaches involve a surrogate target that may not capture the original discrete target, might have biased or unstable gradients, and can create a difficult optimization problem. In this work, we develop a variational flow family for discrete distributions without any continuous embedding. First, we develop a measure-preserving and discrete (MAD) invertible map that leaves the discrete target invariant, and then create a mixed variational flow (MAD Mix) based on that map. We also develop an extension to MAD Mix that handles joint discrete and continuous models. Our experiments suggest that MAD Mix produces more reliable approximations than continuous-embedding flows while 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#35270;&#35282;&#30340;&#37327;&#21270;&#20248;&#21270;&#26041;&#27861;&#22312;&#20840;&#23616;&#20248;&#21270;&#20013;&#21033;&#29992;&#34203;&#23450;&#35860;&#26041;&#31243;&#25512;&#23548;&#30340;&#38567;&#36947;&#25928;&#24212;&#65292;&#20174;&#32780;&#33021;&#22815;&#36991;&#20813;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.11594</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#35270;&#35282;&#30340;&#37327;&#21270;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantization-based Optimization with Perspective of Quantum Mechanics. (arXiv:2308.11594v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11594
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#35270;&#35282;&#30340;&#37327;&#21270;&#20248;&#21270;&#26041;&#27861;&#22312;&#20840;&#23616;&#20248;&#21270;&#20013;&#21033;&#29992;&#34203;&#23450;&#35860;&#26041;&#31243;&#25512;&#23548;&#30340;&#38567;&#36947;&#25928;&#24212;&#65292;&#20174;&#32780;&#33021;&#22815;&#36991;&#20813;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28909;&#21147;&#23398;&#30340;&#32479;&#35745;&#21644;&#38543;&#26426;&#20998;&#26512;&#19968;&#30452;&#26159;&#38543;&#26426;&#20840;&#23616;&#20248;&#21270;&#30340;&#20027;&#35201;&#20998;&#26512;&#26694;&#26550;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#29992;&#20110;&#20840;&#23616;&#20248;&#21270;&#30340;&#37327;&#23376;&#36864;&#28779;&#25110;&#37327;&#23376;&#38567;&#36947;&#31639;&#27861;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#26694;&#26550;&#26469;&#36827;&#34892;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#34203;&#23450;&#35860;&#26041;&#31243;&#30340;&#37327;&#21270;&#20248;&#21270;&#30340;&#20998;&#26512;&#65292;&#20197;&#25581;&#31034;&#37327;&#23376;&#21147;&#23398;&#20013;&#30340;&#21738;&#20123;&#23646;&#24615;&#20351;&#20840;&#23616;&#20248;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#34203;&#23450;&#35860;&#26041;&#31243;&#25512;&#23548;&#20986;&#30340;&#38567;&#36947;&#25928;&#24212;&#20351;&#24471;&#37327;&#21270;&#20248;&#21270;&#33021;&#22815;&#36867;&#31163;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#35748;&#36825;&#31181;&#38567;&#36947;&#25928;&#24212;&#26159;&#21253;&#21547;&#22312;&#22522;&#20110;&#37327;&#23376;&#21147;&#23398;&#30340;&#20840;&#23616;&#20248;&#21270;&#20013;&#30340;&#30456;&#21516;&#23646;&#24615;&#12290;&#23545;&#26631;&#20934;&#22810;&#27169;&#24577;&#22522;&#20934;&#20989;&#25968;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical and stochastic analysis based on thermodynamics has been the main analysis framework for stochastic global optimization. Recently, appearing quantum annealing or quantum tunneling algorithm for global optimization, we require a new researching framework for global optimization algorithms. In this paper, we provide the analysis for quantization-based optimization based on the Schr\"odinger equation to reveal what property in quantum mechanics enables global optimization. We present that the tunneling effect derived by the Schr\"odinger equation in quantization-based optimization enables to escape of a local minimum. Additionally, we confirm that this tunneling effect is the same property included in quantum mechanics-based global optimization. Experiments with standard multi-modal benchmark functions represent that the proposed analysis is valid.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20110;ImageNet&#30340;&#39044;&#35757;&#32451;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#22686;&#24378;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.07688</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#30340;&#33258;&#28982;&#22270;&#20687;&#22686;&#24378;&#21307;&#30103;AI&#27169;&#22411;&#30340;&#32593;&#32476;&#21021;&#22987;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images. (arXiv:2308.07688v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07688
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20110;ImageNet&#30340;&#39044;&#35757;&#32451;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#21307;&#30103;AI&#27169;&#22411;&#20013;&#22686;&#24378;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#24050;&#25104;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#23398;&#20064;&#24378;&#22823;&#29305;&#24449;&#30340;&#26426;&#20250;&#65292;&#20174;&#32780;&#21487;&#20197;&#32469;&#36807;&#32321;&#37325;&#30340;&#26631;&#27880;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;SSL&#39044;&#35757;&#32451;&#22312;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#26159;&#21542;&#21487;&#20197;&#24212;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#65292;&#24182;&#19982;&#38750;&#21307;&#23398;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#24182;&#26681;&#25454;&#20197;&#19979;&#26041;&#24335;&#21021;&#22987;&#21270;&#20854;&#26435;&#37325;&#65306;&#65288;i&#65289;&#22522;&#20110;&#33258;&#28982;&#22270;&#20687;&#30340;SSL&#39044;&#35757;&#32451;&#65288;DINOv2&#65289;&#12289;&#65288;ii&#65289;&#22522;&#20110;&#33258;&#28982;&#22270;&#20687;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;ImageNet&#25968;&#25454;&#38598;&#65289;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#22522;&#20110;MIMIC-CXR&#25968;&#25454;&#24211;&#20013;&#30340;&#33016;&#37096;X&#23556;&#32447;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#20845;&#20010;&#20840;&#29699;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;800,000&#22810;&#24352;&#33016;&#37096;X&#23556;&#32447;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35786;&#26029;&#20102;20&#22810;&#31181;&#19981;&#21516;&#30340;&#24433;&#20687;&#25152;&#35265;&#12290;&#25105;&#20204;&#30340;SSL&#39044;&#35757;&#32451;&#22312;&#32463;&#36807;&#31579;&#36873;&#30340;&#22270;&#20687;&#19978;&#19981;&#20165;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#22522;&#20110;ImageNet&#30340;&#39044;&#35757;&#32451;&#65288;&#23545;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;P&lt;0.001&#65289;&#65292;&#32780;&#19988;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#36824;&#36229;&#36807;&#20102;&#22522;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training datasets, like ImageNet, have become the gold standard in medical image analysis. However, the emergence of self-supervised learning (SSL), which leverages unlabeled data to learn robust features, presents an opportunity to bypass the intensive labeling process. In this study, we explored if SSL for pre-training on non-medical images can be applied to chest radiographs and how it compares to supervised pre-training on non-medical images and on medical images. We utilized a vision transformer and initialized its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on chest radiographs from the MIMIC-CXR database. We tested our approach on over 800,000 chest radiographs from six large global datasets, diagnosing more than 20 different imaging findings. Our SSL pre-training on curated images not only outperformed ImageNet-based pre-training (P&lt;0.001 for all datasets) but, in cert
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23384;&#22312;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#21253;&#25324;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#19981;&#36275;&#12289;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#19981;&#36275;&#21644;&#19981;&#36866;&#24212;&#19981;&#30830;&#23450;&#29615;&#22659;&#30340;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#21487;&#20449;&#24230;&#32593;&#32476;&#30340;&#35774;&#35745;&#32423;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03666</link><description>&lt;p&gt;
&#26550;&#36215;&#21487;&#20449;&#24230;&#19982;&#24320;&#25918;&#19990;&#30028;&#23398;&#20064;&#30340;&#26725;&#26753;&#65306;&#19968;&#31181;&#25506;&#32034;&#24615;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness. (arXiv:2308.03666v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03666
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23384;&#22312;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#21253;&#25324;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#19981;&#36275;&#12289;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#19981;&#36275;&#21644;&#19981;&#36866;&#24212;&#19981;&#30830;&#23450;&#29615;&#22659;&#30340;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#21487;&#20449;&#24230;&#32593;&#32476;&#30340;&#35774;&#35745;&#32423;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30740;&#31350;&#20154;&#21592;&#21162;&#21147;&#32553;&#23567;&#26426;&#22120;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36890;&#36807;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25105;&#20204;&#24517;&#39035;&#35748;&#35782;&#21040;&#21487;&#20449;&#24230;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#37325;&#35201;&#24615;&#65292;&#22312;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#23545;&#27599;&#20010;&#20154;&#37117;&#24050;&#32463;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23384;&#22312;&#20960;&#20010;&#25361;&#25112;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#20449;&#20219;&#21361;&#26426;&#65306;1&#65289;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#35299;&#37322;&#19981;&#36275;&#65307;2&#65289;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#19981;&#36275;&#65307;3&#65289;&#23545;&#19981;&#30830;&#23450;&#29615;&#22659;&#30340;&#36866;&#24212;&#33021;&#21147;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31070;&#32463;&#31243;&#24207;&#65292;&#29992;&#20110;&#26550;&#36215;&#21487;&#20449;&#24230;&#19982;&#24320;&#25918;&#19990;&#30028;&#23398;&#20064;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20174;&#21333;&#27169;&#24577;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#22330;&#26223;&#65292;&#20197;&#20379;&#35835;&#32773;&#20351;&#29992;&#12290;1&#65289;&#20026;&#20102;&#22686;&#24378;&#35774;&#35745;&#32423;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#21046;&#20102;&#20855;&#26377;&#29305;&#23450;&#29289;&#29702;&#21547;&#20041;&#30340;&#21487;&#20449;&#32593;&#32476;&#65307;2&#65289;&#28982;&#21518;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#23398;&#20064;&#27491;&#21017;&#21270;&#22120;&#35774;&#35745;&#29615;&#22659;&#31119;&#31049;&#20219;&#21153;&#25509;&#21475;&#65292;&#20197;&#25913;&#21892;&#21487;&#20449;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As researchers strive to narrow the gap between machine intelligence and human through the development of artificial intelligence technologies, it is imperative that we recognize the critical importance of trustworthiness in open-world, which has become ubiquitous in all aspects of daily life for everyone. However, several challenges may create a crisis of trust in current artificial intelligence systems that need to be bridged: 1) Insufficient explanation of predictive results; 2) Inadequate generalization for learning models; 3) Poor adaptability to uncertain environments. Consequently, we explore a neural program to bridge trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. 1) To enhance design-level interpretability, we first customize trustworthy networks with specific physical meanings; 2) We then design environmental well-being task-interfaces via flexible learning regularizers for improving the generalization of trustworthy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#22270;&#20687;&#27169;&#22411;&#30340;&#20840;&#23616;k&#31354;&#38388;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#21644;k&#31354;&#38388;&#25554;&#20540;&#65292;&#20351;&#29992;Transformer&#32593;&#32476;&#26469;&#23398;&#20064;2D+t k&#31354;&#38388;&#30340;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;k&#31354;&#38388;&#36845;&#20195;&#32454;&#21270;&#27169;&#22359;&#26469;&#22686;&#24378;&#39640;&#39057;&#25104;&#20998;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.12672</link><description>&lt;p&gt;
&#21160;&#24577;MRI&#37325;&#24314;&#20013;&#22522;&#20110;&#36974;&#34109;&#22270;&#20687;&#27169;&#22411;&#30340;&#20840;&#23616;k&#31354;&#38388;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling. (arXiv:2307.12672v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#22270;&#20687;&#27169;&#22411;&#30340;&#20840;&#23616;k&#31354;&#38388;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#21644;k&#31354;&#38388;&#25554;&#20540;&#65292;&#20351;&#29992;Transformer&#32593;&#32476;&#26469;&#23398;&#20064;2D+t k&#31354;&#38388;&#30340;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;k&#31354;&#38388;&#36845;&#20195;&#32454;&#21270;&#27169;&#22359;&#26469;&#22686;&#24378;&#39640;&#39057;&#25104;&#20998;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#65292;&#30001;&#20110;&#25195;&#25551;&#26102;&#38388;&#26377;&#38480;&#65292;&#36890;&#24120;&#20250;&#23545;k&#31354;&#38388;&#36827;&#34892;&#27424;&#37319;&#26679;&#65292;&#23548;&#33268;&#22270;&#20687;&#39046;&#22495;&#20013;&#20986;&#29616;&#20266;&#24433;&#12290;&#22240;&#27492;&#65292;&#21160;&#24577;MR&#37325;&#24314;&#19981;&#20165;&#38656;&#35201;&#24314;&#27169;k&#31354;&#38388;x&#21644;y&#26041;&#21521;&#30340;&#31354;&#38388;&#39057;&#29575;&#20998;&#37327;&#65292;&#36824;&#38656;&#35201;&#32771;&#34385;&#26102;&#38388;&#20887;&#20313;&#12290;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#20381;&#36182;&#20110;&#22270;&#20687;&#22495;&#27491;&#21017;&#21270;&#22120;&#65288;&#20808;&#39564;&#65289;&#36827;&#34892;MR&#37325;&#24314;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#33719;&#21462;&#20613;&#37324;&#21494;&#21464;&#25442;&#22270;&#20687;&#20043;&#21069;&#23545;&#27424;&#37319;&#26679;&#30340;k&#31354;&#38388;&#36827;&#34892;&#25554;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#19982;k&#31354;&#38388;&#25554;&#20540;&#30456;&#36830;&#25509;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;k&#31354;&#38388;&#20840;&#23616;&#25554;&#20540;&#32593;&#32476;&#65292;&#31216;&#20026;k-GIN&#12290;&#25105;&#20204;&#30340;k-GIN&#23398;&#20064;&#20102;2D+t k&#31354;&#38388;&#30340;&#20302;&#39057;&#21644;&#39640;&#39057;&#25104;&#20998;&#20043;&#38388;&#30340;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#29992;&#20110;&#25554;&#20540;&#26410;&#37319;&#26679;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;k&#31354;&#38388;&#36845;&#20195;&#32454;&#21270;&#27169;&#22359;&#65288;k-IRM&#65289;&#65292;&#20197;&#22686;&#24378;&#39640;&#39057;&#25104;&#20998;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;92&#20010;&#20869;&#37096;2D+t
&lt;/p&gt;
&lt;p&gt;
In dynamic Magnetic Resonance Imaging (MRI), k-space is typically undersampled due to limited scan time, resulting in aliasing artifacts in the image domain. Hence, dynamic MR reconstruction requires not only modeling spatial frequency components in the x and y directions of k-space but also considering temporal redundancy. Most previous works rely on image-domain regularizers (priors) to conduct MR reconstruction. In contrast, we focus on interpolating the undersampled k-space before obtaining images with Fourier transform. In this work, we connect masked image modeling with k-space interpolation and propose a novel Transformer-based k-space Global Interpolation Network, termed k-GIN. Our k-GIN learns global dependencies among low- and high-frequency components of 2D+t k-space and uses it to interpolate unsampled data. Further, we propose a novel k-space Iterative Refinement Module (k-IRM) to enhance the high-frequency components learning. We evaluate our approach on 92 in-house 2D+t 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;JoinGym&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26597;&#35810;&#20248;&#21270;&#29615;&#22659;&#12290;&#36890;&#36807;&#23558;&#21152;&#20837;&#39034;&#24207;&#36873;&#25321;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29616;&#65292;&#35813;&#23454;&#29616;&#23436;&#20840;&#22522;&#20110;&#31163;&#32447;&#36319;&#36394;&#65292;&#24182;&#19988;&#26080;&#38656;&#35774;&#32622;&#31995;&#32479;&#21363;&#21487;&#36827;&#34892;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;3300&#20010;&#26032;SQL&#26597;&#35810;&#30340;&#25152;&#26377;&#21487;&#33021;&#21152;&#20837;&#36861;&#36394;&#12290;</title><link>http://arxiv.org/abs/2307.11704</link><description>&lt;p&gt;
JoinGym: &#19968;&#31181;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26597;&#35810;&#20248;&#21270;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning. (arXiv:2307.11704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JoinGym&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26597;&#35810;&#20248;&#21270;&#29615;&#22659;&#12290;&#36890;&#36807;&#23558;&#21152;&#20837;&#39034;&#24207;&#36873;&#25321;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29616;&#65292;&#35813;&#23454;&#29616;&#23436;&#20840;&#22522;&#20110;&#31163;&#32447;&#36319;&#36394;&#65292;&#24182;&#19988;&#26080;&#38656;&#35774;&#32622;&#31995;&#32479;&#21363;&#21487;&#36827;&#34892;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;3300&#20010;&#26032;SQL&#26597;&#35810;&#30340;&#25152;&#26377;&#21487;&#33021;&#21152;&#20837;&#36861;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JoinGym&#65292;&#19968;&#31181;&#39640;&#25928;&#19988;&#36731;&#37327;&#32423;&#30340;&#24378;&#21270;&#23398;&#20064;&#26597;&#35810;&#20248;&#21270;&#29615;&#22659;&#12290;&#21152;&#20837;&#39034;&#24207;&#36873;&#25321;&#65288;JOS&#65289;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;NP-hard&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#29992;&#20110;&#25968;&#25454;&#24211;&#26597;&#35810;&#20248;&#21270;&#65292;&#21487;&#20316;&#20026;RL&#31639;&#27861;&#27867;&#21270;&#33021;&#21147;&#30340;&#23454;&#38469;&#27979;&#35797;&#24179;&#21488;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#23558;&#24038;&#28145;&#21644;&#32321;&#33538;&#30340;JOS&#38382;&#39064;&#30340;&#27599;&#20010;&#21464;&#31181;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#25552;&#20379;&#31526;&#21512;&#26631;&#20934;Gymnasium API&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#24378;&#35843;&#25105;&#20204;&#30340;&#23454;&#29616;JoinGym&#23436;&#20840;&#22522;&#20110;&#25152;&#26377;&#21487;&#33021;&#36830;&#25509;&#30340;&#31163;&#32447;&#36861;&#36394;&#65292;&#20351;RL&#20174;&#19994;&#32773;&#33021;&#22815;&#36731;&#26494;&#24555;&#36895;&#22320;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#25968;&#25454;&#31649;&#29702;&#38382;&#39064;&#19978;&#27979;&#35797;&#20182;&#20204;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#35774;&#32622;&#20219;&#20309;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20174;IMDB&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;3300&#20010;&#26032;SQL&#26597;&#35810;&#30340;&#25152;&#26377;&#21487;&#33021;&#36830;&#25509;&#36861;&#36394;&#12290;&#22312;&#23545;&#27969;&#34892;&#30340;RL&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#33267;&#23569;&#26377;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present \textsc{JoinGym}, an efficient and lightweight query optimization environment for reinforcement learning (RL). Join order selection (JOS) is a classic NP-hard combinatorial optimization problem from database query optimization and can serve as a practical testbed for the generalization capabilities of RL algorithms. We describe how to formulate each of the left-deep and bushy variants of the JOS problem as a Markov Decision Process (MDP), and we provide an implementation adhering to the standard Gymnasium API. We highlight that our implementation \textsc{JoinGym} is completely based on offline traces of all possible joins, which enables RL practitioners to easily and quickly test their methods on a realistic data management problem without needing to setup any systems. Moreover, we also provide all possible join traces on $3300$ novel SQL queries generated from the IMDB dataset. Upon benchmarking popular RL algorithms, we find that at least one method can obta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.08813</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#27604;&#36739;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge. (arXiv:2307.08813v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21462;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#21644;&#36890;&#36335;&#30693;&#35782;&#23545;&#20110;&#25581;&#31034;&#29983;&#29289;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#30740;&#31350;&#29983;&#29289;&#21151;&#33021;&#21644;&#22797;&#26434;&#30142;&#30149;&#30340;&#22522;&#26412;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#26469;&#33258;&#25991;&#29486;&#21644;&#20854;&#20182;&#28304;&#30340;&#31574;&#21010;&#29983;&#29289;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#19981;&#23436;&#25972;&#19988;&#32500;&#25252;&#24037;&#20316;&#32321;&#37325;&#65292;&#22240;&#27492;&#38656;&#35201;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#33258;&#21160;&#20174;&#30456;&#20851;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#36825;&#20123;&#30693;&#35782;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#12289;&#36890;&#36335;&#21644;&#22522;&#22240;&#35843;&#25511;&#20851;&#31995;&#31561;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#37325;&#35201;&#30340;&#21457;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#26410;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#38142;&#25509;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding protein interactions and pathway knowledge is crucial for unraveling the complexities of living systems and investigating the underlying mechanisms of biological functions and complex diseases. While existing databases provide curated biological data from literature and other sources, they are often incomplete and their maintenance is labor-intensive, necessitating alternative approaches. In this study, we propose to harness the capabilities of large language models to address these issues by automatically extracting such knowledge from the relevant scientific literature. Toward this goal, in this work, we investigate the effectiveness of different large language models in tasks that involve recognizing protein interactions, pathways, and gene regulatory relations. We thoroughly evaluate the performance of various models, highlight the significant findings, and discuss both the future opportunities and the remaining challenges associated with this approach. The code and d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#38598;&#20013;&#22312;&#35821;&#35328;&#32452;&#20214;&#19978;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07063</link><description>&lt;p&gt;
&#20351;&#29992;&#35299;&#32806;&#30340;&#35821;&#35328;&#39044;&#35757;&#32451;&#20026;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#24341;&#20837;&#24341;&#23548;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Vision-Language Learning with Decoupled Language Pre-training. (arXiv:2307.07063v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#38598;&#20013;&#22312;&#35821;&#35328;&#32452;&#20214;&#19978;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20248;&#21270;&#20923;&#32467;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36164;&#28304;&#23494;&#38598;&#22411;&#35270;&#35273;&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#12290;&#24403;&#21069;&#30340;&#33539;&#24335;&#20351;&#29992;&#35270;&#35273;&#29305;&#24449;&#20316;&#20026;&#25552;&#31034;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#65292;&#37325;&#28857;&#26159;&#30830;&#23450;&#19982;&#30456;&#24212;&#25991;&#26412;&#26368;&#30456;&#20851;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#38598;&#20013;&#22312;&#35821;&#35328;&#32452;&#20214;&#19978;&#65292;&#20855;&#20307;&#26159;&#30830;&#23450;&#19982;&#35270;&#35273;&#29305;&#24449;&#23545;&#40784;&#30340;&#26368;&#20339;&#25552;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt-Transformer&#65288;P-Former&#65289;&#65292;&#19968;&#31181;&#21487;&#20197;&#39044;&#27979;&#36825;&#20123;&#29702;&#24819;&#25552;&#31034;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20165;&#22312;&#35821;&#35328;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#30340;&#38656;&#35201;&#12290;&#36825;&#31181;&#31574;&#30053;&#23558;&#31471;&#21040;&#31471;&#30340;VL&#35757;&#32451;&#36807;&#31243;&#24039;&#22937;&#22320;&#20998;&#20026;&#20102;&#39069;&#22806;&#30340;&#29420;&#31435;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#31283;&#20581;&#30340;&#22270;&#20687;&#21040;&#25991;&#26412;&#22522;&#32447;&#65288;BLIP-2&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#20351;&#29992;4M&#25110;129M&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel methodology aimed at optimizing the application of frozen large language models (LLMs) for resource-intensive vision-language (VL) pre-training. The current paradigm uses visual features as prompts to guide language models, with a focus on determining the most relevant visual features for corresponding text. Our approach diverges by concentrating on the language component, specifically identifying the optimal prompts to align with visual features. We introduce the Prompt-Transformer (P-Former), a model that predicts these ideal prompts, which is trained exclusively on linguistic data, bypassing the need for image-text pairings. This strategy subtly bifurcates the end-to-end VL training process into an additional, separate stage. Our experiments reveal that our framework significantly enhances the performance of a robust image-to-text baseline (BLIP-2), and effectively narrows the performance gap between models trained with either 4M or 129M image-text pairs. Importan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#24615;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#24402;&#22240;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#23646;&#24615;&#21270;&#25552;&#31034;&#23545;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#20851;&#20110;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#31995;&#32479;&#24615;&#20559;&#24046;&#23384;&#22312;&#20110;&#29983;&#25104;&#25968;&#25454;&#20013;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15895</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#65306;&#22810;&#26679;&#24615;&#21644;&#20559;&#24046;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias. (arXiv:2306.15895v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#24615;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#24402;&#22240;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#23646;&#24615;&#21270;&#25552;&#31034;&#23545;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#20851;&#20110;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#31995;&#32479;&#24615;&#20559;&#24046;&#23384;&#22312;&#20110;&#29983;&#25104;&#25968;&#25454;&#20013;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#31867;&#21035;&#26465;&#20214;&#25552;&#31034;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#29983;&#25104;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#32487;&#25215;&#20102;LLM&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#23646;&#24615;&#30340;&#25552;&#31034;(&#20363;&#22914;&#25351;&#23450;&#38271;&#24230;&#21644;&#39118;&#26684;&#31561;&#23646;&#24615;)&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#65292;&#36825;&#26377;&#28508;&#21147;&#20135;&#29983;&#22810;&#26679;&#21644;&#24402;&#22240;&#30340;&#29983;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#20855;&#26377;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23646;&#24615;&#21270;&#25552;&#31034;&#22312;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#31616;&#21333;&#30340;&#31867;&#21035;&#26465;&#20214;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#39033;&#21253;&#25324;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#31561;&#20851;&#38190;&#26041;&#38754;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#39318;&#20808;&#65292;&#31995;&#32479;&#24615;&#20559;&#24046;&#22312;&#29983;&#25104;&#25968;&#25454;&#20013;&#23384;&#22312;&#65307;&#20854;&#27425;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65307;&#26368;&#21518;&#65292;&#36827;&#34892;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, sy
&lt;/p&gt;</description></item><item><title>&#26377;&#38480;&#20869;&#23384;&#36138;&#23146;&#25311;&#29275;&#39039;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26631;&#20934;&#25311;&#29275;&#39039;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#21644;&#20869;&#23384;&#38656;&#27714;&#36807;&#39640;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#26377;&#20855;&#26377;&#38750;&#28176;&#36827;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.15444</link><description>&lt;p&gt;
&#26377;&#38480;&#20869;&#23384;&#36138;&#23146;&#25311;&#29275;&#39039;&#26041;&#27861;&#19982;&#38750;&#28176;&#36827;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Limited-Memory Greedy Quasi-Newton Method with Non-asymptotic Superlinear Convergence Rate. (arXiv:2306.15444v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15444
&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#20869;&#23384;&#36138;&#23146;&#25311;&#29275;&#39039;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26631;&#20934;&#25311;&#29275;&#39039;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#21644;&#20869;&#23384;&#38656;&#27714;&#36807;&#39640;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#26377;&#20855;&#26377;&#38750;&#28176;&#36827;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#28176;&#36827;&#25910;&#25947;&#20998;&#26512;&#34920;&#26126;&#65292;&#25311;&#29275;&#39039;&#26041;&#27861;&#30340;&#26174;&#24335;&#36229;&#32447;&#24615;&#36895;&#29575;&#20026;O$((1/\sqrt{t})^t)$&#12290;&#28982;&#32780;&#65292;&#33719;&#24471;&#36825;&#19968;&#36895;&#29575;&#30340;&#26041;&#27861;&#23384;&#22312;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#32570;&#28857;&#65306;&#23427;&#20204;&#38656;&#35201;&#23384;&#20648;&#20808;&#21069;&#30340;&#40657;&#22622;&#36817;&#20284;&#30697;&#38453;&#65292;&#25110;&#32773;&#23384;&#20648;&#25152;&#26377;&#36807;&#21435;&#30340;&#26354;&#29575;&#20449;&#24687;&#20197;&#24418;&#25104;&#24403;&#21069;&#30340;&#40657;&#22622;&#36870;&#36817;&#20284;&#12290;&#26377;&#38480;&#20869;&#23384;&#30340;&#25311;&#29275;&#39039;&#26041;&#27861;&#65288;&#22914;&#33879;&#21517;&#30340;L-BFGS&#65289;&#36890;&#36807;&#21033;&#29992;&#26377;&#38480;&#31383;&#21475;&#30340;&#36807;&#21435;&#26354;&#29575;&#20449;&#24687;&#26469;&#26500;&#36896;&#40657;&#22622;&#36870;&#36817;&#20284;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#21644;&#23384;&#20648;&#38656;&#27714;&#20026;O$(\tau d)$&#65292;&#20854;&#20013;$\tau \le d$ &#26159;&#31383;&#21475;&#30340;&#22823;&#23567;&#65292;$d$ &#26159;&#38382;&#39064;&#30340;&#32500;&#25968;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#26631;&#20934;&#25311;&#29275;&#39039;&#26041;&#27861;&#30340;O$(d^2)$ &#35745;&#31639;&#25104;&#26412;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27809;&#26377;&#32467;&#26524;&#34920;&#26126;&#26377;&#38480;&#20869;&#23384;&#25311;&#29275;&#39039;&#26041;&#27861;&#23384;&#22312;&#38750;&#28176;&#36827;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-asymptotic convergence analysis of quasi-Newton methods has gained attention with a landmark result establishing an explicit superlinear rate of O$((1/\sqrt{t})^t)$. The methods that obtain this rate, however, exhibit a well-known drawback: they require the storage of the previous Hessian approximation matrix or instead storing all past curvature information to form the current Hessian inverse approximation. Limited-memory variants of quasi-Newton methods such as the celebrated L-BFGS alleviate this issue by leveraging a limited window of past curvature information to construct the Hessian inverse approximation. As a result, their per iteration complexity and storage requirement is O$(\tau d)$ where $\tau \le d$ is the size of the window and $d$ is the problem dimension reducing the O$(d^2)$ computational cost and memory requirement of standard quasi-Newton methods. However, to the best of our knowledge, there is no result showing a non-asymptotic superlinear convergence rate for a
&lt;/p&gt;</description></item><item><title>DORSal&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#65292;&#24182;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.08068</link><description>&lt;p&gt;
DORSal: &#22522;&#20110;&#25193;&#25955;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$. (arXiv:2306.08068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08068
&lt;/p&gt;
&lt;p&gt;
DORSal&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#65292;&#24182;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#19977;&#32500;&#22330;&#26223;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#36328;&#22823;&#37327;&#19981;&#21516;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#25193;&#23637;&#34920;&#31034;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#21644;&#29289;&#20307;&#30340;&#27867;&#21270;&#65292;&#20165;&#36890;&#36807;&#21333;&#20010;&#25110;&#23569;&#25968;&#22270;&#20687;&#28210;&#26579;&#26032;&#35270;&#22270;&#65292;&#20197;&#21450;&#25903;&#25345;&#32534;&#36753;&#30340;&#21487;&#25511;&#22330;&#26223;&#29983;&#25104;&#29616;&#22312;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#32852;&#21512;&#35757;&#32451;&#22823;&#37327;&#22330;&#26223;&#36890;&#24120;&#20250;&#22312;&#28210;&#26579;&#36136;&#37327;&#19978;&#22949;&#21327;&#65292;&#32780;&#19982;&#21333;&#20010;&#22330;&#26223;&#20248;&#21270;&#27169;&#22411;&#65288;&#22914;NeRF&#65289;&#30456;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#20351;&#19977;&#32500;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#20855;&#22791;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DORSal&#65292;&#23427;&#22522;&#20110;&#25193;&#25955;&#35270;&#39057;&#26550;&#26500;&#65292;&#20026;&#22522;&#20110;&#29289;&#20307;&#20013;&#24515;&#30340;&#22330;&#26223;&#25554;&#27133;&#34920;&#31034;&#30340;&#19977;&#32500;&#22330;&#26223;&#29983;&#25104;&#25552;&#20379;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#22312;&#22797;&#26434;&#30340;&#21512;&#25104;&#22810;&#29289;&#20307;&#22330;&#26223;&#21644;&#29616;&#23454;&#19990;&#30028;&#22823;&#35268;&#27169;&#34903;&#26223;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22330;&#26223;&#26032;&#35270;&#22270;&#65292;&#21516;&#26102;&#25903;&#25345;&#29289;&#20307;&#32423;&#21035;&#30340;&#32534;&#36753;&#65292;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#32441;&#29702;&#21644;&#21453;&#23556;&#31561;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in 3D scene understanding enables scalable learning of representations across large datasets of diverse scenes. As a consequence, generalization to unseen scenes and objects, rendering novel views from just a single or a handful of input images, and controllable scene generation that supports editing, is now possible. However, training jointly on a large number of scenes typically compromises rendering quality when compared to single-scene optimized models such as NeRFs. In this paper, we leverage recent progress in diffusion models to equip 3D scene representation learning models with the ability to render high-fidelity novel views, while retaining benefits such as object-level scene editing to a large degree. In particular, we propose DORSal, which adapts a video diffusion architecture for 3D scene generation conditioned on object-centric slot-based representations of scenes. On both complex synthetic multi-object scenes and on the real-world large-scale Street View d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#31283;&#20581;&#27169;&#25311;&#29983;&#25104;&#38543;&#26426;&#22270;&#24182;&#23558;&#21152;&#26435;&#25216;&#26415;&#32467;&#21512;UCB&#31639;&#27861;&#65292;&#20197;&#21327;&#20316;&#26041;&#24335;&#20943;&#23567;&#25972;&#20010;&#31995;&#32479;&#30340;&#24635;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2306.05579</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#38543;&#26426;&#20998;&#24067;&#30340;&#24322;&#26500;&#22870;&#21169;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with Heterogeneous Rewards. (arXiv:2306.05579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#31283;&#20581;&#27169;&#25311;&#29983;&#25104;&#38543;&#26426;&#22270;&#24182;&#23558;&#21152;&#26435;&#25216;&#26415;&#32467;&#21512;UCB&#31639;&#27861;&#65292;&#20197;&#21327;&#20316;&#26041;&#24335;&#20943;&#23567;&#25972;&#20010;&#31995;&#32479;&#30340;&#24635;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#36890;&#36807;&#30001;&#29615;&#22659;&#25552;&#20379;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#38543;&#26426;&#22270;&#36827;&#34892;&#36830;&#25509;&#12290;&#27599;&#20010;&#33218;&#30340;&#22870;&#21169;&#20998;&#24067;&#22240;&#23458;&#25143;&#32780;&#24322;&#65292;&#24182;&#19988;&#22870;&#21169;&#26159;&#26681;&#25454;&#21253;&#25324;&#20122;&#25351;&#25968;&#21644;&#20122;&#39640;&#26031;&#20998;&#24067;&#22312;&#20869;&#30340;&#20998;&#24067;&#65292;&#30001;&#29615;&#22659;&#29420;&#31435;&#22320;&#38543;&#26102;&#38388;&#29983;&#25104;&#30340;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#37117;&#20250;&#25289;&#21160;&#19968;&#20010;&#33218;&#65292;&#24182;&#26681;&#25454;&#30001;&#29615;&#22659;&#25552;&#20379;&#30340;&#22270;&#19982;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#21327;&#20316;&#26469;&#20943;&#23567;&#25972;&#20010;&#31995;&#32479;&#30340;&#24635;&#36951;&#25022;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#39318;&#20808;&#25552;&#20379;&#20102;&#20351;&#29992;&#24555;&#36895;&#28151;&#21512;&#39532;&#23572;&#21487;&#22827;&#38142;&#25110;&#38543;&#26426;&#22270;&#27169;&#22411;&#29983;&#25104;&#38543;&#26426;&#22270;&#30340;&#31283;&#20581;&#20223;&#30495;&#26041;&#27861;&#65292;&#28982;&#21518;&#23558;&#22522;&#20110;&#24179;&#22343;&#19968;&#33268;&#24615;&#26041;&#27861;&#21644;&#26032;&#25552;&#20986;&#30340;&#21152;&#26435;&#25216;&#26415;&#20197;&#21450;&#19978;&#32622;&#20449;&#38480;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;UCB&#31867;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#32771;&#34385;&#21040;&#20102;&#22270;&#24418;&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#28040;&#38500;&#20102;&#38480;&#21046;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a decentralized multi-agent multi-armed bandit problem in which multiple clients are connected by time dependent random graphs provided by an environment. The reward distributions of each arm vary across clients and rewards are generated independently over time by an environment based on distributions that include both sub-exponential and sub-gaussian distributions. Each client pulls an arm and communicates with neighbors based on the graph provided by the environment. The goal is to minimize the overall regret of the entire system through collaborations. To this end, we introduce a novel algorithmic framework, which first provides robust simulation methods for generating random graphs using rapidly mixing Markov chains or the random graph model, and then combines an averaging-based consensus approach with a newly proposed weighting technique and the upper confidence bound to deliver a UCB-type solution. Our algorithms account for the randomness in the graphs, removing the con
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24207;&#25968;&#21183;&#20989;&#25968;&#30340;&#29609;&#23478;&#35780;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#36870;&#26144;&#23556;&#30340;&#23884;&#22871;&#35745;&#31639;&#65292;&#33021;&#22815;&#20445;&#25345;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05366</link><description>&lt;p&gt;
&#24207;&#25968;&#21183;&#20989;&#25968;&#30340;&#29609;&#23478;&#35780;&#32423;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ordinal Potential-based Player Rating. (arXiv:2306.05366v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05366
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24207;&#25968;&#21183;&#20989;&#25968;&#30340;&#29609;&#23478;&#35780;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#36870;&#26144;&#23556;&#30340;&#23884;&#22871;&#35745;&#31639;&#65292;&#33021;&#22815;&#20445;&#25345;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#23545;&#20110;&#20219;&#24847;&#32431;&#31574;&#30053;$x$&#12289;$y$&#21644;$z$&#65292;&#22914;&#26524;$x$&#27604;$y$&#26356;&#22909;&#65292;$y$&#27604;$z$&#26356;&#22909;&#65292;&#21017;$x$&#27604;$z$&#26356;&#22909;&#65292;&#21017;&#20004;&#20010;&#23545;&#31216;&#30340;&#38646;&#21644;&#21338;&#24328;&#26159;&#21487;&#20256;&#36882;&#30340;&#12290;&#26368;&#36817;&#35266;&#23519;&#21040;&#65292;Elo&#35780;&#32423;&#26410;&#33021;&#20445;&#25345;&#31574;&#30053;&#20043;&#38388;&#30340;&#20256;&#36882;&#20851;&#31995;&#65292;&#22240;&#27492;&#19981;&#33021;&#27491;&#30830;&#25552;&#21462;&#28216;&#25103;&#30340;&#20256;&#36882;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#34920;&#26126;&#24403;&#22312;&#27491;&#30830;&#30340;&#31354;&#38388;&#20013;&#35745;&#31639;Elo&#35780;&#32423;&#26102;&#65292;Elo&#35780;&#32423;&#30830;&#23454;&#33021;&#22815;&#20445;&#25345;&#20256;&#36882;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#21512;&#36866;&#30340;&#21487;&#36870;&#26144;&#23556;$\varphi$&#23558;&#28216;&#25103;&#24212;&#29992;&#20110;$\varphi$&#65292;&#28982;&#21518;&#35745;&#31639;Elo&#35780;&#32423;&#65292;&#26368;&#21518;&#36890;&#36807;&#24212;&#29992;$\varphi^{-1}$&#22238;&#21040;&#21407;&#22987;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;&#21487;&#20256;&#36882;&#28216;&#25103;&#30340;&#34920;&#24449;&#20026;&#21183;&#28216;&#25103;&#30340;&#19968;&#20010;&#24369;&#21464;&#20307;&#65292;&#20854;&#21183;&#20989;&#25968;&#26159;&#21152;&#24615;&#21487;&#20998;&#31163;&#30340;&#12290;&#21033;&#29992;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20256;&#36882;&#24207;&#25968;&#30340;&#27010;&#24565;&#65292;&#21363;&#23558;&#21487;&#20256;&#36882;&#28216;&#25103;&#30340;&#25910;&#30410;&#36716;&#21270;&#20026;&#20854;&#24046;&#24322;&#25152;&#38656;&#30340;&#26368;&#23567;&#21487;&#36870;&#26144;&#23556;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
A two-player symmetric zero-sum game is transitive if for any pure strategies $x$, $y$, $z$, if $x$ is better than $y$, and $y$ is better than $z$, then $x$ is better than $z$. It was recently observed that the Elo rating fails at preserving transitive relations among strategies and therefore cannot correctly extract the transitive component of a game. Our first contribution is to show that the Elo rating actually does preserve transitivity when computed in the right space. Precisely, using a suitable invertible mapping $\varphi$, we first apply $\varphi$ to the game, then compute Elo ratings, then go back to the original space by applying $\varphi^{-1}$. We provide a characterization of transitive games as a weak variant of ordinal potential games with additively separable potential functions. Leveraging this insight, we introduce the concept of transitivity order, the minimum number of invertible mappings required to transform the payoff of a transitive game into (differences of) its
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#37051;&#23884;&#20837;&#26041;&#27861;&#26469;&#20272;&#35745;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20256;&#26579;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ProEmb&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#21644;&#23545;&#25239;&#32593;&#32476;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#29983;&#25104;&#39640;&#32500;&#20195;&#29702;&#21464;&#37327;&#30340;&#24179;&#34913;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#26579;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.02479</link><description>&lt;p&gt;
&#20351;&#29992;&#36817;&#37051;&#23884;&#20837;&#20272;&#35745;&#20256;&#26579;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Contagion Effect Estimation Using Proximal Embeddings. (arXiv:2306.02479v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#37051;&#23884;&#20837;&#26041;&#27861;&#26469;&#20272;&#35745;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20256;&#26579;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ProEmb&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#21644;&#23545;&#25239;&#32593;&#32476;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#29983;&#25104;&#39640;&#32500;&#20195;&#29702;&#21464;&#37327;&#30340;&#24179;&#34913;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#26579;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#26579;&#25928;&#24212;&#25351;&#30340;&#26159;&#31038;&#20132;&#32593;&#32476;&#20013;&#21516;&#20276;&#34892;&#20026;&#23545;&#20010;&#20307;&#32467;&#26524;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#22312;&#35266;&#23519;&#30740;&#31350;&#20013;&#20272;&#35745;&#20256;&#26579;&#25928;&#24212;&#30340;&#26174;&#33879;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#27809;&#26377;&#26410;&#27979;&#37327;&#30340;&#28151;&#26434;&#22240;&#32032;&#65292;&#20294;&#30001;&#20110;&#28508;&#22312;&#30340;&#21516;&#36136;&#24615;&#65292;&#20256;&#26579;&#21487;&#33021;&#20250;&#34987;&#28151;&#28102;&#65306;&#21516;&#36136;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#20542;&#21521;&#20110;&#19982;&#20855;&#26377;&#30456;&#20284;&#23646;&#24615;&#30340;&#21516;&#20276;&#24314;&#31435;&#32852;&#31995;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#20114;&#30456;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#30456;&#20284;&#30340;&#34892;&#20026;&#12290;&#35299;&#20915;&#28508;&#22312;&#21516;&#36136;&#24615;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#32771;&#34385;&#26410;&#35266;&#23519;&#28151;&#26434;&#22240;&#32032;&#30340;&#20195;&#29702;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#39640;&#32500;&#20195;&#29702;&#21464;&#37327;&#26102;&#65292;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#20256;&#26579;&#25928;&#24212;&#20272;&#35745;&#30340;&#20005;&#37325;&#20559;&#24046;&#65292;&#27491;&#22914;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#28436;&#31034;&#30340;&#37027;&#26679;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#36817;&#37051;&#23884;&#20837;&#65288;ProEmb&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#21644;&#23545;&#25239;&#32593;&#32476;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#29983;&#25104;&#19981;&#21516;&#22788;&#29702;&#32452;&#39640;&#32500;&#20195;&#29702;&#21464;&#37327;&#30340;&#24179;&#34913;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#19988;&#22312;&#22240;&#26524;&#25512;&#35770;&#20013;&#32771;&#34385;&#20102;&#20256;&#26579;&#25928;&#24212;&#30340;&#20272;&#35745;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contagion effect refers to the causal effect of peers' behavior on the outcome of an individual in social networks. While prominent methods for estimating contagion effects in observational studies often assume that there are no unmeasured confounders, contagion can be confounded due to latent homophily: nodes in a homophilous network tend to have ties to peers with similar attributes and can behave similarly without influencing one another. One way to account for latent homophily is by considering proxies for the unobserved confounders. However, in the presence of high-dimensional proxies, proxy-based methods can lead to substantially biased estimation of contagion effects, as we demonstrate in this paper. To tackle this issue, we introduce the novel Proximal Embeddings (ProEmb), a framework which integrates Variational Autoencoders (VAEs) and adversarial networks to generate balanced low-dimensional representations of high-dimensional proxies for different treatment groups and identi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;MuZero&#31639;&#27861;&#65292;&#21457;&#29616;&#23427;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#25512;&#24191;&#21040;&#35780;&#20272;&#26410;&#35265;&#31574;&#30053;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#24403;&#21069;&#31574;&#30053;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.00840</link><description>&lt;p&gt;
MuZero&#23398;&#21040;&#20102;&#20160;&#20040;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
What model does MuZero learn?. (arXiv:2306.00840v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;MuZero&#31639;&#27861;&#65292;&#21457;&#29616;&#23427;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#25512;&#24191;&#21040;&#35780;&#20272;&#26410;&#35265;&#31574;&#30053;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#24403;&#21069;&#31574;&#30053;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#26395;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#26377;&#21487;&#33021;&#20174;&#22797;&#26434;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#32039;&#20945;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#35268;&#21010;&#33021;&#21147;&#30340;&#25552;&#21319;&#24403;&#21069;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;MuZero&#36825;&#20010;&#33879;&#21517;&#30340;&#22522;&#20110;&#28145;&#24230;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#22312;&#23454;&#29616;&#20540;&#31561;&#20215;&#27169;&#22411;&#30340;&#23398;&#20064;&#30446;&#26631;&#19978;&#30340;&#25104;&#23601;&#20197;&#21450;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#23545;&#31574;&#30053;&#25913;&#36827;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#35832;&#22810;&#20854;&#20182;&#35266;&#28857;&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;MuZero&#23398;&#21040;&#30340;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#35780;&#20272;&#26410;&#35265;&#31574;&#30053;&#65292;&#36825;&#38480;&#21046;&#20102;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#35268;&#21010;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#24403;&#21069;&#31574;&#30053;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning has drawn considerable interest in recent years, given its promise to improve sample efficiency. Moreover, when using deep-learned models, it is potentially possible to learn compact models from complex sensor data. However, the effectiveness of these learned models, particularly their capacity to plan, i.e., to improve the current policy, remains unclear. In this work, we study MuZero, a well-known deep model-based reinforcement learning algorithm, and explore how far it achieves its learning objective of a value-equivalent model and how useful the learned models are for policy improvement. Amongst various other insights, we conclude that the model learned by MuZero cannot effectively generalize to evaluate unseen policies, which limits the extent to which we can additionally improve the current policy by planning with the model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;Bayesian&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26469;&#21387;&#32553;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270; $\beta$-ELBO &#30452;&#25509;&#20248;&#21270;&#30721;-&#22833;&#30495;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972; $\beta$ &#26469;&#38024;&#23545;&#32473;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#19981;&#21516;&#30340;&#30721;-&#22833;&#30495;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.19185</link><description>&lt;p&gt;
Bayesian&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#19979;&#30340;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Compression with Bayesian Implicit Neural Representations. (arXiv:2305.19185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;Bayesian&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26469;&#21387;&#32553;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270; $\beta$-ELBO &#30452;&#25509;&#20248;&#21270;&#30721;-&#22833;&#30495;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972; $\beta$ &#26469;&#38024;&#23545;&#32473;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#19981;&#21516;&#30340;&#30721;-&#22833;&#30495;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24120;&#35265;&#31867;&#22411;&#30340;&#25968;&#25454;&#21487;&#20197;&#34920;&#31034;&#20026;&#23558;&#22352;&#26631;&#26144;&#23556;&#21040;&#20449;&#21495;&#20540;&#30340;&#20989;&#25968;&#65292;&#20363;&#22914;&#22270;&#20687;&#20013;&#30340;&#20687;&#32032;&#20301;&#32622;&#21040;RGB&#20540;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#21151;&#33021;&#34920;&#31034;&#36827;&#34892;&#36229;&#25311;&#21512;&#65292;&#28982;&#21518;&#32534;&#30721;&#32593;&#32476;&#26435;&#37325;&#26469;&#21387;&#32553;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#37117;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#23558;&#31934;&#24230;&#37327;&#21270;&#21040;&#20302;&#27604;&#29305;&#20250;&#22823;&#24133;&#38477;&#20302;&#37325;&#26500;&#36136;&#37327;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36807;&#24230;&#25311;&#21512;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26469;&#21387;&#32553;&#36817;&#20284;&#21518;&#39564;&#26435;&#37325;&#26679;&#26412;&#65292;&#32780;&#19981;&#26159;&#37327;&#21270;&#21644;&#29109;&#32534;&#30721;&#23427;&#12290;&#35813;&#31574;&#30053;&#36890;&#36807;&#26368;&#23567;&#21270; $\beta$-ELBO &#30452;&#25509;&#20248;&#21270;&#30721;-&#22833;&#30495;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972; $\beta$ &#26469;&#38024;&#23545;&#32473;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#19981;&#21516;&#30340;&#30721;-&#22833;&#30495;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23398;&#20064;&#20808;&#39564;&#26435;&#37325;&#20998;&#24067;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#37319;&#29992;&#20027;&#21160;&#23610;&#23544;&#35843;&#25972;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#21344;&#29992;&#26102;&#38388;&#27010;&#24565;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25193;&#23637;&#20102;&#31163;&#25955;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;PG&#12289;TRPO&#21644;PPO&#26041;&#27861;&#65292;&#20026;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18901</link><description>&lt;p&gt;
&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization for Continuous Reinforcement Learning. (arXiv:2305.18901v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#21344;&#29992;&#26102;&#38388;&#27010;&#24565;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25193;&#23637;&#20102;&#31163;&#25955;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;PG&#12289;TRPO&#21644;PPO&#26041;&#27861;&#65292;&#20026;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#21644;&#31354;&#38388;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#37319;&#29992;&#25240;&#25187;&#22870;&#21169;&#21644;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#22522;&#26412;&#21160;&#24577;&#12290;&#22312;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21344;&#29992;&#26102;&#38388;&#30340;&#27010;&#24565;&#65288;&#29305;&#21035;&#26159;&#38024;&#23545;&#25240;&#25187;&#22870;&#21169;&#65289;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#23427;&#26469;&#23548;&#20986;&#24615;&#33021;&#24046;&#24322;&#21644;&#23616;&#37096;&#36924;&#36817;&#20844;&#24335;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#20123;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102; PG&#65288;&#31574;&#30053;&#26799;&#24230;&#65289;&#12289;TRPO&#65288;&#20449;&#20219;&#21306;&#22495;&#31574;&#30053;&#20248;&#21270;&#65289;&#21644; PPO&#65288;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65289;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#31163;&#25955;&#24378;&#21270;&#23398;&#20064;&#20013;&#26159;&#29087;&#30693;&#21644;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#22312;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#21457;&#23637;&#12290;&#36890;&#36807;&#25968;&#23383;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study reinforcement learning (RL) in the setting of continuous time and space, for an infinite horizon with a discounted objective and the underlying dynamics driven by a stochastic differential equation. Built upon recent advances in the continuous approach to RL, we develop a notion of occupation time (specifically for a discounted objective), and show how it can be effectively used to derive performance-difference and local-approximation formulas. We further extend these results to illustrate their applications in the PG (policy gradient) and TRPO/PPO (trust region policy optimization/ proximal policy optimization) methods, which have been familiar and powerful tools in the discrete RL setting but under-developed in continuous RL. Through numerical experiments, we demonstrate the effectiveness and advantages of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Koopa&#30340;Koopman&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#28388;&#27874;&#22120;&#23558;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#21464;&#20307;&#21644;&#26102;&#38388;&#19981;&#21464;&#25104;&#20998;&#20998;&#31163;&#20986;&#26469;&#65292;&#24182;&#20351;&#29992;Koopman&#31639;&#23376;&#20316;&#20026;&#32447;&#24615;&#34920;&#36848;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#28145;&#24230;&#39044;&#27979;&#27169;&#22411;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.18803</link><description>&lt;p&gt;
Koopa: &#20351;&#29992;Koopman&#39044;&#27979;&#22120;&#23398;&#20064;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors. (arXiv:2305.18803v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Koopa&#30340;Koopman&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#28388;&#27874;&#22120;&#23558;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#21464;&#20307;&#21644;&#26102;&#38388;&#19981;&#21464;&#25104;&#20998;&#20998;&#31163;&#20986;&#26469;&#65292;&#24182;&#20351;&#29992;Koopman&#31639;&#23376;&#20316;&#20026;&#32447;&#24615;&#34920;&#36848;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#28145;&#24230;&#39044;&#27979;&#27169;&#22411;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#20869;&#22312;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#23545;&#20110;&#28145;&#24230;&#39044;&#27979;&#27169;&#22411;&#32780;&#35328;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#20195;Koopman&#29702;&#35770;&#26469;&#22788;&#29702;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65292;&#35813;&#29702;&#35770;&#20174;&#26681;&#26412;&#19978;&#32771;&#34385;&#20102;&#24213;&#23618;&#30340;&#26102;&#38388;&#21464;&#21270;&#21160;&#24577;&#12290;&#21463;&#21040;&#25551;&#36848;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#30340;Koopman&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#20613;&#37324;&#21494;&#28388;&#27874;&#22120;&#23558;&#38750;&#24179;&#31283;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#21464;&#20307;&#21644;&#26102;&#38388;&#19981;&#21464;&#25104;&#20998;&#20998;&#31163;&#20986;&#26469;&#65292;&#24182;&#35774;&#35745;&#20102;Koopman&#39044;&#27979;&#22120;&#26469;&#25512;&#36827;&#21508;&#33258;&#30340;&#21160;&#24577;&#12290;&#25216;&#26415;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Koopa&#30340;&#26032;&#22411;Koopman&#39044;&#27979;&#22120;&#65292;&#23427;&#30001;&#21487;&#22534;&#21472;&#30340;&#22359;&#32452;&#25104;&#65292;&#23398;&#20064;&#20998;&#23618;&#21160;&#24577;&#12290;Koopa&#23547;&#25214;Koopman&#23884;&#20837;&#30340;&#27979;&#37327;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;Koopman&#31639;&#23376;&#20316;&#20026;&#38544;&#21547;&#36716;&#25442;&#30340;&#32447;&#24615;&#34920;&#36848;&#12290;&#20026;&#20102;&#22788;&#29702;&#20855;&#26377;&#24378;&#23616;&#37096;&#24615;&#30340;&#26102;&#38388;&#21464;&#20307;&#21160;&#24577;&#65292;Koopa&#22312;&#26102;&#38388;&#37051;&#22495;&#20013;&#35745;&#31639;&#19978;&#19979;&#25991;&#24863;&#30693;&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world time series are characterized by intrinsic non-stationarity that poses a principal challenge for deep forecasting models. While previous models suffer from complicated series variations induced by changing temporal distribution, we tackle non-stationary time series with modern Koopman theory that fundamentally considers the underlying time-variant dynamics. Inspired by Koopman theory of portraying complex dynamical systems, we disentangle time-variant and time-invariant components from intricate non-stationary series by Fourier Filter and design Koopman Predictor to advance respective dynamics forward. Technically, we propose Koopa as a novel Koopman forecaster composed of stackable blocks that learn hierarchical dynamics. Koopa seeks measurement functions for Koopman embedding and utilizes Koopman operators as linear portraits of implicit transition. To cope with time-variant dynamics that exhibits strong locality, Koopa calculates context-aware operators in the temporal ne
&lt;/p&gt;</description></item><item><title>&#27492;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20030;&#38598;&#23725;&#20272;&#35745;&#22120;&#20013;&#23376;&#37319;&#26679;&#21644;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#21457;&#29616;&#20108;&#32773;&#22312;&#19968;&#23450;&#36335;&#24452;&#20013;&#26159;&#28176;&#36817;&#31561;&#20215;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#30456;&#20851;&#30340;&#26041;&#27861;&#30830;&#23450;&#31561;&#20215;&#36335;&#24452;&#65292;&#38388;&#25509;&#35299;&#20915;&#20102;&#23725;&#22238;&#24402;&#35843;&#20248;&#20013;&#39044;&#27979;&#39118;&#38505;&#21333;&#35843;&#24615;&#30340;&#24433;&#21709;&#22240;&#32032;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18496</link><description>&lt;p&gt;
&#23376;&#37319;&#26679;&#19982;&#23725;&#22238;&#24402;&#30340;&#24191;&#20041;&#31561;&#20215;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalized equivalences between subsampling and ridge regularization. (arXiv:2305.18496v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18496
&lt;/p&gt;
&lt;p&gt;
&#27492;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20030;&#38598;&#23725;&#20272;&#35745;&#22120;&#20013;&#23376;&#37319;&#26679;&#21644;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#21457;&#29616;&#20108;&#32773;&#22312;&#19968;&#23450;&#36335;&#24452;&#20013;&#26159;&#28176;&#36817;&#31561;&#20215;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#30456;&#20851;&#30340;&#26041;&#27861;&#30830;&#23450;&#31561;&#20215;&#36335;&#24452;&#65292;&#38388;&#25509;&#35299;&#20915;&#20102;&#23725;&#22238;&#24402;&#35843;&#20248;&#20013;&#39044;&#27979;&#39118;&#38505;&#21333;&#35843;&#24615;&#30340;&#24433;&#21709;&#22240;&#32032;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#20030;&#38598;&#23725;&#20272;&#35745;&#22120;&#65292;&#24314;&#31435;&#20102;&#23376;&#37319;&#26679;&#21644;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#31934;&#30830;&#32467;&#26500;&#21644;&#39118;&#38505;&#31561;&#20215;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#24403;&#29992;&#19981;&#21516;&#30340;&#23725;&#27491;&#21017;&#21270;&#27700;&#24179;$\lambda$&#21644;&#23376;&#37319;&#26679;&#27604;&#20363;$\psi$&#25311;&#21512;&#23376;&#26679;&#23725;&#20272;&#35745;&#22120;&#30340;&#32447;&#24615;&#21644;&#20108;&#27425;&#27867;&#20989;&#65292;&#22312;$(\lambda,\psi)$-&#24179;&#38754;&#19978;&#27839;&#30528;&#29305;&#23450;&#36335;&#24452;&#28176;&#36817;&#31561;&#20215;&#65288;&#20854;&#20013;$\psi$&#26159;&#29305;&#24449;&#32500;&#24230;&#19982;&#23376;&#37319;&#26679;&#22823;&#23567;&#30340;&#27604;&#29575;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20165;&#35201;&#27714;&#29305;&#24449;&#21644;&#21709;&#24212;&#20998;&#24067;&#20855;&#26377;&#26377;&#30028;&#30697;&#65292;&#24182;&#20801;&#35768;&#20219;&#24847;&#32852;&#21512;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#30456;&#20851;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;$(\lambda,\psi)$&#30340;&#31561;&#20215;&#36335;&#24452;&#12290;&#25105;&#20204;&#32467;&#26524;&#30340;&#38388;&#25509;&#21547;&#20041;&#26159;&#65292;&#22312;&#25968;&#25454;&#26041;&#38754;&#27604;&#20363;&#20013;&#65292;&#35843;&#20248;&#30340;&#23725;&#22238;&#24402;&#21576;&#29616;&#20986;&#21333;&#35843;&#39044;&#27979;&#39118;&#38505;&#12290;&#36825;&#35299;&#20915;&#20102;Nakkiran&#31561;&#20154;&#25552;&#20986;&#30340;&#19968;&#20010;&#36817;&#26399;&#26410;&#35299;&#20915;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#22312;&#19968;&#33324;&#25968;&#25454;&#20998;&#24067;&#21644;&#28201;&#21644;&#30340;&#27491;&#21017;&#26465;&#20214;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish precise structural and risk equivalences between subsampling and ridge regularization for ensemble ridge estimators. Specifically, we prove that linear and quadratic functionals of subsample ridge estimators, when fitted with different ridge regularization levels $\lambda$ and subsample aspect ratios $\psi$, are asymptotically equivalent along specific paths in the $(\lambda, \psi )$-plane (where $\psi$ is the ratio of the feature dimension to the subsample size). Our results only require bounded moment assumptions on feature and response distributions and allow for arbitrary joint distributions. Furthermore, we provide a datadependent method to determine the equivalent paths of $(\lambda, \psi )$. An indirect implication of our equivalences is that optimally-tuned ridge regression exhibits a monotonic prediction risk in the data aspect ratio. This resolves a recent open problem raised by Nakkiran et al. under general data distributions and a mild regularity condition that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#20004;&#31181;&#38543;&#26426;&#31639;&#27861;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29702;&#35770;&#19978;&#25910;&#25947;&#21040;&#24085;&#32047;&#25176;&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.18409</link><description>&lt;p&gt;
&#38754;&#21521;&#26041;&#21521;&#30340;&#22810;&#30446;&#26631;&#23398;&#20064;&#65306;&#31616;&#21333;&#19988;&#21487;&#35777;&#26126;&#30340;&#38543;&#26426;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms. (arXiv:2305.18409v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#20004;&#31181;&#38543;&#26426;&#31639;&#27861;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29702;&#35770;&#19978;&#25910;&#25947;&#21040;&#24085;&#32047;&#25176;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#19982;&#22810;&#20010;&#30446;&#26631;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65288;&#22914;&#22810;&#26631;&#20934;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#65289;&#20013;&#19968;&#20010;&#26377;&#24433;&#21709;&#21147;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#26041;&#21521;&#30340;&#37051;&#22495;&#20869;&#38480;&#21046;&#20844;&#20849;&#19979;&#38477;&#26041;&#21521;&#26469;&#35268;&#33539;&#32447;&#24615;&#32452;&#21512;&#30446;&#26631;&#30340;&#26368;&#20248;&#26041;&#21521;&#65292;&#20363;&#22914;MTL&#20013;&#30340;&#24179;&#22343;&#25439;&#22833;&#12290; &#36825;&#20010;&#20844;&#24335;&#21253;&#25324;GD&#21644;MGDA&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65292;&#20139;&#21463;&#20687;CAGrad&#20013;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#22909;&#22788;&#65292;&#20197;&#21450;&#26377;&#21033;&#20110;&#38543;&#26426;&#31639;&#27861;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38543;&#26426;&#26041;&#21521;&#23548;&#21521;&#22810;&#30446;&#26631;&#26799;&#24230;&#19979;&#38477;&#65288;SDMGrad&#65289;&#65292;&#23427;&#20351;&#29992;&#31616;&#21333;&#30340;SGD&#31867;&#22411;&#30340;&#26356;&#26032;&#31639;&#27861;&#65292;&#20197;&#21450;&#22312;&#30446;&#26631;&#25968;&#37327;&#36739;&#22810;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#39640;&#25928;&#30340;&#30446;&#26631;&#37319;&#26679;&#30340;SDMGrad-OS&#31639;&#27861;&#12290; &#23545;&#20110;&#24658;&#23450;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#955;&#65292;&#25105;&#20204;&#35777;&#26126;SDMGrad&#21644;SDMGrad-OS&#30830;&#23454;&#25910;&#25947;&#21040;&#24085;&#32047;&#25176;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective optimization (MOO) has become an influential framework in many machine learning problems with multiple objectives such as learning with multiple criteria and multi-task learning (MTL). In this paper, we propose a new direction-oriented multi-objective problem by regularizing the common descent direction within a neighborhood of a direction that optimizes a linear combination of objectives such as the average loss in MTL. This formulation includes GD and MGDA as special cases, enjoys the direction-oriented benefit as in CAGrad, and facilitates the design of stochastic algorithms. To solve this problem, we propose Stochastic Direction-oriented Multi-objective Gradient descent (SDMGrad) with simple SGD type of updates, and its variant SDMGrad-OS with an efficient objective sampling in the setting where the number of objectives is large. For a constant-level regularization parameter $\lambda$, we show that SDMGrad and SDMGrad-OS provably converge to a Pareto stationary poin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#20445;&#30495;&#24230;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#12289;&#39640;&#20445;&#30495;&#24230;&#26679;&#26412;&#20013;&#20272;&#35745;&#29289;&#29702;&#31995;&#32479;&#20013;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24179;&#34913;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#25968;&#20540;&#31934;&#24230;&#20043;&#38388;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.16530</link><description>&lt;p&gt;
&#21452;&#20445;&#30495;&#24230;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bi-fidelity Variational Auto-encoder for Uncertainty Quantification. (arXiv:2305.16530v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#20445;&#30495;&#24230;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#12289;&#39640;&#20445;&#30495;&#24230;&#26679;&#26412;&#20013;&#20272;&#35745;&#29289;&#29702;&#31995;&#32479;&#20013;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24179;&#34913;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#25968;&#20540;&#31934;&#24230;&#20043;&#38388;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#22411;&#39564;&#35777;&#20013;&#65292;&#37327;&#21270;&#29289;&#29702;&#31995;&#32479;&#24863;&#20852;&#36259;&#30340;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#38656;&#35201;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#25968;&#20540;&#31934;&#24230;&#20043;&#38388;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#20445;&#30495;&#24230;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;BF-VAE&#65289;&#20844;&#24335;&#65292;&#26088;&#22312;&#20174;&#29289;&#29702;&#31995;&#32479;&#20013;&#20302;&#12289;&#39640;&#20445;&#30495;&#24230;&#26679;&#26412;&#20013;&#20272;&#35745;&#19982;&#37327;&#24863;&#20852;&#36259;&#30340;&#37327;&#26377;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#20174;&#20302;&#20445;&#30495;&#24230;&#26679;&#26412;&#24471;&#20986;&#30340;&#20449;&#24687;&#26469;&#36924;&#36817;&#39640;&#20445;&#30495;&#24230;&#37327;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21452;&#20445;&#30495;&#24230;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#23558;&#20854;&#25972;&#21512;&#21040;VAE&#30340;&#27010;&#29575;&#32534;&#30721;-&#35299;&#30721;&#32467;&#26500;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20197;&#22312;&#23384;&#22312;&#26377;&#38480;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#22823;&#21270;&#39640;&#20445;&#30495;&#24230;&#23545;&#25968;&#20284;&#28982;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#20174;&#32780;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#21512;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#20540;&#31034;&#20363;&#20013;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;&#38543;&#26426;&#31995;&#32479;&#21644;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying the uncertainty of quantities of interest (QoIs) from physical systems is a primary objective in model validation. However, achieving this goal entails balancing the need for computational efficiency with the requirement for numerical accuracy. To address this trade-off, we propose a novel bi-fidelity formulation of variational auto-encoders (BF-VAE) designed to estimate the uncertainty associated with a QoI from low-fidelity (LF) and high-fidelity (HF) samples of the QoI. This model allows for the approximation of the statistics of the HF QoI by leveraging information derived from its LF counterpart. Specifically, we design a bi-fidelity auto-regressive model in the latent space that is integrated within the VAE's probabilistic encoder-decoder structure. An effective algorithm is proposed to maximize the variational lower bound of the HF log-likelihood in the presence of limited HF data, resulting in the synthesis of HF realizations with a reduced computational cost. Addit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;IIR&#28388;&#27874;&#22120;&#26469;&#32858;&#28966;&#27880;&#24847;&#21147;&#30340;&#26032;&#23618;&#12290;&#36825;&#20123;&#28388;&#27874;&#22120;&#22522;&#20110;&#36755;&#20837;&#24207;&#21015;&#30340;&#21069;&#20960;&#20010;&#22359;&#26469;&#30830;&#23450;&#31995;&#25968;&#65292;&#33021;&#22815;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#30456;&#20851;&#30340;&#24207;&#21015;&#20803;&#32032;&#19978;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20854;&#20182;&#32593;&#32476;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#27425;&#20108;&#27425;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#35813;&#23618;&#22312;&#22810;&#20010;&#38271;&#31243;&#24207;&#21015;&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36234;&#20102;Heyna&#12289;GPT2&#21644;Mega&#31561;&#23618;&#12290;</title><link>http://arxiv.org/abs/2305.14952</link><description>&lt;p&gt;
&#32858;&#28966;&#24744;&#30340;&#27880;&#24847;&#21147;&#65288;&#36890;&#36807;&#33258;&#36866;&#24212;IIR&#28388;&#27874;&#22120;&#65289;
&lt;/p&gt;
&lt;p&gt;
Focus Your Attention (with Adaptive IIR Filters). (arXiv:2305.14952v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;IIR&#28388;&#27874;&#22120;&#26469;&#32858;&#28966;&#27880;&#24847;&#21147;&#30340;&#26032;&#23618;&#12290;&#36825;&#20123;&#28388;&#27874;&#22120;&#22522;&#20110;&#36755;&#20837;&#24207;&#21015;&#30340;&#21069;&#20960;&#20010;&#22359;&#26469;&#30830;&#23450;&#31995;&#25968;&#65292;&#33021;&#22815;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#30456;&#20851;&#30340;&#24207;&#21015;&#20803;&#32032;&#19978;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20854;&#20182;&#32593;&#32476;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#27425;&#20108;&#27425;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#35813;&#23618;&#22312;&#22810;&#20010;&#38271;&#31243;&#24207;&#21015;&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36234;&#20102;Heyna&#12289;GPT2&#21644;Mega&#31561;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23618;&#65292;&#22312;&#20854;&#20013;&#20351;&#29992;&#21160;&#24577;&#65288;&#21363;&#36755;&#20837;&#30456;&#20851;&#30340;&#65289;&#20108;&#38454;&#26080;&#38480;&#20914;&#28608;&#21709;&#24212;&#65288;IIR&#65289;&#28388;&#27874;&#22120;&#26469;&#22788;&#29702;&#36755;&#20837;&#24207;&#21015;&#65292;&#28982;&#21518;&#20877;&#24212;&#29992;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#36755;&#20837;&#34987;&#20998;&#25104;&#22359;&#65292;&#24182;&#19988;&#36825;&#20123;&#28388;&#27874;&#22120;&#30340;&#31995;&#25968;&#22522;&#20110;&#21069;&#38754;&#30340;&#22359;&#26469;&#30830;&#23450;&#20197;&#20445;&#25345;&#22240;&#26524;&#24615;&#12290;&#23613;&#31649;&#23427;&#20204;&#30456;&#23545;&#36739;&#20302;&#38454;&#65292;&#20294;&#36825;&#20123;&#22240;&#26524;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#34987;&#35777;&#26126;&#21487;&#20197;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#30456;&#20851;&#30340;&#24207;&#21015;&#20803;&#32032;&#19978;&#12290;&#36825;&#19968;&#26032;&#23618;&#22522;&#20110;&#25511;&#21046;&#29702;&#35770;&#65292;&#24182;&#23637;&#31034;&#20102;&#33021;&#22815;&#25512;&#24191;&#23545;&#35282;&#29366;&#24577;&#31354;&#38388;&#23618;&#12290;&#35813;&#23618;&#30340;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#30456;&#24403;&#65292;&#20294;&#21442;&#25968;&#37327;&#36739;&#23569;&#65292;&#24182;&#19988;&#26102;&#38388;&#22797;&#26434;&#24230;&#38543;&#36755;&#20837;&#22823;&#23567;&#30340;&#22686;&#38271;&#26159;&#27425;&#20108;&#27425;&#30340;&#12290;&#25152;&#24471;&#21040;&#30340;&#23618;&#22312;&#22810;&#20010;&#38271;&#31243;&#24207;&#21015;&#38382;&#39064;&#19978;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#24615;&#33021;&#27700;&#24179;&#37117;&#20248;&#20110;Heyna&#12289;GPT2&#21644;Mega&#31561;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new layer in which dynamic (i.e.,input-dependent) Infinite Impulse Response (IIR) filters of order two are used to process the input sequence prior to applying conventional attention. The input is split into chunks, and the coefficients of these filters are determined based on previous chunks to maintain causality. Despite their relatively low order, the causal adaptive filters are shown to focus attention on the relevant sequence elements. The new layer is grounded in control theory, and is shown to generalize diagonal state-space layers. The layer performs on-par with state-of-the-art networks, with a fraction of their parameters and with time complexity that is sub-quadratic with input size. The obtained layer is favorable to layers such as Heyna, GPT2, and Mega, both with respect to the number of parameters and the obtained level of performance on multiple long-range sequence problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#21462;&#36974;&#34109;&#25968;&#25454;&#30340;&#26041;&#27861;&#65288;Difference-Masking&#65289;&#65292;&#20197;&#25552;&#39640;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26041;&#27861;&#26159;&#36890;&#36807;&#32771;&#34385;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#19982;&#39044;&#35757;&#32451;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;&#26469;&#36827;&#34892;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21319;&#32487;&#32493;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#19988;&#20855;&#26377;&#36328;&#20219;&#21153;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14577</link><description>&lt;p&gt;
&#24046;&#24322;&#24615;&#36974;&#25377;&#65306;&#36873;&#25321;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#36974;&#25377;&#20160;&#20040;
&lt;/p&gt;
&lt;p&gt;
Difference-Masking: Choosing What to Mask in Continued Pretraining. (arXiv:2305.14577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#21462;&#36974;&#34109;&#25968;&#25454;&#30340;&#26041;&#27861;&#65288;Difference-Masking&#65289;&#65292;&#20197;&#25552;&#39640;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26041;&#27861;&#26159;&#36890;&#36807;&#32771;&#34385;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#19982;&#39044;&#35757;&#32451;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;&#26469;&#36827;&#34892;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21319;&#32487;&#32493;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#19988;&#20855;&#26377;&#36328;&#20219;&#21153;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#65292;&#29305;&#21035;&#26159;&#36974;&#25377;&#39044;&#27979;&#30446;&#26631;&#30340;&#30446;&#26631;&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#38543;&#26426;&#22320;&#36827;&#34892;&#26631;&#35760;&#21644;&#36974;&#25377;&#65292;&#32780;&#22312;&#25945;&#32946;&#39046;&#22495;&#26377;&#24378;&#28872;&#30340;&#30452;&#35273;&#35748;&#20026;&#65292;&#20915;&#23450;&#20160;&#20040;&#38656;&#35201;&#36974;&#25377;&#21487;&#20197;&#23454;&#36136;&#24615;&#22320;&#25913;&#21892;&#23398;&#20064;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24046;&#24322;&#36974;&#25377;(Difference-Masking)&#65292;&#19968;&#31181;&#33258;&#21160;&#36873;&#25321;&#36974;&#25377;&#20160;&#20040;&#30340;&#26041;&#27861;&#65292;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#20013;&#36890;&#36807;&#32771;&#34385;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#19982;&#39044;&#35757;&#32451;&#22495;&#30340;&#19981;&#21516;&#20043;&#22788;&#26469;&#23454;&#29616;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#24046;&#24322;&#36974;&#25377;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#35270;&#39057;&#20219;&#21153;&#30340;&#32487;&#32493;&#39044;&#35757;&#32451;&#35774;&#32622;&#20013;&#20248;&#20110;&#22522;&#32447;&#12290;&#24046;&#24322;&#24615;&#36974;&#25377;&#30340;&#36328;&#20219;&#21153;&#36866;&#29992;&#24615;&#25903;&#25345;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;SSL&#39044;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) and the objective of masking-and-predicting in particular have led to promising SSL performance on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition from the field of education that deciding what to mask can substantially improve learning outcomes. We introduce Difference-Masking, an approach that automatically chooses what to mask during continued pretraining by considering what makes an unlabelled target domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language and multimodal video tasks. The cross-task applicability of Difference-Masking supports the effectiveness of our framework for SSL pretraining in language, vision, and other domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#19968;&#20010;&#24658;&#23450;&#30340;&#32622;&#20449;&#24230;&#24230;&#37327;&#21644;&#25298;&#32477;&#38408;&#20540;&#26469;&#35299;&#20915;&#26080;&#26631;&#31614;&#24773;&#20917;&#19979;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.13189</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#19982;&#25298;&#32477;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Anomaly Detection with Rejection. (arXiv:2305.13189v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#19968;&#20010;&#24658;&#23450;&#30340;&#32622;&#20449;&#24230;&#24230;&#37327;&#21644;&#25298;&#32477;&#38408;&#20540;&#26469;&#35299;&#20915;&#26080;&#26631;&#31614;&#24773;&#20917;&#19979;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26088;&#22312;&#26816;&#27979;&#25968;&#25454;&#20013;&#30340;&#24847;&#22806;&#34892;&#20026;&#12290;&#30001;&#20110;&#24322;&#24120;&#26816;&#27979;&#36890;&#24120;&#26159;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#20256;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#30452;&#35273;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#23398;&#20064;&#20915;&#31574;&#36793;&#30028;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#24456;&#38590;&#39564;&#35777;&#12290;&#36825;&#24341;&#20837;&#20102;&#19968;&#20123;&#19981;&#30830;&#23450;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#65292;&#21487;&#33021;&#38477;&#20302;&#29992;&#25143;&#23545;&#26816;&#27979;&#22120;&#39044;&#27979;&#30340;&#20449;&#20219;&#12290;&#25269;&#24481;&#36825;&#31181;&#24773;&#20917;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20801;&#35768;&#26816;&#27979;&#22120;&#25298;&#32477;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#31034;&#20363;&#65288;&#23398;&#20064;&#25298;&#32477;&#65289;&#12290;&#36825;&#38656;&#35201;&#20351;&#29992;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21040;&#19982;&#20915;&#31574;&#36793;&#30028;&#30340;&#36317;&#31163;&#30340;&#32622;&#20449;&#24230;&#24230;&#37327;&#65292;&#24182;&#35774;&#32622;&#19968;&#20010;&#25298;&#32477;&#38408;&#20540;&#26469;&#25298;&#32477;&#20302;&#32622;&#20449;&#24230;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#36866;&#24403;&#30340;&#24230;&#37327;&#21644;&#35774;&#32622;&#25298;&#32477;&#38408;&#20540;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;ExCeeD&#35745;&#31639;&#30340;&#31283;&#23450;&#24230;&#37327;&#19978;&#35774;&#32622;&#19968;&#20010;&#24658;&#23450;&#30340;&#25298;&#32477;&#38408;&#20540;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#27934;&#23519;&#21147;&#20381;&#36182;&#20110;&#23545;&#36825;&#31181;&#24230;&#37327;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#32622;&#20102;&#19968;&#20010;&#22522;&#20110;&#31283;&#23450;&#24615;&#24230;&#37327;&#30340;&#25298;&#32477;&#38408;&#20540;&#65292;&#20197;&#27492;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection aims at detecting unexpected behaviours in the data. Because anomaly detection is usually an unsupervised task, traditional anomaly detectors learn a decision boundary by employing heuristics based on intuitions, which are hard to verify in practice. This introduces some uncertainty, especially close to the decision boundary, that may reduce the user trust in the detector's predictions. A way to combat this is by allowing the detector to reject examples with high uncertainty (Learning to Reject). This requires employing a confidence metric that captures the distance to the decision boundary and setting a rejection threshold to reject low-confidence predictions. However, selecting a proper metric and setting the rejection threshold without labels are challenging tasks. In this paper, we solve these challenges by setting a constant rejection threshold on the stability metric computed by ExCeeD. Our insight relies on a theoretical analysis of such a metric. Moreover, set
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#24230;&#29616;&#23454;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#23545;&#31232;&#30095;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#30456;&#20301;&#30456;&#20851;&#30340;&#27874;&#28010;&#34920;&#38754;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2305.11913</link><description>&lt;p&gt;
&#29992;&#20110;&#20174;&#31232;&#30095;&#36828;&#31243;&#20256;&#24863;&#22120;&#25968;&#25454;&#37325;&#24314;&#38750;&#32447;&#24615;&#28023;&#27915;&#27874;&#28010;&#34920;&#38754;&#30456;&#20301;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine learning for phase-resolved reconstruction of nonlinear ocean wave surface elevations from sparse remote sensing data. (arXiv:2305.11913v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#24230;&#29616;&#23454;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#23545;&#31232;&#30095;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#30456;&#20301;&#30456;&#20851;&#30340;&#27874;&#28010;&#34920;&#38754;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#30456;&#20301;&#30456;&#20851;&#30340;&#27700;&#27874;&#26465;&#20214;&#23545;&#20110;&#28023;&#27915;&#24037;&#31243;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36828;&#31243;&#30417;&#27979;&#27874;&#28010;&#39044;&#27979;&#27169;&#22411;&#30340;&#21021;&#22987;&#21270;&#39318;&#20808;&#38656;&#35201;&#20174;&#31867;&#20284;&#38647;&#36798;&#30340;&#31232;&#30095;&#27979;&#37327;&#20013;&#37325;&#24314;&#27874;&#28010;&#34920;&#38754;&#12290;&#29616;&#26377;&#30340;&#37325;&#24314;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#31616;&#21270;&#30340;&#27169;&#22411;&#20551;&#35774;&#65292;&#36825;&#20250;&#24433;&#21709;&#25972;&#20010;&#39044;&#27979;&#36807;&#31243;&#30340;&#23454;&#26102;&#24615;&#25110;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#21644;Fourier&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#30456;&#20301;&#30456;&#20851;&#30340;&#27874;&#28010;&#34920;&#38754;&#37325;&#24314;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20855;&#26377;&#39640;&#24230;&#29616;&#23454;&#24615;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#22312;&#22343;&#21248;&#30340;&#19968;&#32500;&#32593;&#26684;&#19978;&#30001;&#27874;&#28010;&#27169;&#25311;&#30340;&#39640;&#38454;&#35889;&#26041;&#27861;&#21644;&#20960;&#20309;&#38647;&#36798;&#24314;&#27169;&#26041;&#27861;&#29983;&#25104;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#31181;&#27169;&#22411;&#37117;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#27874;&#28010;&#37325;&#24314;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate short-term prediction of phase-resolved water wave conditions is crucial for decision-making in ocean engineering. However, the initialization of remote-sensing-based wave prediction models first requires a reconstruction of wave surfaces from sparse measurements like radar. Existing reconstruction methods either rely on computationally intensive optimization procedures or simplistic modeling assumptions that compromise real-time capability or accuracy of the entire prediction process. We therefore address these issues by proposing a novel approach for phase-resolved wave surface reconstruction using neural networks based on the U-Net and Fourier neural operator (FNO) architectures. Our approach utilizes synthetic yet highly realistic training data on uniform one-dimensional grids, that is generated by the high-order spectral method for wave simulation and a geometric radar modeling approach. The investigation reveals that both models deliver accurate wave reconstruction resul
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11141</link><description>&lt;p&gt;
Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford Group Equivariant Neural Networks. (arXiv:2305.11141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11141
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;Clifford&#32676;&#65292;&#23427;&#26159;Clifford&#20195;&#25968;&#20013;&#30340;&#19968;&#20010;&#23376;&#32676;&#65292;&#20854;&#23450;&#20041;&#32463;&#36807;&#35843;&#25972;&#20197;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;&#20027;&#35201;&#22320;&#65292;&#35813;&#32676;&#30340;&#20316;&#29992;&#24418;&#25104;&#20102;&#19968;&#20010;&#27491;&#20132;&#33258;&#21516;&#26500;&#65292;&#25193;&#23637;&#21040;&#25972;&#20010;Clifford&#20195;&#25968;&#65292;&#21516;&#26102;&#23562;&#37325;&#22810;&#30690;&#20998;&#32423;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#24212;&#20110;&#22810;&#30690;&#20998;&#35299;&#30340;&#22810;&#20010;&#38750;&#31561;&#20215;&#23376;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#20316;&#29992;&#19981;&#20165;&#23562;&#37325;Clifford&#20195;&#25968;&#30340;&#21521;&#37327;&#31354;&#38388;&#32467;&#26500;&#65292;&#36824;&#23562;&#37325;&#20854;&#20056;&#27861;&#32467;&#26500;&#65292;&#21363;&#20960;&#20309;&#20056;&#31215;&#12290;&#36825;&#20123;&#21457;&#29616;&#24847;&#21619;&#30528;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#22312;&#20219;&#24847;&#32500;&#30340;&#20869;&#31215;&#31354;&#38388;&#20013;&#20248;&#38597;&#22320;&#25512;&#24191;&#30340;&#34920;&#36798;&#23618;&#12290;&#25105;&#20204;&#29305;&#21035;&#23637;&#31034;&#20102;&#20174;&#19968;&#20010;sin
&lt;/p&gt;
&lt;p&gt;
We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing $\mathrm{O}(n)$- and $\mathrm{E}(n)$-equivariant models. We identify and study the $\textit{Clifford group}$, a subgroup inside the Clifford algebra whose definition we adjust to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a sin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#38271;&#26399;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#21095;&#38598;&#24335;MDP&#27169;&#22411;&#65292;&#25552;&#20986;&#22312;&#38754;&#20020;&#36716;&#25442;&#21644;&#22870;&#21169;&#29305;&#24615;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#22522;&#20110;&#21344;&#26377;&#24230;&#37327;&#25552;&#20379;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#20854;&#26399;&#26395;&#36951;&#25022;&#21463;&#21040;&#30028;&#38480;&#32422;&#26463; $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ .</title><link>http://arxiv.org/abs/2305.10744</link><description>&lt;p&gt;
&#38754;&#21521;&#21095;&#38598;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Resource Allocation in Episodic Markov Decision Processes. (arXiv:2305.10744v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#38271;&#26399;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#21095;&#38598;&#24335;MDP&#27169;&#22411;&#65292;&#25552;&#20986;&#22312;&#38754;&#20020;&#36716;&#25442;&#21644;&#22870;&#21169;&#29305;&#24615;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#22522;&#20110;&#21344;&#26377;&#24230;&#37327;&#25552;&#20379;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#20854;&#26399;&#26395;&#36951;&#25022;&#21463;&#21040;&#30028;&#38480;&#32422;&#26463; $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ .
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#38271;&#26399;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#23427;&#38656;&#35201;&#22312;&#22810;&#20010;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#22810;&#38454;&#27573;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21095;&#38598;&#24335;&#26377;&#38480;&#26102;&#38388;&#27573;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#20854;&#20013;&#36716;&#25442;&#21644;&#22870;&#21169;&#20197;&#21450;&#27599;&#19968;&#27425;&#30340;&#36164;&#28304;&#28040;&#32791;&#20989;&#25968;&#37117;&#26159;&#38750;&#23450;&#24577;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31561;&#25928;&#30340;&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#37325;&#26500;&#26041;&#27861;&#65292;&#22522;&#20110;&#21344;&#26377;&#24230;&#37327;&#65292;&#20026;&#27492;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#36164;&#28304;&#20998;&#37197;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#22788;&#29702;&#20102;&#22312;&#20272;&#31639;&#30495;&#23454;&#21487;&#34892;&#38598;&#26102;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#35823;&#24046;&#65292;&#36825;&#26159;&#30456;&#23545;&#29420;&#31435;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#38543;&#26426;&#22870;&#21169;&#21644;&#36164;&#28304;&#28040;&#32791;&#20989;&#25968;&#65292;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#30340;&#26399;&#26395;&#36951;&#25022;&#21463;&#21040;&#30028;&#38480;&#32422;&#26463;&#65292;&#20854;&#30028;&#38480;&#21463;&#21040; $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ &#30340;&#32422;&#26463;&#65292;&#20854;&#20013; $\rho\in(0,1)$ &#26159;&#39044;&#31639;&#21442;&#25968;&#65292;$H$ &#26159;&#22320;&#24179;&#32447;&#38271;&#24230;&#65292;$S$ &#21644; $A$ &#26159;. . .
&lt;/p&gt;
&lt;p&gt;
This paper studies a long-term resource allocation problem over multiple periods where each period requires a multi-stage decision-making process. We formulate the problem as an online resource allocation problem in an episodic finite-horizon Markov decision process with unknown non-stationary transitions and stochastic non-stationary reward and resource consumption functions for each episode. We provide an equivalent online linear programming reformulation based on occupancy measures, for which we develop an online mirror descent algorithm. Our online dual mirror descent algorithm for resource allocation deals with uncertainties and errors in estimating the true feasible set, which is of independent interest. We prove that under stochastic reward and resource consumption functions, the expected regret of the online mirror descent algorithm is bounded by $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ where $\rho\in(0,1)$ is the budget parameter, $H$ is the length of the horizon, $S$ and $A$ are the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06026</link><description>&lt;p&gt;
&#25628;&#32034;UGLE&#30495;&#30456;&#65306;&#26080;&#30417;&#30563;GNN&#23398;&#20064;&#29615;&#22659;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments. (arXiv:2305.06026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#26159;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#22270;&#32467;&#26500;&#19978;&#30340;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#31038;&#21306;&#26816;&#27979;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;GNN&#36827;&#34892;&#12290;&#21033;&#29992;&#33410;&#28857;&#29305;&#24449;&#30340;&#22810;&#32500;&#24230;&#19982;&#22270;&#30340;&#36830;&#25509;&#24615;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23545;&#20174;&#31038;&#20132;&#32593;&#32476;&#21040;&#22522;&#22240;&#32452;&#23398;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#30340;&#20805;&#20998;&#22522;&#20934;&#29615;&#22659;&#65292;&#20174;&#32780;&#21487;&#33021;&#38459;&#30861;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#22256;&#38590;&#26159;&#27169;&#31946;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#29615;&#22659;&#19982;&#24615;&#33021;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#20914;&#31361;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21644;&#35780;&#20272;&#20102;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;GNN&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#33268;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#65292;&#21453;&#26144;&#20102;&#26816;&#27979;&#21040;&#30340;&#31038;&#21306;&#30340;&#20869;&#22312;&#36136;&#37327;&#20197;&#21450;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a pertinent tool for any machine learning task due to their ability to learn functions over graph structures, a powerful and expressive data representation. The detection of communities, an unsupervised task has increasingly been performed with GNNs. Clustering nodes in a graph using the multi-dimensionality of node features with the connectivity of the graph has many applications to real world tasks from social networks to genomics. Unfortunately, there is currently a gap in the literature with no established sufficient benchmarking environment for fairly and rigorously evaluating GNN based community detection, thereby potentially impeding progress in this nascent field. We observe the particular difficulties in this setting is the ambiguous hyperparameter tuning environments combined with conflicting metrics of performance and evaluation datasets. In this work, we propose and evaluate frameworks for the consistent comparisons of community detection al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeformerNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#20302;&#32500;&#34920;&#31034;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#23545;&#29289;&#20307;&#24418;&#29366;&#30340;&#25805;&#32437;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#24037;&#29305;&#24449;&#21644;&#29289;&#20307;&#29305;&#23450;&#30340;&#25511;&#21046;&#27169;&#22411;&#65292;&#21487;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#28436;&#31034;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.04449</link><description>&lt;p&gt;
DeformerNet: &#23398;&#20064;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#21452;&#25163;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
DeformerNet: Learning Bimanual Manipulation of 3D Deformable Objects. (arXiv:2305.04449v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeformerNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#20302;&#32500;&#34920;&#31034;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#23545;&#29289;&#20307;&#24418;&#29366;&#30340;&#25805;&#32437;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#24037;&#29305;&#24449;&#21644;&#29289;&#20307;&#29305;&#23450;&#30340;&#25511;&#21046;&#27169;&#22411;&#65292;&#21487;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#28436;&#31034;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23478;&#24237;&#25252;&#29702;&#21040;&#20179;&#24211;&#37197;&#36865;&#20877;&#21040;&#22806;&#31185;&#25163;&#26415;&#21161;&#29702;&#31561;&#39046;&#22495;&#65292;&#24212;&#29992;&#38656;&#35201;&#26426;&#22120;&#20154;&#21487;&#38752;&#22320;&#25805;&#32437;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#24418;&#29366;&#12290;&#24377;&#24615;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#20998;&#26512;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#21442;&#25968;&#26469;&#25551;&#36848;&#20915;&#23450;&#29289;&#20307;&#24418;&#29366;&#30340;&#21487;&#33021;&#26080;&#38480;&#33258;&#30001;&#24230;&#12290;&#20197;&#24448;&#30340;3D&#24418;&#29366;&#25511;&#21046;&#23581;&#35797;&#20381;&#36182;&#20110;&#25163;&#24037;&#29305;&#24449;&#26469;&#34920;&#31034;&#29289;&#20307;&#24418;&#29366;&#65292;&#24182;&#38656;&#35201;&#35757;&#32451;&#29289;&#20307;&#29305;&#23450;&#30340;&#25511;&#21046;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#22411;DeformerNet&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#26550;&#26500;&#22312;&#34987;&#25805;&#32437;&#29289;&#20307;&#30340;&#37096;&#20998;&#35270;&#22270;&#28857;&#20113;&#21644;&#30446;&#26631;&#24418;&#29366;&#30340;&#28857;&#20113;&#19978;&#36816;&#34892;&#65292;&#23398;&#20064;&#29289;&#20307;&#24418;&#29366;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#36825;&#20010;&#24418;&#29366;&#23884;&#20837;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23398;&#20064;&#19968;&#31181;&#35270;&#35273;&#20282;&#26381;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#35745;&#31639;&#20986;&#25152;&#38656;&#30340;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#21160;&#20316;&#65292;&#23558;&#29289;&#20307;&#36845;&#20195;&#22320;&#21464;&#24418;&#21521;&#30446;&#26631;&#24418;&#29366;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#28436;&#31034;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications in fields ranging from home care to warehouse fulfillment to surgical assistance require robots to reliably manipulate the shape of 3D deformable objects. Analytic models of elastic, 3D deformable objects require numerous parameters to describe the potentially infinite degrees of freedom present in determining the object's shape. Previous attempts at performing 3D shape control rely on hand-crafted features to represent the object shape and require training of object-specific control models. We overcome these issues through the use of our novel DeformerNet neural network architecture, which operates on a partial-view point cloud of the manipulated object and a point cloud of the goal shape to learn a low-dimensional representation of the object shape. This shape embedding enables the robot to learn a visual servo controller that computes the desired robot end-effector action to iteratively deform the object toward the target shape. We demonstrate both in simulation and on 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#20005;&#37325;&#27424;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;&#25104;&#23545;&#30456;&#20851;&#24615;&#25552;&#20379;&#20102;&#24674;&#22797;&#23616;&#37096;&#32467;&#26500;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#22797;&#26434;&#22810;&#21464;&#37327;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#29616;&#20195;&#20851;&#27880;&#26426;&#21046;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#25104;&#21151;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.04386</link><description>&lt;p&gt;
&#20174;&#25104;&#23545;&#30456;&#20851;&#24615;&#20013;&#25512;&#26029;&#23616;&#37096;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Inferring Local Structure from Pairwise Correlations. (arXiv:2305.04386v2 [physics.data-an] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#20005;&#37325;&#27424;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;&#25104;&#23545;&#30456;&#20851;&#24615;&#25552;&#20379;&#20102;&#24674;&#22797;&#23616;&#37096;&#32467;&#26500;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#22797;&#26434;&#22810;&#21464;&#37327;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#29616;&#20195;&#20851;&#27880;&#26426;&#21046;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#25104;&#21151;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#22823;&#22411;&#22810;&#21464;&#37327;&#22797;&#26434;&#31995;&#32479;&#30340;&#27169;&#22411;&#65292;&#27604;&#22914;&#29983;&#29289;&#23398;&#20013;&#30340;&#31995;&#32479;&#65292;&#25105;&#20204;&#38656;&#35201;&#38480;&#21046;&#20801;&#35768;&#30456;&#20114;&#20316;&#29992;&#30340;&#21464;&#37327;&#12290;&#36825;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#26816;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#8220;&#23616;&#37096;&#8221;&#32467;&#26500;&#12290;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#20108;&#32500;&#33258;&#28982;&#21644;&#21512;&#25104;&#22270;&#20687;&#30340;&#29609;&#20855;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#20005;&#37325;&#27424;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;&#21464;&#37327;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#20063;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#24674;&#22797;&#23616;&#37096;&#20851;&#31995;&#65292;&#21253;&#25324;&#25968;&#25454;&#30340;&#32500;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#37325;&#26500;&#23436;&#20840;&#28151;&#20081;&#30340;&#22270;&#20687;&#20013;&#20687;&#32032;&#30340;&#25490;&#21015;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#32467;&#26500;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#25104;&#21151;&#30340;&#12290;&#25105;&#20204;&#23545;&#25104;&#21151;&#32972;&#21518;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#30452;&#35266;&#20998;&#26512;&#65292;&#24076;&#26395;&#33021;&#23545;&#24314;&#27169;&#22797;&#26434;&#22810;&#21464;&#37327;&#31995;&#32479;&#20197;&#21450;&#35299;&#37322;&#29616;&#20195;&#22522;&#20110;&#20851;&#27880;&#26426;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#25104;&#21151;&#26377;&#25152;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
To construct models of large, multivariate complex systems, such as those in biology, one needs to constrain which variables are allowed to interact. This can be viewed as detecting "local" structures among the variables. In the context of a simple toy model of 2D natural and synthetic images, we show that pairwise correlations between the variables -- even when severely undersampled -- provide enough information to recover local relations, including the dimensionality of the data, and to reconstruct arrangement of pixels in fully scrambled images. This proves to be successful even though higher order interaction structures are present in our data. We build intuition behind the success, which we hope might contribute to modeling complex, multivariate systems and to explaining the success of modern attention-based machine learning approaches.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.02997</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20309;&#26102;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#32988;&#36807;&#22686;&#24378;&#26641;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Do Neural Nets Outperform Boosted Trees on Tabular Data?. (arXiv:2305.02997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02997
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#31867;&#22411;&#20043;&#19968;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#20154;&#20204;&#20173;&#22312;&#31215;&#26497;&#35752;&#35770;NN&#26159;&#21542;&#36890;&#24120;&#20248;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#35201;&#20040;&#35748;&#20026;GBDT&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19968;&#36143;&#20248;&#20110;NN&#65292;&#35201;&#20040;&#35748;&#20026;NN&#20248;&#20110;GBDT&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#38382;&#65306;'&#36825;&#37325;&#35201;&#21527;&#65311;'&#25105;&#20204;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#27604;&#36739;19&#31181;&#31639;&#27861;&#65292;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;'NN vs. GBDT'&#20105;&#35770;&#34987;&#36807;&#20998;&#24378;&#35843;&#65306;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#30456;&#24403;&#22810;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#35201;&#20040;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#35201;&#20040;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;965&#20010;&#20803;&#29305;&#24449;&#65292;&#20197;&#30830;&#23450;&#25968;&#25454;&#38598;&#30340;&#21738;&#20123;&#29305;&#24615;&#20351;NN&#25110;GBDT&#26356;&#36866;&#21512;&#34920;&#29616;&#33391;&#22909;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;GBDT&#35201;&#27604;NN&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, 'does it matter?' We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better th
&lt;/p&gt;</description></item><item><title>IMAP&#26159;&#19968;&#20010;&#20869;&#22312;&#39537;&#21160;&#30340;&#23545;&#25239;&#31574;&#30053;&#65292;&#26080;&#38656;&#21463;&#23475;&#32773;&#31574;&#30053;&#30340;&#20219;&#20309;&#30693;&#35782;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#40657;&#30418;&#35268;&#36991;&#25915;&#20987;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#21644;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02605</link><description>&lt;p&gt;
IMAP: &#20869;&#22312;&#39537;&#21160;&#30340;&#23545;&#25239;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
IMAP: Intrinsically Motivated Adversarial Policy. (arXiv:2305.02605v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02605
&lt;/p&gt;
&lt;p&gt;
IMAP&#26159;&#19968;&#20010;&#20869;&#22312;&#39537;&#21160;&#30340;&#23545;&#25239;&#31574;&#30053;&#65292;&#26080;&#38656;&#21463;&#23475;&#32773;&#31574;&#30053;&#30340;&#20219;&#20309;&#30693;&#35782;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#40657;&#30418;&#35268;&#36991;&#25915;&#20987;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#21644;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#23481;&#26131;&#21463;&#21040;&#35268;&#36991;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#21333;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23545;&#31574;&#30053;&#25110;&#20540;&#32593;&#32476;&#30340;&#36755;&#20837;&#25110;&#36755;&#20986;&#27880;&#20837;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65307;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;&#23545;&#25163;&#38388;&#25509;&#24433;&#21709;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#12290; &#23545;&#25239;&#24615;&#31574;&#30053;&#20026;&#35299;&#20915;&#27492;&#31867;&#25915;&#20987;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#21463;&#23475;&#32773;&#25919;&#31574;&#30340;&#23436;&#32654;&#25110;&#37096;&#20998;&#30693;&#35782;&#65292;&#35201;&#20040;&#30001;&#20110;&#20219;&#21153;&#30456;&#20851;&#22870;&#21169;&#30340;&#31232;&#30095;&#24615;&#32780;&#23548;&#33268;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20869;&#22312;&#39537;&#21160;&#30340;&#23545;&#25239;&#25919;&#31574;&#65288;IMAP&#65289;&#65292;&#29992;&#20110;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#39640;&#25928;&#30340;&#40657;&#30418;&#35268;&#36991;&#25915;&#20987;&#65292;&#32780;&#19981;&#38656;&#20219;&#20309;&#20851;&#20110;&#21463;&#23475;&#32773;&#31574;&#30053;&#30340;&#30693;&#35782;&#12290; IMAP&#21033;&#29992;&#22522;&#20110;&#29366;&#24577;&#35206;&#30422;&#29575;&#65292;&#31574;&#30053;&#35206;&#30422;&#29575;&#65292;&#39118;&#38505;&#21644;&#25919;&#31574;&#20998;&#27495;&#30340;&#22235;&#20010;&#20869;&#22312;&#30446;&#26631;&#65292;&#20197;&#40723;&#21169;&#25506;&#32034;&#24182;&#21457;&#29616;&#26356;&#24378;&#30340;&#25915;&#20987;&#25216;&#33021;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#23454;&#21147;&#30340;&#23545;&#25163;&#30340;&#21487;&#25512;&#24191;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IMAP&#22312;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;Atari&#28216;&#25103;&#65292;&#19968;&#20010;&#26426;&#22120;&#20154;&#36816;&#21160;&#20219;&#21153;&#21644;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) agents are known to be vulnerable to evasion attacks during deployment. In single-agent environments, attackers can inject imperceptible perturbations on the policy or value network's inputs or outputs; in multi-agent environments, attackers can control an adversarial opponent to indirectly influence the victim's observation. Adversarial policies offer a promising solution to craft such attacks. Still, current approaches either require perfect or partial knowledge of the victim policy or suffer from sample inefficiency due to the sparsity of task-related rewards. To overcome these limitations, we propose the Intrinsically Motivated Adversarial Policy (IMAP) for efficient black-box evasion attacks in single- and multi-agent environments without any knowledge of the victim policy. IMAP uses four intrinsic objectives based on state coverage, policy coverage, risk, and policy divergence to encourage exploration and discover stronger attacking skills. We also des
&lt;/p&gt;</description></item><item><title>SCooLS&#26159;&#19968;&#20010;&#26234;&#33021;&#21512;&#32422;&#23398;&#20064;&#24341;&#25806;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;&#20197;&#22826;&#22346;&#21512;&#32422;&#23383;&#33410;&#30721;&#24182;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#21151;&#33021;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#65292;&#20934;&#30830;&#24230;&#39640;&#36798;98.4%&#65292;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28431;&#27934;&#20998;&#26512;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.10737</link><description>&lt;p&gt;
&#21033;&#29992;&#24858;&#34850;&#21512;&#21516;&#36827;&#34892;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Schooling to Exploit Foolish Contracts. (arXiv:2304.10737v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10737
&lt;/p&gt;
&lt;p&gt;
SCooLS&#26159;&#19968;&#20010;&#26234;&#33021;&#21512;&#32422;&#23398;&#20064;&#24341;&#25806;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;&#20197;&#22826;&#22346;&#21512;&#32422;&#23383;&#33410;&#30721;&#24182;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#21151;&#33021;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#65292;&#20934;&#30830;&#24230;&#39640;&#36798;98.4%&#65292;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28431;&#27934;&#20998;&#26512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SCooLS&#65292;&#21363;&#25105;&#20204;&#30340;&#26234;&#33021;&#21512;&#32422;&#23398;&#20064;&#65288;&#21322;&#30417;&#30563;&#65289;&#24341;&#25806;&#12290;SCooLS&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#20998;&#26512;&#20197;&#22826;&#22346;&#21512;&#32422;&#23383;&#33410;&#30721;&#24182;&#35782;&#21035;&#29305;&#23450;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#21151;&#33021;&#12290;SCooLS&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#20803;&#32032;&#65306;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#21322;&#30417;&#30563;&#23398;&#20064;&#27604;&#26080;&#30417;&#30563;&#23398;&#20064;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#22823;&#22411;&#30340;&#26631;&#35760;&#35757;&#32451;&#38598;&#65292;&#32780;&#26377;&#30417;&#30563;&#23398;&#20064;&#21017;&#38656;&#35201;&#12290;GNN&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;&#26234;&#33021;&#21512;&#32422;&#23383;&#33410;&#30721;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#23450;&#20041;&#30340;&#27169;&#24335;&#25110;&#19987;&#23478;&#35268;&#21017;&#12290;SCooLS&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;&#26234;&#33021;&#21512;&#32422;&#28431;&#27934;&#20998;&#26512;&#30340;&#39318;&#20010;&#24212;&#29992;&#65292;&#20063;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28431;&#27934;&#20998;&#26512;&#22120;&#65292;&#21487;&#20197;&#35782;&#21035;&#29305;&#23450;&#26131;&#21463;&#25915;&#20987;&#30340;&#21151;&#33021;&#12290;SCooLS&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#65292;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;98.4%&#65292;F1&#24471;&#20998;&#36798;&#21040;&#20102;90.5%&#65292;&#20551;&#38451;&#24615;&#29575;&#20165;&#20026;0.8%&#12290;&#27492;&#22806;&#65292;SCooLS&#36895;&#24230;&#24456;&#24555;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce SCooLS, our Smart Contract Learning (Semi-supervised) engine. SCooLS uses neural networks to analyze Ethereum contract bytecode and identifies specific vulnerable functions. SCooLS incorporates two key elements: semi-supervised learning and graph neural networks (GNNs). Semi-supervised learning produces more accurate models than unsupervised learning, while not requiring the large oracle-labeled training set that supervised learning requires. GNNs enable direct analysis of smart contract bytecode without any manual feature engineering, predefined patterns, or expert rules.  SCooLS is the first application of semi-supervised learning to smart contract vulnerability analysis, as well as the first deep learning-based vulnerability analyzer to identify specific vulnerable functions. SCooLS's performance is better than existing tools, with an accuracy level of 98.4%, an F1 score of 90.5%, and an exceptionally low false positive rate of only 0.8%. Furthermore, SCooLS is fast, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#23558;&#22797;&#26434;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#36731;&#37327;&#32423;&#27169;&#22411;&#19978;&#65292;&#20174;&#32780;&#20351;&#20302;&#20869;&#23384;&#35774;&#22791;&#20063;&#33021;&#36827;&#34892;&#22797;&#26434;&#32570;&#38519;&#20998;&#31867;&#65292;&#19988;&#26080;&#38656;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.13974</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#20302;&#20869;&#23384;&#35774;&#22791;&#30340;&#28151;&#21512;&#30789;&#29255;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Mixed-Type Wafer Classification For Low Memory Devices Using Knowledge Distillation. (arXiv:2303.13974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#23558;&#22797;&#26434;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#36731;&#37327;&#32423;&#27169;&#22411;&#19978;&#65292;&#20174;&#32780;&#20351;&#20302;&#20869;&#23384;&#35774;&#22791;&#20063;&#33021;&#36827;&#34892;&#22797;&#26434;&#32570;&#38519;&#20998;&#31867;&#65292;&#19988;&#26080;&#38656;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#36896;&#30789;&#29255;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#25968;&#21315;&#20010;&#27493;&#39588;&#12290;&#30789;&#29255;&#22320;&#22270;&#30340;&#32570;&#38519;&#27169;&#24335;&#35782;&#21035;&#23545;&#20110;&#30830;&#23450;&#29983;&#20135;&#32570;&#38519;&#30340;&#26681;&#26412;&#21407;&#22240;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#21487;&#33021;&#36827;&#19968;&#27493;&#20026;&#30789;&#29255;&#24037;&#21378;&#30340;&#20135;&#37327;&#25552;&#39640;&#25552;&#20379;&#35265;&#35299;&#12290;&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#65292;&#21508;&#31181;&#32570;&#38519;&#21487;&#33021;&#21333;&#29420;&#20986;&#29616;&#22312;&#30789;&#29255;&#20013;&#65292;&#20063;&#21487;&#33021;&#20197;&#19981;&#21516;&#30340;&#32452;&#21512;&#24418;&#24335;&#20986;&#29616;&#12290;&#35782;&#21035;&#30789;&#29255;&#20013;&#30340;&#22810;&#20010;&#32570;&#38519;&#36890;&#24120;&#27604;&#35782;&#21035;&#21333;&#20010;&#32570;&#38519;&#26356;&#38590;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#28151;&#21512;&#31867;&#22411;DPR&#26041;&#38754;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32570;&#38519;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#22823;&#22411;&#22797;&#26434;&#27169;&#22411;&#65292;&#20351;&#23427;&#20204;&#24456;&#38590;&#22312;&#36890;&#24120;&#29992;&#20110;&#21046;&#36896;&#23454;&#39564;&#23460;&#30340;&#20302;&#20869;&#23384;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#36816;&#34892;&#12290;&#21478;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#26469;&#35757;&#32451;&#22797;&#26434;&#32593;&#32476;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#35757;&#32451;&#31243;&#24207;&#65292;&#23558;&#22797;&#26434;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#36731;&#37327;&#32423;&#21487;&#37096;&#32626;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20998;&#31867;&#27169;&#22411;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#36275;&#22815;&#39640;&#25928;&#65292;&#21487;&#22312;&#20302;&#20869;&#23384;&#35774;&#22791;&#19978;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manufacturing wafers is an intricate task involving thousands of steps. Defect Pattern Recognition (DPR) of wafer maps is crucial for determining the root cause of production defects, which may further provide insight for yield improvement in wafer foundry. During manufacturing, various defects may appear standalone in the wafer or may appear as different combinations. Identifying multiple defects in a wafer is generally harder compared to identifying a single defect. Recently, deep learning methods have gained significant traction in mixed-type DPR. However, the complexity of defects requires complex and large models making them very difficult to operate on low-memory embedded devices typically used in fabrication labs. Another common issue is the unavailability of labeled data to train complex networks. In this work, we propose an unsupervised training routine to distill the knowledge of complex pre-trained models to lightweight deployment-ready models. We empirically show that this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#35821;&#20041;&#36716;&#25442;&#23545;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;&#30340;&#35821;&#20041;&#36716;&#25442;&#29983;&#25104;&#27169;&#22411;&#25104;&#21151;&#28151;&#28102;&#20102;&#22810;&#20010;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.13408</link><description>&lt;p&gt;
&#35821;&#20041;&#36716;&#25442;&#28151;&#28102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#65292;&#32780;&#26816;&#32034;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. (arXiv:2303.13408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#35821;&#20041;&#36716;&#25442;&#23545;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;&#30340;&#35821;&#20041;&#36716;&#25442;&#29983;&#25104;&#27169;&#22411;&#25104;&#21151;&#28151;&#28102;&#20102;&#22810;&#20010;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#26377;&#22810;&#31181;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#29992;&#20110;&#35782;&#21035;&#24694;&#24847;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (&#20363;&#22914;&#34394;&#20551;&#20869;&#23481;&#21019;&#24314;&#25110;&#23398;&#26415;&#25220;&#34989;)&#20013;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#65292;&#21253;&#25324;&#36890;&#36807;&#27700;&#21360;&#25110;&#32479;&#35745;&#24322;&#24120;&#28857;&#12290;&#26412;&#25991;&#25506;&#31350;&#36825;&#20123;&#25991;&#26412;&#26816;&#27979;&#31639;&#27861;&#23545;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#21547;&#20041;&#36716;&#25442;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;11B&#21442;&#25968;&#30340;&#35821;&#20041;&#36716;&#25442;&#29983;&#25104;&#27169;&#22411;(DIPPER)&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#27573;&#33853;&#36827;&#34892;&#35821;&#20041;&#36716;&#25442;&#65292;&#21487;&#36873;&#25321;&#21033;&#29992;&#21608;&#22260;&#25991;&#26412;(&#20363;&#22914;&#29992;&#25143;&#20889;&#30340;&#25552;&#31034;)&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;DIPPER&#36824;&#20351;&#29992;&#26631;&#37327;&#26059;&#38062;&#26469;&#25511;&#21046;&#35821;&#20041;&#36716;&#25442;&#20013;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#37325;&#26032;&#25490;&#21015;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;DIPPER&#26469;&#36827;&#34892;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#35821;&#20041;&#36716;&#25442;&#65292;&#25104;&#21151;&#22320;&#28151;&#28102;&#20102;&#22810;&#20010;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26816;&#27979;&#12289;GPTZero&#12289;DetectGPT&#21644;OpenAI&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#20363;&#22914;&#65292;DIPPER&#23558;DetectGPT&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#20174;70.3%&#38477;&#33267;4.6%&#65288;&#22312;&#24658;&#23450;&#30340;1%&#35823;&#25253;&#29575;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
To detect the deployment of large language models for malicious use cases (e.g., fake content creation or academic plagiarism), several approaches have recently been proposed for identifying AI-generated text via watermarks or statistical irregularities. How robust are these detection algorithms to paraphrases of AI-generated text? To stress test these detectors, we first train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, optionally leveraging surrounding text (e.g., user-written prompts) as context. DIPPER also uses scalar knobs to control the amount of lexical diversity and reordering in the paraphrases. Paraphrasing text generated by three large language models (including GPT3.5-davinci-003) with DIPPER successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), withou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26126;&#30830;&#22320;&#32534;&#30721;&#34507;&#30333;&#36136;&#32467;&#26500;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26500;&#24863;&#30693;&#34507;&#30333;&#36136;&#34920;&#31034;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06275</link><description>&lt;p&gt;
&#32467;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Enhancing Protein Language Models with Structure-based Encoder and Pre-training. (arXiv:2303.06275v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26126;&#30830;&#22320;&#32534;&#30721;&#34507;&#30333;&#36136;&#32467;&#26500;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26500;&#24863;&#30693;&#34507;&#30333;&#36136;&#34920;&#31034;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes enhancing protein language models with structure-based encoder and pre-training to explicitly encode protein structures for better structure-aware protein representations, and empirically verifies its effectiveness.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#34507;&#30333;&#36136;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#34507;&#30333;&#36136;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#23613;&#31649;&#33021;&#22815;&#38544;&#24335;&#22320;&#25429;&#33719;&#27531;&#22522;&#38388;&#30340;&#25509;&#35302;&#20449;&#24687;&#65292;&#20294;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;PLMs&#19981;&#33021;&#26126;&#30830;&#22320;&#32534;&#30721;&#34507;&#30333;&#36136;&#32467;&#26500;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26500;&#24863;&#30693;&#34507;&#30333;&#36136;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#32467;&#26500;&#23545;&#20110;&#30830;&#23450;&#21151;&#33021;&#24456;&#37325;&#35201;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#22312;&#21487;&#29992;&#34507;&#30333;&#36136;&#32467;&#26500;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#25913;&#36827;&#36825;&#20123;PLMs&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20351;&#29992;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;PLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein language models (PLMs) pre-trained on large-scale protein sequence corpora have achieved impressive performance on various downstream protein understanding tasks. Despite the ability to implicitly capture inter-residue contact information, transformer-based PLMs cannot encode protein structures explicitly for better structure-aware protein representations. Besides, the power of pre-training on available protein structures has not been explored for improving these PLMs, though structures are important to determine functions. To tackle these limitations, in this work, we enhance the PLMs with structure-based encoder and pre-training. We first explore feasible model architectures to combine the advantages of a state-of-the-art PLM (i.e., ESM-1b1) and a state-of-the-art protein structure encoder (i.e., GearNet). We empirically verify the ESM-GearNet that connects two encoders in a series way as the most effective combination model. To further improve the effectiveness of ESM-GearNe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26410;&#26469;&#30340;&#24341;&#21147;&#27874;&#20219;&#21153;&#37325;&#24314;&#20102;&#21704;&#21187;&#21442;&#25968;&#65292;&#24471;&#20986;&#20102;&#39640;&#26031;&#36807;&#31243;&#22312;&#37325;&#24314;&#23431;&#23449;&#33192;&#32960;&#21382;&#21490;&#26041;&#38754;&#30340;&#31283;&#20581;&#24615;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#26410;&#26469;&#20219;&#21153;&#33021;&#22815;&#25552;&#20379;&#19982;&#24403;&#21069;&#25968;&#25454;&#38598;&#30456;&#31454;&#20105;&#30340;&#23545;&#21704;&#21187;&#21442;&#25968;&#21644;&#21704;&#21187;&#24120;&#25968;&#30340;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2303.05169</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26410;&#26469;&#24341;&#21147;&#27874;&#20219;&#21153;&#37325;&#24314;&#21704;&#21187;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Reconstructing the Hubble parameter with future Gravitational Wave missions using Machine Learning. (arXiv:2303.05169v2 [astro-ph.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26410;&#26469;&#30340;&#24341;&#21147;&#27874;&#20219;&#21153;&#37325;&#24314;&#20102;&#21704;&#21187;&#21442;&#25968;&#65292;&#24471;&#20986;&#20102;&#39640;&#26031;&#36807;&#31243;&#22312;&#37325;&#24314;&#23431;&#23449;&#33192;&#32960;&#21382;&#21490;&#26041;&#38754;&#30340;&#31283;&#20581;&#24615;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#26410;&#26469;&#20219;&#21153;&#33021;&#22815;&#25552;&#20379;&#19982;&#24403;&#21069;&#25968;&#25454;&#38598;&#30456;&#31454;&#20105;&#30340;&#23545;&#21704;&#21187;&#21442;&#25968;&#21644;&#21704;&#21187;&#24120;&#25968;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#65292;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#21363;&#23558;&#21040;&#26469;&#30340;&#24341;&#21147;&#27874;&#20219;&#21153;&#65292;&#21363;&#36827;&#21270;&#30340;&#28608;&#20809;&#24178;&#28041;&#31354;&#38388;&#22825;&#32447;&#65288;eLISA&#65289;&#21644;&#29233;&#22240;&#26031;&#22374;&#26395;&#36828;&#38236;&#65288;ET&#65289;&#65292;&#37325;&#24314;&#21704;&#21187;&#21442;&#25968;$H(z)$&#30340;&#21069;&#26223;&#12290;&#22312;&#20551;&#35774;&#21508;&#31181;&#32972;&#26223;&#23431;&#23449;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;GP&#20197;&#38750;&#21442;&#25968;&#21270;&#30340;&#26041;&#24335;&#37325;&#24314;&#20102;&#21704;&#21187;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#27599;&#20010;&#20219;&#21153;&#30340;&#30495;&#23454;&#29983;&#25104;&#30446;&#24405;&#12290;&#25105;&#20204;&#37325;&#28857;&#20998;&#21035;&#20851;&#27880;&#20102;&#26089;&#26399;&#21644;&#21518;&#26399;&#30340;&#20808;&#39564;&#23545;$H(z)$&#21644;&#21704;&#21187;&#24120;&#25968;($H_0$)&#37325;&#24314;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#32771;&#34385;&#30340;&#29305;&#23450;&#20219;&#21153;&#30340;&#35266;&#27979;&#31383;&#21475;&#33539;&#22260;&#20869;&#65292;GP&#22312;&#37325;&#24314;&#23431;&#23449;&#30340;&#33192;&#32960;&#21382;&#21490;&#26041;&#38754;&#38750;&#24120;&#31283;&#20581;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#35748;&#65292;eLISA&#21644;ET&#37117;&#23558;&#33021;&#22815;&#25552;&#20379;&#23545;$H(z)$&#21644;$H_0$&#30340;&#32422;&#26463;&#65292;&#36825;&#23558;&#19982;&#24403;&#21069;&#25968;&#25454;&#38598;&#24471;&#20986;&#30340;&#32422;&#26463;&#30456;&#31454;&#20105;&#12290;&#23588;&#20854;&#26159;&#65292;w
&lt;/p&gt;
&lt;p&gt;
We study the prospects of Gaussian processes (GP), a machine learning (ML) algorithm, as a tool to reconstruct the Hubble parameter $H(z)$ with two upcoming gravitational wave missions, namely the evolved Laser Interferometer Space Antenna (eLISA) and the Einstein Telescope (ET). Assuming various background cosmological models, the Hubble parameter has been reconstructed in a non-parametric manner with the help of GP using realistically generated catalogs for each mission. The effects of early-time and late-time priors on the reconstruction of $H(z)$, and hence on the Hubble constant ($H_0$), have also been focused on separately. Our analysis reveals that GP is quite robust in reconstructing the expansion history of the Universe within the observational window of the specific missions under consideration. We further confirm that both eLISA and ET would be able to provide constraints on $H(z)$ and $H_0$ which would be competitive to those inferred from current datasets. In particular, w
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65288;MAFL&#65289;&#65292;&#35813;&#31995;&#32479;&#19981;&#20165;&#36866;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#20915;&#31574;&#26641;&#31561;&#29305;&#23450;&#31867;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36824;&#33021;&#22312;&#38750;DNN&#22330;&#26223;&#20013;&#24212;&#29992;&#65292;&#24182;&#19988;&#36890;&#36807;&#20248;&#21270;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.04906</link><description>&lt;p&gt;
&#27169;&#22411;&#26080;&#20851;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-Agnostic Federated Learning. (arXiv:2303.04906v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04906
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65288;MAFL&#65289;&#65292;&#35813;&#31995;&#32479;&#19981;&#20165;&#36866;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#20915;&#31574;&#26641;&#31561;&#29305;&#23450;&#31867;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36824;&#33021;&#22312;&#38750;DNN&#22330;&#26223;&#20013;&#24212;&#29992;&#65292;&#24182;&#19988;&#36890;&#36807;&#20248;&#21270;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;2016&#24180;&#24341;&#20837;&#20197;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#19968;&#30452;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#20869;&#37096;&#24037;&#20316;&#32039;&#23494;&#30456;&#20851;&#12290;&#19968;&#26041;&#38754;&#65292;&#36825;&#20351;&#24471;&#20854;&#24471;&#20197;&#22312;DNN&#24191;&#27867;&#20351;&#29992;&#30340;&#21516;&#26102;&#21457;&#23637;&#21644;&#25512;&#24191;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36825;&#24573;&#35270;&#20102;&#25152;&#26377;&#37027;&#20123;&#19981;&#21487;&#34892;&#25110;&#19981;&#20855;&#20248;&#21183;&#20351;&#29992;DNN&#30340;&#24773;&#20917;&#12290;&#30446;&#21069;&#22823;&#37096;&#20998;FL&#26694;&#26550;&#20165;&#20801;&#35768;&#35757;&#32451;DNN&#65292;&#21152;&#21095;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;DNN&#22330;&#26223;&#19979;&#32570;&#20047;FL&#35299;&#20915;&#26041;&#26696;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MAFL&#65288;&#27169;&#22411;&#26080;&#20851;&#30340;&#32852;&#37030;&#23398;&#20064;&#65289;&#12290;MAFL&#23558;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;FL&#31639;&#27861;AdaBoost.F&#19982;&#19968;&#31181;&#24320;&#25918;&#30340;&#24037;&#19994;&#32423;FL&#26694;&#26550;Intel OpenFL&#32467;&#21512;&#36215;&#26469;&#12290;MAFL&#26159;&#31532;&#19968;&#20010;&#19981;&#19982;&#20219;&#20309;&#29305;&#23450;&#31867;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32465;&#23450;&#30340;FL&#31995;&#32479;&#65292;&#20801;&#35768;&#25506;&#32034;&#36229;&#36234;DNN&#21644;&#26641;&#30340;FL&#22330;&#26223;&#12290;&#25105;&#20204;&#20174;&#22810;&#20010;&#35282;&#24230;&#27979;&#35797;&#20102;MAFL&#65292;&#35780;&#20272;&#20102;&#20854;&#27491;&#30830;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#26368;&#22810;&#36798;&#21040;64&#20010;&#33410;&#28857;&#12290;&#25105;&#20204;&#23545;&#22522;&#30784;&#36719;&#20214;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#22312;&#26631;&#20934;FL&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;5.5&#20493;&#30340;&#21152;&#36895;&#12290;MAFL&#19982;
&lt;/p&gt;
&lt;p&gt;
Since its debut in 2016, Federated Learning (FL) has been tied to the inner workings of Deep Neural Networks (DNNs). On the one hand, this allowed its development and widespread use as DNNs proliferated. On the other hand, it neglected all those scenarios in which using DNNs is not possible or advantageous. The fact that most current FL frameworks only allow training DNNs reinforces this problem. To address the lack of FL solutions for non-DNN-based use cases, we propose MAFL (Model-Agnostic Federated Learning). MAFL marries a model-agnostic FL algorithm, AdaBoost.F, with an open industry-grade FL framework: Intel OpenFL. MAFL is the first FL system not tied to any specific type of machine learning model, allowing exploration of FL scenarios beyond DNNs and trees. We test MAFL from multiple points of view, assessing its correctness, flexibility and scaling properties up to 64 nodes. We optimised the base software achieving a 5.5x speedup on a standard FL scenario. MAFL is compatible wi
&lt;/p&gt;</description></item><item><title>Taylor TD&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;TD&#26356;&#26032;&#30340;&#27888;&#21202;&#32423;&#25968;&#23637;&#24320;&#65292;&#20943;&#23569;&#20102;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#35774;&#32622;&#20013;&#30340;&#26041;&#24046;&#65292;&#24182;&#20855;&#26377;&#19982;&#26631;&#20934;TD&#23398;&#20064;&#30456;&#21516;&#30340;&#31283;&#23450;&#23398;&#20064;&#20445;&#35777;&#12290;TaTD3&#26159;Taylor TD&#19982;TD3&#31639;&#27861;&#30456;&#32467;&#21512;&#25152;&#24418;&#25104;&#30340;&#26041;&#27861;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2302.14182</link><description>&lt;p&gt;
Taylor TD&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Taylor TD-learning. (arXiv:2302.14182v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14182
&lt;/p&gt;
&lt;p&gt;
Taylor TD&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;TD&#26356;&#26032;&#30340;&#27888;&#21202;&#32423;&#25968;&#23637;&#24320;&#65292;&#20943;&#23569;&#20102;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#35774;&#32622;&#20013;&#30340;&#26041;&#24046;&#65292;&#24182;&#20855;&#26377;&#19982;&#26631;&#20934;TD&#23398;&#20064;&#30456;&#21516;&#30340;&#31283;&#23450;&#23398;&#20064;&#20445;&#35777;&#12290;TaTD3&#26159;Taylor TD&#19982;TD3&#31639;&#27861;&#30456;&#32467;&#21512;&#25152;&#24418;&#25104;&#30340;&#26041;&#27861;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#26102;&#38388;&#24046;&#20998;&#65288;TD&#65289;&#23398;&#20064;&#26469;&#23398;&#20064;&#19968;&#20010;&#35780;&#35770;&#23478;&#12290;&#28982;&#32780;&#65292;TD&#23398;&#20064;&#30340;&#26356;&#26032;&#21487;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#26041;&#24046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#26694;&#26550;&#65292;&#21363;Taylor TD&#65292;&#23427;&#20943;&#23569;&#20102;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#35774;&#32622;&#20013;&#30340;&#26041;&#24046;&#12290;Taylor TD&#20351;&#29992;TD&#26356;&#26032;&#30340;&#19968;&#38454;&#27888;&#21202;&#32423;&#25968;&#23637;&#24320;&#12290;&#35813;&#23637;&#24320;&#20801;&#35768;Taylor TD&#22312;&#34892;&#21160;&#36873;&#25321;&#30340;&#38543;&#26426;&#24615;&#21644;&#27599;&#20010;TD&#26356;&#26032;&#30340;&#21021;&#22987;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#29366;&#24577;&#20998;&#24067;&#30340;&#19968;&#20123;&#38543;&#26426;&#24615;&#19978;&#36827;&#34892;&#20998;&#26512;&#31215;&#20998;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;Taylor TD&#30340;&#26356;&#26032;&#30830;&#23454;&#27604;&#26631;&#20934;&#30340;TD&#26356;&#26032;&#20855;&#26377;&#36739;&#20302;&#30340;&#26041;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#21512;&#29702;&#30340;&#20551;&#35774;&#19979;&#65292;Taylor TD&#20855;&#26377;&#19982;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#26631;&#20934;TD&#23398;&#20064;&#30456;&#21516;&#30340;&#31283;&#23450;&#23398;&#20064;&#20445;&#35777;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;Taylor TD&#19982;TD3&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;TaTD3&#12290;&#25105;&#20204;&#23637;&#31034;TaTD3&#30340;&#34920;&#29616;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#22522;&#20934;&#30456;&#24403;&#65292;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many reinforcement learning approaches rely on temporal-difference (TD) learning to learn a critic. However, TD-learning updates can be high variance. Here, we introduce a model-based RL framework, Taylor TD, which reduces this variance in continuous state-action settings. Taylor TD uses a first-order Taylor series expansion of TD updates. This expansion allows Taylor TD to analytically integrate over stochasticity in the action-choice, and some stochasticity in the state distribution for the initial state and action of each TD update. We include theoretical and empirical evidence that Taylor TD updates are indeed lower variance than standard TD updates. Additionally, we show Taylor TD has the same stable learning guarantees as standard TD-learning with linear function approximation under a reasonable assumption. Next, we combine Taylor TD with the TD3 algorithm, forming TaTD3. We show TaTD3 performs as well, if not better, than several state-of-the art model-free and model-based basel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#31232;&#30095;&#23398;&#20064;&#26694;&#26550;S3MA&#65292;&#29992;&#20110;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#30340;&#31232;&#30095;&#31354;&#38388;&#21644;&#22810;&#31890;&#24230;&#30456;&#20284;&#24230;&#26469;&#25913;&#36827;&#26816;&#32034;&#25928;&#26524;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09473</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#31232;&#30095;&#22810;&#31890;&#24230;&#23398;&#20064;&#36827;&#34892;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Video-Text Retrieval by Supervised Sparse Multi-Grained Learning. (arXiv:2302.09473v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#31232;&#30095;&#23398;&#20064;&#26694;&#26550;S3MA&#65292;&#29992;&#20110;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#30340;&#31232;&#30095;&#31354;&#38388;&#21644;&#22810;&#31890;&#24230;&#30456;&#20284;&#24230;&#26469;&#25913;&#36827;&#26816;&#32034;&#25928;&#26524;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65292;&#26368;&#36817;&#22312;&#25506;&#32034;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#31890;&#24230;&#31232;&#30095;&#23398;&#20064;&#26694;&#26550;S3MA&#65292;&#29992;&#20110;&#23398;&#20064;&#35270;&#39057;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#20849;&#20139;&#31232;&#30095;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#12290;&#20849;&#20139;&#31232;&#30095;&#31354;&#38388;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#31232;&#30095;&#27010;&#24565;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#27599;&#20010;&#27010;&#24565;&#37117;&#23545;&#24212;&#19968;&#20123;&#35789;&#35821;&#12290;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#25105;&#20204;&#20197;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#21644;&#26356;&#26032;&#20849;&#20139;&#31232;&#30095;&#31354;&#38388;&#65292;&#20351;&#29992;&#25552;&#20986;&#30340;&#30456;&#20284;&#24230;&#21644;&#23545;&#40784;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#22810;&#31890;&#24230;&#23545;&#40784;&#65292;&#25105;&#20204;&#23558;&#24103;&#34920;&#31034;&#26041;&#27861;&#32435;&#20837;&#27169;&#22411;&#65292;&#26356;&#22909;&#22320;&#23545;&#35270;&#39057;&#27169;&#24577;&#36827;&#34892;&#24314;&#27169;&#21644;&#35745;&#31639;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#30340;&#30456;&#20284;&#24230;&#12290;&#36890;&#36807;&#23398;&#20064;&#24471;&#21040;&#30340;&#20849;&#20139;&#31232;&#30095;&#31354;&#38388;&#21644;&#22810;&#31890;&#24230;&#30456;&#20284;&#24230;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;S3MA&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/yim&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent progress in video-text retrieval has been advanced by the exploration of better representation learning, in this paper, we present a novel multi-grained sparse learning framework, S3MA, to learn an aligned sparse space shared between the video and the text for video-text retrieval. The shared sparse space is initialized with a finite number of sparse concepts, each of which refers to a number of words. With the text data at hand, we learn and update the shared sparse space in a supervised manner using the proposed similarity and alignment losses. Moreover, to enable multi-grained alignment, we incorporate frame representations for better modeling the video modality and calculating fine-grained and coarse-grained similarities. Benefiting from the learned shared sparse space and multi-grained similarities, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of S3MA over existing methods. Our code is available at https://github.com/yim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#40657;&#30418;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#26041;&#27861;Marich&#65292;&#23427;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#30340;&#26368;&#23567;&#25968;&#37327;&#26597;&#35810;&#26469;&#21019;&#24314;&#19968;&#20010;&#19982;&#30446;&#26631;&#27169;&#22411;&#20855;&#26377;&#20449;&#24687;&#20016;&#23500;&#24230;&#21644;&#20998;&#24067;&#31561;&#20215;&#24615;&#30340;&#21103;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;Marich&#33021;&#25552;&#21462;&#20855;&#26377;60-95%&#30495;&#23454;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.08466</link><description>&lt;p&gt;
Marich&#65306;&#19968;&#31181;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#30340;&#26597;&#35810;&#25928;&#29575;&#39640;&#30340;&#20998;&#24067;&#31561;&#20215;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack using Public Data. (arXiv:2302.08466v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#40657;&#30418;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#26041;&#27861;Marich&#65292;&#23427;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#30340;&#26368;&#23567;&#25968;&#37327;&#26597;&#35810;&#26469;&#21019;&#24314;&#19968;&#20010;&#19982;&#30446;&#26631;&#27169;&#22411;&#20855;&#26377;&#20449;&#24687;&#20016;&#23500;&#24230;&#21644;&#20998;&#24067;&#31561;&#20215;&#24615;&#30340;&#21103;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;Marich&#33021;&#25552;&#21462;&#20855;&#26377;60-95%&#30495;&#23454;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#35774;&#35745;&#40657;&#30418;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#39044;&#27979;API&#20174;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21521;&#30446;&#26631;ML&#27169;&#22411;&#21457;&#36865;&#26368;&#23567;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#20449;&#24687;&#20016;&#23500;&#24230;&#21644;&#20998;&#24067;&#31561;&#20215;&#24615;&#30340;&#30446;&#26631;&#21103;&#26412;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20998;&#24067;&#31561;&#20215;&#21644;&#26368;&#22823;&#20449;&#24687;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#24182;&#23558;&#23427;&#20204;&#31616;&#21270;&#20026;&#19968;&#20010;&#21464;&#20998;&#20248;&#21270;&#38382;&#39064;&#12290;&#25915;&#20987;&#32773;&#39034;&#24207;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#29109;&#21644;&#38477;&#20302;&#30446;&#26631;&#21644;&#30423;&#31363;&#27169;&#22411;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#25277;&#26679;&#30340;&#26597;&#35810;&#36873;&#25321;&#31639;&#27861;Marich&#65292;&#23427;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#20197;&#21450;&#19981;&#21516;&#30340;&#27169;&#22411;&#19978;&#35780;&#20272;&#20102;Marich&#12290;Marich&#25552;&#21462;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#30495;&#23454;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;60-95&#65285;&#65292;&#24182;&#20351;&#29992;&#20102;&#26469;&#33258;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;1,000-8,500&#20010;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study design of black-box model extraction attacks that can send minimal number of queries from a publicly available dataset to a target ML model through a predictive API with an aim to create an informative and distributionally equivalent replica of the target. First, we define distributionally equivalent and Max-Information model extraction attacks, and reduce them into a variational optimisation problem. The attacker sequentially solves this optimisation problem to select the most informative queries that simultaneously maximise the entropy and reduce the mismatch between the target and the stolen models. This leads to an active sampling-based query selection algorithm, Marich, which is model-oblivious. Then, we evaluate Marich on different text and image data sets, and different models, including CNNs and BERT. Marich extracts models that achieve $\sim 60-95\%$ of true model's accuracy and uses $\sim 1,000 - 8,500$ queries from the publicly available datasets, which are differen
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65292;&#29992;&#20110;&#23558;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#26696;&#26144;&#23556;&#21040;FastFlow&#24182;&#34892;&#32534;&#31243;&#24211;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#22788;&#29702;&#22120;&#24179;&#21488;&#19978;&#29983;&#25104;&#19981;&#21516;&#30340;DML&#26041;&#26696;&#65292;&#30740;&#31350;&#32773;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#26696;&#21644;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#31227;&#26893;&#20102;PyTorch&#26694;&#26550;&#21040;RISC-V&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2302.07946</link><description>&lt;p&gt;
&#29992;&#20110;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#20852;RISC-V&#31995;&#32479;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Experimenting with Emerging RISC-V Systems for Decentralised Machine Learning. (arXiv:2302.07946v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07946
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65292;&#29992;&#20110;&#23558;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#26696;&#26144;&#23556;&#21040;FastFlow&#24182;&#34892;&#32534;&#31243;&#24211;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#22788;&#29702;&#22120;&#24179;&#21488;&#19978;&#29983;&#25104;&#19981;&#21516;&#30340;DML&#26041;&#26696;&#65292;&#30740;&#31350;&#32773;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#26696;&#21644;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#31227;&#26893;&#20102;PyTorch&#26694;&#26550;&#21040;RISC-V&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;DML&#65289;&#20351;&#21512;&#20316;&#26426;&#22120;&#23398;&#20064;&#25670;&#33073;&#20102;&#38598;&#20013;&#24335;&#36755;&#20837;&#25968;&#25454;&#12290;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#36793;&#32536;&#25512;&#26029;&#26159;DML&#30340;&#31034;&#20363;&#12290;&#34429;&#28982;DML&#24037;&#20855;&#65288;&#29305;&#21035;&#26159;FL&#65289;&#24320;&#22987;&#34028;&#21187;&#21457;&#23637;&#65292;&#20294;&#35768;&#22810;&#24037;&#20855;&#19981;&#22815;&#28789;&#27963;&#21644;&#20415;&#25658;&#65292;&#26080;&#27861;&#29992;&#20110;&#23454;&#39564;&#26032;&#22411;&#22788;&#29702;&#22120;&#65288;&#20363;&#22914;RISC-V&#65289;&#65292;&#38750;&#20840;&#36830;&#25509;&#32593;&#32476;&#25299;&#25169;&#21644;&#24322;&#27493;&#21327;&#20316;&#26041;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#23558;DML&#26041;&#26696;&#26144;&#23556;&#21040;&#22522;&#30784;&#20013;&#38388;&#20214;&#65288;&#21363;FastFlow&#24182;&#34892;&#32534;&#31243;&#24211;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;x86-64&#21644;ARM&#24179;&#21488;&#20197;&#21450;&#26032;&#20852;&#30340;RISC-V&#24179;&#21488;&#19978;&#29983;&#25104;&#19981;&#21516;&#30340;DML&#26041;&#26696;&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#21644;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#12290;&#20316;&#20026;&#38468;&#24102;&#20135;&#21697;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PyTorch&#26694;&#26550;&#30340;RISC-V&#31227;&#26893;&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#31227;&#26893;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralised Machine Learning (DML) enables collaborative machine learning without centralised input data. Federated Learning (FL) and Edge Inference are examples of DML. While tools for DML (especially FL) are starting to flourish, many are not flexible and portable enough to experiment with novel processors (e.g., RISC-V), non-fully connected network topologies, and asynchronous collaboration schemes. We overcome these limitations via a domain-specific language allowing us to map DML schemes to an underlying middleware, i.e. the FastFlow parallel programming library. We experiment with it by generating different working DML schemes on x86-64 and ARM platforms and an emerging RISC-V one. We characterise the performance and energy efficiency of the presented schemes and systems. As a byproduct, we introduce a RISC-V porting of the PyTorch framework, the first publicly available to our knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;DTI&#39046;&#22495;&#20013;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#26469;&#27719;&#38598;&#21046;&#33647;&#25968;&#25454;&#65292;&#30456;&#23545;&#20110;&#26368;&#20339;&#38750;&#38544;&#31169;&#20445;&#25252;&#26367;&#20195;&#26041;&#27861;&#21487;&#33719;&#24471;&#39640;&#36798;15%&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#19988;&#38750;IID&#25968;&#25454;&#20998;&#24067;&#19981;&#20250;&#38477;&#20302;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07684</link><description>&lt;p&gt;
&#38754;&#21521;&#33647;&#29289;&#38774;&#21521;&#30456;&#20114;&#20316;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Federated Learning Benchmark for Drug-Target Interaction. (arXiv:2302.07684v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;DTI&#39046;&#22495;&#20013;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#26469;&#27719;&#38598;&#21046;&#33647;&#25968;&#25454;&#65292;&#30456;&#23545;&#20110;&#26368;&#20339;&#38750;&#38544;&#31169;&#20445;&#25252;&#26367;&#20195;&#26041;&#27861;&#21487;&#33719;&#24471;&#39640;&#36798;15%&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#19988;&#38750;IID&#25968;&#25454;&#20998;&#24067;&#19981;&#20250;&#38477;&#20302;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#38774;&#21521;&#30456;&#20114;&#20316;&#29992;(DTI)&#39046;&#22495;&#20013;&#27719;&#38598;&#21046;&#33647;&#25968;&#25454;&#20855;&#26377;&#25552;&#20379;&#25405;&#25937;&#29983;&#21629;&#30340;&#31361;&#30772;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30417;&#31649;&#38480;&#21046;&#21644;&#21830;&#19994;&#21033;&#30410;&#65292;&#36825;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#35748;&#20026;&#36825;&#19982;&#34892;&#19994;&#30340;&#38480;&#21046;&#26159;&#21644;&#35299;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#20849;&#20139;&#20219;&#20309;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#25581;&#31034;&#23454;&#20307;&#30340;&#25968;&#25454;&#25110;&#20219;&#20309;&#20854;&#20182;&#30340;&#39640;&#27700;&#24179;&#24635;&#32467;&#12290;&#24403;&#36816;&#29992;&#20110;&#20195;&#34920;&#24615;&#30340;GraphDTA&#27169;&#22411;&#21644;KIBA&#25968;&#25454;&#38598;&#26102;&#65292;&#30456;&#23545;&#20110;&#26368;&#20339;&#30340;&#38750;&#38544;&#31169;&#20445;&#25252;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;15%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;DTI&#25968;&#25454;&#38598;&#20013;&#65292;&#19982;&#20854;&#20182;&#39046;&#22495;&#19981;&#21516;&#30340;&#26159;&#65292;&#38750;IID&#25968;&#25454;&#20998;&#24067;&#19981;&#20250;&#38477;&#20302;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#28155;&#21152;&#26032;&#25968;&#25454;&#30340;&#30410;&#22788;&#19982;&#28155;&#21152;&#26356;&#22810;&#23458;&#25143;&#30340;&#25104;&#26412;&#20043;&#38388;&#30340;&#23454;&#36136;&#24615;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aggregating pharmaceutical data in the drug-target interaction (DTI) domain has the potential to deliver life-saving breakthroughs. It is, however, notoriously difficult due to regulatory constraints and commercial interests. This work proposes the application of federated learning, which we argue to be reconcilable with the industry's constraints, as it does not require sharing of any information that would reveal the entities' data or any other high-level summary of it. When used on a representative GraphDTA model and the KIBA dataset it achieves up to 15% improved performance relative to the best available non-privacy preserving alternative. Our extensive battery of experiments shows that, unlike in other domains, the non-IID data distribution in the DTI datasets does not deteriorate FL performance. Additionally, we identify a material trade-off between the benefits of adding new data, and the cost of adding more clients.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21363;&#22238;&#39038;&#38142;&#65292;&#21487;&#20197;&#36731;&#26494;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.02676</link><description>&lt;p&gt;
&#22238;&#39038;&#38142;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#21453;&#39304;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21363;&#22238;&#39038;&#38142;&#65292;&#21487;&#20197;&#36731;&#26494;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#36825;&#26679;&#25165;&#33021;&#23545;&#20154;&#31867;&#26377;&#25152;&#24110;&#21161;&#24182;&#31526;&#21512;&#20154;&#31867;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#26469;&#29702;&#35299;&#21644;&#36981;&#24490;&#25351;&#20196;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#26159;&#22522;&#20110;&#34987;&#20154;&#31867;&#27880;&#37322;&#32773;&#21916;&#27426;&#30340;&#25163;&#21160;&#25361;&#36873;&#30340;&#27169;&#22411;&#29983;&#25104;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#25968;&#25454;&#21033;&#29992;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#19988;&#26222;&#36941;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#22870;&#21169;&#20989;&#25968;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#36825;&#23481;&#26131;&#20986;&#29616;&#22870;&#21169;&#20989;&#25968;&#19981;&#23436;&#32654;&#21644;&#26497;&#38590;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#8220;&#22238;&#39038;&#38142;&#8221;&#65292;&#23427;&#26131;&#20110;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#21463;&#21040;&#20102;&#20154;&#31867;&#22914;&#20309;&#20174;&#20197;&#35821;&#35328;&#24418;&#24335;&#21576;&#29616;&#30340;&#24191;&#27867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#31867;&#22411;&#30340;&#21453;&#39304;&#36716;&#25442;&#25104;&#21477;&#23376;&#65292;&#28982;&#21518;&#29992;&#23427;&#20204;&#26469;&#24494;&#35843;&#27169;&#22411;&#65292;&#20174;&#32780;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human preferences is important for language models to be helpful and useful for humans, and to align with human and social values. Prior work have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them ineffective in terms of data utilization and challenging to apply in general, or they depend on reward functions and reinforcement learning, which are prone to imperfect reward function and extremely challenging to optimize. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sentences, which are then used to fine-tune the model, allowing us to take advantage
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#30340;&#20851;&#31995;&#32467;&#26500;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#39046;&#22495;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2301.12321</link><description>&lt;p&gt;
&#31070;&#32463;&#20851;&#31995;&#22270;&#65306;&#35782;&#21035;&#26631;&#31614;&#22122;&#38899;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data. (arXiv:2301.12321v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#30340;&#20851;&#31995;&#32467;&#26500;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#39046;&#22495;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35786;&#26029;&#21644;&#28165;&#29702;&#25968;&#25454;&#26159;&#26500;&#24314;&#20581;&#22766;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23384;&#22312;&#22797;&#26434;&#38382;&#39064;&#65292;&#22914;&#26631;&#31614;&#38169;&#35823;&#12289;&#27424;&#34920;&#31034;&#21644;&#24322;&#24120;&#20540;&#65292;&#22240;&#27492;&#22312;&#20855;&#26377;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29305;&#24449;&#23884;&#20837;&#31354;&#38388;&#20013;&#25968;&#25454;&#30340;&#20851;&#31995;&#32467;&#26500;&#36825;&#19968;&#34987;&#24573;&#35270;&#30340;&#20449;&#24687;&#26469;&#28304;&#65292;&#26469;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#25968;&#25454;&#30340;&#32479;&#19968;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#20851;&#31995;&#22270;&#32467;&#26500;&#26469;&#26816;&#27979;&#26631;&#31614;&#38169;&#35823;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#21644;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#25552;&#20379;&#29305;&#24449;&#23884;&#20837;&#31354;&#38388;&#20013;&#25968;&#25454;&#28857;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20316;&#20026;&#20132;&#20114;&#24335;&#35786;&#26029;&#25968;&#25454;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#39046;&#22495;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26631;&#31614;&#38169;&#35823;&#21644;&#31163;&#32676;&#20540;/&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosing and cleaning data is a crucial step for building robust machine learning systems. However, identifying problems within large-scale datasets with real-world distributions is challenging due to the presence of complex issues such as label errors, under-representation, and outliers. In this paper, we propose a unified approach for identifying the problematic data by utilizing a largely ignored source of information: a relational structure of data in the feature-embedded space. To this end, we present scalable and effective algorithms for detecting label errors and outlier data based on the relational graph structure of data. We further introduce a visualization tool that provides contextual information of a data point in the feature-embedded space, serving as an effective tool for interactively diagnosing data. We evaluate the label error and outlier/out-of-distribution (OOD) detection performances of our approach on the large-scale image, speech, and language domain tasks, inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#21464;&#21338;&#24328;&#20013;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;OGD&#65289;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#26126;&#30830;&#30340;&#25910;&#25947;&#30028;&#38480;&#65292;&#24182;&#24314;&#31435;&#20102;&#36866;&#29992;&#20110;&#26102;&#21464;&#24635;&#21644;&#22810;&#20154;&#21338;&#24328;&#21644;&#20803;&#23398;&#20064;&#30340;&#26032;&#22411;&#21452;&#32447;&#24615;&#20844;&#24335;&#12290;</title><link>http://arxiv.org/abs/2301.11241</link><description>&lt;p&gt;
&#20851;&#20110;&#26102;&#21464;&#21338;&#24328;&#20013;&#26080;&#24724;&#23398;&#20064;&#21160;&#24577;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of No-Regret Learning Dynamics in Time-Varying Games. (arXiv:2301.11241v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#21464;&#21338;&#24328;&#20013;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;OGD&#65289;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#26126;&#30830;&#30340;&#25910;&#25947;&#30028;&#38480;&#65292;&#24182;&#24314;&#31435;&#20102;&#36866;&#29992;&#20110;&#26102;&#21464;&#24635;&#21644;&#22810;&#20154;&#21338;&#24328;&#21644;&#20803;&#23398;&#20064;&#30340;&#26032;&#22411;&#21452;&#32447;&#24615;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20851;&#20110;&#21338;&#24328;&#23398;&#20064;&#30340;&#25991;&#29486;&#37117;&#38598;&#20013;&#20110;&#24213;&#23618;&#37325;&#22797;&#21338;&#24328;&#19981;&#21457;&#29983;&#21464;&#21270;&#30340;&#20005;&#26684;&#27169;&#24335;&#19979;&#12290;&#23545;&#20110;&#21160;&#24577;&#22810;&#26234;&#20307;&#28216;&#25103;&#20013;&#26080;&#24724;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26102;&#21464;&#21338;&#24328;&#20013;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;OGD&#65289;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38024;&#23545;&#33258;&#28982;&#21464;&#21270;&#24230;&#37327;&#30340;&#21338;&#24328;&#24207;&#21015;&#30340;&#22343;&#34913;&#38388;&#38553;&#65292;&#20026;OGD&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#25910;&#25947;&#30028;&#38480;&#65292;&#20174;&#32780;&#28085;&#30422;&#20102;&#38745;&#24577;&#21338;&#24328;&#30340;&#24050;&#30693;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#21482;&#35201;&#27599;&#22330;&#28216;&#25103;&#37117;&#36827;&#34892;&#20102;&#22810;&#27425;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#24378;&#20984;&#24615;-&#24378;&#20985;&#24615;&#24314;&#31435;&#20102;&#25913;&#36827;&#30340;&#20108;&#38454;&#21464;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#36866;&#29992;&#20110;&#26102;&#21464;&#30340;&#24635;&#21644;&#22810;&#20154;&#21338;&#24328;&#65292;&#36890;&#36807;&#30456;&#20851;&#22343;&#34913;&#30340;&#21452;&#32447;&#24615;&#20844;&#24335;&#65292;&#36825;&#23545;&#20803;&#23398;&#20064;&#20197;&#21450;&#33719;&#24471;&#38024;&#23545;&#21464;&#21270;&#20381;&#36182;&#24615;&#21518;&#24724;&#30028;&#38480;&#30340;&#31934;&#32454;&#38656;&#27714;&#20855;&#26377;&#26032;&#39062;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the literature on learning in games has focused on the restrictive setting where the underlying repeated game does not change over time. Much less is known about the convergence of no-regret learning algorithms in dynamic multiagent settings. In this paper, we characterize the convergence of optimistic gradient descent (OGD) in time-varying games. Our framework yields sharp convergence bounds for the equilibrium gap of OGD in zero-sum games parameterized on natural variation measures of the sequence of games, subsuming known results for static games. Furthermore, we establish improved second-order variation bounds under strong convexity-concavity, as long as each game is repeated multiple times. Our results also apply to time-varying general-sum multi-player games via a bilinear formulation of correlated equilibria, which has novel implications for meta-learning and for obtaining refined variation-dependent regret bounds, addressing questions left open in prior papers. Finally,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#35752;&#35770;&#20102;&#39640;&#24615;&#33021;&#35745;&#31639;&#31038;&#21306;&#20013;&#27969;&#20256;&#30340;&#31070;&#35805;&#21644;&#20256;&#35828;&#65292;&#36825;&#20123;&#31070;&#35805;&#24448;&#24448;&#19981;&#22522;&#20110;&#31185;&#23398;&#20107;&#23454;&#65292;&#32780;&#26159;&#22522;&#20110;&#19968;&#20123;&#35777;&#25454;&#25110;&#35770;&#35777;&#12290;&#34429;&#28982;&#26377;&#20123;&#38382;&#39064;&#20173;&#28982;&#26159;&#26080;&#20241;&#27490;&#30340;&#21746;&#23398;&#36777;&#35770;&#65292;&#20294;&#26032;&#30340;&#26041;&#21521;&#27491;&#22312;&#20986;&#29616;&#65292;&#22914;&#31639;&#27861;&#30340;&#35268;&#27169;&#21270;&#25110;&#26032;&#30340;&#26550;&#26500;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2301.02432</link><description>&lt;p&gt;
&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#30340;&#31070;&#35805;&#19982;&#20256;&#35828;
&lt;/p&gt;
&lt;p&gt;
Myths and Legends in High-Performance Computing. (arXiv:2301.02432v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02432
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#35752;&#35770;&#20102;&#39640;&#24615;&#33021;&#35745;&#31639;&#31038;&#21306;&#20013;&#27969;&#20256;&#30340;&#31070;&#35805;&#21644;&#20256;&#35828;&#65292;&#36825;&#20123;&#31070;&#35805;&#24448;&#24448;&#19981;&#22522;&#20110;&#31185;&#23398;&#20107;&#23454;&#65292;&#32780;&#26159;&#22522;&#20110;&#19968;&#20123;&#35777;&#25454;&#25110;&#35770;&#35777;&#12290;&#34429;&#28982;&#26377;&#20123;&#38382;&#39064;&#20173;&#28982;&#26159;&#26080;&#20241;&#27490;&#30340;&#21746;&#23398;&#36777;&#35770;&#65292;&#20294;&#26032;&#30340;&#26041;&#21521;&#27491;&#22312;&#20986;&#29616;&#65292;&#22914;&#31639;&#27861;&#30340;&#35268;&#27169;&#21270;&#25110;&#26032;&#30340;&#26550;&#26500;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#21457;&#20154;&#28145;&#30465;&#30340;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#39640;&#24615;&#33021;&#35745;&#31639;&#31038;&#21306;&#25104;&#21592;&#38388;&#27969;&#20256;&#30340;&#19968;&#20123;&#31070;&#35805;&#21644;&#20256;&#35828;&#12290;&#25105;&#20204;&#20174;&#20250;&#35758;&#21644;&#20250;&#35758;&#19978;&#30340;&#23545;&#35805;&#12289;&#20135;&#21697;&#24191;&#21578;&#12289;&#35770;&#25991;&#20197;&#21450;&#31038;&#21306;&#20869;&#22806;&#30340;&#25512;&#29305;&#12289;&#21338;&#23458;&#21644;&#26032;&#38395;&#25991;&#31456;&#20013;&#25910;&#38598;&#20102;&#36825;&#20123;&#31070;&#35805;&#12290;&#25105;&#20204;&#35748;&#20026;&#23427;&#20204;&#20195;&#34920;&#20102;&#24403;&#21069;&#26102;&#20195;&#30340;&#26102;&#20195;&#31934;&#31070;&#65292;&#36825;&#20010;&#26102;&#20195;&#27491;&#22312;&#32463;&#21382;&#35768;&#22810;&#35268;&#27169;&#23450;&#24459;&#30340;&#32456;&#32467;&#65292;&#22914;Dennard&#23450;&#24459;&#21644;&#25705;&#23572;&#23450;&#24459;&#12290;&#34429;&#28982;&#19968;&#20123;&#23450;&#24459;&#32467;&#26463;&#20102;&#65292;&#20294;&#26032;&#30340;&#26041;&#21521;&#27491;&#22312;&#20986;&#29616;&#65292;&#27604;&#22914;&#31639;&#27861;&#30340;&#35268;&#27169;&#21270;&#25110;&#26032;&#30340;&#26550;&#26500;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31070;&#35805;&#24456;&#23569;&#22522;&#20110;&#31185;&#23398;&#20107;&#23454;&#65292;&#32780;&#26159;&#22522;&#20110;&#19968;&#20123;&#35777;&#25454;&#25110;&#35770;&#35777;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#27491;&#26159;&#35768;&#22810;&#31070;&#35805;&#23384;&#22312;&#30340;&#21407;&#22240;&#65292;&#20063;&#26159;&#20026;&#20160;&#20040;&#23427;&#20204;&#26080;&#27861;&#24471;&#21040;&#26126;&#30830;&#31572;&#26696;&#30340;&#21407;&#22240;&#12290;&#34429;&#28982;&#27599;&#20010;&#31070;&#35805;&#37117;&#24212;&#35813;&#26377;&#26126;&#30830;&#30340;&#31572;&#26696;&#65292;&#20294;&#26377;&#20123;&#38382;&#39064;&#21487;&#33021;&#20173;&#28982;&#26159;&#26080;&#20241;&#27490;&#30340;&#21746;&#23398;&#36777;&#35770;&#65292;&#27604;&#22914;&#36125;&#22810;&#33452;&#27604;&#35841;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this thought-provoking article, we discuss certain myths and legends that are folklore among members of the high-performance computing community. We gathered these myths from conversations at conferences and meetings, product advertisements, papers, and other communications such as tweets, blogs, and news articles within and beyond our community. We believe they represent the zeitgeist of the current era of massive change, driven by the end of many scaling laws such as Dennard scaling and Moore's law. While some laws end, new directions are emerging, such as algorithmic scaling or novel architecture research. Nevertheless, these myths are rarely based on scientific facts, but rather on some evidence or argumentation. In fact, we believe that this is the very reason for the existence of many myths and why they cannot be answered clearly. While it feels like there should be clear answers for each, some may remain endless philosophical debates, such as whether Beethoven was better than
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#30340;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21015;&#34920;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#23454;&#29616;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2212.10764</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#25490;&#21517;&#30340;&#21015;&#34920;&#32423;&#21035;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#30340;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21015;&#34920;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#23454;&#29616;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36866;&#24212;&#26088;&#22312;&#23558;&#22312;&#65288;&#25968;&#25454;&#20016;&#23500;&#65289;&#28304;&#39046;&#22495;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#65288;&#36164;&#28304;&#26377;&#38480;&#65289;&#30446;&#26631;&#39046;&#22495;&#65292;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#65292;&#23427;&#21305;&#37197;&#24182;&#23545;&#40784;&#29305;&#24449;&#31354;&#38388;&#19978;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#20294;&#22312;&#25490;&#21517;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#21364;&#26159;&#38646;&#25955;&#30340;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#20960;&#31181;&#23454;&#29616;&#32570;&#20047;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#25490;&#21517;&#30340;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#23457;&#26597;&#20043;&#21069;&#30340;&#24037;&#20316;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#23454;&#26045;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;&#39033;&#30446;&#32423;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32858;&#21512;&#30340;&#25152;&#26377;&#21015;&#34920;&#20013;&#23545;&#36827;&#34892;&#25490;&#21517;&#30340;&#39033;&#30446;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#65292;&#20294;&#24573;&#30053;&#20102;&#21015;&#34920;&#30340;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#21015;&#34920;&#30340;&#32467;&#26500;&#24212;&#35813;&#34987;&#21033;&#29992;&#65292;&#22240;&#20026;&#23427;&#26159;&#25490;&#21517;&#38382;&#39064;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#24230;&#37327;&#26159;&#22312;&#21015;&#34920;&#19978;&#23450;&#20041;&#21644;&#35745;&#31639;&#30340;&#65292;&#32780;&#19981;&#26159;&#22312;&#39033;&#30446;&#26412;&#36523;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#19981;&#19968;&#33268;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call item-level alignment, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose list-level alignment -learning
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRACE&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#32456;&#36523;&#27169;&#22411;&#32534;&#36753;&#65292;&#23427;&#36890;&#36807;&#22312;&#27969;&#24335;&#38169;&#35823;&#19978;&#25191;&#34892;&#30446;&#26631;&#32534;&#36753;&#26469;&#20462;&#22797;&#37096;&#32626;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#29983;&#25104;&#19968;&#20010;&#31163;&#25955;&#12289;&#26412;&#22320;&#30340;&#32534;&#36753;&#32534;&#30721;&#26412;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#65292;&#22312;&#36827;&#34892;&#25968;&#21315;&#20010;&#39034;&#24207;&#32534;&#36753;&#26102;&#34920;&#29616;&#20026;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11031</link><description>&lt;p&gt;
GRACE&#65306;&#31163;&#25955;&#38190;&#20540;&#36866;&#37197;&#22120;&#23454;&#29616;&#30340;&#32456;&#36523;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adapters. (arXiv:2211.11031v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRACE&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#32456;&#36523;&#27169;&#22411;&#32534;&#36753;&#65292;&#23427;&#36890;&#36807;&#22312;&#27969;&#24335;&#38169;&#35823;&#19978;&#25191;&#34892;&#30446;&#26631;&#32534;&#36753;&#26469;&#20462;&#22797;&#37096;&#32626;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#29983;&#25104;&#19968;&#20010;&#31163;&#25955;&#12289;&#26412;&#22320;&#30340;&#32534;&#36753;&#32534;&#30721;&#26412;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#65292;&#22312;&#36827;&#34892;&#25968;&#21315;&#20010;&#39034;&#24207;&#32534;&#36753;&#26102;&#34920;&#29616;&#20026;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#30340;&#27169;&#22411;&#38543;&#26102;&#38388;&#25512;&#31227;&#20250;&#34928;&#36864;&#65292;&#21407;&#22240;&#26159;&#36755;&#20837;&#30340;&#21464;&#21270;&#12289;&#29992;&#25143;&#38656;&#27714;&#19981;&#26029;&#25913;&#21464;&#12289;&#25110;&#30001;&#20110;&#20986;&#29616;&#30693;&#35782;&#31354;&#32570;&#12290;&#24403;&#21457;&#29616;&#26377;&#23475;&#34892;&#20026;&#26102;&#65292;&#38656;&#35201;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#22120;&#22312;&#22810;&#27425;&#32534;&#36753;&#20013;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GRACE&#65292;&#19968;&#31181;&#32456;&#36523;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#23427;&#22312;&#37096;&#32626;&#27169;&#22411;&#30340;&#27969;&#24335;&#38169;&#35823;&#19978;&#23454;&#29616;&#20102;&#38382;&#39064;&#20462;&#34917;&#65292;&#30830;&#20445;&#23545;&#19981;&#30456;&#20851;&#30340;&#36755;&#20837;&#30340;&#24433;&#21709;&#26368;&#23567;&#21270;&#12290;GRACE&#23558;&#26032;&#30340;&#26144;&#23556;&#39033;&#20889;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#31163;&#25955;&#30340;&#12289;&#26412;&#22320;&#30340;&#32534;&#30721;&#26412;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#21482;&#20351;&#29992;&#27969;&#24335;&#38169;&#35823;&#23454;&#29616;&#25968;&#21315;&#20010;&#39034;&#24207;&#32534;&#36753;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;T5&#12289;BERT&#21644;GPT&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;GRACE&#22312;&#36827;&#34892;&#24182;&#20445;&#30041;&#32534;&#36753;&#26041;&#38754;&#30340;&#24615;&#33021;&#22788;&#20110;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#21516;&#26102;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://www.github.com/thartvigsen/grace}{github.com/thartvigsen/grace&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployed models decay over time due to shifting inputs, changing user needs, or emergent knowledge gaps. When harmful behaviors are identified, targeted edits are required. However, current model editors, which adjust specific behaviors of pre-trained models, degrade model performance over multiple edits. We propose GRACE, a Lifelong Model Editing method, which implements spot-fixes on streaming errors of a deployed model, ensuring minimal impact on unrelated inputs. GRACE writes new mappings into a pre-trained model's latent space, creating a discrete, local codebook of edits without altering model weights. This is the first method enabling thousands of sequential edits using only streaming errors. Our experiments on T5, BERT, and GPT models show GRACE's state-of-the-art performance in making and retaining edits, while generalizing to unseen inputs. Our code is available at https://www.github.com/thartvigsen/grace}{github.com/thartvigsen/grace}.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FairMILE&#30340;&#22810;&#23618;&#33539;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2211.09925</link><description>&lt;p&gt;
FairMILE: &#23454;&#29616;&#20844;&#24179;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#25928;&#29575;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FairMILE: Towards an Efficient Framework for Fair Graph Representation Learning. (arXiv:2211.09925v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09925
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FairMILE&#30340;&#22810;&#23618;&#33539;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#23398;&#20064;&#21040;&#20559;&#35265;&#30340;&#34920;&#31034;&#65292;&#23548;&#33268;&#27495;&#35270;&#24615;&#30340;&#32467;&#26524;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20943;&#36731;&#22270;&#34920;&#31034;&#20013;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#24494;&#35843;&#26102;&#38656;&#35201;&#24322;&#24120;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#25928;&#30340;&#20844;&#24179;&#22270;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;FairMILE&#12290;FairMILE&#26159;&#19968;&#20010;&#22810;&#23618;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#20844;&#24179;&#24615;&#21644;&#20445;&#25345;&#23454;&#29992;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#23398;&#20064;&#22270;&#34920;&#31034;&#12290;&#23427;&#21487;&#20197;&#19982;&#20219;&#20309;&#26080;&#30417;&#30563;&#23884;&#20837;&#26041;&#27861;&#37197;&#21512;&#20351;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#20844;&#24179;&#24615;&#32422;&#26463;&#12290;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;FairMILE&#22312;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#20844;&#24179;&#24615;&#21644;&#25928;&#26524;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#20248;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning models have demonstrated great capability in many real-world applications. Nevertheless, prior research indicates that these models can learn biased representations leading to discriminatory outcomes. A few works have been proposed to mitigate the bias in graph representations. However, most existing works require exceptional time and computing resources for training and fine-tuning. To this end, we study the problem of efficient fair graph representation learning and propose a novel framework FairMILE. FairMILE is a multi-level paradigm that can efficiently learn graph representations while enforcing fairness and preserving utility. It can work in conjunction with any unsupervised embedding approach and accommodate various fairness constraints. Extensive experiments across different downstream tasks demonstrate that FairMILE significantly outperforms state-of-the-art baselines in terms of running time while achieving a superior trade-off between fairness 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; FedFA &#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#24449;&#38170;&#23450;&#26469;&#23545;&#40784;&#29305;&#24449;&#26144;&#23556;&#24182;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#22312;&#24322;&#26500;&#25968;&#25454;&#26102;&#20998;&#31867;&#22120;&#21644;&#29305;&#24449;&#26144;&#23556;&#20043;&#38388;&#30340;&#24694;&#24615;&#24490;&#29615;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.09299</link><description>&lt;p&gt;
FedFA: &#38024;&#23545;&#24322;&#26500;&#25968;&#25454;&#30340;&#29305;&#24449;&#38170;&#23450;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedFA: Federated Learning with Feature Anchors to Align Features and Classifiers for Heterogeneous Data. (arXiv:2211.09299v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; FedFA &#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#24449;&#38170;&#23450;&#26469;&#23545;&#40784;&#29305;&#24449;&#26144;&#23556;&#24182;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#22312;&#24322;&#26500;&#25968;&#25454;&#26102;&#20998;&#31867;&#22120;&#21644;&#29305;&#24449;&#26144;&#23556;&#20043;&#38388;&#30340;&#24694;&#24615;&#24490;&#29615;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#20132;&#25442;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#23458;&#25143;&#31471;&#23384;&#22312;&#24322;&#26500;&#25968;&#25454;&#26102;&#65292;&#23427;&#20250;&#36973;&#21463;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#24120;&#35265;&#30340;&#26412;&#22320;&#35757;&#32451;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#35774;&#35745;&#29305;&#23450;&#30340;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#26469;&#35268;&#33539;&#26435;&#37325;&#24046;&#24322;&#25110;&#29305;&#24449;&#19981;&#19968;&#33268;&#24615;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#20998;&#31867;&#22120;&#21644;&#29305;&#24449;&#26144;&#23556;&#19981;&#19968;&#33268;&#20043;&#38388;&#30340;&#24694;&#24615;&#24490;&#29615;&#65292;&#23548;&#33268;&#23458;&#25143;&#31471;&#27169;&#22411;&#22312;&#29305;&#24449;&#31354;&#38388;&#21644;&#20998;&#31867;&#22120;&#24046;&#24322;&#30340;&#19981;&#19968;&#33268;&#29305;&#24449;&#31354;&#38388;&#20013;&#26356;&#26032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026; FedFA &#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#22312;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#29305;&#24449;&#38170;&#23450;&#26469;&#23545;&#40784;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#29305;&#24449;&#26144;&#23556;&#24182;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#20351;&#23458;&#25143;&#31471;&#27169;&#22411;&#22312;&#20849;&#20139;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;&#19968;&#33268;&#30340;&#20998;&#31867;&#22120;&#19979;&#26356;&#26032;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#24322;&#26500;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#20462;&#25913;&#21518;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning allows multiple clients to collaboratively train a model without exchanging their data, thus preserving data privacy. Unfortunately, it suffers significant performance degradation under heterogeneous data at clients. Common solutions in local training involve designing a specific auxiliary loss to regularize weight divergence or feature inconsistency. However, we discover that these approaches fall short of the expected performance because they ignore the existence of a vicious cycle between classifier divergence and feature mapping inconsistency across clients, such that client models are updated in inconsistent feature space with diverged classifiers. We then propose a simple yet effective framework named Federated learning with Feature Anchors (FedFA) to align the feature mappings and calibrate classifier across clients during local training, which allows client models updating in a shared feature space with consistent classifiers. We demonstrate that this modific
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26500;&#24314;&#36317;&#31163;&#25935;&#24863;&#39044;&#27979;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#26367;&#20195;&#36335;&#24452;&#30340;&#32452;&#21512;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2211.02681</link><description>&lt;p&gt;
&#28145;&#24230;&#36317;&#31163;&#25935;&#24863;&#39044;&#27979;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Distance Sensitivity Oracles. (arXiv:2211.02681v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02681
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26500;&#24314;&#36317;&#31163;&#25935;&#24863;&#39044;&#27979;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#26367;&#20195;&#36335;&#24452;&#30340;&#32452;&#21512;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22522;&#26412;&#30340;&#22270;&#38382;&#39064;&#20043;&#19968;&#26159;&#23547;&#25214;&#20174;&#28304;&#33410;&#28857;&#21040;&#30446;&#26631;&#33410;&#28857;&#30340;&#26368;&#30701;&#36335;&#24452;&#12290;&#34429;&#28982;&#38382;&#39064;&#30340;&#22522;&#26412;&#24418;&#24335;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#24182;&#19988;&#24050;&#30693;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20294;&#26159;&#24403;&#22270;&#30340;&#26576;&#20123;&#37096;&#20998;&#23481;&#26131;&#22833;&#36133;&#26102;&#65292;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#23613;&#31649;&#21487;&#20197;&#22312;&#27599;&#27425;&#25925;&#38556;&#21518;&#37325;&#26032;&#35745;&#31639;&#26367;&#20195;&#36335;&#24452;&#30340;&#26368;&#30701;&#36335;&#24452;&#65292;&#20294;&#36825;&#22312;&#26102;&#38388;&#21644;/&#25110;&#23384;&#20648;&#19978;&#38750;&#24120;&#20302;&#25928;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#35745;&#31639;&#36127;&#25285;&#20174;&#26597;&#35810;&#36716;&#31227;&#21040;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#65292;&#20854;&#20013;&#35745;&#31639;&#20986;&#19968;&#20010;&#25968;&#25454;&#32467;&#26500;&#65292;&#20801;&#35768;&#24555;&#36895;&#26597;&#35810;&#26367;&#20195;&#36335;&#24452;&#65292;&#36890;&#24120;&#31216;&#20026;&#36317;&#31163;&#25935;&#24863;&#39044;&#27979;&#31639;&#27861;&#65288;DSO&#65289;&#12290;&#23613;&#31649;&#22312;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30028;&#23545;DSO&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26500;&#24314;DSO&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#21033;&#29992;&#26367;&#20195;&#36335;&#24452;&#30340;&#32452;&#21512;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most fundamental graph problems is finding a shortest path from a source to a target node. While in its basic forms the problem has been studied extensively and efficient algorithms are known, it becomes significantly harder as soon as parts of the graph are susceptible to failure. Although one can recompute a shortest replacement path after every outage, this is rather inefficient both in time and/or storage. One way to overcome this problem is to shift computational burden from the queries into a pre-processing step, where a data structure is computed that allows for fast querying of replacement paths, typically referred to as a Distance Sensitivity Oracle (DSO). While DSOs have been extensively studied in the theoretical computer science community, to the best of our knowledge this is the first work to construct DSOs using deep learning techniques. We show how to use deep learning to utilize a combinatorial structure of replacement paths. More specifically, we utilize the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#36125;&#21494;&#26031;&#30340;&#35282;&#24230;&#20272;&#35745;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#27745;&#26579;&#22240;&#23376;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#36755;&#20986;&#20316;&#20026;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#30340;&#28151;&#21512;&#24418;&#24335;&#36827;&#34892;&#20272;&#35745;&#12290;&#22312;22&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.10487</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#27745;&#26579;&#22240;&#23376;&#20998;&#24067;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimating the Contamination Factor's Distribution in Unsupervised Anomaly Detection. (arXiv:2210.10487v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10487
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#36125;&#21494;&#26031;&#30340;&#35282;&#24230;&#20272;&#35745;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#27745;&#26579;&#22240;&#23376;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#36755;&#20986;&#20316;&#20026;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#30340;&#28151;&#21512;&#24418;&#24335;&#36827;&#34892;&#20272;&#35745;&#12290;&#22312;22&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#36890;&#36807;&#20026;&#31034;&#20363;&#20998;&#37197;&#22522;&#20110;&#21508;&#31181;&#21551;&#21457;&#24335;&#35268;&#21017;&#30340;&#23454;&#20540;&#24322;&#24120;&#20998;&#25968;&#26469;&#35782;&#21035;&#19981;&#31526;&#21512;&#39044;&#26399;&#34892;&#20026;&#30340;&#31034;&#20363;&#12290;&#36825;&#20123;&#20998;&#25968;&#38656;&#35201;&#36890;&#36807;&#38408;&#20540;&#21270;&#36716;&#25442;&#20026;&#23454;&#38469;&#39044;&#27979;&#65292;&#20174;&#32780;&#20351;&#34987;&#26631;&#35760;&#20026;&#24322;&#24120;&#30340;&#31034;&#20363;&#27604;&#20363;&#31561;&#20110;&#39044;&#26399;&#30340;&#24322;&#24120;&#27604;&#20363;&#65292;&#31216;&#20026;&#27745;&#26579;&#22240;&#23376;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#27809;&#26377;&#22909;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#27745;&#26579;&#22240;&#23376;&#26412;&#36523;&#12290;&#25105;&#20204;&#20174;&#36125;&#21494;&#26031;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20272;&#35745;&#32473;&#23450;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#27745;&#26579;&#22240;&#23376;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#20010;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#36755;&#20986;&#20316;&#20026;&#24050;&#32463;&#25429;&#25417;&#21040;&#24322;&#24120;&#24615;&#30340;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;&#28151;&#21512;&#24418;&#24335;&#26469;&#20272;&#35745;&#27745;&#26579;&#12290;&#22312;22&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#20272;&#35745;&#30340;&#20998;&#24067;&#26159;&#33391;&#22909;&#26657;&#20934;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#35774;&#32622;&#38408;&#20540;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection methods identify examples that do not follow the expected behaviour, typically in an unsupervised fashion, by assigning real-valued anomaly scores to the examples based on various heuristics. These scores need to be transformed into actual predictions by thresholding, so that the proportion of examples marked as anomalies equals the expected proportion of anomalies, called contamination factor. Unfortunately, there are no good methods for estimating the contamination factor itself. We address this need from a Bayesian perspective, introducing a method for estimating the posterior distribution of the contamination factor of a given unlabeled dataset. We leverage on outputs of several anomaly detectors as a representation that already captures the basic notion of anomalousness and estimate the contamination using a specific mixture formulation. Empirically on 22 datasets, we show that the estimated distribution is well-calibrated and that setting the threshold using the
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20998;&#23376;&#22270;&#23884;&#20837;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;MOLGRAPHEVAL&#25581;&#31034;&#20102;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;GSSL&#26041;&#27861;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2206.08005</link><description>&lt;p&gt;
&#35780;&#20272;&#20998;&#23376;&#22270;&#23884;&#20837;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evaluating Self-Supervised Learning for Molecular Graph Embeddings. (arXiv:2206.08005v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08005
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20998;&#23376;&#22270;&#23884;&#20837;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;MOLGRAPHEVAL&#25581;&#31034;&#20102;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;GSSL&#26041;&#27861;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;GSSL&#65289;&#20026;&#33719;&#21462;&#23884;&#20837;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#36884;&#24452;&#65292;&#26080;&#38656;&#19987;&#23478;&#26631;&#27880;&#65292;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20998;&#23376;&#22270;&#20855;&#26377;&#28145;&#21051;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#28508;&#22312;&#20998;&#23376;&#30340;&#25968;&#37327;&#24778;&#20154;&#65292;&#24182;&#19988;&#33719;&#21462;&#26631;&#31614;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;&#28982;&#32780;&#65292;GSSL&#26041;&#27861;&#19981;&#26159;&#20026;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#26159;&#20026;&#20102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#21487;&#36716;&#31227;&#24615;&#12290;&#36825;&#31181;&#24191;&#27867;&#36866;&#29992;&#24615;&#20351;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#22797;&#26434;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20998;&#23376;&#22270;&#34920;&#31034;&#35780;&#20272;&#8221;&#65288;MOLGRAPHEVAL&#65289;&#65292;&#29983;&#25104;&#20855;&#26377;&#21487;&#35299;&#37322;&#21644;&#22810;&#26679;&#21270;&#23646;&#24615;&#30340;&#20998;&#23376;&#22270;&#23884;&#20837;&#30340;&#35814;&#32454;&#21078;&#26512;&#12290;MOLGRAPHEVAL&#25552;&#20379;&#20102;&#19968;&#32452;&#25506;&#27979;&#20219;&#21153;&#65292;&#20998;&#20026;&#19977;&#31867;&#65306;&#65288;i&#65289;&#36890;&#29992;&#22270;&#24418;&#65292;&#65288;ii&#65289;&#20998;&#23376;&#20122;&#32467;&#26500;&#21644;&#65288;iii&#65289;&#23884;&#20837;&#31354;&#38388;&#23646;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;MOLGRAPHEVAL&#26469;&#22522;&#20934;&#21270;&#29616;&#26377;&#30340;GSSL&#26041;&#27861;&#65292;&#23545;&#27604;&#24403;&#21069;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#20197;&#21450;&#25105;&#20204;&#30340;&#20219;&#21153;&#22871;&#20214;&#65292;&#25105;&#20204;&#21457;&#29616;GSSL&#26041;&#27861;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Self-Supervised Learning (GSSL) provides a robust pathway for acquiring embeddings without expert labelling, a capability that carries profound implications for molecular graphs due to the staggering number of potential molecules and the high cost of obtaining labels. However, GSSL methods are designed not for optimisation within a specific domain but rather for transferability across a variety of downstream tasks. This broad applicability complicates their evaluation. Addressing this challenge, we present "Molecular Graph Representation Evaluation" (MOLGRAPHEVAL), generating detailed profiles of molecular graph embeddings with interpretable and diversified attributes. MOLGRAPHEVAL offers a suite of probing tasks grouped into three categories: (i) generic graph, (ii) molecular substructure, and (iii) embedding space properties. By leveraging MOLGRAPHEVAL to benchmark existing GSSL methods against both current downstream datasets and our suite of tasks, we uncover significant inco
&lt;/p&gt;</description></item><item><title>COVID-Net Biochem&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#21033;&#29992;&#20020;&#24202;&#21644;&#29983;&#21270;&#25968;&#25454;&#26469;&#39044;&#27979;COVID-19&#24739;&#32773;&#30340;&#29983;&#23384;&#21644;&#32958;&#25439;&#20260;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2204.11210</link><description>&lt;p&gt;
COVID-Net&#29983;&#21270;&#65306;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;&#26694;&#26550;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;COVID-19&#24739;&#32773;&#30340;&#29983;&#23384;&#21644;&#32958;&#25439;&#20260;&#29366;&#20917;&#30340;&#20020;&#24202;&#21644;&#29983;&#21270;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
COVID-Net Biochem: An Explainability-driven Framework to Building Machine Learning Models for Predicting Survival and Kidney Injury of COVID-19 Patients from Clinical and Biochemistry Data. (arXiv:2204.11210v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11210
&lt;/p&gt;
&lt;p&gt;
COVID-Net Biochem&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#21033;&#29992;&#20020;&#24202;&#21644;&#29983;&#21270;&#25968;&#25454;&#26469;&#39044;&#27979;COVID-19&#24739;&#32773;&#30340;&#29983;&#23384;&#21644;&#32958;&#25439;&#20260;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#22312;2020&#24180;&#23459;&#24067;COVID-19&#20026;&#20840;&#29699;&#22823;&#27969;&#34892;&#30149;&#20197;&#26469;&#65292;&#20840;&#29699;&#31038;&#20250;&#19968;&#30452;&#38754;&#20020;&#30528;&#25511;&#21046;&#21644;&#32531;&#35299;SARS-CoV-2&#30149;&#27602;&#21450;&#20854;&#19981;&#26029;&#36827;&#21270;&#30340;&#20122;&#22411;&#21644;&#37325;&#32452;&#29289;&#20256;&#25773;&#30340;&#25345;&#32493;&#25361;&#25112;&#12290;&#22312;&#22823;&#27969;&#34892;&#26399;&#38388;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#19981;&#20165;&#26159;&#20934;&#30830;&#26816;&#27979;&#38451;&#24615;&#30149;&#20363;&#65292;&#36824;&#21253;&#25324;&#39640;&#25928;&#39044;&#27979;&#24182;&#21457;&#30151;&#39118;&#38505;&#21644;&#24739;&#32773;&#29983;&#23384;&#27010;&#29575;&#12290;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#30456;&#24403;&#37327;&#30340;&#20020;&#24202;&#36164;&#28304;&#20998;&#37197;&#21644;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;COVID-Net&#29983;&#21270;&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20020;&#24202;&#21644;&#29983;&#21270;&#25968;&#25454;&#20197;&#36879;&#26126;&#12289;&#31995;&#32479;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#39044;&#27979;COVID-19&#24739;&#32773;&#30340;&#29983;&#23384;&#21644;&#20303;&#38498;&#26399;&#38388;&#21457;&#23637;&#24613;&#24615;&#32958;&#25439;&#20260;&#30340;&#21487;&#33021;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#19982;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26080;&#32541;&#38598;&#25104;&#65292;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the World Health Organization declared COVID-19 a pandemic in 2020, the global community has faced ongoing challenges in controlling and mitigating the transmission of the SARS-CoV-2 virus, as well as its evolving subvariants and recombinants. A significant challenge during the pandemic has not only been the accurate detection of positive cases but also the efficient prediction of risks associated with complications and patient survival probabilities. These tasks entail considerable clinical resource allocation and attention.In this study, we introduce COVID-Net Biochem, a versatile and explainable framework for constructing machine learning models. We apply this framework to predict COVID-19 patient survival and the likelihood of developing Acute Kidney Injury during hospitalization, utilizing clinical and biochemical data in a transparent, systematic approach. The proposed approach advances machine learning model design by seamlessly integrating domain expertise with explainabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#22823;&#21270;&#20559;&#24046;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#20004;&#26679;&#26412;&#26816;&#39564;&#30340;&#20272;&#35745;&#22120;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#25554;&#20540;&#36807;&#24230;&#20272;&#35745;&#21644;&#27424;&#20272;&#35745;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;$Q$&#23398;&#20064;&#21644;&#24341;&#23548;&#21270;&#28145;&#24230;Q&#32593;&#32476;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2201.08078</link><description>&lt;p&gt;
&#29992;&#20004;&#26679;&#26412;&#26816;&#39564;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26368;&#22823;&#21270;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Maximization Bias in Reinforcement Learning with Two-Sample Testing. (arXiv:2201.08078v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#22823;&#21270;&#20559;&#24046;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#20004;&#26679;&#26412;&#26816;&#39564;&#30340;&#20272;&#35745;&#22120;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#25554;&#20540;&#36807;&#24230;&#20272;&#35745;&#21644;&#27424;&#20272;&#35745;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;$Q$&#23398;&#20064;&#21644;&#24341;&#23548;&#21270;&#28145;&#24230;Q&#32593;&#32476;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20540;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#28216;&#25103;&#12289;&#26426;&#22120;&#20154;&#23398;&#21644;&#20854;&#20182;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;&#36807;&#24230;&#20272;&#35745;&#20559;&#24046;&#26159;&#36825;&#20123;&#31639;&#27861;&#38754;&#20020;&#30340;&#24050;&#30693;&#23041;&#32961;&#65292;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#24613;&#21095;&#19979;&#38477;&#29978;&#33267;&#23436;&#20840;&#22833;&#36133;&#12290;&#25105;&#20204;&#23558;&#20559;&#24046;&#38382;&#39064;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#36827;&#34892;&#26694;&#26550;&#21270;&#65292;&#23558;&#20854;&#35270;&#20026;&#20272;&#35745;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#30340;&#26368;&#22823;&#26399;&#26395;&#20540;&#65288;MEV&#65289;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#26679;&#26412;&#26816;&#39564;&#30340;$T$-&#20272;&#35745;&#22120;&#65288;TE&#65289;&#65292;&#36890;&#36807;&#35843;&#25972;&#24213;&#23618;&#20551;&#35774;&#26816;&#39564;&#30340;&#26174;&#33879;&#24615;&#27700;&#24179;&#65292;&#28789;&#27963;&#22320;&#25554;&#20540;&#36807;&#24230;&#20272;&#35745;&#21644;&#27424;&#20272;&#35745;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#19968;&#31181;&#21629;&#21517;&#20026;$K$-&#20272;&#35745;&#22120;&#65288;KE&#65289;&#30340;&#25512;&#24191;&#36981;&#23432;&#19982;TE&#30456;&#21516;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#30028;&#38480;&#65292;&#21516;&#26102;&#20381;&#36182;&#20110;&#20960;&#20046;&#20219;&#24847;&#30340;&#26680;&#20989;&#25968;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20351;&#29992;TE&#21644;KE&#30340;$Q$&#23398;&#20064;&#21644;&#24341;&#23548;&#21270;&#28145;&#24230;Q&#32593;&#32476;&#65288;BDQN&#65289;&#30340;&#20462;&#25913;&#65292;&#24182;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#35777;&#26126;&#20854;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21464;&#20307;&#30340;&#22522;&#20110;TE&#30340;BDQN&#12290;
&lt;/p&gt;
&lt;p&gt;
Value-based reinforcement-learning algorithms have shown strong results in games, robotics, and other real-world applications. Overestimation bias is a known threat to those algorithms and can lead to dramatic performance decreases or even complete algorithmic failure. We frame the bias problem statistically and consider it an instance of estimating the maximum expected value (MEV) of a set of random variables. We propose the $T$-Estimator (TE) based on two-sample testing for the mean, that flexibly interpolates between over- and underestimation by adjusting the significance level of the underlying hypothesis tests. A generalization, termed $K$-Estimator (KE), obeys the same bias and variance bounds as the TE while relying on a nearly arbitrary kernel function. We introduce modifications of $Q$-Learning and the Bootstrapped Deep $Q$-Network (BDQN) using the TE and the KE, and prove convergence in the tabular setting. Furthermore, we propose an adaptive variant of the TE-based BDQN that
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;CTR&#39044;&#27979;&#20013;&#21152;&#20837;&#20102;&#31232;&#30095;&#20998;&#32452;Lasso&#30340;&#27491;&#21017;&#39033;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#22312;&#30456;&#21516;&#31232;&#30095;&#27700;&#24179;&#19979;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#26497;&#39640;&#30340;&#31232;&#30095;&#24615;&#12290;</title><link>http://arxiv.org/abs/2107.14432</link><description>&lt;p&gt;
CTR&#39044;&#27979;&#20013;&#22522;&#20110;&#31232;&#30095;&#20998;&#32452;Lasso&#30340;&#31070;&#32463;&#32593;&#32476;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Adaptive Optimizers with Sparse Group Lasso for Neural Networks in CTR Prediction. (arXiv:2107.14432v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.14432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;CTR&#39044;&#27979;&#20013;&#21152;&#20837;&#20102;&#31232;&#30095;&#20998;&#32452;Lasso&#30340;&#27491;&#21017;&#39033;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#22312;&#30456;&#21516;&#31232;&#30095;&#27700;&#24179;&#19979;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#26497;&#39640;&#30340;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#31232;&#30095;&#20998;&#32452;Lasso&#30340;&#27491;&#21017;&#39033;&#21152;&#20837;&#21040;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#20013;&#65292;&#22914;Momentum&#12289;Adagrad&#12289;Adam&#12289;AMSGrad&#12289;AdaHessian&#31561;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;&#20248;&#21270;&#22120;&#65292;&#20998;&#21035;&#21629;&#21517;&#20026;Group Momentum&#12289;Group Adagrad&#12289;Group Adam&#12289;Group AMSGrad&#21644;Group AdaHessian&#31561;&#12290;&#25105;&#20204;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#22312;&#38543;&#26426;&#20984;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#29702;&#35770;&#19978;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#19977;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#24191;&#21578;&#28857;&#20987;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#26032;&#20248;&#21270;&#22120;&#30340;&#27491;&#21017;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20351;&#29992;&#24133;&#24230;&#20462;&#21098;&#26041;&#27861;&#30340;&#21407;&#22987;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;&#27169;&#22411;&#22312;&#30456;&#21516;&#31232;&#30095;&#27700;&#24179;&#19978;&#30340;&#24615;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#19982;&#27809;&#26377;&#24133;&#24230;&#20462;&#21098;&#30340;&#24773;&#20917;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26497;&#39640;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#25110;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel framework that adds the regularizers of the sparse group lasso to a family of adaptive optimizers in deep learning, such as Momentum, Adagrad, Adam, AMSGrad, AdaHessian, and create a new class of optimizers, which are named Group Momentum, Group Adagrad, Group Adam, Group AMSGrad and Group AdaHessian, etc., accordingly. We establish theoretically proven convergence guarantees in the stochastic convex settings, based on primal-dual methods. We evaluate the regularized effect of our new optimizers on three large-scale real-world ad click datasets with state-of-the-art deep learning models. The experimental results reveal that compared with the original optimizers with the post-processing procedure which uses the magnitude pruning method, the performance of the models can be significantly improved on the same sparsity level. Furthermore, in comparison to the cases without magnitude pruning, our methods can achieve extremely high sparsity with significantly better or hig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;24&#31181;&#19981;&#21516;&#37327;&#21270;&#26041;&#27861;&#22312;&#36229;&#36807;40&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#35777;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#37327;&#21270;&#26041;&#27861;&#27604;&#36739;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#20108;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;&#22522;&#20110;&#38408;&#20540;&#36873;&#25321;&#30340;Median Sweep&#21644;TSMax&#26041;&#27861;&#12289;DyS&#26694;&#26550;&#21644;&#24343;&#37324;&#24503;&#26364;&#30340;&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65307;&#32780;&#22312;&#22810;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;Generaliz&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2103.03223</link><description>&lt;p&gt;
&#37327;&#21270;&#26041;&#27861;&#30340;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comparative Evaluation of Quantification Methods. (arXiv:2103.03223v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.03223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;24&#31181;&#19981;&#21516;&#37327;&#21270;&#26041;&#27861;&#22312;&#36229;&#36807;40&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#35777;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#37327;&#21270;&#26041;&#27861;&#27604;&#36739;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#20108;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;&#22522;&#20110;&#38408;&#20540;&#36873;&#25321;&#30340;Median Sweep&#21644;TSMax&#26041;&#27861;&#12289;DyS&#26694;&#26550;&#21644;&#24343;&#37324;&#24503;&#26364;&#30340;&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65307;&#32780;&#22312;&#22810;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;Generaliz&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#25351;&#22312;&#25968;&#25454;&#38598;&#20013;&#39044;&#27979;&#31867;&#21035;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;&#23427;&#20063;&#20195;&#34920;&#30528;&#19968;&#20010;&#22312;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#26029;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#22823;&#37327;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#20221;&#20840;&#38754;&#30340;&#23454;&#35777;&#27604;&#36739;&#37327;&#21270;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#20197;&#25903;&#25345;&#31639;&#27861;&#36873;&#25321;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#36229;&#36807;40&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;24&#31181;&#19981;&#21516;&#37327;&#21270;&#26041;&#27861;&#30340;&#24443;&#24213;&#23454;&#35777;&#24615;&#24615;&#33021;&#27604;&#36739;&#65292;&#21253;&#25324;&#20108;&#20998;&#31867;&#21644;&#22810;&#20998;&#31867;&#37327;&#21270;&#35774;&#32622;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#27809;&#26377;&#21333;&#19968;&#31639;&#27861;&#33021;&#22815;&#22312;&#25152;&#26377;&#31454;&#20105;&#23545;&#25163;&#20013;&#22987;&#32456;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#32452;&#22312;&#20108;&#20998;&#31867;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#38408;&#20540;&#36873;&#25321;&#30340;Median Sweep&#21644;TSMax&#26041;&#27861;&#12289;DyS&#26694;&#26550;&#21644;&#24343;&#37324;&#24503;&#26364;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#22810;&#20998;&#31867;&#35774;&#32622;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21478;&#19968;&#32452;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#21253;&#25324;Generaliz&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantification represents the problem of predicting class distributions in a dataset. It also represents a growing research field in supervised machine learning, for which a large variety of different algorithms has been proposed in recent years. However, a comprehensive empirical comparison of quantification methods that supports algorithm selection is not available yet. In this work, we close this research gap by conducting a thorough empirical performance comparison of 24 different quantification methods on overall more than 40 data sets, considering binary as well as multiclass quantification settings. We observe that no single algorithm generally outperforms all competitors, but identify a group of methods including the threshold selection-based Median Sweep and TSMax methods, the DyS framework, and Friedman's method that performs best in the binary setting. For the multiclass setting, we observe that a different group of algorithms yields good performance, including the Generaliz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#31209;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#24352;&#37327;&#30340;CP&#20998;&#37327;&#21521;&#37327;&#30340;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#65292;&#38388;&#25509;&#26368;&#23567;&#21270;&#20102;Schatten-p&#20934;&#33539;&#65292;&#29992;&#20110;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#21644;&#24352;&#37327;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#22411;&#24352;&#37327;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#27604;&#26680;&#33539;&#25968;&#26356;&#31934;&#30830;&#30340;&#31209;&#20195;&#29702;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#27604;&#36739;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;LRTC&#30340;&#27867;&#21270;&#33021;&#21147;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2012.03436</link><description>&lt;p&gt;
&#22522;&#20110;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#35825;&#23548;&#30340;Schatten-p&#20934;&#33539;&#21017;&#27491;&#21017;&#21270;&#22312;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#21644;&#24352;&#37327;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Euclidean-Norm-Induced Schatten-p Quasi-Norm Regularization for Low-Rank Tensor Completion and Tensor Robust Principal Component Analysis. (arXiv:2012.03436v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.03436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#31209;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#24352;&#37327;&#30340;CP&#20998;&#37327;&#21521;&#37327;&#30340;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#65292;&#38388;&#25509;&#26368;&#23567;&#21270;&#20102;Schatten-p&#20934;&#33539;&#65292;&#29992;&#20110;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#21644;&#24352;&#37327;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#22411;&#24352;&#37327;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#27604;&#26680;&#33539;&#25968;&#26356;&#31934;&#30830;&#30340;&#31209;&#20195;&#29702;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#27604;&#36739;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;LRTC&#30340;&#27867;&#21270;&#33021;&#21147;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#33539;&#25968;&#21644;Schatten-p&#20934;&#33539;&#26159;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#20013;&#24120;&#29992;&#30340;&#31209;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#65292;&#35745;&#31639;&#24352;&#37327;&#30340;&#26680;&#33539;&#25968;&#25110;Schatten-p&#20934;&#33539;&#37117;&#24456;&#22256;&#38590;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;(LRTC)&#21644;&#24352;&#37327;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512;(TRPCA)&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#30340;CP&#20998;&#37327;&#21521;&#37327;&#30340;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#30340;&#26032;&#31867;&#24352;&#37327;&#31209;&#27491;&#21017;&#21270;&#22120;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#20123;&#27491;&#21017;&#21270;&#22120;&#26159;&#24352;&#37327;Schatten-p&#20934;&#33539;&#30340;&#21333;&#35843;&#21464;&#25442;&#12290;&#36825;&#31181;&#36830;&#25509;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#20998;&#37327;&#21521;&#37327;&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;LRTC&#21644;TRPCA&#20013;&#30340;Schatten-p&#20934;&#33539;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22411;&#24352;&#37327;&#65292;&#24182;&#19988;&#19982;&#26680;&#33539;&#25968;&#30456;&#27604;&#65292;&#22312;&#20302;&#31209;&#24352;&#37327;&#24674;&#22797;&#20013;&#25552;&#20379;&#20102;&#20219;&#24847;&#26356;&#23574;&#38160;&#30340;&#31209;&#20195;&#29702;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;Schatten-p&#20934;&#33539;&#27491;&#21017;&#21270;&#22120;&#21644;&#35813;&#25552;&#35758;&#27491;&#21017;&#21270;&#22120;&#30340;LRTC&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23450;&#29702;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
The nuclear norm and Schatten-$p$ quasi-norm are popular rank proxies in low-rank matrix recovery. However, computing the nuclear norm or Schatten-$p$ quasi-norm of a tensor is hard in both theory and practice, hindering their application to low-rank tensor completion (LRTC) and tensor robust principal component analysis (TRPCA). In this paper, we propose a new class of tensor rank regularizers based on the Euclidean norms of the CP component vectors of a tensor and show that these regularizers are monotonic transformations of tensor Schatten-$p$ quasi-norm. This connection enables us to minimize the Schatten-$p$ quasi-norm in LRTC and TRPCA implicitly via the component vectors. The method scales to big tensors and provides an arbitrarily sharper rank proxy for low-rank tensor recovery compared to the nuclear norm. On the other hand, we study the generalization abilities of LRTC with the Schatten-$p$ quasi-norm regularizer and LRTC with the proposed regularizers. The theorems show that
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;Fr&#233;chet&#22343;&#20540;&#21644;&#24418;&#29366;&#19981;&#21464;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21151;&#33021;&#24615;&#36718;&#24275;&#20013;&#30340;&#24418;&#29366;&#21464;&#21270;&#65292;&#24182;&#26500;&#24314;&#20102;&#21151;&#33021;&#24615;&#25968;&#25454;&#30340;&#25511;&#21046;&#22270;&#65292;&#21487;&#35299;&#37322;&#24615;&#24378;&#19988;&#33021;&#35782;&#21035;&#28508;&#22312;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2010.02968</link><description>&lt;p&gt;
&#21151;&#33021;&#24615;&#36718;&#24275;&#24314;&#27169;&#21644;&#21487;&#35299;&#37322;&#24418;&#29366;&#21464;&#21270;&#26816;&#27979;&#65306;&#32467;&#21512;Fr&#233;chet&#22343;&#20540;&#19982;&#24418;&#29366;&#19981;&#21464;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modelling of functional profiles and explainable shape shifts detection: An approach combining the notion of the Fr\'echet mean with the shape invariant model}. (arXiv:2010.02968v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.02968
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;Fr&#233;chet&#22343;&#20540;&#21644;&#24418;&#29366;&#19981;&#21464;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21151;&#33021;&#24615;&#36718;&#24275;&#20013;&#30340;&#24418;&#29366;&#21464;&#21270;&#65292;&#24182;&#26500;&#24314;&#20102;&#21151;&#33021;&#24615;&#25968;&#25454;&#30340;&#25511;&#21046;&#22270;&#65292;&#21487;&#35299;&#37322;&#24615;&#24378;&#19988;&#33021;&#35782;&#21035;&#28508;&#22312;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26816;&#27979;&#21151;&#33021;&#24615;&#36718;&#24275;&#20013;&#24418;&#29366;&#21464;&#21270;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;Fr&#233;chet&#22343;&#20540;&#27010;&#24565;&#21644;&#21464;&#24418;&#27169;&#22411;&#30340;&#27010;&#24565;&#12290;&#21033;&#29992;Fr&#233;chet&#22343;&#20540;&#25552;&#20379;&#30340;&#24191;&#20041;&#22343;&#20540;&#24863;&#30693;&#33021;&#22815;&#25429;&#25417;&#30740;&#31350;&#23545;&#35937;&#36718;&#24275;&#30340;&#20856;&#22411;&#27169;&#24335;&#65292;&#32780;&#21464;&#24418;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#29305;&#21035;&#26159;&#24418;&#29366;&#19981;&#21464;&#27169;&#22411;&#65292;&#20801;&#35768;&#23545;&#36718;&#24275;&#19982;&#20856;&#22411;&#24418;&#29366;&#20043;&#38388;&#30340;&#20559;&#24046;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#21442;&#25968;&#21270;&#12290;&#26500;&#24314;&#21644;&#25552;&#20986;&#20102;&#19982;&#25968;&#25454;&#30340;&#21151;&#33021;&#24615;&#29305;&#24615;&#21644;&#25152;&#37319;&#29992;&#30340;&#21464;&#24418;&#27169;&#22411;&#30456;&#20860;&#23481;&#30340;EWMA&#31867;&#22411;&#25511;&#21046;&#22270;&#65292;&#21033;&#29992;&#30740;&#31350;&#23545;&#35937;&#30340;&#36718;&#24275;&#22312;&#24191;&#20041;&#22343;&#20540;&#24863;&#30693;&#19979;&#30340;&#26576;&#20123;&#24418;&#29366;&#29305;&#24449;&#65292;&#23454;&#29616;&#23545;&#24418;&#29366;&#21644;/&#25110;&#21464;&#24418;&#36807;&#31243;&#28508;&#22312;&#21464;&#21270;&#30340;&#35782;&#21035;&#12290;&#36827;&#19968;&#27493;&#23558;&#24418;&#29366;&#21464;&#24418;&#36807;&#31243;&#30340;&#28508;&#22312;&#21464;&#21270;&#21306;&#20998;&#20026;&#19982;&#24133;&#24230;&#21644;/&#25110;&#30456;&#20301;&#30456;&#20851;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A modelling framework suitable for detecting shape shifts in functional profiles combining the notion of Fr\'echet mean and the concept of deformation models is developed and proposed. The generalized mean sense offerred by the Fr\'echet mean notion is employed to capture the typical pattern of the profiles under study, while the concept of deformation models, and in particular of the shape invariant model, allows for interpretable parameterizations of profile's deviations from the typical shape. EWMA-type control charts compatible with the functional nature of data and the employed deformation model are built and proposed, exploiting certain shape characteristics of the profiles under study with respect to the generalised mean sense, allowing for the identification of potential shifts concerning the shape and/or the deformation process. Potential shifts in the shape deformation process, are further distingu\-ished to significant shifts with respect to amplitude and/or the phase of the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#20934;&#26368;&#23567;&#20989;&#25968;&#65292;&#23558;&#20581;&#22766;-&#26368;&#20248;&#25351;&#25968;&#25512;&#33267;&#36127;&#26080;&#31351;&#26469;&#23454;&#29616;&#23545;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;Hessian&#30697;&#38453;&#20013;&#25193;&#23637;&#20984;&#24615;&#21306;&#22495;&#26469;&#20445;&#35777;&#26368;&#20248;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/1510.03826</link><description>&lt;p&gt;
&#22312;&#25311;&#21512;&#21644;&#23398;&#20064;&#20013;&#37319;&#29992;&#20581;&#22766;&#24615;&#21644;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adopting Robustness and Optimality in Fitting and Learning. (arXiv:1510.03826v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1510.03826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#20934;&#26368;&#23567;&#20989;&#25968;&#65292;&#23558;&#20581;&#22766;-&#26368;&#20248;&#25351;&#25968;&#25512;&#33267;&#36127;&#26080;&#31351;&#26469;&#23454;&#29616;&#23545;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;Hessian&#30697;&#38453;&#20013;&#25193;&#23637;&#20984;&#24615;&#21306;&#22495;&#26469;&#20445;&#35777;&#26368;&#20248;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23558;&#20581;&#22766;-&#26368;&#20248;&#65288;RO&#65289;&#25351;&#25968;&#955;&#25512;&#33267;&#36127;&#26080;&#31351;&#65292;&#20197;&#36890;&#36807;&#20248;&#21270;&#20934;&#26368;&#23567;&#20989;&#25968;&#23454;&#29616;&#23545;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#25512;&#24191;&#20102;&#20462;&#25913;&#30340;&#25351;&#25968;&#21270;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;RO&#25351;&#25968;&#33258;&#36866;&#24212;&#22320;&#23454;&#29616;&#21644;&#25511;&#21046;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#39044;&#23450;&#20041;&#30340;&#38408;&#20540;&#12290;&#36890;&#36807;&#22312;Hessian&#30697;&#38453;&#20013;&#25193;&#23637;&#20984;&#24615;&#21306;&#22495;&#65292;&#20197;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#20445;&#35777;&#26368;&#20248;&#24615;&#12290;&#23545;&#40065;&#26834;&#24615;&#21644;&#26368;&#20248;&#24615;&#30340;&#35814;&#32454;&#23450;&#37327;&#20998;&#26512;&#34987;&#25552;&#20379;&#12290;&#22312;&#19977;&#20010;&#26377;&#22122;&#38750;&#20984;&#20989;&#25968;&#21644;MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#25311;&#21512;&#20219;&#21153;&#20197;&#21450;&#25968;&#23383;&#35782;&#21035;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We generalized a modified exponentialized estimator by pushing the robust-optimal (RO) index $\lambda$ to $-\infty$ for achieving robustness to outliers by optimizing a quasi-Minimin function. The robustness is realized and controlled adaptively by the RO index without any predefined threshold. Optimality is guaranteed by expansion of the convexity region in the Hessian matrix to largely avoid local optima. Detailed quantitative analysis on both robustness and optimality are provided. The results of proposed experiments on fitting tasks for three noisy non-convex functions and the digits recognition task on the MNIST dataset consolidate the conclusions.
&lt;/p&gt;</description></item></channel></rss>