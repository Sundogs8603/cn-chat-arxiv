<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>ImageBind-LLM&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#32852;&#21512;&#23884;&#20837;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.03905</link><description>&lt;p&gt;
ImageBind-LLM: &#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ImageBind-LLM: Multi-modality Instruction Tuning. (arXiv:2309.03905v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03905
&lt;/p&gt;
&lt;p&gt;
ImageBind-LLM&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#32852;&#21512;&#23884;&#20837;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;ImageBind&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#35328;&#21644;&#22270;&#20687;&#25351;&#20196;&#35843;&#20248;&#26041;&#38754;&#65292;&#19982;&#27492;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;ImageBind-LLM&#21487;&#20197;&#21709;&#24212;&#22810;&#27169;&#24577;&#26465;&#20214;&#65292;&#21253;&#25324;&#38899;&#39057;&#12289;3D&#28857;&#20113;&#12289;&#35270;&#39057;&#20197;&#21450;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#31639;&#26415;&#65292;&#21482;&#38656;&#36827;&#34892;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#21487;&#23398;&#20064;&#30340;Bind&#32593;&#32476;&#26469;&#23545;&#40784;LLaMA&#21644;ImageBind&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;Bind&#32593;&#32476;&#36716;&#25442;&#30340;&#22270;&#20687;&#29305;&#24449;&#34987;&#28155;&#21152;&#21040;LLaMA&#30340;&#25152;&#26377;&#23618;&#30340;&#21333;&#35789;&#26631;&#35760;&#20013;&#65292;&#36890;&#36807;&#19968;&#20010;&#26080;&#27880;&#24847;&#21147;&#21644;&#38646;&#21021;&#22987;&#21270;&#30340;&#38376;&#25511;&#26426;&#21046;&#36880;&#27493;&#27880;&#20837;&#35270;&#35273;&#25351;&#20196;&#12290;&#22312;ImageBind&#30340;&#32852;&#21512;&#23884;&#20837;&#30340;&#24110;&#21161;&#19979;&#65292;&#31616;&#21333;&#30340;&#22270;&#20687;-&#25991;&#26412;&#35757;&#32451;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22810;&#27169;&#24577;&#36755;&#20837;&#34987;&#36865;&#20837;&#30456;&#24212;&#30340;ImageBind&#32534;&#30721;&#22120;&#65292;&#24182;&#34987;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by 
&lt;/p&gt;</description></item><item><title>DiffusionEngine&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#24341;&#25806;&#65292;&#20351;&#29992;Diffusion&#27169;&#22411;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#12290;&#23427;&#36890;&#36807;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#19968;&#20010;&#26816;&#27979;&#36866;&#37197;&#22120;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#38454;&#27573;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#21487;&#25193;&#23637;&#12289;&#22810;&#26679;&#21270;&#21644;&#21487;&#27867;&#21270;&#30340;&#26816;&#27979;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.03893</link><description>&lt;p&gt;
DiffusionEngine: &#25193;&#23637;&#25968;&#25454;&#24341;&#25806;&#30340;&#25193;&#25955;&#27169;&#22411;&#36866;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection. (arXiv:2309.03893v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03893
&lt;/p&gt;
&lt;p&gt;
DiffusionEngine&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#24341;&#25806;&#65292;&#20351;&#29992;Diffusion&#27169;&#22411;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#12290;&#23427;&#36890;&#36807;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#19968;&#20010;&#26816;&#27979;&#36866;&#37197;&#22120;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#38454;&#27573;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#21487;&#25193;&#23637;&#12289;&#22810;&#26679;&#21270;&#21644;&#21487;&#27867;&#21270;&#30340;&#26816;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#28145;&#24230;&#23398;&#20064;&#30340;&#22522;&#30707;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#26368;&#36817;&#24320;&#21457;&#30340;Diffusion&#27169;&#22411;&#26159;&#19968;&#20010;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;&#21487;&#25193;&#23637;&#25968;&#25454;&#24341;&#25806;&#12290;&#29616;&#26377;&#30340;&#32553;&#25918;&#26816;&#27979;&#23548;&#21521;&#25968;&#25454;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#25163;&#21160;&#25910;&#38598;&#25110;&#29983;&#25104;&#27169;&#22411;&#26469;&#33719;&#21462;&#30446;&#26631;&#22270;&#20687;&#65292;&#28982;&#21518;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21644;&#26631;&#27880;&#20135;&#29983;&#35757;&#32451;&#23545;&#65292;&#36825;&#20123;&#26041;&#27861;&#25104;&#26412;&#39640;&#12289;&#22797;&#26434;&#25110;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffusionEngine&#65288;DE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#25454;&#25193;&#23637;&#24341;&#25806;&#65292;&#20197;&#21333;&#19968;&#38454;&#27573;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#23548;&#21521;&#35757;&#32451;&#23545;&#12290;DE&#30001;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#19968;&#20010;&#26377;&#25928;&#30340;&#26816;&#27979;&#36866;&#37197;&#22120;&#32452;&#25104;&#65292;&#20026;&#29983;&#25104;&#21487;&#25193;&#23637;&#12289;&#22810;&#26679;&#21270;&#21644;&#21487;&#27867;&#21270;&#30340;&#26816;&#27979;&#25968;&#25454;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#25903;&#25345;&#21363;&#25554;&#21363;&#29992;&#12290;&#26816;&#27979;&#36866;&#37197;&#22120;&#36890;&#36807;&#23398;&#20064;&#23558;&#29616;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#38544;&#24335;&#35821;&#20041;&#21644;&#20301;&#32622;&#30693;&#35782;&#19982;&#26816;&#27979;&#30456;&#20851;&#30340;&#20449;&#21495;&#36827;&#34892;&#23545;&#40784;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#36793;&#30028;&#26694;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#21363;COCO-DE&#21644;...
&lt;/p&gt;
&lt;p&gt;
Data is the cornerstone of deep learning. This paper reveals that the recently developed Diffusion Model is a scalable data engine for object detection. Existing methods for scaling up detection-oriented data often require manual collection or generative models to obtain target images, followed by data augmentation and labeling to produce training pairs, which are costly, complex, or lacking diversity. To address these issues, we presentDiffusionEngine (DE), a data scaling-up engine that provides high-quality detection-oriented training pairs in a single stage. DE consists of a pre-trained diffusion model and an effective Detection-Adapter, contributing to generating scalable, diverse and generalizable detection data in a plug-and-play manner. Detection-Adapter is learned to align the implicit semantic and location knowledge in off-the-shelf diffusion models with detection-aware signals to make better bounding-box predictions. Additionally, we contribute two datasets, i.e., COCO-DE and
&lt;/p&gt;</description></item><item><title>ArtiGrasp&#26159;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#29289;&#29702;&#27169;&#25311;&#30340;&#26041;&#24335;&#65292;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#31574;&#30053;&#26469;&#21512;&#25104;&#21452;&#25163;&#28789;&#24039;&#25235;&#25569;&#21644;&#20851;&#33410;&#34920;&#36798;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03891</link><description>&lt;p&gt;
ArtiGrasp&#65306;&#21452;&#25163;&#28789;&#24039;&#25235;&#25569;&#21644;&#20851;&#33410;&#34920;&#36798;&#30340;&#29289;&#29702;&#21512;&#29702;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation. (arXiv:2309.03891v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03891
&lt;/p&gt;
&lt;p&gt;
ArtiGrasp&#26159;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#29289;&#29702;&#27169;&#25311;&#30340;&#26041;&#24335;&#65292;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#31574;&#30053;&#26469;&#21512;&#25104;&#21452;&#25163;&#28789;&#24039;&#25235;&#25569;&#21644;&#20851;&#33410;&#34920;&#36798;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ArtiGrasp&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#21512;&#25104;&#21253;&#25324;&#25235;&#25569;&#21644;&#20851;&#33410;&#34920;&#36798;&#22312;&#20869;&#30340;&#21452;&#25163;&#25163;-&#29289;&#20307;&#20132;&#20114;&#12290;&#30001;&#20110;&#20840;&#23616;&#25163;&#33109;&#36816;&#21160;&#21644;&#31934;&#30830;&#30340;&#25163;&#25351;&#25511;&#21046;&#23545;&#20110;&#29289;&#20307;&#30340;&#20851;&#33410;&#34920;&#36798;&#26159;&#24517;&#35201;&#30340;&#65292;&#36825;&#20010;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;ArtiGrasp&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#29289;&#29702;&#27169;&#25311;&#35757;&#32451;&#19968;&#20010;&#25511;&#21046;&#20840;&#23616;&#21644;&#23616;&#37096;&#25163;&#23039;&#24577;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#25163;&#23039;&#24577;&#21442;&#32771;&#19979;&#32479;&#19968;&#20102;&#25235;&#25569;&#21644;&#20851;&#33410;&#34920;&#36798;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35757;&#32451;&#20851;&#33410;&#34920;&#36798;&#25152;&#38656;&#30340;&#31934;&#30830;&#25163;&#25351;&#25511;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36880;&#28176;&#22686;&#21152;&#38590;&#24230;&#30340;&#23398;&#20064;&#35838;&#31243;&#12290;&#23427;&#20174;&#21333;&#25163;&#25805;&#20316;&#38745;&#27490;&#29289;&#20307;&#24320;&#22987;&#65292;&#28982;&#21518;&#36827;&#34892;&#21253;&#25324;&#20004;&#21482;&#25163;&#21644;&#38750;&#38745;&#27490;&#29289;&#20307;&#30340;&#22810;&#26234;&#33021;&#20307;&#35757;&#32451;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#29289;&#20307;&#25235;&#25569;&#21644;&#20851;&#33410;&#34920;&#36798;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#29289;&#20307;&#31227;&#21040;&#30446;&#26631;&#20851;&#33410;&#23039;&#24577;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#38656;&#35201;&#25235;&#25569;&#65292;&#20851;&#33410;&#34920;&#36798;&#21644;&#29289;&#20307;&#23039;&#24577;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ArtiGrasp, a novel method to synthesize bi-manual hand-object interactions that include grasping and articulation. This task is challenging due to the diversity of the global wrist motions and the precise finger control that are necessary to articulate objects. ArtiGrasp leverages reinforcement learning and physics simulations to train a policy that controls the global and local hand pose. Our framework unifies grasping and articulation within a single policy guided by a single hand pose reference. Moreover, to facilitate the training of the precise finger control required for articulation, we present a learning curriculum with increasing difficulty. It starts with single-hand manipulation of stationary objects and continues with multi-agent training including both hands and non-stationary objects. To evaluate our method, we introduce Dynamic Object Grasping and Articulation, a task that involves bringing an object into a target articulated pose. This task requires grasping,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#35813;&#22871;&#20214;&#21253;&#25324;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.03886</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#21151;&#33021;&#35299;&#37322;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Function Interpretation Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#35813;&#22871;&#20214;&#21253;&#25324;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21487;&#35835;&#30340;&#25551;&#36848;&#26631;&#35760;&#31070;&#32463;&#32593;&#32476;&#23376;&#27169;&#22359;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65306;&#36825;&#20123;&#25551;&#36848;&#21487;&#20197;&#26292;&#38706;&#22833;&#36133;&#12289;&#24341;&#23548;&#24178;&#39044;&#65292;&#29978;&#33267;&#21487;&#20197;&#35299;&#37322;&#37325;&#35201;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#26426;&#26800;&#21407;&#29702;&#30340;&#24050;&#35757;&#32451;&#32593;&#32476;&#25551;&#36848;&#37117;&#28041;&#21450;&#21040;&#23567;&#27169;&#22411;&#12289;&#29421;&#20041;&#29616;&#35937;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#12290;&#22312;&#19981;&#26029;&#22686;&#21152;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#20013;&#26631;&#35760;&#20986;&#25152;&#26377;&#20154;&#21487;&#35299;&#37322;&#30340;&#23376;&#35745;&#31639;&#20960;&#20046;&#32943;&#23450;&#38656;&#35201;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#39564;&#35777;&#25551;&#36848;&#30340;&#24037;&#20855;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#26631;&#35760;&#30340;&#25216;&#26415;&#24320;&#22987;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#26377;&#38480;&#19988;&#20020;&#26102;&#12290;&#25105;&#20204;&#24212;&#35813;&#22914;&#20309;&#39564;&#35777;&#21644;&#27604;&#36739;&#24320;&#25918;&#24335;&#26631;&#35760;&#24037;&#20855;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;FIND&#65288;&#20989;&#25968;&#35299;&#37322;&#21644;&#25551;&#36848;&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#26041;&#27861;&#26500;&#24314;&#27169;&#22359;&#30340;&#22522;&#20934;&#22871;&#20214;&#12290;FIND&#21253;&#21547;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#30340;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of tr
&lt;/p&gt;</description></item><item><title>DoLa&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#23618;&#27425;&#30340;&#36923;&#36753;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20943;&#23569;&#24187;&#35273;&#65292;&#26080;&#38656;&#22806;&#37096;&#30693;&#35782;&#25110;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2309.03883</link><description>&lt;p&gt;
DoLa&#65306;&#36890;&#36807;&#23545;&#27604;&#23618;&#27425;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. (arXiv:2309.03883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03883
&lt;/p&gt;
&lt;p&gt;
DoLa&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#23618;&#27425;&#30340;&#36923;&#36753;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#21644;&#20943;&#23569;&#24187;&#35273;&#65292;&#26080;&#38656;&#22806;&#37096;&#30693;&#35782;&#25110;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#19982;&#39044;&#35757;&#32451;&#26399;&#38388;&#35266;&#23519;&#21040;&#30340;&#20107;&#23454;&#20559;&#31163;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#20943;&#23569;&#39044;&#35757;&#32451;LLMs&#20013;&#30340;&#24187;&#35273;&#65292;&#23427;&#19981;&#38656;&#35201;&#22312;&#26816;&#32034;&#30340;&#22806;&#37096;&#30693;&#35782;&#25110;&#39069;&#22806;&#30340;&#24494;&#35843;&#19978;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23558;&#36739;&#26202;&#23618;&#21644;&#36739;&#26089;&#23618;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#24471;&#21040;&#30340;&#36923;&#36753;&#24046;&#24322;&#26469;&#33719;&#24471;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#20998;&#24067;&#65292;&#21033;&#29992;&#20102;LLMs&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#36890;&#24120;&#34987;&#35777;&#26126;&#23616;&#37096;&#21270;&#22312;&#29305;&#23450;&#30340;Transformer&#23618;&#20013;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#36890;&#36807;&#23545;&#27604;&#23618;&#27425;&#30340;&#35299;&#30721;&#65288;DoLa&#65289;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23637;&#31034;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#20943;&#23569;&#29983;&#25104;&#19981;&#27491;&#30830;&#20107;&#23454;&#30340;&#24773;&#20917;&#12290;DoLa&#22312;&#22810;&#20010;&#36873;&#25321;&#20219;&#21153;&#21644;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#21319;&#20102;&#30495;&#23454;&#24615;&#65292;&#20363;&#22914;&#25913;&#21892;&#20102;LLaMA&#31995;&#21015;&#27169;&#22411;&#22312;TruthfulQA&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#19968;&#22871;&#20505;&#36873;&#39564;&#35777;&#26631;&#20934;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#35780;&#20272;&#27969;&#34892;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#20998;&#26512;&#20102;&#22312;&#20351;&#29992;&#33391;&#22909;&#30340;&#35780;&#20272;&#23454;&#36341;&#26102;&#30340;&#22495;&#33258;&#36866;&#24212;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#35770;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03879</link><description>&lt;p&gt;
&#25552;&#21319;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20339;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Better Practices for Domain Adaptation. (arXiv:2309.03879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#19968;&#22871;&#20505;&#36873;&#39564;&#35777;&#26631;&#20934;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#35780;&#20272;&#27969;&#34892;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#20998;&#26512;&#20102;&#22312;&#20351;&#29992;&#33391;&#22909;&#30340;&#35780;&#20272;&#23454;&#36341;&#26102;&#30340;&#22495;&#33258;&#36866;&#24212;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#35770;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20998;&#24067;&#20559;&#31227;&#26159;&#38750;&#24120;&#24120;&#35265;&#30340;&#29616;&#35937;&#12290;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#21508;&#31181;&#26694;&#26550;&#26469;&#36866;&#24212;&#27169;&#22411;&#21040;&#37096;&#32626;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#26631;&#31614;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22495;&#20559;&#31227;&#22330;&#26223;&#24341;&#21457;&#20102;&#21478;&#19968;&#20010;&#36739;&#20026;&#24494;&#22937;&#30340;&#25361;&#25112;&#65306;&#22312;&#27809;&#26377;&#24102;&#26631;&#31614;&#30340;&#39564;&#35777;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#36825;&#20123;&#33258;&#36866;&#24212;&#31639;&#27861;&#25191;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#30340;&#22256;&#38590;&#12290;DA&#30340;&#19981;&#26126;&#30830;&#30340;&#39564;&#35777;&#21327;&#35758;&#23548;&#33268;&#20102;&#25991;&#29486;&#20013;&#30340;&#19981;&#33391;&#23454;&#36341;&#65292;&#20363;&#22914;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#30446;&#26631;&#27979;&#35797;&#26631;&#31614;&#26469;&#25191;&#34892;HPO&#65292;&#32780;&#36825;&#20123;&#26631;&#31614;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;DA&#30740;&#31350;&#36827;&#23637;&#19982;&#23454;&#38469;&#24773;&#20917;&#30456;&#27604;&#23384;&#22312;&#36807;&#24230;&#20048;&#35266;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#22871;&#20505;&#36873;&#39564;&#35777;&#26631;&#20934;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#35780;&#20272;&#27969;&#34892;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#20998;&#26512;&#20102;&#22312;&#20351;&#29992;&#33391;&#22909;&#30340;&#35780;&#20272;&#23454;&#36341;&#26102;&#30340;&#22495;&#33258;&#36866;&#24212;&#29366;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#35770;&#30340;&#19977;&#20010;&#20998;&#25903;&#37117;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution shifts are all too common in real-world applications of machine learning. Domain adaptation (DA) aims to address this by providing various frameworks for adapting models to the deployment data without using labels. However, the domain shift scenario raises a second more subtle challenge: the difficulty of performing hyperparameter optimisation (HPO) for these adaptation algorithms without access to a labelled validation set. The unclear validation protocol for DA has led to bad practices in the literature, such as performing HPO using the target test labels when, in real-world scenarios, they are not available. This has resulted in over-optimism about DA research progress compared to reality. In this paper, we analyse the state of DA when using good evaluation practice, by benchmarking a suite of candidate validation criteria and using them to assess popular adaptation algorithms. We show that there are challenges across all three branches of domain adaptation methodology 
&lt;/p&gt;</description></item><item><title>OpinionGPT&#26159;&#19968;&#20010;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;web&#28436;&#31034;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#21508;&#31181;&#20559;&#35265;&#24182;&#36827;&#34892;&#27604;&#36739;&#65292;&#23427;&#24847;&#22312;&#20351;&#27169;&#22411;&#30340;&#20559;&#35265;&#26174;&#24615;&#21644;&#36879;&#26126;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.03876</link><description>&lt;p&gt;
OpinionGPT: &#27169;&#25311;&#26174;&#24615;&#20559;&#35265;&#30340;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)
&lt;/p&gt;
&lt;p&gt;
OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs. (arXiv:2309.03876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03876
&lt;/p&gt;
&lt;p&gt;
OpinionGPT&#26159;&#19968;&#20010;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;web&#28436;&#31034;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#21508;&#31181;&#20559;&#35265;&#24182;&#36827;&#34892;&#27604;&#36739;&#65292;&#23427;&#24847;&#22312;&#20351;&#27169;&#22411;&#30340;&#20559;&#35265;&#26174;&#24615;&#21644;&#36879;&#26126;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#29983;&#25104;&#19982;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30456;&#21305;&#37197;&#30340;&#22238;&#24212;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#28041;&#21450;&#35757;&#32451;&#27169;&#22411;&#21644;&#23427;&#20204;&#30340;&#22238;&#24212;&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#29992;&#20110;&#35843;&#25972;LLM&#30340;&#25968;&#25454;&#20027;&#35201;&#30001;&#20855;&#26377;&#29305;&#23450;&#25919;&#27835;&#20559;&#35265;&#30340;&#20154;&#32534;&#20889;&#65292;&#25105;&#20204;&#21487;&#33021;&#20250;&#26399;&#26395;&#29983;&#25104;&#30340;&#22238;&#31572;&#20063;&#20849;&#20139;&#36825;&#31181;&#20559;&#35265;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#24037;&#20316;&#26088;&#22312;&#38500;&#21435;&#36825;&#26679;&#30340;&#27169;&#22411;&#20559;&#35265;&#65292;&#25110;&#25233;&#21046;&#21487;&#33021;&#26377;&#20559;&#35265;&#30340;&#22238;&#31572;&#12290;&#36890;&#36807;&#36825;&#20010;&#28436;&#31034;&#65292;&#25105;&#20204;&#23545;&#25351;&#20196;&#35843;&#25972;&#20013;&#30340;&#20559;&#35265;&#25345;&#26377;&#19981;&#21516;&#30340;&#35266;&#28857;&#65306;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#26159;&#25233;&#21046;&#23427;&#20204;&#65292;&#32780;&#26159;&#20351;&#23427;&#20204;&#26174;&#24615;&#21644;&#36879;&#26126;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;OpinionGPT&#65292;&#19968;&#20010;&#32593;&#32476;&#28436;&#31034;&#65292;&#29992;&#25143;&#21487;&#20197;&#25552;&#38382;&#24182;&#36873;&#25321;&#25152;&#26377;&#20182;&#20204;&#24076;&#26395;&#35843;&#26597;&#30340;&#20559;&#35265;&#12290;&#35813;&#28436;&#31034;&#23558;&#20351;&#29992;&#22312;&#20195;&#34920;&#27599;&#20010;&#36873;&#25321;&#20559;&#35265;&#30340;&#25991;&#26412;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#24182;&#25490;&#27604;&#36739;&#12290;&#20026;&#20102;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#36873;&#21462;&#20102;11&#20010;...
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable ability to generate fitting responses to natural language instructions. However, an open research question concerns the inherent biases of trained models and their responses. For instance, if the data used to tune an LLM is dominantly written by persons with a specific political bias, we might expect generated answers to share this bias. Current research work seeks to de-bias such models, or suppress potentially biased answers. With this demonstration, we take a different view on biases in instruction-tuning: Rather than aiming to suppress them, we aim to make them explicit and transparent. To this end, we present OpinionGPT, a web demo in which users can ask questions and select all biases they wish to investigate. The demo will answer this question using a model fine-tuned on text representing each of the selected biases, allowing side-by-side comparison. To train the underlying model, we identified 11 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#25945;&#31243;&#20171;&#32461;&#20102;&#31995;&#32479;&#35782;&#21035;&#20013;&#26368;&#36817;&#21457;&#23637;&#30340;&#38750;&#28176;&#36817;&#26041;&#27861;&#12289;&#24378;&#35843;&#20102;&#35206;&#30422;&#25216;&#26415;&#12289;Hanson-Wright&#19981;&#31561;&#24335;&#21644;&#33258;&#26631;&#20934;&#21270;&#39532;&#19969;&#26684;&#23572;&#27861;&#31561;&#24037;&#20855;&#30340;&#24212;&#29992;&#12289;&#24182;&#32473;&#20986;&#20102;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#31616;&#21270;&#35777;&#26126;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#22312;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#21442;&#25968;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#12289;&#26368;&#21518;&#20171;&#32461;&#20102;&#23558;&#36825;&#20123;&#24605;&#24819;&#25193;&#23637;&#21040;&#26576;&#20123;&#38750;&#32447;&#24615;&#35782;&#21035;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03873</link><description>&lt;p&gt;
&#31995;&#32479;&#35782;&#21035;&#30340;&#38750;&#28176;&#36817;&#29702;&#35770;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
A Tutorial on the Non-Asymptotic Theory of System Identification. (arXiv:2309.03873v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03873
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#25945;&#31243;&#20171;&#32461;&#20102;&#31995;&#32479;&#35782;&#21035;&#20013;&#26368;&#36817;&#21457;&#23637;&#30340;&#38750;&#28176;&#36817;&#26041;&#27861;&#12289;&#24378;&#35843;&#20102;&#35206;&#30422;&#25216;&#26415;&#12289;Hanson-Wright&#19981;&#31561;&#24335;&#21644;&#33258;&#26631;&#20934;&#21270;&#39532;&#19969;&#26684;&#23572;&#27861;&#31561;&#24037;&#20855;&#30340;&#24212;&#29992;&#12289;&#24182;&#32473;&#20986;&#20102;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#31616;&#21270;&#35777;&#26126;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#22312;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#21442;&#25968;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#12289;&#26368;&#21518;&#20171;&#32461;&#20102;&#23558;&#36825;&#20123;&#24605;&#24819;&#25193;&#23637;&#21040;&#26576;&#20123;&#38750;&#32447;&#24615;&#35782;&#21035;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#25945;&#31243;&#20171;&#32461;&#26368;&#36817;&#21457;&#23637;&#30340;&#38750;&#28176;&#36817;&#26041;&#27861;&#22312;&#20027;&#35201;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#29702;&#35770;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24378;&#35843;&#19968;&#20123;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#29305;&#21035;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#22914;&#35206;&#30422;&#25216;&#26415;&#12289;Hanson-Wright&#19981;&#31561;&#24335;&#21644;&#33258;&#26631;&#20934;&#21270;&#39532;&#19969;&#26684;&#23572;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#26469;&#32473;&#20986;&#19968;&#20123;&#22522;&#20110;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#30340;&#31616;&#21270;&#35777;&#26126;&#65292;&#29992;&#20110;&#35782;&#21035;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#22914;&#20309;&#23558;&#25152;&#21576;&#29616;&#30340;&#24605;&#24819;&#25193;&#23637;&#21040;&#26576;&#20123;&#38750;&#32447;&#24615;&#35782;&#21035;&#38382;&#39064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This tutorial serves as an introduction to recently developed non-asymptotic methods in the theory of -- mainly linear -- system identification. We emphasize tools we deem particularly useful for a range of problems in this domain, such as the covering technique, the Hanson-Wright Inequality and the method of self-normalized martingales. We then employ these tools to give streamlined proofs of the performance of various least-squares based estimators for identifying the parameters in autoregressive models. We conclude by sketching out how the ideas presented herein can be extended to certain nonlinear identification problems.
&lt;/p&gt;</description></item><item><title>CenTime&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#20107;&#20214;&#26465;&#20214;&#23457;&#26597;&#26426;&#21046;&#30452;&#25509;&#20272;&#35745;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#65292;&#22312;&#22788;&#29702;&#26410;&#34987;&#23457;&#26597;&#30340;&#25968;&#25454;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03851</link><description>&lt;p&gt;
CenTime: &#20107;&#20214;&#26465;&#20214;&#27169;&#22411;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
CenTime: Event-Conditional Modelling of Censoring in Survival Analysis. (arXiv:2309.03851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03851
&lt;/p&gt;
&lt;p&gt;
CenTime&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#20107;&#20214;&#26465;&#20214;&#23457;&#26597;&#26426;&#21046;&#30452;&#25509;&#20272;&#35745;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#65292;&#22312;&#22788;&#29702;&#26410;&#34987;&#23457;&#26597;&#30340;&#25968;&#25454;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22522;&#20110;&#22522;&#32447;&#35266;&#27979;&#26469;&#20272;&#35745;&#29305;&#23450;&#20107;&#20214;&#65288;&#22914;&#27515;&#20129;&#25110;&#30284;&#30151;&#22797;&#21457;&#65289;&#21457;&#29983;&#30340;&#26102;&#38388;&#12290;&#36825;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#25968;&#25454;&#39044;&#27979;&#20020;&#24202;&#37325;&#35201;&#20107;&#20214;&#30340;&#39044;&#21518;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24120;&#24120;&#23384;&#22312;&#23616;&#38480;&#24615;&#65307;&#26377;&#20123;&#26041;&#27861;&#21482;&#20851;&#27880;&#23558;&#24739;&#32773;&#25353;&#29983;&#23384;&#33021;&#21147;&#36827;&#34892;&#25490;&#21517;&#65292;&#24573;&#35270;&#20102;&#23545;&#23454;&#38469;&#20107;&#20214;&#26102;&#38388;&#30340;&#20272;&#35745;&#65307;&#32780;&#20854;&#20182;&#26041;&#27861;&#23558;&#38382;&#39064;&#35270;&#20026;&#20998;&#31867;&#20219;&#21153;&#65292;&#24573;&#35270;&#20102;&#20107;&#20214;&#30340;&#26102;&#38388;&#39034;&#24207;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#26377;&#25928;&#21033;&#29992;&#34987;&#23457;&#26597;&#26679;&#26412;&#65288;&#35757;&#32451;&#25968;&#25454;&#28857;&#65292;&#20854;&#20013;&#30830;&#20999;&#20107;&#20214;&#26102;&#38388;&#19981;&#21487;&#30693;&#65289;&#23545;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CenTime&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#30452;&#25509;&#20272;&#35745;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#21019;&#26032;&#30340;&#20107;&#20214;&#26465;&#20214;&#23457;&#26597;&#26426;&#21046;&#65292;&#21363;&#20351;&#27809;&#26377;&#26410;&#34987;&#23457;&#26597;&#30340;&#25968;&#25454;&#65292;&#20063;&#33021;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis is a valuable tool for estimating the time until specific events, such as death or cancer recurrence, based on baseline observations. This is particularly useful in healthcare to prognostically predict clinically important events based on patient data. However, existing approaches often have limitations; some focus only on ranking patients by survivability, neglecting to estimate the actual event time, while others treat the problem as a classification task, ignoring the inherent time-ordered structure of the events. Furthermore, the effective utilization of censored samples - training data points where the exact event time is unknown - is essential for improving the predictive accuracy of the model. In this paper, we introduce CenTime, a novel approach to survival analysis that directly estimates the time to event. Our method features an innovative event-conditional censoring mechanism that performs robustly even when uncensored data is scarce. We demonstrate that ou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#21644;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20272;&#35745;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#26080;&#38656;&#23545;GMMs&#20570;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2309.03847</link><description>&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#29289;&#21487;&#20197;&#36890;&#36807;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples. (arXiv:2309.03847v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#21644;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20272;&#35745;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#26080;&#38656;&#23545;GMMs&#20570;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;(DP)&#32422;&#26463;&#19979;&#20272;&#35745;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#20351;&#29992;$\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$&#20010;&#26679;&#26412;&#21363;&#21487;&#22312;&#28385;&#36275;$(\varepsilon, \delta)$-DP&#30340;&#26465;&#20214;&#19979;&#20272;&#35745;$k$&#20010;&#39640;&#26031;&#28151;&#21512;&#29289;&#65292;&#20351;&#20854;&#36798;&#21040;&#24635;&#21464;&#24046;&#36317;&#31163;$\alpha$&#12290;&#36825;&#26159;&#35813;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24615;&#19978;&#38480;&#65292;&#32780;&#26080;&#38656;&#23545;GMMs&#20570;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#20110;&#20854;&#20182;&#20219;&#21153;&#21487;&#33021;&#20063;&#26377;&#29992;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#19968;&#20010;&#20998;&#24067;&#31867;&#65288;&#27604;&#22914;&#39640;&#26031;&#20998;&#24067;&#65289;&#26159;&#65288;1&#65289;&#21487;&#21015;&#34920;&#35793;&#30721;&#30340;&#24182;&#19988;&#65288;2&#65289;&#22312;&#24635;&#21464;&#24046;&#36317;&#31163;&#26041;&#38754;&#20855;&#26377;&#8220;&#23616;&#37096;&#23567;&#8221;&#35206;&#30422;[ BKSW19]&#65292;&#21017;&#20854;&#28151;&#21512;&#29289;&#31867;&#26159;&#31169;&#23494;&#21487;&#23398;&#20064;&#30340;&#12290;&#35777;&#26126;&#32469;&#36807;&#20102;&#19968;&#20010;&#24050;&#30693;&#38556;&#30861;&#65292;&#34920;&#26126;&#19982;&#39640;&#26031;&#20998;&#24067;&#19981;&#21516;&#65292;GMMs&#19981;&#20855;&#26377;&#23616;&#37096;&#23567;&#30340;&#35206;&#30422;[AAL21]&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of estimating mixtures of Gaussians under the constraint of differential privacy (DP). Our main result is that $\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$ samples are sufficient to estimate a mixture of $k$ Gaussians up to total variation distance $\alpha$ while satisfying $(\varepsilon, \delta)$-DP. This is the first finite sample complexity upper bound for the problem that does not make any structural assumptions on the GMMs.  To solve the problem, we devise a new framework which may be useful for other tasks. On a high level, we show that if a class of distributions (such as Gaussians) is (1) list decodable and (2) admits a "locally small'' cover [BKSW19] with respect to total variation distance, then the class of its mixtures is privately learnable. The proof circumvents a known barrier indicating that, unlike Gaussians, GMMs do not admit a locally small cover [AAL21].
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#29305;&#24449;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#20102;&#22312;&#38750;&#21508;&#21521;&#24322;&#24615;&#35774;&#32622;&#20013;&#65292;&#24120;&#29992;&#30340;&#29699;&#24418;&#26799;&#24230;&#21160;&#21147;&#23398;&#21487;&#33021;&#26080;&#27861;&#24674;&#22797;&#30495;&#23454;&#26041;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#24403;&#30340;&#26435;&#37325;&#24402;&#19968;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21327;&#26041;&#24046;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#21487;&#20197;&#33719;&#24471;&#27604;&#21508;&#21521;&#21516;&#24615;&#24773;&#20917;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.03843</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Gradient-Based Feature Learning under Structured Data. (arXiv:2309.03843v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#29305;&#24449;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#20102;&#22312;&#38750;&#21508;&#21521;&#24322;&#24615;&#35774;&#32622;&#20013;&#65292;&#24120;&#29992;&#30340;&#29699;&#24418;&#26799;&#24230;&#21160;&#21147;&#23398;&#21487;&#33021;&#26080;&#27861;&#24674;&#22797;&#30495;&#23454;&#26041;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#24403;&#30340;&#26435;&#37325;&#24402;&#19968;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#21327;&#26041;&#24046;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#21487;&#20197;&#33719;&#24471;&#27604;&#21508;&#21521;&#21516;&#24615;&#24773;&#20917;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#21333;&#25351;&#25968;&#27169;&#22411;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21462;&#20915;&#20110;&#23427;&#20204;&#30340;&#20449;&#24687;&#25351;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#20165;&#28041;&#21450;&#21508;&#21521;&#21516;&#24615;&#25968;&#25454;&#65292;&#32780;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36755;&#20837;&#24448;&#24448;&#21253;&#21547;&#39069;&#22806;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#38544;&#21547;&#22320;&#25351;&#23548;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#23574;&#23792;&#21327;&#26041;&#24046;&#32467;&#26500;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#29616;&#35937;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#38750;&#21508;&#21521;&#24322;&#24615;&#35774;&#32622;&#20013;&#65292;&#24120;&#29992;&#30340;&#29699;&#24418;&#26799;&#24230;&#21160;&#21147;&#23398;&#21363;&#20351;&#22312;&#23574;&#23792;&#19982;&#30446;&#26631;&#26041;&#21521;&#23436;&#20840;&#23545;&#40784;&#26102;&#20063;&#21487;&#33021;&#26080;&#27861;&#24674;&#22797;&#30495;&#23454;&#26041;&#21521;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#36866;&#24403;&#30340;&#26435;&#37325;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#65288;&#23574;&#23792;&#65289;&#36755;&#20837;&#21327;&#26041;&#24046;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27604;&#21508;&#21521;&#21516;&#24615;&#24773;&#20917;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated that the sample complexity of gradient-based learning of single index models, i.e. functions that depend on a 1-dimensional projection of the input data, is governed by their information exponent. However, these results are only concerned with isotropic data, while in practice the input often contains additional structure which can implicitly guide the algorithm. In this work, we investigate the effect of a spiked covariance structure and reveal several interesting phenomena. First, we show that in the anisotropic setting, the commonly used spherical gradient dynamics may fail to recover the true direction, even when the spike is perfectly aligned with the target direction. Next, we show that appropriate weight normalization that is reminiscent of batch normalization can alleviate this issue. Further, by exploiting the alignment between the (spiked) input covariance and the target, we obtain improved sample complexity compared to the isotropic case. In pa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#21521;&#24322;&#24615;&#25193;&#25955;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20302;&#32500;&#27969;&#24418;&#20013;&#30340;&#28508;&#22312;&#28436;&#21270;&#21160;&#24577;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#26089;&#26399;&#35686;&#25253;&#20449;&#21495;&#26469;&#26816;&#27979;&#22797;&#26434;&#31995;&#32479;&#25110;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21160;&#21147;&#23398;&#36716;&#21464;&#65292;&#24182;&#22312;&#30495;&#23454;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.03842</link><description>&lt;p&gt;
&#38544;&#24615;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#30340;&#36716;&#21521;&#39044;&#35686;
&lt;/p&gt;
&lt;p&gt;
Early warning via transitions in latent stochastic dynamical systems. (arXiv:2309.03842v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#21521;&#24322;&#24615;&#25193;&#25955;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20302;&#32500;&#27969;&#24418;&#20013;&#30340;&#28508;&#22312;&#28436;&#21270;&#21160;&#24577;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#26089;&#26399;&#35686;&#25253;&#20449;&#21495;&#26469;&#26816;&#27979;&#22797;&#26434;&#31995;&#32479;&#25110;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21160;&#21147;&#23398;&#36716;&#21464;&#65292;&#24182;&#22312;&#30495;&#23454;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22914;&#22522;&#22240;&#31361;&#21464;&#12289;&#33041;&#30142;&#30149;&#12289;&#33258;&#28982;&#28798;&#23475;&#12289;&#37329;&#34701;&#21361;&#26426;&#21644;&#24037;&#31243;&#21487;&#38752;&#24615;&#65292;&#23545;&#22797;&#26434;&#31995;&#32479;&#25110;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21160;&#21147;&#23398;&#36716;&#21464;&#36827;&#34892;&#26089;&#26399;&#35686;&#25253;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#26377;&#25928;&#25552;&#21462;&#26089;&#26399;&#35686;&#25253;&#20449;&#21495;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#23450;&#21521;&#24322;&#24615;&#25193;&#25955;&#22270;&#65292;&#23427;&#25429;&#25417;&#20102;&#20302;&#32500;&#27969;&#24418;&#20013;&#30340;&#28508;&#22312;&#28436;&#21270;&#21160;&#24577;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#30495;&#23454;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#25105;&#20204;&#25104;&#21151;&#25214;&#21040;&#20102;&#36866;&#24403;&#30340;&#26377;&#25928;&#22352;&#26631;&#65292;&#24182;&#25512;&#23548;&#20986;&#33021;&#22815;&#26816;&#27979;&#29366;&#24577;&#36716;&#21464;&#20013;&#20020;&#30028;&#28857;&#30340;&#26089;&#26399;&#35686;&#25253;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#28508;&#22312;&#21160;&#24577;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#32852;&#31995;&#36215;&#26469;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#23494;&#24230;&#21644;&#36716;&#21464;&#27010;&#29575;&#31561;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31532;&#20108;&#20010;&#22352;&#26631;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#20013;&#20445;&#25345;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early warnings for dynamical transitions in complex systems or high-dimensional observation data are essential in many real world applications, such as gene mutation, brain diseases, natural disasters, financial crises, and engineering reliability. To effectively extract early warning signals, we develop a novel approach: the directed anisotropic diffusion map that captures the latent evolutionary dynamics in low-dimensional manifold. Applying the methodology to authentic electroencephalogram (EEG) data, we successfully find the appropriate effective coordinates, and derive early warning signals capable of detecting the tipping point during the state transition. Our method bridges the latent dynamics with the original dataset. The framework is validated to be accurate and effective through numerical experiments, in terms of density and transition probability. It is shown that the second coordinate holds meaningful information for critical transition in various evaluation metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35757;&#32451;&#33258;&#36866;&#24212;&#20154;&#26426;&#30028;&#38754;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#36890;&#36807;&#22024;&#26434;&#30340;&#39640;&#32500;&#36755;&#20837;&#36890;&#36947;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#12290;&#30740;&#31350;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#26469;&#34920;&#31034;&#21644;&#25512;&#26029;&#29992;&#25143;&#23545;&#32473;&#23450;&#36712;&#36857;&#30340;&#38271;&#26399;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2309.03839</link><description>&lt;p&gt;
&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20026;&#33258;&#36866;&#24212;&#20154;&#26426;&#30028;&#38754;&#24341;&#23548;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning. (arXiv:2309.03839v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35757;&#32451;&#33258;&#36866;&#24212;&#20154;&#26426;&#30028;&#38754;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#36890;&#36807;&#22024;&#26434;&#30340;&#39640;&#32500;&#36755;&#20837;&#36890;&#36947;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#12290;&#30740;&#31350;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#26469;&#34920;&#31034;&#21644;&#25512;&#26029;&#29992;&#25143;&#23545;&#32473;&#23450;&#36712;&#36857;&#30340;&#38271;&#26399;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#30028;&#38754;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#22312;&#32473;&#23450;&#22024;&#26434;&#30340;&#39640;&#32500;&#21629;&#20196;&#20449;&#21495;&#65288;&#20363;&#22914;&#26469;&#33258;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65289;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#36965;&#25805;&#20316;&#12290;&#20154;&#22312;&#24490;&#29615;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#36825;&#20123;&#31995;&#32479;&#33021;&#22815;&#36890;&#36807;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#26469;&#25913;&#36827;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#21463;&#38480;&#20110;&#20174;&#21333;&#20010;&#29992;&#25143;&#25910;&#38598;&#30340;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#35757;&#32451;&#30028;&#38754;&#20197;&#23558;&#21407;&#22987;&#21629;&#20196;&#20449;&#21495;&#26144;&#23556;&#21040;&#21160;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#22024;&#26434;&#21629;&#20196;&#20449;&#21495;&#21644;&#31232;&#30095;&#22870;&#21169;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#31034;&#21644;&#25512;&#26029;&#29992;&#25143;&#23545;&#32473;&#23450;&#36712;&#36857;&#30340;&#38271;&#26399;&#24847;&#22270;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#20027;&#35201;&#36890;&#36807;&#19968;&#20010;&#29992;&#25143;&#30740;&#31350;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24110;&#21161;&#20165;&#36890;&#36807;&#22024;&#26434;&#30340;&#39640;&#32500;&#36755;&#20837;&#36890;&#36947;&#36827;&#34892;&#36890;&#20449;&#30340;&#29992;&#25143;&#26102;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#20013;&#26377;12&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#27169;&#25311;&#23548;&#33322;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive interfaces can help users perform sequential decision-making tasks like robotic teleoperation given noisy, high-dimensional command signals (e.g., from a brain-computer interface). Recent advances in human-in-the-loop machine learning enable such systems to improve by interacting with users, but tend to be limited by the amount of data that they can collect from individual users in practice. In this paper, we propose a reinforcement learning algorithm to address this by training an interface to map raw command signals to actions using a combination of offline pre-training and online fine-tuning. To address the challenges posed by noisy command signals and sparse rewards, we develop a novel method for representing and inferring the user's long-term intent for a given trajectory. We primarily evaluate our method's ability to assist users who can only communicate through noisy, high-dimensional input channels through a user study in which 12 participants performed a simulated nav
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#36328;&#20219;&#21153;&#38388;&#26356;&#22909;&#22320;&#21033;&#29992;&#20449;&#24687;&#20849;&#20139;&#65292;&#20197;&#25552;&#39640;&#21307;&#23398;&#24433;&#20687;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36328;&#20219;&#21153;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25105;&#20204;&#30340;&#36328;&#20219;&#21153;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;CTAN&#65289;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#26377;&#25928;&#25972;&#21512;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.03837</link><description>&lt;p&gt;
&#36328;&#20219;&#21153;&#27880;&#24847;&#21147;&#32593;&#32476;&#65306;&#25913;&#36827;&#21307;&#23398;&#24433;&#20687;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-Task Attention Network: Improving Multi-Task Learning for Medical Imaging Applications. (arXiv:2309.03837v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#36328;&#20219;&#21153;&#38388;&#26356;&#22909;&#22320;&#21033;&#29992;&#20449;&#24687;&#20849;&#20139;&#65292;&#20197;&#25552;&#39640;&#21307;&#23398;&#24433;&#20687;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36328;&#20219;&#21153;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25105;&#20204;&#30340;&#36328;&#20219;&#21153;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;CTAN&#65289;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#26377;&#25928;&#25972;&#21512;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#20219;&#21153;&#30340;&#20449;&#24687;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#65292;MTL&#24050;&#32463;&#23637;&#29616;&#20986;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21307;&#23398;&#24433;&#20687;MTL&#26550;&#26500;&#22312;&#36328;&#20219;&#21153;&#20449;&#24687;&#20849;&#20139;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#38477;&#20302;&#20102;MTL&#30340;&#28508;&#22312;&#24615;&#33021;&#25913;&#36827;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;MTL&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#20219;&#21153;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;CTAN&#65289;&#65292;&#21033;&#29992;&#36328;&#20219;&#21153;&#27880;&#24847;&#21147;&#26426;&#21046;&#36890;&#36807;&#20219;&#21153;&#38388;&#30340;&#20132;&#20114;&#26469;&#25972;&#21512;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#39044;&#27979;&#12289;&#33394;&#32032;&#24615;&#30382;&#32932;&#25439;&#20260;&#20998;&#21106;&#21644;&#35786;&#26029;&#31561;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#22235;&#20010;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;CTAN&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) is a powerful approach in deep learning that leverages the information from multiple tasks during training to improve model performance. In medical imaging, MTL has shown great potential to solve various tasks. However, existing MTL architectures in medical imaging are limited in sharing information across tasks, reducing the potential performance improvements of MTL. In this study, we introduce a novel attention-based MTL framework to better leverage inter-task interactions for various tasks from pixel-level to image-level predictions. Specifically, we propose a Cross-Task Attention Network (CTAN) which utilizes cross-task attention mechanisms to incorporate information by interacting across tasks. We validated CTAN on four medical imaging datasets that span different domains and tasks including: radiation treatment planning prediction using planning CT images of two different target cancers (Prostate, OpenKBP); pigmented skin lesion segmentation and diagnosi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31034;&#25945;&#23398;&#30340;&#31034;&#25945;&#23398;&#20064;&#30340;&#26367;&#20195;&#33539;&#24335;&#65292;&#36890;&#36807;&#35201;&#27714;&#29992;&#25143;&#22312;&#22330;&#26223;&#30340;&#20108;&#32500;&#22270;&#20687;&#19978;&#21246;&#21202;&#31034;&#33539;&#36712;&#36857;&#26469;&#25945;&#26426;&#22120;&#20154;&#26032;&#30340;&#25216;&#33021;&#65292;&#24182;&#23558;&#20854;&#21512;&#25104;&#20026;&#19977;&#32500;&#20219;&#21153;&#31354;&#38388;&#20013;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.03835</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#24615;&#22270;&#31034;&#25945;&#23398;&#36827;&#34892;&#31034;&#25945;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Demonstration via Probabilistic Diagrammatic Teaching. (arXiv:2309.03835v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31034;&#25945;&#23398;&#30340;&#31034;&#25945;&#23398;&#20064;&#30340;&#26367;&#20195;&#33539;&#24335;&#65292;&#36890;&#36807;&#35201;&#27714;&#29992;&#25143;&#22312;&#22330;&#26223;&#30340;&#20108;&#32500;&#22270;&#20687;&#19978;&#21246;&#21202;&#31034;&#33539;&#36712;&#36857;&#26469;&#25945;&#26426;&#22120;&#20154;&#26032;&#30340;&#25216;&#33021;&#65292;&#24182;&#23558;&#20854;&#21512;&#25104;&#20026;&#19977;&#32500;&#20219;&#21153;&#31354;&#38388;&#20013;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31034;&#25945;&#23398;&#20064;&#65288;Learning for Demonstration&#65292;LfD&#65289;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#31034;&#33539;&#26469;&#33719;&#24471;&#26032;&#25216;&#33021;&#65292;&#20801;&#35768;&#29992;&#25143;&#20197;&#30452;&#35266;&#30340;&#26041;&#24335;&#20256;&#36798;&#20182;&#20204;&#30340;&#25351;&#31034;&#12290;&#26368;&#36817;&#22312;LfD&#39046;&#22495;&#30340;&#36827;&#23637;&#24448;&#24448;&#20381;&#36182;&#20110;&#21160;&#20316;&#31034;&#33539;&#25945;&#23398;&#25110;&#36828;&#31243;&#25805;&#20316;&#20316;&#20026;&#29992;&#25143;&#25351;&#23450;&#31034;&#33539;&#30340;&#25163;&#27573;&#12290;&#21160;&#20316;&#31034;&#33539;&#25945;&#23398;&#38656;&#35201;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#29289;&#29702;&#25805;&#32437;&#65292;&#32780;&#36828;&#31243;&#25805;&#20316;&#21017;&#38656;&#35201;&#29087;&#32451;&#25484;&#25569;&#39069;&#22806;&#30340;&#30828;&#20214;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#31034;&#25945;&#23398;&#30340;LfD&#30340;&#26367;&#20195;&#33539;&#24335;&#12290;&#22270;&#31034;&#25945;&#23398;&#26088;&#22312;&#36890;&#36807;&#35201;&#27714;&#29992;&#25143;&#22312;&#22330;&#26223;&#30340;&#20108;&#32500;&#22270;&#20687;&#19978;&#21246;&#21202;&#31034;&#33539;&#36712;&#36857;&#26469;&#25945;&#26426;&#22120;&#20154;&#26032;&#30340;&#25216;&#33021;&#65292;&#28982;&#21518;&#36825;&#20123;&#36712;&#36857;&#23558;&#34987;&#21512;&#25104;&#20026;&#19977;&#32500;&#20219;&#21153;&#31354;&#38388;&#20013;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29992;&#20110;&#22270;&#31034;&#25945;&#23398;&#30340;&#23556;&#32447;&#36861;&#36394;&#27010;&#29575;&#36712;&#36857;&#23398;&#20064;&#65288;RPTL&#65289;&#26694;&#26550;&#12290;RPTL&#20174;&#20108;&#32500;&#22270;&#31034;&#20013;&#25552;&#21462;&#26102;&#38388;&#21464;&#21270;&#30340;&#27010;&#29575;&#23494;&#24230;&#65292;&#24182;&#24212;&#29992;&#23556;&#32447;&#36861;&#36394;&#26469;&#23547;&#25214;&#30456;&#24212;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning for Demonstration (LfD) enables robots to acquire new skills by imitating expert demonstrations, allowing users to communicate their instructions in an intuitive manner. Recent progress in LfD often relies on kinesthetic teaching or teleoperation as the medium for users to specify the demonstrations. Kinesthetic teaching requires physical handling of the robot, while teleoperation demands proficiency with additional hardware. This paper introduces an alternative paradigm for LfD called Diagrammatic Teaching. Diagrammatic Teaching aims to teach robots novel skills by prompting the user to sketch out demonstration trajectories on 2D images of the scene, these are then synthesised as a generative model of motion trajectories in 3D task space. Additionally, we present the Ray-tracing Probabilistic Trajectory Learning (RPTL) framework for Diagrammatic Teaching. RPTL extracts time-varying probability densities from the 2D sketches, applies ray-tracing to find corresponding regions i
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28418;&#31227;&#12290;&#36890;&#36807;&#32534;&#30721;&#29983;&#20135;&#25968;&#25454;&#26679;&#26412;&#21644;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#21033;&#29992;&#26680;&#32479;&#35745;&#26816;&#39564;&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#36317;&#31163;&#24230;&#37327;&#26469;&#27604;&#36739;&#20998;&#24067;&#65292;&#21487;&#20197;&#26377;&#25928;&#20272;&#35745;&#28418;&#31227;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.03831</link><description>&lt;p&gt;
&#25581;&#31034;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#28418;&#31227;&#65306;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#21644;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Uncovering Drift in Textual Data: An Unsupervised Method for Detecting and Mitigating Drift in Machine Learning Models. (arXiv:2309.03831v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03831
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28418;&#31227;&#12290;&#36890;&#36807;&#32534;&#30721;&#29983;&#20135;&#25968;&#25454;&#26679;&#26412;&#21644;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#21033;&#29992;&#26680;&#32479;&#35745;&#26816;&#39564;&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#36317;&#31163;&#24230;&#37327;&#26469;&#27604;&#36739;&#20998;&#24067;&#65292;&#21487;&#20197;&#26377;&#25928;&#20272;&#35745;&#28418;&#31227;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28418;&#31227;&#25351;&#30340;&#26159;&#25968;&#25454;&#25110;&#27169;&#22411;&#36816;&#34892;&#19978;&#19979;&#25991;&#30340;&#32479;&#35745;&#29305;&#24615;&#38543;&#26102;&#38388;&#21464;&#21270;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#29616;&#35937;&#12290;&#22240;&#27492;&#65292;&#20445;&#25345;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#25345;&#32493;&#30417;&#25511;&#36807;&#31243;&#23545;&#20110;&#39044;&#38450;&#28508;&#22312;&#24615;&#33021;&#22238;&#36864;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26377;&#30417;&#30563;&#30340;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#65292;&#20174;&#32780;&#23548;&#33268;&#28418;&#31227;&#26816;&#27979;&#21644;&#20943;&#36731;&#36807;&#31243;&#26102;&#38388;&#36739;&#38271;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26080;&#30417;&#30563;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#20010;&#27493;&#39588;&#30340;&#27969;&#31243;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#28041;&#21450;&#23558;&#29983;&#20135;&#25968;&#25454;&#30340;&#19968;&#20010;&#26679;&#26412;&#20316;&#20026;&#30446;&#26631;&#20998;&#24067;&#65292;&#23558;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20316;&#20026;&#21442;&#32771;&#20998;&#24067;&#36827;&#34892;&#32534;&#30721;&#12290;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26680;&#30340;&#32479;&#35745;&#26816;&#39564;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#36317;&#31163;&#24230;&#37327;&#26469;&#27604;&#36739;&#21442;&#32771;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#65292;&#20272;&#35745;&#20219;&#20309;&#28508;&#22312;&#30340;&#28418;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#30830;&#23450;&#29983;&#20135;&#25968;&#25454;&#23376;&#38598;&#30340;&#28418;&#31227;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drift in machine learning refers to the phenomenon where the statistical properties of data or context, in which the model operates, change over time leading to a decrease in its performance. Therefore, maintaining a constant monitoring process for machine learning model performance is crucial in order to proactively prevent any potential performance regression. However, supervised drift detection methods require human annotation and consequently lead to a longer time to detect and mitigate the drift. In our proposed unsupervised drift detection method, we follow a two step process. Our first step involves encoding a sample of production data as the target distribution, and the model training data as the reference distribution. In the second step, we employ a kernel-based statistical test that utilizes the maximum mean discrepancy (MMD) distance metric to compare the reference and target distributions and estimate any potential drift. Our method also identifies the subset of production
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ArtHDR-Net&#30340;&#26550;&#26500;&#65292;&#23427;&#21487;&#20197;&#20934;&#30830;&#22320;&#21019;&#24314;&#24863;&#30693;&#36924;&#30495;&#30340;HDR&#20869;&#23481;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#27880;&#37325;&#37325;&#24314;&#32467;&#26524;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#20687;&#32032;&#31934;&#30830;&#24615;&#65292;&#36824;&#24378;&#35843;&#20445;&#30041;&#22270;&#20687;&#30340;&#33402;&#26415;&#24847;&#22270;&#65292;&#20197;&#31526;&#21512;&#20154;&#31867;&#30340;&#35270;&#35273;&#24863;&#30693;&#12290;&#23454;&#39564;&#35777;&#26126;ArtHDR-Net&#22312;HDR&#20869;&#23481;&#21019;&#24314;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03827</link><description>&lt;p&gt;
ArtHDR-Net: &#24863;&#30693;&#36924;&#30495;&#21644;&#20934;&#30830;&#30340;HDR&#20869;&#23481;&#21019;&#24314;
&lt;/p&gt;
&lt;p&gt;
ArtHDR-Net: Perceptually Realistic and Accurate HDR Content Creation. (arXiv:2309.03827v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ArtHDR-Net&#30340;&#26550;&#26500;&#65292;&#23427;&#21487;&#20197;&#20934;&#30830;&#22320;&#21019;&#24314;&#24863;&#30693;&#36924;&#30495;&#30340;HDR&#20869;&#23481;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#27880;&#37325;&#37325;&#24314;&#32467;&#26524;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#20687;&#32032;&#31934;&#30830;&#24615;&#65292;&#36824;&#24378;&#35843;&#20445;&#30041;&#22270;&#20687;&#30340;&#33402;&#26415;&#24847;&#22270;&#65292;&#20197;&#31526;&#21512;&#20154;&#31867;&#30340;&#35270;&#35273;&#24863;&#30693;&#12290;&#23454;&#39564;&#35777;&#26126;ArtHDR-Net&#22312;HDR&#20869;&#23481;&#21019;&#24314;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#21160;&#24577;&#33539;&#22260;&#65288;HDR&#65289;&#20869;&#23481;&#21019;&#24314;&#24050;&#25104;&#20026;&#29616;&#20195;&#23186;&#20307;&#21644;&#23089;&#20048;&#34892;&#19994;&#12289;&#28216;&#25103;&#21644;&#22686;&#24378;/&#34394;&#25311;&#29616;&#23454;&#34892;&#19994;&#30340;&#37325;&#35201;&#35758;&#39064;&#12290;&#35768;&#22810;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#65292;&#29992;&#20110;&#37325;&#24314;&#36755;&#20837;&#20302;&#21160;&#24577;&#33539;&#22260;&#65288;LDR&#65289;&#22270;&#20687;/&#35270;&#39057;&#30340;HDR&#23545;&#24212;&#29289;&#65292;&#32473;&#23450;&#21333;&#20010;&#26333;&#20809;&#25110;&#22810;&#26333;&#20809;LDR&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#37325;&#24314;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#20687;&#32032;&#31934;&#30830;&#24615;&#30340;&#20445;&#25345;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20256;&#32479;&#26041;&#27861;&#24182;&#19981;&#24378;&#35843;&#22312;&#23186;&#20307;&#12289;&#23089;&#20048;&#21644;&#28216;&#25103;&#26041;&#38754;&#20445;&#25345;&#22270;&#20687;&#30340;&#33402;&#26415;&#24847;&#22270;&#65292;&#36825;&#26159;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20803;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#30740;&#31350;&#21644;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;ArtHDR-Net&#65292;&#23427;&#20351;&#29992;&#22810;&#26333;&#20809;LDR&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ArtHDR-Net&#22312;HDR-VDP-2&#24471;&#20998;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65288;&#21363;&#24179;&#22343;&#24847;&#35265;&#24471;&#20998;indec&#65289;
&lt;/p&gt;
&lt;p&gt;
High Dynamic Range (HDR) content creation has become an important topic for modern media and entertainment sectors, gaming and Augmented/Virtual Reality industries. Many methods have been proposed to recreate the HDR counterparts of input Low Dynamic Range (LDR) images/videos given a single exposure or multi-exposure LDRs. The state-of-the-art methods focus primarily on the preservation of the reconstruction's structural similarity and the pixel-wise accuracy. However, these conventional approaches do not emphasize preserving the artistic intent of the images in terms of human visual perception, which is an essential element in media, entertainment and gaming. In this paper, we attempt to study and fill this gap. We propose an architecture called ArtHDR-Net based on a Convolutional Neural Network that uses multi-exposed LDR features as input. Experimental results show that ArtHDR-Net can achieve state-of-the-art performance in terms of the HDR-VDP-2 score (i.e., mean opinion score inde
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;Prime&#21644;Modulate&#26041;&#27861;&#65292;&#36890;&#36807;&#24102;&#31526;&#21495;&#30340;&#21453;&#21521;&#20256;&#25773;&#21644;&#20840;&#23616;&#30456;&#20851;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#21069;&#21521;&#27169;&#22411;&#30340;&#38381;&#29615;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#29190;&#28856;&#21644;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03825</link><description>&lt;p&gt;
Prime&#21644;Modulate&#23398;&#20064;&#65306;&#36890;&#36807;&#24102;&#31526;&#21495;&#21453;&#21521;&#20256;&#25773;&#21644;&#29615;&#22659;&#32447;&#32034;&#29983;&#25104;&#21069;&#21521;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prime and Modulate Learning: Generation of forward models with signed back-propagation and environmental cues. (arXiv:2309.03825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03825
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;Prime&#21644;Modulate&#26041;&#27861;&#65292;&#36890;&#36807;&#24102;&#31526;&#21495;&#30340;&#21453;&#21521;&#20256;&#25773;&#21644;&#20840;&#23616;&#30456;&#20851;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#21069;&#21521;&#27169;&#22411;&#30340;&#38381;&#29615;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#29190;&#28856;&#21644;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#21487;&#33021;&#38754;&#20020;&#26799;&#24230;&#29190;&#28856;&#21644;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20363;&#22914;&#35268;&#33539;&#21270;&#25216;&#26415;&#25110;&#23558;&#28608;&#27963;&#20989;&#25968;&#38480;&#21046;&#20026;&#32447;&#24615;&#25972;&#27969;&#21333;&#20803;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#21069;&#21521;&#27169;&#22411;&#30340;&#38381;&#29615;&#23398;&#20064;&#65292;&#20854;&#20013;&#21453;&#21521;&#20256;&#25773;&#20165;&#20351;&#29992;&#35823;&#24046;&#20449;&#21495;&#30340;&#31526;&#21495;&#26469;&#24341;&#23548;&#23398;&#20064;&#65292;&#32780;&#20840;&#23616;&#30456;&#20851;&#20449;&#21495;&#35843;&#33410;&#23398;&#20064;&#36895;&#29575;&#12290;&#36825;&#21463;&#21040;&#23616;&#37096;&#21487;&#22609;&#24615;&#19982;&#20840;&#23616;&#31070;&#32463;&#35843;&#33410;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#21551;&#21457;&#12290;&#20363;&#22914;&#65292;&#22312;&#31354;&#26103;&#30340;&#36947;&#36335;&#19978;&#34892;&#39542;&#26102;&#65292;&#21487;&#20197;&#20801;&#35768;&#23545;&#34892;&#21160;&#36827;&#34892;&#32531;&#24930;&#36880;&#27493;&#30340;&#20248;&#21270;&#65292;&#32780;&#22312;&#32321;&#24537;&#30340;&#21313;&#23383;&#36335;&#21475;&#65292;&#24517;&#39035;&#31435;&#21363;&#32416;&#27491;&#38169;&#35823;&#12290;&#22240;&#27492;&#65292;&#38169;&#35823;&#23601;&#26159;&#24341;&#23548;&#20449;&#21495;&#65292;&#32463;&#21382;&#30340;&#24378;&#24230;&#26159;&#26435;&#37325;&#21464;&#21270;&#30340;&#35843;&#33410;&#22240;&#32032;&#12290;&#36825;&#31181;Prime&#21644;Modulate&#33539;&#24335;&#30340;&#20248;&#28857;&#26159;&#21452;&#37325;&#30340;&#65306;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks employing error back-propagation for learning can suffer from exploding and vanishing gradient problems. Numerous solutions have been proposed such as normalisation techniques or limiting activation functions to linear rectifying units. In this work we follow a different approach which is particularly applicable to closed-loop learning of forward models where back-propagation makes exclusive use of the sign of the error signal to prime the learning, whilst a global relevance signal modulates the rate of learning. This is inspired by the interaction between local plasticity and a global neuromodulation. For example, whilst driving on an empty road, one can allow for slow step-wise optimisation of actions, whereas, at a busy junction, an error must be corrected at once. Hence, the error is the priming signal and the intensity of the experience is a modulating factor in the weight change. The advantages of this Prime and Modulate paradigm is twofold: it is free from n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#21152;&#36895;&#20302;&#31209;&#20998;&#35299;&#27169;&#22411;&#30340;&#25216;&#26415;&#65306;&#31209;&#20248;&#21270;&#21644;&#39034;&#24207;&#20923;&#32467;&#20998;&#35299;&#23618;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#39640;&#36798;60%&#65292;&#25512;&#29702;&#21534;&#21520;&#37327;&#39640;&#36798;37%&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#25509;&#36817;&#21407;&#22987;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.03824</link><description>&lt;p&gt;
&#20302;&#31209;&#20998;&#35299;&#32593;&#32476;&#30340;&#35757;&#32451;&#21152;&#36895;&#65306;&#39034;&#24207;&#20923;&#32467;&#21644;&#31209;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization. (arXiv:2309.03824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#21152;&#36895;&#20302;&#31209;&#20998;&#35299;&#27169;&#22411;&#30340;&#25216;&#26415;&#65306;&#31209;&#20248;&#21270;&#21644;&#39034;&#24207;&#20923;&#32467;&#20998;&#35299;&#23618;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#39640;&#36798;60%&#65292;&#25512;&#29702;&#21534;&#21520;&#37327;&#39640;&#36798;37%&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#25509;&#36817;&#21407;&#22987;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#20998;&#35299;&#65288;LRD&#65289;&#26159;&#19968;&#31181;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26435;&#37325;&#24352;&#37327;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#24212;&#29992;LRD&#21518;&#22312;&#26550;&#26500;&#20013;&#28155;&#21152;&#20102;&#22823;&#37327;&#26032;&#23618;&#65292;&#22914;&#26524;&#20998;&#35299;&#31209;&#19981;&#22815;&#23567;&#65292;&#21017;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;/&#25512;&#29702;&#21152;&#36895;&#24615;&#19981;&#39640;&#12290;&#38382;&#39064;&#22312;&#20110;&#65292;&#20351;&#29992;&#36739;&#23567;&#30340;&#31209;&#20250;&#22686;&#21152;&#20998;&#35299;&#21518;&#30340;&#26174;&#33879;&#20934;&#30830;&#29575;&#19979;&#38477;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21152;&#36895;&#20302;&#31209;&#20998;&#35299;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#36739;&#23567;&#30340;&#31209;&#36827;&#34892;&#20998;&#35299;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#31209;&#20248;&#21270;&#21644;&#39034;&#24207;&#20923;&#32467;&#20998;&#35299;&#23618;&#12290;&#25105;&#20204;&#22312;&#21367;&#31215;&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#20445;&#25345;&#25509;&#36817;&#21407;&#22987;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#21534;&#21520;&#37327;&#39640;&#36798;60%&#65292;&#25512;&#29702;&#21534;&#21520;&#37327;&#39640;&#36798;37%&#12290;
&lt;/p&gt;
&lt;p&gt;
Low Rank Decomposition (LRD) is a model compression technique applied to the weight tensors of deep learning models in order to reduce the number of trainable parameters and computational complexity. However, due to high number of new layers added to the architecture after applying LRD, it may not lead to a high training/inference acceleration if the decomposition ranks are not small enough. The issue is that using small ranks increases the risk of significant accuracy drop after decomposition. In this paper, we propose two techniques for accelerating low rank decomposed models without requiring to use small ranks for decomposition. These methods include rank optimization and sequential freezing of decomposed layers. We perform experiments on both convolutional and transformer-based models. Experiments show that these techniques can improve the model throughput up to 60% during training and 37% during inference when combined together while preserving the accuracy close to that of the o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#23614;&#20998;&#24067;&#26465;&#20214;&#19979;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#39118;&#38505;&#20540;&#36873;&#25321;&#20248;&#21270;&#22120;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#36890;&#29992;&#38142;&#26041;&#27861;&#24314;&#31435;&#20102;&#36807;&#24230;&#39118;&#38505;&#30340;&#19978;&#30028;&#12290;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;Catoni&#39118;&#26684;&#20272;&#35745;&#30340;&#32463;&#39564;&#39118;&#38505;&#20248;&#21270;&#22120;&#27604;&#20854;&#20182;&#22522;&#20934;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.03818</link><description>&lt;p&gt;
&#26080;&#26041;&#24046;&#25439;&#22833;&#19979;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Empirical Risk Minimization for Losses without Variance. (arXiv:2309.03818v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#23614;&#20998;&#24067;&#26465;&#20214;&#19979;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#39118;&#38505;&#20540;&#36873;&#25321;&#20248;&#21270;&#22120;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#36890;&#29992;&#38142;&#26041;&#27861;&#24314;&#31435;&#20102;&#36807;&#24230;&#39118;&#38505;&#30340;&#19978;&#30028;&#12290;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;Catoni&#39118;&#26684;&#20272;&#35745;&#30340;&#32463;&#39564;&#39118;&#38505;&#20248;&#21270;&#22120;&#27604;&#20854;&#20182;&#22522;&#20934;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#37325;&#23614;&#20998;&#24067;&#26465;&#20214;&#19979;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#27809;&#26377;&#26377;&#38480;&#26041;&#24046;&#65292;&#20294;&#21482;&#26377;$p$&#38454;&#30697;&#65292;&#20854;&#20013;$p \in (1,2)$&#12290;&#25105;&#20204;&#36873;&#25321;&#36890;&#36807;&#26368;&#23567;&#21270;&#39118;&#38505;&#20540;&#26469;&#36873;&#25321;&#20248;&#21270;&#22120;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#22522;&#20110;&#25130;&#26029;&#35266;&#27979;&#25968;&#25454;&#30340;&#20272;&#35745;&#36807;&#31243;&#12290;&#36825;&#20123;&#39118;&#38505;&#20540;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;Catoni&#30340;&#26174;&#33879;&#26041;&#27861;&#65288;Catoni, 2012&#65289;&#36827;&#34892;&#31283;&#20581;&#20272;&#35745;&#26469;&#24471;&#21040;&#12290;&#30001;&#20110;Catoni&#22411;&#24433;&#21709;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#30340;&#36890;&#29992;&#38142;&#26041;&#27861;&#24314;&#31435;&#36807;&#24230;&#39118;&#38505;&#30340;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#35745;&#31639;&#38382;&#39064;&#12290;&#25105;&#20204;&#29305;&#21035;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#31283;&#20581;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#21644;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#20540;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;Catoni&#39118;&#26684;&#20272;&#35745;&#30340;&#32463;&#39564;&#39118;&#38505;&#20248;&#21270;&#22120;&#30830;&#23454;&#27604;&#20854;&#20182;&#22522;&#20934;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#34920;&#26126;&#30452;&#25509;&#22522;&#20110;&#25130;&#26029;&#25968;&#25454;&#30340;&#20272;&#35745;&#21487;&#33021;&#20250;&#23548;&#33268;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers an empirical risk minimization problem under heavy-tailed settings, where data does not have finite variance, but only has $p$-th moment with $p \in (1,2)$. Instead of using estimation procedure based on truncated observed data, we choose the optimizer by minimizing the risk value. Those risk values can be robustly estimated via using the remarkable Catoni's method (Catoni, 2012). Thanks to the structure of Catoni-type influence functions, we are able to establish excess risk upper bounds via using generalized generic chaining methods. Moreover, we take computational issues into consideration. We especially theoretically investigate two types of optimization methods, robust gradient descent algorithm and empirical risk-based methods. With an extensive numerical study, we find that the optimizer based on empirical risks via Catoni-style estimation indeed shows better performance than other baselines. It indicates that estimation directly based on truncated data may 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#20307;&#27604;&#20363;&#27979;&#37327;&#26500;&#24314;&#30340;AnthroNet&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21508;&#31181;&#19981;&#21516;&#24418;&#29366;&#21644;&#23039;&#21183;&#30340;&#20154;&#20307;&#65292;&#24182;&#20197;&#20219;&#24847;&#23039;&#21183;&#29983;&#25104;&#29305;&#23450;&#20154;&#29289;&#36523;&#20221;&#30340;&#20154;&#20307;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#25552;&#20379;&#20102;&#39640;&#31934;&#24230;&#30340;&#20154;&#20307;&#32593;&#26684;&#34920;&#31034;&#21644;&#31934;&#30830;&#30340;&#20154;&#20307;&#27604;&#20363;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.03812</link><description>&lt;p&gt;
AnthroNet: &#36890;&#36807;&#20154;&#20307;&#27604;&#20363;&#29983;&#25104;&#26465;&#20214;&#21270;&#30340;&#20154;&#20307;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnthroNet: Conditional Generation of Humans via Anthropometrics. (arXiv:2309.03812v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03812
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#20307;&#27604;&#20363;&#27979;&#37327;&#26500;&#24314;&#30340;AnthroNet&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21508;&#31181;&#19981;&#21516;&#24418;&#29366;&#21644;&#23039;&#21183;&#30340;&#20154;&#20307;&#65292;&#24182;&#20197;&#20219;&#24847;&#23039;&#21183;&#29983;&#25104;&#29305;&#23450;&#20154;&#29289;&#36523;&#20221;&#30340;&#20154;&#20307;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#25552;&#20379;&#20102;&#39640;&#31934;&#24230;&#30340;&#20154;&#20307;&#32593;&#26684;&#34920;&#31034;&#21644;&#31934;&#30830;&#30340;&#20154;&#20307;&#27604;&#20363;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#27867;&#30340;&#20154;&#20307;&#27604;&#20363;&#27979;&#37327;&#32780;&#26500;&#24314;&#30340;&#26032;&#39062;&#20154;&#20307;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#21508;&#31181;&#19981;&#21516;&#24418;&#29366;&#21644;&#23039;&#21183;&#30340;&#20154;&#20307;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#23545;&#29305;&#23450;&#20154;&#29289;&#36523;&#20221;&#30340;&#30452;&#25509;&#24314;&#27169;&#65292;&#24182;&#33021;&#22815;&#20197;&#20219;&#24847;&#23039;&#21183;&#29983;&#25104;&#20154;&#20307;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#36890;&#36807;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#19981;&#20165;&#25552;&#20379;&#20102;&#39640;&#31934;&#24230;&#30340;&#20154;&#20307;&#32593;&#26684;&#34920;&#31034;&#65292;&#36824;&#20801;&#35768;&#23545;&#20154;&#20307;&#36827;&#34892;&#31934;&#30830;&#30340;&#20154;&#20307;&#27604;&#20363;&#27979;&#37327;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#21160;&#30011;&#24211;&#65292;&#25105;&#20204;&#20026;&#21512;&#25104;&#20154;&#20307;&#30340;&#36523;&#20307;&#21644;&#25163;&#37096;&#36827;&#34892;&#20102;&#20851;&#33410;&#22788;&#29702;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#20013;&#21487;&#23398;&#20064;&#20808;&#39564;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19968;&#20010;&#21253;&#21547;10&#19975;&#20010;&#31243;&#24207;&#29983;&#25104;&#30340;&#20154;&#20307;&#32593;&#26684;&#21644;&#30456;&#24212;&#20154;&#20307;&#27604;&#20363;&#27979;&#37327;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38750;&#21830;&#19994;&#23398;&#26415;&#29992;&#36884;&#19979;&#30340;&#25968;&#30334;&#19975;&#20010;&#29420;&#29305;&#20154;&#29289;&#36523;&#20221;&#21644;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel human body model formulated by an extensive set of anthropocentric measurements, which is capable of generating a wide range of human body shapes and poses. The proposed model enables direct modeling of specific human identities through a deep generative architecture, which can produce humans in any arbitrary pose. It is the first of its kind to have been trained end-to-end using only synthetically generated data, which not only provides highly accurate human mesh representations but also allows for precise anthropometry of the body. Moreover, using a highly diverse animation library, we articulated our synthetic humans' body and hands to maximize the diversity of the learnable priors for model training. Our model was trained on a dataset of $100k$ procedurally-generated posed human meshes and their corresponding anthropometric measurements. Our synthetic data generator can be used to generate millions of unique human identities and poses for non-commercial academic 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#35889;&#26041;&#27861;&#25913;&#36827;&#20102;&#25490;&#21517;&#32858;&#21512;&#38382;&#39064;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#26410;&#24402;&#19968;&#21270;&#21644;&#24402;&#19968;&#21270;&#25968;&#25454;&#30697;&#38453;&#30340;&#35889;&#25490;&#21517;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25200;&#21160;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.03808</link><description>&lt;p&gt;
&#36890;&#36807;&#35889;&#26041;&#27861;&#25913;&#36827;&#20102;&#25490;&#21517;&#32858;&#21512;&#30340;&#29702;&#35770;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Improved theoretical guarantee for rank aggregation via spectral method. (arXiv:2309.03808v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#35889;&#26041;&#27861;&#25913;&#36827;&#20102;&#25490;&#21517;&#32858;&#21512;&#38382;&#39064;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#26410;&#24402;&#19968;&#21270;&#21644;&#24402;&#19968;&#21270;&#25968;&#25454;&#30697;&#38453;&#30340;&#35889;&#25490;&#21517;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25200;&#21160;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#22810;&#20010;&#39033;&#30446;&#20043;&#38388;&#30340;&#25104;&#23545;&#27604;&#36739;&#65292;&#22914;&#20309;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#21517;&#20197;&#20351;&#24471;&#25490;&#21517;&#19982;&#35266;&#23519;&#30456;&#21305;&#37197;&#65311;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#25490;&#21517;&#32858;&#21512;&#65292;&#22312;&#20307;&#32946;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#20854;&#20182;&#32593;&#32476;&#24212;&#29992;&#20013;&#24050;&#32463;&#25214;&#21040;&#20102;&#35768;&#22810;&#24212;&#29992;&#12290;&#30001;&#20110;&#25214;&#21040;&#26368;&#23567;&#21270;&#19981;&#21305;&#37197;&#30340;&#20840;&#23616;&#25490;&#21517;&#36890;&#24120;&#26159;NP&#22256;&#38590;&#30340;&#65288;&#31216;&#20026;Kemeny&#20248;&#21270;&#65289;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;Erd\"os-R\'enyi&#31163;&#32676;&#28857;&#65288;ERO&#65289;&#27169;&#22411;&#19978;&#12290;&#22312;&#36825;&#20010;&#25490;&#21517;&#38382;&#39064;&#20013;&#65292;&#27599;&#20010;&#25104;&#23545;&#27604;&#36739;&#26159;&#30495;&#23454;&#20998;&#25968;&#24046;&#24322;&#30340;&#34987;&#25439;&#22351;&#21103;&#26412;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#26410;&#24402;&#19968;&#21270;&#21644;&#24402;&#19968;&#21270;&#25968;&#25454;&#30697;&#38453;&#30340;&#35889;&#25490;&#21517;&#31639;&#27861;&#12290;&#20851;&#38190;&#26159;&#29702;&#35299;&#23427;&#20204;&#22312;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#24674;&#22797;&#27599;&#20010;&#39033;&#30446;&#30340;&#28508;&#22312;&#20998;&#25968;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36825;&#24402;&#32467;&#20026;&#25512;&#23548;&#26410;&#24402;&#19968;&#21270;/&#24402;&#19968;&#21270;&#25968;&#25454;&#30697;&#38453;&#30340;&#21069;&#20960;&#20010;&#29305;&#24449;&#21521;&#37327;&#21644;&#20854;&#24635;&#20307;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#36880;&#20010;&#39033;&#25200;&#21160;&#35823;&#24046;&#30028;&#38480;&#12290;&#36890;&#36807;&#20351;&#29992;&#30041;&#20986;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#20934;&#30830;&#30340;$\ell_{\infty}$-norm&#25200;&#21160;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given pairwise comparisons between multiple items, how to rank them so that the ranking matches the observations? This problem, known as rank aggregation, has found many applications in sports, recommendation systems, and other web applications. As it is generally NP-hard to find a global ranking that minimizes the mismatch (known as the Kemeny optimization), we focus on the Erd\"os-R\'enyi outliers (ERO) model for this ranking problem. Here, each pairwise comparison is a corrupted copy of the true score difference. We investigate spectral ranking algorithms that are based on unnormalized and normalized data matrices. The key is to understand their performance in recovering the underlying scores of each item from the observed data. This reduces to deriving an entry-wise perturbation error bound between the top eigenvectors of the unnormalized/normalized data matrix and its population counterpart. By using the leave-one-out technique, we provide a sharper $\ell_{\infty}$-norm perturbati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#20013;&#30340;&#24494;&#22937;&#36873;&#25321;&#65292;&#29305;&#21035;&#20851;&#27880;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#21457;&#29616;&#31232;&#30095;&#21021;&#22987;&#21270;&#21644;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19988;&#21512;&#25104;&#31232;&#30095;&#22855;&#20598;&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#30495;&#23454;&#38382;&#39064;&#30340;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.03800</link><description>&lt;p&gt;
&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65306;&#25968;&#25454;&#12289;&#35745;&#31639;&#12289;&#23485;&#24230;&#21644;&#36816;&#27668;
&lt;/p&gt;
&lt;p&gt;
Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck. (arXiv:2309.03800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#20013;&#30340;&#24494;&#22937;&#36873;&#25321;&#65292;&#29305;&#21035;&#20851;&#27880;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#21457;&#29616;&#31232;&#30095;&#21021;&#22987;&#21270;&#21644;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19988;&#21512;&#25104;&#31232;&#30095;&#22855;&#20598;&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#30495;&#23454;&#38382;&#39064;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#20013;&#24494;&#22937;&#30340;&#31639;&#27861;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;&#31163;&#32447;&#31232;&#30095;&#22855;&#20598;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#20851;&#22810;&#23618;&#24863;&#30693;&#22120;&#26799;&#24230;&#35757;&#32451;&#30340;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20855;&#26377;&#32479;&#35745;&#26597;&#35810;&#19979;&#30028;&#12290;&#36825;&#20010;&#19979;&#30028;&#21487;&#20197;&#35299;&#37322;&#20026;&#22810;&#36164;&#28304;&#30340;&#26435;&#34913;&#21069;&#27839;&#65306;&#25104;&#21151;&#23398;&#20064;&#21482;&#26377;&#22312;&#19968;&#20010;&#36275;&#22815;&#20016;&#23500;&#65288;&#22823;&#22411;&#27169;&#22411;&#65289;&#12289;&#30693;&#35782;&#28170;&#21338;&#65288;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65289;&#12289;&#32784;&#24515;&#65288;&#35757;&#32451;&#36845;&#20195;&#27425;&#25968;&#22810;&#65289;&#25110;&#24184;&#36816;&#65288;&#38543;&#26426;&#29468;&#27979;&#27425;&#25968;&#22810;&#65289;&#30340;&#24773;&#20917;&#19979;&#25165;&#33021;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31232;&#30095;&#21021;&#22987;&#21270;&#21644;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;&#36825;&#37324;&#65292;&#23485;&#24230;&#36215;&#21040;&#20102;&#24182;&#34892;&#25628;&#32034;&#30340;&#20316;&#29992;&#65306;&#23427;&#22686;&#21152;&#20102;&#25214;&#21040;&#8220;&#24184;&#36816;&#31070;&#32463;&#20803;&#8221;&#30340;&#27010;&#29575;&#65292;&#36825;&#20123;&#31070;&#32463;&#20803;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#31232;&#30095;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#21512;&#25104;&#31232;&#30095;&#22855;&#20598;&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#30495;&#23454;&#38382;&#39064;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the nuanced algorithm design choices for deep learning in the presence of computational-statistical gaps. We begin by considering offline sparse parity learning, a supervised classification problem which admits a statistical query lower bound for gradient-based training of a multilayer perceptron. This lower bound can be interpreted as a multi-resource tradeoff frontier: successful learning can only occur if one is sufficiently rich (large model), knowledgeable (large dataset), patient (many training iterations), or lucky (many random guesses). We show, theoretically and experimentally, that sparse initialization and increasing network width yield significant improvements in sample efficiency in this setting. Here, width plays the role of parallel search: it amplifies the probability of finding "lottery ticket" neurons, which learn sparse features more sample-efficiently. Finally, we show that the synthetic sparse parity task can be useful as a proxy for real pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25193;&#23637;&#31639;&#27861;&#65292;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#29702;&#35770;&#35206;&#30422;&#20445;&#35777;&#30340;&#24207;&#21015;&#38598;&#21512;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#22823;&#23567;&#30340;&#23376;&#38598;&#25552;&#20379;&#32467;&#26524;&#65292;&#20294;&#21487;&#36798;&#21040;&#30340;&#20445;&#35777;&#21463;&#21040;&#26657;&#20934;&#24230;&#37327;&#30340;&#19978;&#30028;&#38480;&#21046;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#23558;&#31526;&#21512;&#24615;&#38598;&#39044;&#27979;&#36807;&#31243;&#19982;&#35299;&#30721;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26681;&#25454;&#24403;&#21069;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#35843;&#25972;&#30340;&#21487;&#21464;&#27874;&#26463;&#23485;&#24230;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#21270;&#23398;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.03797</link><description>&lt;p&gt;
&#24322;&#24120;&#33258;&#22238;&#24402;&#29983;&#25104;&#65306;&#24102;&#35206;&#30422;&#20445;&#35777;&#30340;&#27874;&#26463;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Conformal Autoregressive Generation: Beam Search with Coverage Guarantees. (arXiv:2309.03797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25193;&#23637;&#31639;&#27861;&#65292;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#29702;&#35770;&#35206;&#30422;&#20445;&#35777;&#30340;&#24207;&#21015;&#38598;&#21512;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#22823;&#23567;&#30340;&#23376;&#38598;&#25552;&#20379;&#32467;&#26524;&#65292;&#20294;&#21487;&#36798;&#21040;&#30340;&#20445;&#35777;&#21463;&#21040;&#26657;&#20934;&#24230;&#37327;&#30340;&#19978;&#30028;&#38480;&#21046;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#23558;&#31526;&#21512;&#24615;&#38598;&#39044;&#27979;&#36807;&#31243;&#19982;&#35299;&#30721;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26681;&#25454;&#24403;&#21069;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#35843;&#25972;&#30340;&#21487;&#21464;&#27874;&#26463;&#23485;&#24230;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#21270;&#23398;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#30340;&#20004;&#20010;&#26032;&#25193;&#23637;&#65292;&#29992;&#20110;&#20135;&#29983;&#20855;&#26377;&#29702;&#35770;&#35206;&#30422;&#20445;&#35777;&#30340;&#24207;&#21015;&#38598;&#21512;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#38750;&#24120;&#31616;&#21333;&#65292;&#25552;&#20986;&#20102;&#21160;&#24577;&#22823;&#23567;&#30340;&#27874;&#26463;&#25628;&#32034;&#32467;&#26524;&#23376;&#38598;&#65292;&#20294;&#19982;&#20856;&#22411;&#30340;&#31526;&#21512;&#24615;&#31243;&#24207;&#19981;&#21516;&#65292;&#23427;&#22312;&#21487;&#36798;&#21040;&#30340;&#20445;&#35777;&#19978;&#26377;&#19968;&#20010;&#19978;&#30028;&#65292;&#21462;&#20915;&#20110;&#20107;&#21518;&#26657;&#20934;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#31181;&#31639;&#27861;&#23558;&#31526;&#21512;&#24615;&#38598;&#39044;&#27979;&#36807;&#31243;&#20316;&#20026;&#35299;&#30721;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#20135;&#29983;&#19968;&#20010;&#33021;&#22815;&#26681;&#25454;&#24403;&#21069;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#35843;&#25972;&#30340;&#21487;&#21464;&#27874;&#26463;&#23485;&#24230;&#12290;&#34429;&#28982;&#26356;&#22797;&#26434;&#65292;&#20294;&#35813;&#36807;&#31243;&#21487;&#20197;&#23454;&#29616;&#39044;&#20808;&#36873;&#23450;&#30340;&#35206;&#30422;&#20445;&#35777;&#12290;&#25105;&#20204;&#20026;&#27599;&#31181;&#26041;&#27861;&#25552;&#20379;&#36793;&#32536;&#35206;&#30422;&#36793;&#30028;&#65292;&#24182;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#21270;&#23398;&#20013;&#36873;&#25321;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce two new extensions to the beam search algorithm based on conformal predictions (CP) to produce sets of sequences with theoretical coverage guarantees. The first method is very simple and proposes dynamically-sized subsets of beam search results but, unlike typical CP procedures, has an upper bound on the achievable guarantee depending on a post-hoc calibration measure. Our second algorithm introduces the conformal set prediction procedure as part of the decoding process, producing a variable beam width which adapts to the current uncertainty. While more complex, this procedure can achieve coverage guarantees selected a priori. We provide marginal coverage bounds for each method, and evaluate them empirically on a selection of tasks drawing from natural language processing and chemistry.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;ARMOR_D&#26469;&#21152;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#27491;&#21017;&#21270;&#24046;&#24322;&#65292;&#36890;&#36807;&#22312;&#20998;&#24067;&#30340;&#37051;&#22495;&#19978;&#36827;&#34892;&#26368;&#22823;&#21270;&#26399;&#26395;&#25439;&#22833;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ARMOR_D&#26041;&#27861;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#22270;&#20687;&#35782;&#21035;&#24212;&#29992;&#20013;&#33021;&#22815;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.03791</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#27491;&#21017;&#21270;&#24046;&#24322;&#26469;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences. (arXiv:2309.03791v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;ARMOR_D&#26469;&#21152;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#27491;&#21017;&#21270;&#24046;&#24322;&#65292;&#36890;&#36807;&#22312;&#20998;&#24067;&#30340;&#37051;&#22495;&#19978;&#36827;&#34892;&#26368;&#22823;&#21270;&#26399;&#26395;&#25439;&#22833;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ARMOR_D&#26041;&#27861;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#22270;&#20687;&#35782;&#21035;&#24212;&#29992;&#20013;&#33021;&#22815;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;ARMOR_D&#26041;&#27861;&#20316;&#20026;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#19968;&#31181;&#26032;&#30340;&#26368;&#20248;&#20256;&#36755;&#27491;&#21017;&#21270;&#24046;&#24322;&#31867;&#65292;&#36890;&#36807;&#20449;&#24687;&#24046;&#24322;&#21644;&#26368;&#20248;&#20256;&#36755;&#25104;&#26412;&#20043;&#38388;&#30340;infimal&#21367;&#31215;&#26500;&#24314;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#22312;&#20998;&#24067;&#30340;&#37051;&#22495;&#19978;&#26368;&#22823;&#21270;&#26399;&#26395;&#25439;&#22833;&#65292;&#36825;&#34987;&#31216;&#20026;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#12290;&#20316;&#20026;&#26500;&#24314;&#23545;&#25239;&#26679;&#26412;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#26679;&#26412;&#26681;&#25454;&#26368;&#20248;&#20256;&#36755;&#25104;&#26412;&#36827;&#34892;&#20256;&#36755;&#65292;&#24182;&#26681;&#25454;&#20449;&#24687;&#24046;&#24322;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#12290;&#25105;&#20204;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#22270;&#20687;&#35782;&#21035;&#24212;&#29992;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#22686;&#24378;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#26041;&#38754;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;ARMOR_D&#22312;FGSM&#25915;&#20987;&#19979;&#30340;robustified&#20934;&#30830;&#29575;&#36798;&#21040;98.29%&#65292;&#22312;&#20854;&#20182;&#25915;&#20987;&#19979;&#36798;&#21040;98.18%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the $ARMOR_D$ methods as novel approaches to enhancing the adversarial robustness of deep learning models. These methods are based on a new class of optimal-transport-regularized divergences, constructed via an infimal convolution between an information divergence and an optimal-transport (OT) cost. We use these as tools to enhance adversarial robustness by maximizing the expected loss over a neighborhood of distributions, a technique known as distributionally robust optimization. Viewed as a tool for constructing adversarial samples, our method allows samples to be both transported, according to the OT cost, and re-weighted, according to the information divergence. We demonstrate the effectiveness of our method on malware detection and image recognition applications and find that, to our knowledge, it outperforms existing methods at enhancing the robustness against adversarial attacks. $ARMOR_D$ yields the robustified accuracy of $98.29\%$ against $FGSM$ and $98.18\%$ aga
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32418;&#22806;&#25506;&#27979;&#22120;&#27169;&#22411;&#21644;&#31890;&#23376;&#30896;&#25758;&#20107;&#20214;&#27169;&#25311;&#22120;&#26469;&#29983;&#25104;&#31616;&#21270;&#25968;&#25454;&#65292;&#20197;&#20415;&#20110;&#39640;&#33021;&#29289;&#29702;&#30740;&#31350;&#21644;&#25945;&#32946;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#21644;&#35299;&#20915;&#26041;&#26696;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.03780</link><description>&lt;p&gt;
&#39640;&#33021;&#29289;&#29702;&#20013;&#30340;&#31616;&#21270;&#27169;&#25311;&#65306;&#25968;&#25454;&#39537;&#21160;&#29289;&#29702;&#30740;&#31350;&#30340;&#25240;&#20013;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reduced Simulations for High-Energy Physics, a Middle Ground for Data-Driven Physics Research. (arXiv:2309.03780v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03780
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32418;&#22806;&#25506;&#27979;&#22120;&#27169;&#22411;&#21644;&#31890;&#23376;&#30896;&#25758;&#20107;&#20214;&#27169;&#25311;&#22120;&#26469;&#29983;&#25104;&#31616;&#21270;&#25968;&#25454;&#65292;&#20197;&#20415;&#20110;&#39640;&#33021;&#29289;&#29702;&#30740;&#31350;&#21644;&#25945;&#32946;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#21644;&#35299;&#20915;&#26041;&#26696;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20122;&#21407;&#23376;&#31890;&#23376;&#36712;&#36857;&#37325;&#24314;&#65288;&#36861;&#36394;&#65289;&#26159;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#36861;&#36394;&#20219;&#21153;&#22312;&#35745;&#31639;&#19978;&#26497;&#20855;&#25361;&#25112;&#24615;&#65292;&#30446;&#21069;&#20351;&#29992;&#30340;&#20256;&#32479;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#19981;&#33021;&#32447;&#24615;&#25193;&#23637;&#12290;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#31616;&#21270;&#20102;&#30340;&#38382;&#39064;&#25551;&#36848;&#21644;&#25152;&#20195;&#34920;&#30340;&#25968;&#25454;&#23558;&#26377;&#21161;&#20110;&#35299;&#20915;&#26041;&#26696;&#30340;&#25506;&#32034;&#24037;&#20316;&#27969;&#31243;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32463;&#36807;&#31616;&#21270;&#30340;&#34394;&#25311;&#25506;&#27979;&#22120;&#65288;REDVID&#65289;&#20316;&#20026;&#22797;&#26434;&#24230;&#31616;&#21270;&#25506;&#27979;&#22120;&#27169;&#22411;&#21644;&#31890;&#23376;&#30896;&#25758;&#20107;&#20214;&#27169;&#25311;&#22120;&#30340;&#32452;&#21512;&#12290;REDVID&#26088;&#22312;&#20316;&#20026;&#19968;&#20010;&#27169;&#25311;-&#24490;&#29615;&#26469;&#39640;&#25928;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#31616;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;&#19982;&#29289;&#29702;&#31934;&#30830;&#27169;&#25311;&#30456;&#27604;&#65292;&#25105;&#20204;&#24037;&#20855;&#30340;&#23436;&#20840;&#21442;&#25968;&#21270;&#29305;&#24615;&#20801;&#35768;&#29983;&#25104;&#19981;&#21516;&#23618;&#27425;&#30340;&#30740;&#31350;&#21644;&#25945;&#32946;&#31616;&#21270;&#25968;&#25454;&#12290;&#30001;&#20110;&#31616;&#21270;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
Subatomic particle track reconstruction (tracking) is a vital task in High-Energy Physics experiments. Tracking is exceptionally computationally challenging and fielded solutions, relying on traditional algorithms, do not scale linearly. Machine Learning (ML) assisted solutions are a promising answer. We argue that a complexity-reduced problem description and the data representing it, will facilitate the solution exploration workflow. We provide the REDuced VIrtual Detector (REDVID) as a complexity-reduced detector model and particle collision event simulator combo. REDVID is intended as a simulation-in-the-loop, to both generate synthetic data efficiently and to simplify the challenge of ML model design. The fully parametric nature of our tool, with regards to system-level configuration, while in contrast to physics-accurate simulations, allows for the generation of simplified data for research and education, at different levels. Resulting from the reduced complexity, we showcase the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#26102;&#38388;&#32534;&#30721;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#24212;&#29992;&#30340;CPU&#39057;&#29575;&#35843;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#25512;&#23548;&#20986;&#39640;&#25928;&#30340;&#21151;&#29575;&#31649;&#29702;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;Linux&#20869;&#32622;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.03779</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#32534;&#30721;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#24212;&#29992;&#30340;CPU&#39057;&#29575;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
CPU frequency scheduling of real-time applications on embedded devices with temporal encoding-based deep reinforcement learning. (arXiv:2309.03779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#26102;&#38388;&#32534;&#30721;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#24212;&#29992;&#30340;CPU&#39057;&#29575;&#35843;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#25512;&#23548;&#20986;&#39640;&#25928;&#30340;&#21151;&#29575;&#31649;&#29702;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;Linux&#20869;&#32622;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#22411;&#35774;&#22791;&#32463;&#24120;&#29992;&#20110;&#29289;&#32852;&#32593;&#21644;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#65292;&#29992;&#20110;&#25191;&#34892;&#26377;&#36719;&#25130;&#27490;&#26399;&#30340;&#21608;&#26399;&#24615;&#19987;&#29992;&#20219;&#21153;&#12290;&#36825;&#39033;&#24037;&#20316;&#33268;&#21147;&#20110;&#24320;&#21457;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#25512;&#23548;&#20986;&#39640;&#25928;&#30340;&#21151;&#29575;&#31649;&#29702;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#29616;&#26377;&#30340;Linux&#20869;&#32622;&#26041;&#27861;&#22312;&#23567;&#22411;&#35774;&#22791;&#20013;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#31181;&#20856;&#22411;&#30340;&#24037;&#20316;&#36127;&#33655;/&#31995;&#32479;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#23545;&#20110;Linux&#20869;&#32622;&#35299;&#20915;&#26041;&#26696;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#26102;&#38388;&#32534;&#30721;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#20197;&#25512;&#23548;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;DVFS&#35843;&#24230;&#31243;&#24207;&#65292;&#21363;&#20351;&#23384;&#22312;&#36825;&#19977;&#31181;&#31995;&#32479;&#27169;&#24335;&#12290;&#25512;&#23548;&#20986;&#30340;&#35843;&#24230;&#31243;&#24207;&#20165;&#20351;&#29992;&#19968;&#20010;&#24615;&#33021;&#35745;&#25968;&#22120;&#65292;&#19982;&#20869;&#32622;&#30340;Linux&#26426;&#21046;&#30456;&#21516;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#24037;&#20316;&#36127;&#33655;&#30340;&#26174;&#24335;&#20219;&#21153;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Nvidia Jetson Nano Board&#19978;&#23454;&#29616;&#20102;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#65292;&#24182;&#36827;&#34892;&#20102;&#20845;&#31181;&#24212;&#29992;&#31243;&#24207;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#20004;&#20010;&#33258;&#35774;&#35745;&#30340;&#21644;&#22235;&#20010;&#22522;&#20934;&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Small devices are frequently used in IoT and smart-city applications to perform periodic dedicated tasks with soft deadlines. This work focuses on developing methods to derive efficient power-management methods for periodic tasks on small devices. We first study the limitations of the existing Linux built-in methods used in small devices. We illustrate three typical workload/system patterns that are challenging to manage with Linux's built-in solutions. We develop a reinforcement-learning-based technique with temporal encoding to derive an effective DVFS governor even with the presence of the three system patterns. The derived governor uses only one performance counter, the same as the built-in Linux mechanism, and does not require an explicit task model for the workload. We implemented a prototype system on the Nvidia Jetson Nano Board and experimented with it with six applications, including two self-designed and four benchmark applications. Under different deadline constraints, our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#32771;&#34385;&#20316;&#20026;&#32467;&#26500;&#20803;&#32032;&#65292;&#20197;&#31995;&#32479;&#32508;&#21512;&#30340;&#26041;&#24335;&#30830;&#20445;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#19968;&#27010;&#24565;&#19981;&#20165;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#26631;&#20934;&#30456;&#22865;&#21512;&#65292;&#36824;&#20026;AI&#23433;&#20840;&#30456;&#20851;&#30340;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#26631;&#20934;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03774</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#23433;&#20840;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Safety Concerns in Automated Driving Perception. (arXiv:2309.03774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#32771;&#34385;&#20316;&#20026;&#32467;&#26500;&#20803;&#32032;&#65292;&#20197;&#31995;&#32479;&#32508;&#21512;&#30340;&#26041;&#24335;&#30830;&#20445;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#19968;&#27010;&#24565;&#19981;&#20165;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#26631;&#20934;&#30456;&#22865;&#21512;&#65292;&#36824;&#20026;AI&#23433;&#20840;&#30456;&#20851;&#30340;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#26631;&#20934;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#24863;&#30693;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#23548;&#33268;&#20102;&#23545;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#24212;&#29992;&#30340;&#22686;&#21152;&#38656;&#27714;&#12290;&#36825;&#31867;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#38656;&#35201;&#32771;&#34385;DNNs&#30340;&#29420;&#29305;&#23646;&#24615;&#12290;&#20026;&#20102;&#20197;&#31995;&#32479;&#32508;&#21512;&#30340;&#26041;&#24335;&#30830;&#20445;&#22522;&#20110;DNNs&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#23433;&#20840;&#32771;&#34385;&#20316;&#20026;&#36866;&#24403;&#30340;&#32467;&#26500;&#20803;&#32032;&#12290;&#19968;&#26041;&#38754;&#65292;&#23433;&#20840;&#32771;&#34385;&#30340;&#27010;&#24565;&#35774;&#35745;&#19982;&#29616;&#26377;&#30340;&#19982;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#23433;&#20840;&#30456;&#20851;&#30340;&#26631;&#20934;&#22914;ISO 21448&#65288;SOTIF&#65289;&#38750;&#24120;&#22865;&#21512;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#24050;&#32463;&#28608;&#21457;&#20102;&#20960;&#31687;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#21363;&#23558;&#20986;&#21488;&#30340;&#20851;&#20110;AI&#23433;&#20840;&#30340;&#26631;&#20934;&#65292;&#22914;ISO PAS 8800&#12290;&#34429;&#28982;&#23433;&#20840;&#32771;&#34385;&#30340;&#27010;&#24565;&#20197;&#21069;&#24050;&#32463;&#34987;&#20171;&#32461;&#36807;&#65292;&#20294;&#26412;&#25991;&#23545;&#20854;&#36827;&#34892;&#20102;&#25193;&#23637;&#21644;&#20248;&#21270;&#65292;&#20511;&#37492;&#20102;&#21508;&#20010;&#39046;&#22495;&#21644;&#23433;&#20840;&#19987;&#23478;&#30340;&#21453;&#39304;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in the field of deep learning and impressive performance of deep neural networks (DNNs) for perception have resulted in an increased demand for their use in automated driving (AD) systems. The safety of such systems is of utmost importance and thus requires to consider the unique properties of DNNs.  In order to achieve safety of AD systems with DNN-based perception components in a systematic and comprehensive approach, so-called safety concerns have been introduced as a suitable structuring element. On the one hand, the concept of safety concerns is -- by design -- well aligned to existing standards relevant for safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it has already inspired several academic publications and upcoming standards on AI safety such as ISO PAS 8800.  While the concept of safety concerns has been previously introduced, this paper extends and refines it, leveraging feedback from various domain and safety experts in the field. In par
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#22871;&#32034;&#26041;&#27861;&#65292;&#23558;&#22871;&#32034;&#21644;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#27169;&#20223;&#32479;&#35745;&#26694;&#26550;&#36827;&#34892;&#20462;&#25913;&#30340;&#26041;&#24335;&#65292;&#22312;&#21464;&#37327;&#36873;&#25321;&#20013;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.03770</link><description>&lt;p&gt;
&#31070;&#32463;&#22871;&#32034;&#65306;&#19968;&#31181;&#23558;&#22871;&#32034;&#21644;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural lasso: a unifying approach of lasso and neural networks. (arXiv:2309.03770v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#22871;&#32034;&#26041;&#27861;&#65292;&#23558;&#22871;&#32034;&#21644;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#27169;&#20223;&#32479;&#35745;&#26694;&#26550;&#36827;&#34892;&#20462;&#25913;&#30340;&#26041;&#24335;&#65292;&#22312;&#21464;&#37327;&#36873;&#25321;&#20013;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#25216;&#26415;&#30456;&#32467;&#21512;&#20197;&#33719;&#24471;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#28909;&#28857;&#12290;&#26412;&#25991;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26469;&#34920;&#31034;&#21464;&#37327;&#36873;&#25321;&#30340;&#32479;&#35745;&#25216;&#26415;&#22871;&#32034;&#12290;&#35266;&#23519;&#21457;&#29616;&#65292;&#23613;&#31649;&#32479;&#35745;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#29256;&#26412;&#20855;&#26377;&#30456;&#21516;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20294;&#30001;&#20110;&#20248;&#21270;&#26041;&#27861;&#19981;&#21516;&#32780;&#23384;&#22312;&#24046;&#24322;&#12290;&#29305;&#21035;&#26159;&#65292;&#31070;&#32463;&#32593;&#32476;&#29256;&#26412;&#36890;&#24120;&#20351;&#29992;&#21333;&#20010;&#39564;&#35777;&#38598;&#36827;&#34892;&#19968;&#27493;&#20248;&#21270;&#65292;&#32780;&#32479;&#35745;&#23545;&#24212;&#26041;&#27861;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#36827;&#34892;&#20004;&#27493;&#20248;&#21270;&#12290;&#32479;&#35745;&#26041;&#27861;&#26356;&#20026;&#31934;&#32454;&#30340;&#20248;&#21270;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#21442;&#25968;&#20272;&#35745;&#65292;&#23588;&#20854;&#26159;&#22312;&#35757;&#32451;&#38598;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#27169;&#20223;&#32479;&#35745;&#26694;&#26550;&#12290;&#22312;&#24320;&#21457;&#19978;&#36848;&#20462;&#25913;&#30340;&#36807;&#31243;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there is a growing interest in combining techniques attributed to the areas of Statistics and Machine Learning in order to obtain the benefits of both approaches. In this article, the statistical technique lasso for variable selection is represented through a neural network. It is observed that, although both the statistical approach and its neural version have the same objective function, they differ due to their optimization. In particular, the neural version is usually optimized in one-step using a single validation set, while the statistical counterpart uses a two-step optimization based on cross-validation. The more elaborated optimization of the statistical method results in more accurate parameter estimation, especially when the training set is small. For this reason, a modification of the standard approach for training neural networks, that mimics the statistical framework, is proposed. During the development of the above modification, a new optimization algori
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#36816;&#21160;&#27169;&#24335;&#26469;&#20272;&#35745;&#23556;&#34880;&#20998;&#25968;&#21644;&#20998;&#31867;&#24515;&#32908;&#30149;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#20154;&#24037;M&#27169;&#24335;&#22270;&#20687;&#24182;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#65292;&#33410;&#30465;&#20102;&#26102;&#38388;&#21644;&#19987;&#19994;&#30693;&#35782;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.03759</link><description>&lt;p&gt;
&#20351;&#29992;&#36816;&#21160;&#27169;&#24335;&#26469;&#39044;&#27979;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#23556;&#34880;&#20998;&#25968;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
M(otion)-mode Based Prediction of Ejection Fraction using Echocardiograms. (arXiv:2309.03759v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#36816;&#21160;&#27169;&#24335;&#26469;&#20272;&#35745;&#23556;&#34880;&#20998;&#25968;&#21644;&#20998;&#31867;&#24515;&#32908;&#30149;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#20154;&#24037;M&#27169;&#24335;&#22270;&#20687;&#24182;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#65292;&#33410;&#30465;&#20102;&#26102;&#38388;&#21644;&#19987;&#19994;&#30693;&#35782;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#26399;&#31579;&#26597;&#23613;&#26089;&#26816;&#27979;&#24515;&#33039;&#21151;&#33021;&#38556;&#30861;&#23545;&#35786;&#26029;&#24515;&#34880;&#31649;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#23556;&#34880;&#20998;&#25968;&#65288;EF&#65289;&#26159;&#34913;&#37327;&#24515;&#33039;&#21151;&#33021;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#36739;&#20302;&#30340;EF&#19982;&#24515;&#32908;&#30149;&#30456;&#20851;&#12290;&#36229;&#22768;&#24515;&#21160;&#22270;&#26159;&#24515;&#33039;&#30149;&#23398;&#20013;&#24120;&#29992;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#36229;&#22768;&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#23454;&#26102;&#12289;&#26080;&#36752;&#23556;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#35780;&#20272;&#36229;&#22768;&#24515;&#21160;&#22270;&#20197;&#35745;&#31639;EF&#32791;&#26102;&#19988;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#22240;&#27492;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;M(otion)-mode&#26469;&#20272;&#35745;EF&#21644;&#20998;&#31867;&#24515;&#32908;&#30149;&#12290;&#25105;&#20204;&#20174;&#21333;&#20010;&#36229;&#22768;&#24515;&#21160;&#22270;&#29983;&#25104;&#22810;&#20010;&#20154;&#24037;M&#27169;&#24335;&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#29616;&#25104;&#30340;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#25193;&#23637;&#21040;&#24515;&#33039;&#25104;&#20687;&#20013;&#65292;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#21033;&#29992;&#32467;&#26500;&#26469;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#39640;&#31934;&#24230;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early detection of cardiac dysfunction through routine screening is vital for diagnosing cardiovascular diseases. An important metric of cardiac function is the left ventricular ejection fraction (EF), where lower EF is associated with cardiomyopathy. Echocardiography is a popular diagnostic tool in cardiology, with ultrasound being a low-cost, real-time, and non-ionizing technology. However, human assessment of echocardiograms for calculating EF is time-consuming and expertise-demanding, raising the need for an automated approach. In this work, we propose using the M(otion)-mode of echocardiograms for estimating the EF and classifying cardiomyopathy. We generate multiple artificial M-mode images from a single echocardiogram and combine them using off-the-shelf model architectures. Additionally, we extend contrastive learning (CL) to cardiac imaging to learn meaningful representations from exploiting structures in unlabeled data allowing the model to achieve high accuracy, even with li
&lt;/p&gt;</description></item><item><title>TSGBench&#26159;&#39318;&#20010;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#22522;&#20934;&#65292;&#29992;&#20110;&#32479;&#19968;&#21644;&#20840;&#38754;&#35780;&#20272;TSG&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;&#35780;&#20272;&#12289;&#25968;&#25454;&#38598;&#36873;&#25321;&#21644;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.03755</link><description>&lt;p&gt;
TSGBench&#65306;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TSGBench: Time Series Generation Benchmark. (arXiv:2309.03755v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03755
&lt;/p&gt;
&lt;p&gt;
TSGBench&#26159;&#39318;&#20010;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#22522;&#20934;&#65292;&#29992;&#20110;&#32479;&#19968;&#21644;&#20840;&#38754;&#35780;&#20272;TSG&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;&#35780;&#20272;&#12289;&#25968;&#25454;&#38598;&#36873;&#25321;&#21644;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;(TSG)&#22312;&#25968;&#25454;&#22686;&#24378;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#22810;&#20010;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19977;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;(1)&#23427;&#20204;&#32463;&#24120;&#38024;&#23545;&#31867;&#20284;&#30340;&#27169;&#22411;&#31867;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#38480;&#21046;&#20102;&#23545;&#24615;&#33021;&#33021;&#21147;&#30340;&#25972;&#20307;&#35270;&#35282;&#12290;(2)&#20351;&#29992;&#19987;&#38376;&#30340;&#21512;&#25104;&#21644;&#31169;&#26377;&#25968;&#25454;&#38598;&#24341;&#20837;&#20102;&#20559;&#20506;&#65292;&#38459;&#30861;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;(3)&#27169;&#31946;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24448;&#24448;&#19982;&#33258;&#23450;&#20041;&#32593;&#32476;&#25110;&#19979;&#28216;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#38459;&#30861;&#20102;&#19968;&#33268;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;\textsf {TSGBench}&#65292;&#20316;&#20026;&#39318;&#20010;TSG&#22522;&#20934;&#65292;&#26088;&#22312;&#32479;&#19968;&#21644;&#20840;&#38754;&#35780;&#20272;TSG&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;(1)&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#12289;&#38754;&#21521;TSG&#30340;&#20844;&#24320;&#23454;&#38469;&#25968;&#25454;&#38598;&#25910;&#38598;&#65292;&#20197;&#21450;&#26631;&#20934;&#21270;&#30340;&#39044;&#22788;&#29702;&#27969;&#31243;&#65307;(2)&#19968;&#22871;&#32508;&#21512;&#30340;&#35780;&#20272;&#25351;&#26631;&#22871;&#20214;&#65292;&#21253;&#25324;&#22522;&#26412;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Synthetic Time Series Generation (TSG) is crucial in a range of applications, including data augmentation, anomaly detection, and privacy preservation. Although significant strides have been made in this field, existing methods exhibit three key limitations: (1) They often benchmark against similar model types, constraining a holistic view of performance capabilities. (2) The use of specialized synthetic and private datasets introduces biases and hampers generalizability. (3) Ambiguous evaluation measures, often tied to custom networks or downstream tasks, hinder consistent and fair comparison.  To overcome these limitations, we introduce \textsf{TSGBench}, the inaugural TSG Benchmark, designed for a unified and comprehensive assessment of TSG methods. It comprises three modules: (1) a curated collection of publicly available, real-world datasets tailored for TSG, together with a standardized preprocessing pipeline; (2) a comprehensive evaluation measures suite including vanilla measur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#38656;&#35201;&#37096;&#20998;&#21516;&#27493;&#21644;&#38480;&#21046;&#24615;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#20998;&#25955;&#24335;&#21644;&#24322;&#27493;SGD&#65288;DASGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03754</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;ASGD&#30340;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Decentralized ASGD. (arXiv:2309.03754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#38656;&#35201;&#37096;&#20998;&#21516;&#27493;&#21644;&#38480;&#21046;&#24615;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#20998;&#25955;&#24335;&#21644;&#24322;&#27493;SGD&#65288;DASGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#19968;&#30452;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#30028;&#30340;&#24191;&#27867;&#30740;&#31350;&#12290;&#23613;&#31649;&#20854;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#36807;SGD&#20248;&#21270;&#22823;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#65292;&#24120;&#24120;&#23558;&#35757;&#32451;&#36807;&#31243;&#20998;&#24067;&#22312;&#22810;&#20010;&#35774;&#22791;&#19978;&#12290;&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#65292;&#24322;&#27493;SGD&#65288;ASGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#24635;&#26159;&#27604;&#23567;&#25209;&#37327;SGD&#24555;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#29702;&#35770;&#19978;&#30340;&#25913;&#21892;&#65292;&#22823;&#22810;&#25968;ASGD&#25910;&#25947;&#36895;&#24230;&#30340;&#35777;&#26126;&#20173;&#28982;&#20381;&#36182;&#20110;&#19968;&#20010;&#38598;&#20013;&#24335;&#21442;&#25968;&#26381;&#21153;&#22120;&#65292;&#22312;&#23558;&#26799;&#24230;&#35745;&#31639;&#25193;&#23637;&#21040;&#35768;&#22810;&#20998;&#24067;&#24335;&#36827;&#31243;&#26102;&#23481;&#26131;&#25104;&#20026;&#29942;&#39048;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#25955;&#24335;&#24322;&#27493;SGD&#65288;DASGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#20998;&#26512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#33410;&#28857;&#20043;&#38388;&#30340;&#37096;&#20998;&#21516;&#27493;&#65292;&#20063;&#19981;&#38656;&#35201;&#38480;&#21046;&#24615;&#30340;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decades, Stochastic Gradient Descent (SGD) has been intensively studied by the Machine Learning community. Despite its versatility and excellent performance, the optimization of large models via SGD still is a time-consuming task. To reduce training time, it is common to distribute the training process across multiple devices. Recently, it has been shown that the convergence of asynchronous SGD (ASGD) will always be faster than mini-batch SGD. However, despite these improvements in the theoretical bounds, most ASGD convergence-rate proofs still rely on a centralized parameter server, which is prone to become a bottleneck when scaling out the gradient computations across many distributed processes.  In this paper, we present a novel convergence-rate analysis for decentralized and asynchronous SGD (DASGD) which does not require partial synchronization among nodes nor restrictive network topologies. Specifically, we provide a bound of $\mathcal{O}(\sigma\epsilon^{-2}) + \mat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#21160;&#32858;&#31867;&#25968;&#37327;&#36873;&#25321;&#30340;&#20013;&#24515;&#36718;&#24275;&#32858;&#31867;&#31639;&#27861;&#65292;&#20854;&#20013;&#32467;&#21512;&#20102;&#21407;&#22987;&#36718;&#24275;&#31995;&#25968;&#21644;PAM&#31639;&#27861;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#24555;&#36895;&#29256;&#26412;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#22312;&#23454;&#39564;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.03751</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#21160;&#32858;&#31867;&#25968;&#37327;&#36873;&#25321;&#30340;&#20013;&#24515;&#36718;&#24275;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Medoid Silhouette clustering with automatic cluster number selection. (arXiv:2309.03751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#21160;&#32858;&#31867;&#25968;&#37327;&#36873;&#25321;&#30340;&#20013;&#24515;&#36718;&#24275;&#32858;&#31867;&#31639;&#27861;&#65292;&#20854;&#20013;&#32467;&#21512;&#20102;&#21407;&#22987;&#36718;&#24275;&#31995;&#25968;&#21644;PAM&#31639;&#27861;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#24555;&#36895;&#29256;&#26412;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#22312;&#23454;&#39564;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#32467;&#26524;&#30340;&#35780;&#20272;&#26159;&#22256;&#38590;&#30340;&#65292;&#39640;&#24230;&#20381;&#36182;&#20110;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#21644;&#35266;&#23519;&#32773;&#30340;&#35266;&#28857;&#12290;&#26377;&#35768;&#22810;&#19981;&#21516;&#30340;&#32858;&#31867;&#36136;&#37327;&#24230;&#37327;&#65292;&#35797;&#22270;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#30340;&#24230;&#37327;&#26469;&#39564;&#35777;&#32858;&#31867;&#32467;&#26524;&#12290;&#19968;&#20010;&#38750;&#24120;&#27969;&#34892;&#30340;&#24230;&#37327;&#26159;&#36718;&#24275;&#31995;&#25968;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#39640;&#25928;&#30340;&#22522;&#20110;&#20013;&#24515;&#28857;&#30340;&#36718;&#24275;&#31995;&#25968;&#65292;&#23545;&#20854;&#23646;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20004;&#20010;&#30452;&#25509;&#20248;&#21270;&#30340;&#24555;&#36895;&#29256;&#26412;&#65292;&#24182;&#35752;&#35770;&#20102;&#36873;&#25321;&#26368;&#20339;&#32858;&#31867;&#25968;&#37327;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#21407;&#22987;&#36718;&#24275;&#31995;&#25968;&#21644;&#33879;&#21517;&#30340;PAM&#31639;&#27861;&#20197;&#21450;&#20854;&#26368;&#26032;&#25913;&#36827;FasterPAM&#30340;&#24605;&#24819;&#30456;&#32467;&#21512;&#12290;&#20854;&#20013;&#19968;&#20010;&#29256;&#26412;&#20445;&#35777;&#19982;&#21407;&#22987;&#29256;&#26412;&#30456;&#31561;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;$O(k^2)$&#30340;&#36816;&#34892;&#21152;&#36895;&#12290;&#22312;&#20855;&#26377;30000&#20010;&#26679;&#26412;&#21644;$k$=100&#30340;&#23454;&#38469;&#25968;&#25454;&#19978;&#65292;&#19982;&#21407;&#22987;&#30340;PAMMEDSIL&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;10464&#20493;&#30340;&#21152;&#36895;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#25509;&#36873;&#25321;&#26368;&#20339;&#32858;&#31867;&#25968;&#37327;&#30340;&#21464;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of clustering results is difficult, highly dependent on the evaluated data set and the perspective of the beholder. There are many different clustering quality measures, which try to provide a general measure to validate clustering results. A very popular measure is the Silhouette. We discuss the efficient medoid-based variant of the Silhouette, perform a theoretical analysis of its properties, provide two fast versions for the direct optimization, and discuss the use to choose the optimal number of clusters. We combine ideas from the original Silhouette with the well-known PAM algorithm and its latest improvements FasterPAM. One of the versions guarantees equal results to the original variant and provides a run speedup of $O(k^2)$. In experiments on real data with 30000 samples and $k$=100, we observed a 10464$\times$ speedup compared to the original PAMMEDSIL algorithm. Additionally, we provide a variant to choose the optimal number of clusters directly.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#23545;&#35805;&#31995;&#32479;&#12290;&#22312;&#35774;&#35745;&#21644;&#24320;&#21457;&#38454;&#27573;&#65292;LLM&#21487;&#20197;&#24110;&#21161;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12289;&#25552;&#21462;&#23454;&#20307;&#21644;&#21516;&#20041;&#35789;&#12289;&#26412;&#22320;&#21270;&#21644;&#35282;&#33394;&#35774;&#35745;&#12290;&#22312;&#36816;&#33829;&#38454;&#27573;&#65292;LLM&#21487;&#20197;&#36741;&#21161;&#19978;&#19979;&#25991;&#21270;&#12289;&#24847;&#22270;&#20998;&#31867;&#12289;&#33258;&#21160;&#32416;&#27491;&#35805;&#35821;&#12289;&#25913;&#20889;&#22238;&#22797;&#12289;&#25688;&#35201;&#21644;&#20351;&#38381;&#21512;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#31169;&#20154;&#38134;&#34892;&#39046;&#22495;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03748</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Enhancing Pipeline-Based Conversational Agents with Large Language Models. (arXiv:2309.03748v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#23545;&#35805;&#31995;&#32479;&#12290;&#22312;&#35774;&#35745;&#21644;&#24320;&#21457;&#38454;&#27573;&#65292;LLM&#21487;&#20197;&#24110;&#21161;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12289;&#25552;&#21462;&#23454;&#20307;&#21644;&#21516;&#20041;&#35789;&#12289;&#26412;&#22320;&#21270;&#21644;&#35282;&#33394;&#35774;&#35745;&#12290;&#22312;&#36816;&#33829;&#38454;&#27573;&#65292;LLM&#21487;&#20197;&#36741;&#21161;&#19978;&#19979;&#25991;&#21270;&#12289;&#24847;&#22270;&#20998;&#31867;&#12289;&#33258;&#21160;&#32416;&#27491;&#35805;&#35821;&#12289;&#25913;&#20889;&#22238;&#22797;&#12289;&#25688;&#35201;&#21644;&#20351;&#38381;&#21512;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#31169;&#20154;&#38134;&#34892;&#39046;&#22495;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20195;&#29702;&#22120;&#65288;&#22914;GPT-4&#65289;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21830;&#19994;&#21270;&#23545;&#35805;&#31995;&#32479;&#24320;&#21457;&#24037;&#20855;&#26159;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#65292;&#24182;&#19988;&#22312;&#36827;&#34892;&#20154;&#31867;&#23545;&#35805;&#26102;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#22312;&#20197;&#19979;&#20004;&#20010;&#38454;&#27573;&#20013;&#22686;&#24378;&#22522;&#20110;&#27969;&#27700;&#32447;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#65306;1&#65289;&#35774;&#35745;&#21644;&#24320;&#21457;&#38454;&#27573;&#65307;2&#65289;&#36816;&#33829;&#38454;&#27573;&#12290;&#22312;1&#65289;&#20013;&#65292;LLM&#21487;&#20197;&#22312;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12289;&#25552;&#21462;&#23454;&#20307;&#21644;&#21516;&#20041;&#35789;&#12289;&#26412;&#22320;&#21270;&#21644;&#35282;&#33394;&#35774;&#35745;&#26041;&#38754;&#25552;&#20379;&#24110;&#21161;&#12290;&#22312;2&#65289;&#20013;&#65292;LLM&#21487;&#20197;&#36741;&#21161;&#19978;&#19979;&#25991;&#21270;&#12289;&#24847;&#22270;&#20998;&#31867;&#20197;&#38450;&#27490;&#23545;&#35805;&#20013;&#26029;&#21644;&#22788;&#29702;&#36229;&#20986;&#33539;&#22260;&#30340;&#38382;&#39064;&#12289;&#33258;&#21160;&#32416;&#27491;&#35805;&#35821;&#12289;&#25913;&#20889;&#22238;&#22797;&#12289;&#21046;&#23450;&#28040;&#27495;&#38382;&#21477;&#12289;&#25688;&#35201;&#21644;&#20351;&#38381;&#21512;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#31169;&#20154;&#38134;&#34892;&#39046;&#22495;&#36827;&#34892;&#20102;&#20351;&#29992;GPT-4&#30340;&#38750;&#27491;&#24335;&#23454;&#39564;&#65292;&#20197;&#23454;&#38469;&#31034;&#20363;&#35777;&#26126;&#19978;&#36848;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The latest advancements in AI and deep learning have led to a breakthrough in large language model (LLM)-based agents such as GPT-4. However, many commercial conversational agent development tools are pipeline-based and have limitations in holding a human-like conversation. This paper investigates the capabilities of LLMs to enhance pipeline-based conversational agents during two phases: 1) in the design and development phase and 2) during operations. In 1) LLMs can aid in generating training data, extracting entities and synonyms, localization, and persona design. In 2) LLMs can assist in contextualization, intent classification to prevent conversational breakdown and handle out-of-scope questions, auto-correcting utterances, rephrasing responses, formulating disambiguation questions, summarization, and enabling closed question-answering capabilities. We conducted informal experiments with GPT-4 in the private banking domain to demonstrate the scenarios above with a practical example.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CBRNet&#65292;&#19968;&#31181;&#36890;&#36807;&#34920;&#31034;&#24179;&#34913;&#23398;&#20064;&#36830;&#32493;&#20540;&#27835;&#30103;&#25928;&#26524;&#30340;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03731</link><description>&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#24179;&#34913;&#23398;&#20064;&#36830;&#32493;&#20540;&#27835;&#30103;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Learning continuous-valued treatment effects through representation balancing. (arXiv:2309.03731v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CBRNet&#65292;&#19968;&#31181;&#36890;&#36807;&#34920;&#31034;&#24179;&#34913;&#23398;&#20064;&#36830;&#32493;&#20540;&#27835;&#30103;&#25928;&#26524;&#30340;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#12289;&#21830;&#19994;&#12289;&#32463;&#27982;&#31561;&#39046;&#22495;&#65292;&#20272;&#35745;&#19982;&#27835;&#30103;&#21058;&#37327;&#30456;&#20851;&#30340;&#27835;&#30103;&#25928;&#26524;&#65288;&#21363;&#8220;&#21058;&#37327;&#21453;&#24212;&#8221;&#65289;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36830;&#32493;&#20540;&#27835;&#30103;&#25928;&#26524;&#36890;&#24120;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#24471;&#21040;&#30340;&#65292;&#32780;&#35266;&#27979;&#25968;&#25454;&#21487;&#33021;&#23384;&#22312;&#21058;&#37327;&#36873;&#25321;&#20559;&#24046;&#65292;&#21363;&#21058;&#37327;&#20998;&#37197;&#21463;&#21040;&#39044;&#22788;&#29702;&#21327;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#23384;&#22312;&#21058;&#37327;&#36873;&#25321;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#20934;&#30830;&#23398;&#20064;&#21040;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CBRNet&#30340;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#20010;&#20307;&#30340;&#21058;&#37327;&#21453;&#24212;&#12290;CBRNet&#37319;&#29992;&#20102;Neyman-Rubin&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#65292;&#24182;&#25193;&#23637;&#20102;&#24179;&#34913;&#34920;&#31034;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#20197;&#20811;&#26381;&#36830;&#32493;&#20540;&#27835;&#30103;&#20013;&#30340;&#36873;&#25321;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#22312;&#36830;&#32493;&#20540;&#27835;&#30103;&#20013;&#24212;&#29992;&#34920;&#31034;&#24179;&#34913;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the effects of treatments with an associated dose on an instance's outcome, the "dose response", is relevant in a variety of domains, from healthcare to business, economics, and beyond. Such effects, also known as continuous-valued treatment effects, are typically estimated from observational data, which may be subject to dose selection bias. This means that the allocation of doses depends on pre-treatment covariates. Previous studies have shown that conventional machine learning approaches fail to learn accurate individual estimates of dose responses under the presence of dose selection bias. In this work, we propose CBRNet, a causal machine learning approach to estimate an individual dose response from observational data. CBRNet adopts the Neyman-Rubin potential outcome framework and extends the concept of balanced representation learning for overcoming selection bias to continuous-valued treatments. Our work is the first to apply representation balancing in a continuous-v
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23558;&#23450;&#20215;&#38382;&#39064;&#35270;&#20026;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#65292;&#39318;&#27425;&#23545;&#36873;&#25321;&#20559;&#24046;&#23545;&#36151;&#27454;&#23450;&#20215;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.03730</link><description>&lt;p&gt;
&#36151;&#27454;&#23450;&#20215;&#30340;&#22240;&#26524;&#35270;&#35282;: &#25506;&#35752;&#36873;&#25321;&#20559;&#24046;&#23545;&#36776;&#35782;&#20986;&#20215;&#21709;&#24212;&#20989;&#25968;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
A Causal Perspective on Loan Pricing: Investigating the Impacts of Selection Bias on Identifying Bid-Response Functions. (arXiv:2309.03730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23558;&#23450;&#20215;&#38382;&#39064;&#35270;&#20026;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#65292;&#39318;&#27425;&#23545;&#36873;&#25321;&#20559;&#24046;&#23545;&#36151;&#27454;&#23450;&#20215;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36151;&#27454;&#39046;&#22495;&#65292;&#20215;&#26684;&#26082;&#19982;&#23458;&#25143;&#26377;&#20851;&#21448;&#19982;&#20135;&#21697;&#26377;&#20851;&#65292;&#30830;&#20445;&#19968;&#20010;&#33391;&#22909;&#36816;&#20316;&#30340;&#20010;&#24615;&#21270;&#23450;&#20215;&#31574;&#30053;&#23545;&#19994;&#21153;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#27492;&#31867;&#31574;&#30053;&#24517;&#39035;&#22522;&#20110;&#35266;&#27979;&#25968;&#25454;&#25512;&#23548;&#32780;&#26469;&#65292;&#36825;&#20250;&#24341;&#20837;&#19968;&#20123;&#25361;&#25112;&#12290;&#34429;&#28982;&#8220;&#20869;&#29983;&#24615;&#8221;&#38382;&#39064;&#22312;&#29616;&#26377;&#30340;&#23450;&#20215;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65288;&#25110;&#32773;&#26356;&#20934;&#30830;&#22320;&#35828;&#26159;&#20986;&#20215;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65289;&#36824;&#26410;&#24471;&#21040;&#37325;&#35270;&#12290;&#25105;&#20204;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#25506;&#35752;&#36873;&#25321;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#23450;&#20215;&#38382;&#39064;&#35270;&#20026;&#19968;&#20010;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#65292;&#32771;&#34385;&#23458;&#25143;&#23545;&#20215;&#26684;&#30340;&#21453;&#24212;&#20316;&#20026;&#19968;&#31181;&#27835;&#30103;&#25928;&#24212;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27604;&#21033;&#26102;&#30340;&#25269;&#25276;&#36151;&#27454;&#30003;&#35831;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#27169;&#25311;&#19981;&#21516;&#31243;&#24230;&#30340;&#36873;&#25321;&#20559;&#24046;&#65292;&#30740;&#31350;&#21442;&#25968;&#21270;&#21644;&#38750;&#21442;&#25968;&#21270;&#26041;&#27861;&#22312;&#35782;&#21035;&#20010;&#20307;&#20986;&#20215;&#21709;&#24212;&#20989;&#25968;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#65289;&#22312;&#22788;&#29702;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In lending, where prices are specific to both customers and products, having a well-functioning personalized pricing policy in place is essential to effective business making. Typically, such a policy must be derived from observational data, which introduces several challenges. While the problem of ``endogeneity'' is prominently studied in the established pricing literature, the problem of selection bias (or, more precisely, bid selection bias) is not. We take a step towards understanding the effects of selection bias by posing pricing as a problem of causal inference. Specifically, we consider the reaction of a customer to price a treatment effect. In our experiments, we simulate varying levels of selection bias on a semi-synthetic dataset on mortgage loan applications in Belgium. We investigate the potential of parametric and nonparametric methods for the identification of individual bid-response functions. Our results illustrate how conventional methods such as logistic regression a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03720</link><description>&lt;p&gt;
&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism. (arXiv:2309.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35268;&#21010;&#22825;&#28982;&#27668;&#20379;&#24212;&#21644;&#28040;&#36153;&#20197;&#21450;&#20248;&#21270;&#33719;&#24471;&#22825;&#28982;&#27668;&#25104;&#26412;&#26041;&#38754;&#65292;&#32771;&#34385;&#23395;&#33410;&#24615;&#21644;&#36235;&#21183;&#24615;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27493; ahead &#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#38598;&#25104;&#20102;&#21464;&#28857;&#26816;&#27979;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;Hoeffding&#26641;&#39044;&#27979;&#22120;&#20316;&#20026;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21098;&#35009;&#30340;&#31934;&#30830;&#32447;&#24615;&#26102;&#38388;&#65288;PELT&#65289;&#31639;&#27861;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#12290;&#21464;&#28857;&#26816;&#27979;&#38598;&#25104;&#20351;&#24471;&#36873;&#25321;&#19981;&#21516;&#30340;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting natural gas consumption, considering seasonality and trends, is crucial in planning its supply and consumption and optimizing the cost of obtaining it, mainly by industrial entities. However, in times of threats to its supply, it is also a critical element that guarantees the supply of this raw material to meet individual consumers' needs, ensuring society's energy security. This article introduces a novel multistep ahead forecasting of natural gas consumption with change point detection integration for model collection selection with continual learning capabilities using data stream processing. The performance of the forecasting models based on the proposed approach is evaluated in a complex real-world use case of natural gas consumption forecasting. We employed Hoeffding tree predictors as forecasting models and the Pruned Exact Linear Time (PELT) algorithm for the change point detection procedure. The change point detection integration enables selecting a different model
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#36882;&#20943;&#36793;&#38469;&#25928;&#29992;&#29616;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;$\lambda$&#34920;&#31034;&#65288;$\lambda$R&#65289;&#30340;&#26032;&#22411;&#29366;&#24577;&#34920;&#31034;&#65292;&#29992;&#20110;&#24555;&#36895;&#31574;&#30053;&#35780;&#20272;&#65292;&#35813;&#34920;&#31034;&#33021;&#22815;&#25512;&#24191;&#24050;&#26377;&#30340;&#29366;&#24577;&#34920;&#31034;&#24182;&#20855;&#22791;&#19968;&#20123;&#27491;&#24335;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03710</link><description>&lt;p&gt;
&#19968;&#31181;&#36882;&#20943;&#22870;&#21169;&#30340;&#29366;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A State Representation for Diminishing Rewards. (arXiv:2309.03710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#36882;&#20943;&#36793;&#38469;&#25928;&#29992;&#29616;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;$\lambda$&#34920;&#31034;&#65288;$\lambda$R&#65289;&#30340;&#26032;&#22411;&#29366;&#24577;&#34920;&#31034;&#65292;&#29992;&#20110;&#24555;&#36895;&#31574;&#30053;&#35780;&#20272;&#65292;&#35813;&#34920;&#31034;&#33021;&#22815;&#25512;&#24191;&#24050;&#26377;&#30340;&#29366;&#24577;&#34920;&#31034;&#24182;&#20855;&#22791;&#19968;&#20123;&#27491;&#24335;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#24773;&#26223;&#35201;&#27714;&#20195;&#29702;&#24555;&#36895;&#36866;&#24212;&#20174;&#22266;&#23450;&#20998;&#24067;&#20013;&#38543;&#26426;&#37319;&#26679;&#30340;&#21508;&#31181;&#38745;&#24577;&#22870;&#21169;&#20989;&#25968;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21518;&#32487;&#29366;&#24577;&#34920;&#31034;&#65288;SR&#65289;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#31574;&#30053;&#30340;&#39044;&#26399;&#25240;&#25187;&#32047;&#31215;&#29366;&#24577;&#20998;&#24067;&#19982;&#29305;&#23450;&#22870;&#21169;&#20989;&#25968;&#35299;&#32806;&#65292;&#25903;&#25345;&#24555;&#36895;&#31574;&#30053;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#28982;&#30028;&#20013;&#65292;&#39034;&#24207;&#20219;&#21153;&#24456;&#23569;&#26159;&#29420;&#31435;&#30340;&#65292;&#32780;&#26159;&#22522;&#20110;&#22870;&#21169;&#21050;&#28608;&#30340;&#21487;&#29992;&#24615;&#21644;&#20027;&#35266;&#24863;&#30693;&#21453;&#26144;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#20248;&#20808;&#32423;&#12290;&#20026;&#20102;&#21453;&#26144;&#36825;&#31181;&#19981;&#21327;&#35843;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#36882;&#20943;&#36793;&#38469;&#25928;&#29992;&#30340;&#29616;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#31216;&#20026;$\lambda$&#34920;&#31034;&#65288;$\lambda$R&#65289;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#35813;&#35774;&#32622;&#20013;&#38656;&#35201;&#29992;&#20110;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;SR&#20197;&#21450;&#25991;&#29486;&#20013;&#30340;&#20854;&#20182;&#20960;&#31181;&#29366;&#24577;&#34920;&#31034;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;$\lambda$R&#30340;&#27491;&#24335;&#23646;&#24615;&#24182;&#30740;&#31350;&#20102;&#23427;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common setting in multitask reinforcement learning (RL) demands that an agent rapidly adapt to various stationary reward functions randomly sampled from a fixed distribution. In such situations, the successor representation (SR) is a popular framework which supports rapid policy evaluation by decoupling a policy's expected discounted, cumulative state occupancies from a specific reward function. However, in the natural world, sequential tasks are rarely independent, and instead reflect shifting priorities based on the availability and subjective perception of rewarding stimuli. Reflecting this disjunction, in this paper we study the phenomenon of diminishing marginal utility and introduce a novel state representation, the $\lambda$ representation ($\lambda$R) which, surprisingly, is required for policy evaluation in this setting and which generalizes the SR as well as several other state representations from the literature. We establish the $\lambda$R's formal properties and examine 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#32842;&#22825;&#22833;&#36133;&#21644;&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#38381;&#29615;&#25511;&#21046;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#31561;&#35299;&#20915;&#26041;&#26696;&#20197;&#38477;&#20302;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2309.03708</link><description>&lt;p&gt;
&#32842;&#22825;&#22833;&#36133;&#21644;&#38382;&#39064;&#65306;&#21407;&#22240;&#21644;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Chat Failures and Troubles: Reasons and Solutions. (arXiv:2309.03708v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#32842;&#22825;&#22833;&#36133;&#21644;&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#38381;&#29615;&#25511;&#21046;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#31561;&#35299;&#20915;&#26041;&#26696;&#20197;&#38477;&#20302;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#32842;&#22825;&#23548;&#33268;&#22833;&#36133;&#21644;&#38382;&#39064;&#30340;&#19968;&#20123;&#24120;&#35265;&#38382;&#39064;&#12290;&#19968;&#20010;&#32473;&#23450;&#30340;&#29992;&#20363;&#30340;&#35774;&#35745;&#20915;&#31574;&#22987;&#20110;&#36873;&#25321;&#21512;&#36866;&#30340;&#26426;&#22120;&#20154;&#12289;&#21512;&#36866;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#35782;&#21035;&#23548;&#33268;&#22833;&#36133;&#30340;&#24120;&#35265;&#38382;&#39064;&#65292;&#25214;&#20986;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35745;&#21010;&#25345;&#32493;&#25913;&#36827;&#12290;&#24635;&#32467;&#36215;&#26469;&#65292;&#24314;&#35758;&#20351;&#29992;&#38381;&#29615;&#25511;&#21046;&#31639;&#27861;&#26469;&#24341;&#23548;&#35757;&#32451;&#22909;&#30340;&#20154;&#24037;&#26234;&#33021;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#24182;&#25552;&#20379;&#35789;&#27719;&#36807;&#28388;&#12289;&#23545;&#26032;&#25968;&#25454;&#38598;&#37325;&#26032;&#35757;&#32451;&#25209;&#27425;&#27169;&#22411;&#12289;&#22312;&#32447;&#23398;&#20064;&#25968;&#25454;&#27969;&#20197;&#21450;/&#25110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#26469;&#33258;&#25105;&#26356;&#26032;&#35757;&#32451;&#27169;&#22411;&#20197;&#38477;&#20302;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines some common problems in Human-Robot Interaction (HRI) causing failures and troubles in Chat. A given use case's design decisions start with the suitable robot, the suitable chatting model, identifying common problems that cause failures, identifying potential solutions, and planning continuous improvement. In conclusion, it is recommended to use a closed-loop control algorithm that guides the use of trained Artificial Intelligence (AI) pre-trained models and provides vocabulary filtering, re-train batched models on new datasets, learn online from data streams, and/or use reinforcement learning models to self-update the trained models and reduce errors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21322;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#21442;&#25968;&#21270;&#30340;&#19977;&#20803;&#39532;&#23572;&#31185;&#22827;&#38142;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#38024;&#23545;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#25512;&#23548;&#20986;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#29992;&#20110;&#39034;&#24207;&#36125;&#21494;&#26031;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.03707</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#19977;&#20803;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#27010;&#29575;&#21322;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Semi-Supervised Approach with Triplet Markov Chains. (arXiv:2309.03707v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21322;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#21442;&#25968;&#21270;&#30340;&#19977;&#20803;&#39532;&#23572;&#31185;&#22827;&#38142;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#38024;&#23545;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#25512;&#23548;&#20986;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#29992;&#20110;&#39034;&#24207;&#36125;&#21494;&#26031;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#20803;&#39532;&#23572;&#31185;&#22827;&#38142;&#26159;&#29992;&#20110;&#39034;&#24207;&#25968;&#25454;&#30340;&#19968;&#31181;&#36890;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#19977;&#31181;&#38543;&#26426;&#21464;&#37327;&#65306;&#65288;&#24102;&#22122;&#65289;&#35266;&#27979;&#20540;&#12289;&#23427;&#20204;&#30456;&#20851;&#30340;&#31163;&#25955;&#26631;&#31614;&#21644;&#26088;&#22312;&#22686;&#24378;&#35266;&#27979;&#20540;&#21450;&#20854;&#30456;&#20851;&#26631;&#31614;&#20998;&#24067;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#27809;&#26377;&#25152;&#26377;&#19982;&#35266;&#27979;&#20540;&#30456;&#20851;&#30340;&#26631;&#31614;&#26469;&#20272;&#35745;&#36825;&#31181;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21322;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#21442;&#25968;&#21270;&#30340;&#19977;&#20803;&#39532;&#23572;&#31185;&#22827;&#38142;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#38024;&#23545;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#25512;&#23548;&#20986;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#29992;&#20110;&#39034;&#24207;&#36125;&#21494;&#26031;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Triplet Markov chains are general generative models for sequential data which take into account three kinds of random variables: (noisy) observations, their associated discrete labels and latent variables which aim at strengthening the distribution of the observations and their associated labels. However, in practice, we do not have at our disposal all the labels associated to the observations to estimate the parameters of such models. In this paper, we propose a general framework based on a variational Bayesian inference to train parameterized triplet Markov chain models in a semi-supervised context. The generality of our approach enables us to derive semi-supervised algorithms for a variety of generative models for sequential Bayesian classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37325;&#26500;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20813;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#21644;&#20860;&#23481;&#24615;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#25239;&#23041;&#32961;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03702</link><description>&lt;p&gt;
DiffDefense&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
DiffDefense: Defending against Adversarial Attacks via Diffusion Models. (arXiv:2309.03702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37325;&#26500;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20813;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#21644;&#20860;&#23481;&#24615;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#25239;&#23041;&#32961;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#24314;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#20813;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#32780;&#26080;&#38656;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#24494;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#20351;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#23613;&#31649;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#36890;&#24120;&#22240;&#20854;&#32531;&#24930;&#30340;&#21453;&#21521;&#36807;&#31243;&#32780;&#34987;&#24573;&#35270;&#65292;&#20294;&#26412;&#25991;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38450;&#24481;&#23545;&#25239;&#23041;&#32961;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#24178;&#20928;&#30340;&#20934;&#30830;&#24615;&#12289;&#36895;&#24230;&#21644;&#21363;&#25554;&#21363;&#29992;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel reconstruction method that leverages Diffusion Models to protect machine learning classifiers against adversarial attacks, all without requiring any modifications to the classifiers themselves. The susceptibility of machine learning models to minor input perturbations renders them vulnerable to adversarial attacks. While diffusion-based methods are typically disregarded for adversarial defense due to their slow reverse process, this paper demonstrates that our proposed method offers robustness against adversarial threats while preserving clean accuracy, speed, and plug-and-play compatibility. Code at: https://github.com/HondamunigePrasannaSilva/DiffDefence.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35745;&#31639;&#25928;&#29575;&#20248;&#21270;&#26694;&#26550;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#12289;&#35299;&#37322;&#24615;&#19981;&#36879;&#26126;&#21644;&#39640;&#35745;&#31639;&#24320;&#38144;&#31561;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.03694</link><description>&lt;p&gt;
&#20351;&#29992;&#31890;&#23376;&#32676;&#20248;&#21270;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#22686;&#24378;&#30340;CNN-LSTM&#32593;&#32476;&#36827;&#34892;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Short-Term Load Forecasting Using A Particle-Swarm Optimized Multi-Head Attention-Augmented CNN-LSTM Network. (arXiv:2309.03694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03694
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35745;&#31639;&#25928;&#29575;&#20248;&#21270;&#26694;&#26550;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#12289;&#35299;&#37322;&#24615;&#19981;&#36879;&#26126;&#21644;&#39640;&#35745;&#31639;&#24320;&#38144;&#31561;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#23545;&#20110;&#30005;&#21147;&#31995;&#32479;&#30340;&#39640;&#25928;&#36816;&#34892;&#21644;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#65292;&#32473;&#23450;&#20854;&#22266;&#26377;&#30340;&#38750;&#32447;&#24615;&#21644;&#21160;&#24577;&#29305;&#24615;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38754;&#20020;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#12289;&#35299;&#37322;&#24615;&#19981;&#36879;&#26126;&#21644;&#23454;&#26102;&#37096;&#32626;&#30340;&#39640;&#35745;&#31639;&#24320;&#38144;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#33258;&#20027;&#22320;&#25506;&#32034;&#21644;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#35782;&#21035;&#23545;&#20934;&#30830;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#26174;&#33879;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#31616;&#21270;&#30340;&#26694;&#26550;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#30495;&#23454;&#30005;&#21147;&#38656;&#27714;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#35780;&#20272;&#12290;&#32467;&#26524;&#31361;&#26174;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;...
&lt;/p&gt;
&lt;p&gt;
Short-term load forecasting is of paramount importance in the efficient operation and planning of power systems, given its inherent non-linear and dynamic nature. Recent strides in deep learning have shown promise in addressing this challenge. However, these methods often grapple with hyperparameter sensitivity, opaqueness in interpretability, and high computational overhead for real-time deployment. In this paper, I propose a novel solution that surmounts these obstacles. Our approach harnesses the power of the Particle-Swarm Optimization algorithm to autonomously explore and optimize hyperparameters, a Multi-Head Attention mechanism to discern the salient features crucial for accurate forecasting, and a streamlined framework for computational efficiency. Our method undergoes rigorous evaluation using a genuine electricity demand dataset. The results underscore its superiority in terms of accuracy, robustness, and computational efficiency. Notably, our Mean Absolute Percentage Error o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#36731;&#37327;&#32423;&#30340;&#23433;&#20840;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;Nadaraya-Watson&#20272;&#35745;&#22120;&#25552;&#20379;&#27010;&#29575;&#23433;&#20840;&#20445;&#35777;&#65292;&#24182;&#35299;&#20915;&#20102;&#39640;&#26031;&#36807;&#31243;&#25512;&#29702;&#22312;&#39640;&#32500;&#21644;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#35745;&#31639;&#22797;&#26434;&#24230;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03672</link><description>&lt;p&gt;
&#19968;&#31181;&#35745;&#31639;&#36731;&#37327;&#32423;&#30340;&#23433;&#20840;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A computationally lightweight safe learning algorithm. (arXiv:2309.03672v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#36731;&#37327;&#32423;&#30340;&#23433;&#20840;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;Nadaraya-Watson&#20272;&#35745;&#22120;&#25552;&#20379;&#27010;&#29575;&#23433;&#20840;&#20445;&#35777;&#65292;&#24182;&#35299;&#20915;&#20102;&#39640;&#26031;&#36807;&#31243;&#25512;&#29702;&#22312;&#39640;&#32500;&#21644;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#35745;&#31639;&#22797;&#26434;&#24230;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#29289;&#29702;&#31995;&#32479;&#30340;&#25511;&#21046;&#31574;&#30053;&#26102;&#65292;&#23433;&#20840;&#26159;&#19968;&#20010;&#24517;&#19981;&#21487;&#23569;&#30340;&#22240;&#32032;&#65292;&#22240;&#20026;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36829;&#21453;&#23433;&#20840;&#32422;&#26463;&#21487;&#33021;&#23548;&#33268;&#26114;&#36149;&#30340;&#30828;&#20214;&#25439;&#22351;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#31181;&#38656;&#27714;&#65292;&#23433;&#20840;&#23398;&#20064;&#39046;&#22495;&#28044;&#29616;&#20986;&#20102;&#19968;&#20123;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#30693;&#36947;&#24213;&#23618;&#31995;&#32479;&#21160;&#24577;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#27010;&#29575;&#23433;&#20840;&#20445;&#35777;&#12290;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39640;&#26031;&#36807;&#31243;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#39640;&#26031;&#36807;&#31243;&#25512;&#29702;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#30528;&#25968;&#25454;&#28857;&#25968;&#37327;&#30340;&#22686;&#21152;&#21576;&#31435;&#26041;&#22686;&#38271;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#39640;&#32500;&#21644;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#27010;&#29575;&#23433;&#20840;&#20445;&#35777;&#65292;&#24182;&#21033;&#29992;&#20102;Nadaraya-Watson&#20272;&#35745;&#22120;&#20195;&#26367;&#39640;&#26031;&#36807;&#31243;&#12290;&#23545;&#20110;Nadaraya-Watson&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#19982;&#25968;&#25454;&#28857;&#25968;&#37327;&#30340;&#23545;&#25968;&#27604;&#20363;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#20026;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#23558;&#20854;&#23884;&#20837;&#21040;&#23433;&#20840;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#24182;&#22312;&#19968;&#20010;&#27169;&#25311;&#30340;&#19971;&#33258;&#30001;&#24230;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety is an essential asset when learning control policies for physical systems, as violating safety constraints during training can lead to expensive hardware damage. In response to this need, the field of safe learning has emerged with algorithms that can provide probabilistic safety guarantees without knowledge of the underlying system dynamics. Those algorithms often rely on Gaussian process inference. Unfortunately, Gaussian process inference scales cubically with the number of data points, limiting applicability to high-dimensional and embedded systems. In this paper, we propose a safe learning algorithm that provides probabilistic safety guarantees but leverages the Nadaraya-Watson estimator instead of Gaussian processes. For the Nadaraya-Watson estimator, we can reach logarithmic scaling with the number of data points. We provide theoretical guarantees for the estimates, embed them into a safe learning algorithm, and show numerical experiments on a simulated seven-degrees-of-f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24369;&#26631;&#35760;&#35270;&#39057;&#20013;&#29983;&#25104;&#20525;&#40657;&#29481;&#29481;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#31350;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#31639;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25968;&#25454;&#20934;&#22791;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#27491;&#30830;&#30340;&#25968;&#25454;&#20998;&#31163;&#23545;&#20110;&#20998;&#31867;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;</title><link>http://arxiv.org/abs/2309.03671</link><description>&lt;p&gt;
&#20174;&#24369;&#26631;&#35760;&#35270;&#39057;&#20013;&#29983;&#25104;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20525;&#40657;&#29481;&#29481;&#20998;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dataset Generation and Bonobo Classification from Weakly Labelled Videos. (arXiv:2309.03671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24369;&#26631;&#35760;&#35270;&#39057;&#20013;&#29983;&#25104;&#20525;&#40657;&#29481;&#29481;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#31350;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#31639;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25968;&#25454;&#20934;&#22791;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#27491;&#30830;&#30340;&#25968;&#25454;&#20998;&#31163;&#23545;&#20110;&#20998;&#31867;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#24120;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#30340;&#20525;&#40657;&#29481;&#29481;&#26816;&#27979;&#21644;&#20998;&#31867;&#27969;&#31243;&#12290;&#35813;&#24212;&#29992;&#30340;&#21160;&#26426;&#26159;&#20026;&#20102;&#22312;&#27809;&#26377;&#20154;&#30340;&#24110;&#21161;&#19979;&#65292;&#20351;&#29992;&#35302;&#25720;&#23631;&#35774;&#22791;&#23545;&#20525;&#40657;&#29481;&#29481;&#22312;&#23427;&#20204;&#30340;&#22260;&#26639;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#22522;&#20110;&#20525;&#40657;&#29481;&#29481;&#24405;&#20687;&#30340;&#33258;&#21160;&#20135;&#29983;&#30340;&#12290;&#36825;&#20123;&#24405;&#20687;&#26159;&#24369;&#26631;&#35760;&#30340;&#65292;&#24182;&#36890;&#36807;&#29461;&#29492;&#26816;&#27979;&#22120;&#36827;&#34892;&#31354;&#38388;&#26816;&#27979;&#65292;&#20197;&#26816;&#27979;&#35270;&#39057;&#20013;&#20986;&#29616;&#30340;&#20010;&#20307;&#12290;&#20351;&#29992;&#25163;&#24037;&#29305;&#24449;&#20197;&#21450;&#19981;&#21516;&#30340;&#20998;&#31867;&#31639;&#27861;&#21644;&#22522;&#20110;ResNet&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20525;&#40657;&#29481;&#29481;&#35782;&#21035;&#30340;&#30740;&#31350;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#31163;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#24211;&#30340;&#25286;&#20998;&#19978;&#65292;&#20197;&#20998;&#31867;&#20934;&#30830;&#24230;&#20316;&#20026;&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25968;&#25454;&#20934;&#22791;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#38169;&#35823;&#30340;&#25968;&#25454;&#20998;&#31163;&#22914;&#20309;&#23548;&#33268;&#34394;&#20551;&#30340;&#22909;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#32463;&#36807;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#20998;&#31163;&#21518;&#65292;&#24471;&#21040;&#20102;&#26368;&#20339;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a bonobo detection and classification pipeline built from the commonly used machine learning methods. Such application is motivated by the need to test bonobos in their enclosure using touch screen devices without human assistance. This work introduces a newly acquired dataset based on bonobo recordings generated semi-automatically. The recordings are weakly labelled and fed to a macaque detector in order to spatially detect the individual present in the video. Handcrafted features coupled with different classification algorithms and deep-learning methods using a ResNet architecture are investigated for bonobo identification. Performance is compared in terms of classification accuracy on the splits of the database using different data separation methods. We demonstrate the importance of data preparation and how a wrong data separation can lead to false good results. Finally, after a meaningful separation of the data, the best classification performance is obtained u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#22914;&#20309;&#36890;&#36807;&#24494;&#23567;&#20462;&#25913;&#24178;&#25200;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#21457;&#29616;&#36825;&#21487;&#33021;&#26159;&#39640;&#32500;&#36755;&#20837;&#25968;&#25454;&#19979;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#23454;&#38469;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#34892;&#20026;&#65292;&#21253;&#25324;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#21516;&#26102;&#23545;&#38543;&#26426;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#39564;&#35777;&#23454;&#39564;&#36824;&#34920;&#26126;&#20102;&#30456;&#21516;&#29616;&#35937;&#22312;&#23454;&#38469;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2309.03665</link><description>&lt;p&gt;
&#22914;&#20309;&#25915;&#20987;&#21487;&#20197;&#24178;&#25200;&#30475;&#20284;&#31283;&#23450;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
How adversarial attacks can disrupt seemingly stable accurate classifiers. (arXiv:2309.03665v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#22914;&#20309;&#36890;&#36807;&#24494;&#23567;&#20462;&#25913;&#24178;&#25200;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#21457;&#29616;&#36825;&#21487;&#33021;&#26159;&#39640;&#32500;&#36755;&#20837;&#25968;&#25454;&#19979;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#23454;&#38469;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#34892;&#20026;&#65292;&#21253;&#25324;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#21516;&#26102;&#23545;&#38543;&#26426;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#39564;&#35777;&#23454;&#39564;&#36824;&#34920;&#26126;&#20102;&#30456;&#21516;&#29616;&#35937;&#22312;&#23454;&#38469;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#36890;&#36807;&#23545;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#65292;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#21407;&#26412;&#20934;&#30830;&#30340;&#23398;&#20064;&#31995;&#32479;&#30340;&#36755;&#20986;&#12290;&#20855;&#26377;&#35773;&#21050;&#24847;&#21619;&#30340;&#26159;&#65292;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#21363;&#20351;&#31995;&#32479;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#22823;&#24133;&#24230;&#38543;&#26426;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36755;&#20837;&#25968;&#25454;&#30340;&#23567;&#20247;&#12289;&#26131;&#20110;&#26500;&#36896;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#21487;&#33021;&#26159;&#39640;&#32500;&#36755;&#20837;&#25968;&#25454;&#19979;&#20998;&#31867;&#22120;&#30340;&#19968;&#20010;&#22522;&#26412;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36890;&#29992;&#24615;&#21644;&#26222;&#36866;&#24615;&#26694;&#26550;&#65292;&#20854;&#20013;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#34892;&#20026;&#20855;&#26377;&#39640;&#27010;&#29575;&#20986;&#29616;&#65292;&#23588;&#20854;&#26159;&#65288;&#21407;&#26412;&#20934;&#30830;&#30340;&#65289;&#27169;&#22411;&#23545;&#26131;&#20110;&#26500;&#36896;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#21516;&#26102;&#23481;&#26131;&#21463;&#21040;&#36755;&#20837;&#25968;&#25454;&#30340;&#38543;&#26426;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#19978;&#39564;&#35777;&#20102;&#30456;&#21516;&#29616;&#35937;&#22312;&#23454;&#38469;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30452;&#25509;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#20351;&#26159;&#22823;&#24133;&#24230;&#30340;&#21152;&#24615;&#38543;&#26426;&#22122;&#22768;&#20063;&#26080;&#27861;&#24178;&#25200;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks dramatically change the output of an otherwise accurate learning system using a seemingly inconsequential modification to a piece of input data. Paradoxically, empirical evidence indicates that even systems which are robust to large random perturbations of the input data remain susceptible to small, easily constructed, adversarial perturbations of their inputs. Here, we show that this may be seen as a fundamental feature of classifiers working with high dimensional input data. We introduce a simple generic and generalisable framework for which key behaviours observed in practical systems arise with high probability -- notably the simultaneous susceptibility of the (otherwise accurate) model to easily constructed adversarial attacks, and robustness to random perturbations of the input data. We confirm that the same phenomena are directly observed in practical neural networks trained on standard image classification problems, where even large additive random noise fai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25289;&#26364;&#20809;&#35889;&#21644;&#25299;&#25169;&#20998;&#26512;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#30830;&#35748;&#25110;&#21542;&#23450;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#20020;&#24202;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2309.03664</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#26426;&#22120;&#23398;&#20064;&#20174;&#33041;&#33034;&#28082;&#30340;&#25289;&#26364;&#20809;&#35889;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;
&lt;/p&gt;
&lt;p&gt;
Alzheimer Disease Detection from Raman Spectroscopy of the Cerebrospinal Fluid via Topological Machine Learning. (arXiv:2309.03664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25289;&#26364;&#20809;&#35889;&#21644;&#25299;&#25169;&#20998;&#26512;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#30830;&#35748;&#25110;&#21542;&#23450;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#20020;&#24202;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#25289;&#26364;&#20809;&#35889;&#20998;&#26512;&#20102;19&#21517;&#20020;&#24202;&#35786;&#26029;&#20026;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#21644;5&#21517;&#30149;&#29702;&#23545;&#29031;&#32773;&#30340;&#33041;&#33034;&#28082;&#65288;CSF&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21407;&#22987;&#21644;&#39044;&#22788;&#29702;&#25289;&#26364;&#20809;&#35889;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#21306;&#20998;AD&#21644;&#23545;&#29031;&#32452;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24212;&#29992;&#24120;&#35268;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24471;&#21040;&#20102;&#19981;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20174;&#21407;&#22987;&#20809;&#35889;&#25552;&#21462;&#30340;&#25299;&#25169;&#25551;&#36848;&#31526;&#38598;&#21512;&#65292;&#23454;&#29616;&#20102;&#38750;&#24120;&#22909;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65288;&gt;87%&#65289;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#21021;&#27493;&#30340;&#65292;&#20294;&#23427;&#20204;&#34920;&#26126;&#25289;&#26364;&#20809;&#35889;&#21644;&#25299;&#25169;&#20998;&#26512;&#30340;&#32467;&#21512;&#21487;&#33021;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#35748;&#25110;&#21542;&#23450;AD&#30340;&#20020;&#24202;&#35786;&#26029;&#12290;&#19979;&#19968;&#27493;&#23558;&#21253;&#25324;&#25193;&#22823;CSF&#26679;&#26412;&#25968;&#25454;&#38598;&#20197;&#26356;&#22909;&#22320;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#21487;&#33021;&#20102;&#35299;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26159;&#21542;&#33021;&#25903;&#25345;AD&#20122;&#22411;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cerebrospinal fluid (CSF) of 19 subjects who received a clinical diagnosis of Alzheimer's disease (AD) as well as of 5 pathological controls have been collected and analysed by Raman spectroscopy (RS). We investigated whether the raw and preprocessed Raman spectra could be used to distinguish AD from controls. First, we applied standard Machine Learning (ML) methods obtaining unsatisfactory results. Then, we applied ML to a set of topological descriptors extracted from raw spectra, achieving a very good classification accuracy (&gt;87%). Although our results are preliminary, they indicate that RS and topological analysis together may provide an effective combination to confirm or disprove a clinical diagnosis of AD. The next steps will include enlarging the dataset of CSF samples to validate the proposed method better and, possibly, to understand if topological data analysis could support the characterization of AD subtypes.
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#20013;&#30693;&#35782;&#33976;&#39311;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;25&#31181;&#33976;&#39311;&#25439;&#22833;&#39033;&#65292;&#24182;&#25351;&#20986;&#30001;&#20110;&#35757;&#32451;&#37197;&#32622;&#30340;&#24046;&#24322;&#23548;&#33268;&#26415;&#35821;&#27604;&#36739;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36229;&#21442;&#25968;&#36873;&#25321;&#19981;&#24403;&#20250;&#23548;&#33268;&#26497;&#31471;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.03659</link><description>&lt;p&gt;
&#22312;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#20013;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Towards Comparable Knowledge Distillation in Semantic Image Segmentation. (arXiv:2309.03659v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03659
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#20013;&#30693;&#35782;&#33976;&#39311;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;25&#31181;&#33976;&#39311;&#25439;&#22833;&#39033;&#65292;&#24182;&#25351;&#20986;&#30001;&#20110;&#35757;&#32451;&#37197;&#32622;&#30340;&#24046;&#24322;&#23548;&#33268;&#26415;&#35821;&#27604;&#36739;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36229;&#21442;&#25968;&#36873;&#25321;&#19981;&#24403;&#20250;&#23548;&#33268;&#26497;&#31471;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#35299;&#20915;&#35821;&#20041;&#20998;&#21106;&#20013;&#22823;&#27169;&#22411;&#23610;&#23544;&#21644;&#24930;&#25512;&#29702;&#36895;&#24230;&#30340;&#19968;&#31181;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#36807;&#21435;4&#24180;&#30340;14&#20010;&#20986;&#29256;&#29289;&#20013;&#37492;&#23450;&#20986;&#20102;25&#20010;&#25552;&#20986;&#30340;&#33976;&#39311;&#25439;&#22833;&#39033;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22522;&#20110;&#24050;&#21457;&#24067;&#32467;&#26524;&#30340;&#26415;&#35821;&#27604;&#36739;&#36890;&#24120;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#22240;&#20026;&#35757;&#32451;&#37197;&#32622;&#30340;&#24046;&#24322;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#26159;&#23545;&#27604;2022&#24180;&#30340;&#20004;&#20010;&#20986;&#29256;&#29289;&#12290;&#20351;&#29992;&#30456;&#21516;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#32467;&#26500;&#21644;&#32479;&#35745;&#32441;&#29702;&#33976;&#39311;&#65288;SSTKD&#65289;&#25253;&#21578;&#20102;&#23398;&#29983;mIoU&#22686;&#21152;&#20102;4.54&#20010;&#30334;&#20998;&#28857;&#65292;&#26368;&#32456;&#24615;&#33021;&#36798;&#21040;&#20102;29.19&#65292;&#32780;&#33258;&#36866;&#24212;&#36879;&#35270;&#33976;&#39311;&#65288;APD&#65289;&#20165;&#20165;&#25552;&#39640;&#20102;&#23398;&#29983;&#24615;&#33021;2.06&#20010;&#30334;&#20998;&#28857;&#65292;&#20294;&#23454;&#29616;&#20102;39.25&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;&#36825;&#31181;&#26497;&#31471;&#24046;&#24322;&#30340;&#21407;&#22240;&#36890;&#24120;&#26159;&#36229;&#21442;&#25968;&#30340;&#27425;&#20248;&#36873;&#25321;&#20197;&#21450;&#20316;&#20026;&#21442;&#32771;&#28857;&#30340;&#23398;&#29983;&#27169;&#22411;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36229;&#21442;&#25968;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD) is one proposed solution to large model sizes and slow inference speed in semantic segmentation. In our research we identify 25 proposed distillation loss terms from 14 publications in the last 4 years. Unfortunately, a comparison of terms based on published results is often impossible, because of differences in training configurations. A good illustration of this problem is the comparison of two publications from 2022. Using the same models and dataset, Structural and Statistical Texture Distillation (SSTKD) reports an increase of student mIoU of 4.54 and a final performance of 29.19, while Adaptive Perspective Distillation (APD) only improves student performance by 2.06 percentage points, but achieves a final performance of 39.25. The reason for such extreme differences is often a suboptimal choice of hyperparameters and a resulting underperformance of the student model used as reference point. In our work, we reveal problems of insufficient hyperparameter
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;GNN&#30340;Lipschitz&#30028;&#38480;&#34920;&#24449;&#20102;GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#21644;&#22266;&#26377;&#20559;&#20506;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#23545;&#20110;&#38480;&#21046;GNN&#36755;&#20986;&#30340;&#25200;&#21160;&#20197;&#20445;&#38556;&#20844;&#24179;&#24615;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03648</link><description>&lt;p&gt;
GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;Lipschitz&#29305;&#24615;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Characterizing Lipschitz Stability of GNN for Fairness. (arXiv:2309.03648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03648
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;GNN&#30340;Lipschitz&#30028;&#38480;&#34920;&#24449;&#20102;GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#21644;&#22266;&#26377;&#20559;&#20506;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#23545;&#20110;&#38480;&#21046;GNN&#36755;&#20986;&#30340;&#25200;&#21160;&#20197;&#20445;&#38556;&#20844;&#24179;&#24615;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lipschitz&#30028;&#38480;&#26159;&#20174;&#40065;&#26834;&#32479;&#35745;&#23398;&#20013;&#20511;&#37492;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#38480;&#21046;&#36755;&#20986;&#30456;&#23545;&#20110;&#36755;&#20837;&#30340;&#26368;&#22823;&#21464;&#21270;&#65292;&#32771;&#34385;&#21040;&#30456;&#20851;&#30340;&#38750;&#20851;&#38190;&#20559;&#20506;&#22240;&#32032;&#12290;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#26597;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#31283;&#23450;&#24615;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26368;&#36817;&#65292;&#23545;&#20110;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#19978;&#25805;&#20316;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#27809;&#26377;&#30740;&#31350;&#35843;&#26597;GNN&#30340;Lipschitz&#30028;&#38480;&#20197;&#25581;&#31034;&#27169;&#22411;&#36755;&#20986;&#30340;&#31283;&#23450;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#20855;&#26377;&#22266;&#26377;&#20559;&#20506;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26102;&#12290;&#30001;&#20110;&#24120;&#35265;&#22270;&#24418;&#25968;&#25454;&#22312;GNN&#35757;&#32451;&#20013;&#23384;&#22312;&#22266;&#26377;&#20559;&#24046;&#65292;&#36825;&#32473;&#38480;&#21046;&#30001;&#36755;&#20837;&#20559;&#24046;&#24341;&#36215;&#30340;GNN&#36755;&#20986;&#25200;&#21160;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#38556;&#20844;&#24179;&#24615;&#65292;&#24102;&#26469;&#20102;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#23613;&#31649;Lipschitz&#24120;&#25968;&#22312;&#25511;&#21046;&#27431;&#20960;&#37324;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#26041;&#38754;&#26377;&#25152;&#24212;&#29992;&#65292;&#20294;&#31934;&#30830;Lipschitz&#24120;&#25968;&#30340;&#35745;&#31639;&#21313;&#20998;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Lipschitz bound, a technique from robust statistics, can limit the maximum changes in the output concerning the input, taking into account associated irrelevant biased factors. It is an efficient and provable method for examining the output stability of machine learning models without incurring additional computation costs. Recently, Graph Neural Networks (GNNs), which operate on non-Euclidean data, have gained significant attention. However, no previous research has investigated the GNN Lipschitz bounds to shed light on stabilizing model outputs, especially when working on non-Euclidean data with inherent biases. Given the inherent biases in common graph data used for GNN training, it poses a serious challenge to constraining the GNN output perturbations induced by input biases, thereby safeguarding fairness during training. Recently, despite the Lipschitz constant's use in controlling the stability of Euclideanneural networks, the calculation of the precise Lipschitz constant rem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25193;&#23637;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;Transformer&#27169;&#22411;&#22312;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#20013;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#24182;&#25104;&#21151;&#35782;&#21035;&#20986;&#20102;&#19982;&#29983;&#29289;&#23398;&#21644;&#21270;&#23398;&#30456;&#20851;&#30340;&#24207;&#21015;&#37096;&#20998;&#65292;&#20026;&#34507;&#30333;&#36136;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#32447;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.03631</link><description>&lt;p&gt;
&#23545;&#20110;&#34507;&#30333;&#21151;&#33021;&#39044;&#27979;&#20013;Transformer&#27169;&#22411;&#20869;&#37096;&#36816;&#20316;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
Insights Into the Inner Workings of Transformer Models for Protein Function Prediction. (arXiv:2309.03631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25193;&#23637;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;Transformer&#27169;&#22411;&#22312;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#20013;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#24182;&#25104;&#21151;&#35782;&#21035;&#20986;&#20102;&#19982;&#29983;&#29289;&#23398;&#21644;&#21270;&#23398;&#30456;&#20851;&#30340;&#24207;&#21015;&#37096;&#20998;&#65292;&#20026;&#34507;&#30333;&#36136;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#65306;&#25105;&#20204;&#25506;&#32034;&#20102;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22914;&#20309;&#24110;&#21161;&#25581;&#31034;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#30340;&#20869;&#37096;&#36816;&#20316;&#65292;&#36890;&#36807;&#25193;&#23637;&#24191;&#27867;&#20351;&#29992;&#30340;XAI&#26041;&#27861;&#8212;&#8212;&#38598;&#25104;&#26799;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#26816;&#26597;&#35843;&#25972;&#20026;&#22522;&#22240;&#26412;&#20307;&#26415;&#35821;&#21644;&#37238;&#22996;&#21592;&#20250;&#32534;&#21495;&#39044;&#27979;&#30340;Transformer&#27169;&#22411;&#20869;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#32467;&#26524;&#65306;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#20986;&#21464;&#21387;&#22120;&#22312;&#24207;&#21015;&#20013;&#29305;&#21035;&#20851;&#27880;&#30340;&#27688;&#22522;&#37240;&#65292;&#24182;&#23637;&#31034;&#36825;&#20123;&#30456;&#20851;&#30340;&#24207;&#21015;&#37096;&#20998;&#21453;&#26144;&#20102;&#29983;&#29289;&#23398;&#21644;&#21270;&#23398;&#30340;&#39044;&#26399;&#65292;&#26080;&#35770;&#26159;&#22312;&#23884;&#20837;&#23618;&#36824;&#26159;&#27169;&#22411;&#20869;&#37096;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21464;&#21387;&#22120;&#22836;&#19982;&#22320;&#38754;&#30495;&#23454;&#24207;&#21015;&#27880;&#37322;&#65288;&#20363;&#22914;&#65292;&#36328;&#33180;&#21306;&#22495;&#65292;&#27963;&#24615;&#20301;&#28857;&#65289;&#20043;&#38388;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#23545;&#24212;&#30340;&#24402;&#22240;&#22270;&#30340;&#21464;&#21387;&#22120;&#22836;&#65292;&#36825;&#22312;&#22810;&#20010;&#34507;&#30333;&#36136;&#20013;&#37117;&#26377;&#20986;&#29616;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/markuswenzel/xai-proteins &#19978;&#33719;&#21462;&#21644;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: We explored how explainable AI (XAI) can help to shed light into the inner workings of neural networks for protein function prediction, by extending the widely used XAI method of integrated gradients such that latent representations inside of transformer models, which were finetuned to Gene Ontology term and Enzyme Commission number prediction, can be inspected too. Results: The approach enabled us to identify amino acids in the sequences that the transformers pay particular attention to, and to show that these relevant sequence parts reflect expectations from biology and chemistry, both in the embedding layer and inside of the model, where we identified transformer heads with a statistically significant correspondence of attribution maps with ground truth sequence annotations (e.g., transmembrane regions, active sites) across many proteins. Availability and Implementation: Source code can be accessed at https://github.com/markuswenzel/xai-proteins .
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26631;&#20934;&#21270;&#28508;&#21464;&#37327;&#21644;&#35843;&#25972;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Modified Barlow Twins (MBT) &#26041;&#27861;&#26469;&#25913;&#21892;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35821;&#38899;&#34920;&#31034;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#26377;&#38480;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#26102;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#21457;&#23637;&#21487;&#37325;&#29992;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2309.03619</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#21464;&#24615;&#21644;&#20887;&#20313;&#20943;&#23569;&#29702;&#35299;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35821;&#38899;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction. (arXiv:2309.03619v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03619
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26631;&#20934;&#21270;&#28508;&#21464;&#37327;&#21644;&#35843;&#25972;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Modified Barlow Twins (MBT) &#26041;&#27861;&#26469;&#25913;&#21892;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35821;&#38899;&#34920;&#31034;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#26377;&#38480;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#26102;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#21457;&#23637;&#21487;&#37325;&#29992;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#20989;&#25968;&#30340;&#36873;&#25321;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Barlow Twins (BT) &#30446;&#26631;&#30340;&#19981;&#21516;&#34920;&#36798;&#24418;&#24335;&#22914;&#20309;&#24433;&#21709;&#35821;&#38899;&#25968;&#25454;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#26631;&#20934;&#21270;&#28508;&#21464;&#37327;&#30340;Modified Barlow Twins (MBT) &#26469;&#24378;&#21046;&#23610;&#24230;&#19981;&#21464;&#65292;&#24182;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#24615;&#21035;&#35782;&#21035;&#21644;&#20851;&#38190;&#35789;&#26816;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;MBT&#22312;&#26377;&#38480;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#26102;&#65292;&#25913;&#21892;&#20102;&#34920;&#31034;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#30456;&#23545;&#20110;&#21407;&#22987;BT&#12290;&#36825;&#20984;&#26174;&#20102;&#35774;&#35745;&#40723;&#21169;&#19981;&#21464;&#24615;&#21644;&#21487;&#20256;&#36755;&#34920;&#31034;&#30340;&#30446;&#26631;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#35843;&#25972;BT&#23398;&#20064;&#30446;&#26631;&#20197;&#29983;&#25104;&#22312;&#36866;&#29992;&#20110;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#35821;&#38899;&#34920;&#31034;&#30340;&#35265;&#35299;&#12290;&#36825;&#39033;&#30740;&#31350;&#26159;&#21457;&#23637;&#21487;&#37325;&#29992;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The choice of the objective function is crucial in emerging high-quality representations from self-supervised learning. This paper investigates how different formulations of the Barlow Twins (BT) objective impact downstream task performance for speech data. We propose Modified Barlow Twins (MBT) with normalized latents to enforce scale-invariance and evaluate on speaker identification, gender recognition and keyword spotting tasks. Our results show MBT improves representation generalization over original BT, especially when fine-tuning with limited target data. This highlights the importance of designing objectives that encourage invariant and transferable representations. Our analysis provides insights into how the BT learning objective can be tailored to produce speech representations that excel when adapted to new downstream tasks. This study is an important step towards developing reusable self-supervised speech representations.
&lt;/p&gt;</description></item><item><title>&#36807;&#28388;&#34920;&#38754;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#22270;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#22788;&#29702;&#20381;&#36182;&#20110;&#36793;&#26435;&#37325;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#26102;&#20248;&#20110;&#20808;&#21069;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#24635;&#20307;&#26631;&#20934;&#24046;&#65292;&#24182;&#19988;&#35201;&#20040;&#23436;&#20840;&#26080;&#21442;&#25968;&#65292;&#35201;&#20040;&#26368;&#22810;&#21482;&#26377;&#19968;&#20010;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.03616</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#20998;&#31867;&#30340;&#36807;&#28388;&#34920;&#38754;
&lt;/p&gt;
&lt;p&gt;
Filtration Surfaces for Dynamic Graph Classification. (arXiv:2309.03616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03616
&lt;/p&gt;
&lt;p&gt;
&#36807;&#28388;&#34920;&#38754;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#22270;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#22788;&#29702;&#20381;&#36182;&#20110;&#36793;&#26435;&#37325;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#26102;&#20248;&#20110;&#20808;&#21069;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#24635;&#20307;&#26631;&#20934;&#24046;&#65292;&#24182;&#19988;&#35201;&#20040;&#23436;&#20840;&#26080;&#21442;&#25968;&#65292;&#35201;&#20040;&#26368;&#22810;&#21482;&#26377;&#19968;&#20010;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21160;&#24577;&#22270;&#20998;&#31867;&#26041;&#27861;&#35201;&#20040;&#23558;&#22270;&#20869;&#26680;&#25193;&#23637;&#21040;&#26102;&#38388;&#22495;&#65292;&#35201;&#20040;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#32447;&#26041;&#27861;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#26080;&#27861;&#22788;&#29702;&#19981;&#26029;&#21464;&#21270;&#30340;&#33410;&#28857;&#38598;&#65292;&#25110;&#32773;&#19981;&#33021;&#32771;&#34385;&#36793;&#26435;&#37325;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36807;&#28388;&#34920;&#38754;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#20197;&#20943;&#36731;&#19978;&#36848;&#38480;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#36807;&#28388;&#34920;&#38754;&#22312;&#20381;&#36182;&#36793;&#26435;&#37325;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23436;&#20840;&#26080;&#21442;&#25968;&#25110;&#26368;&#22810;&#19968;&#20010;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20135;&#29983;&#20102;&#26368;&#20302;&#30340;&#24635;&#20307;&#26631;&#20934;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches for classifying dynamic graphs either lift graph kernels to the temporal domain, or use graph neural networks (GNNs). However, current baselines have scalability issues, cannot handle a changing node set, or do not take edge weight information into account. We propose filtration surfaces, a novel method that is scalable and flexible, to alleviate said restrictions. We experimentally validate the efficacy of our model and show that filtration surfaces outperform previous state-of-the-art baselines on datasets that rely on edge weight information. Our method does so while being either completely parameter-free or having at most one parameter, and yielding the lowest overall standard deviation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;DCAuth&#21644;EISthentication&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21033;&#29992;&#30005;&#27744;&#30340;&#20869;&#37096;&#29305;&#24449;&#65292;&#25913;&#21892;&#20102;&#30005;&#27744;&#35748;&#35777;&#30340;&#25216;&#26415;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#37492;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#65292;&#20174;&#32780;&#38450;&#33539;&#20266;&#36896;&#30005;&#27744;&#30340;&#23433;&#20840;&#38544;&#24739;&#12290;</title><link>http://arxiv.org/abs/2309.03607</link><description>&lt;p&gt;
&#20320;&#30340;&#30005;&#27744;&#26377;&#24739;&#28779;&#28798;&#30340;&#38544;&#24739;&#65281;&#36890;&#36807;&#35748;&#35777;&#26469;&#38450;&#33539;&#20266;&#20882;&#30005;&#27744;&#12290;
&lt;/p&gt;
&lt;p&gt;
Your Battery Is a Blast! Safeguarding Against Counterfeit Batteries with Authentication. (arXiv:2309.03607v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03607
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;DCAuth&#21644;EISthentication&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21033;&#29992;&#30005;&#27744;&#30340;&#20869;&#37096;&#29305;&#24449;&#65292;&#25913;&#21892;&#20102;&#30005;&#27744;&#35748;&#35777;&#30340;&#25216;&#26415;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#37492;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#65292;&#20174;&#32780;&#38450;&#33539;&#20266;&#36896;&#30005;&#27744;&#30340;&#23433;&#20840;&#38544;&#24739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38043;&#37240;&#38146;&#65288;Li-ion&#65289;&#30005;&#27744;&#30001;&#20110;&#20854;&#39640;&#33021;&#37327;&#23494;&#24230;&#21644;&#21151;&#29575;&#23494;&#24230;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#25104;&#20026;&#20027;&#35201;&#21160;&#21147;&#28304;&#12290;2022&#24180;&#65292;&#20854;&#24066;&#22330;&#35268;&#27169;&#20272;&#35745;&#39640;&#36798;480&#20159;&#32654;&#20803;&#12290;&#28982;&#32780;&#65292;Li-ion&#30005;&#27744;&#30340;&#24191;&#27867;&#24212;&#29992;&#23548;&#33268;&#20102;&#20266;&#36896;&#30005;&#27744;&#30340;&#29983;&#20135;&#65292;&#32473;&#29992;&#25143;&#24102;&#26469;&#20102;&#23433;&#20840;&#38544;&#24739;&#12290;&#20266;&#36896;&#30005;&#27744;&#21487;&#33021;&#24341;&#21457;&#29190;&#28856;&#25110;&#28779;&#28798;&#65292;&#24182;&#19988;&#20854;&#22312;&#24066;&#22330;&#19978;&#30340;&#26222;&#36941;&#23384;&#22312;&#20351;&#24471;&#29992;&#25143;&#38590;&#20197;&#26816;&#27979;&#20551;&#20882;&#30005;&#27744;&#12290;&#30446;&#21069;&#30340;&#30005;&#27744;&#35748;&#35777;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#20808;&#36827;&#30340;&#20266;&#36896;&#25216;&#26415;&#30340;&#25915;&#20987;&#65292;&#24182;&#19988;&#36890;&#24120;&#26080;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#30005;&#27744;&#21644;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#26032;&#26041;&#27861;DCAuth&#21644;EISthentication&#65292;&#21033;&#29992;&#27599;&#20010;&#30005;&#27744;&#30340;&#20869;&#37096;&#29305;&#24449;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25913;&#21892;&#20102;&#30005;&#27744;&#35748;&#35777;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20174;&#30005;&#27744;&#30340;&#27491;&#24120;&#20351;&#29992;&#20013;&#33719;&#21462;&#30340;&#25968;&#25454;&#33258;&#21160;&#37492;&#21035;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#22411;&#21495;&#21644;&#26550;&#26500;&#65292;&#32780;&#19981;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
Lithium-ion (Li-ion) batteries are the primary power source in various applications due to their high energy and power density. Their market was estimated to be up to 48 billion U.S. dollars in 2022. However, the widespread adoption of Li-ion batteries has resulted in counterfeit cell production, which can pose safety hazards to users. Counterfeit cells can cause explosions or fires, and their prevalence in the market makes it difficult for users to detect fake cells. Indeed, current battery authentication methods can be susceptible to advanced counterfeiting techniques and are often not adaptable to various cells and systems. In this paper, we improve the state of the art on battery authentication by proposing two novel methodologies, DCAuth and EISthentication, which leverage the internal characteristics of each cell through Machine Learning models. Our methods automatically authenticate lithium-ion battery models and architectures using data from their regular usage without the need
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03581</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#22312;&#22810;&#30446;&#26631;&#38382;&#39064;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning. (arXiv:2309.03581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#23545;&#20110;&#21457;&#25381;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#29992;&#25143;&#36890;&#24120;&#23545;&#22810;&#30446;&#26631;&#38382;&#39064;&#24863;&#20852;&#36259;&#65292;&#21363;&#20248;&#21270;&#21487;&#33021;&#23384;&#22312;&#20914;&#31361;&#30340;&#30446;&#26631;&#65292;&#27604;&#22914;&#20934;&#30830;&#24615;&#21644;&#33021;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32477;&#22823;&#22810;&#25968;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23558;&#19968;&#32452;&#38750;&#25903;&#37197;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#36820;&#22238;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#36825;&#31181;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35780;&#20272;&#19968;&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#28041;&#21450;&#35780;&#20272;&#24471;&#21040;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#26377;&#19968;&#20123;&#25351;&#26631;&#21487;&#20197;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#23646;&#24615;&#65288;&#22914;&#20307;&#31215;&#12289;&#19982;&#21442;&#32771;&#28857;&#30340;&#25509;&#36817;&#31243;&#24230;&#65289;&#26469;&#35780;&#20272;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#65288;&#20363;&#22914;&#36229;&#20307;&#31215;&#12289;R2&#65289;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#65292;&#36873;&#25321;&#23548;&#33268;&#26399;&#26395;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#25351;&#26631;&#21487;&#33021;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#38024;&#23545;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is important to leverage the full potential of machine learning (ML). In practice, users are often interested in multi-objective (MO) problems, i.e., optimizing potentially conflicting objectives, like accuracy and energy consumption. To tackle this, the vast majority of MO-ML algorithms return a Pareto front of non-dominated machine learning models to the user. Optimizing the hyperparameters of such algorithms is non-trivial as evaluating a hyperparameter configuration entails evaluating the quality of the resulting Pareto front. In literature, there are known indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by quantifying different properties (e.g., volume, proximity to a reference point). However, choosing the indicator that leads to the desired Pareto front might be a hard task for a user. In this paper, we propose a human-centered interactive HPO approach tailored towards multi-objective ML leveraging preference learnin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DTW+S&#30340;&#26032;&#22411;&#27979;&#37327;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21019;&#24314;&#23616;&#37096;&#36235;&#21183;&#30340;&#30697;&#38453;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#35745;&#31639;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#23616;&#37096;&#36235;&#21183;&#30456;&#20284;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03579</link><description>&lt;p&gt;
DTW+S: &#20351;&#29992;&#26377;&#24207;&#23616;&#37096;&#36235;&#21183;&#36827;&#34892;&#22522;&#20110;&#24418;&#29366;&#30340;&#26102;&#38388;&#24207;&#21015;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend. (arXiv:2309.03579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03579
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DTW+S&#30340;&#26032;&#22411;&#27979;&#37327;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21019;&#24314;&#23616;&#37096;&#36235;&#21183;&#30340;&#30697;&#38453;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#35745;&#31639;&#36317;&#31163;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#23616;&#37096;&#36235;&#21183;&#30456;&#20284;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#36317;&#31163;&#25110;&#30456;&#20284;&#24230;&#30340;&#27979;&#37327;&#26159;&#35768;&#22810;&#24212;&#29992;&#21253;&#25324;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#22522;&#26412;&#26041;&#38754;&#12290;&#29616;&#26377;&#30340;&#27979;&#37327;&#26041;&#27861;&#21487;&#33021;&#30001;&#20110;&#23616;&#37096;&#36235;&#21183;&#65288;&#24418;&#29366;&#65289;&#32780;&#26080;&#27861;&#25429;&#25417;&#21040;&#30456;&#20284;&#20043;&#22788;&#65292;&#29978;&#33267;&#21487;&#33021;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#23547;&#25214;&#22312;&#30456;&#20284;&#26102;&#38388;&#21608;&#22260;&#21457;&#29983;&#30340;&#30456;&#20284;&#36235;&#21183;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#24212;&#29992;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26131;&#20110;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#36825;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#26377;&#24207;&#30340;&#26377;&#24847;&#20041;&#30340;&#23616;&#37096;&#36235;&#21183;&#24207;&#21015;&#30340;&#24212;&#29992;&#29305;&#21035;&#26377;&#29992;&#65292;&#20363;&#22914;&#22312;&#27969;&#34892;&#30149;&#20013;&#65288;&#20174;&#22686;&#38271;&#21040;&#23792;&#20540;&#20877;&#21040;&#20943;&#23569;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;DTW+S&#65292;&#23427;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#8220;&#20445;&#25345;&#25509;&#36817;&#24615;&#8221;&#30340;&#30697;&#38453;&#34920;&#31034;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#20013;&#27599;&#19968;&#21015;&#20195;&#34920;&#23616;&#37096;&#36235;&#21183;&#65292;&#28982;&#21518;&#24212;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#35745;&#31639;&#36825;&#20123;&#30697;&#38453;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25903;&#25345;&#36825;&#31181;&#34920;&#31034;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DTW+S&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring distance or similarity between time-series data is a fundamental aspect of many applications including classification and clustering. Existing measures may fail to capture similarities due to local trends (shapes) and may even produce misleading results. Our goal is to develop a measure that looks for similar trends occurring around similar times and is easily interpretable for researchers in applied domains. This is particularly useful for applications where time-series have a sequence of meaningful local trends that are ordered, such as in epidemics (a surge to an increase to a peak to a decrease). We propose a novel measure, DTW+S, which creates an interpretable "closeness-preserving" matrix representation of the time-series, where each column represents local trends, and then it applies Dynamic Time Warping to compute distances between these matrices. We present a theoretical analysis that supports the choice of this representation. We demonstrate the utility of DTW+S in 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#36710;&#32852;&#32593;&#29289;&#20307;&#26816;&#27979;&#20013;&#37319;&#29992;&#31232;&#30095;&#32852;&#21512;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#20849;&#20139;&#26412;&#22320;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#65292;&#21487;&#20197;&#25552;&#39640;&#36793;&#32536;&#35774;&#22791;&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#20943;&#23569;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2309.03569</link><description>&lt;p&gt;
&#31232;&#30095;&#32852;&#21512;&#35757;&#32451;&#22312;&#36710;&#32852;&#32593;&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sparse Federated Training of Object Detection in the Internet of Vehicles. (arXiv:2309.03569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03569
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#36710;&#32852;&#32593;&#29289;&#20307;&#26816;&#27979;&#20013;&#37319;&#29992;&#31232;&#30095;&#32852;&#21512;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#20849;&#20139;&#26412;&#22320;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#31232;&#30095;&#35757;&#32451;&#65292;&#21487;&#20197;&#25552;&#39640;&#36793;&#32536;&#35774;&#22791;&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#20943;&#23569;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36710;&#32852;&#32593;&#65288;IoV&#65289;&#22312;&#32531;&#35299;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#29289;&#20307;&#26816;&#27979;&#26159;IoV&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#65292;&#36890;&#36807;&#20998;&#26512;&#21450;&#26102;&#25935;&#24863;&#30340;&#36710;&#36742;&#30456;&#20851;&#20449;&#24687;&#65292;&#24191;&#27867;&#29992;&#20110;&#25552;&#20379;&#20132;&#36890;&#31649;&#29702;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#22823;&#22810;&#22522;&#20110;&#38598;&#20013;&#24335;&#28145;&#24230;&#35757;&#32451;&#65292;&#21363;&#36793;&#32536;&#35774;&#22791;&#33719;&#21462;&#30340;&#25935;&#24863;&#25968;&#25454;&#38656;&#35201;&#19978;&#20256;&#21040;&#26381;&#21153;&#22120;&#65292;&#24341;&#21457;&#20102;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#20943;&#36731;&#27492;&#38544;&#31169;&#27844;&#38706;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22522;&#20110;&#32852;&#21512;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#20849;&#20139;&#35757;&#32451;&#33391;&#22909;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36793;&#32536;&#35774;&#22791;&#36890;&#24120;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#65292;&#24182;&#19988;&#36710;&#32852;&#32593;&#23545;&#20302;&#24310;&#36831;&#26377;&#20005;&#26684;&#35201;&#27714;&#65292;&#22240;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#31232;&#30095;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#27169;&#22411;&#36127;&#25285;&#65292;&#24182;&#30830;&#20445;&#20854;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#20174;&#32780;&#20943;&#23569;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an essential component part of the Intelligent Transportation System (ITS), the Internet of Vehicles (IoV) plays a vital role in alleviating traffic issues. Object detection is one of the key technologies in the IoV, which has been widely used to provide traffic management services by analyzing timely and sensitive vehicle-related information. However, the current object detection methods are mostly based on centralized deep training, that is, the sensitive data obtained by edge devices need to be uploaded to the server, which raises privacy concerns. To mitigate such privacy leakage, we first propose a federated learning-based framework, where well-trained local models are shared in the central server. However, since edge devices usually have limited computing power, plus a strict requirement of low latency in IoVs, we further propose a sparse training process on edge devices, which can effectively lighten the model, and ensure its training efficiency on edge devices, thereby reduc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#32467;&#26524;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.03564</link><description>&lt;p&gt;
&#35780;&#20272;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media. (arXiv:2309.03564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#32467;&#26524;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#31867;&#20284;&#24555;&#36895;&#21457;&#23637;&#30340;GPT&#31995;&#21015;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24433;&#21709;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#24515;&#29702;&#23398;&#31561;&#21307;&#23398;&#39046;&#22495;&#23545;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#27987;&#21402;&#20852;&#36259;&#65292;&#20294;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#20855;&#20307;&#25506;&#32034;&#20173;&#28982;&#24456;&#23569;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#29992;&#25143;&#36234;&#26469;&#36234;&#22810;&#22320;&#34920;&#36798;&#20010;&#20154;&#24773;&#24863;&#65307;&#22312;&#29305;&#23450;&#30340;&#20027;&#39064;&#19979;&#65292;&#36825;&#20123;&#24773;&#24863;&#36890;&#24120;&#34920;&#29616;&#20026;&#28040;&#26497;&#24773;&#32490;&#65292;&#26377;&#26102;&#20250;&#21319;&#32423;&#20026;&#33258;&#26432;&#20542;&#21521;&#12290;&#21450;&#26102;&#36776;&#35782;&#36825;&#26679;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#23545;&#26377;&#25928;&#24178;&#39044;&#21644;&#28508;&#22312;&#36991;&#20813;&#20005;&#37325;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#36827;&#34892;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#33258;&#26432;&#39118;&#38505;&#21644;&#35748;&#30693;&#20559;&#24046;&#35782;&#21035;&#30340;&#23454;&#39564;&#65292;&#36827;&#20837;&#20102;&#36825;&#20010;&#39046;&#22495;&#12290;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#65306;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#24494;&#35843;&#65292;&#32771;&#23519;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models, particularly those akin to the rapidly progressing GPT series, are gaining traction for their expansive influence. While there is keen interest in their applicability within medical domains such as psychology, tangible explorations on real-world data remain scant. Concurrently, users on social media platforms are increasingly vocalizing personal sentiments; under specific thematic umbrellas, these sentiments often manifest as negative emotions, sometimes escalating to suicidal inclinations. Timely discernment of such cognitive distortions and suicidal risks is crucial to effectively intervene and potentially avert dire circumstances. Our study ventured into this realm by experimenting on two pivotal tasks: suicidal risk and cognitive distortion identification on Chinese social media platforms. Using supervised learning as a baseline, we examined and contrasted the efficacy of large language models via three distinct strategies: zero-shot, few-shot, and fine-tunin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#19977;&#20540;&#20915;&#31574;&#26641;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#20915;&#31574;&#26641;&#22312;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#31639;&#27861;&#19981;&#20551;&#35774;&#32570;&#22833;&#20540;&#21253;&#21547;&#20219;&#20309;&#20851;&#20110;&#21709;&#24212;&#30340;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#29305;&#23450;&#32570;&#22833;&#25968;&#25454;&#22330;&#26223;&#19979;&#65292;&#19977;&#20540;&#20915;&#31574;&#26641;&#22312;MCAR&#35774;&#32622;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22312;IM&#35774;&#32622;&#20013;&#30053;&#36874;&#19968;&#31609;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#19977;&#20540;&#20915;&#31574;&#26641;&#19982;&#32570;&#22833;&#22312;&#23646;&#24615;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#35757;&#32451;&#36895;&#24230;&#36739;&#24930;&#65292;&#20294;&#19977;&#20540;&#20915;&#31574;&#26641;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#19988;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03561</link><description>&lt;p&gt;
&#32570;&#22833;&#20540;&#22788;&#29702;&#30340;&#19977;&#20540;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Trinary Decision Trees for missing value handling. (arXiv:2309.03561v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#19977;&#20540;&#20915;&#31574;&#26641;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#20915;&#31574;&#26641;&#22312;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#26102;&#30340;&#34920;&#29616;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#31639;&#27861;&#19981;&#20551;&#35774;&#32570;&#22833;&#20540;&#21253;&#21547;&#20219;&#20309;&#20851;&#20110;&#21709;&#24212;&#30340;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#29305;&#23450;&#32570;&#22833;&#25968;&#25454;&#22330;&#26223;&#19979;&#65292;&#19977;&#20540;&#20915;&#31574;&#26641;&#22312;MCAR&#35774;&#32622;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22312;IM&#35774;&#32622;&#20013;&#30053;&#36874;&#19968;&#31609;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#19977;&#20540;&#20915;&#31574;&#26641;&#19982;&#32570;&#22833;&#22312;&#23646;&#24615;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#35757;&#32451;&#36895;&#24230;&#36739;&#24930;&#65292;&#20294;&#19977;&#20540;&#20915;&#31574;&#26641;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#19988;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19977;&#20540;&#20915;&#31574;&#26641;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#25913;&#21892;&#20915;&#31574;&#26641;&#22238;&#24402;&#22120;&#21644;&#20998;&#31867;&#22120;&#20013;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#31639;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#19977;&#20540;&#20915;&#31574;&#26641;&#19981;&#20551;&#35774;&#32570;&#22833;&#20540;&#21253;&#21547;&#26377;&#20851;&#21709;&#24212;&#30340;&#20219;&#20309;&#20449;&#24687;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#21644;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#31034;&#20363;&#65292;&#27604;&#36739;&#20102;&#20854;&#22312;&#19981;&#21516;&#32570;&#22833;&#25968;&#25454;&#22330;&#26223;&#65288;&#23436;&#20840;&#38543;&#26426;&#32570;&#22833;&#65288;MCAR&#65289;&#21644;&#20449;&#24687;&#24615;&#32570;&#22833;&#65288;IM&#65289;&#65289;&#20013;&#19982;&#24050;&#24314;&#31435;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;MCAR&#35774;&#32622;&#20013;&#65292;&#19977;&#20540;&#26641;&#22312;&#21482;&#26377;&#26679;&#26412;&#22806;&#32570;&#22833;&#25968;&#25454;&#26102;&#34920;&#29616;&#20248;&#20110;&#20854;&#21516;&#34892;&#65292;&#32780;&#22312;IM&#35774;&#32622;&#20013;&#33853;&#21518;&#12290;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#21363;&#19977;&#20540;&#32570;&#22833;&#22312;&#23646;&#24615;&#65288;MIA&#65289;&#26041;&#27861;&#21644;&#19977;&#20540;&#26641;&#30456;&#32467;&#21512;&#30340;TrinaryMIA&#26641;&#65292;&#22312;&#25152;&#26377;&#32570;&#22833;&#31867;&#22411;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#35757;&#32451;&#36895;&#24230;&#36739;&#24930;&#21487;&#33021;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#32570;&#28857;&#65292;&#20294;&#19977;&#20540;&#20915;&#31574;&#26641;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#19988;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Trinary decision tree, an algorithm designed to improve the handling of missing data in decision tree regressors and classifiers. Unlike other approaches, the Trinary decision tree does not assume that missing values contain any information about the response. Both theoretical calculations on estimator bias and numerical illustrations using real data sets are presented to compare its performance with established algorithms in different missing data scenarios (Missing Completely at Random (MCAR), and Informative Missingness (IM)). Notably, the Trinary tree outperforms its peers in MCAR settings, especially when data is only missing out-of-sample, while lacking behind in IM settings. A hybrid model, the TrinaryMIA tree, which combines the Trinary tree and the Missing In Attributes (MIA) approach, shows robust performance in all types of missingness. Despite the potential drawback of slower training speed, the Trinary tree offers a promising and more accurate met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#28388;&#27874;/&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.03557</link><description>&lt;p&gt;
&#35770;&#22810;&#26234;&#33021;&#20307;&#38750;&#32447;&#24615;&#28388;&#27874;&#21644;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
On the dynamics of multi agent nonlinear filtering and learning. (arXiv:2309.03557v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#28388;&#27874;/&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#36890;&#36807;&#20998;&#25955;&#19968;&#33268;&#24615;&#23547;&#27714;&#21160;&#21147;&#23398;&#26469;&#23436;&#25104;&#39640;&#24230;&#22797;&#26434;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#20854;&#22312;&#20449;&#21495;&#22788;&#29702;&#21644;&#35745;&#31639;&#26234;&#33021;&#31038;&#21306;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#28388;&#27874;/&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#31995;&#32479;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#30340;&#19968;&#33324;&#34920;&#36848;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#29616;&#21327;&#21516;&#23398;&#20064;&#34892;&#20026;&#30340;&#26465;&#20214;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36824;&#20171;&#32461;&#20102;&#35813;&#25512;&#23548;&#26694;&#26550;&#22312;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiagent systems aim to accomplish highly complex learning tasks through decentralised consensus seeking dynamics and their use has garnered a great deal of attention in the signal processing and computational intelligence societies. This article examines the behaviour of multiagent networked systems with nonlinear filtering/learning dynamics. To this end, a general formulation for the actions of an agent in multiagent networked systems is presented and conditions for achieving a cohesive learning behaviour is given. Importantly, application of the so derived framework in distributed and federated learning scenarios are presented.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MVD&#21644;MVDA&#20004;&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22768;&#23398;&#20132;&#36890;&#30417;&#27979;&#21644;&#36710;&#36742;&#31867;&#22411;&#20998;&#31867;&#31639;&#27861;&#30340;&#24320;&#21457;&#12290;&#36890;&#36807;&#20351;&#29992;&#20498;&#35889;&#21644;&#39057;&#35889;&#31561;&#29305;&#24449;&#20197;&#21450;&#22810;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#20998;&#31867;&#31227;&#21160;&#36710;&#36742;&#30340;&#22768;&#38899;&#20449;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.03544</link><description>&lt;p&gt;
MVD&#65306;&#19968;&#31181;&#29992;&#20110;&#22768;&#23398;&#36710;&#36742;&#31867;&#22411;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MVD:A Novel Methodology and Dataset for Acoustic Vehicle Type Classification. (arXiv:2309.03544v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MVD&#21644;MVDA&#20004;&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22768;&#23398;&#20132;&#36890;&#30417;&#27979;&#21644;&#36710;&#36742;&#31867;&#22411;&#20998;&#31867;&#31639;&#27861;&#30340;&#24320;&#21457;&#12290;&#36890;&#36807;&#20351;&#29992;&#20498;&#35889;&#21644;&#39057;&#35889;&#31561;&#29305;&#24449;&#20197;&#21450;&#22810;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#20998;&#31867;&#31227;&#21160;&#36710;&#36742;&#30340;&#22768;&#38899;&#20449;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#26029;&#22686;&#38271;&#30340;&#22478;&#24066;&#20154;&#21475;&#23548;&#33268;&#36710;&#36742;&#20351;&#29992;&#28608;&#22686;&#65292;&#20132;&#36890;&#30417;&#27979;&#21644;&#31649;&#29702;&#21464;&#24471;&#19981;&#21487;&#25110;&#32570;&#12290;&#22768;&#23398;&#20132;&#36890;&#30417;&#27979;&#65288;ATM&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#21487;&#20197;&#26367;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#31561;&#35745;&#31639;&#25104;&#26412;&#26356;&#39640;&#30340;&#20132;&#36890;&#30417;&#27979;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MVD&#21644;MVDA&#20004;&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22768;&#23398;&#20132;&#36890;&#30417;&#27979;&#21644;&#36710;&#36742;&#31867;&#22411;&#20998;&#31867;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#20854;&#20013;&#21253;&#21547;&#31227;&#21160;&#36710;&#36742;&#30340;&#38899;&#39057;&#35760;&#24405;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#22235;&#20010;&#31867;&#21035;-&#21345;&#36710;&#12289;&#27773;&#36710;&#12289;&#25705;&#25176;&#36710;&#21644;&#26080;&#36710;&#36742;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#20498;&#35889;&#21644;&#39057;&#35889;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#38899;&#39057;&#29305;&#24449;&#20197;&#21450;&#22810;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#26469;&#20934;&#30830;&#20998;&#31867;&#36825;&#20123;&#22768;&#38899;&#20449;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#20197;&#21069;&#24037;&#20316;&#30340;&#22522;&#32447;&#65292;&#24182;&#22312;MVD&#21644;MVDA&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;91.98&#65285;&#21644;96.66&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rising urban populations have led to a surge in vehicle use and made traffic monitoring and management indispensable. Acoustic traffic monitoring (ATM) offers a cost-effective and efficient alternative to more computationally expensive methods of monitoring traffic such as those involving computer vision technologies. In this paper, we present MVD and MVDA: two open datasets for the development of acoustic traffic monitoring and vehicle-type classification algorithms, which contain audio recordings of moving vehicles. The dataset contain four classes- Trucks, Cars, Motorbikes, and a No-vehicle class. Additionally, we propose a novel and efficient way to accurately classify these acoustic signals using cepstrum and spectrum based local and global audio features, and a multi-input neural network. Experimental results show that our methodology improves upon the established baselines of previous works and achieves an accuracy of 91.98% and 96.66% on MVD and MVDA Datasets, respectively. Fin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23376;&#22270;&#30340;&#32039;&#26694;&#26550;&#26500;&#36896;&#26041;&#27861;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#35843;&#25972;&#26694;&#26550;&#30340;&#28040;&#22833;&#30697;&#21644;&#20854;&#20182;&#23646;&#24615;&#65292;&#23454;&#29616;&#23545;&#20855;&#26377;&#36335;&#24452;&#25903;&#25345;&#30340;&#22270;&#20449;&#21495;&#30340;&#39640;&#25928;&#34920;&#31034;&#65292;&#22312;&#38750;&#32447;&#24615;&#36924;&#36817;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03537</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#22270;&#30340;&#32039;&#26694;&#26550;&#22312;&#20855;&#26377;&#32039;&#33268;&#25903;&#25345;&#21644;&#28176;&#28040;&#30952;&#30340;&#22270;&#19978;
&lt;/p&gt;
&lt;p&gt;
Subgraph-based Tight Frames on Graphs with Compact Supports and Vanishing Moments. (arXiv:2309.03537v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23376;&#22270;&#30340;&#32039;&#26694;&#26550;&#26500;&#36896;&#26041;&#27861;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#35843;&#25972;&#26694;&#26550;&#30340;&#28040;&#22833;&#30697;&#21644;&#20854;&#20182;&#23646;&#24615;&#65292;&#23454;&#29616;&#23545;&#20855;&#26377;&#36335;&#24452;&#25903;&#25345;&#30340;&#22270;&#20449;&#21495;&#30340;&#39640;&#25928;&#34920;&#31034;&#65292;&#22312;&#38750;&#32447;&#24615;&#36924;&#36817;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#19968;&#31995;&#21015;&#20998;&#23618;&#20998;&#21306;&#26500;&#24314;&#20855;&#26377;&#32039;&#33268;&#25903;&#25345;&#30340;&#22270;&#19978;&#30340;&#32039;&#26694;&#26550;&#12290;&#20174;&#25105;&#20204;&#30340;&#25277;&#35937;&#26500;&#36896;&#24320;&#22987;&#65292;&#25105;&#20204;&#33021;&#22815;&#28789;&#27963;&#22320;&#23558;&#23376;&#22270;Laplacians&#32435;&#20837;&#21040;&#25105;&#20204;&#30340;&#22270;&#26694;&#26550;&#35774;&#35745;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#36890;&#29992;&#26041;&#27861;&#20801;&#35768;&#35843;&#25972;&#26694;&#26550;&#30340;&#65288;&#23376;&#22270;&#65289;&#28040;&#22833;&#30697;&#21644;&#20854;&#20182;&#23646;&#24615;&#65292;&#22914;&#26041;&#21521;&#24615;&#65292;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#20855;&#26377;&#36335;&#24452;&#25903;&#25345;&#30340;&#22270;&#20449;&#21495;&#12290;&#25105;&#20204;&#26126;&#30830;&#23450;&#20041;&#24182;&#27979;&#35797;&#20102;&#20960;&#20010;&#21464;&#20307;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22270;&#26694;&#26550;&#22312;&#38750;&#32447;&#24615;&#36924;&#36817;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we proposed a novel and general method to construct tight frames on graphs with compact supports based on a series of hierarchical partitions. Starting from our abstract construction that generalizes previous methods based on partition trees, we are able to flexibly incorporate subgraph Laplacians into our design of graph frames. Consequently, our general methods permit adjusting the (subgraph) vanishing moments of the framelets and extra properties, such as directionality, for efficiently representing graph signals with path-like supports. Several variants are explicitly defined and tested. Experimental results show our proposed graph frames perform superiorly in non-linear approximation tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#22686;&#24378;&#20998;&#21106;&#32593;&#32476;&#65288;FES-Net&#65289;&#65292;&#29992;&#20110;&#31934;&#30830;&#22320;&#20998;&#21106;&#35270;&#32593;&#33180;&#34880;&#31649;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#25552;&#31034;&#21367;&#31215;&#22359;&#36827;&#34892;&#19979;&#37319;&#26679;&#65292;&#24182;&#37319;&#29992;&#27973;&#23618;&#19978;&#37319;&#26679;&#26041;&#27861;&#29983;&#25104;&#27599;&#20010;&#31867;&#21035;&#30340;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20854;&#20182;&#22270;&#20687;&#22686;&#24378;&#27493;&#39588;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.03535</link><description>&lt;p&gt;
&#29305;&#24449;&#22686;&#24378;&#20998;&#21106;&#32593;&#32476;&#65288;FES-Net&#65289;&#29992;&#20110;&#34880;&#31649;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Feature Enhancer Segmentation Network (FES-Net) for Vessel Segmentation. (arXiv:2309.03535v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#22686;&#24378;&#20998;&#21106;&#32593;&#32476;&#65288;FES-Net&#65289;&#65292;&#29992;&#20110;&#31934;&#30830;&#22320;&#20998;&#21106;&#35270;&#32593;&#33180;&#34880;&#31649;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#25552;&#31034;&#21367;&#31215;&#22359;&#36827;&#34892;&#19979;&#37319;&#26679;&#65292;&#24182;&#37319;&#29992;&#27973;&#23618;&#19978;&#37319;&#26679;&#26041;&#27861;&#29983;&#25104;&#27599;&#20010;&#31867;&#21035;&#30340;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20854;&#20182;&#22270;&#20687;&#22686;&#24378;&#27493;&#39588;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#21644;&#32769;&#24180;&#24615;&#40644;&#26001;&#21464;&#24615;&#31561;&#30142;&#30149;&#23545;&#35270;&#21147;&#26500;&#25104;&#37325;&#22823;&#21361;&#38505;&#65292;&#24378;&#35843;&#20102;&#23545;&#35270;&#32593;&#33180;&#34880;&#31649;&#36827;&#34892;&#31934;&#30830;&#20998;&#21106;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#20415;&#36319;&#36394;&#21644;&#35786;&#26029;&#36827;&#23637;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20381;&#36182;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#30340;&#34880;&#31649;&#20998;&#21106;&#26041;&#27861;&#22312;&#25429;&#25417;&#35270;&#32593;&#33180;&#34880;&#31649;&#37197;&#32622;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23548;&#33268;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#29305;&#24449;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#38590;&#20197;&#35843;&#21644;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#22686;&#24378;&#20998;&#21106;&#32593;&#32476;&#65288;FES-Net&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#22270;&#20687;&#22686;&#24378;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#31934;&#30830;&#30340;&#20687;&#32032;&#32423;&#20998;&#21106;&#12290;FES-Net&#30452;&#25509;&#22788;&#29702;&#36755;&#20837;&#22270;&#20687;&#65292;&#24182;&#22312;&#19979;&#37319;&#26679;&#36807;&#31243;&#20013;&#21033;&#29992;&#22235;&#20010;&#25552;&#31034;&#21367;&#31215;&#22359;&#65288;PCBs&#65289;&#65292;&#24182;&#32467;&#21512;&#27973;&#23618;&#19978;&#37319;&#26679;&#26041;&#27861;&#20026;&#27599;&#20010;&#31867;&#21035;&#29983;&#25104;&#20108;&#36827;&#21046;&#25513;&#30721;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20844;&#24320;&#30340;&#26368;&#20808;&#36827;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;FES-Net&#30340;&#24615;&#33021;&#65306;DRIVE, STARE, CH
&lt;/p&gt;
&lt;p&gt;
Diseases such as diabetic retinopathy and age-related macular degeneration pose a significant risk to vision, highlighting the importance of precise segmentation of retinal vessels for the tracking and diagnosis of progression. However, existing vessel segmentation methods that heavily rely on encoder-decoder structures struggle to capture contextual information about retinal vessel configurations, leading to challenges in reconciling semantic disparities between encoder and decoder features. To address this, we propose a novel feature enhancement segmentation network (FES-Net) that achieves accurate pixel-wise segmentation without requiring additional image enhancement steps. FES-Net directly processes the input image and utilizes four prompt convolutional blocks (PCBs) during downsampling, complemented by a shallow upsampling approach to generate a binary mask for each class. We evaluate the performance of FES-Net on four publicly available state-of-the-art datasets: DRIVE, STARE, CH
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#37096;&#20998;&#39046;&#22495;&#36866;&#24212;&#65288;PDA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#40065;&#26834;&#30340;&#30446;&#26631;&#30417;&#30563;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#39046;&#22495;&#26080;&#20851;&#30340;&#26041;&#24335;&#19979;&#20248;&#21270;&#20102;&#31867;&#20869;&#32039;&#20945;&#24615;&#21644;&#31867;&#38388;&#20998;&#31163;&#24615;&#12290;&#36890;&#36807;&#39044;&#20808;&#25512;&#26029;&#28304;&#21407;&#22411;&#65292;&#30830;&#20445;&#20102;&#28304;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03531</link><description>&lt;p&gt;
&#20351;&#29992;&#28304;&#21407;&#22411;&#30340;&#24378;&#22823;&#36127;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#34892;&#37096;&#20998;&#39046;&#22495;&#36866;&#24212;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Robust Negative Learning Approach to Partial Domain Adaptation Using Source Prototypes. (arXiv:2309.03531v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#37096;&#20998;&#39046;&#22495;&#36866;&#24212;&#65288;PDA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#40065;&#26834;&#30340;&#30446;&#26631;&#30417;&#30563;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#24182;&#22312;&#39046;&#22495;&#26080;&#20851;&#30340;&#26041;&#24335;&#19979;&#20248;&#21270;&#20102;&#31867;&#20869;&#32039;&#20945;&#24615;&#21644;&#31867;&#38388;&#20998;&#31163;&#24615;&#12290;&#36890;&#36807;&#39044;&#20808;&#25512;&#26029;&#28304;&#21407;&#22411;&#65292;&#30830;&#20445;&#20102;&#28304;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#37096;&#20998;&#39046;&#22495;&#36866;&#24212;&#65288;PDA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#40065;&#26834;&#30340;&#30446;&#26631;&#30417;&#30563;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#38598;&#25104;&#23398;&#20064;&#21644;&#21253;&#21547;&#22810;&#26679;&#21270;&#12289;&#20114;&#34917;&#30340;&#26631;&#31614;&#21453;&#39304;&#65292;&#20943;&#36731;&#20102;&#38169;&#35823;&#21453;&#39304;&#30340;&#24433;&#21709;&#65292;&#24182;&#20419;&#36827;&#20266;&#26631;&#31614;&#30340;&#25913;&#36827;&#12290;&#19982;&#20165;&#20381;&#36182;&#20998;&#24067;&#23545;&#40784;&#30340;&#19968;&#38454;&#30697;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#28304;&#21407;&#22411;&#21644;&#39640;&#21487;&#20449;&#30340;&#30446;&#26631;&#26679;&#26412;&#65292;&#22312;&#39046;&#22495;&#26080;&#20851;&#30340;&#26041;&#24335;&#19979;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#31867;&#20869;&#32039;&#20945;&#24615;&#21644;&#31867;&#38388;&#20998;&#31163;&#24615;&#30340;&#26126;&#30830;&#30446;&#26631;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#39044;&#20808;&#25512;&#26029;&#28304;&#21407;&#22411;&#65292;&#30830;&#20445;&#20102;&#28304;&#25968;&#25454;&#30340;&#38544;&#31169;&#24615;&#65292;&#22312;&#36866;&#24212;&#38454;&#27573;&#26080;&#38656;&#35775;&#38382;&#28304;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#28040;&#34701;&#20998;&#26512;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#37096;&#20998;&#39046;&#22495;&#36866;&#24212;&#20219;&#21153;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#20840;&#38754;&#35780;&#20272;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a robust Partial Domain Adaptation (PDA) framework that mitigates the negative transfer problem by incorporating a robust target-supervision strategy. It leverages ensemble learning and includes diverse, complementary label feedback, alleviating the effect of incorrect feedback and promoting pseudo-label refinement. Rather than relying exclusively on first-order moments for distribution alignment, our approach offers explicit objectives to optimize intra-class compactness and inter-class separation with the inferred source prototypes and highly-confident target samples in a domain-invariant fashion. Notably, we ensure source data privacy by eliminating the need to access the source data during the adaptation phase through a priori inference of source prototypes. We conducted a series of comprehensive experiments, including an ablation analysis, covering a range of partial domain adaptation tasks. Comprehensive evaluations on benchmark datasets corroborate our framewo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#26426;&#22120;&#20154;&#20013;&#39640;&#25928;&#26816;&#27979;&#21333;&#19968;&#29289;&#20307;&#30340;&#26041;&#27861;&#65292;&#22312;RoboCup&#27604;&#36187;&#20013;&#29305;&#21035;&#20851;&#27880;&#29699;&#30340;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;&#26426;&#22120;&#20154;&#24179;&#21488;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#22270;&#20687;&#34917;&#19969;&#20013;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#21333;&#19968;&#29289;&#20307;&#20998;&#31867;&#65292;&#21516;&#26102;&#30830;&#23450;&#20854;&#31934;&#30830;&#30340;&#31354;&#38388;&#20301;&#32622;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#26089;&#26399;&#20013;&#27490;&#25216;&#26415;&#20197;&#20943;&#23569;&#32972;&#26223;&#31867;&#20013;&#26131;&#20110;&#25298;&#32477;&#30340;&#24773;&#20917;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#22797;&#21512;&#25439;&#22833;&#20989;&#25968;&#21644;&#25968;&#25454;&#22686;&#24378;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#36798;&#21040;&#20102;100%&#30340;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.03530</link><description>&lt;p&gt;
&#38024;&#23545;&#31227;&#21160;&#26426;&#22120;&#20154;&#20013;&#21333;&#19968;&#29289;&#20307;&#26816;&#27979;&#30340;&#39640;&#25928;&#26041;&#27861;&#21450;&#26089;&#26399;&#20013;&#27490;&#22686;&#24378;&#39640;&#31934;&#24230;CNNs
&lt;/p&gt;
&lt;p&gt;
Efficient Single Object Detection on Image Patches with Early Exit Enhanced High-Precision CNNs. (arXiv:2309.03530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#26426;&#22120;&#20154;&#20013;&#39640;&#25928;&#26816;&#27979;&#21333;&#19968;&#29289;&#20307;&#30340;&#26041;&#27861;&#65292;&#22312;RoboCup&#27604;&#36187;&#20013;&#29305;&#21035;&#20851;&#27880;&#29699;&#30340;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;&#26426;&#22120;&#20154;&#24179;&#21488;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#22270;&#20687;&#34917;&#19969;&#20013;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#21333;&#19968;&#29289;&#20307;&#20998;&#31867;&#65292;&#21516;&#26102;&#30830;&#23450;&#20854;&#31934;&#30830;&#30340;&#31354;&#38388;&#20301;&#32622;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#26089;&#26399;&#20013;&#27490;&#25216;&#26415;&#20197;&#20943;&#23569;&#32972;&#26223;&#31867;&#20013;&#26131;&#20110;&#25298;&#32477;&#30340;&#24773;&#20917;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#22797;&#21512;&#25439;&#22833;&#20989;&#25968;&#21644;&#25968;&#25454;&#22686;&#24378;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#36798;&#21040;&#20102;100%&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;RoboCup&#26631;&#20934;&#24179;&#21488;&#32852;&#36187;&#32972;&#26223;&#19979;&#21033;&#29992;&#31227;&#21160;&#26426;&#22120;&#20154;&#26816;&#27979;&#29289;&#20307;&#30340;&#26032;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#26816;&#27979;&#29699;&#12290;&#25361;&#25112;&#22312;&#20110;&#22312;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#21644;&#24555;&#36895;&#36816;&#21160;&#24341;&#36215;&#30340;&#22270;&#20687;&#27169;&#31946;&#24773;&#20917;&#19979;&#26816;&#27979;&#21160;&#24577;&#29289;&#20307;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#35745;&#31639;&#21463;&#38480;&#30340;&#26426;&#22120;&#20154;&#24179;&#21488;&#35774;&#35745;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;CNN&#32463;&#36807;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;&#22270;&#20687;&#34917;&#19969;&#20013;&#23454;&#29616;&#23545;&#21333;&#19968;&#29289;&#20307;&#30340;&#39640;&#31934;&#24230;&#20998;&#31867;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#30340;&#31934;&#30830;&#31354;&#38388;&#20301;&#32622;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#23558;&#26089;&#26399;&#36864;&#20986;(Early Exits)&#25972;&#21512;&#21040;&#29616;&#26377;&#30340;&#39640;&#31934;&#24230;CNN&#26550;&#26500;&#20013;&#65292;&#20197;&#20943;&#23569;&#32972;&#26223;&#31867;&#20013;&#26131;&#20110;&#25298;&#32477;&#30340;&#24773;&#20917;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#35757;&#32451;&#36807;&#31243;&#28041;&#21450;&#22522;&#20110;&#32622;&#20449;&#24230;&#21644;&#20301;&#32622;&#25439;&#22833;&#30340;&#22797;&#21512;&#25439;&#22833;&#20989;&#25968;&#65292;&#20855;&#26377;&#21160;&#24577;&#21152;&#26435;&#21644;&#25968;&#25454;&#22686;&#24378;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;v&#20013;&#36798;&#21040;&#20102;100%&#30340;&#31934;&#30830;&#24230;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel approach for detecting objects using mobile robots in the context of the RoboCup Standard Platform League, with a primary focus on detecting the ball. The challenge lies in detecting a dynamic object in varying lighting conditions and blurred images caused by fast movements. To address this challenge, the paper presents a convolutional neural network architecture designed specifically for computationally constrained robotic platforms. The proposed CNN is trained to achieve high precision classification of single objects in image patches and to determine their precise spatial positions. The paper further integrates Early Exits into the existing high-precision CNN architecture to reduce the computational cost of easily rejectable cases in the background class. The training process involves a composite loss function based on confidence and positional losses with dynamic weighting and data augmentation. The proposed approach achieves a precision of 100% on the v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#25345;&#32493;&#24615;&#32852;&#37030;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#20849;&#25391;&#29702;&#35770;&#32858;&#31867;&#31639;&#27861;&#20316;&#20026;&#22522;&#30784;&#32858;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#26410;&#30693;&#25110;&#25345;&#32493;&#21464;&#21270;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.03487</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#33258;&#36866;&#24212;&#20849;&#25391;&#29702;&#35770;&#25345;&#32493;&#32852;&#37030;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving Continual Federated Clustering via Adaptive Resonance Theory. (arXiv:2309.03487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#25345;&#32493;&#24615;&#32852;&#37030;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#20849;&#25391;&#29702;&#35770;&#32858;&#31867;&#31639;&#27861;&#20316;&#20026;&#22522;&#30784;&#32858;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#26410;&#30693;&#25110;&#25345;&#32493;&#21464;&#21270;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#35768;&#22810;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;&#22312;&#32858;&#31867;&#39046;&#22495;&#20013;&#65292;&#21508;&#31181;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65288;&#21363;&#32852;&#37030;&#32858;&#31867;&#65289;&#30340;&#31639;&#27861;&#24050;&#32463;&#34987;&#31215;&#26497;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#39640;&#25928;&#30340;&#32858;&#31867;&#24615;&#33021;&#20197;&#21450;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#32858;&#31867;&#31639;&#27861;&#20013;&#20351;&#29992;&#30340;&#22823;&#37096;&#20998;&#22522;&#30784;&#32858;&#31867;&#22120;&#65288;&#21363;&#32858;&#31867;&#31639;&#27861;&#65289;&#38656;&#35201;&#39044;&#20808;&#25351;&#23450;&#32858;&#31867;&#25968;&#37327;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#31639;&#27861;&#26080;&#27861;&#22788;&#29702;&#25968;&#25454;&#20998;&#24067;&#26410;&#30693;&#25110;&#25345;&#32493;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#25345;&#32493;&#24615;&#32852;&#37030;&#32858;&#31867;&#31639;&#27861;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20013;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#20849;&#25391;&#29702;&#35770;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#23427;&#20855;&#22791;&#25345;&#32493;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing importance of data privacy protection, various privacy-preserving machine learning methods have been proposed. In the clustering domain, various algorithms with a federated learning framework (i.e., federated clustering) have been actively studied and showed high clustering performance while preserving data privacy. However, most of the base clusterers (i.e., clustering algorithms) used in existing federated clustering algorithms need to specify the number of clusters in advance. These algorithms, therefore, are unable to deal with data whose distributions are unknown or continually changing. To tackle this problem, this paper proposes a privacy-preserving continual federated clustering algorithm. In the proposed algorithm, an adaptive resonance theory-based clustering algorithm capable of continual learning is used as a base clusterer. Therefore, the proposed algorithm inherits the ability of continual learning. Experimental results with synthetic and real-world da
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24555;&#36895;&#30340;FixMatch&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35838;&#31243;&#25209;&#27425;&#22823;&#23567;&#65288;CBS&#65289;&#26469;&#20943;&#23569;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#35757;&#32451;&#35745;&#31639;&#37327;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#26631;&#35760;&#22686;&#24378;&#21644;&#35838;&#31243;&#20266;&#26631;&#31614;&#36827;&#34892;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.03469</link><description>&lt;p&gt;
&#24555;&#36895;&#30340;FixMatch: &#22522;&#20110;&#35838;&#31243;&#25209;&#27425;&#22823;&#23567;&#30340;&#24555;&#36895;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size. (arXiv:2309.03469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24555;&#36895;&#30340;FixMatch&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35838;&#31243;&#25209;&#27425;&#22823;&#23567;&#65288;CBS&#65289;&#26469;&#20943;&#23569;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#35757;&#32451;&#35745;&#31639;&#37327;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#26631;&#35760;&#22686;&#24378;&#21644;&#35838;&#31243;&#20266;&#26631;&#31614;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#36827;&#23637;&#20960;&#20046;&#23436;&#20840;&#28040;&#38500;&#20102;SSL&#21644;&#30417;&#30563;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#26631;&#31614;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#24615;&#33021;&#25552;&#21319;&#24448;&#24448;&#26159;&#20197;&#26174;&#33879;&#22686;&#21152;&#30340;&#35757;&#32451;&#35745;&#31639;&#20026;&#20195;&#20215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35838;&#31243;&#25209;&#27425;&#22823;&#23567;&#65288;CBS&#65289;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35757;&#32451;&#21160;&#24577;&#65292;&#37319;&#29992;&#19968;&#20010;&#23567;&#30340;&#26410;&#26631;&#35760;&#30340;&#25209;&#27425;&#22823;&#23567;&#24320;&#22987;&#35757;&#32451;&#65292;&#24182;&#36880;&#28176;&#22686;&#21152;&#21040;&#35757;&#32451;&#32467;&#26463;&#12290;&#26080;&#35770;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#25110;&#35757;&#32451;&#36718;&#27425;&#65292;&#37117;&#20351;&#29992;&#22266;&#23450;&#30340;&#35838;&#31243;&#65292;&#36890;&#36807;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#20943;&#23569;&#35757;&#32451;&#35745;&#31639;&#37327;&#12290;&#25105;&#20204;&#23558;CBS&#12289;&#24378;&#21270;&#26631;&#35760;&#22686;&#24378;&#21644;&#35838;&#31243;&#20266;&#26631;&#31614;&#65288;CPL&#65289;&#24212;&#29992;&#20110;FixMatch&#65292;&#24182;&#23558;&#36825;&#20010;&#26032;&#30340;SSL&#31639;&#27861;&#31216;&#20026;&#24555;&#36895;&#30340;FixMatch&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21106;&#23454;&#39564;&#65292;&#34920;&#26126;&#24378;&#21270;&#26631;&#35760;&#22686;&#24378;&#21644;/&#25110;CPL&#24182;&#19981;&#26174;&#33879;&#22320;&#20943;&#23569;&#35757;&#32451;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in Semi-Supervised Learning (SSL) have almost entirely closed the gap between SSL and Supervised Learning at a fraction of the number of labels. However, recent performance improvements have often come \textit{at the cost of significantly increased training computation}. To address this, we propose Curriculum Batch Size (CBS), \textit{an unlabeled batch size curriculum which exploits the natural training dynamics of deep neural networks.} A small unlabeled batch size is used in the beginning of training and is gradually increased to the end of training. A fixed curriculum is used regardless of dataset, model or number of epochs, and reduced training computations is demonstrated on all settings. We apply CBS, strong labeled augmentation, Curriculum Pseudo Labeling (CPL) \citep{FlexMatch} to FixMatch \citep{FixMatch} and term the new SSL algorithm Fast FixMatch. We perform an ablation study to show that strong labeled augmentation and/or CPL do not significantly reduce training 
&lt;/p&gt;</description></item><item><title>Bongard&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#20174;&#19968;&#32452;&#27491;&#36127;&#22270;&#20687;&#20013;&#25512;&#23548;&#20986;&#25277;&#35937;&#27010;&#24565;&#24182;&#36827;&#34892;&#20998;&#31867;&#30340;&#26234;&#21147;&#27979;&#35797;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;Bongard&#38382;&#39064;&#20013;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#25972;&#21512;&#25903;&#25345;&#38598;&#21512;&#20013;&#30340;&#20449;&#24687;&#65292;&#32780;&#26159;&#20165;&#20381;&#36182;&#20110;&#21333;&#20010;&#25903;&#25345;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.03468</link><description>&lt;p&gt;
&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#23545;&#20110;Bongard&#38382;&#39064;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Cross-Image Context Matters for Bongard Problems. (arXiv:2309.03468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03468
&lt;/p&gt;
&lt;p&gt;
Bongard&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#20174;&#19968;&#32452;&#27491;&#36127;&#22270;&#20687;&#20013;&#25512;&#23548;&#20986;&#25277;&#35937;&#27010;&#24565;&#24182;&#36827;&#34892;&#20998;&#31867;&#30340;&#26234;&#21147;&#27979;&#35797;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;Bongard&#38382;&#39064;&#20013;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#25972;&#21512;&#25903;&#25345;&#38598;&#21512;&#20013;&#30340;&#20449;&#24687;&#65292;&#32780;&#26159;&#20165;&#20381;&#36182;&#20110;&#21333;&#20010;&#25903;&#25345;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;Bongard&#38382;&#39064;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;Bongard&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#20174;&#19968;&#32452;&#27491;&#36127;&#8220;&#25903;&#25345;&#8221;&#22270;&#20687;&#20013;&#25512;&#23548;&#20986;&#25277;&#35937;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#23545;&#20110;&#26032;&#30340;&#26597;&#35810;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#21028;&#26029;&#23427;&#26159;&#21542;&#25551;&#36848;&#20102;&#20851;&#38190;&#27010;&#24565;&#30340;&#26234;&#21147;&#27979;&#35797;&#12290;&#22312;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;Bongard&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;Bongard-HOI&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#20165;&#36798;&#21040;&#20102;66%&#65288;&#20598;&#28982;&#20934;&#30830;&#29575;&#20026;50%&#65289;&#12290;&#20302;&#20934;&#30830;&#29575;&#36890;&#24120;&#24402;&#22240;&#20110;&#31070;&#32463;&#32593;&#32476;&#32570;&#20047;&#21457;&#29616;&#31867;&#20284;&#20154;&#31867;&#31526;&#21495;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#38382;&#39064;&#32780;&#22833;&#21435;&#20102;&#20934;&#30830;&#24615;&#65306;&#23427;&#20204;&#27809;&#26377;&#23558;&#25903;&#25345;&#38598;&#21512;&#20013;&#30340;&#20449;&#24687;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#21152;&#20837;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#20174;&#21333;&#20010;&#25903;&#25345;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#19982;&#28041;&#21450;&#23545;&#35937;&#20998;&#31867;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#19981;&#21516;&#65292;&#19968;&#20010;&#20856;&#22411;&#30340;Bongard&#38382;&#39064;&#20013;&#30340;&#8220;&#20851;&#38190;&#27010;&#24565;&#8221;&#21482;&#33021;&#20351;&#29992;&#22810;&#20010;&#27491;&#20363;&#21644;&#22810;&#20010;&#21453;&#20363;&#26469;&#21306;&#20998;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current machine learning methods struggle to solve Bongard problems, which are a type of IQ test that requires deriving an abstract "concept" from a set of positive and negative "support" images, and then classifying whether or not a new query image depicts the key concept. On Bongard-HOI, a benchmark for natural-image Bongard problems, existing methods have only reached 66% accuracy (where chance is 50%). Low accuracy is often attributed to neural nets' lack of ability to find human-like symbolic rules. In this work, we point out that many existing methods are forfeiting accuracy due to a much simpler problem: they do not incorporate information contained in the support set as a whole, and rely instead on information extracted from individual supports. This is a critical issue, because unlike in few-shot learning tasks concerning object classification, the "key concept" in a typical Bongard problem can only be distinguished using multiple positives and multiple negatives. We explore a
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#36741;&#21161;&#32593;&#32476;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#34920;&#31034;&#26469;&#35757;&#32451;&#26356;&#22909;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#25512;&#26029;&#36807;&#31243;&#20013;&#27169;&#24577;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03452</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#36741;&#21161;&#32593;&#32476;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Guidance Network For Missing Modality Inference. (arXiv:2309.03452v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03452
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#36741;&#21161;&#32593;&#32476;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#34920;&#31034;&#26469;&#35757;&#32451;&#26356;&#22909;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#25512;&#26029;&#36807;&#31243;&#20013;&#27169;&#24577;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#26368;&#36817;&#20960;&#24180;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26631;&#20934;&#22810;&#27169;&#24577;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#22312;&#35757;&#32451;&#38454;&#27573;&#21644;&#25512;&#26029;&#38454;&#27573;&#27169;&#24577;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#35768;&#22810;&#22330;&#26223;&#26080;&#27861;&#28385;&#36275;&#36825;&#26679;&#30340;&#20551;&#35774;&#65292;&#25512;&#26029;&#36807;&#31243;&#20013;&#20250;&#20986;&#29616;&#32570;&#22833;&#27169;&#24577;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#37325;&#24314;&#32570;&#22833;&#27169;&#24577;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#22686;&#21152;&#20102;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#23588;&#20854;&#23545;&#20110;&#22823;&#22411;&#37096;&#32626;&#31995;&#32479;&#32780;&#35328;&#21487;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#20174;&#20004;&#20010;&#26041;&#38754;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#32593;&#32476;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#34920;&#31034;&#26469;&#35757;&#32451;&#26356;&#22909;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#12290;&#22312;&#26292;&#21147;&#26816;&#27979;&#30340;&#29616;&#23454;&#29983;&#27963;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#25512;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal models have gained significant success in recent years. Standard multimodal approaches often assume unchanged modalities from training stage to inference stage. In practice, however, many scenarios fail to satisfy such assumptions with missing modalities during inference, leading to limitations on where multimodal models can be applied. While existing methods mitigate the problem through reconstructing the missing modalities, it increases unnecessary computational cost, which could be just as critical, especially for large, deployed systems. To solve the problem from both sides, we propose a novel guidance network that promotes knowledge sharing during training, taking advantage of the multimodal representations to train better single-modality models for inference. Real-life experiment in violence detection shows that our proposed framework trains single-modality models that significantly outperform its traditionally trained counterparts while maintaining the same inference 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22495;&#22768;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#27700;&#19979;&#22768;&#38899;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#26469;&#20998;&#26512;&#27700;&#19979;&#22768;&#23398;&#25968;&#25454;&#12290;&#36890;&#36807;&#32858;&#31867;&#21644;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#20505;&#36873;&#26631;&#31614;&#36873;&#25321;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23454;&#29616;&#20102;&#23545;&#31354;&#27668;&#26538;&#22768;&#30340;&#39640;&#25928;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.03451</link><description>&lt;p&gt;
&#36328;&#22495;&#22768;&#38899;&#35782;&#21035;&#29992;&#20110;&#39640;&#25928;&#30340;&#27700;&#19979;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Sound Recognition for Efficient Underwater Data Analysis. (arXiv:2309.03451v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22495;&#22768;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#27700;&#19979;&#22768;&#38899;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#26469;&#20998;&#26512;&#27700;&#19979;&#22768;&#23398;&#25968;&#25454;&#12290;&#36890;&#36807;&#32858;&#31867;&#21644;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#20505;&#36873;&#26631;&#31614;&#36873;&#25321;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23454;&#29616;&#20102;&#23545;&#31354;&#27668;&#26538;&#22768;&#30340;&#39640;&#25928;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22312;&#24191;&#35889;&#38750;&#27700;&#19979;&#65288;&#31354;&#20013;&#65289;&#22768;&#38899;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20998;&#26512;&#28023;&#37327;&#27700;&#19979;&#22768;&#23398;&#25968;&#25454;&#12290;&#37492;&#20110;&#26631;&#35760;&#22823;&#37327;&#27700;&#19979;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#26041;&#27861;&#26469;&#21152;&#36895;&#36825;&#19968;&#21171;&#21160;&#23494;&#38598;&#22411;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#31532;&#19968;&#37096;&#20998;&#28041;&#21450;&#20351;&#29992;&#31354;&#20013;&#22768;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#29305;&#24449;&#21521;&#37327;&#23545;&#27700;&#19979;&#25968;&#25454;&#36827;&#34892;PCA&#21644;UMAP&#21487;&#35270;&#21270;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#21548;&#21462;&#36825;&#20123;&#32858;&#31867;&#20013;&#30340;&#28857;&#20197;&#20102;&#35299;&#20854;&#23450;&#20041;&#29305;&#24615;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#31616;&#21270;&#20102;&#36873;&#25321;&#20505;&#36873;&#26631;&#31614;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#36807;&#31243;&#12290;&#22312;&#31532;&#20108;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#36873;&#23450;&#30340;&#27700;&#19979;&#25968;&#25454;&#21644;&#38750;&#27700;&#19979;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23450;&#37327;&#20998;&#26512;&#65292;&#34913;&#37327;&#20102;&#25105;&#20204;&#27169;&#22411;&#35782;&#21035;&#31354;&#27668;&#26538;&#22768;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel deep learning approach for analyzing massive underwater acoustic data by leveraging a model trained on a broad spectrum of non-underwater (aerial) sounds. Recognizing the challenge in labeling vast amounts of underwater data, we propose a two-fold methodology to accelerate this labor-intensive procedure.  The first part of our approach involves PCA and UMAP visualization of the underwater data using the feature vectors of an aerial sound recognition model. This enables us to cluster the data in a two dimensional space and listen to points within these clusters to understand their defining characteristics. This innovative method simplifies the process of selecting candidate labels for further training.  In the second part, we train a neural network model using both the selected underwater data and the non-underwater dataset. We conducted a quantitative analysis to measure the precision, recall, and F1 score of our model for recognizing airgun sounds, a common
&lt;/p&gt;</description></item><item><title>XGen-7B&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20811;&#26381;&#24320;&#28304;LLMs&#22312;&#25903;&#25345;&#38271;&#24207;&#21015;&#38271;&#24230;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#19978;&#21462;&#24471;&#19982;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;LLMs&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#25512;&#36827;&#20102;&#30740;&#31350;&#36827;&#23637;&#21644;&#21830;&#19994;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.03450</link><description>&lt;p&gt;
XGen-7B&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
XGen-7B Technical Report. (arXiv:2309.03450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03450
&lt;/p&gt;
&lt;p&gt;
XGen-7B&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20811;&#26381;&#24320;&#28304;LLMs&#22312;&#25903;&#25345;&#38271;&#24207;&#21015;&#38271;&#24230;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#19978;&#21462;&#24471;&#19982;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;LLMs&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#25512;&#36827;&#20102;&#30740;&#31350;&#36827;&#23637;&#21644;&#21830;&#19994;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21464;&#24471;&#26222;&#36941;&#65292;&#25913;&#21464;&#20102;&#25105;&#20204;&#19982;&#20449;&#24687;&#20132;&#20114;&#21644;&#36827;&#34892;&#30740;&#31350;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#39640;&#24615;&#33021;&#30340;LLMs&#20173;&#28982;&#21463;&#38480;&#20110;&#19987;&#26377;&#22681;&#22721;&#65292;&#38459;&#30861;&#20102;&#31185;&#23398;&#36827;&#23637;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22810;&#25968;&#24320;&#28304;&#30340;LLMs&#22312;&#25903;&#25345;&#36739;&#38271;&#24207;&#21015;&#38271;&#24230;&#26041;&#38754;&#26377;&#38480;&#65292;&#32780;&#36825;&#23545;&#20110;&#35768;&#22810;&#38656;&#35201;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#36827;&#34892;&#25512;&#29702;&#30340;&#20219;&#21153;&#26469;&#35828;&#26159;&#19968;&#20010;&#20851;&#38190;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;XGen&#65292;&#19968;&#31995;&#21015;7B&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#21487;&#25903;&#25345;&#38271;&#24230;&#20026;8K&#30340;&#24207;&#21015;&#21644;1.5T&#20010;&#20196;&#29260;&#12290;&#25105;&#20204;&#36824;&#23545;XGen&#27169;&#22411;&#36827;&#34892;&#20102;&#20844;&#20849;&#39046;&#22495;&#25945;&#23398;&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#21019;&#24314;&#20102;&#23427;&#20204;&#30340;&#25945;&#23398;&#20248;&#21270;&#29256;&#26412;&#65288;XGen-Inst&#65289;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#24320;&#28304;&#65292;&#29992;&#20110;&#30740;&#31350;&#36827;&#23637;&#21644;&#21830;&#19994;&#24212;&#29992;&#12290;&#25105;&#20204;&#23545;&#26631;&#20934;&#22522;&#20934;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;LLMs&#30456;&#27604;&#65292;XGen&#27169;&#22411;&#23454;&#29616;&#20102;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#38024;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#36827;&#34892;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have become ubiquitous across various domains, transforming the way we interact with information and conduct research. However, most high-performing LLMs remain confined behind proprietary walls, hindering scientific progress. Most open-source LLMs, on the other hand, are limited in their ability to support longer sequence lengths, which is a key requirement for many tasks that require inference over an input context. To address this, we have trained XGen, a series of 7B parameter models on up to 8K sequence length for up to 1.5T tokens. We have also finetuned the XGen models on public-domain instructional data, creating their instruction-tuned counterparts (XGen-Inst). We open-source our models for both research advancements and commercial applications. Our evaluation on standard benchmarks shows that XGen models achieve comparable or better results when compared with state-of-the-art open-source LLMs. Our targeted evaluation on long sequence modeling task
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#31639;&#23376;&#30340;&#25968;&#25454;&#39537;&#21160;&#22320;&#38754;&#36816;&#21160;&#21512;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#21442;&#25968;&#29983;&#25104;&#19977;&#20998;&#37327;&#21152;&#36895;&#24230;&#26102;&#38388;&#21382;&#21490;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#27169;&#22411;&#35757;&#32451;&#19981;&#21463;&#25968;&#25454;&#37319;&#26679;&#39057;&#29575;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39564;&#35777;&#21644;&#24212;&#29992;&#23454;&#20363;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#21487;&#29992;&#20110;&#29983;&#25104;&#26085;&#26412;&#22320;&#38663;&#21160;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.03447</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#31639;&#23376;&#23454;&#29616;&#23485;&#24102;&#22320;&#38754;&#36816;&#21160;&#21512;&#25104;: &#24320;&#21457;&#19982;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Broadband Ground Motion Synthesis via Generative Adversarial Neural Operators: Development and Validation. (arXiv:2309.03447v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#31639;&#23376;&#30340;&#25968;&#25454;&#39537;&#21160;&#22320;&#38754;&#36816;&#21160;&#21512;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#21442;&#25968;&#29983;&#25104;&#19977;&#20998;&#37327;&#21152;&#36895;&#24230;&#26102;&#38388;&#21382;&#21490;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#27169;&#22411;&#35757;&#32451;&#19981;&#21463;&#25968;&#25454;&#37319;&#26679;&#39057;&#29575;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39564;&#35777;&#21644;&#24212;&#29992;&#23454;&#20363;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#21487;&#29992;&#20110;&#29983;&#25104;&#26085;&#26412;&#22320;&#38663;&#21160;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#31639;&#23376;&#65288;GANO&#65289;&#30340;&#25968;&#25454;&#39537;&#21160;&#22320;&#38754;&#36816;&#21160;&#21512;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#24320;&#25918;&#33719;&#21462;&#30340;&#24378;&#38663;&#21160;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26681;&#25454;&#30697;&#38663;&#32423;&#65288;M&#65289;&#12289;&#26029;&#35010;&#36317;&#31163;&#65288;R_{rup}&#65289;&#12289;&#39030;&#37096;30m&#22788;&#30340;&#26102;&#38388;&#24179;&#22343;&#21098;&#20999;&#27874;&#36895;&#24230;&#65288;V_{S30}&#65289;&#21644;&#26500;&#36896;&#29615;&#22659;&#25110;&#26029;&#23618;&#31867;&#22411;&#29983;&#25104;&#19977;&#20998;&#37327;&#21152;&#36895;&#24230;&#26102;&#38388;&#21382;&#21490;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#65292;&#36825;&#26159;&#19968;&#31181;&#20998;&#36776;&#29575;&#26080;&#20851;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#20445;&#35777;&#27169;&#22411;&#35757;&#32451;&#19982;&#25968;&#25454;&#37319;&#26679;&#39057;&#29575;&#26080;&#20851;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#22320;&#38754;&#36816;&#21160;&#21512;&#25104;&#31639;&#27861;&#65288;&#20197;&#19979;&#31616;&#31216;cGM-GANO&#65289;&#24182;&#35752;&#35770;&#20854;&#19982;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#30340;&#20248;&#21183;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#21335;&#21152;&#24030;&#22320;&#38663;&#20013;&#24515;&#65288;SCEC&#65289;&#23485;&#24102;&#24179;&#21488;&#65288;BBP&#65289;&#20135;&#29983;&#30340;&#27169;&#25311;&#22320;&#38663;&#21160;&#39564;&#35777;&#20102;cGM-GANO&#26694;&#26550;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#26085;&#26412;&#30340;KiK-net&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;cGM-GANO&#65292;&#34920;&#26126;&#35813;&#26694;&#26550;&#21487;&#20197;&#37325;&#26032;&#29983;&#25104;&#22320;&#38663;&#21160;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a data-driven model for ground-motion synthesis using a Generative Adversarial Neural Operator (GANO) that combines recent advancements in machine learning and open access strong motion data sets to generate three-component acceleration time histories conditioned on moment magnitude ($M$), rupture distance ($R_{rup}$), time-average shear-wave velocity at the top $30m$ ($V_{S30}$), and tectonic environment or style of faulting. We use Neural Operators, a resolution invariant architecture that guarantees that the model training is independent of the data sampling frequency. We first present the conditional ground-motion synthesis algorithm (referred to heretofore as cGM-GANO) and discuss its advantages compared to previous work. Next, we verify the cGM-GANO framework using simulated ground motions generated with the Southern California Earthquake Center (SCEC) Broadband Platform (BBP). We lastly train cGM-GANO on a KiK-net dataset from Japan, showing that the framework can rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21453;&#20107;&#23454;&#29983;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#20219;&#21153;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#26089;&#20135;&#20799;&#28857;&#29366;&#30333;&#36136;&#30149;&#21464;&#30340;&#20934;&#30830;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2309.03440</link><description>&lt;p&gt;
&#26089;&#20135;&#20799;&#28857;&#29366;&#30333;&#36136;&#30149;&#21464;&#20998;&#21106;&#30740;&#31350;&#65306;&#22522;&#20110;&#21453;&#20107;&#23454;&#29983;&#25104;&#23398;&#20064;&#30340;&#21160;&#21147;
&lt;/p&gt;
&lt;p&gt;
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning. (arXiv:2309.03440v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21453;&#20107;&#23454;&#29983;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#20219;&#21153;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#26089;&#20135;&#20799;&#28857;&#29366;&#30333;&#36136;&#30149;&#21464;&#30340;&#20934;&#30830;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#20998;&#21106;&#28857;&#29366;&#30333;&#36136;&#30149;&#21464;&#65288;PWMLs&#65289;&#23545;&#30456;&#20851;&#21457;&#32946;&#38556;&#30861;&#30340;&#21450;&#26102;&#35786;&#26029;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#32771;&#34385;&#21040;&#30149;&#21464;&#36890;&#24120;&#36739;&#23567;&#12289;&#23545;&#27604;&#24230;&#36739;&#20302;&#65292;&#24182;&#19988;&#30149;&#21464;&#25968;&#37327;&#22312;&#19981;&#21516;&#21463;&#35797;&#32773;&#20043;&#38388;&#21487;&#33021;&#21457;&#29983;&#24040;&#22823;&#21464;&#21270;&#65292;&#20174;&#23156;&#20799;&#33041;MR&#22270;&#20687;&#20013;&#33258;&#21160;&#20998;&#21106;PWMLs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30452;&#25509;&#23558;&#36890;&#29992;&#30340;&#32593;&#32476;&#32467;&#26500;&#24212;&#29992;&#20110;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#21040;PWMLs&#30340;&#35814;&#32454;&#20301;&#32622;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#20005;&#37325;&#30340;&#27424;&#20998;&#21106;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#24605;&#24819;&#32467;&#21512;&#33041;&#32452;&#32455;&#20998;&#21106;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#23398;&#20064;&#31934;&#32454;&#30340;&#20301;&#32622;&#21644;&#24418;&#24577;&#34920;&#31034;&#20197;&#23454;&#29616;PWMLs&#30340;&#20934;&#30830;&#23450;&#20301;&#21644;&#20998;&#21106;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#26131;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;&#21363;DeepPWML&#65289;&#12290;&#23427;&#23558;&#30149;&#21464;&#21453;&#20107;&#23454;&#22270;&#19982;&#32452;&#32455;&#27010;&#29575;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate segmentation of punctate white matter lesions (PWMLs) are fundamental for the timely diagnosis and treatment of related developmental disorders. Automated PWMLs segmentation from infant brain MR images is challenging, considering that the lesions are typically small and low-contrast, and the number of lesions may dramatically change across subjects. Existing learning-based methods directly apply general network architectures to this challenging task, which may fail to capture detailed positional information of PWMLs, potentially leading to severe under-segmentations. In this paper, we propose to leverage the idea of counterfactual reasoning coupled with the auxiliary task of brain tissue segmentation to learn fine-grained positional and morphological representations of PWMLs for accurate localization and segmentation. A simple and easy-to-implement deep-learning framework (i.e., DeepPWML) is accordingly designed. It combines the lesion counterfactual map with the tissue probab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#30340;Tucker&#20998;&#35299;&#65288;perTucker&#65289;&#26041;&#27861;&#26469;&#25429;&#25417;&#24352;&#37327;&#25968;&#25454;&#20013;&#30340;&#24322;&#36136;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#29420;&#29305;&#21644;&#20849;&#24615;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;perTucker&#22312;&#24322;&#24120;&#26816;&#27979;&#12289;&#23458;&#25143;&#20998;&#31867;&#21644;&#32858;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03439</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;Tucker&#20998;&#35299;&#65306;&#23545;&#24352;&#37327;&#25968;&#25454;&#20013;&#30340;&#20849;&#24615;&#21644;&#29420;&#29305;&#24615;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Personalized Tucker Decomposition: Modeling Commonality and Peculiarity on Tensor Data. (arXiv:2309.03439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#30340;Tucker&#20998;&#35299;&#65288;perTucker&#65289;&#26041;&#27861;&#26469;&#25429;&#25417;&#24352;&#37327;&#25968;&#25454;&#20013;&#30340;&#24322;&#36136;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#29420;&#29305;&#21644;&#20849;&#24615;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;perTucker&#22312;&#24322;&#24120;&#26816;&#27979;&#12289;&#23458;&#25143;&#20998;&#31867;&#21644;&#32858;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#30340;Tucker&#20998;&#35299;&#65288;perTucker&#65289;&#26469;&#35299;&#20915;&#20256;&#32479;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#22312;&#25429;&#25417;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;perTucker&#23558;&#24352;&#37327;&#25968;&#25454;&#20998;&#35299;&#20026;&#20849;&#20139;&#30340;&#20840;&#23616;&#20998;&#37327;&#21644;&#20010;&#24615;&#21270;&#30340;&#23616;&#37096;&#20998;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#24335;&#27491;&#20132;&#24615;&#20551;&#35774;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25910;&#25947;&#20110;&#31283;&#23450;&#28857;&#30340;&#36817;&#31471;&#26799;&#24230;&#27491;&#21017;&#21270;&#22359;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#12290;&#36890;&#36807;&#23398;&#20064;&#36328;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#21644;&#20849;&#21516;&#34920;&#31034;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#27169;&#25311;&#30740;&#31350;&#21644;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65288;&#22826;&#38451;&#32768;&#26001;&#26816;&#27979;&#21644;&#21544;&#20301;&#20449;&#21495;&#20998;&#31867;&#65289;&#23637;&#31034;&#20102;perTucker&#22312;&#24322;&#24120;&#26816;&#27979;&#12289;&#23458;&#25143;&#20998;&#31867;&#21644;&#32858;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose personalized Tucker decomposition (perTucker) to address the limitations of traditional tensor decomposition methods in capturing heterogeneity across different datasets. perTucker decomposes tensor data into shared global components and personalized local components. We introduce a mode orthogonality assumption and develop a proximal gradient regularized block coordinate descent algorithm that is guaranteed to converge to a stationary point. By learning unique and common representations across datasets, we demonstrate perTucker's effectiveness in anomaly detection, client classification, and clustering through a simulation study and two case studies on solar flare detection and tonnage signal classification.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65292;&#20855;&#26377;&#26041;&#24046;&#20943;&#23569;&#21644;&#24046;&#20998;&#38544;&#31169;&#25216;&#26415;&#65292;&#26088;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#24182;&#25552;&#39640;&#23545;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03437</link><description>&lt;p&gt;
&#20855;&#26377;&#26041;&#24046;&#20943;&#23569;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#25308;&#21344;&#24237;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Byzantine-Robust Federated Learning with Variance Reduction and Differential Privacy. (arXiv:2309.03437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03437
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65292;&#20855;&#26377;&#26041;&#24046;&#20943;&#23569;&#21644;&#24046;&#20998;&#38544;&#31169;&#25216;&#26415;&#65292;&#26088;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#24182;&#25552;&#39640;&#23545;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20854;&#20013;&#25968;&#25454;&#20445;&#30041;&#22312;&#23458;&#25143;&#31471;&#65288;&#21363;&#29289;&#32852;&#32593;&#35774;&#22791;&#65289;&#19978;&#65292;&#20165;&#36890;&#36807;&#20849;&#21516;&#23398;&#20064;&#30340;&#36845;&#20195;&#36807;&#31243;&#20849;&#20139;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#25915;&#20987;&#21644;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#24433;&#21709;&#65306;&#22312;&#25972;&#20010;&#32852;&#37030;&#23398;&#20064;&#32593;&#32476;&#20013;&#20849;&#20139;&#30340;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#23558;&#27844;&#38706;&#26377;&#20851;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#30340;&#31169;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#23427;&#20204;&#36824;&#21487;&#20197;&#34987;&#25308;&#21344;&#24237;&#25915;&#20987;&#32773;&#24694;&#24847;&#21046;&#20316;&#20197;&#24178;&#25200;&#23398;&#20064;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65292;&#26088;&#22312;&#20445;&#35777;&#20005;&#26684;&#30340;&#38544;&#31169;&#24615;&#21516;&#26102;&#25552;&#39640;&#31995;&#32479;&#23545;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#31232;&#30095;&#21270;&#21644;&#21160;&#37327;&#39537;&#21160;&#30340;&#26041;&#24046;&#20943;&#23569;&#24341;&#20837;&#21040;&#23458;&#25143;&#31471;&#32423;&#21035;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#20013;&#65292;&#20197;&#38450;&#27490;&#25308;&#21344;&#24237;&#25915;&#20987;&#12290;&#23433;&#20840;&#35774;&#35745;&#19981;&#20250;&#36829;&#21453;&#23458;&#25143;&#31471;&#32423;&#21035;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;&#23458;&#25143;&#31471;&#32423;&#21035;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is designed to preserve data privacy during model training, where the data remains on the client side (i.e., IoT devices), and only model updates of clients are shared iteratively for collaborative learning. However, this process is vulnerable to privacy attacks and Byzantine attacks: the local model updates shared throughout the FL network will leak private information about the local training data, and they can also be maliciously crafted by Byzantine attackers to disturb the learning. In this paper, we propose a new FL scheme that guarantees rigorous privacy and simultaneously enhances system robustness against Byzantine attacks. Our approach introduces sparsification- and momentum-driven variance reduction into the client-level differential privacy (DP) mechanism, to defend against Byzantine attackers. The security design does not violate the privacy guarantee of the client-level DP mechanism; hence, our approach achieves the same client-level DP guarantee a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Equal Long-term Benefit Rate&#65288;ELBERT&#65289;&#30340;&#38271;&#26399;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#32771;&#34385;&#21040;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#65292;&#24182;&#23558;&#38745;&#24577;&#20844;&#24179;&#24615;&#21407;&#21017;&#24212;&#29992;&#20110;&#39034;&#24207;&#35774;&#32622;&#20013;&#12290;</title><link>http://arxiv.org/abs/2309.03426</link><description>&lt;p&gt;
&#30456;&#31561;&#30340;&#38271;&#26399;&#25928;&#30410;&#29575;&#65306;&#23558;&#38745;&#24577;&#20844;&#24179;&#24615;&#27010;&#24565;&#24212;&#29992;&#21040;&#39034;&#24207;&#20915;&#31574;&#20013;
&lt;/p&gt;
&lt;p&gt;
Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making. (arXiv:2309.03426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03426
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Equal Long-term Benefit Rate&#65288;ELBERT&#65289;&#30340;&#38271;&#26399;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#32771;&#34385;&#21040;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#65292;&#24182;&#23558;&#38745;&#24577;&#20844;&#24179;&#24615;&#21407;&#21017;&#24212;&#29992;&#20110;&#39034;&#24207;&#35774;&#32622;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#21487;&#33021;&#23545;&#26102;&#38388;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#65292;&#22240;&#27492;&#38271;&#26399;&#20844;&#24179;&#24615;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24573;&#30053;&#38271;&#26399;&#24433;&#21709;&#26102;&#65292;&#31616;&#21333;&#22320;&#24212;&#29992;&#38745;&#24577;&#20844;&#24179;&#24615;&#20934;&#21017;&#23454;&#38469;&#19978;&#20250;&#21152;&#21095;&#20559;&#35265;&#12290;&#20026;&#20102;&#26126;&#30830;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#20559;&#35265;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26694;&#26550;&#20013;&#21046;&#23450;&#20102;&#38271;&#26399;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#20182;&#20204;&#23558;&#38271;&#26399;&#20559;&#35265;&#23450;&#20041;&#20026;&#27599;&#20010;&#26102;&#38388;&#27493;&#19978;&#38745;&#24577;&#20559;&#35265;&#30340;&#24635;&#21644;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#22320;&#23545;&#36880;&#27493;&#20559;&#35265;&#27714;&#21644;&#21487;&#33021;&#23548;&#33268;&#34394;&#20551;&#30340;&#20844;&#24179;&#24863;&#65292;&#22240;&#20026;&#23427;&#26410;&#32771;&#34385;&#21040;&#36716;&#25442;&#36807;&#31243;&#20013;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#37325;&#35201;&#24615;&#24046;&#24322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Equal Long-term Benefit Rate&#65288;ELBERT&#65289;&#30340;&#38271;&#26399;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#23427;&#26126;&#30830;&#32771;&#34385;&#21040;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#65292;&#24182;&#23558;&#38745;&#24577;&#20844;&#24179;&#24615;&#21407;&#21017;&#24212;&#29992;&#20110;&#39034;&#24207;&#35774;&#32622;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38271;&#26399;&#25910;&#30410;&#30340;&#31574;&#30053;&#26799;&#24230;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decisions made by machine learning models may have lasting impacts over time, making long-term fairness a crucial consideration. It has been shown that when ignoring the long-term effect, naively imposing fairness criterion in static settings can actually exacerbate bias over time. To explicitly address biases in sequential decision-making, recent works formulate long-term fairness notions in Markov Decision Process (MDP) framework. They define the long-term bias to be the sum of static bias over each time step. However, we demonstrate that naively summing up the step-wise bias can cause a false sense of fairness since it fails to consider the importance difference of different time steps during transition. In this work, we introduce a long-term fairness notion called Equal Long-term Benefit Rate (ELBERT), which explicitly considers varying temporal importance and adapts static fairness principles to the sequential setting. Moreover, we show that the policy gradient of Long-term Benefi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03409</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Optimizers. (arXiv:2309.03409v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#34429;&#28982;&#22522;&#20110;&#23548;&#25968;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#26159;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#26159;&#27809;&#26377;&#26799;&#24230;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;&#20248;&#21270;&#20219;&#21153;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#25551;&#36848;&#12290;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#65292;LLM&#20174;&#21253;&#21547;&#20808;&#21069;&#29983;&#25104;&#30340;&#35299;&#19982;&#20854;&#20540;&#30340;&#25552;&#31034;&#20013;&#29983;&#25104;&#26032;&#30340;&#35299;&#65292;&#28982;&#21518;&#23545;&#26032;&#30340;&#35299;&#36827;&#34892;&#35780;&#20272;&#24182;&#28155;&#21152;&#21040;&#25552;&#31034;&#20013;&#65292;&#29992;&#20110;&#19979;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;OPRO&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#28982;&#21518;&#36716;&#21521;&#25552;&#31034;&#20248;&#21270;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#33021;&#26368;&#22823;&#21270;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#25351;&#20196;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;LLM&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;OPRO&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#22312;GSM8K&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;8%&#65292;&#22312;Big-Bench Hard&#20219;&#21153;&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#30340;&#20998;&#32423;&#27491;&#31867;-&#26410;&#26631;&#35760;&#65288;PU&#65289;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#24930;&#24615;&#30149;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#19981;&#21516;&#20154;&#32676;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;PU&#27169;&#22411;&#26641;&#21644;&#32858;&#21512;&#20854;&#36755;&#20986;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.03386</link><description>&lt;p&gt;
&#22522;&#20110;&#31038;&#21306;&#30340;&#20998;&#32423;&#27491;&#31867;-&#26410;&#26631;&#35760;&#65288;PU&#65289;&#27169;&#22411;&#34701;&#21512;&#29992;&#20110;&#24930;&#24615;&#30149;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Community-Based Hierarchical Positive-Unlabeled (PU) Model Fusion for Chronic Disease Prediction. (arXiv:2309.03386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#30340;&#20998;&#32423;&#27491;&#31867;-&#26410;&#26631;&#35760;&#65288;PU&#65289;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#24930;&#24615;&#30149;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#19981;&#21516;&#20154;&#32676;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;PU&#27169;&#22411;&#26641;&#21644;&#32858;&#21512;&#20854;&#36755;&#20986;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#31867;-&#26410;&#26631;&#35760;&#65288;PU&#65289;&#23398;&#20064;&#26159;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#19968;&#20010;&#25361;&#25112;&#65292;&#22312;&#36825;&#31181;&#38382;&#39064;&#20013;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#23569;&#37327;&#27491;&#31867;&#25968;&#25454;&#23454;&#20363;&#65292;&#21487;&#20197;&#29992;&#20110;&#24930;&#24615;&#30149;&#31579;&#26597;&#38382;&#39064;&#12290;&#26368;&#20808;&#36827;&#30340;PU&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#23548;&#33268;&#20102;&#21508;&#31181;&#39118;&#38505;&#20272;&#35745;&#22120;&#30340;&#21457;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#24573;&#35270;&#20102;&#19981;&#21516;&#20154;&#32676;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#31867;-&#26410;&#26631;&#35760;&#23398;&#20064;&#26641;&#65288;PUtree&#65289;&#31639;&#27861;&#12290;PUtree&#26088;&#22312;&#32771;&#34385;&#31038;&#21306;&#65292;&#22914;&#19981;&#21516;&#30340;&#24180;&#40836;&#25110;&#25910;&#20837;&#27573;&#65292;&#22312;&#24930;&#24615;&#30149;&#39044;&#27979;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#20998;&#31867;&#20915;&#31574;&#26041;&#27861;&#65292;&#23427;&#20197;&#23618;&#27425;&#26041;&#24335;&#26500;&#24314;&#22522;&#20110;&#31038;&#21306;&#30340;PU&#27169;&#22411;&#65292;&#28982;&#21518;&#32858;&#21512;&#23427;&#20204;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#26641;&#19978;&#27599;&#20010;PU&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#38750;&#21494;&#33410;&#28857;&#30340;&#20998;&#35010;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#25513;&#30721;&#24674;&#22797;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20351;&#24471;&#27169;&#22411;&#22312;&#20010;&#20307;&#19978;&#33021;&#36827;&#34892;&#20805;&#20998;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positive-Unlabeled (PU) Learning is a challenge presented by binary classification problems where there is an abundance of unlabeled data along with a small number of positive data instances, which can be used to address chronic disease screening problem. State-of-the-art PU learning methods have resulted in the development of various risk estimators, yet they neglect the differences among distinct populations. To address this issue, we present a novel Positive-Unlabeled Learning Tree (PUtree) algorithm. PUtree is designed to take into account communities such as different age or income brackets, in tasks of chronic disease prediction. We propose a novel approach for binary decision-making, which hierarchically builds community-based PU models and then aggregates their deliverables. Our method can explicate each PU model on the tree for the optimized non-leaf PU node splitting. Furthermore, a mask-recovery data augmentation strategy enables sufficient training of the model in individua
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ViewMix&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#19987;&#38376;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03360</link><description>&lt;p&gt;
ViewMix&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#31283;&#20581;&#34920;&#31034;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ViewMix: Augmentation for Robust Representation in Self-Supervised Learning. (arXiv:2309.03360v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03360
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ViewMix&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#19987;&#38376;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32852;&#21512;&#23884;&#20837;&#26550;&#26500;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#35748;&#20026;&#25968;&#25454;&#22686;&#24378;&#30340;&#32452;&#21512;&#26159;&#20854;&#24378;&#22823;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#23613;&#31649;&#22312;&#30417;&#30563;&#26041;&#27861;&#20013;&#65292;&#21306;&#22495;&#24615;&#20002;&#22833;&#31574;&#30053;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#24341;&#23548;&#27169;&#22411;&#20851;&#27880;&#29289;&#20307;&#30340;&#36739;&#19981;&#26126;&#26174;&#37096;&#20998;&#65292;&#20294;&#22312;&#33258;&#30417;&#30563;&#26041;&#27861;&#20013;&#23578;&#26410;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#29983;&#25104;&#27491;&#26679;&#26412;&#23545;&#12290;&#36825;&#26159;&#22240;&#20026;&#21306;&#22495;&#24615;&#20002;&#22833;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#36755;&#20837;&#37319;&#26679;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20174;&#27491;&#26679;&#26412;&#20013;&#20002;&#24323;&#20449;&#24687;&#24615;&#20687;&#32032;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#25928;&#29575;&#20302;&#19979;&#65292;&#29992;&#19981;&#21516;&#23545;&#35937;&#30340;&#34917;&#19969;&#26367;&#25442;&#29305;&#23450;&#23545;&#35937;&#30340;&#34917;&#19969;&#21487;&#20197;&#20351;&#27169;&#22411;&#26080;&#27861;&#26368;&#22823;&#21270;&#19981;&#21516;&#27491;&#26679;&#26412;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#32852;&#21512;&#23884;&#20837;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#23578;&#26410;&#23558;&#31283;&#20581;&#24615;&#20316;&#20026;&#20854;&#20027;&#35201;&#35757;&#32451;&#30446;&#26631;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ViewMix&#22686;&#24378;&#31574;&#30053;&#65292;&#29305;&#21035;&#38024;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint Embedding Architecture-based self-supervised learning methods have attributed the composition of data augmentations as a crucial factor for their strong representation learning capabilities. While regional dropout strategies have proven to guide models to focus on lesser indicative parts of the objects in supervised methods, it hasn't been adopted by self-supervised methods for generating positive pairs. This is because the regional dropout methods are not suitable for the input sampling process of the self-supervised methodology. Whereas dropping informative pixels from the positive pairs can result in inefficient training, replacing patches of a specific object with a different one can steer the model from maximizing the agreement between different positive pairs. Moreover, joint embedding representation learning methods have not made robustness their primary training outcome. To this end, we propose the ViewMix augmentation policy, specially designed for self-supervised learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#25104;&#32447;&#24615;&#25554;&#20540;&#22120;&#22914;&#20309;&#31283;&#23450;&#21644;&#25552;&#21319;&#20010;&#20307;&#25554;&#20540;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20056;&#25968;&#33258;&#21161;&#27861;&#30340;&#34955;&#35013;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#12290;&#22312;&#27604;&#20363;&#21306;&#22495;&#20869;&#65292;&#30740;&#31350;&#20102;&#31616;&#21270;&#21644;&#34955;&#35013;&#30340;&#22806;&#26679;&#26412;&#39044;&#27979;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.03354</link><description>&lt;p&gt;
&#38598;&#25104;&#32447;&#24615;&#25554;&#20540;&#22120;: &#38598;&#25104;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Ensemble linear interpolators: The role of ensembling. (arXiv:2309.03354v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#25104;&#32447;&#24615;&#25554;&#20540;&#22120;&#22914;&#20309;&#31283;&#23450;&#21644;&#25552;&#21319;&#20010;&#20307;&#25554;&#20540;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20056;&#25968;&#33258;&#21161;&#27861;&#30340;&#34955;&#35013;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#12290;&#22312;&#27604;&#20363;&#21306;&#22495;&#20869;&#65292;&#30740;&#31350;&#20102;&#31616;&#21270;&#21644;&#34955;&#35013;&#30340;&#22806;&#26679;&#26412;&#39044;&#27979;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25554;&#20540;&#22120;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#20363;&#22914;&#65292;&#24403;&#22788;&#29702;&#22122;&#22768;&#25968;&#25454;&#26102;&#65292;&#26368;&#23567;l2&#33539;&#25968;&#26368;&#23567;&#20108;&#20056;&#25554;&#20540;&#22120;&#30340;&#27979;&#35797;&#35823;&#24046;&#20250;&#26080;&#30028;&#22686;&#38271;&#12290;&#26412;&#25991;&#30740;&#31350;&#38598;&#25104;&#22914;&#20309;&#31283;&#23450;&#21644;&#25552;&#21319;&#20010;&#20307;&#25554;&#20540;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#22806;&#26679;&#26412;&#39044;&#27979;&#39118;&#38505;&#36827;&#34892;&#34913;&#37327;&#12290;&#25105;&#20204;&#20197;&#34955;&#35013;&#32447;&#24615;&#25554;&#20540;&#22120;&#20026;&#37325;&#28857;&#65292;&#22240;&#20026;&#34955;&#35013;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#38543;&#26426;&#21270;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#24182;&#34892;&#23454;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#20056;&#25968;&#33258;&#21161;&#27861;&#30340;&#34955;&#35013;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#23558;&#20854;&#34920;&#36798;&#20026;&#31616;&#21270;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#24179;&#22343;&#20540;&#12290;&#25152;&#25552;&#20986;&#30340;&#20056;&#25968;&#33258;&#21161;&#27861;&#21253;&#21547;&#20102;&#32463;&#20856;&#30340;&#26377;&#25918;&#22238;&#33258;&#21161;&#27861;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#20197;&#21450;&#26356;&#26377;&#36259;&#30340;&#21464;&#20307;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20271;&#21162;&#21033;&#33258;&#21161;&#27861;&#12290;&#22312;&#26679;&#26412;&#22823;&#23567;&#19982;&#29305;&#24449;&#32500;&#24230;&#25104;&#27491;&#27604;&#30340;&#27604;&#20363;&#21306;&#22495;&#20869;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31616;&#21270;&#21644;&#34955;&#35013;&#30340;&#22806;&#26679;&#26412;&#39044;&#27979;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpolators are unstable. For example, the mininum $\ell_2$ norm least square interpolator exhibits unbounded test errors when dealing with noisy data. In this paper, we study how ensemble stabilizes and thus improves the generalization performance, measured by the out-of-sample prediction risk, of an individual interpolator. We focus on bagged linear interpolators, as bagging is a popular randomization-based ensemble method that can be implemented in parallel. We introduce the multiplier-bootstrap-based bagged least square estimator, which can then be formulated as an average of the sketched least square estimators. The proposed multiplier bootstrap encompasses the classical bootstrap with replacement as a special case, along with a more intriguing variant which we call the Bernoulli bootstrap.  Focusing on the proportional regime where the sample size scales proportionally with the feature dimensionality, we investigate the out-of-sample prediction risks of the sketched and bagged 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30450;&#21462;&#35777;&#25216;&#26415;&#65292;&#29992;&#20110;&#25968;&#23383;&#35270;&#39057;&#20013;&#28304;&#25668;&#20687;&#26426;&#30340;&#35782;&#21035;&#19982;&#26816;&#27979;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#29305;&#24449;&#25552;&#21462;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#28304;&#20998;&#31867;&#65292;&#26377;&#25928;&#22320;&#30830;&#23450;&#35270;&#39057;&#30340;&#21407;&#22987;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.03353</link><description>&lt;p&gt;
&#25968;&#23383;&#35270;&#39057;&#20013;&#30340;&#28304;&#25668;&#20687;&#26426;&#35782;&#21035;&#19982;&#26816;&#27979;&#30340;&#30450;&#21462;&#35777;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Source Camera Identification and Detection in Digital Videos through Blind Forensics. (arXiv:2309.03353v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30450;&#21462;&#35777;&#25216;&#26415;&#65292;&#29992;&#20110;&#25968;&#23383;&#35270;&#39057;&#20013;&#28304;&#25668;&#20687;&#26426;&#30340;&#35782;&#21035;&#19982;&#26816;&#27979;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#29305;&#24449;&#25552;&#21462;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#28304;&#20998;&#31867;&#65292;&#26377;&#25928;&#22320;&#30830;&#23450;&#35270;&#39057;&#30340;&#21407;&#22987;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#35270;&#39057;&#20013;&#30340;&#28304;&#25668;&#20687;&#26426;&#35782;&#21035;&#26159;&#23558;&#26410;&#30693;&#25968;&#23383;&#35270;&#39057;&#19982;&#20854;&#28304;&#35774;&#22791;&#20851;&#32852;&#36215;&#26469;&#65292;&#22312;&#21487;&#33021;&#35774;&#22791;&#30340;&#26377;&#38480;&#38598;&#21512;&#20869;&#36827;&#34892;&#12290;&#23384;&#22312;&#30340;&#28304;&#26816;&#27979;&#25216;&#26415;&#35797;&#22270;&#22312;&#35270;&#39057;&#20013;&#25214;&#21040;&#23454;&#38469;&#28304;&#30340;&#25351;&#32441;&#65292;&#37319;&#29992;PRNU&#65288;&#20809;&#21709;&#24212;&#38750;&#22343;&#21248;&#24615;&#65289;&#30340;&#24418;&#24335;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#21487;&#33021;&#35774;&#22791;&#30340;SPN&#65288;&#20256;&#24863;&#22120;&#27169;&#24335;&#22122;&#22768;&#65289;&#36827;&#34892;&#21305;&#37197;&#12290;&#26368;&#39640;&#30340;&#30456;&#20851;&#24615;&#34920;&#31034;&#27491;&#30830;&#30340;&#28304;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#30740;&#31350;&#35270;&#39057;&#28304;&#30340;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#21518;&#32493;&#28304;&#20998;&#31867;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30450;&#21462;&#35777;&#25216;&#26415;&#65292;&#29992;&#20110;&#35270;&#39057;&#28304;&#30340;&#35748;&#35777;&#21644;&#35782;&#21035;&#12290;&#20027;&#35201;&#30446;&#30340;&#26159;&#30830;&#23450;&#35270;&#39057;&#30340;&#22768;&#26126;&#28304;&#26159;&#21542;&#20026;&#20854;&#21407;&#22987;&#28304;&#65292;&#22914;&#26524;&#19981;&#26159;&#65292;&#25105;&#20204;&#23558;&#35782;&#21035;&#20854;&#21407;&#22987;&#28304;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Source camera identification in digital videos is the problem of associating an unknown digital video with its source device, within a closed set of possible devices. The existing techniques in source detection of digital videos try to find a fingerprint of the actual source in the video in form of PRNU (Photo Response Non--Uniformity), and match it against the SPN (Sensor Pattern Noise) of each possible device. The highest correlation indicates the correct source. We investigate the problem of identifying a video source through a feature based approach using machine learning. In this paper, we present a blind forensic technique of video source authentication and identification, based on feature extraction, feature selection and subsequent source classification. The main aim is to determine whether a claimed source for a video is actually its original source. If not, we identify its original source. Our experimental results prove the efficiency of the proposed method compared to tradit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#29992;&#20110;&#24555;&#36895;&#20272;&#35745;&#39640;&#20998;&#36776;&#29575;SAR&#22270;&#20687;&#30340;&#31895;&#31961;&#24230;&#21442;&#25968;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#24555;&#36895;&#12289;&#35823;&#24046;&#26356;&#23567;&#19988;&#26356;&#21487;&#38752;&#12290;</title><link>http://arxiv.org/abs/2309.03351</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#24555;&#36895;&#20272;&#35745;&#39640;&#20998;&#36776;&#29575;SAR&#22270;&#20687;&#30340;&#31895;&#31961;&#24230;
&lt;/p&gt;
&lt;p&gt;
Using Neural Networks for Fast SAR Roughness Estimation of High Resolution Images. (arXiv:2309.03351v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#29992;&#20110;&#24555;&#36895;&#20272;&#35745;&#39640;&#20998;&#36776;&#29575;SAR&#22270;&#20687;&#30340;&#31895;&#31961;&#24230;&#21442;&#25968;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#24555;&#36895;&#12289;&#35823;&#24046;&#26356;&#23567;&#19988;&#26356;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#22270;&#20687;&#30340;&#20998;&#26512;&#26159;&#36965;&#24863;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#25955;&#26001;&#22122;&#22768;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;$G_I^0$&#20998;&#24067;&#27169;&#22411;&#21270;&#25968;&#25454;&#24182;&#25552;&#21462;&#20854;&#31895;&#31961;&#24230;&#20449;&#24687;&#65292;&#36825;&#26679;&#21487;&#20197;&#22312;&#21518;&#32493;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#20013;&#20351;&#29992;&#65292;&#22914;&#20998;&#21106;&#12289;&#20998;&#31867;&#21644;&#35299;&#37322;&#12290;&#36825;&#38656;&#35201;&#20174;SAR&#25968;&#25454;&#20013;&#24555;&#36895;&#32780;&#21487;&#38752;&#22320;&#20272;&#35745;&#31895;&#31961;&#24230;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20256;&#32479;&#30340;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#36895;&#24230;&#24930;&#19988;&#23481;&#26131;&#20986;&#29616;&#20272;&#35745;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#26694;&#26550;&#65292;&#39318;&#20808;&#23398;&#20064;&#22914;&#20309;&#39044;&#27979;$G_I^0$&#26679;&#26412;&#30340;&#22522;&#26412;&#21442;&#25968;&#65292;&#28982;&#21518;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#26410;&#35265;&#25968;&#25454;&#30340;&#31895;&#31961;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#19968;&#20010;&#27604;&#20256;&#32479;&#20272;&#35745;&#26041;&#27861;&#26356;&#24555;&#12289;&#35823;&#24046;&#26356;&#23567;&#19988;&#38590;&#20197;&#20986;&#38169;&#30340;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of Synthetic Aperture Radar (SAR) imagery is an important step in remote sensing applications, and it is a challenging problem due to its inherent speckle noise. One typical solution is to model the data using the $G_I^0$ distribution and extract its roughness information, which in turn can be used in posterior imaging tasks, such as segmentation, classification and interpretation. This leads to the need of quick and reliable estimation of the roughness parameter from SAR data, especially with high resolution images. Unfortunately, traditional parameter estimation procedures are slow and prone to estimation failures. In this work, we proposed a neural network-based estimation framework that first learns how to predict underlying parameters of $G_I^0$ samples and then can be used to estimate the roughness of unseen data. We show that this approach leads to an estimator that is quicker, yields less estimation error and is less prone to failures than the traditional estimatio
&lt;/p&gt;</description></item><item><title>&#20013;&#32487;&#25193;&#25955;&#27169;&#22411;&#65288;RDM&#65289;&#36890;&#36807;&#27169;&#31946;&#25193;&#25955;&#21644;&#22359;&#22122;&#22768;&#23558;&#20302;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#25110;&#22122;&#22768;&#36716;&#21270;&#20026;&#31561;&#25928;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#20998;&#36776;&#29575;&#19979;&#26080;&#38656;&#37325;&#26032;&#24320;&#22987;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03350</link><description>&lt;p&gt;
&#20013;&#32487;&#25193;&#25955;&#65306;&#32479;&#19968;&#19981;&#21516;&#20998;&#36776;&#29575;&#19979;&#30340;&#22270;&#20687;&#21512;&#25104;&#25193;&#25955;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Relay Diffusion: Unifying diffusion process across resolutions for image synthesis. (arXiv:2309.03350v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03350
&lt;/p&gt;
&lt;p&gt;
&#20013;&#32487;&#25193;&#25955;&#27169;&#22411;&#65288;RDM&#65289;&#36890;&#36807;&#27169;&#31946;&#25193;&#25955;&#21644;&#22359;&#22122;&#22768;&#23558;&#20302;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#25110;&#22122;&#22768;&#36716;&#21270;&#20026;&#31561;&#25928;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#20998;&#36776;&#29575;&#19979;&#26080;&#38656;&#37325;&#26032;&#24320;&#22987;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#39640;&#20998;&#36776;&#29575;&#29983;&#25104;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#36890;&#36807;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#65292;&#25105;&#20204;&#21457;&#29616;&#20027;&#35201;&#21407;&#22240;&#22312;&#20110;&#8220;&#22312;&#39640;&#20998;&#36776;&#29575;&#19978;&#30456;&#21516;&#30340;&#22122;&#22768;&#27700;&#24179;&#20250;&#23548;&#33268;&#39057;&#22495;&#20013;&#36739;&#39640;&#30340;&#20449;&#22122;&#27604;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20013;&#32487;&#25193;&#25955;&#27169;&#22411;&#65288;RDM&#65289;&#65292;&#36890;&#36807;&#27169;&#31946;&#25193;&#25955;&#21644;&#22359;&#22122;&#22768;&#23558;&#20302;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#25110;&#22122;&#22768;&#36716;&#21270;&#20026;&#31561;&#25928;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#20197;&#20379;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25193;&#25955;&#36807;&#31243;&#21487;&#20197;&#22312;&#20219;&#20309;&#26032;&#30340;&#20998;&#36776;&#29575;&#25110;&#27169;&#22411;&#19978;&#26080;&#32541;&#22320;&#32487;&#32493;&#36827;&#34892;&#65292;&#32780;&#26080;&#38656;&#20174;&#32431;&#22122;&#22768;&#25110;&#20302;&#20998;&#36776;&#29575;&#26465;&#20214;&#37325;&#26032;&#24320;&#22987;&#12290;RDM&#22312;CelebA-HQ&#21644;ImageNet 256&#215;256&#19978;&#37117;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#21644;sFID&#65292;&#22823;&#22823;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;ADM&#12289;LDM&#21644;DiT&#31561;&#24037;&#20316;&#12290;&#25152;&#26377;&#30340;&#20195;&#30721;&#21644;&#26816;&#26597;&#28857;&#37117;&#20197;&#24320;&#28304;&#26041;&#24335;&#22312;https://github.com/THUDM/RelayDiffusion&#19978;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that \emph{the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain}. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256$\times$256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at \url{https://github.com/THUDM/RelayDiffusion}.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#28789;&#24039;&#25805;&#32437;&#25216;&#33021;&#30340;&#39640;&#25928;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#24341;&#23548;&#65292;&#23454;&#29616;&#20102;&#37325;&#29992;&#25968;&#25454;&#20197;&#38477;&#20302;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03322</link><description>&lt;p&gt;
REBOOT: &#37325;&#29992;&#25968;&#25454;&#20197;&#24341;&#23548;&#39640;&#25928;&#30340;&#29616;&#23454;&#19990;&#30028;&#28789;&#24039;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation. (arXiv:2309.03322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#28789;&#24039;&#25805;&#32437;&#25216;&#33021;&#30340;&#39640;&#25928;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#24341;&#23548;&#65292;&#23454;&#29616;&#20102;&#37325;&#29992;&#25968;&#25454;&#20197;&#38477;&#20302;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28041;&#21450;&#25509;&#35302;&#23494;&#38598;&#20132;&#20114;&#30340;&#28789;&#24039;&#25805;&#32437;&#20219;&#21153;&#65292;&#27169;&#22411;&#39537;&#21160;&#30340;&#25511;&#21046;&#31995;&#32479;&#21644;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#37117;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22797;&#26434;&#24615;&#26469;&#33258;&#20110;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#38656;&#35201;&#21160;&#24577;&#24314;&#31435;&#21644;&#26029;&#24320;&#25509;&#35302;&#12289;&#24179;&#34913;&#38750;&#20280;&#25163;&#25345;&#21147;&#24182;&#25511;&#21046;&#22823;&#37327;&#33258;&#30001;&#24230;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30001;&#20110;&#20854;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#33258;&#20027;&#33719;&#21462;&#26368;&#20339;&#25805;&#32437;&#31574;&#30053;&#30340;&#33021;&#21147;&#32780;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#24120;&#24120;&#21463;&#21040;&#29983;&#25104;&#22823;&#37327;&#26679;&#26412;&#12289;&#37325;&#32622;&#29615;&#22659;&#21644;&#33719;&#21462;&#22870;&#21169;&#20449;&#21495;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;RL&#30340;&#28789;&#24039;&#25805;&#32437;&#25216;&#33021;&#30340;&#39640;&#25928;&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#23558;&#26368;&#36817;&#22312;&#26679;&#26412;&#39640;&#25928;RL&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#24341;&#23548;&#26041;&#38754;&#30340;&#36827;&#23637;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#32452;&#21512;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;&#25110;&#29289;&#20307;&#30340;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dexterous manipulation tasks involving contact-rich interactions pose a significant challenge for both model-based control systems and imitation learning algorithms. The complexity arises from the need for multi-fingered robotic hands to dynamically establish and break contacts, balance non-prehensile forces, and control large degrees of freedom. Reinforcement learning (RL) offers a promising approach due to its general applicability and capacity to autonomously acquire optimal manipulation strategies. However, its real-world application is often hindered by the necessity to generate a large number of samples, reset the environment, and obtain reward signals. In this work, we introduce an efficient system for learning dexterous manipulation skills with RL to alleviate these challenges. The main idea of our approach is the integration of recent advances in sample-efficient RL and replay buffer bootstrapping. This combination allows us to utilize data from different tasks or objects as a
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36951;&#20256;&#31639;&#27861;&#20013;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36827;&#21270;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#36866;&#24212;&#24230;&#24471;&#20998;&#35201;&#20040;&#19982;&#23436;&#20840;&#36816;&#34892;&#30340;&#36951;&#20256;&#31639;&#27861;&#30456;&#21516;&#65292;&#35201;&#20040;&#31245;&#24494;&#20302;&#19968;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.03318</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Fitness Approximation through Machine Learning. (arXiv:2309.03318v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03318
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36951;&#20256;&#31639;&#27861;&#20013;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36827;&#21270;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#36866;&#24212;&#24230;&#24471;&#20998;&#35201;&#20040;&#19982;&#23436;&#20840;&#36816;&#34892;&#30340;&#36951;&#20256;&#31639;&#27861;&#30456;&#21516;&#65292;&#35201;&#20040;&#31245;&#24494;&#20302;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36951;&#20256;&#31639;&#27861;&#20013;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;&#65292;&#37325;&#28857;&#26159;&#22312;Gymnasium&#65288;&#28216;&#25103;&#65289;&#27169;&#25311;&#22120;&#20013;&#30340;&#36827;&#21270;&#20195;&#29702;&#19978; - &#22312;&#36825;&#37324;&#36866;&#24212;&#24230;&#35745;&#31639;&#26159;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#32500;&#25252;&#19968;&#20010;&#37319;&#26679;&#20010;&#20307;&#21450;&#20854;&#23454;&#38469;&#36866;&#24212;&#24230;&#24471;&#20998;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#25972;&#20010;&#36827;&#21270;&#36807;&#31243;&#20013;&#19981;&#26029;&#26356;&#26032;&#19968;&#20010;&#36866;&#24212;&#24230;&#36817;&#20284;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;1&#65289;&#22312;&#23454;&#38469;&#36866;&#24212;&#24230;&#21644;&#36817;&#20284;&#36866;&#24212;&#24230;&#20043;&#38388;&#20999;&#25442;&#65292;2&#65289;&#23545;&#31181;&#32676;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#21450;3&#65289;&#21152;&#26435;&#37319;&#26679;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36866;&#24212;&#24230;&#35745;&#31639;&#30340;&#36817;&#20284;&#27604;&#20363;&#21462;&#20915;&#20110;&#23436;&#20840;&#36816;&#34892;GA&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36827;&#21270;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#36866;&#24212;&#24230;&#24471;&#20998;&#35201;&#20040;&#19982;&#23436;&#20840;&#36816;&#34892;&#30340;GA&#30456;&#21516;&#65292;&#35201;&#20040;&#31245;&#24494;&#20302;&#19968;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#24212;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to performing fitness approximation in genetic algorithms (GAs) using machine-learning (ML) models, focusing on evolutionary agents in Gymnasium (game) simulators -- where fitness computation is costly. Maintaining a dataset of sampled individuals along with their actual fitness scores, we continually update throughout an evolutionary run a fitness-approximation ML model. We compare different methods for: 1) switching between actual and approximate fitness, 2) sampling the population, and 3) weighting the samples. Experimental findings demonstrate significant improvement in evolutionary runtimes, with fitness scores that are either identical or slightly lower than that of the fully run GA -- depending on the ratio of approximate-to-actual-fitness computation. Our approach is generic and can be easily applied to many different domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#19968;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#25968;&#30334;&#27425;&#30340;&#20050;&#20051;&#29699;&#22238;&#21512;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#23558;&#29699;&#36820;&#22238;&#21040;&#39044;&#23450;&#30340;&#30446;&#26631;&#20301;&#32622;&#12290;&#35813;&#31995;&#32479;&#25972;&#21512;&#20102;&#24863;&#30693;&#23376;&#31995;&#32479;&#12289;&#39640;&#36895;&#20302;&#24310;&#36831;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#12289;&#20223;&#30495;&#33539;&#20363;&#12289;&#33258;&#21160;&#37325;&#32622;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#31561;&#21151;&#33021;&#65292;&#24182;&#23545;&#31995;&#32479;&#30340;&#35774;&#35745;&#20915;&#31574;&#21644;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#35814;&#32454;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2309.03315</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20050;&#20051;&#29699;&#65306;&#19968;&#20010;&#39640;&#36895;&#23398;&#20064;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Robotic Table Tennis: A Case Study into a High Speed Learning System. (arXiv:2309.03315v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#19968;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#25968;&#30334;&#27425;&#30340;&#20050;&#20051;&#29699;&#22238;&#21512;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#23558;&#29699;&#36820;&#22238;&#21040;&#39044;&#23450;&#30340;&#30446;&#26631;&#20301;&#32622;&#12290;&#35813;&#31995;&#32479;&#25972;&#21512;&#20102;&#24863;&#30693;&#23376;&#31995;&#32479;&#12289;&#39640;&#36895;&#20302;&#24310;&#36831;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#12289;&#20223;&#30495;&#33539;&#20363;&#12289;&#33258;&#21160;&#37325;&#32622;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#31561;&#21151;&#33021;&#65292;&#24182;&#23545;&#31995;&#32479;&#30340;&#35774;&#35745;&#20915;&#31574;&#21644;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#35814;&#32454;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#30340;&#28145;&#20837;&#30740;&#31350;&#65292;&#27492;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#34920;&#26126;&#35813;&#31995;&#32479;&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#25968;&#30334;&#27425;&#30340;&#20050;&#20051;&#29699;&#22238;&#21512;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#23558;&#29699;&#36820;&#22238;&#21040;&#39044;&#23450;&#30340;&#30446;&#26631;&#20301;&#32622;&#12290;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;&#24863;&#30693;&#23376;&#31995;&#32479;&#12289;&#39640;&#36895;&#20302;&#24310;&#36831;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#12289;&#38450;&#27490;&#29616;&#23454;&#19990;&#30028;&#20013;&#25439;&#22351;&#24182;&#33021;&#22815;&#36827;&#34892;&#38646;-shot&#36716;&#31227;&#31574;&#30053;&#35757;&#32451;&#30340;&#20223;&#30495;&#33539;&#20363;&#65292;&#20197;&#21450;&#33258;&#21160;&#37325;&#32622;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#65292;&#20351;&#33258;&#20027;&#35757;&#32451;&#21644;&#35780;&#20272;&#22312;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#25972;&#20010;&#31995;&#32479;&#65292;&#21253;&#25324;&#36890;&#24120;&#19981;&#24191;&#27867;&#20256;&#25773;&#30340;&#22823;&#37327;&#35774;&#35745;&#20915;&#31574;&#65292;&#24182;&#32467;&#21512;&#19968;&#31995;&#21015;&#30740;&#31350;&#26469;&#38416;&#26126;&#32531;&#35299;&#21508;&#31181;&#24310;&#36831;&#28304;&#30340;&#37325;&#35201;&#24615;&#12289;&#32771;&#34385;&#35757;&#32451;&#21644;&#37096;&#32626;&#20998;&#24067;&#21464;&#21270;&#12289;&#24863;&#30693;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#12289;&#31574;&#30053;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#21644;&#21160;&#20316;&#31354;&#38388;&#36873;&#25321;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#35270;&#39057;&#23637;&#31034;&#20102;&#31995;&#32479;&#30340;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep-dive into a real-world robotic learning system that, in previous work, was shown to be capable of hundreds of table tennis rallies with a human and has the ability to precisely return the ball to desired targets. This system puts together a highly optimized perception subsystem, a high-speed low-latency robot controller, a simulation paradigm that can prevent damage in the real world and also train policies for zero-shot transfer, and automated real world environment resets that enable autonomous training and evaluation on physical robots. We complement a complete system description, including numerous design decisions that are typically not widely disseminated, with a collection of studies that clarify the importance of mitigating various sources of latency, accounting for training and deployment distribution shifts, robustness of the perception system, sensitivity to policy hyper-parameters, and choice of action space. A video demonstrating the components of the sys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#29983;&#25104;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#35775;&#38382;&#65292;&#24182;&#22312;&#20248;&#21270;&#30005;&#36335;&#37197;&#32622;&#26102;&#21516;&#26102;&#32771;&#34385;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#38376;&#25104;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26368;&#20339;&#30005;&#36335;&#37197;&#32622;&#20013;&#32416;&#32544;&#38376;&#38656;&#35201;&#30456;&#24212;&#30340;&#25968;&#37327;&#65292;&#19982;&#20043;&#21069;&#30740;&#31350;&#30456;&#21453;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#25968;&#25454;&#30340;&#21487;&#20998;&#31163;&#24615;&#25351;&#25968;&#30830;&#23450;&#26368;&#20339;&#37197;&#32622;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03307</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#29983;&#25104;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generating quantum feature maps using multi-objective genetic algorithm. (arXiv:2309.03307v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#29983;&#25104;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#35775;&#38382;&#65292;&#24182;&#22312;&#20248;&#21270;&#30005;&#36335;&#37197;&#32622;&#26102;&#21516;&#26102;&#32771;&#34385;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#38376;&#25104;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26368;&#20339;&#30005;&#36335;&#37197;&#32622;&#20013;&#32416;&#32544;&#38376;&#38656;&#35201;&#30456;&#24212;&#30340;&#25968;&#37327;&#65292;&#19982;&#20043;&#21069;&#30740;&#31350;&#30456;&#21453;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#25968;&#25454;&#30340;&#21487;&#20998;&#31163;&#24615;&#25351;&#25968;&#30830;&#23450;&#26368;&#20339;&#37197;&#32622;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#29992;&#20110;&#37327;&#23376;&#22686;&#24378;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#39640;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#35775;&#38382;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#26368;&#22823;&#21270;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#30005;&#36335;&#30340;&#26412;&#22320;&#38376;&#25104;&#26412;&#21644;&#38750;&#26412;&#22320;&#38376;&#25104;&#26412;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20026;&#26412;&#22320;&#38376;&#21644;&#32416;&#32544;&#38376;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#36866;&#24212;&#24230;&#20989;&#25968;&#12290;&#19982;&#32463;&#20856;&#20998;&#31867;&#22120;&#30340;&#27604;&#36739;&#26377;&#21161;&#20110;&#29702;&#35299;&#20351;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#37327;&#23376;&#26680;&#26041;&#27861;&#30340;&#26368;&#20339;&#30005;&#36335;&#37197;&#32622;&#20013;&#21253;&#21547;&#20102;&#30456;&#24212;&#25968;&#37327;&#30340;&#38750;&#26412;&#22320;&#38376;&#29992;&#20110;&#32416;&#32544;&#65292;&#19982;&#20043;&#21069;&#30340;&#25991;&#29486;&#30456;&#21453;&#65292;&#20043;&#21069;&#30340;&#25991;&#29486;&#20013;&#38750;&#26412;&#22320;&#38376;&#34987;&#22823;&#37096;&#20998;&#25233;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#25968;&#25454;&#30340;&#21487;&#20998;&#31163;&#24615;&#25351;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#30830;&#23450;&#26368;&#20339;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for efficiently generating quantum feature maps for quantum-enhanced support vector machines, a kernel-based classifier, enabling access to high-dimensional Hilbert space. Our method employs a multi-objective genetic algorithm that simultaneously maximizes classification accuracy while minimizing both the local and non-local gate costs of the quantum feature map's circuit. To achieve this, we define distinct fitness functions for local gates and entanglement gates. Comparisons with classical classifiers are given in order to understand the advantages of using quantum machine learning. Surprisingly, our experiments reveal that the optimal configuration of quantum circuits for the quantum kernel method incorporates a proportional number of non-local gates for entanglement, contrary to previous literature where non-local gates were largely suppressed.  Furthermore, we demonstrate that the separability indexes of data can be effectively leveraged to determine th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36882;&#24402;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#20837;&#20405;&#21709;&#24212;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#35299;&#20915;&#24182;&#34892;&#23376;&#28216;&#25103;&#21644;&#35745;&#31639;&#38408;&#20540;&#32467;&#26500;&#30340;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.03292</link><description>&lt;p&gt;
&#36890;&#36807;&#36882;&#24402;&#20998;&#35299;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#20837;&#20405;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Scalable Learning of Intrusion Responses through Recursive Decomposition. (arXiv:2309.03292v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36882;&#24402;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#20837;&#20405;&#21709;&#24212;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#35299;&#20915;&#24182;&#34892;&#23376;&#28216;&#25103;&#21644;&#35745;&#31639;&#38408;&#20540;&#32467;&#26500;&#30340;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#20837;&#20405;&#24212;&#23545;&#65292;&#24182;&#23558;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#24418;&#24335;&#24314;&#27169;&#20026;&#37096;&#20998;&#35266;&#27979;&#30340;&#38543;&#26426;&#28216;&#25103;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#28216;&#25103;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#33258;&#25105;&#23545;&#24328;&#36827;&#34892;&#21327;&#21516;&#28436;&#21270;&#65292;&#20197;&#36798;&#21040;&#24179;&#34913;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23567;&#22411;&#22522;&#30784;&#35774;&#26045;&#30340;&#21487;&#34892;&#24615;&#65292;&#20294;&#38754;&#23545;&#23454;&#38469;&#24773;&#22659;&#30001;&#20110;&#22522;&#30784;&#35774;&#26045;&#35268;&#27169;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#32780;&#26080;&#27861;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28216;&#25103;&#36882;&#24402;&#20998;&#35299;&#25104;&#21487;&#20197;&#24182;&#34892;&#35299;&#20915;&#30340;&#23376;&#28216;&#25103;&#30340;&#26041;&#27861;&#12290;&#24212;&#29992;&#26368;&#20248;&#20572;&#27490;&#29702;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#23376;&#28216;&#25103;&#20013;&#30340;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#20855;&#26377;&#38408;&#20540;&#32467;&#26500;&#65292;&#36825;&#20801;&#35768;&#25105;&#20204;&#39640;&#25928;&#22320;&#35745;&#31639;&#23427;&#20204;&#12290;&#20026;&#20102;&#35299;&#20915;&#20998;&#35299;&#30340;&#28216;&#25103;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Decompositional Fictitious Self-Play (DFSP) &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#33258;&#25105;&#23545;&#24328;&#23398;&#20064;&#32435;&#20160;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study automated intrusion response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed stochastic game. To solve the game we follow an approach where attack and defense strategies co-evolve through reinforcement learning and self-play toward an equilibrium. Solutions proposed in previous work prove the feasibility of this approach for small infrastructures but do not scale to realistic scenarios due to the exponential growth in computational complexity with the infrastructure size. We address this problem by introducing a method that recursively decomposes the game into subgames which can be solved in parallel. Applying optimal stopping theory we show that the best response strategies in these subgames exhibit threshold structures, which allows us to compute them efficiently. To solve the decomposed game we introduce an algorithm called Decompositional Fictitious Self-Play (DFSP), which learns Nash equilibria through stoc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#39057;&#29575;&#30340;&#37327;&#23376;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#22120;&#20013;&#24341;&#20837;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#29983;&#25104;&#22120;&#65292;&#21253;&#25324;&#38750;&#24120;&#35268;&#38388;&#38548;&#39057;&#29575;&#21644;&#28789;&#27963;&#30340;&#39057;&#35889;&#20016;&#23500;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;Navier-Stokes&#26041;&#31243;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03279</link><description>&lt;p&gt;
&#35753;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#36873;&#25321;&#33258;&#24049;&#30340;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
Let Quantum Neural Networks Choose Their Own Frequencies. (arXiv:2309.03279v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03279
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#39057;&#29575;&#30340;&#37327;&#23376;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#22120;&#20013;&#24341;&#20837;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#29983;&#25104;&#22120;&#65292;&#21253;&#25324;&#38750;&#24120;&#35268;&#38388;&#38548;&#39057;&#29575;&#21644;&#28789;&#27963;&#30340;&#39057;&#35889;&#20016;&#23500;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;Navier-Stokes&#26041;&#31243;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#23558;&#36755;&#20837;&#29305;&#24449;&#30340;&#37096;&#20998;&#20613;&#31435;&#21494;&#32423;&#25968;&#34920;&#31034;&#26469;&#25551;&#36848;&#65292;&#20854;&#20013;&#39057;&#29575;&#30001;&#29305;&#24449;&#26144;&#23556;&#30340;&#29983;&#25104;&#21704;&#23494;&#39039;&#37327;&#21807;&#19968;&#30830;&#23450;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#25968;&#25454;&#32534;&#30721;&#29983;&#25104;&#22120;&#26159;&#25552;&#21069;&#36873;&#25321;&#30340;&#65292;&#22266;&#23450;&#20102;&#21487;&#20197;&#34920;&#31034;&#30340;&#20989;&#25968;&#31354;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#37327;&#23376;&#27169;&#22411;&#25512;&#24191;&#21040;&#29983;&#25104;&#22120;&#20013;&#21253;&#25324;&#19968;&#32452;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#21487;&#35757;&#32451;&#39057;&#29575;&#30340;&#37327;&#23376;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#21487;&#35757;&#32451;&#39057;&#29575;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#29983;&#25104;&#22120;&#65292;&#21253;&#25324;&#20854;&#39057;&#35889;&#20013;&#30340;&#38750;&#24120;&#35268;&#38388;&#38548;&#39057;&#29575;&#21644;&#28789;&#27963;&#30340;&#39057;&#35889;&#20016;&#23500;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#20165;&#23545;&#27599;&#20010;&#32534;&#30721;&#25805;&#20316;&#28155;&#21152;&#19968;&#20010;&#21442;&#25968;&#30340;&#21487;&#35757;&#32451;&#39057;&#29575;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27714;&#35299;Navier-Stokes&#26041;&#31243;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameterized quantum circuits as machine learning models are typically well described by their representation as a partial Fourier series of the input features, with frequencies uniquely determined by the feature map's generator Hamiltonians. Ordinarily, these data-encoding generators are chosen in advance, fixing the space of functions that can be represented. In this work we consider a generalization of quantum models to include a set of trainable parameters in the generator, leading to a trainable frequency (TF) quantum model. We numerically demonstrate how TF models can learn generators with desirable properties for solving the task at hand, including non-regularly spaced frequencies in their spectra and flexible spectral richness. Finally, we showcase the real-world effectiveness of our approach, demonstrating an improved accuracy in solving the Navier-Stokes equations using a TF model with only a single parameter added to each encoding operation. Since TF models encompass conven
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#65292;&#37319;&#29992;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.03251</link><description>&lt;p&gt;
&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning. (arXiv:2309.03251v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#65292;&#37319;&#29992;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#26159;&#20256;&#32479;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#30340;&#25193;&#23637;&#65292;&#34701;&#20837;&#20102;&#26102;&#38388;&#32500;&#24230;&#12290;&#22312;TKGs&#19978;&#36827;&#34892;&#25512;&#29702;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#20107;&#20214;&#39044;&#27979;&#26410;&#26469;&#20107;&#23454;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#25581;&#31034;&#21382;&#21490;&#23376;&#22270;&#21644;&#26102;&#38388;&#27169;&#24335;&#20013;&#30340;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#38752;&#23454;&#20307;&#24314;&#27169;&#26469;&#27169;&#25311;TKGs&#65292;&#22240;&#20026;&#22270;&#20013;&#30340;&#33410;&#28857;&#22312;&#30693;&#35782;&#34920;&#31034;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#22330;&#26223;&#36890;&#24120;&#28041;&#21450;&#22823;&#37327;&#23454;&#20307;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20250;&#20986;&#29616;&#26032;&#23454;&#20307;&#12290;&#36825;&#20351;&#24471;&#20381;&#36182;&#20110;&#23454;&#20307;&#30340;&#26041;&#27861;&#24456;&#38590;&#24212;&#23545;&#22823;&#37327;&#23454;&#20307;&#65292;&#24182;&#19988;&#26377;&#25928;&#22788;&#29702;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#20063;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#65292;&#23427;&#20197;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#23545;&#21382;&#21490;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TiPNN&#37319;&#29992;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22270;&#65292;&#21517;&#20026;&#21382;&#21490;&#26102;&#38388;&#22270;&#65292;&#26469;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph (KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial task that aims to predict future facts based on historical occurrences. The key challenge lies in uncovering structural dependencies within historical subgraphs and temporal patterns. Most existing approaches model TKGs relying on entity modeling, as nodes in the graph play a crucial role in knowledge representation. However, the real-world scenario often involves an extensive number of entities, with new entities emerging over time. This makes it challenging for entity-dependent methods to cope with extensive volumes of entities, and effectively handling newly emerging entities also becomes a significant challenge. Therefore, we propose Temporal Inductive Path Neural Network (TiPNN), which models historical information in an entity-independent perspective. Specifically, TiPNN adopts a unified graph, namely history temporal graph, to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22270;&#35770;&#31639;&#27861;&#22312;&#22320;&#29702;&#31354;&#38388;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;&#20204;&#22312;&#32593;&#32476;&#20998;&#26512;&#12289;&#31354;&#38388;&#36830;&#25509;&#24615;&#12289;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#20197;&#21450;&#21508;&#31181;&#31354;&#38388;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21015;&#20030;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#23454;&#26045;&#30340;&#24191;&#27867;&#30740;&#31350;&#12289;&#21019;&#26032;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03249</link><description>&lt;p&gt;
&#22270;&#35770;&#22312;&#39640;&#32423;&#22320;&#29702;&#31354;&#38388;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Graph Theory Applications in Advanced Geospatial Research. (arXiv:2309.03249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22270;&#35770;&#31639;&#27861;&#22312;&#22320;&#29702;&#31354;&#38388;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;&#20204;&#22312;&#32593;&#32476;&#20998;&#26512;&#12289;&#31354;&#38388;&#36830;&#25509;&#24615;&#12289;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#20197;&#21450;&#21508;&#31181;&#31354;&#38388;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21015;&#20030;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#23454;&#26045;&#30340;&#24191;&#27867;&#30740;&#31350;&#12289;&#21019;&#26032;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#31354;&#38388;&#31185;&#23398;&#28085;&#30422;&#20102;&#20174;&#29615;&#22659;&#30417;&#27979;&#12289;&#20132;&#36890;&#21040;&#22522;&#30784;&#35774;&#26045;&#35268;&#21010;&#12289;&#20197;&#21450;&#22522;&#20110;&#20301;&#32622;&#30340;&#20998;&#26512;&#21644;&#26381;&#21153;&#31561;&#24191;&#27867;&#24212;&#29992;&#12290;&#25968;&#23398;&#20013;&#30340;&#22270;&#35770;&#31639;&#27861;&#30001;&#20110;&#20854;&#39640;&#25928;&#22320;&#24314;&#27169;&#21644;&#20998;&#26512;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#25506;&#35752;&#20102;&#22270;&#35770;&#31639;&#27861;&#22312;&#22320;&#29702;&#31354;&#38388;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;&#20204;&#22312;&#32593;&#32476;&#20998;&#26512;&#12289;&#31354;&#38388;&#36830;&#25509;&#24615;&#12289;&#22320;&#29702;&#20449;&#24687;&#31995;&#32479;&#20197;&#21450;&#21508;&#31181;&#31354;&#38388;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20316;&#29992;&#12290;&#23427;&#25552;&#20379;&#20102;&#23545;&#22270;&#35770;&#30340;&#20851;&#38190;&#27010;&#24565;&#21644;&#31639;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#20197;&#21161;&#20110;&#24314;&#27169;&#36807;&#31243;&#12290;&#35813;&#25253;&#21578;&#28145;&#20837;&#20998;&#26512;&#20102;&#22270;&#35770;&#22312;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#22320;&#29702;&#31354;&#38388;&#25361;&#25112;&#21644;&#26426;&#36935;&#20013;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#23427;&#36824;&#21015;&#20030;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#23454;&#26045;&#30340;&#24191;&#27867;&#30740;&#31350;&#12289;&#21019;&#26032;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geospatial sciences include a wide range of applications, from environmental monitoring transportation to infrastructure planning, as well as location-based analysis and services. Graph theory algorithms in mathematics have emerged as indispensable tools in these domains due to their capability to model and analyse spatial relationships efficiently. This technical report explores the applications of graph theory algorithms in geospatial sciences, highlighting their role in network analysis, spatial connectivity, geographic information systems, and various other spatial problem-solving scenarios. It provides a comprehensive idea about the key concepts and algorithms of graph theory that assist the modelling processes. The report provides insights into the practical significance of graph theory in addressing real-world geospatial challenges and opportunities. It lists the extensive research, innovative technologies and methodologies implemented in this field.
&lt;/p&gt;</description></item><item><title>EvoCLINICAL&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#36827;&#24335;&#30149;&#30284;&#30331;&#35760;&#31995;&#32479;&#30340;&#32593;&#32476;&#30149;&#30284;&#21452;&#29983;&#20307;&#65292;&#36890;&#36807;&#20027;&#21160;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;CCDT&#19982;&#30495;&#23454;&#31995;&#32479;&#30340;&#21516;&#27493;&#65292;&#24182;&#20026;GURI&#30340;&#36816;&#34892;&#29366;&#24577;&#25552;&#20379;&#20102;&#21508;&#31181;&#23454;&#39564;&#21644;&#39640;&#32423;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.03246</link><description>&lt;p&gt;
EvoCLINICAL: &#25645;&#36733;&#20027;&#21160;&#36801;&#31227;&#23398;&#20064;&#30340;&#28436;&#36827;&#24335;&#30149;&#30284;&#30331;&#35760;&#31995;&#32479;&#30340;&#32593;&#32476;&#30149;&#30284;&#21452;&#29983;&#20307;
&lt;/p&gt;
&lt;p&gt;
EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning for Automated Cancer Registry System. (arXiv:2309.03246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03246
&lt;/p&gt;
&lt;p&gt;
EvoCLINICAL&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#36827;&#24335;&#30149;&#30284;&#30331;&#35760;&#31995;&#32479;&#30340;&#32593;&#32476;&#30149;&#30284;&#21452;&#29983;&#20307;&#65292;&#36890;&#36807;&#20027;&#21160;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;CCDT&#19982;&#30495;&#23454;&#31995;&#32479;&#30340;&#21516;&#27493;&#65292;&#24182;&#20026;GURI&#30340;&#36816;&#34892;&#29366;&#24577;&#25552;&#20379;&#20102;&#21508;&#31181;&#23454;&#39564;&#21644;&#39640;&#32423;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25386;&#23041;&#30284;&#30151;&#30331;&#35760;&#22788;&#65288;CRN&#65289;&#36890;&#36807;&#20174;&#25386;&#23041;&#30340;&#21508;&#20010;&#21307;&#30103;&#23454;&#20307;&#65288;&#20363;&#22914;&#21307;&#23398;&#23454;&#39564;&#23460;&#21644;&#21307;&#38498;&#65289;&#25509;&#25910;&#30284;&#30151;&#20449;&#24687;&#26469;&#25910;&#38598;&#20851;&#20110;&#30284;&#30151;&#24739;&#32773;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#30001;&#33258;&#21160;&#21270;&#30284;&#30151;&#30331;&#35760;&#31995;&#32479;GURI&#36827;&#34892;&#39564;&#35777;&#12290;&#23427;&#30340;&#27491;&#24120;&#36816;&#34892;&#23545;&#20110;&#30284;&#30151;&#30740;&#31350;&#21644;&#21521;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#20851;&#38190;&#30340;&#30284;&#30151;&#32479;&#35745;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;GURI&#26500;&#24314;&#19968;&#20010;&#32593;&#32476;&#30149;&#30284;&#21452;&#29983;&#20307;&#65288;CCDT&#65289;&#21487;&#20197;&#22312;&#19981;&#19982;&#30495;&#23454;&#31995;&#32479;&#36827;&#34892;&#28145;&#20837;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#65292;&#20419;&#36827;&#23545;GURI&#36816;&#34892;&#29366;&#24577;&#30340;&#21508;&#31181;&#23454;&#39564;&#21644;&#39640;&#32423;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26032;&#30340;&#21307;&#23398;&#35786;&#26029;&#21644;&#27835;&#30103;&#12289;&#25216;&#26415;&#36827;&#27493;&#31561;&#21407;&#22240;&#65292;GURI&#19981;&#26029;&#21457;&#23637;&#12290;&#30456;&#24212;&#22320;&#65292;CCDT&#20063;&#24212;&#35813;&#38543;&#20043;&#28436;&#36827;&#20197;&#19982;GURI&#21516;&#27493;&#12290;&#23454;&#29616;&#36825;&#31181;&#21516;&#27493;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#28436;&#36827;&#30340;CCDT&#38656;&#35201;&#30001;&#26032;&#30340;GURI&#26631;&#35760;&#30340;&#20016;&#23500;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvoCLINICAL&#65292;&#23427;&#32771;&#34385;&#20102;&#20026;GURI&#24320;&#21457;&#30340;CCDT
&lt;/p&gt;
&lt;p&gt;
The Cancer Registry of Norway (CRN) collects information on cancer patients by receiving cancer messages from different medical entities (e.g., medical labs, and hospitals) in Norway. Such messages are validated by an automated cancer registry system: GURI. Its correct operation is crucial since it lays the foundation for cancer research and provides critical cancer-related statistics to its stakeholders. Constructing a cyber-cyber digital twin (CCDT) for GURI can facilitate various experiments and advanced analyses of the operational state of GURI without requiring intensive interactions with the real system. However, GURI constantly evolves due to novel medical diagnostics and treatment, technological advances, etc. Accordingly, CCDT should evolve as well to synchronize with GURI. A key challenge of achieving such synchronization is that evolving CCDT needs abundant data labelled by the new GURI. To tackle this challenge, we propose EvoCLINICAL, which considers the CCDT developed for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#20869;&#23384;&#26465;&#20214;&#19979;&#65292;&#22312;&#27969;&#27169;&#22411;&#20013;&#27979;&#35797;&#20998;&#24067;&#30340;&#23646;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20110;&#26631;&#20934;&#35775;&#38382;&#27169;&#22411;&#21644;&#26465;&#20214;&#35775;&#38382;&#27169;&#22411;&#30340;&#25240;&#34935;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#25928;&#23398;&#20064;&#21333;&#35843;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#21487;&#20998;&#35299;&#20998;&#24067;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.03245</link><description>&lt;p&gt;
&#22312;&#27969;&#27169;&#22411;&#20013;&#27979;&#35797;&#20998;&#24067;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Testing properties of distributions in the streaming model. (arXiv:2309.03245v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#20869;&#23384;&#26465;&#20214;&#19979;&#65292;&#22312;&#27969;&#27169;&#22411;&#20013;&#27979;&#35797;&#20998;&#24067;&#30340;&#23646;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20110;&#26631;&#20934;&#35775;&#38382;&#27169;&#22411;&#21644;&#26465;&#20214;&#35775;&#38382;&#27169;&#22411;&#30340;&#25240;&#34935;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#25928;&#23398;&#20064;&#21333;&#35843;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#21487;&#20998;&#35299;&#20998;&#24067;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#20869;&#23384;&#26465;&#20214;&#19979;&#65292;&#26631;&#20934;&#35775;&#38382;&#27169;&#22411;&#21644;&#26465;&#20214;&#35775;&#38382;&#27169;&#22411;&#20013;&#30340;&#20998;&#24067;&#27979;&#35797;&#38382;&#39064;&#12290;&#22312;&#36825;&#20004;&#31181;&#22330;&#26223;&#20013;&#65292;&#26679;&#26412;&#20197;&#22312;&#32447;&#26041;&#24335;&#20986;&#29616;&#65292;&#30446;&#26631;&#26159;&#20351;&#29992;&#26368;&#20248;&#25968;&#37327;&#30340;&#26679;&#26412;&#22312;&#32473;&#23450;&#26102;&#38388;&#20869;&#23384;&#20648;&#30340;&#20869;&#23384;&#32422;&#26463;&#19979;&#27979;&#35797;&#20998;&#24067;&#30340;&#23646;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#26679;&#26412;&#25353;&#26465;&#20214;&#35775;&#38382;&#39044;&#36873;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#25240;&#34935;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#20855;&#26377;&#20960;&#20046;&#26368;&#20248;&#26679;&#26412;&#25968;&#37327;&#30340;&#20869;&#23384;&#32422;&#26463;&#30340;&#21333;&#35843;&#20998;&#24067;&#30340;&#31616;&#27905;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21333;&#35843;&#20998;&#24067;&#31639;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#21487;&#20998;&#35299;&#20998;&#24067;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distribution testing in the standard access model and the conditional access model when the memory available to the testing algorithm is bounded. In both scenarios, the samples appear in an online fashion and the goal is to test the properties of distribution using an optimal number of samples subject to a memory constraint on how many samples can be stored at a given time. First, we provide a trade-off between the sample complexity and the space complexity for testing identity when the samples are drawn according to the conditional access oracle. We then show that we can learn a succinct representation of a monotone distribution efficiently with a memory constraint on the number of samples that are stored that is almost optimal. We also show that the algorithm for monotone distributions can be extended to a larger class of decomposable distributions.
&lt;/p&gt;</description></item><item><title>EGIC&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;&#20302;&#20301;&#36895;&#29575;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#20998;&#21106;&#25552;&#20379;&#25351;&#23548;&#12290;&#23427;&#22312;&#22833;&#30495;&#24863;&#30693;&#21644;&#22833;&#30495;&#26041;&#21521;&#22522;&#32447;&#26041;&#27861;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#24182;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#20248;&#31168;&#30340;&#25554;&#20540;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03244</link><description>&lt;p&gt;
EGIC:&#22686;&#24378;&#30340;&#20302;&#20301;&#36895;&#29575;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#22312;&#35821;&#20041;&#20998;&#21106;&#30340;&#25351;&#23548;&#19979;
&lt;/p&gt;
&lt;p&gt;
EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation. (arXiv:2309.03244v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03244
&lt;/p&gt;
&lt;p&gt;
EGIC&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;&#20302;&#20301;&#36895;&#29575;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#20998;&#21106;&#25552;&#20379;&#25351;&#23548;&#12290;&#23427;&#22312;&#22833;&#30495;&#24863;&#30693;&#21644;&#22833;&#30495;&#26041;&#21521;&#22522;&#32447;&#26041;&#27861;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#24182;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#20248;&#31168;&#30340;&#25554;&#20540;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;EGIC&#65292;&#23427;&#20801;&#35768;&#20174;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#26377;&#25928;&#22320;&#36941;&#21382;&#22833;&#30495;&#24863;&#30693;&#26354;&#32447;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#32534;&#30721;&#30340;&#22270;&#20687;&#25554;&#20540;&#21464;&#20307;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;MSE&#20248;&#21270;&#21644;GAN&#20248;&#21270;&#35299;&#30721;&#22120;&#36755;&#20986;&#20043;&#38388;&#30340;&#27531;&#24046;&#12290;&#22312;&#25509;&#25910;&#31471;&#65292;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#27531;&#24046;&#23545;&#22522;&#20110;GAN&#30340;&#37325;&#24314;&#30340;&#24433;&#21709;&#12290;&#32467;&#21512;&#25913;&#36827;&#30340;&#22522;&#20110;GAN&#30340;&#26500;&#24314;&#22359;&#65292;EGIC&#22312;&#24863;&#30693;&#23548;&#21521;&#21644;&#22833;&#30495;&#23548;&#21521;&#30340;&#22522;&#32447;&#26041;&#27861;&#65288;&#21253;&#25324;HiFiC&#65292;MRIC&#21644;DIRAC&#65289;&#19978;&#34920;&#29616;&#20248;&#20110;&#22823;&#22810;&#25968;&#26041;&#27861;&#65292;&#22312;&#22833;&#30495;&#31471;&#19982;VTM-20.0&#20960;&#20046;&#30456;&#24403;&#12290;EGIC&#23454;&#29616;&#31616;&#21333;&#65292;&#38750;&#24120;&#36731;&#37327;&#32423;&#65288;&#19982;HiFiC&#30456;&#27604;&#65292;&#27169;&#22411;&#21442;&#25968;&#21482;&#26377;0.18&#20493;&#65289;&#65292;&#24182;&#25552;&#20379;&#20248;&#24322;&#30340;&#25554;&#20540;&#29305;&#24615;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#38024;&#23545;&#20302;&#20301;&#33539;&#22260;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce EGIC, a novel generative image compression method that allows traversing the distortion-perception curve efficiently from a single model. Specifically, we propose an implicitly encoded variant of image interpolation that predicts the residual between a MSE-optimized and GAN-optimized decoder output. On the receiver side, the user can then control the impact of the residual on the GAN-based reconstruction. Together with improved GAN-based building blocks, EGIC outperforms a wide-variety of perception-oriented and distortion-oriented baselines, including HiFiC, MRIC and DIRAC, while performing almost on par with VTM-20.0 on the distortion end. EGIC is simple to implement, very lightweight (e.g. 0.18x model parameters compared to HiFiC) and provides excellent interpolation characteristics, which makes it a promising candidate for practical applications targeting the low bit range.
&lt;/p&gt;</description></item><item><title>AutoBA&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;AI&#20195;&#29702;&#31243;&#24207;&#65292;&#36890;&#36807;&#26368;&#23569;&#30340;&#29992;&#25143;&#36755;&#20837;&#31616;&#21270;&#29983;&#29289;&#20449;&#24687;&#23398;&#20998;&#26512;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#36880;&#27493;&#35745;&#21010;&#12290;&#32463;&#36807;&#39564;&#35777;&#65292;AutoBA&#22312;&#21508;&#31181;&#32452;&#23398;&#20998;&#26512;&#26696;&#20363;&#20013;&#34920;&#29616;&#20986;&#20581;&#22766;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2309.03242</link><description>&lt;p&gt;
&#36890;&#36807;AutoBA&#23454;&#29616;&#33258;&#21160;&#21270;&#29983;&#29289;&#20449;&#24687;&#23398;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Automated Bioinformatics Analysis via AutoBA. (arXiv:2309.03242v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03242
&lt;/p&gt;
&lt;p&gt;
AutoBA&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;AI&#20195;&#29702;&#31243;&#24207;&#65292;&#36890;&#36807;&#26368;&#23569;&#30340;&#29992;&#25143;&#36755;&#20837;&#31616;&#21270;&#29983;&#29289;&#20449;&#24687;&#23398;&#20998;&#26512;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#36880;&#27493;&#35745;&#21010;&#12290;&#32463;&#36807;&#39564;&#35777;&#65292;AutoBA&#22312;&#21508;&#31181;&#32452;&#23398;&#20998;&#26512;&#26696;&#20363;&#20013;&#34920;&#29616;&#20986;&#20581;&#22766;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32452;&#23398;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#28436;&#21464;&#65292;&#23545;&#22788;&#29702;&#20998;&#26512;&#30340;&#31616;&#21270;&#21644;&#36866;&#24212;&#24615;&#24037;&#20855;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#20026;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Auto Bioinformatics Analysis (AutoBA)&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;AI&#20195;&#29702;&#31243;&#24207;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20256;&#32479;&#32452;&#23398;&#25968;&#25454;&#20998;&#26512;&#12290;AutoBA&#36890;&#36807;&#38656;&#35201;&#26368;&#23569;&#30340;&#29992;&#25143;&#36755;&#20837;&#26469;&#31616;&#21270;&#20998;&#26512;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#36880;&#27493;&#35745;&#21010;&#65292;&#29992;&#20110;&#23436;&#25104;&#21508;&#31181;&#29983;&#29289;&#20449;&#24687;&#23398;&#20219;&#21153;&#12290;&#32463;&#36807;&#19987;&#23478;&#29983;&#29289;&#20449;&#24687;&#23398;&#23478;&#30340;&#20005;&#26684;&#39564;&#35777;&#65292;AutoBA&#30340;&#20581;&#22766;&#24615;&#21644;&#36866;&#24212;&#24615;&#22312;&#21508;&#31181;&#32452;&#23398;&#20998;&#26512;&#26696;&#20363;&#20013;&#24471;&#21040;&#35777;&#23454;&#65292;&#21253;&#25324;&#20840;&#22522;&#22240;&#32452;&#27979;&#24207;&#65288;WGS&#65289;&#65292;RNA&#27979;&#24207;&#65288;RNA-seq&#65289;&#65292;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#65292;ChIP-seq&#21644;&#31354;&#38388;&#36716;&#24405;&#32452;&#23398;&#12290;AutoBA&#30340;&#29420;&#29305;&#33021;&#21147;&#26159;&#26681;&#25454;&#36755;&#20837;&#25968;&#25454;&#30340;&#21464;&#21270;&#33258;&#35774;&#35745;&#20998;&#26512;&#27969;&#31243;&#65292;&#36827;&#19968;&#27493;&#20984;&#26174;&#20102;&#20854;&#22810;&#21151;&#33021;&#24615;&#12290;&#19982;&#22312;&#32447;&#29983;&#29289;&#20449;&#24687;&#23398;&#26381;&#21153;&#30456;&#27604;&#65292;AutoBA&#22312;&#26412;&#22320;&#37096;&#32626;&#20998;&#26512;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the fast-growing and evolving omics data, the demand for streamlined and adaptable tools to handle the analysis continues to grow. In response to this need, we introduce Auto Bioinformatics Analysis (AutoBA), an autonomous AI agent based on a large language model designed explicitly for conventional omics data analysis. AutoBA simplifies the analytical process by requiring minimal user input while delivering detailed step-by-step plans for various bioinformatics tasks. Through rigorous validation by expert bioinformaticians, AutoBA's robustness and adaptability are affirmed across a diverse range of omics analysis cases, including whole genome sequencing (WGS), RNA sequencing (RNA-seq), single-cell RNA-seq, ChIP-seq, and spatial transcriptomics. AutoBA's unique capacity to self-design analysis processes based on input data variations further underscores its versatility. Compared with online bioinformatic services, AutoBA deploys the analysis locally, preserving data privacy. Moreo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20805;&#20998;&#35757;&#32451;&#65292;&#19968;&#20010;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20197;&#20960;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-4&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#36890;&#36807;&#22312;&#38468;&#21152;&#30340;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#19982;GPT-4&#22312;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#19978;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03241</link><description>&lt;p&gt;
GPT&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
GPT Can Solve Mathematical Problems Without a Calculator. (arXiv:2309.03241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20805;&#20998;&#35757;&#32451;&#65292;&#19968;&#20010;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20197;&#20960;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-4&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#36890;&#36807;&#22312;&#38468;&#21152;&#30340;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#19982;GPT-4&#22312;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#19978;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#22312;&#27809;&#26377;&#35745;&#31639;&#22120;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#25191;&#34892;&#31639;&#26415;&#36816;&#31639;&#65292;&#29305;&#21035;&#26159;&#36229;&#36807;8&#20301;&#25968;&#23383;&#30340;&#20056;&#27861;&#65292;&#20197;&#21450;&#28041;&#21450;&#23567;&#25968;&#21644;&#20998;&#25968;&#30340;&#36816;&#31639;&#12290;&#26412;&#25991;&#26088;&#22312;&#25361;&#25112;&#36825;&#31181;&#35823;&#35299;&#12290;&#36890;&#36807;&#20805;&#20998;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#19968;&#20010;&#25317;&#26377;20&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20197;&#36817;&#20046;100%&#30340;&#20934;&#30830;&#24230;&#25191;&#34892;&#22810;&#20301;&#25968;&#30340;&#31639;&#26415;&#36816;&#31639;&#65292;&#32780;&#19988;&#27809;&#26377;&#25968;&#25454;&#27844;&#38706;&#65292;&#26174;&#33879;&#36229;&#36807;&#20102;GPT-4&#65288;&#20854;&#22810;&#20301;&#25968;&#20056;&#27861;&#20934;&#30830;&#29575;&#20165;&#20026;4.3%&#65289;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;MathGLM&#65292;&#23427;&#26159;&#36890;&#36807;&#22312;&#21253;&#21547;&#20102;&#25991;&#26412;&#25551;&#36848;&#30340;&#38468;&#21152;&#22810;&#27493;&#39588;&#31639;&#26415;&#36816;&#31639;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#19978;&#20174;GLM-10B&#24494;&#35843;&#32780;&#25104;&#30340;&#65292;&#23427;&#22312;&#19968;&#20010;&#21253;&#21547;5000&#20010;&#26679;&#26412;&#30340;&#20013;&#25991;&#25968;&#23398;&#38382;&#39064;&#27979;&#35797;&#38598;&#19978;&#30340;&#34920;&#29616;&#19982;GPT-4&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of &gt;8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;POI&#32423;&#21035;&#20154;&#32676;&#27969;&#25512;&#26029;&#30340;&#26102;&#31354;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23646;&#24615;&#22270;&#34920;&#31034;&#23398;&#20064;&#20197;&#35299;&#20915;&#25968;&#25454;&#26631;&#35760;&#19981;&#36275;&#12289;POI&#38388;&#26102;&#31354;&#20381;&#36182;&#24615;&#22797;&#26434;&#21644;&#20154;&#32676;&#27969;&#37327;&#19982;GPS&#25253;&#21578;&#20043;&#38388;&#30456;&#20851;&#24615;&#22810;&#26679;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03239</link><description>&lt;p&gt;
POI&#32423;&#21035;&#20154;&#32676;&#27969;&#25512;&#26029;&#30340;&#26102;&#31354;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference. (arXiv:2309.03239v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;POI&#32423;&#21035;&#20154;&#32676;&#27969;&#25512;&#26029;&#30340;&#26102;&#31354;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23646;&#24615;&#22270;&#34920;&#31034;&#23398;&#20064;&#20197;&#35299;&#20915;&#25968;&#25454;&#26631;&#35760;&#19981;&#36275;&#12289;POI&#38388;&#26102;&#31354;&#20381;&#36182;&#24615;&#22797;&#26434;&#21644;&#20154;&#32676;&#27969;&#37327;&#19982;GPS&#25253;&#21578;&#20043;&#38388;&#30456;&#20851;&#24615;&#22810;&#26679;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#33719;&#21462;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#30340;&#20154;&#32676;&#27969;&#37327;&#23545;&#20110;&#26377;&#25928;&#30340;&#20132;&#36890;&#31649;&#29702;&#12289;&#20844;&#20849;&#26381;&#21153;&#21644;&#22478;&#24066;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22914;&#27492;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#22478;&#24066;&#24863;&#30693;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#22823;&#22810;&#25968;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#36136;&#37327;&#19981;&#36275;&#20197;&#30417;&#27979;&#27599;&#20010;POI&#30340;&#20154;&#32676;&#27969;&#21160;&#12290;&#36825;&#20351;&#24471;&#20174;&#20302;&#36136;&#37327;&#25968;&#25454;&#20013;&#25512;&#26029;&#20934;&#30830;&#30340;&#20154;&#32676;&#27969;&#37327;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#19968;&#22797;&#26434;&#24615;&#20027;&#35201;&#30001;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#24341;&#36215;&#65306;1&#65289;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#32597;&#35265;&#24615;&#65307;2&#65289;POI&#20043;&#38388;&#22797;&#26434;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65307;3&#65289;&#31934;&#30830;&#20154;&#32676;&#27969;&#37327;&#19982;GPS&#25253;&#21578;&#20043;&#38388;&#30340;&#20247;&#22810;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#20154;&#32676;&#27969;&#25512;&#26029;&#38382;&#39064;&#37325;&#26032;&#26500;&#24314;&#20026;&#33258;&#30417;&#30563;&#23646;&#24615;&#22270;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#25968;&#25454;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65288;model&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#26500;&#24314;&#19968;&#20010;&#31354;&#38388;&#22270;&#24320;&#22987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal for effective traffic management, public service, and urban planning. Despite this importance, due to the limitations of urban sensing techniques, the data quality from most sources is inadequate for monitoring crowd flow at each POI. This renders the inference of accurate crowd flow from low-quality data a critical and challenging task. The complexity is heightened by three key factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad correlations between precise crowd flow and GPS reports}.  To address these challenges, we recast the crowd flow inference problem as a self-supervised attributed graph representation learning task and introduce a novel \underline{C}ontrastive \underline{S}elf-learning framework for \underline{S}patio-\underline{T}emporal data (\model). Our approach initiates with the construction of a spati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#25910;&#38598;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#38750;&#20856;&#22411;&#35757;&#32451;&#25968;&#25454;&#12289;&#20998;&#26512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#27880;&#37322;&#26041;&#26696;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#25239;&#32593;&#32476;&#22788;&#29702;&#33258;&#28982;&#28151;&#28102;&#21464;&#37327;&#21644;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#24320;&#21457;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.03238</link><description>&lt;p&gt;
&#38544;&#21547;&#35774;&#35745;&#36873;&#25321;&#21450;&#20854;&#23545;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#24320;&#21457;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation. (arXiv:2309.03238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#25910;&#38598;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#38750;&#20856;&#22411;&#35757;&#32451;&#25968;&#25454;&#12289;&#20998;&#26512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#27880;&#37322;&#26041;&#26696;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#25239;&#32593;&#32476;&#22788;&#29702;&#33258;&#28982;&#28151;&#28102;&#21464;&#37327;&#21644;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#24320;&#21457;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#35782;&#21035;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#24773;&#32490;&#30340;&#24863;&#30693;&#21644;&#34920;&#36798;&#20855;&#26377;&#22266;&#26377;&#30340;&#20027;&#35266;&#24615;&#12290;&#24773;&#32490;&#30340;&#20027;&#35266;&#24615;&#22312;&#24320;&#21457;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#35745;&#31639;&#27169;&#22411;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#35770;&#25991;&#32771;&#23519;&#20102;&#24773;&#32490;&#35782;&#21035;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#20174;&#25910;&#38598;&#26088;&#22312;&#32771;&#34385;&#24773;&#32490;&#20135;&#29983;&#24515;&#29702;&#22240;&#32032;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#24320;&#22987;&#12290;&#20026;&#20102;&#24212;&#23545;&#38750;&#20856;&#22411;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25910;&#38598;&#20102;&#22810;&#27169;&#24577;&#24212;&#28608;&#24773;&#32490;&#25968;&#25454;&#38598;&#65292;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#21463;&#25511;&#30340;&#21387;&#21147;&#22240;&#32032;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#24773;&#32490;&#20135;&#29983;&#30340;&#30495;&#23454;&#29615;&#22659;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#31614;&#20027;&#35266;&#24615;&#38382;&#39064;&#65292;&#35813;&#30740;&#31350;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#27880;&#37322;&#26041;&#26696;&#22914;&#20309;&#24433;&#21709;&#24773;&#32490;&#24863;&#30693;&#21644;&#27880;&#37322;&#32773;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#32593;&#32476;&#26469;&#22788;&#29702;&#33258;&#28982;&#28151;&#28102;&#21464;&#37327;&#21644;&#21464;&#21270;&#65292;&#20197;&#38548;&#31163;&#20851;&#38190;&#22240;&#32032;&#65288;&#22914;&#21387;&#21147;&#65289;&#19982;&#23398;&#20064;&#21040;&#30340;&#24773;&#32490;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition is a complex task due to the inherent subjectivity in both the perception and production of emotions. The subjectivity of emotions poses significant challenges in developing accurate and robust computational models. This thesis examines critical facets of emotion recognition, beginning with the collection of diverse datasets that account for psychological factors in emotion production.  To handle the challenge of non-representative training data, this work collects the Multimodal Stressed Emotion dataset, which introduces controlled stressors during data collection to better represent real-world influences on emotion production. To address issues with label subjectivity, this research comprehensively analyzes how data augmentation techniques and annotation schemes impact emotion perception and annotator labels. It further handles natural confounding variables and variations by employing adversarial networks to isolate key factors like stress from learned emotion rep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#22402;&#30452;&#20998;&#35299;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2309.03237</link><description>&lt;p&gt;
&#22270;&#20687;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#22402;&#30452;&#20998;&#35299;&#21644;&#39044;&#35757;&#32451;&#39592;&#24178;&#36739;&#38590;&#25915;&#20811;
&lt;/p&gt;
&lt;p&gt;
Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat. (arXiv:2309.03237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#22402;&#30452;&#20998;&#35299;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#35748;&#30495;&#35780;&#20272;&#65292;&#24182;&#27979;&#35797;&#23427;&#20204;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#35768;&#22810;&#20197;&#21069;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#23398;&#20064;&#19981;&#20855;&#26377;&#22810;&#26679;&#22270;&#20687;&#38598;&#30340;&#25968;&#25454;&#38598;&#20250;&#24433;&#21709;&#32467;&#26524;&#65307;&#26159;&#21542;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#25552;&#21462;&#8220;&#39592;&#24178;&#8221;&#65307;&#22914;&#20309;&#35780;&#20272;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#65288;&#25105;&#20204;&#35748;&#20026;&#20165;&#20165;&#20998;&#31867;&#20934;&#30830;&#24615;&#26159;&#19981;&#22815;&#30340;&#65289;&#31561;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22402;&#30452;&#20998;&#35299;&#20284;&#20046;&#33021;&#22815;&#24471;&#21040;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#19988;&#20248;&#20110;&#26356;&#24120;&#35265;&#30340;&#21327;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We carefully evaluate a number of algorithms for learning in a federated environment, and test their utility for a variety of image classification tasks. We consider many issues that have not been adequately considered before: whether learning over data sets that do not have diverse sets of images affects the results; whether to use a pre-trained feature extraction "backbone"; how to evaluate learner performance (we argue that classification accuracy is not enough), among others. Overall, across a wide variety of settings, we find that vertically decomposing a neural network seems to give the best results, and outperforms more standard reconciliation-used methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;&#33258;&#28982;&#31034;&#20363;&#20026;&#22522;&#30784;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#31034;&#20363;&#20316;&#20026;&#35299;&#37322;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#19982;&#20154;&#31867;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#36807;&#31243;&#30456;&#31526;&#65292;&#20351;&#35299;&#37322;&#26356;&#33258;&#28982;&#21644;&#26131;&#25026;&#12290;</title><link>http://arxiv.org/abs/2309.03234</link><description>&lt;p&gt;
&#33258;&#28982;&#31034;&#20363;&#20026;&#22522;&#30784;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Natural Example-Based Explainability: a Survey. (arXiv:2309.03234v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#33258;&#28982;&#31034;&#20363;&#20026;&#22522;&#30784;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#31034;&#20363;&#20316;&#20026;&#35299;&#37322;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#19982;&#20154;&#31867;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#36807;&#31243;&#30456;&#31526;&#65292;&#20351;&#35299;&#37322;&#26356;&#33258;&#28982;&#21644;&#26131;&#25026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#22312;XAI&#39046;&#22495;&#65292;&#31361;&#20986;&#22270;&#24050;&#32463;&#25104;&#20026;&#20027;&#35282;&#22810;&#24180;&#65292;&#20294;&#20854;&#21453;&#26144;&#27169;&#22411;&#20869;&#37096;&#36807;&#31243;&#30340;&#33021;&#21147;&#21463;&#21040;&#20102;&#36136;&#30097;&#12290;&#23613;&#31649;&#19981;&#22826;&#21463;&#20851;&#27880;&#65292;&#20294;&#22522;&#20110;&#31034;&#20363;&#30340;XAI&#26041;&#27861;&#20173;&#22312;&#19981;&#26029;&#25913;&#36827;&#12290;&#23427;&#21253;&#25324;&#20351;&#29992;&#31034;&#20363;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#36825;&#31526;&#21512;&#20154;&#31867;&#25512;&#29702;&#30340;&#24515;&#29702;&#26426;&#21046;&#65292;&#20351;&#22522;&#20110;&#31034;&#20363;&#30340;&#35299;&#37322;&#23545;&#29992;&#25143;&#26469;&#35828;&#33258;&#28982;&#21644;&#30452;&#35266;&#26131;&#25026;&#12290;&#20107;&#23454;&#19978;&#65292;&#20154;&#31867;&#36890;&#36807;&#22522;&#20110;&#31034;&#20363;&#24418;&#25104;&#27010;&#24565;&#30340;&#24515;&#29702;&#34920;&#31034;&#26469;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#33258;&#28982;&#31034;&#20363;&#20026;&#22522;&#30784;&#30340;XAI&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25551;&#36848;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;&#25152;&#35859;&#8220;&#33258;&#28982;&#8221;&#31034;&#20363;&#25351;&#30340;&#26159;&#30452;&#25509;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#32472;&#21046;&#32780;&#26469;&#65292;&#32780;&#19981;&#28041;&#21450;&#20219;&#20309;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence (XAI) has become increasingly significant for improving the interpretability and trustworthiness of machine learning models. While saliency maps have stolen the show for the last few years in the XAI field, their ability to reflect models' internal processes has been questioned. Although less in the spotlight, example-based XAI methods have continued to improve. It encompasses methods that use examples as explanations for a machine learning model's predictions. This aligns with the psychological mechanisms of human reasoning and makes example-based explanations natural and intuitive for users to understand. Indeed, humans learn and reason by forming mental representations of concepts based on examples.  This paper provides an overview of the state-of-the-art in natural example-based XAI, describing the pros and cons of each approach. A "natural" example simply means that it is directly drawn from the training data without involving any generative pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38646;&#21806;&#24215;&#39038;&#23458;&#34892;&#20026;&#20998;&#26512;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;&#39038;&#23458;&#34892;&#20026;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#21644;&#20998;&#26512;&#65292;&#20026;&#21830;&#24215;&#32463;&#29702;&#25552;&#20379;&#20102;&#27934;&#23519;&#39038;&#23458;&#20559;&#22909;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.03232</link><description>&lt;p&gt;
&#38646;&#21806;&#24215;&#39038;&#23458;&#34892;&#20026;&#20998;&#26512;&#31995;&#32479;&#65306;&#35774;&#35745;&#19982;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Retail store customer behavior analysis system: Design and Implementation. (arXiv:2309.03232v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38646;&#21806;&#24215;&#39038;&#23458;&#34892;&#20026;&#20998;&#26512;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;&#39038;&#23458;&#34892;&#20026;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#21644;&#20998;&#26512;&#65292;&#20026;&#21830;&#24215;&#32463;&#29702;&#25552;&#20379;&#20102;&#27934;&#23519;&#39038;&#23458;&#20559;&#22909;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#38646;&#21806;&#24215;&#39038;&#23458;&#34892;&#20026;&#23545;&#20110;&#36890;&#36807;&#20026;&#26381;&#21153;&#22686;&#21152;&#20010;&#24615;&#21270;&#20215;&#20540;&#25552;&#21319;&#39038;&#23458;&#28385;&#24847;&#24230;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#34892;&#20026;&#20998;&#26512;&#25581;&#31034;&#20102;&#39038;&#23458;&#19982;&#21830;&#21697;&#21644;&#20854;&#20182;&#20154;&#20114;&#21160;&#20013;&#30340;&#19968;&#33324;&#21644;&#35814;&#32454;&#27169;&#24335;&#65292;&#20026;&#21830;&#24215;&#32463;&#29702;&#25552;&#20379;&#20102;&#27934;&#23519;&#39038;&#23458;&#20559;&#22909;&#30340;&#20449;&#24687;&#12290;&#30446;&#21069;&#26377;&#20960;&#31181;&#35299;&#20915;&#26041;&#26696;&#26088;&#22312;&#21033;&#29992;&#25968;&#25454;&#36890;&#36807;&#32479;&#35745;&#21487;&#35270;&#21270;&#30340;&#26041;&#24335;&#35782;&#21035;&#29305;&#23450;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#23616;&#38480;&#20110;&#20998;&#26512;&#23567;&#35268;&#27169;&#30340;&#39038;&#23458;&#34892;&#20026;&#38598;&#65292;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#26469;&#26816;&#27979;&#34892;&#20026;&#12290;&#23427;&#20204;&#19981;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#36825;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21487;&#35270;&#21270;&#31995;&#32479;&#33719;&#21462;&#30340;&#34892;&#20026;&#25968;&#25454;&#26102;&#25552;&#20379;&#30340;&#22270;&#24418;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#30340;&#26694;&#26550;&#65306;&#39038;&#23458;&#34892;&#20026;&#30340;&#25968;&#23398;&#24314;&#27169;&#65292;&#20351;&#29992;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#34892;&#20026;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding customer behavior in retail stores plays a crucial role in improving customer satisfaction by adding personalized value to services. Behavior analysis reveals both general and detailed patterns in the interaction of customers with a store items and other people, providing store managers with insight into customer preferences. Several solutions aim to utilize this data by recognizing specific behaviors through statistical visualization. However, current approaches are limited to the analysis of small customer behavior sets, utilizing conventional methods to detect behaviors. They do not use deep learning techniques such as deep neural networks, which are powerful methods in the field of computer vision. Furthermore, these methods provide limited figures when visualizing the behavioral data acquired by the system. In this study, we propose a framework that includes three primary parts: mathematical modeling of customer behaviors, behavior analysis using an efficient deep le
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#19982;&#30417;&#25511;&#39046;&#22495;&#30340;RentinaNet&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#24320;&#21457;&#20102;&#31216;&#20026;Quantum-RetinaNet&#30340;&#26234;&#33021;&#30417;&#25511;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.03231</link><description>&lt;p&gt;
&#37327;&#23376;AI&#22686;&#24378;&#26234;&#33021;&#30417;&#25511;&#65306;&#36890;&#36807;&#21019;&#26032;&#30340;&#36829;&#31105;&#21697;&#26816;&#27979;&#25552;&#21319;&#20844;&#20849;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
Quantum-AI empowered Intelligent Surveillance: Advancing Public Safety Through Innovative Contraband Detection. (arXiv:2309.03231v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#19982;&#30417;&#25511;&#39046;&#22495;&#30340;RentinaNet&#27169;&#22411;&#36827;&#34892;&#25972;&#21512;&#65292;&#24320;&#21457;&#20102;&#31216;&#20026;Quantum-RetinaNet&#30340;&#26234;&#33021;&#30417;&#25511;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#25511;&#31995;&#32479;&#22312;&#32500;&#25252;&#29616;&#20195;&#31038;&#20250;&#30340;&#21644;&#24179;&#19982;&#23433;&#20840;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#23427;&#20204;&#30340;&#26222;&#21450;&#24615;&#26377;&#21161;&#20110;&#26377;&#25928;&#30417;&#25511;&#21487;&#30097;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#22312;&#20154;&#21475;&#23494;&#38598;&#30340;&#29615;&#22659;&#20013;&#65292;&#25345;&#32493;&#20027;&#21160;&#30417;&#25511;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#65292;&#24517;&#39035;&#24320;&#21457;&#26234;&#33021;&#30417;&#25511;&#31995;&#32479;&#12290;AI&#22312;&#30417;&#25511;&#39046;&#22495;&#30340;&#25972;&#21512;&#26159;&#19968;&#27425;&#37325;&#22823;&#38761;&#21629;&#65292;&#28982;&#32780;&#36895;&#24230;&#38382;&#39064;&#38459;&#30861;&#20102;&#20854;&#22312;&#35813;&#39046;&#22495;&#30340;&#24191;&#27867;&#23454;&#26045;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#22522;&#20110;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#30340;&#30417;&#25511;&#31995;&#32479;&#19981;&#20165;&#26356;&#20934;&#30830;&#65292;&#32780;&#19988;&#33021;&#22815;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#26159;&#20197;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#12290;&#26412;&#30740;&#31350;&#23558;RentinaNet&#27169;&#22411;&#19982;&#37327;&#23376;CNN&#38598;&#25104;&#65292;&#31216;&#20026;Quantum-RetinaNet&#12290;&#36890;&#36807;&#21033;&#29992;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#37327;&#23376;&#33021;&#21147;&#65292;Quantum-RetinaNet&#22312;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surveillance systems have emerged as crucial elements in upholding peace and security in the modern world. Their ubiquity aids in monitoring suspicious activities effectively. However, in densely populated environments, continuous active monitoring becomes impractical, necessitating the development of intelligent surveillance systems. AI integration in the surveillance domain was a big revolution, however, speed issues have prevented its widespread implementation in the field. It has been observed that quantum artificial intelligence has led to a great breakthrough. Quantum artificial intelligence-based surveillance systems have shown to be more accurate as well as capable of performing well in real-time scenarios, which had never been seen before. In this research, a RentinaNet model is integrated with Quantum CNN and termed as Quantum-RetinaNet. By harnessing the Quantum capabilities of QCNN, Quantum-RetinaNet strikes a balance between accuracy and speed. This innovative integration 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#20363;&#31354;&#38388;&#20998;&#26512;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#38382;&#39064;&#30340;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#36873;&#25321;&#31995;&#32479;&#65292;&#39044;&#27979;&#20102;&#22312;&#32473;&#23450;&#29305;&#24449;&#19979;&#26368;&#36866;&#21512;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#28145;&#20837;&#20102;&#35299;&#20102;&#31639;&#27861;&#24615;&#33021;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.03229</link><description>&lt;p&gt;
&#22914;&#20309;&#36873;&#25321;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#20013;&#30340;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Which algorithm to select in sports timetabling?. (arXiv:2309.03229v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#20363;&#31354;&#38388;&#20998;&#26512;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#38382;&#39064;&#30340;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#36873;&#25321;&#31995;&#32479;&#65292;&#39044;&#27979;&#20102;&#22312;&#32473;&#23450;&#29305;&#24449;&#19979;&#26368;&#36866;&#21512;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#28145;&#20837;&#20102;&#35299;&#20102;&#31639;&#27861;&#24615;&#33021;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#20307;&#32946;&#31454;&#36187;&#37117;&#38656;&#35201;&#19968;&#20010;&#36187;&#31243;&#23433;&#25490;&#65292;&#30830;&#23450;&#27604;&#36187;&#38431;&#20237;&#20309;&#26102;&#20309;&#22320;&#30456;&#36935;&#12290;&#26368;&#36817;&#30340;&#22269;&#38469;&#36187;&#31243;&#23433;&#25490;&#31454;&#36187;(ITC2021)&#25581;&#31034;&#20102;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#34429;&#28982;&#21487;&#33021;&#24320;&#21457;&#20986;&#36890;&#29992;&#31639;&#27861;&#65292;&#20294;&#27599;&#20010;&#31639;&#27861;&#22312;&#38382;&#39064;&#23454;&#20363;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#24456;&#22823;&#12290;&#26412;&#25991;&#22312;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#26041;&#38754;&#25552;&#20379;&#20102;&#23454;&#20363;&#31354;&#38388;&#20998;&#26512;&#65292;&#20174;&#32780;&#28145;&#20837;&#20102;&#35299;&#20102;&#20843;&#31181;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#36873;&#25321;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#38382;&#39064;&#23454;&#20363;&#30340;&#29305;&#24449;&#39044;&#27979;&#21738;&#31181;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#21487;&#33021;&#34920;&#29616;&#26368;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#21738;&#20123;&#29305;&#24449;&#22312;&#20570;&#20986;&#39044;&#27979;&#26102;&#24456;&#37325;&#35201;&#65292;&#20174;&#32780;&#28145;&#20837;&#20102;&#35299;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#24314;&#35758;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#23454;&#20363;&#30340;&#32463;&#39564;&#38590;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22522;&#20110;&#22823;&#35268;&#27169;&#35745;&#31639;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Any sports competition needs a timetable, specifying when and where teams meet each other. The recent International Timetabling Competition (ITC2021) on sports timetabling showed that, although it is possible to develop general algorithms, the performance of each algorithm varies considerably over the problem instances. This paper provides an instance space analysis for sports timetabling, resulting in powerful insights into the strengths and weaknesses of eight state-of-the-art algorithms. Based on machine learning techniques, we propose an algorithm selection system that predicts which algorithm is likely to perform best when given the characteristics of a sports timetabling problem instance. Furthermore, we identify which characteristics are important in making that prediction, providing insights in the performance of the algorithms, and suggestions to further improve them. Finally, we assess the empirical hardness of the instances. Our results are based on large computational exper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#36890;&#36807;&#26500;&#24314;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#21487;&#20197;&#32508;&#21512;&#20998;&#26512;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#20026;&#33647;&#29289;&#20877;&#23450;&#20301;&#30740;&#31350;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2309.03227</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#25581;&#31034;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#30340;&#25216;&#26415;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates. (arXiv:2309.03227v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#36890;&#36807;&#26500;&#24314;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#21487;&#20197;&#32508;&#21512;&#20998;&#26512;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#20026;&#33647;&#29289;&#20877;&#23450;&#20301;&#30740;&#31350;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#20877;&#23450;&#20301;&#26159;&#19968;&#31181;&#21457;&#29616;&#29616;&#26377;&#33647;&#29289;&#26032;&#27835;&#30103;&#29992;&#36884;&#30340;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#65292;&#36817;&#24180;&#26469;&#22312;&#35745;&#31639;&#31185;&#23398;&#25991;&#29486;&#20013;&#20351;&#29992;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#30340;&#25216;&#26415;&#28508;&#21147;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32508;&#21512;&#20998;&#26512;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#31561;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;s-BKG&#65289;&#65292;&#21253;&#25324;&#26469;&#33258;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30340;&#33647;&#29289;&#12289;&#30142;&#30149;&#21644;&#22522;&#22240;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35782;&#21035;&#22312;s-BKG&#20013;&#19982;&#30446;&#26631;&#30142;&#30149;&#20851;&#32852;&#26377;&#38480;&#20294;&#22312;&#31354;&#38388;&#19978;&#32039;&#23494;&#30456;&#37051;&#30340;&#33647;&#29289;&#20316;&#20026;&#28508;&#22312;&#30340;&#33647;&#29289;&#20505;&#36873;&#29289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#33647;&#29289;&#19987;&#21033;&#20449;&#24687;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;p-BKG&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug repositioning-a promising strategy for discovering new therapeutic uses for existing drugs-has been increasingly explored in the computational science literature using biomedical databases. However, the technological potential of drug repositioning candidates has often been overlooked. This study presents a novel protocol to comprehensively analyse various sources such as pharmaceutical patents and biomedical databases, and identify drug repositioning candidates with both technological potential and scientific evidence. To this end, first, we constructed a scientific biomedical knowledge graph (s-BKG) comprising relationships between drugs, diseases, and genes derived from biomedical databases. Our protocol involves identifying drugs that exhibit limited association with the target disease but are closely located in the s-BKG, as potential drug candidates. We constructed a patent-informed biomedical knowledge graph (p-BKG) by adding pharmaceutical patent information. Finally, we d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#26469;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#19981;&#36275;&#21644;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#27169;&#22411;&#21644;&#24341;&#20837;&#36335;&#24452;&#39564;&#35777;&#22120;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#30340;&#25628;&#32034;&#21644;&#25512;&#29702;&#36335;&#24452;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.03224</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#20381;&#28982;&#33021;&#33719;&#30410;&#65306;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function. (arXiv:2309.03224v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03224
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#26469;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#19981;&#36275;&#21644;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#27169;&#22411;&#21644;&#24341;&#20837;&#36335;&#24452;&#39564;&#35777;&#22120;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#30340;&#25628;&#32034;&#21644;&#25512;&#29702;&#36335;&#24452;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#32972;&#26223;&#23398;&#20064;&#33021;&#21147;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36807;&#31243;&#30417;&#30563;&#65292;&#23558;PLMs&#24212;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#36890;&#24120;&#26080;&#27861;&#29983;&#25104;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#21644;&#26368;&#32456;&#31572;&#26696;&#65292;&#21363;&#20351;&#35299;&#20915;&#26041;&#26696;&#27010;&#29575;&#24456;&#39640;&#12290;&#20026;&#20102;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#24494;&#35843;&#30340;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#21644;&#36731;&#37327;&#32423;&#33021;&#37327;&#20989;&#25968;&#20026;LLMs&#36171;&#20104;&#21363;&#26102;&#21453;&#24212;&#21644;&#31934;&#32454;&#25512;&#29702;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#24494;&#35843;&#30340;LLMs&#37325;&#26032;&#23450;&#20041;&#20026;&#22522;&#20110;&#27531;&#24046;&#30340;&#33021;&#37327;&#27169;&#22411;&#65288;Residual-EBM&#65289;&#65292;&#24182;&#24212;&#29992;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#26469;&#20272;&#35745;&#33021;&#37327;&#20989;&#25968;&#30340;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24102;&#26377;&#33021;&#37327;&#20989;&#25968;&#30340;MCTS&#20316;&#20026;&#36335;&#24452;&#39564;&#35777;&#22120;&#26469;&#25628;&#32034;&#36755;&#20986;&#31354;&#38388;&#24182;&#35780;&#20272;&#25512;&#29702;&#36335;&#24452;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit impressive language understanding and in-context learning abilities including natural language processing (NLP) tasks and challenging mathematical reasoning. However, due to the lack of process-supervision, applying PLMs to mathematical reasoning tasks often fail to generate correct reasoning steps and final answer even though solutions have high probabilities. To unleash the mathematical reasoning of finetuned-LLMs without any further fineutuning steps, we propose a method to endow LLMs with immediate reaction and delicate reasoning system via Monte Carlo Tree Search(MCTS) and a light energy function to rank the decision steps. In particular, We first re-formalize the finetuned-LLMs to a Residual-based Energy Model~(Residual-EBM) and apply noise contrastive estimation to estimate the parameters of energy function . Then we use MCTS with energy function as path verifier to search the output space and evaluating the reasoning path. Through extensive 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23478;&#26063;&#21382;&#21490;&#25910;&#38598;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#38754;&#23545;&#38754;&#37319;&#35775;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#35752;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#22320;&#29702;&#21644;&#25216;&#26415;&#38480;&#21046;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#38271;&#30340;&#37319;&#35775;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.03223</link><description>&lt;p&gt;
&#30740;&#31350;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#25910;&#38598;&#23478;&#26063;&#21382;&#21490;&#20449;&#24687;&#26041;&#38754;&#19982;&#20256;&#32479;&#38754;&#23545;&#38754;&#35775;&#35848;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Examining the Effectiveness of Chatbots in Gathering Family History Information in Comparison to the Standard In-Person Interview-Based Approach. (arXiv:2309.03223v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23478;&#26063;&#21382;&#21490;&#25910;&#38598;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#38754;&#23545;&#38754;&#37319;&#35775;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#35752;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#22320;&#29702;&#21644;&#25216;&#26415;&#38480;&#21046;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#38271;&#30340;&#37319;&#35775;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#23478;&#35889;&#23398;&#23478;&#24120;&#24120;&#35201;&#20570;&#30340;&#20107;&#24773;&#20043;&#19968;&#23601;&#26159;&#36890;&#36807;&#38754;&#23545;&#38754;&#37319;&#35775;&#25110;&#20351;&#29992;&#35832;&#22914;ancestry.com&#20043;&#31867;&#30340;&#24179;&#21488;&#25910;&#38598;&#19968;&#20010;&#20154;&#30340;&#23478;&#26063;&#21382;&#21490;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#20026;&#23478;&#35889;&#23398;&#23478;&#25171;&#19979;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#36827;&#34892;&#36825;&#20123;&#37319;&#35775;&#30340;&#33021;&#21147;&#24448;&#24448;&#21463;&#21040;&#22320;&#29702;&#20301;&#32622;&#38480;&#21046;&#21644;&#34987;&#37319;&#35775;&#32773;&#30340;&#25216;&#26415;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#22312;&#36825;&#20123;&#31867;&#22411;&#30340;&#37319;&#35775;&#20013;&#65292;&#34987;&#37319;&#35775;&#32773;&#36890;&#24120;&#26159;&#19968;&#20010;&#24180;&#38271;&#19988;&#25216;&#26415;&#33021;&#21147;&#20302;&#20110;&#24179;&#22343;&#27700;&#24179;&#30340;&#20154;&#12290;&#22522;&#20110;&#36825;&#19968;&#28857;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#25454;&#25105;&#20204;&#22522;&#20110;&#20808;&#21069;&#30340;&#30740;&#31350;&#30456;&#20449;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#38754;&#21521;&#23478;&#26063;&#21382;&#21490;&#25910;&#38598;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#24182;&#36890;&#36807;&#19982;&#19978;&#36848;&#26367;&#20195;&#26041;&#26696;&#30340;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#36827;&#34892;&#27604;&#36739;&#26469;&#25506;&#32034;&#21033;&#29992;&#36825;&#31181;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23613;&#31649;&#36827;&#34892;&#37319;&#35775;&#30340;&#24179;&#22343;&#26102;&#38388;&#21487;&#33021;&#36739;&#38271;&#65292;
&lt;/p&gt;
&lt;p&gt;
One of the most common things that a genealogist is tasked with is the gathering of a person's initial family history, normally via in-person interviews or with the use of a platform such as ancestry.com, as this can provide a strong foundation upon which a genealogist may build. However, the ability to conduct these interviews can often be hindered by both geographical constraints and the technical proficiency of the interviewee, as the interviewee in these types of interviews is most often an elderly person with a lower than average level of technical proficiency. With this in mind, this study presents what we believe, based on prior research, to be the first chatbot geared entirely towards the gathering of family histories, and explores the viability of utilising such a chatbot by comparing the performance and usability of such a method with the aforementioned alternatives. With a chatbot-based approach, we show that, though the average time taken to conduct an interview may be long
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#24863;&#30693;&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20276;&#20387;&#21160;&#29289;&#30142;&#30149;&#35786;&#26029;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#34701;&#21512;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#20449;&#24687;&#21644;&#22270;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#37325;&#35201;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.03219</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#24863;&#30693;&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#30340;&#20276;&#20387;&#21160;&#29289;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Companion Animal Disease Diagnostics based on Literal-aware Medical Knowledge Graph Representation Learning. (arXiv:2309.03219v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03219
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#24863;&#30693;&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20276;&#20387;&#21160;&#29289;&#30142;&#30149;&#35786;&#26029;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#34701;&#21512;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#20449;&#24687;&#21644;&#22270;&#32467;&#26500;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#37325;&#35201;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#34987;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#65288;&#22914;&#31508;&#35760;&#21644;&#20861;&#21307;&#35760;&#24405;&#65289;&#26469;&#21463;&#30410;&#20110;&#21160;&#29289;&#30142;&#30149;&#30340;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#29992;&#20110;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#24102;&#26377;&#25991;&#26412;&#20449;&#24687;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#34920;&#31034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#30693;&#35782;&#22270;&#35889;&#26174;&#31034;&#20986;&#24322;&#26500;&#29305;&#24615;&#21644;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#26088;&#22312;&#20445;&#30041;&#22260;&#32469;&#30446;&#26631;&#33410;&#28857;&#30340;&#22270;&#32467;&#26500;&#65292;&#32780;&#26080;&#27861;&#32771;&#34385;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#65292;&#32780;&#36825;&#20123;&#25991;&#26412;&#20063;&#21487;&#33021;&#21253;&#21547;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#25928;&#35786;&#26029;&#21160;&#29289;&#30142;&#30149;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#20449;&#24687;&#21644;&#22270;&#32467;&#26500;&#65292;&#24182;&#23558;&#23427;&#20204;&#34701;&#21512;&#20026;&#32479;&#19968;&#30340;&#34920;&#31034;&#65292;&#21363;LiteralKG&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#65292;&#35813;&#22270;&#35889;&#26159;&#20174;&#21508;&#20010;&#21160;&#29289;&#21307;&#38498;&#25910;&#38598;&#30340;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#21450;&#25991;&#26412;&#20449;&#24687;&#26500;&#24314;&#32780;&#26469;&#12290;&#25105;&#20204;&#28982;&#21518;&#34701;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#20197;&#21450;&#25991;&#26412;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) embedding has been used to benefit the diagnosis of animal diseases by analyzing electronic medical records (EMRs), such as notes and veterinary records. However, learning representations to capture entities and relations with literal information in KGs is challenging as the KGs show heterogeneous properties and various types of literal information. Meanwhile, the existing methods mostly aim to preserve graph structures surrounding target nodes without considering different types of literals, which could also carry significant information. In this paper, we propose a knowledge graph embedding model for the efficient diagnosis of animal diseases, which could learn various types of literal information and graph structure and fuse them into unified representations, namely LiteralKG. Specifically, we construct a knowledge graph that is built from EMRs along with literal information collected from various animal hospitals. We then fuse different types of entities and no
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;S&amp;P 500&#25351;&#25968;&#19978;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22810;&#26679;&#21270;&#25237;&#36164;&#32452;&#21512;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21253;&#21547;COVID-19&#26102;&#26399;&#30340;&#24066;&#22330;&#25968;&#25454;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22522;&#20110;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;VI&#21644;SARSA&#65289;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;Q&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.03202</link><description>&lt;p&gt;
&#23545;&#22810;&#26679;&#21270;&#25237;&#36164;&#32452;&#21512;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Reinforcement Learning Techniques for Trading on a Diverse Portfolio. (arXiv:2309.03202v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;S&amp;P 500&#25351;&#25968;&#19978;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22810;&#26679;&#21270;&#25237;&#36164;&#32452;&#21512;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21253;&#21547;COVID-19&#26102;&#26399;&#30340;&#24066;&#22330;&#25968;&#25454;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22522;&#20110;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;VI&#21644;SARSA&#65289;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;Q&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#20851;&#20110;&#22312;S&amp;P 500&#25351;&#25968;&#19978;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;&#31574;&#30053;&#30340;&#20215;&#20540;&#36845;&#20195;&#65288;VI&#65289;&#21644;&#29366;&#24577;-&#21160;&#20316;-&#22870;&#21169;-&#29366;&#24577;-&#21160;&#20316;&#65288;SARSA&#65289;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#22522;&#20110;&#31574;&#30053;&#22806;&#30340;Q&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#21547;2000-2023&#24180;&#22810;&#24180;&#32929;&#24066;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#20998;&#26512;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#19981;&#21516;&#26102;&#38388;&#27573;&#19978;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#30340;&#32467;&#26524;&#21644;&#21457;&#29616;&#65306;&#19968;&#20010;&#21253;&#25324;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#30340;&#24180;&#20221;&#65292;&#19968;&#20010;&#19981;&#21253;&#25324;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;COVID-19&#26102;&#26399;&#30340;&#24066;&#22330;&#25968;&#25454;&#27604;&#22522;&#20934;&#31574;&#30053;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#22522;&#20110;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;VI&#21644;SARSA&#65289;&#20248;&#20110;Q&#23398;&#20064;&#65292;&#20984;&#26174;&#20102;&#31616;&#21333;&#31574;&#30053;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;Q&#23398;&#20064;&#30340;&#24615;&#33021;&#21487;&#33021;&#22240;&#26465;&#20214;&#30340;&#21464;&#21270;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work seeks to answer key research questions regarding the viability of reinforcement learning over the S&amp;P 500 index. The on-policy techniques of Value Iteration (VI) and State-action-reward-state-action (SARSA) are implemented along with the off-policy technique of Q-Learning. The models are trained and tested on a dataset comprising multiple years of stock market data from 2000-2023. The analysis presents the results and findings from training and testing the models using two different time periods: one including the COVID-19 pandemic years and one excluding them. The results indicate that including market data from the COVID-19 period in the training dataset leads to superior performance compared to the baseline strategies. During testing, the on-policy approaches (VI and SARSA) outperform Q-learning, highlighting the influence of bias-variance tradeoff and the generalization capabilities of simpler policies. However, it is noted that the performance of Q-learning may vary depe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38142;&#25509;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#19982;&#19981;&#21463;&#20449;&#20219;&#30340;&#26381;&#21153;&#22120;&#30340;&#21327;&#20316;&#35757;&#32451;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#20272;&#35745;&#65292;&#23558;&#38544;&#31169;&#39044;&#31639;&#20998;&#21035;&#29992;&#20110;&#38142;&#25509;&#21644;&#22270;&#30340;&#24230;&#65292;&#32531;&#35299;&#24046;&#20998;&#38544;&#31169;&#23545;&#35757;&#32451;&#20934;&#30830;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#38480;&#21046;&#38142;&#25509;&#27010;&#29575;&#25512;&#26029;&#19982;&#30495;&#23454;&#22270;&#25299;&#25169;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;&#25552;&#20986;&#30340;LDP&#26426;&#21046;&#26377;&#20004;&#20010;&#21464;&#20307;&#65292;&#22312;&#19981;&#21516;&#38544;&#31169;&#35774;&#32622;&#19979;&#20114;&#34917;&#20351;&#29992;&#65292;&#20197;&#36991;&#20813;&#35823;&#25253;&#38142;&#25509;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03190</link><description>&lt;p&gt;
Blink: &#20351;&#29992;&#36125;&#21494;&#26031;&#20272;&#35745;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#36890;&#36807;&#38142;&#25509;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation. (arXiv:2309.03190v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38142;&#25509;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#19982;&#19981;&#21463;&#20449;&#20219;&#30340;&#26381;&#21153;&#22120;&#30340;&#21327;&#20316;&#35757;&#32451;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#20272;&#35745;&#65292;&#23558;&#38544;&#31169;&#39044;&#31639;&#20998;&#21035;&#29992;&#20110;&#38142;&#25509;&#21644;&#22270;&#30340;&#24230;&#65292;&#32531;&#35299;&#24046;&#20998;&#38544;&#31169;&#23545;&#35757;&#32451;&#20934;&#30830;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#38480;&#21046;&#38142;&#25509;&#27010;&#29575;&#25512;&#26029;&#19982;&#30495;&#23454;&#22270;&#25299;&#25169;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;&#25552;&#20986;&#30340;LDP&#26426;&#21046;&#26377;&#20004;&#20010;&#21464;&#20307;&#65292;&#22312;&#19981;&#21516;&#38544;&#31169;&#35774;&#32622;&#19979;&#20114;&#34917;&#20351;&#29992;&#65292;&#20197;&#36991;&#20813;&#35823;&#25253;&#38142;&#25509;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30001;&#20110;&#22312;&#21508;&#31181;&#22270;&#25512;&#29702;&#20219;&#21153;&#20013;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#30340;&#21331;&#36234;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#35757;&#32451;&#23427;&#20204;&#21487;&#33021;&#24341;&#36215;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#38142;&#25509;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#26469;&#36827;&#34892;&#20998;&#25955;&#33410;&#28857;&#30340;&#21327;&#20316;&#65292;&#20351;&#24471;GNNs&#21487;&#20197;&#19982;&#19981;&#21463;&#20449;&#20219;&#30340;&#26381;&#21153;&#22120;&#36827;&#34892;&#35757;&#32451;&#32780;&#19981;&#27844;&#38706;&#20219;&#20309;&#38142;&#25509;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#38544;&#31169;&#39044;&#31639;&#20998;&#21035;&#29992;&#20110;&#26381;&#21153;&#22120;&#19978;&#30340;&#38142;&#25509;&#21644;&#22270;&#30340;&#24230;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#20272;&#35745;&#26356;&#22909;&#22320;&#21435;&#22122;&#22270;&#25299;&#25169;&#32467;&#26500;&#65292;&#32531;&#35299;&#24046;&#20998;&#38544;&#31169;&#23545;&#35757;&#32451;GNNs&#20934;&#30830;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#38480;&#21046;&#20174;&#25512;&#26029;&#20986;&#30340;&#38142;&#25509;&#27010;&#29575;&#19982;&#30495;&#23454;&#22270;&#25299;&#25169;&#20043;&#38388;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#38544;&#31169;&#35774;&#32622;&#19979;&#20114;&#34917;&#30340;LDP&#26426;&#21046;&#30340;&#21464;&#20307;&#20043;&#19968;&#65292;&#20854;&#20013;&#22312;&#36739;&#20302;&#30340;&#38544;&#31169;&#39044;&#31639;&#19979;&#20272;&#35745;&#36739;&#23569;&#30340;&#38142;&#25509;&#65292;&#20197;&#36991;&#20813;&#24403;&#19981;&#30830;&#23450;&#24615;&#36739;&#39640;&#26102;&#20986;&#29616;&#35823;&#25253;&#38142;&#25509;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have gained an increasing amount of popularity due to their superior capability in learning node embeddings for various graph inference tasks, but training them can raise privacy concerns. To address this, we propose using link local differential privacy over decentralized nodes, enabling collaboration with an untrusted server to train GNNs without revealing the existence of any link. Our approach spends the privacy budget separately on links and degrees of the graph for the server to better denoise the graph topology using Bayesian estimation, alleviating the negative impact of LDP on the accuracy of the trained GNNs. We bound the mean absolute error of the inferred link probabilities against the ground truth graph topology. We then propose two variants of our LDP mechanism complementing each other in different privacy settings, one of which estimates fewer links under lower privacy budgets to avoid false positive link estimates when the uncertainty is hig
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21360;&#35937;&#24863;&#30693;&#30340;&#22810;&#34892;&#20026;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20174;&#34892;&#20026;&#38388;&#21644;&#34892;&#20026;&#20869;&#37096;&#33719;&#21462;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#22810;&#23618;&#32423;&#22270;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#22312;&#22788;&#29702;&#22810;&#20010;&#34892;&#20026;&#20043;&#38388;&#20114;&#21160;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03169</link><description>&lt;p&gt;
&#22522;&#20110;&#21360;&#35937;&#24863;&#30693;&#30340;&#22810;&#34892;&#20026;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#23618;&#27425;&#22270;&#27880;&#24847;&#21147;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Impression-Informed Multi-Behavior Recommender System: A Hierarchical Graph Attention Approach. (arXiv:2309.03169v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03169
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21360;&#35937;&#24863;&#30693;&#30340;&#22810;&#34892;&#20026;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20174;&#34892;&#20026;&#38388;&#21644;&#34892;&#20026;&#20869;&#37096;&#33719;&#21462;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#22810;&#23618;&#32423;&#22270;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#22312;&#22788;&#29702;&#22810;&#20010;&#34892;&#20026;&#20043;&#38388;&#20114;&#21160;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25512;&#33616;&#31995;&#32479;&#20174;&#38544;&#24335;&#21453;&#39304;&#20013;&#33719;&#30410;&#33391;&#22810;&#65292;&#20294;&#24448;&#24448;&#20250;&#24573;&#30053;&#29992;&#25143;&#19982;&#29289;&#21697;&#20043;&#38388;&#30340;&#22810;&#34892;&#20026;&#20114;&#21160;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#21382;&#21490;&#19978;&#65292;&#36825;&#20123;&#31995;&#32479;&#35201;&#20040;&#23558;&#25152;&#26377;&#34892;&#20026;&#65292;&#22914;&#8220;&#21360;&#35937;&#8221;&#65288;&#20197;&#21069;&#31216;&#20026;&#8220;&#27983;&#35272;&#8221;&#65289;&#12289;&#8220;&#28155;&#21152;&#21040;&#36141;&#29289;&#36710;&#8221;&#21644;&#8220;&#36141;&#20080;&#8221;&#65292;&#24402;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#8220;&#20114;&#21160;&#8221;&#26631;&#31614;&#65292;&#35201;&#20040;&#20165;&#20248;&#20808;&#32771;&#34385;&#30446;&#26631;&#34892;&#20026;&#65292;&#36890;&#24120;&#26159;&#8220;&#36141;&#20080;&#8221;&#34892;&#20026;&#65292;&#24182;&#20002;&#24323;&#26377;&#20215;&#20540;&#30340;&#36741;&#21161;&#20449;&#21495;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36827;&#23637;&#35797;&#22270;&#35299;&#20915;&#36825;&#31181;&#31616;&#21270;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#20110;&#20248;&#21270;&#30446;&#26631;&#34892;&#20026;&#65292;&#19982;&#25968;&#25454;&#31232;&#32570;&#20316;&#26007;&#20105;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24448;&#24448;&#32469;&#36807;&#20102;&#19982;&#34892;&#20026;&#20869;&#22312;&#23618;&#27425;&#32467;&#26500;&#26377;&#20851;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;H&#8221;ierarchical &#8220;M&#8221;ulti-behavior &#8220;G&#8221;raph Attention &#8220;N&#8221;etwork&#65288;HMGN&#65289;&#12290;&#36825;&#20010;&#24320;&#21019;&#24615;&#30340;&#26694;&#26550;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20174;&#34892;&#20026;&#38388;&#21644;&#34892;&#20026;&#20869;&#37096;&#33719;&#21462;&#20449;&#24687;&#65292;&#21516;&#26102;&#37319;&#29992;&#22810;
&lt;/p&gt;
&lt;p&gt;
While recommender systems have significantly benefited from implicit feedback, they have often missed the nuances of multi-behavior interactions between users and items. Historically, these systems either amalgamated all behaviors, such as \textit{impression} (formerly \textit{view}), \textit{add-to-cart}, and \textit{buy}, under a singular 'interaction' label, or prioritized only the target behavior, often the \textit{buy} action, discarding valuable auxiliary signals. Although recent advancements tried addressing this simplification, they primarily gravitated towards optimizing the target behavior alone, battling with data scarcity. Additionally, they tended to bypass the nuanced hierarchy intrinsic to behaviors. To bridge these gaps, we introduce the \textbf{H}ierarchical \textbf{M}ulti-behavior \textbf{G}raph Attention \textbf{N}etwork (HMGN). This pioneering framework leverages attention mechanisms to discern information from both inter and intra-behaviors while employing a multi-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#32908;&#32905;&#39592;&#39612;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#39640;&#32500;&#24230;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#20013;&#33258;&#28982;&#19988;&#31283;&#20581;&#30340;&#34892;&#36208;&#12290;</title><link>http://arxiv.org/abs/2309.02976</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#39640;&#32500;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#20013;&#23454;&#29616;&#33258;&#28982;&#19988;&#31283;&#20581;&#30340;&#34892;&#36208;&#65292;&#26080;&#38656;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models. (arXiv:2309.02976v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#32908;&#32905;&#39592;&#39612;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#39640;&#32500;&#24230;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#20013;&#33258;&#28982;&#19988;&#31283;&#20581;&#30340;&#34892;&#36208;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#22797;&#26434;&#30340;&#33258;&#28982;&#29615;&#22659;&#20013;&#20197;&#31283;&#20581;&#30340;&#21452;&#36275;&#34892;&#36208;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#20182;&#20204;&#20805;&#20998;&#35843;&#25972;&#29983;&#29289;&#21147;&#23398;&#32908;&#32905;&#21160;&#21147;&#23398;&#21644;&#31070;&#32463;&#20449;&#21495;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#22312;&#22320;&#38754;&#26465;&#20214;&#30340;&#19981;&#30830;&#23450;&#24615;&#19979;&#20445;&#25345;&#31283;&#20581;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#27809;&#26377;&#23436;&#20840;&#29702;&#35299;&#31070;&#32463;&#31995;&#32479;&#22914;&#20309;&#35299;&#20915;&#32908;&#32905;&#39592;&#39612;&#20887;&#20313;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#32771;&#34385;&#31283;&#23450;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#22810;&#30446;&#26631;&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#35745;&#31639;&#26426;&#27169;&#25311;&#20013;&#65292;&#33021;&#37327;&#26368;&#23567;&#21270;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#25104;&#21151;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#22312;&#36712;&#36857;&#20248;&#21270;&#25110;&#22522;&#20110;&#21453;&#23556;&#30340;&#25511;&#21046;&#26041;&#27861;&#20013;&#37325;&#29616;&#20102;&#33258;&#28982;&#34892;&#36208;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#19968;&#27425;&#21482;&#20851;&#27880;&#29305;&#23450;&#30340;&#36816;&#21160;&#65292;&#24182;&#19988;&#22312;&#34917;&#20607;&#24178;&#25200;&#26102;&#65292;&#25152;&#20135;&#29983;&#30340;&#25511;&#21046;&#22120;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#22312;&#22235;&#36275;&#31995;&#32479;&#19978;&#23454;&#29616;&#20102;&#39640;&#24230;&#31283;&#23450;&#65288;&#21644;&#39640;&#25928;&#65289;&#30340;&#36816;&#21160;&#65292;&#20294;&#35201;&#20351;&#29992;&#21452;&#36275;&#29983;&#29289;&#21147;&#23398;&#27169;&#22411;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#34892;&#36208;&#30340;&#34892;&#36208;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans excel at robust bipedal walking in complex natural environments. In each step, they adequately tune the interaction of biomechanical muscle dynamics and neuronal signals to be robust against uncertainties in ground conditions. However, it is still not fully understood how the nervous system resolves the musculoskeletal redundancy to solve the multi-objective control problem considering stability, robustness, and energy efficiency. In computer simulations, energy minimization has been shown to be a successful optimization target, reproducing natural walking with trajectory optimization or reflex-based control methods. However, these methods focus on particular motions at a time and the resulting controllers are limited when compensating for perturbations. In robotics, reinforcement learning~(RL) methods recently achieved highly stable (and efficient) locomotion on quadruped systems, but the generation of human-like walking with bipedal biomechanical models has required extensive 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#30693;&#35782;&#33976;&#39311;&#23618;&#65292;&#33021;&#22815;&#26126;&#30830;&#22320;&#23884;&#20837;&#25945;&#24072;&#30340;&#30693;&#35782;&#21040;&#23398;&#29983;&#30340;&#29305;&#24449;&#21464;&#25442;&#20013;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.02843</link><description>&lt;p&gt;
&#35753;&#23398;&#29983;&#20915;&#31574;&#30340;&#30693;&#35782;&#33976;&#39311;&#23618;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation Layer that Lets the Student Decide. (arXiv:2309.02843v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#30693;&#35782;&#33976;&#39311;&#23618;&#65292;&#33021;&#22815;&#26126;&#30830;&#22320;&#23884;&#20837;&#25945;&#24072;&#30340;&#30693;&#35782;&#21040;&#23398;&#29983;&#30340;&#29305;&#24449;&#21464;&#25442;&#20013;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20013;&#30340;&#20856;&#22411;&#25216;&#26415;&#26159;&#36890;&#36807;&#23558;&#23398;&#29983;&#30340;&#21709;&#24212;&#19982;&#24378;&#22823;&#27169;&#22411;&#65288;&#25945;&#24072;&#65289;&#30340;&#21709;&#24212;&#21305;&#37197;&#26469;&#35268;&#33539;&#23398;&#29983;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#23398;&#29983;&#30340;&#29305;&#24449;&#21464;&#25442;&#30340;&#20316;&#29992;&#30456;&#23545;&#38544;&#21547;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#20013;&#38388;&#23618;&#30340;&#23454;&#36341;&#12290;&#20026;&#20102;&#26126;&#30830;&#22320;&#23884;&#20837;&#25945;&#24072;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;KD&#23618;&#65292;&#23427;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#33021;&#21147;&#25913;&#36827;&#20102;KD&#65306;i&#65289;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#25945;&#24072;&#30340;&#30693;&#35782;&#65292;&#33021;&#22815;&#20002;&#24323;&#26080;&#20851;&#30340;&#20449;&#24687;&#65307;ii&#65289;&#23558;&#36716;&#31227;&#30340;&#30693;&#35782;&#21521;&#21069;&#20256;&#36882;&#24471;&#26356;&#28145;&#20837;&#12290;&#22240;&#27492;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#23398;&#29983;&#21487;&#20197;&#20139;&#21463;&#21040;&#25945;&#24072;&#30340;&#30693;&#35782;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#35757;&#32451;&#20013;&#12290;&#27491;&#24335;&#22320;&#35828;&#65292;&#25105;&#20204;&#37325;&#26032;&#20998;&#37197;1x1-BN-ReLU-1x1&#21367;&#31215;&#22359;&#65292;&#26681;&#25454;&#23398;&#29983;&#23545;&#24212;&#21306;&#22495;&#19982;&#27169;&#26495;&#65288;&#30001;&#25945;&#24072;&#30417;&#30563;&#65289;&#30340;&#21305;&#37197;&#24773;&#20917;&#65292;&#20026;&#27599;&#20010;&#23616;&#37096;&#21306;&#22495;&#20998;&#37197;&#19968;&#20010;&#35821;&#20041;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typical technique in knowledge distillation (KD) is regularizing the learning of a limited capacity model (student) by pushing its responses to match a powerful model's (teacher). Albeit useful especially in the penultimate layer and beyond, its action on student's feature transform is rather implicit, limiting its practice in the intermediate layers. To explicitly embed the teacher's knowledge in feature transform, we propose a learnable KD layer for the student which improves KD with two distinct abilities: i) learning how to leverage the teacher's knowledge, enabling to discard nuisance information, and ii) feeding forward the transferred knowledge deeper. Thus, the student enjoys the teacher's knowledge during the inference besides training. Formally, we repurpose 1x1-BN-ReLU-1x1 convolution block to assign a semantic vector to each local region according to the template (supervised by the teacher) that the corresponding region of the student matches. To facilitate template learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#38598;&#25104;SE(3)&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.02685</link><description>&lt;p&gt;
Diffusion-EDFs: &#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation. (arXiv:2309.02685v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#22312;&#35270;&#35273;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#24212;&#29992;&#30340;&#22522;&#20110;SE(3)&#30340;&#31561;&#21464;&#21435;&#22122;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#36890;&#36807;&#38598;&#25104;SE(3)&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#39564;&#35777;&#20102;&#31561;&#21464;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#24314;&#27169;&#26368;&#36817;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#38543;&#26426;&#34892;&#20026;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#23398;&#20064;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-EDFs&#65292;&#19968;&#31181;&#23558;&#31354;&#38388;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#21363;SE(3)&#31561;&#21464;&#24615;&#24341;&#20837;&#25193;&#25955;&#29983;&#25104;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;SE(3)&#31561;&#21464;&#24615;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#26550;&#26500;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#22312;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#26102;&#21482;&#38656;5&#21040;10&#20010;&#20219;&#21153;&#28436;&#31034;&#21363;&#21487;&#12290;&#27492;&#22806;&#65292;&#19982;&#20043;&#21069;&#22522;&#20110;&#25193;&#25955;&#30340;&#25805;&#20316;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have verified that equivariant methods can significantly improve the data efficiency, generalizability, and robustness in robot learning. Meanwhile, denoising diffusion-based generative modeling has recently gained significant attention as a promising approach for robotic manipulation learning from demonstrations with stochastic behaviors. In this paper, we present Diffusion-EDFs, a novel approach that incorporates spatial roto-translation equivariance, i.e., SE(3)-equivariance to diffusion generative modeling. By integrating SE(3)-equivariance into our model architectures, we demonstrate that our proposed method exhibits remarkable data efficiency, requiring only 5 to 10 task demonstrations for effective end-to-end training. Furthermore, our approach showcases superior generalizability compared to previous diffusion-based manipulation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35270;&#35273;Transformer(ViT)&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#35299;&#20915;&#20351;&#29992;&#36716;&#25442;&#22270;&#20687;&#35757;&#32451;&#27169;&#22411;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;&#21152;&#23494;&#22270;&#20687;&#26102;&#20063;&#33021;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02556</link><description>&lt;p&gt;
&#20351;&#29992;&#21152;&#23494;&#22270;&#20687;&#26377;&#25928;&#24494;&#35843;&#35270;&#35273;Transformer&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images. (arXiv:2309.02556v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35270;&#35273;Transformer(ViT)&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#35299;&#20915;&#20351;&#29992;&#36716;&#25442;&#22270;&#20687;&#35757;&#32451;&#27169;&#22411;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;&#21152;&#23494;&#22270;&#20687;&#26102;&#20063;&#33021;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#36716;&#25442;&#25968;&#25454;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#24050;&#34987;&#24212;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#12289;&#35775;&#38382;&#25511;&#21046;&#21644;&#23545;&#25239;&#38450;&#24481;&#31561;&#21508;&#31181;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36716;&#25442;&#25968;&#25454;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;Transformer(ViT)&#23545;&#20351;&#29992;&#36716;&#25442;&#22270;&#20687;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#12290;&#25152;&#25552;&#20986;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#19981;&#20250;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#38477;&#20302;&#65292;&#24182;&#19988;&#26159;&#22312;ViT&#30340;&#23884;&#20837;&#32467;&#26500;&#22522;&#30784;&#19978;&#36827;&#34892;&#30340;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#20855;&#26377;CIFAR-10&#21644;CIFAR-100&#25968;&#25454;&#38598;&#30340;&#21152;&#23494;&#22270;&#20687;&#26102;&#38450;&#27490;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep neural networks (DNNs) trained with transformed data have been applied to various applications such as privacy-preserving learning, access control, and adversarial defenses. However, the use of transformed data decreases the performance of models. Accordingly, in this paper, we propose a novel method for fine-tuning models with transformed images under the use of the vision transformer (ViT). The proposed domain adaptation method does not cause the accuracy degradation of models, and it is carried out on the basis of the embedding structure of ViT. In experiments, we confirmed that the proposed method prevents accuracy degradation even when using encrypted images with the CIFAR-10 and CIFAR-100 datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#24102;&#36890;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30005;&#24433;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#22768;&#23398;&#30340;&#39057;&#29575;&#23610;&#24230;&#26469;&#23450;&#20041;&#39057;&#24102;&#24182;&#19988;&#21033;&#29992;&#20849;&#21516;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#20449;&#24687;&#20849;&#20139;&#29305;&#24615;&#25552;&#39640;&#20102;&#20998;&#31163;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02539</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30005;&#24433;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#36890;&#29992;&#24102;&#36890;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation. (arXiv:2309.02539v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#24102;&#36890;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30005;&#24433;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#36890;&#36807;&#20351;&#29992;&#24515;&#29702;&#22768;&#23398;&#30340;&#39057;&#29575;&#23610;&#24230;&#26469;&#23450;&#20041;&#39057;&#24102;&#24182;&#19988;&#21033;&#29992;&#20849;&#21516;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#20449;&#24687;&#20849;&#20139;&#29305;&#24615;&#25552;&#39640;&#20102;&#20998;&#31163;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#24433;&#38899;&#39057;&#28304;&#20998;&#31163;&#26159;&#38899;&#39057;&#28304;&#20998;&#31163;&#30340;&#19968;&#20010;&#30456;&#23545;&#36739;&#26032;&#30340;&#23376;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#20174;&#28151;&#38899;&#20013;&#25552;&#21462;&#23545;&#35805;&#38899;&#36712;&#12289;&#38899;&#20048;&#38899;&#36712;&#21644;&#29305;&#25928;&#38899;&#36712;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#23545;&#39057;&#29575;&#36724;&#30340;&#20219;&#20309;&#23436;&#20840;&#25110;&#36807;&#23436;&#22791;&#30340;&#20998;&#21306;&#36827;&#34892;&#27867;&#21270;&#12290;&#22522;&#20110;&#24515;&#29702;&#22768;&#23398;&#30340;&#39057;&#29575;&#23610;&#24230;&#29992;&#20110;&#30830;&#23450;&#24102;&#36890;&#30340;&#23450;&#20041;&#65292;&#29616;&#22312;&#20855;&#22791;&#20887;&#20313;&#24615;&#20197;&#36827;&#34892;&#26356;&#21487;&#38752;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#22522;&#20110;&#20449;&#22122;&#27604;&#21644;1-&#33539;&#25968;&#30340;&#31232;&#30095;&#20419;&#36827;&#23646;&#24615;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#20849;&#21516;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#20449;&#24687;&#20849;&#20139;&#29305;&#24615;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#25913;&#21892;&#38590;&#20197;&#27867;&#21270;&#30340;&#22768;&#38899;&#31867;&#21035;&#30340;&#20998;&#31163;&#24615;&#33021;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#25552;&#20379;&#28789;&#27963;&#24615;&#65292;&#21487;&#36731;&#26494;&#20998;&#31163;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;Divide and Remaster&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cinematic audio source separation is a relatively new subtask of audio source separation, with the aim of extracting the dialogue stem, the music stem, and the effects stem from their mixture. In this work, we developed a model generalizing the Bandsplit RNN for any complete or overcomplete partitions of the frequency axis. Psycho-acoustically motivated frequency scales were used to inform the band definitions which are now defined with redundancy for more reliable feature extraction. A loss function motivated by the signal-to-noise ratio and the sparsity-promoting property of the 1-norm was proposed. We additionally exploit the information-sharing property of a common-encoder setup to reduce computational complexity during both training and inference, improve separation performance for hard-to-generalize classes of sounds, and allow flexibility during inference time with easily detachable decoders. Our best model sets the state of the art on the Divide and Remaster dataset with perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24352;&#37327;&#21270;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#24352;&#37327;&#21270;&#26725;&#26753;&#20102;&#25968;&#25454;&#30340;&#22810;&#32500;&#29305;&#24615;&#19982;&#20256;&#32479;&#32447;&#24615;&#20195;&#25968;&#26041;&#27861;&#20013;&#30340;&#20108;&#32500;&#30697;&#38453;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22810;&#32500;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#36798;&#21147;&#21644;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02428</link><description>&lt;p&gt;
&#36890;&#36807;&#24352;&#37327;&#21270;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65306;&#32508;&#36848;&#19982;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Enhancing Deep Learning Models through Tensorization: A Comprehensive Survey and Framework. (arXiv:2309.02428v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24352;&#37327;&#21270;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#24352;&#37327;&#21270;&#26725;&#26753;&#20102;&#25968;&#25454;&#30340;&#22810;&#32500;&#29305;&#24615;&#19982;&#20256;&#32479;&#32447;&#24615;&#20195;&#25968;&#26041;&#27861;&#20013;&#30340;&#20108;&#32500;&#30697;&#38453;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22810;&#32500;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#36798;&#21147;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#39046;&#22495;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;&#30340;&#26085;&#30410;&#22797;&#26434;&#65292;&#20984;&#26174;&#20102;&#23545;&#26356;&#39640;&#25928;&#30340;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#26512;&#25216;&#26415;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#21463;&#21040;Helal&#65288;2023&#65289;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#26088;&#22312;&#20840;&#38754;&#20171;&#32461;&#24352;&#37327;&#21270;&#12290;&#36825;&#31181;&#36716;&#21270;&#24615;&#30340;&#26041;&#27861;&#24357;&#21512;&#20102;&#25968;&#25454;&#30340;&#22266;&#26377;&#22810;&#32500;&#29305;&#24615;&#19982;&#24120;&#29992;&#30340;&#32447;&#24615;&#20195;&#25968;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#31616;&#21270;&#30340;&#20108;&#32500;&#30697;&#38453;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24352;&#37327;&#21270;&#30340;&#27493;&#39588;&#12289;&#22810;&#32500;&#25968;&#25454;&#28304;&#12289;&#21508;&#31181;&#22810;&#26041;&#38754;&#20998;&#26512;&#26041;&#27861;&#30340;&#24212;&#29992;&#20197;&#21450;&#36825;&#20123;&#26041;&#27861;&#30340;&#22909;&#22788;&#12290;&#25991;&#31456;&#36890;&#36807;&#27604;&#36739;&#22312;Python&#20013;&#20351;&#29992;&#20108;&#32500;&#31639;&#27861;&#21644;&#22810;&#26041;&#38754;&#31639;&#27861;&#30340;&#30450;&#28304;&#20998;&#31163;&#65288;BSS&#65289;&#30340;&#23567;&#20363;&#23376;&#65292;&#32467;&#26524;&#34920;&#26126;&#22810;&#26041;&#38754;&#20998;&#26512;&#26356;&#20855;&#34920;&#36798;&#21147;&#12290;&#19982;&#32500;&#24230;&#35781;&#21650;&#30340;&#30452;&#35273;&#30456;&#21453;&#65292;&#21033;&#29992;&#22810;&#32500;&#25968;&#25454;&#38598;&#30340;&#21407;&#22987;&#24418;&#24335;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning growth of public domain data and the increasing complexity of deep learning model architectures have underscored the need for more efficient data representation and analysis techniques. This paper is motivated by the work of Helal (2023) and aims to present a comprehensive overview of tensorization. This transformative approach bridges the gap between the inherently multidimensional nature of data and the simplified 2-dimensional matrices commonly used in linear algebra-based machine learning algorithms. This paper explores the steps involved in tensorization, multidimensional data sources, various multiway analysis methods employed, and the benefits of these approaches. A small example of Blind Source Separation (BSS) is presented comparing 2-dimensional algorithms and a multiway algorithm in Python. Results indicate that multiway analysis is more expressive. Contrary to the intuition of the dimensionality curse, utilising multidimensional datasets in their native form 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23545;&#35821;&#38899;&#38556;&#30861;&#32773;&#30340;&#22768;&#23398;&#21040;&#21457;&#38899;&#21453;&#28436;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DeCoAR&#27169;&#22411;&#22312;&#31934;&#32454;&#35757;&#32451;&#26041;&#26696;&#20013;&#30456;&#23545;&#20110;&#20581;&#24247;&#23545;&#29031;&#32452;&#21644;&#24739;&#32773;&#65292;&#20998;&#21035;&#21462;&#24471;&#20102;&#32422;1.81\%&#21644;&#32422;4.56\%&#30340;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;(CC)&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.01108</link><description>&lt;p&gt;
&#35821;&#38899;&#38556;&#30861;&#32773;&#30340;&#22768;&#23398;&#21040;&#21457;&#38899;&#21453;&#28436;: &#39044;&#20808;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#26159;&#21542;&#26377;&#21033;&#65311;
&lt;/p&gt;
&lt;p&gt;
Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?. (arXiv:2309.01108v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23545;&#35821;&#38899;&#38556;&#30861;&#32773;&#30340;&#22768;&#23398;&#21040;&#21457;&#38899;&#21453;&#28436;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DeCoAR&#27169;&#22411;&#22312;&#31934;&#32454;&#35757;&#32451;&#26041;&#26696;&#20013;&#30456;&#23545;&#20110;&#20581;&#24247;&#23545;&#29031;&#32452;&#21644;&#24739;&#32773;&#65292;&#20998;&#21035;&#21462;&#24471;&#20102;&#32422;1.81\%&#21644;&#32422;4.56\%&#30340;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;(CC)&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#23398;&#21040;&#21457;&#38899;&#21453;&#28436;(ACI)&#28041;&#21450;&#20174;&#22768;&#23398;&#31354;&#38388;&#26144;&#23556;&#21040;&#21457;&#38899;&#31354;&#38388;&#12290;&#20449;&#21495;&#22788;&#29702;&#29305;&#24449;&#22914;MFCCs&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;ACI&#20219;&#21153;&#12290;&#23545;&#20110;&#26377;&#35821;&#38899;&#38556;&#30861;&#30340;&#24739;&#32773;&#65292;&#30001;&#20110;&#19981;&#20934;&#30830;&#21644;&#19981;&#28165;&#26224;&#30340;&#21457;&#38899;&#65292;ACI&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#23545;&#35821;&#38899;&#38556;&#30861;&#30340;ACI&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#29305;&#24449;&#23545;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20302;&#36164;&#28304;ACI&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;x-vectors&#19982;&#25552;&#21462;&#30340;SSL&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;BLSTM&#32593;&#32476;&#12290;&#22312;&#24050;&#30693;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#19977;&#31181;ACI&#35757;&#32451;&#26041;&#26696;&#65288;&#20027;&#39064;&#29305;&#23450;&#65292;&#32858;&#21512;&#21644;&#24494;&#35843;&#65289;&#12290;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;DeCoAR&#22312;&#24494;&#35843;&#26041;&#26696;&#20013;&#65292;&#30456;&#23545;&#20110;&#20581;&#24247;&#23545;&#29031;&#32452;&#21644;&#24739;&#32773;&#65292;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;(CC)&#30340;&#25913;&#36827;&#24133;&#24230;&#20998;&#21035;&#20026;&#32422;1.81\%&#21644;&#32422;4.56\%&#12290;
&lt;/p&gt;
&lt;p&gt;
$ $Acoustic-to-articulatory inversion (AAI) involves mapping from the acoustic space to the articulatory space. Signal-processing features like the MFCCs, have been widely used for the AAI task. For subjects with dysarthric speech, AAI is challenging because of an imprecise and indistinct pronunciation. In this work, we perform AAI for dysarthric speech using representations from pre-trained self-supervised learning (SSL) models. We demonstrate the impact of different pre-trained features on this challenging AAI task, at low-resource conditions. In addition, we also condition x-vectors to the extracted SSL features to train a BLSTM network. In the seen case, we experiment with three AAI training schemes (subject-specific, pooled, and fine-tuned). The results, consistent across training schemes, reveal that DeCoAR, in the fine-tuned scheme, achieves a relative improvement of the Pearson Correlation Coefficient (CC) by ${\sim}$1.81\% and ${\sim}$4.56\% for healthy controls and patients, 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35270;&#35273;&#24863;&#30693;&#12289;&#35821;&#35328;&#21644;&#21475;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#20302;&#32500;&#27010;&#24565;&#23884;&#20837;&#31639;&#27861;&#65292;&#23558;&#20154;&#31867;&#32463;&#39564;&#19982;&#33258;&#21160;&#26426;&#22120;&#30456;&#20284;&#24230;&#26680;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#21475;&#21619;&#20998;&#31867;&#65292;&#24182;&#19982;&#20154;&#31867;&#21475;&#21619;&#30693;&#35273;&#30456;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2308.16900</link><description>&lt;p&gt;
&#23398;&#20064;&#21697;&#21619;&#65306;&#19968;&#20010;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning to Taste: A Multimodal Wine Dataset. (arXiv:2308.16900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16900
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35270;&#35273;&#24863;&#30693;&#12289;&#35821;&#35328;&#21644;&#21475;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#20302;&#32500;&#27010;&#24565;&#23884;&#20837;&#31639;&#27861;&#65292;&#23558;&#20154;&#31867;&#32463;&#39564;&#19982;&#33258;&#21160;&#26426;&#22120;&#30456;&#20284;&#24230;&#26680;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#21475;&#21619;&#20998;&#31867;&#65292;&#24182;&#19982;&#20154;&#31867;&#21475;&#21619;&#30693;&#35273;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;WineSensed&#65292;&#29992;&#20110;&#30740;&#31350;&#35270;&#35273;&#24863;&#30693;&#12289;&#35821;&#35328;&#21644;&#21475;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;89.7&#19975;&#24352;&#33889;&#33796;&#37202;&#26631;&#31614;&#22270;&#29255;&#21644;82.4&#19975;&#26465;&#26469;&#33258;Vivino&#24179;&#21488;&#30340;&#33889;&#33796;&#37202;&#35780;&#35770;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#36229;&#36807;35&#19975;&#20010;&#29420;&#29305;&#30340;&#24180;&#20221;&#65292;&#38468;&#24102;&#20102;&#24180;&#20221;&#12289;&#20135;&#22320;&#12289;&#35780;&#20998;&#12289;&#37202;&#31934;&#21547;&#37327;&#12289;&#20215;&#26684;&#21644;&#33889;&#33796;&#32452;&#25104;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#21697;&#37202;&#23454;&#39564;&#23545;&#37096;&#20998;&#25968;&#25454;&#36827;&#34892;&#20102;&#32454;&#31890;&#24230;&#30340;&#21475;&#21619;&#27880;&#37322;&#65292;&#20849;&#26377;256&#21517;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#26681;&#25454;&#21475;&#21619;&#30340;&#30456;&#20284;&#24615;&#23545;&#33889;&#33796;&#37202;&#36827;&#34892;&#25490;&#24207;&#65292;&#24471;&#21040;&#20102;&#36229;&#36807;5&#21315;&#20010;&#37197;&#23545;&#30340;&#21475;&#21619;&#36317;&#31163;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#32500;&#27010;&#24565;&#23884;&#20837;&#31639;&#27861;&#65292;&#23558;&#20154;&#31867;&#32463;&#39564;&#19982;&#33258;&#21160;&#26426;&#22120;&#30456;&#20284;&#24230;&#26680;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#20010;&#20849;&#20139;&#30340;&#27010;&#24565;&#23884;&#20837;&#31354;&#38388;&#22312;&#31895;&#31890;&#24230;&#21475;&#21619;&#20998;&#31867;&#65288;&#37202;&#31934;&#21547;&#37327;&#65292;&#22269;&#23478;&#65292;&#33889;&#33796;&#65292;&#20215;&#26684;&#65292;&#35780;&#20998;&#65289;&#19978;&#25913;&#36827;&#65292;&#24182;&#19988;&#19982;&#22797;&#26434;&#30340;&#20154;&#31867;&#21475;&#21619;&#30693;&#35273;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present WineSensed, a large multimodal wine dataset for studying the relations between visual perception, language, and flavor. The dataset encompasses 897k images of wine labels and 824k reviews of wines curated from the Vivino platform. It has over 350k unique vintages, annotated with year, region, rating, alcohol percentage, price, and grape composition. We obtained fine-grained flavor annotations on a subset by conducting a wine-tasting experiment with 256 participants who were asked to rank wines based on their similarity in flavor, resulting in more than 5k pairwise flavor distances. We propose a low-dimensional concept embedding algorithm that combines human experience with automatic machine similarity kernels. We demonstrate that this shared concept embedding space improves upon separate embedding spaces for coarse flavor classification (alcohol percentage, country, grape, price, rating) and aligns with the intricate human perception of flavor.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.16898</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16898
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;"Attention Is All You Need"&#20013;&#24341;&#20837;&#36716;&#25442;&#22120;&#26550;&#26500;&#20197;&#26469;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#36716;&#25442;&#22120;&#20013;&#30340;&#27880;&#24847;&#21147;&#23618;&#25509;&#21463;&#36755;&#20837;&#20196;&#29260;&#24207;&#21015;$X$&#24182;&#36890;&#36807;&#35745;&#31639;softmax$(XQK^\top X^\top)$&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#20351;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;$(K,Q)$&#26159;&#21487;&#35757;&#32451;&#30340;&#38190;-&#26597;&#35810;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#20248;&#21270;&#20960;&#20309;&#21644;&#19968;&#20010;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#23545;&#20196;&#29260;&#23545;&#30340;&#22806;&#31215;&#26045;&#21152;&#32447;&#24615;&#32422;&#26463;&#65292;&#23558;&#26368;&#20339;&#36755;&#20837;&#20196;&#29260;&#19982;&#38750;&#26368;&#20339;&#20196;&#29260;&#20998;&#31163;&#12290;&#36825;&#20010;&#24418;&#24335;&#20027;&#20041;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#24449;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#21333;&#23618;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#65306;(1)&#20248;&#21270;&#27880;&#24847;&#21147;&#23618;&#65292;&#20351;&#29992;&#21487;&#21464;&#27491;&#21017;&#21270;&#21442;&#25968;$(K,Q)$&#65292;&#25910;&#25947;&#30340;&#26041;&#21521;&#26159;&#19968;&#20010;&#26368;&#23567;&#21270;&#32508;&#21512;&#21442;&#25968;$W=KQ^\top$&#30340;&#26680;&#33539;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#35299;&#20915;&#26041;&#26696;&#12290;&#32780;&#30452;&#25509;&#20351;&#29992;$W$&#36827;&#34892;&#21442;&#25968;&#21270;&#21017;&#26368;&#23567;&#21270;&#19968;&#20010;Frobenius&#33539;&#25968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#34920;&#24773;&#31526;&#21495;&#22312;&#34394;&#25311;&#24037;&#20316;&#31354;&#38388;&#20013;&#30340;&#20351;&#29992;&#23545;&#24320;&#21457;&#32773;&#21442;&#19982;&#21644;&#38382;&#39064;&#35299;&#20915;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34920;&#24773;&#31526;&#21495;&#21487;&#20197;&#26174;&#33879;&#32553;&#30701;&#38382;&#39064;&#30340;&#35299;&#20915;&#26102;&#38388;&#65292;&#24182;&#21560;&#24341;&#26356;&#22810;&#29992;&#25143;&#21442;&#19982;&#12290;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#23545;&#34920;&#24773;&#31526;&#21495;&#30340;&#25928;&#26524;&#20063;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.16360</link><description>&lt;p&gt;
&#34920;&#24773;&#31526;&#21495;&#20419;&#36827;&#20102;GitHub&#19978;&#24320;&#21457;&#32773;&#30340;&#21442;&#19982;&#21644;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Emoji Promotes Developer Participation and Issue Resolution on GitHub. (arXiv:2308.16360v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#34920;&#24773;&#31526;&#21495;&#22312;&#34394;&#25311;&#24037;&#20316;&#31354;&#38388;&#20013;&#30340;&#20351;&#29992;&#23545;&#24320;&#21457;&#32773;&#21442;&#19982;&#21644;&#38382;&#39064;&#35299;&#20915;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34920;&#24773;&#31526;&#21495;&#21487;&#20197;&#26174;&#33879;&#32553;&#30701;&#38382;&#39064;&#30340;&#35299;&#20915;&#26102;&#38388;&#65292;&#24182;&#21560;&#24341;&#26356;&#22810;&#29992;&#25143;&#21442;&#19982;&#12290;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#23545;&#34920;&#24773;&#31526;&#21495;&#30340;&#25928;&#26524;&#20063;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#30123;&#24773;&#26399;&#38388;&#36828;&#31243;&#24037;&#20316;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#35768;&#22810;&#20154;&#23545;&#36828;&#31243;&#24037;&#20316;&#30340;&#20302;&#25928;&#29575;&#34920;&#31034;&#25285;&#24551;&#12290;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#27807;&#36890;&#20013;&#32570;&#20047;&#38754;&#37096;&#34920;&#24773;&#21644;&#32930;&#20307;&#35821;&#35328;&#31561;&#38750;&#35821;&#35328;&#32447;&#32034;&#65292;&#36825;&#22952;&#30861;&#20102;&#26377;&#25928;&#30340;&#27807;&#36890;&#65292;&#24182;&#23545;&#24037;&#20316;&#32467;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20316;&#20026;&#26367;&#20195;&#30340;&#38750;&#35821;&#35328;&#32447;&#32034;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#20351;&#29992;&#30340;&#34920;&#24773;&#31526;&#21495;&#22312;&#34394;&#25311;&#24037;&#20316;&#31354;&#38388;&#20013;&#20063;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#34920;&#24773;&#31526;&#21495;&#30340;&#20351;&#29992;&#22914;&#20309;&#24433;&#21709;&#34394;&#25311;&#24037;&#20316;&#31354;&#38388;&#20013;&#24320;&#21457;&#32773;&#30340;&#21442;&#19982;&#21644;&#38382;&#39064;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;GitHub&#30340;&#19968;&#20010;&#19968;&#24180;&#21608;&#26399;&#20869;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#34913;&#37327;&#34920;&#24773;&#31526;&#21495;&#23545;&#38382;&#39064;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#25511;&#21046;&#38382;&#39064;&#20869;&#23481;&#12289;&#20179;&#24211;&#21644;&#20316;&#32773;&#20449;&#24687;&#31561;&#28151;&#28102;&#22240;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#34920;&#24773;&#31526;&#21495;&#21487;&#20197;&#26174;&#33879;&#32553;&#30701;&#38382;&#39064;&#30340;&#35299;&#20915;&#26102;&#38388;&#65292;&#24182;&#21560;&#24341;&#26356;&#22810;&#29992;&#25143;&#21442;&#19982;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#30340;&#24322;&#36136;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although remote working is increasingly adopted during the pandemic, many are concerned by the low-efficiency in the remote working. Missing in text-based communication are non-verbal cues such as facial expressions and body language, which hinders the effective communication and negatively impacts the work outcomes. Prevalent on social media platforms, emojis, as alternative non-verbal cues, are gaining popularity in the virtual workspaces well. In this paper, we study how emoji usage influences developer participation and issue resolution in virtual workspaces. To this end, we collect GitHub issues for a one-year period and apply causal inference techniques to measure the causal effect of emojis on the outcome of issues, controlling for confounders such as issue content, repository, and author information. We find that emojis can significantly reduce the resolution time of issues and attract more user participation. We also compare the heterogeneous effect on different types of issue
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#24102;&#23485;&#38480;&#21046;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#30772;&#22351;&#29616;&#26377;&#26631;&#20934;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#20445;&#25252;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.16215</link><description>&lt;p&gt;
&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deep Video Codec Control. (arXiv:2308.16215v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#24102;&#23485;&#38480;&#21046;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#30772;&#22351;&#29616;&#26377;&#26631;&#20934;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#20445;&#25252;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20002;&#22833;&#29575;&#35270;&#39057;&#21387;&#32553;&#36890;&#24120;&#29992;&#20110;&#20256;&#36755;&#21644;&#23384;&#20648;&#35270;&#39057;&#25968;&#25454;&#12290;&#23613;&#31649;&#23384;&#22312;&#36827;&#38454;&#65288;&#31070;&#32463;&#65289;&#21387;&#32553;&#26041;&#27861;&#65292;&#20294;&#32479;&#19968;&#35270;&#39057;&#32534;&#30721;&#22120;&#65288;&#22914;H.264&#25110;H.265&#65289;&#20173;&#28982;&#26159;&#20107;&#23454;&#19978;&#30340;&#26631;&#20934;&#12290;&#22312;&#38754;&#23545;&#21160;&#24577;&#32593;&#32476;&#24102;&#23485;&#26465;&#20214;&#30340;&#35270;&#39057;&#20256;&#36755;&#20013;&#65292;&#35270;&#39057;&#32534;&#30721;&#22120;&#38656;&#35201;&#36866;&#24212;&#38750;&#24120;&#19981;&#21516;&#30340;&#21387;&#32553;&#24378;&#24230;&#12290;&#36895;&#29575;&#25511;&#21046;&#27169;&#22359;&#22686;&#24378;&#32534;&#35299;&#30721;&#22120;&#30340;&#21387;&#32553;&#33021;&#21147;&#65292;&#20197;&#28385;&#36275;&#24102;&#23485;&#38480;&#21046;&#24182;&#23613;&#37327;&#20943;&#23569;&#35270;&#39057;&#22833;&#30495;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#35270;&#39057;&#32534;&#30721;&#22120;&#21450;&#20854;&#36895;&#29575;&#25511;&#21046;&#27169;&#22359;&#26159;&#20026;&#20102;&#26368;&#23567;&#21270;&#20154;&#31867;&#36136;&#37327;&#35780;&#20272;&#32780;&#24320;&#21457;&#30340;&#65292;&#21364;&#27809;&#26377;&#32771;&#34385;&#20445;&#25252;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#24102;&#23485;&#38480;&#21046;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#65292;&#24182;&#19981;&#30772;&#22351;&#29616;&#26377;&#30340;&#26631;&#20934;&#21270;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#20010;&#24120;&#35265;&#30340;&#35270;&#35273;&#20219;&#21153;&#65288;&#35821;&#20041;&#20998;&#21106;...
&lt;/p&gt;
&lt;p&gt;
Lossy video compression is commonly used when transmitting and storing video data. Unified video codecs (e.g., H.264 or H.265) remain the \emph{de facto} standard, despite the availability of advanced (neural) compression approaches. Transmitting videos in the face of dynamic network bandwidth conditions requires video codecs to adapt to vastly different compression strengths. Rate control modules augment the codec's compression such that bandwidth constraints are satisfied and video distortion is minimized. While, both standard video codes and their rate control modules are developed to minimize video distortion w.r.t. human quality assessment, preserving the downstream performance of deep vision models is not considered. In this paper, we present the first end-to-end learnable deep video codec control considering both bandwidth constraints and downstream vision performance, while not breaking existing standardization. We demonstrate for two common vision tasks (semantic segmentation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#26041;&#27861;&#26469;&#25351;&#31034;&#20998;&#31867;&#20915;&#31574;&#20013;&#26368;&#30456;&#20851;&#30340;&#36890;&#36947;&#21644;&#26102;&#38388;&#24207;&#21015;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.15223</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating Explanation Methods for Multivariate Time Series Classification. (arXiv:2308.15223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#26041;&#27861;&#26469;&#25351;&#31034;&#20998;&#31867;&#20915;&#31574;&#20013;&#26368;&#30456;&#20851;&#30340;&#36890;&#36947;&#21644;&#26102;&#38388;&#24207;&#21015;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#35745;&#31639;&#20219;&#21153;&#65292;&#20986;&#29616;&#22312;&#25968;&#25454;&#38543;&#26102;&#38388;&#21644;&#22810;&#20010;&#36890;&#36947;&#35760;&#24405;&#30340;&#24212;&#29992;&#20013;&#12290;&#20363;&#22914;&#65292;&#26234;&#33021;&#25163;&#34920;&#21487;&#20197;&#35760;&#24405;&#20154;&#20307;&#36816;&#21160;&#30340;&#21152;&#36895;&#24230;&#21644;&#26041;&#21521;&#65292;&#36825;&#20123;&#20449;&#21495;&#34987;&#35760;&#24405;&#20026;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#21487;&#20197;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#20102;&#35299;&#21644;&#39044;&#27979;&#20154;&#20307;&#36816;&#21160;&#21644;&#21508;&#31181;&#23646;&#24615;&#65292;&#22914;&#20581;&#36523;&#27700;&#24179;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20165;&#38752;&#20998;&#31867;&#26159;&#19981;&#22815;&#30340;&#65292;&#25105;&#20204;&#36890;&#24120;&#38656;&#35201;&#20998;&#31867;&#65292;&#21516;&#26102;&#36824;&#35201;&#29702;&#35299;&#27169;&#22411;&#23398;&#21040;&#20102;&#20160;&#20040;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#25968;&#25454;&#20013;&#30340;&#21738;&#20123;&#20449;&#24687;&#32473;&#20986;&#20102;&#39044;&#27979;&#65289;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#20998;&#26512;&#21644;&#35780;&#20272;&#19987;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;MTSC&#65289;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#33021;&#25351;&#20986;&#20998;&#31867;&#20915;&#31574;&#20013;&#26368;&#30456;&#20851;&#36890;&#36947;&#21644;&#26102;&#38388;&#24207;&#21015;&#28857;&#30340;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#27969;&#34892;&#19988;&#20934;&#30830;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65292;ROCKET&#21644;...
&lt;/p&gt;
&lt;p&gt;
Multivariate time series classification is an important computational task arising in applications where data is recorded over time and over multiple channels. For example, a smartwatch can record the acceleration and orientation of a person's motion, and these signals are recorded as multivariate time series. We can classify this data to understand and predict human movement and various properties such as fitness levels. In many applications classification alone is not enough, we often need to classify but also understand what the model learns (e.g., why was a prediction given, based on what information in the data). The main focus of this paper is on analysing and evaluating explanation methods tailored to Multivariate Time Series Classification (MTSC). We focus on saliency-based explanation methods that can point out the most relevant channels and time series points for the classification decision. We analyse two popular and accurate multivariate time series classifiers, ROCKET and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#25311;&#36830;&#32493;&#21160;&#24577;&#26465;&#20214;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15116</link><description>&lt;p&gt;
&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators. (arXiv:2308.15116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#25311;&#36830;&#32493;&#21160;&#24577;&#26465;&#20214;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#29983;&#29289;&#20998;&#23376;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#20998;&#23376;&#33021;&#22815;&#27874;&#21160;&#30340;&#21508;&#31181;&#26465;&#20214;&#19979;&#23545;&#19968;&#32452;&#31890;&#23376;&#36827;&#34892;&#27169;&#25311;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36719;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#20998;&#23376;&#21160;&#21147;&#23398;&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#36866;&#24212;&#24615;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#38750;&#24120;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#22330;&#26223;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#24037;&#20316;&#20197;&#28201;&#24230;&#20026;&#27979;&#35797;&#26696;&#20363;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#20351;&#20854;&#21487;&#20197;&#36890;&#36807;&#20219;&#20309;&#36830;&#32493;&#30340;&#21160;&#24577;&#26465;&#20214;&#65288;&#22914;&#21387;&#21147;&#21644;&#20307;&#31215;&#65289;&#36827;&#34892;&#26377;&#25928;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#20004;&#20010;&#38454;&#27573;&#65306;1&#65289;&#20351;&#29992;&#25968;&#25454;&#28151;&#21512;&#25216;&#26415;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22686;&#24378;&#20998;&#23376;&#32467;&#26500;&#25968;&#25454;&#21644;&#28201;&#24230;&#25552;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#27604;&#20363;&#30340;&#26041;&#24335;&#24212;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;2&#65289;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#24494;&#35843;&#26694;&#26550;&#25552;&#39640;&#20102;&#24494;&#35843;&#36807;&#31243;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#20026;&#36719;&#25552;&#31034;&#24494;&#35843;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular dynamics simulations have emerged as a fundamental instrument for studying biomolecules. At the same time, it is desirable to perform simulations of a collection of particles under various conditions in which the molecules can fluctuate. In this paper, we explore and adapt the soft prompt-based learning method to molecular dynamics tasks. Our model can remarkably generalize to unseen and out-of-distribution scenarios with limited training data. While our work focuses on temperature as a test case, the versatility of our approach allows for efficient simulation through any continuous dynamic conditions, such as pressure and volumes. Our framework has two stages: 1) Pre-trains with data mixing technique, augments molecular structure data and temperature prompts, then applies a curriculum learning method by increasing the ratio of them smoothly. 2) Meta-learning-based fine-tuning framework improves sample-efficiency of fine-tuning process and gives the soft prompt-tuning better 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AtmoRep&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#30830;&#23450;&#22797;&#26434;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#30340;&#36890;&#29992;&#25551;&#36848;&#65292;&#20174;&#32780;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#25216;&#33021;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13280</link><description>&lt;p&gt;
AtmoRep:&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning. (arXiv:2308.13280v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13280
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AtmoRep&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#30830;&#23450;&#22797;&#26434;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#30340;&#36890;&#29992;&#25551;&#36848;&#65292;&#20174;&#32780;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#25216;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27668;&#23545;&#20154;&#31867;&#26377;&#22810;&#31181;&#24433;&#21709;&#65292;&#20174;&#22240;&#22825;&#27668;&#19981;&#33391;&#32780;&#20007;&#29983;&#30340;&#25439;&#22833;&#21040;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#31038;&#20250;&#21644;&#32463;&#27982;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23545;&#22823;&#27668;&#21160;&#21147;&#23398;&#36827;&#34892;&#35745;&#31639;&#26426;&#27169;&#25311;&#23545;&#25105;&#20204;&#21644;&#26410;&#26469;&#30340;&#19990;&#20195;&#30340;&#31119;&#31049;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AtmoRep&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#35745;&#31639;&#26426;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;&#24191;&#27867;&#30340;&#24212;&#29992;&#25552;&#20379;&#25216;&#33021;&#32467;&#26524;&#12290;AtmoRep&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#26469;&#30830;&#23450;&#22823;&#27668;&#39640;&#24230;&#22797;&#26434;&#12289;&#38543;&#26426;&#21160;&#21147;&#23398;&#30340;&#36890;&#29992;&#25551;&#36848;&#65292;&#35813;&#25551;&#36848;&#22522;&#20110;&#21382;&#21490;&#36712;&#36857;&#30340;&#26368;&#20339;&#21487;&#29992;&#20272;&#35745;&#65292;&#36825;&#20123;&#21382;&#21490;&#36712;&#36857;&#21463;&#35266;&#27979;&#32422;&#26463;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#21644;&#19968;&#20010;&#29420;&#29305;&#30340;&#38598;&#21512;&#23454;&#29616;&#30340;&#65292;&#35813;&#38598;&#21512;&#20174;&#38543;&#26426;&#27169;&#22411;&#20013;&#37319;&#26679;&#65292;&#20854;&#21487;&#21464;&#24615;&#21463;&#21382;&#21490;&#35760;&#24405;&#20013;&#30340;&#21487;&#21464;&#24615;&#21551;&#21457;&#12290;AtmoRep&#30340;&#20219;&#21153;&#26080;&#20851;&#24615;&#20351;&#20854;&#33021;&#22815;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#28789;&#27963;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The atmosphere affects humans in a multitude of ways, from loss of life due to adverse weather effects to long-term social and economic impacts on societies. Computer simulations of atmospheric dynamics are, therefore, of great importance for the well-being of our and future generations. Here, we propose AtmoRep, a novel, task-independent stochastic computer model of atmospheric dynamics that can provide skillful results for a wide range of applications. AtmoRep uses large-scale representation learning from artificial intelligence to determine a general description of the highly complex, stochastic dynamics of the atmosphere from the best available estimate of the system's historical trajectory as constrained by observations. This is enabled by a novel self-supervised learning objective and a unique ensemble that samples from the stochastic model with a variability informed by the one in the historical record. The task-independent nature of AtmoRep enables skillful results for a divers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#24102;&#22122;&#22768;&#30340;&#20809;&#23398;&#29289;&#29702;&#19981;&#21487;&#22797;&#21046;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24314;&#31435;&#22810;&#39033;&#24335;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#22312;&#20855;&#22791;&#25361;&#25112;-&#21709;&#24212;&#23545;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#20197;&#20219;&#24847;&#39640;&#30340;&#27010;&#29575;&#23398;&#20064;&#21040;&#35813;&#31867;&#20989;&#25968;&#65292;&#36825;&#23545;&#20110;&#29289;&#29702;&#23433;&#20840;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.09199</link><description>&lt;p&gt;
&#23398;&#20064;&#24102;&#22122;&#22768;&#30340;&#20809;&#23398;&#29289;&#29702;&#19981;&#21487;&#22797;&#21046;&#20989;&#25968;&#21644;&#19982;&#23398;&#20064;&#35823;&#24046;&#30340;&#36830;&#25509;&#30340;&#22810;&#39033;&#24335;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Polynomial Bounds for Learning Noisy Optical Physical Unclonable Functions and Connections to Learning With Errors. (arXiv:2308.09199v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#24102;&#22122;&#22768;&#30340;&#20809;&#23398;&#29289;&#29702;&#19981;&#21487;&#22797;&#21046;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24314;&#31435;&#22810;&#39033;&#24335;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#22312;&#20855;&#22791;&#25361;&#25112;-&#21709;&#24212;&#23545;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#20197;&#20219;&#24847;&#39640;&#30340;&#27010;&#29575;&#23398;&#20064;&#21040;&#35813;&#31867;&#20989;&#25968;&#65292;&#36825;&#23545;&#20110;&#29289;&#29702;&#23433;&#20840;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#25361;&#25112;-&#21709;&#24212;&#23545;&#21644;&#22810;&#39033;&#24335;&#35745;&#31639;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#20197;&#20219;&#24847;&#39640;&#30340;&#27010;&#29575;&#23398;&#20064;&#19968;&#31867;&#20809;&#23398;&#29289;&#29702;&#19981;&#21487;&#22797;&#21046;&#20989;&#25968;&#65288;PUFs&#65289;&#65292;&#21069;&#25552;&#26159;&#22122;&#22768;&#21644;&#25361;&#25112;&#21521;&#37327;&#30340;&#20998;&#24067;&#31526;&#21512;&#28201;&#21644;&#30340;&#20551;&#35774;&#12290;&#36825;&#25193;&#23637;&#20102;Rh\"uramir&#31561;&#20154;&#65288;2013&#65289;&#30340;&#32467;&#26524;&#65292;&#20182;&#20204;&#22312;&#27809;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#35813;&#31867;PUFs&#30340;&#19968;&#20010;&#23376;&#38598;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23398;&#20064;&#65292;&#20551;&#35774;PUF&#30340;&#20809;&#23398;&#29305;&#24615;&#26159;&#32447;&#24615;&#30340;&#25110;&#20855;&#26377;&#21487;&#24573;&#30053;&#30340;&#38750;&#32447;&#24615;&#25928;&#24212;&#12290;&#25105;&#20204;&#26681;&#25454;PUF&#30340;&#23610;&#23544;&#21442;&#25968;&#12289;&#25361;&#25112;&#21644;&#22122;&#22768;&#21521;&#37327;&#30340;&#20998;&#24067;&#20197;&#21450;&#22238;&#24402;&#31639;&#27861;&#30340;&#27010;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#23548;&#20986;&#20102;&#25152;&#38656;&#26679;&#26412;&#25968;&#37327;&#21644;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#22810;&#39033;&#24335;&#30028;&#38480;&#65292;&#36825;&#31867;&#20284;&#20110;Bootle&#31561;&#20154;&#65288;2018&#65289;&#30340;&#20998;&#26512;&#65292;&#20182;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
It is shown that a class of optical physical unclonable functions (PUFs) can be learned to arbitrary precision with arbitrarily high probability, even in the presence of noise, given access to polynomially many challenge-response pairs and polynomially bounded computational power, under mild assumptions about the distributions of the noise and challenge vectors. This extends the results of Rh\"uramir et al. (2013), who showed a subset of this class of PUFs to be learnable in polynomial time in the absence of noise, under the assumption that the optics of the PUF were either linear or had negligible nonlinear effects. We derive polynomial bounds for the required number of samples and the computational complexity of a linear regression algorithm, based on size parameters of the PUF, the distributions of the challenge and noise vectors, and the probability and accuracy of the regression algorithm, with a similar analysis to one done by Bootle et al. (2018), who demonstrated a learning att
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21033;&#29992;&#22312;&#32447;LLMs&#29983;&#25104;&#30340;&#24694;&#24847;&#20869;&#23481;&#65292;&#36890;&#36807;&#23558;LLM&#20316;&#20026;&#25915;&#20987;&#32773;&#21644;&#21463;&#23475;&#32773;&#20043;&#38388;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#24694;&#24847;&#36719;&#20214;&#20256;&#25773;&#24182;&#19982;&#21463;&#23475;&#32773;&#31995;&#32479;&#20132;&#20114;&#30340;&#21629;&#20196;&#65292;&#21516;&#26102;&#35268;&#36991;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.09183</link><description>&lt;p&gt;
RatGPT:&#23558;&#22312;&#32447;LLMs&#36716;&#21270;&#20026;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
RatGPT: Turning online LLMs into Proxies for Malware Attacks. (arXiv:2308.09183v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21033;&#29992;&#22312;&#32447;LLMs&#29983;&#25104;&#30340;&#24694;&#24847;&#20869;&#23481;&#65292;&#36890;&#36807;&#23558;LLM&#20316;&#20026;&#25915;&#20987;&#32773;&#21644;&#21463;&#23475;&#32773;&#20043;&#38388;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#24694;&#24847;&#36719;&#20214;&#20256;&#25773;&#24182;&#19982;&#21463;&#23475;&#32773;&#31995;&#32479;&#20132;&#20114;&#30340;&#21629;&#20196;&#65292;&#21516;&#26102;&#35268;&#36991;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21644;&#26032;&#21457;&#24067;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#20026;&#36719;&#20214;&#24037;&#31243;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#23637;&#31034;&#20102;&#20351;&#29992;&#35832;&#22914;ChatGPT&#20043;&#31867;&#30340;LLMs&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#20123;&#20869;&#23481;&#21487;&#20197;&#30452;&#25509;&#34987;&#21033;&#29992;&#25110;&#24341;&#23548;&#32463;&#39564;&#19981;&#36275;&#30340;&#40657;&#23458;&#27494;&#22120;&#21270;&#24037;&#20855;&#21644;&#20195;&#30721;&#12290;&#36825;&#20123;&#30740;&#31350;&#36824;&#28085;&#30422;&#20102;&#38656;&#35201;&#25915;&#20987;&#32773;&#22788;&#20110;&#24490;&#29615;&#20013;&#30340;&#22330;&#26223;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#25554;&#20214;&#65292;&#23558;&#19968;&#20010;LLM&#20316;&#20026;&#25915;&#20987;&#32773;&#21644;&#21463;&#23475;&#32773;&#20043;&#38388;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#20854;&#20013;&#20351;&#29992;ChatGPT&#20256;&#25773;&#24694;&#24847;&#36719;&#20214;&#65292;&#21516;&#26102;&#35268;&#36991;&#26816;&#27979;&#65292;&#24182;&#19982;&#21629;&#20196;&#21644;&#25511;&#21046;&#65288;C2&#65289;&#26381;&#21153;&#22120;&#24314;&#31435;&#36890;&#20449;&#65292;&#20197;&#25509;&#25910;&#19982;&#21463;&#23475;&#32773;&#31995;&#32479;&#20132;&#20114;&#30340;&#21629;&#20196;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20445;&#25345;&#19981;&#34987;&#26816;&#27979;&#21644;&#36827;&#34892;&#25915;&#20987;&#25152;&#38656;&#30340;&#22522;&#26412;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of Generative AI and the capabilities of the newly released Large Language Models (LLMs) open new opportunities in software engineering. However, they also lead to new challenges in cybersecurity. Recently, researchers have shown the possibilities of using LLMs such as ChatGPT to generate malicious content that can directly be exploited or guide inexperienced hackers to weaponize tools and code. These studies covered scenarios that still require the attacker to be in the middle of the loop. In this study, we leverage openly available plugins and use an LLM as proxy between the attacker and the victim. We deliver a proof-of-concept where ChatGPT is used for the dissemination of malicious software while evading detection, alongside establishing the communication to a command and control (C2) server to receive commands to interact with a victim's system. Finally, we present the general approach as well as essential elements in order to stay undetected and make the attack a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;pFedHR&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#24322;&#26500;&#27169;&#22411;&#37325;&#32452;&#23454;&#29616;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;pFedHR&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#20351;&#29992;&#19981;&#20860;&#23481;&#25968;&#25454;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.08643</link><description>&lt;p&gt;
&#36890;&#36807;&#24322;&#26500;&#27169;&#22411;&#37325;&#32452;&#23454;&#29616;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Federated Learning via Heterogeneous Model Reassembly. (arXiv:2308.08643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;pFedHR&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#24322;&#26500;&#27169;&#22411;&#37325;&#32452;&#23454;&#29616;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;pFedHR&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#20351;&#29992;&#19981;&#20860;&#23481;&#25968;&#25454;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#27169;&#22411;&#24322;&#26500;&#30340;&#23454;&#38469;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20855;&#26377;&#19981;&#21516;&#32593;&#32476;&#32467;&#26500;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;pFedHR&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#24322;&#26500;&#27169;&#22411;&#37325;&#32452;&#23454;&#29616;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24322;&#26500;&#27169;&#22411;&#20010;&#24615;&#21270;&#38382;&#39064;&#35270;&#20026;&#26381;&#21153;&#22120;&#31471;&#30340;&#27169;&#22411;&#21305;&#37197;&#20248;&#21270;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;pFedHR&#33021;&#22815;&#33258;&#21160;&#19988;&#21160;&#24577;&#22320;&#29983;&#25104;&#20855;&#26377;&#26368;&#23567;&#20154;&#24037;&#24178;&#39044;&#30340;&#20449;&#24687;&#20016;&#23500;&#19988;&#22810;&#26679;&#21270;&#30340;&#20010;&#24615;&#21270;&#20505;&#36873;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#24322;&#26500;&#27169;&#22411;&#37325;&#32452;&#25216;&#26415;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#36731;&#20102;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#20844;&#20849;&#25968;&#25454;&#19982;&#23458;&#25143;&#31471;&#25968;&#25454;&#36896;&#25104;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;IID&#21644;&#38750;IID&#35774;&#32622;&#19979;&#65292;pFedHR&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;pFedHR&#26377;&#25928;&#38477;&#20302;&#20102;&#20351;&#29992;&#19981;&#20860;&#23481;&#25968;&#25454;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on addressing the practical yet challenging problem of model heterogeneity in federated learning, where clients possess models with different network structures. To track this problem, we propose a novel framework called pFedHR, which leverages heterogeneous model reassembly to achieve personalized federated learning. In particular, we approach the problem of heterogeneous model personalization as a model-matching optimization task on the server side. Moreover, pFedHR automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention. Furthermore, our proposed heterogeneous model reassembly technique mitigates the adverse impact introduced by using public data with different distributions from the client data to a certain extent. Experimental results demonstrate that pFedHR outperforms baselines on three datasets under both IID and Non-IID settings. Additionally, pFedHR effectively reduces the adverse impact of usi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#28909;&#21551;&#21160;&#31574;&#30053;&#23545;&#20110;&#35299;&#20915;&#20998;&#24067;&#21464;&#21270;&#21644;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.04014</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65306;&#22914;&#20309;&#65288;&#37325;&#26032;&#65289;&#28909;&#21551;&#21160;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Continual Pre-Training of Large Language Models: How to (re)warm your model?. (arXiv:2308.04014v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04014
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#28909;&#21551;&#21160;&#31574;&#30053;&#23545;&#20110;&#35299;&#20915;&#20998;&#24067;&#21464;&#21270;&#21644;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#23545;&#25968;&#21313;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#19968;&#26086;&#26377;&#26032;&#25968;&#25454;&#21487;&#29992;&#65292;&#23601;&#20250;&#37325;&#26032;&#24320;&#22987;&#36825;&#20010;&#36807;&#31243;&#12290;&#19968;&#31181;&#26356;&#24265;&#20215;&#21644;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#21363;&#29992;&#26032;&#25968;&#25454;&#26356;&#26032;&#39044;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#26159;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#36890;&#24120;&#20250;&#23548;&#33268;&#36807;&#21435;&#25968;&#25454;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#28909;&#21551;&#21160;&#31574;&#30053;&#23545;&#25345;&#32493;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#22312;&#35757;&#32451;&#26032;&#25968;&#25454;&#38598;&#26102;&#65292;&#38656;&#35201;&#37325;&#26032;&#22686;&#21152;&#23398;&#20064;&#29575;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;Pile&#65288;&#19978;&#28216;&#25968;&#25454;&#65292;300B&#26631;&#35760;&#65289;&#19978;&#25345;&#32493;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;SlimPajama&#65288;&#19979;&#28216;&#25968;&#25454;&#65292;297B&#26631;&#35760;&#65289;&#19978;&#36827;&#34892;&#20102;&#32447;&#24615;&#28909;&#21551;&#21160;&#21644;&#20313;&#24358;&#34928;&#20943;&#30340;&#35843;&#24230;&#12290;&#25105;&#20204;&#22312;Pythia 410M&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#19978;&#36827;&#34892;&#20102;&#25152;&#26377;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are routinely pre-trained on billions of tokens, only to restart the process over again once new data becomes available. A much cheaper and more efficient solution would be to enable the continual pre-training of these models, i.e. updating pre-trained models with new data instead of re-training them from scratch. However, the distribution shift induced by novel data typically results in degraded performance on past data. Taking a step towards efficient continual pre-training, in this work, we examine the effect of different warm-up strategies. Our hypothesis is that the learning rate must be re-increased to improve compute efficiency when training on a new dataset. We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens), following a linear warmup and cosine decay schedule. We conduct all experiments on the Pythia 410M language model architecture and ev
&lt;/p&gt;</description></item><item><title>GraPhSyM&#26159;&#19968;&#31181;&#29992;&#20110;&#20174;&#29289;&#29702;&#21512;&#25104;&#30005;&#36335;&#32593;&#34920;&#20013;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#21518;&#29289;&#29702;&#21512;&#25104;&#30005;&#36335;&#24310;&#36831;&#21644;&#38754;&#31215;&#25351;&#26631;&#30340;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#25351;&#26631;&#21487;&#35265;&#24615;&#32473;&#26089;&#26399;&#30340;EDA&#38454;&#27573;&#65292;&#21487;&#29992;&#20110;&#20840;&#23616;&#21327;&#21516;&#20248;&#21270;&#65292;&#24182;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;EDA&#20248;&#21270;&#26694;&#26550;&#20855;&#26377;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.03944</link><description>&lt;p&gt;
GraPhSyM: &#22270;&#24418;&#29289;&#29702;&#32508;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraPhSyM: Graph Physical Synthesis Model. (arXiv:2308.03944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03944
&lt;/p&gt;
&lt;p&gt;
GraPhSyM&#26159;&#19968;&#31181;&#29992;&#20110;&#20174;&#29289;&#29702;&#21512;&#25104;&#30005;&#36335;&#32593;&#34920;&#20013;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#21518;&#29289;&#29702;&#21512;&#25104;&#30005;&#36335;&#24310;&#36831;&#21644;&#38754;&#31215;&#25351;&#26631;&#30340;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#25351;&#26631;&#21487;&#35265;&#24615;&#32473;&#26089;&#26399;&#30340;EDA&#38454;&#27573;&#65292;&#21487;&#29992;&#20110;&#20840;&#23616;&#21327;&#21516;&#20248;&#21270;&#65292;&#24182;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;EDA&#20248;&#21270;&#26694;&#26550;&#20855;&#26377;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GraPhSyM&#65292;&#19968;&#31181;&#29992;&#20110;&#20174;&#29289;&#29702;&#21512;&#25104;&#30005;&#36335;&#32593;&#34920;&#20013;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#21518;&#29289;&#29702;&#21512;&#25104;&#30005;&#36335;&#24310;&#36831;&#21644;&#38754;&#31215;&#25351;&#26631;&#30340;&#22270;&#24418;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GATv2&#65289;&#27169;&#22411;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#27605;&#65292;GraPhSyM&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#35774;&#35745;&#25351;&#26631;&#21487;&#35265;&#24615;&#32473;&#26089;&#26399;&#30340;EDA&#38454;&#27573;&#65292;&#22914;&#36923;&#36753;&#32508;&#21512;&#65292;&#32780;&#26080;&#38656;&#36816;&#34892;&#32531;&#24930;&#30340;&#29289;&#29702;&#21512;&#25104;&#27969;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#36328;&#38454;&#27573;&#30340;&#20840;&#23616;&#21327;&#21516;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;GraPhSym&#25552;&#20379;&#30340;&#24555;&#36895;&#32780;&#31934;&#30830;&#30340;&#21453;&#39304;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;EDA&#20248;&#21270;&#26694;&#26550;&#33267;&#20851;&#37325;&#35201;&#12290;&#32473;&#23450;&#19968;&#20010;&#34920;&#31034;&#20026;&#22270;&#24418;&#30340;&#30005;&#36335;&#38376;&#32423;&#32593;&#34920;&#65292;GraPhSyM&#21033;&#29992;&#22270;&#24418;&#32467;&#26500;&#12289;&#36830;&#25509;&#24615;&#21644;&#30005;&#24615;&#29305;&#24449;&#26469;&#39044;&#27979;&#29289;&#29702;&#21512;&#25104;&#36716;&#25442;&#65288;&#22914;&#32531;&#20914;&#22120;&#25554;&#20837;&#21644;&#38376;&#23610;&#23544;&#35843;&#25972;&#65289;&#23545;&#30005;&#36335;&#30340;&#24433;&#21709;&#12290;&#24403;&#22312;&#19968;&#20010;&#21253;&#21547;6000&#20010;&#21069;&#32512;&#21152;&#27861;&#22120;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65288;&#21512;&#25104;&#21040;&#28608;&#36827;&#24310;&#36831;&#30446;&#26631;&#65289;&#65292;GraPhSyM&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#21518;&#21512;&#25104;&#30340;&#24310;&#36831;&#65288;98.3%&#65289;&#21644;&#38754;&#31215;&#65288;96.1%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce GraPhSyM, a Graph Attention Network (GATv2) model for fast and accurate estimation of post-physical synthesis circuit delay and area metrics from pre-physical synthesis circuit netlists. Once trained, GraPhSyM provides accurate visibility of final design metrics to early EDA stages, such as logic synthesis, without running the slow physical synthesis flow, enabling global co-optimization across stages. Additionally, the swift and precise feedback provided by GraPhSym is instrumental for machine-learning-based EDA optimization frameworks. Given a gate-level netlist of a circuit represented as a graph, GraPhSyM utilizes graph structure, connectivity, and electrical property features to predict the impact of physical synthesis transformations such as buffer insertion and gate sizing. When trained on a dataset of 6000 prefix adder designs synthesized at an aggressive delay target, GraPhSyM can accurately predict the post-synthesis delay (98.3%) and area (96.1%) m
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;(RAHNet)&#29992;&#20110;&#38271;&#23614;&#22270;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#19979;&#30340;&#20559;&#24046;&#21644;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02335</link><description>&lt;p&gt;
RAHNet: &#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;&#29992;&#20110;&#38271;&#23614;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification. (arXiv:2308.02335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02335
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;(RAHNet)&#29992;&#20110;&#38271;&#23614;&#22270;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#19979;&#30340;&#20559;&#24046;&#21644;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#31867;&#26159;&#35768;&#22810;&#23454;&#38469;&#22810;&#23186;&#20307;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#22270;&#21487;&#20197;&#34920;&#31034;&#21508;&#31181;&#22810;&#23186;&#20307;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#31038;&#20132;&#32593;&#32476;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#65292;&#20854;&#20013;&#31867;&#20998;&#24067;&#26159;&#24179;&#34913;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#25968;&#25454;&#36890;&#24120;&#21576;&#29616;&#20986;&#38271;&#23614;&#31867;&#21035;&#20998;&#24067;&#65292;&#23548;&#33268;&#22312;&#20351;&#29992;GNN&#26102;&#23545;&#22836;&#37096;&#31867;&#21035;&#23384;&#22312;&#20559;&#24046;&#65292;&#19988;&#23545;&#23614;&#37096;&#31867;&#21035;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#37325;&#26032;&#24179;&#34913;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#26410;&#33021;&#26126;&#30830;&#24341;&#20837;&#26032;&#30693;&#35782;&#65292;&#24182;&#29306;&#29298;&#20102;&#22836;&#37096;&#31867;&#21035;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#22411;&#28151;&#21512;&#32593;&#32476;(RAHNet)&#65292;&#20197;&#20998;&#31163;&#30340;&#26041;&#24335;&#32852;&#21512;&#23398;&#20064;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26080;&#20559;&#30340;&#20998;&#31867;&#22120;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#22120;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22270;&#26816;&#32034;&#27169;&#22359;&#26469;&#25628;&#32034;&#30456;&#20851;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph classification is a crucial task in many real-world multimedia applications, where graphs can represent various multimedia data types such as images, videos, and social networks. Previous efforts have applied graph neural networks (GNNs) in balanced situations where the class distribution is balanced. However, real-world data typically exhibit long-tailed class distributions, resulting in a bias towards the head classes when using GNNs and limited generalization ability over the tail classes. Recent approaches mainly focus on re-balancing different classes during model training, which fails to explicitly introduce new knowledge and sacrifices the performance of the head classes. To address these drawbacks, we propose a novel framework called Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature extractor and an unbiased classifier in a decoupled manner. In the feature extractor training stage, we develop a graph retrieval module to search for relevant grap
&lt;/p&gt;</description></item><item><title>VLUCI&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#21453;&#20107;&#23454;&#25512;&#26029;&#20013;&#30340;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#21452;&#37325;&#21464;&#20998;&#25512;&#26029;&#27169;&#22411;&#26469;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#20013;&#35266;&#27979;&#21644;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00904</link><description>&lt;p&gt;
VLUCI: &#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference. (arXiv:2308.00904v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00904
&lt;/p&gt;
&lt;p&gt;
VLUCI&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#21453;&#20107;&#23454;&#25512;&#26029;&#20013;&#30340;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#21452;&#37325;&#21464;&#20998;&#25512;&#26029;&#27169;&#22411;&#26469;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#20013;&#35266;&#27979;&#21644;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#22312;&#27969;&#34892;&#30149;&#23398;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32463;&#27982;&#23398;&#31561;&#39046;&#22495;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#35266;&#23519;&#25968;&#25454;&#20013;&#36827;&#34892;&#21435;&#28151;&#28102;&#21644;&#21453;&#20107;&#23454;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#34429;&#28982;&#29616;&#26377;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#65292;&#20294;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#23384;&#22312;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#25197;&#26354;&#20102;&#22240;&#26524;&#25512;&#26029;&#24182;&#24433;&#21709;&#20102;&#21453;&#20107;&#23454;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#21464;&#21442;&#25968;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21453;&#20107;&#23454;&#25512;&#26029;&#20013;&#30340;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#65288;VLUCI&#65289;&#65292;&#23427;&#29983;&#25104;&#20102;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;VLUCI&#25918;&#26494;&#20102;&#22823;&#22810;&#25968;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#30340;&#26080;&#28151;&#28102;&#20551;&#35774;&#12290;&#36890;&#36807;&#35299;&#32806;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#21644;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#65292;VLUCI&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#37325;&#21464;&#20998;&#25512;&#26029;&#27169;&#22411;&#65292;&#20197;&#36817;&#20284;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#36825;&#20123;&#21464;&#37327;&#29992;&#20110;&#25512;&#26029;&#26356;&#20934;&#30830;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#12290;&#23545;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference plays a vital role in diverse domains like epidemiology, healthcare, and economics. De-confounding and counterfactual prediction in observational data has emerged as a prominent concern in causal inference research. While existing models tackle observed confounders, the presence of unobserved confounders remains a significant challenge, distorting causal inference and impacting counterfactual outcome accuracy. To address this, we propose a novel variational learning model of unobserved confounders for counterfactual inference (VLUCI), which generates the posterior distribution of unobserved confounders. VLUCI relaxes the unconfoundedness assumption often overlooked by most causal inference methods. By disentangling observed and unobserved confounders, VLUCI constructs a doubly variational inference model to approximate the distribution of unobserved confounders, which are used for inferring more accurate counterfactual outcomes. Extensive experiments on synthetic and s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MajorCert&#65292;&#36890;&#36807;&#25214;&#21040;&#22312;&#24213;&#23618;&#20998;&#31867;&#22120;&#20013;&#30001;&#21516;&#19968;&#20010;&#34917;&#19969;&#21306;&#22495;&#22312;&#21516;&#19968;&#20010;&#26679;&#26412;&#19978;&#21487;&#20197;&#25805;&#32437;&#30340;&#25152;&#26377;&#21487;&#33021;&#26631;&#31614;&#38598;&#21512;&#65292;&#24182;&#26816;&#26597;&#23427;&#20204;&#30340;&#22823;&#22810;&#25968;&#19981;&#21464;&#24615;&#26469;&#23545;&#26679;&#26412;&#36827;&#34892;&#34917;&#19969;&#31283;&#20581;&#24615;&#35748;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.00452</link><description>&lt;p&gt;
&#19968;&#20010;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34917;&#19969;&#31283;&#20581;&#24615;&#35748;&#35777;&#30340;&#22823;&#22810;&#25968;&#19981;&#21464;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Majority Invariant Approach to Patch Robustness Certification for Deep Learning Models. (arXiv:2308.00452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MajorCert&#65292;&#36890;&#36807;&#25214;&#21040;&#22312;&#24213;&#23618;&#20998;&#31867;&#22120;&#20013;&#30001;&#21516;&#19968;&#20010;&#34917;&#19969;&#21306;&#22495;&#22312;&#21516;&#19968;&#20010;&#26679;&#26412;&#19978;&#21487;&#20197;&#25805;&#32437;&#30340;&#25152;&#26377;&#21487;&#33021;&#26631;&#31614;&#38598;&#21512;&#65292;&#24182;&#26816;&#26597;&#23427;&#20204;&#30340;&#22823;&#22810;&#25968;&#19981;&#21464;&#24615;&#26469;&#23545;&#26679;&#26412;&#36827;&#34892;&#34917;&#19969;&#31283;&#20581;&#24615;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34917;&#19969;&#31283;&#20581;&#24615;&#35748;&#35777;&#30830;&#20445;&#22312;&#32473;&#23450;&#26679;&#26412;&#19978;&#65292;&#27809;&#26377;&#34917;&#19969;&#33021;&#22815;&#36890;&#36807;&#25805;&#32437;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25216;&#26415;&#19981;&#33021;&#23545;&#26080;&#27861;&#22312;&#20998;&#31867;&#22120;&#25110;&#32773;&#34917;&#19969;&#21306;&#22495;&#27700;&#24179;&#19978;&#36798;&#21040;&#20005;&#26684;&#26631;&#20934;&#30340;&#26679;&#26412;&#36827;&#34892;&#35748;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MajorCert&#12290;MajorCert&#39318;&#20808;&#25214;&#21040;&#22312;&#24213;&#23618;&#20998;&#31867;&#22120;&#20013;&#30001;&#21516;&#19968;&#20010;&#34917;&#19969;&#21306;&#22495;&#22312;&#21516;&#19968;&#20010;&#26679;&#26412;&#19978;&#21487;&#20197;&#25805;&#32437;&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#26631;&#31614;&#38598;&#21512;&#65292;&#28982;&#21518;&#36880;&#20010;&#26522;&#20030;&#23427;&#20204;&#30340;&#32452;&#21512;&#65292;&#24182;&#26368;&#32456;&#26816;&#26597;&#25152;&#26377;&#36825;&#20123;&#32452;&#21512;&#30340;&#22823;&#22810;&#25968;&#19981;&#21464;&#24615;&#26159;&#21542;&#23436;&#25972;&#20197;&#23545;&#26679;&#26412;&#36827;&#34892;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patch robustness certification ensures no patch within a given bound on a sample can manipulate a deep learning model to predict a different label. However, existing techniques cannot certify samples that cannot meet their strict bars at the classifier or patch region levels. This paper proposes MajorCert. MajorCert firstly finds all possible label sets manipulatable by the same patch region on the same sample across the underlying classifiers, then enumerates their combinations element-wise, and finally checks whether the majority invariant of all these combinations is intact to certify samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20316;&#20026;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#24110;&#21161;&#19977;&#32500;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#28857;&#20113;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#31435;&#20307;&#20851;&#31995;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14971</link><description>&lt;p&gt;
Take-A-Photo: &#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#28857;&#20113;&#27169;&#22411;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models. (arXiv:2307.14971v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20316;&#20026;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#24110;&#21161;&#19977;&#32500;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#28857;&#20113;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#31435;&#20307;&#20851;&#31995;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;MAE&#24102;&#39046;&#19979;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#22312;2D&#35270;&#35273;&#39046;&#22495;&#24050;&#32463;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#28508;&#21147;&#26469;&#25552;&#21319;&#22522;&#26412;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;3D&#35270;&#35273;&#39046;&#22495;&#65292;&#23545;Transformer&#20026;&#22522;&#30784;&#30340;&#39592;&#24178;&#32593;&#32476;&#30340;&#36807;&#24230;&#20381;&#36182;&#20197;&#21450;&#28857;&#20113;&#30340;&#26080;&#24207;&#24615;&#38480;&#21046;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#20219;&#20309;&#28857;&#20113;&#27169;&#22411;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#20174;&#19981;&#21516;&#30340;&#23039;&#21183;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20316;&#20026;&#39044;&#35757;&#32451;&#26041;&#26696;&#12290;&#30456;&#27604;&#20110;&#20854;&#28857;&#20113;&#23545;&#24212;&#29289;&#65292;&#29983;&#25104;&#35270;&#22270;&#22270;&#20687;&#20855;&#26377;&#26356;&#31934;&#30830;&#30340;&#30417;&#30563;&#65292;&#20174;&#32780;&#24110;&#21161;3D&#32972;&#39592;&#26356;&#22909;&#22320;&#29702;&#35299;&#28857;&#20113;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#31435;&#20307;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#19977;&#32500;&#21040;&#20108;&#32500;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#26377;&#25928;&#22320;&#25552;&#21319;...
&lt;/p&gt;
&lt;p&gt;
With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision. However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of generative pre-training. In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model. We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme. Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud. Experimental results have proved the superiority of our proposed 3D-to-2D generative pre-training over previous pre-training methods. Our method is also effective in boost
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#35299;&#20915;&#20102;Wasserstein GAN&#20013;&#20998;&#21306;&#20989;&#25968;&#26377;&#20559;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#35206;&#30422;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#26469;&#23454;&#29616;&#23545;&#20998;&#21306;&#20989;&#25968;&#30340;&#26080;&#20559;&#20272;&#35745;&#21644;&#29983;&#25104;&#22120;&#29109;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.09882</link><description>&lt;p&gt;
&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adversarial Likelihood Estimation with One-way Flows. (arXiv:2307.09882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#35299;&#20915;&#20102;Wasserstein GAN&#20013;&#20998;&#21306;&#20989;&#25968;&#26377;&#20559;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#35206;&#30422;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#26469;&#23454;&#29616;&#23545;&#20998;&#21306;&#20989;&#25968;&#30340;&#26080;&#20559;&#20272;&#35745;&#21644;&#29983;&#25104;&#22120;&#29109;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#20294;&#26080;&#27861;&#25552;&#20379;&#26679;&#26412;&#21608;&#22260;&#30340;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#27880;&#24847;&#21040;&#22312;&#33021;&#37327;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#65292;&#26368;&#22823;&#21270;&#23545;&#25968;&#20284;&#28982;&#21487;&#20197;&#23548;&#33268;&#21028;&#21035;&#22120;&#25552;&#20379;&#38750;&#24402;&#19968;&#21270;&#30340;&#23494;&#24230;&#65288;&#36890;&#24120;&#31216;&#20026;&#33021;&#37327;&#65289;&#30340;&#23545;&#25239;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;&#36825;&#19968;&#35266;&#28857;&#65292;&#32467;&#21512;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#19979;&#20869;&#23481;&#65306;1&#65289;Wasserstein GAN&#23545;&#20998;&#21306;&#20989;&#25968;&#36827;&#34892;&#20102;&#26377;&#20559;&#20272;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26080;&#20559;&#20272;&#35745;&#26041;&#27861;&#65307;2&#65289;&#22312;&#26368;&#20248;&#21270;&#20284;&#28982;&#26102;&#65292;&#24517;&#39035;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#12290;&#36825;&#34987;&#20551;&#35774;&#20250;&#25552;&#20379;&#26356;&#22909;&#30340;&#27169;&#24335;&#35206;&#30422;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#26126;&#30830;&#35745;&#31639;&#20102;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#12290;&#36825;&#26159;&#35774;&#35745;&#26080;&#20559;&#20272;&#35745;&#20998;&#21306;&#20989;&#25968;&#20197;&#21450;&#35745;&#31639;&#29983;&#25104;&#22120;&#29109;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#29983;&#25104;&#23494;&#24230;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#30340;&#27969;&#32593;&#32476;&#26469;&#33719;&#24471;&#30340;&#65292;&#31216;&#20026;&#21333;&#21521;&#27969;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#36890;&#36807;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#26500;&#24314;&#19968;&#20010;&#23485;&#24230;&#20026;6N&#12289;&#28145;&#24230;&#20026;2L&#30340;varrho&#28608;&#27963;&#32593;&#32476;&#26469;&#36924;&#36817;&#19968;&#20010;&#23485;&#24230;&#20026;N&#12289;&#28145;&#24230;&#20026;L&#30340;ReLU&#32593;&#32476;&#65292;&#20174;&#32780;&#23558;&#23545;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#32467;&#26524;&#25512;&#24191;&#21040;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.06555</link><description>&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#36924;&#36817;&#65306;&#20174;ReLU&#21040;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Deep Network Approximation: Beyond ReLU to Diverse Activation Functions. (arXiv:2307.06555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#36890;&#36807;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#26500;&#24314;&#19968;&#20010;&#23485;&#24230;&#20026;6N&#12289;&#28145;&#24230;&#20026;2L&#30340;varrho&#28608;&#27963;&#32593;&#32476;&#26469;&#36924;&#36817;&#19968;&#20010;&#23485;&#24230;&#20026;N&#12289;&#28145;&#24230;&#20026;L&#30340;ReLU&#32593;&#32476;&#65292;&#20174;&#32780;&#23558;&#23545;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#32467;&#26524;&#25512;&#24191;&#21040;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23450;&#20041;&#20102;&#19968;&#20010;&#28608;&#27963;&#20989;&#25968;&#38598;&#21512;A&#65292;&#21253;&#25324;&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;ReLU&#12289;LeakyReLU&#12289;ReLU^2&#12289;ELU&#12289;SELU&#12289;Softplus&#12289;GELU&#12289;SiLU&#12289;Swish&#12289;Mish&#12289;Sigmoid&#12289;Tanh&#12289;Arctan&#12289;Softsign&#12289;dSiLU&#21644;SRS&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#24847;&#28608;&#27963;&#20989;&#25968;varrho&#8712;A&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#23485;&#24230;&#20026;6N&#12289;&#28145;&#24230;&#20026;2L&#30340;varrho&#28608;&#27963;&#32593;&#32476;&#22312;&#26377;&#30028;&#38598;&#21512;&#19978;&#20197;&#20219;&#24847;&#31934;&#24230;&#36924;&#36817;&#19968;&#20010;&#23485;&#24230;&#20026;N&#12289;&#28145;&#24230;&#20026;L&#30340;ReLU&#32593;&#32476;&#12290;&#36825;&#19968;&#21457;&#29616;&#20351;&#24471;&#22823;&#37096;&#20998;&#23545;&#20110;ReLU&#32593;&#32476;&#30340;&#36924;&#36817;&#32467;&#26524;&#33021;&#22815;&#25512;&#24191;&#21040;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#65292;&#23613;&#31649;&#38656;&#35201;&#31245;&#22823;&#30340;&#24120;&#25968;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$, $\mathtt{ELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$, $\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$, $\mathtt{Sigmoid}$, $\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$, $\mathtt{dSiLU}$, and $\mathtt{SRS}$. We demonstrate that for any activation function $\varrho\in \mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\varrho$-activated network of width $6N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\mathtt{ReLU}$ networks to a wide variety of other activation functions, at the cost of slightly larger constants.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.13596</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#36793;&#32536;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#20063;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#24778;&#20154;&#25104;&#21151;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#32972;&#21518;&#30340;&#29702;&#35770;&#21407;&#21017;&#23578;&#19981;&#28165;&#26970;&#65292;&#29305;&#21035;&#26159;&#23427;&#30340;&#38750;&#20984;&#20248;&#21270;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21019;&#24615;&#30340;softmax-attention&#27169;&#22411;$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$&#65292;&#20854;&#20013;$\boldsymbol{X}$&#26159;&#26631;&#35760;&#24207;&#21015;&#65292;$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$&#26159;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$\boldsymbol{p}$&#25110;&#31561;&#20215;&#30340;$\boldsymbol{W}$&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#20250;&#27839;&#30528;&#26041;&#21521;&#25910;&#25947;&#21040;&#20998;&#38548;&#8220;&#23616;&#37096;&#26368;&#20248;&#8221;&#26631;&#35760;&#21644;&#8220;&#38750;&#26368;&#20248;&#8221;&#26631;&#35760;&#30340;&#26368;&#22823;&#36793;&#32536;&#35299;&#12290;&#36825;&#26126;&#30830;&#22320;&#24418;&#24335;&#21270;&#20102;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#31181;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19968;&#33324;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#23884;&#20837;$\boldsymbol{Xv}$&#21644;$\texttt{softmax}(\boldsymbol{XWp})$&#31934;&#32454;&#22320;&#34920;&#24449;&#26631;&#35760;&#30340;&#8220;&#26368;&#20248;&#24615;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#23545;&#20110;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#37117;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#31639;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#35797;&#22270;&#36319;&#36394;&#22522;&#20110;&#19979;&#30028;&#35745;&#31639;&#21644;&#36890;&#36807;&#23545;&#27491;&#24120;&#38181;&#20307;&#36793;&#30028;&#36827;&#34892;&#21152;&#26435;&#25237;&#24433;&#25152;&#33719;&#24471;&#30340;&#26368;&#20248;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2306.12774</link><description>&lt;p&gt;
&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#32431;&#25506;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pure Exploration in Bandits with Linear Constraints. (arXiv:2306.12774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#23545;&#20110;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#37117;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#31639;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#35797;&#22270;&#36319;&#36394;&#22522;&#20110;&#19979;&#30028;&#35745;&#31639;&#21644;&#36890;&#36807;&#23545;&#27491;&#24120;&#38181;&#20307;&#36793;&#30028;&#36827;&#34892;&#21152;&#26435;&#25237;&#24433;&#25152;&#33719;&#24471;&#30340;&#26368;&#20248;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#23384;&#22312;&#32447;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#19968;&#23450;&#32622;&#20449;&#24230;&#19979;&#30830;&#23450;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#19982;&#26631;&#20934;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#19981;&#21516;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#31574;&#30053;&#21487;&#33021;&#19981;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#32780;&#26159;&#21487;&#33021;&#22312;&#22810;&#20010;&#33218;&#20043;&#38388;&#36827;&#34892;&#28151;&#21512;&#12290;&#36825;&#31181;&#24773;&#20917;&#25913;&#21464;&#20102;&#38382;&#39064;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#25105;&#20204;&#36890;&#36807;&#20449;&#24687;&#35770;&#19979;&#30028;&#36827;&#34892;&#20102;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#23545;&#20110;&#27492;&#35774;&#32622;&#37117;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#22522;&#20110;&#8220;&#36319;&#36394;&#20572;&#27490;&#8221;&#26041;&#27861;&#65292;&#21478;&#19968;&#20010;&#22522;&#20110;&#21338;&#24328;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#35797;&#22270;&#36319;&#36394;&#22522;&#20110;&#19979;&#30028;&#35745;&#31639;&#21644;&#36890;&#36807;&#23545;&#27491;&#24120;&#38181;&#20307;&#36793;&#30028;&#36827;&#34892;&#21152;&#26435;&#25237;&#24433;&#25152;&#33719;&#24471;&#30340;&#26368;&#20248;&#20998;&#37197;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#32467;&#26524;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#32422;&#26463;&#22914;&#20309;&#25913;&#21464;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of identifying the optimal policy with a fixed confidence level in a multi-armed bandit setup, when \emph{the arms are subject to linear constraints}. Unlike the standard best-arm identification problem which is well studied, the optimal policy in this case may not be deterministic and could mix between several arms. This changes the geometry of the problem which we characterize via an information-theoretic lower bound. We introduce two asymptotically optimal algorithms for this setting, one based on the Track-and-Stop method and the other based on a game-theoretic approach. Both these algorithms try to track an optimal allocation based on the lower bound and computed by a weighted projection onto the boundary of a normal cone. Finally, we provide empirical results that validate our bounds and visualize how constraints change the hardness of the problem.
&lt;/p&gt;</description></item><item><title>Blended-NeRF&#26159;&#19968;&#31181;&#40065;&#26834;&#32780;&#28789;&#27963;&#30340;&#32534;&#36753;NeRF&#22330;&#26223;&#20013;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#30340;&#26694;&#26550;&#65292;&#22312;&#20445;&#25345;&#33258;&#28982;&#24615;&#19982;&#19968;&#33268;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#21487;&#20197;&#23558;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#25110;&#22270;&#20687;&#34917;&#19969;&#30340;&#29289;&#20307;&#21512;&#25104;&#24182;&#28151;&#21512;&#21040;&#21407;&#22987;&#22330;&#26223;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.12760</link><description>&lt;p&gt;
&#28151;&#21512;&#24335;&#31070;&#32463;&#36752;&#23556;&#22330;&#65306;&#38646;&#26679;&#26412;&#29289;&#20307;&#29983;&#25104;&#19982;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields. (arXiv:2306.12760v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12760
&lt;/p&gt;
&lt;p&gt;
Blended-NeRF&#26159;&#19968;&#31181;&#40065;&#26834;&#32780;&#28789;&#27963;&#30340;&#32534;&#36753;NeRF&#22330;&#26223;&#20013;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#30340;&#26694;&#26550;&#65292;&#22312;&#20445;&#25345;&#33258;&#28982;&#24615;&#19982;&#19968;&#33268;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#21487;&#20197;&#23558;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#25110;&#22270;&#20687;&#34917;&#19969;&#30340;&#29289;&#20307;&#21512;&#25104;&#24182;&#28151;&#21512;&#21040;&#21407;&#22987;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;3D&#22330;&#26223;&#20013;&#32534;&#36753;&#23616;&#37096;&#21306;&#22495;&#25110;&#29305;&#23450;&#29289;&#20307;&#24182;&#28151;&#21512;&#21040;&#24050;&#26377;&#30340;NeRF&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#22330;&#26223;&#34920;&#31034;&#30340;&#38544;&#24335;&#23646;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Blended-NeRF&#65292;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#25110;&#22270;&#20687;&#34917;&#19969;&#20197;&#21450;3D ROI&#21253;&#22260;&#30418;&#30340;NeRF&#22330;&#26223;&#20013;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#30340;&#32534;&#36753;&#30340;&#40065;&#26834;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;-&#22270;&#20687;&#27169;&#22411;&#26469;&#24341;&#23548;&#21512;&#25104;&#26397;&#21521;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#25110;&#22270;&#20687;&#34917;&#19969;&#65292;&#36824;&#21033;&#29992;&#19968;&#20010;&#24050;&#23384;&#22312;&#30340;NeRF&#22330;&#26223;&#19978;&#21021;&#22987;&#21270;&#30340;3D MLP&#27169;&#22411;&#26469;&#29983;&#25104;&#29289;&#20307;&#24182;&#23558;&#20854;&#28151;&#21512;&#21040;&#21407;&#22987;&#22330;&#26223;&#20013;&#30340;&#25351;&#23450;&#21306;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;3D ROI&#30418;&#23616;&#37096;&#21270;&#20197;&#23454;&#29616;&#23616;&#37096;&#32534;&#36753;&#65292;&#24182;&#21033;&#29992;&#26032;&#39062;&#30340;&#20307;&#31215;&#28151;&#21512;&#25216;&#26415;&#23558;&#20869;&#37096;&#21512;&#25104;&#20869;&#23481;&#26080;&#32541;&#28151;&#21512;&#21040;&#29616;&#26377;&#22330;&#26223;&#20013;&#65292;&#20197;&#33719;&#24471;&#33258;&#28982;&#32780;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Editing a local region or a specific object in a 3D scene represented by a NeRF is challenging, mainly due to the implicit nature of the scene representation. Consistently blending a new realistic object into the scene adds an additional level of difficulty. We present Blended-NeRF, a robust and flexible framework for editing a specific region of interest in an existing NeRF scene, based on text prompts or image patches, along with a 3D ROI box. Our method leverages a pretrained language-image model to steer the synthesis towards a user-provided text prompt or image patch, along with a 3D MLP model initialized on an existing NeRF scene to generate the object and blend it into a specified region in the original scene. We allow local editing by localizing a 3D ROI box in the input scene, and seamlessly blend the content synthesized inside the ROI with the existing scene using a novel volumetric blending technique. To obtain natural looking and view-consistent results, we leverage existin
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#29289;&#29702;&#20223;&#30495;&#22120;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;5.7&#20159;&#20010;&#22810;&#21464;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#30690;&#37327;&#23545;&#65292;&#29992;&#20110;&#38548;&#31163;&#26412;&#22320;&#23884;&#22871;&#30340;&#39640;&#20998;&#36776;&#29575;&#12289;&#39640;&#20445;&#30495;&#24230;&#29289;&#29702;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.08754</link><description>&lt;p&gt;
ClimSim&#65306;&#29992;&#20110;&#22312;&#28151;&#21512;&#22810;&#23610;&#24230;&#27668;&#20505;&#27169;&#25311;&#22120;&#20013;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#29289;&#29702;&#20223;&#30495;&#22120;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ClimSim: An open large-scale dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators. (arXiv:2306.08754v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08754
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#29289;&#29702;&#20223;&#30495;&#22120;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;5.7&#20159;&#20010;&#22810;&#21464;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#30690;&#37327;&#23545;&#65292;&#29992;&#20110;&#38548;&#31163;&#26412;&#22320;&#23884;&#22871;&#30340;&#39640;&#20998;&#36776;&#29575;&#12289;&#39640;&#20445;&#30495;&#24230;&#29289;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#27668;&#20505;&#39044;&#27979;&#30001;&#20110;&#35745;&#31639;&#38480;&#21046;&#32570;&#20047;&#36275;&#22815;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#19968;&#20010;&#21518;&#26524;&#26159;&#23545;&#20851;&#38190;&#36807;&#31243;&#65288;&#22914;&#26292;&#39118;&#38632;&#65289;&#30340;&#39044;&#27979;&#19981;&#20934;&#30830;&#21644;&#19981;&#31934;&#30830;&#12290;&#23558;&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#27169;&#24335;&#24341;&#20837;&#20102;&#26032;&#19968;&#20195;&#26356;&#39640;&#20445;&#30495;&#24230;&#30340;&#27668;&#20505;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#23494;&#38598;&#22411;&#12289;&#30701;&#12289;&#39640;&#20998;&#36776;&#29575;&#30340;&#27169;&#25311;&#22996;&#25176;&#32473;ML&#20223;&#30495;&#22120;&#65292;&#21487;&#20197;&#36991;&#20813;&#25705;&#23572;&#23450;&#24459;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#28151;&#21512;&#30340;ML-&#29289;&#29702;&#20223;&#30495;&#26041;&#27861;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30340;&#22788;&#29702;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#22521;&#35757;&#25968;&#25454;&#21644;&#30456;&#20851;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#19968;&#30452;&#26080;&#27861;&#35775;&#38382;ML&#19987;&#23478;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; ClimSim&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#20026;&#28151;&#21512;ML-&#29289;&#29702;&#30740;&#31350;&#32780;&#35774;&#35745;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#12290;&#23427;&#30001;&#27668;&#20505;&#31185;&#23398;&#23478;&#21644;ML&#30740;&#31350;&#20154;&#21592;&#32852;&#21512;&#24320;&#21457;&#30340;&#22810;&#23610;&#24230;&#27668;&#20505;&#27169;&#25311;&#32452;&#25104;&#65292;&#21253;&#25324;57&#20159;&#20010;&#22810;&#21464;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#30690;&#37327;&#23545;&#65292;&#38548;&#31163;&#20102;&#26412;&#22320;&#23884;&#22871;&#30340;&#39640;&#20998;&#36776;&#29575;&#21644;&#39640;&#20445;&#30495;&#24230;&#29289;&#29702;&#23398;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise prediction of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore's Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset designed for hybrid ML-physics research. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#21452;&#38750;&#22343;&#21248;&#29615;&#22659;&#19979;&#30740;&#31350;&#20102;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;(OPE)&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#29992;&#20110;&#22870;&#21169;&#21644;&#35266;&#27979;&#36716;&#31227;&#20989;&#25968;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;OPE&#26694;&#26550;&#12290;&#35813;&#30740;&#31350;&#23545;&#26631;&#20934;RL&#20551;&#35774;&#19981;&#28385;&#36275;&#30340;&#29615;&#22659;&#20013;&#30340;OPE&#20570;&#20986;&#20102;&#29702;&#35770;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08719</link><description>&lt;p&gt;
&#21452;&#38750;&#22343;&#21248;&#29615;&#22659;&#19979;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-policy Evaluation in Doubly Inhomogeneous Environments. (arXiv:2306.08719v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#21452;&#38750;&#22343;&#21248;&#29615;&#22659;&#19979;&#30740;&#31350;&#20102;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;(OPE)&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#29992;&#20110;&#22870;&#21169;&#21644;&#35266;&#27979;&#36716;&#31227;&#20989;&#25968;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;OPE&#26694;&#26550;&#12290;&#35813;&#30740;&#31350;&#23545;&#26631;&#20934;RL&#20551;&#35774;&#19981;&#28385;&#36275;&#30340;&#29615;&#22659;&#20013;&#30340;OPE&#20570;&#20986;&#20102;&#29702;&#35770;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#22312;&#20004;&#20010;&#20851;&#38190;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20551;&#35774;&#8212;&#8212;&#26102;&#38388;&#31283;&#23450;&#24615;&#21644;&#20010;&#20307;&#22343;&#21248;&#24615;&#22343;&#34987;&#30772;&#22351;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#22788;&#29702;&#8220;&#21452;&#38750;&#22343;&#21248;&#24615;&#8221;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#29992;&#20110;&#22870;&#21169;&#21644;&#35266;&#27979;&#36716;&#31227;&#20989;&#25968;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#27169;&#22411;&#39537;&#21160;&#21644;&#27169;&#22411;&#33258;&#30001;&#26041;&#27861;&#30340;&#36890;&#29992;OPE&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22312;&#31163;&#32447;RL&#20013;&#24320;&#21457;&#32479;&#35745;&#19978;&#21487;&#38752;&#30340;OPE&#26041;&#27861;&#30340;&#35770;&#25991;&#65292;&#24182;&#19988;&#28041;&#21450;&#20102;&#26631;&#20934;RL&#20551;&#35774;&#19981;&#28385;&#36275;&#30340;&#29615;&#22659;&#12290;&#35813;&#30740;&#31350;&#28145;&#20837;&#29702;&#35299;&#20102;&#26631;&#20934;RL&#20551;&#35774;&#19981;&#28385;&#36275;&#30340;&#29615;&#22659;&#20013;&#30340;OPE&#65292;&#24182;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#20960;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#25152;&#25552;&#20986;&#30340;&#20215;&#20540;&#20272;&#35745;&#22120;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24573;&#35270;&#26102;&#38388;&#38750;&#31283;&#23450;&#24615;&#25110;&#20010;&#20307;&#24322;&#36136;&#24615;&#30340;&#31454;&#20105;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims to study off-policy evaluation (OPE) under scenarios where two key reinforcement learning (RL) assumptions -- temporal stationarity and individual homogeneity are both violated. To handle the ``double inhomogeneities", we propose a class of latent factor models for the reward and observation transition functions, under which we develop a general OPE framework that consists of both model-based and model-free approaches. To our knowledge, this is the first paper that develops statistically sound OPE methods in offline RL with double inhomogeneities. It contributes to a deeper understanding of OPE in environments, where standard RL assumptions are not met, and provides several practical approaches in these settings. We establish the theoretical properties of the proposed value estimators and empirically show that our approach outperforms competing methods that ignore either temporal nonstationarity or individual heterogeneity. Finally, we illustrate our method on a data set
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22240;&#26524;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#26102;&#38388;&#21464;&#21270;&#30340;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#29983;&#25104;&#20132;&#36890;&#39044;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#20132;&#36890;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#20256;&#25773;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.07019</link><description>&lt;p&gt;
&#21160;&#24577;&#22240;&#26524;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic Causal Graph Convolutional Network for Traffic Prediction. (arXiv:2306.07019v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22240;&#26524;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#26102;&#38388;&#21464;&#21270;&#30340;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#29983;&#25104;&#20132;&#36890;&#39044;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#20132;&#36890;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#20256;&#25773;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#30456;&#20851;&#20132;&#36890;&#24207;&#21015;&#20013;&#30340;&#22797;&#26434;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#23545;&#20110;&#20132;&#36890;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#21462;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#24050;&#32463;&#23637;&#31034;&#20102;&#39044;&#27979;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#29992;&#20110;&#34920;&#31034;&#20132;&#36890;&#32593;&#32476;&#30340;&#31354;&#38388;&#25299;&#25169;&#30340;&#22270;&#32467;&#26500;&#30340;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#21464;&#21270;&#30340;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#23884;&#20837;&#21040;&#20132;&#36890;&#25968;&#25454;&#30340;&#32454;&#31890;&#24230;&#26102;&#31354;&#25299;&#25169;&#20013;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#29983;&#25104;&#20132;&#36890;&#39044;&#27979;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#27169;&#25311;&#38750;&#32447;&#24615;&#20132;&#36890;&#20256;&#25773;&#27169;&#24335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22359;&#20316;&#20026;&#36229;&#32593;&#32476;&#26469;&#29983;&#25104;&#36880;&#27493;&#21160;&#24577;&#22240;&#26524;&#22270;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#30340;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#20248;&#36234;&#39044;&#27979;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/MonBG/DCGCN &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling complex spatiotemporal dependencies in correlated traffic series is essential for traffic prediction. While recent works have shown improved prediction performance by using neural networks to extract spatiotemporal correlations, their effectiveness depends on the quality of the graph structures used to represent the spatial topology of the traffic network. In this work, we propose a novel approach for traffic prediction that embeds time-varying dynamic Bayesian network to capture the fine spatiotemporal topology of traffic data. We then use graph convolutional networks to generate traffic forecasts. To enable our method to efficiently model nonlinear traffic propagation patterns, we develop a deep learning-based module as a hyper-network to generate stepwise dynamic causal graphs. Our experimental results on a real traffic dataset demonstrate the superior prediction performance of the proposed method. The code is available at https://github.com/MonBG/DCGCN.
&lt;/p&gt;</description></item><item><title>LDMRes-Net&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#39640;&#25928;&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#37319;&#29992;&#21452;&#37325;&#22810;&#37325;&#27531;&#24046;&#36830;&#25509;&#26469;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#24182;&#26368;&#23567;&#21270;&#35745;&#31639;&#25104;&#26412;&#65292;&#21487;&#29992;&#20110;&#23454;&#26102;&#35270;&#32593;&#33180;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#65292;&#24182;&#22312;8&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06145</link><description>&lt;p&gt;
LDMRes-Net&#65306;&#36890;&#36807;&#39640;&#25928;&#22270;&#20687;&#20998;&#21106;&#23454;&#29616;&#23454;&#26102;&#30142;&#30149;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
LDMRes-Net: Enabling Real-Time Disease Monitoring through Efficient Image Segmentation. (arXiv:2306.06145v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06145
&lt;/p&gt;
&lt;p&gt;
LDMRes-Net&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#39640;&#25928;&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#37319;&#29992;&#21452;&#37325;&#22810;&#37325;&#27531;&#24046;&#36830;&#25509;&#26469;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#24182;&#26368;&#23567;&#21270;&#35745;&#31639;&#25104;&#26412;&#65292;&#21487;&#29992;&#20110;&#23454;&#26102;&#35270;&#32593;&#33180;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#65292;&#24182;&#22312;8&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#32593;&#33180;&#30524;&#30149;&#22914;&#26524;&#19981;&#21450;&#26089;&#35786;&#26029;&#21644;&#27835;&#30103;&#65292;&#21487;&#33021;&#23548;&#33268;&#21452;&#30524;&#19981;&#21487;&#36870;&#30340;&#35270;&#21147;&#25439;&#22833;&#12290;&#30001;&#20110;&#35270;&#32593;&#33180;&#30142;&#30149;&#30340;&#22797;&#26434;&#24615;&#65292;&#35270;&#32593;&#33180;&#22270;&#20687;&#21487;&#33021;&#21253;&#21547;&#20004;&#20010;&#25110;&#26356;&#22810;&#24322;&#24120;&#12290;&#30446;&#21069;&#29992;&#20110;&#20998;&#21106;&#24102;&#26377;&#22810;&#20010;&#26631;&#31614;&#21644;&#29305;&#24449;&#30340;&#35270;&#32593;&#33180;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#23384;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#19981;&#36275;&#21644;&#27867;&#21270;&#24615;&#19981;&#24378;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#39640;&#25928;&#30340;&#32593;&#32476;&#65292;&#37319;&#29992;&#21452;&#37325;&#22810;&#37325;&#27531;&#24046;&#36830;&#25509;&#22686;&#24378;&#20998;&#21106;&#24615;&#33021;&#24182;&#26368;&#23567;&#21270;&#35745;&#31639;&#25104;&#26412;&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;8&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#35270;&#32593;&#33180;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#23545;&#20110;&#35270;&#32593;&#33180;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;&#39640;&#25928;&#35774;&#35745;&#20351;&#20854;&#25104;&#20026;&#23454;&#26102;&#35270;&#32593;&#33180;&#22270;&#20687;&#20998;&#26512;&#24212;&#29992;&#30340;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retinal eye diseases can lead to irreversible vision loss in both eyes if not diagnosed and treated earlier. Owing to the complexities of retinal diseases, the likelihood that retinal images would contain two or more abnormalities is very high. The current deep learning algorithms used for segmenting retinal images with multiple labels and features suffer from inadequate detection accuracy and a lack of generalizability. In this paper, we propose a lightweight and efficient network, featuring dual multi-residual connections to enhance segmentation performance while minimizing computational cost. The proposed network is evaluated on eight publicly available retinal image datasets and achieved promising segmentation results, which demonstrate the effectiveness of the proposed network for retinal image analysis tasks. The proposed network's lightweight and efficient design makes it a promising candidate for real-time retinal image analysis applications.
&lt;/p&gt;</description></item><item><title>&#8220;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#8221;&#25552;&#20986;&#19968;&#31181;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#26080;&#27861;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#23545;&#20445;&#25252;&#21518;&#30340;&#25968;&#25454;&#36827;&#34892;&#21033;&#29992;&#12290;&#36890;&#36807;&#25552;&#20986;&#8220;&#21487;&#23398;&#20064;&#30340;&#26410;&#32463;&#25480;&#26435;&#31034;&#20363;&#8221;&#21644;&#19968;&#31181;&#26032;&#30340;&#32431;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#26356;&#22909;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2305.09241</link><description>&lt;p&gt;
&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#32473;&#20986;&#20102;&#19968;&#31181;&#34394;&#20551;&#30340;&#23433;&#20840;&#24863;&#65306;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20363;&#23376;&#31359;&#36879;&#37027;&#20123;&#26080;&#27861;&#21033;&#29992;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples. (arXiv:2305.09241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09241
&lt;/p&gt;
&lt;p&gt;
&#8220;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#8221;&#25552;&#20986;&#19968;&#31181;&#23545;&#25968;&#25454;&#36827;&#34892;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#26080;&#27861;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#23545;&#20445;&#25252;&#21518;&#30340;&#25968;&#25454;&#36827;&#34892;&#21033;&#29992;&#12290;&#36890;&#36807;&#25552;&#20986;&#8220;&#21487;&#23398;&#20064;&#30340;&#26410;&#32463;&#25480;&#26435;&#31034;&#20363;&#8221;&#21644;&#19968;&#31181;&#26032;&#30340;&#32431;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#26356;&#22909;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#19979;&#38543;&#22788;&#21487;&#35265;&#30340;&#23433;&#20840;&#28431;&#27934;&#20013;&#65292;&#20445;&#25252;&#25968;&#25454;&#20813;&#20110;&#26410;&#32463;&#25480;&#26435;&#30340;&#21033;&#29992;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21483;&#20570;&#8220;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#8221;&#65288;UEs&#65289;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#24494;&#23567;&#30340;&#25200;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#22312;&#21407;&#22987;&#30340;&#24178;&#20928;&#20998;&#24067;&#19978;&#20934;&#30830;&#22320;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#20445;&#25252;&#25514;&#26045;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616; UEs &#24102;&#26469;&#30340;&#23433;&#20840;&#23041;&#32961;&#26159;&#34394;&#20551;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#38459;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#21033;&#29992;&#20854;&#20182;&#26410;&#21463;&#20445;&#25252;&#30340;&#25968;&#25454;&#26469;&#21435;&#38500;&#20445;&#25252;&#65292;&#23558;&#26080;&#27861;&#23398;&#20064;&#30340;&#25968;&#25454;&#37325;&#36716;&#20026;&#21487;&#23398;&#20064;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#31181;&#23041;&#32961;&#65292;&#24341;&#20837;&#20102;&#8220;&#21487;&#23398;&#20064;&#30340;&#26410;&#32463;&#25480;&#26435;&#31034;&#20363;&#8221;&#65288;LEs&#65289;&#65292;&#36825;&#20123;&#26159;&#24050;&#32463;&#21435;&#38500;&#20445;&#25252;&#30340;UEs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32431;&#21270;&#36807;&#31243;&#65292;&#23558;UEs&#25237;&#23556;&#21040;LEs&#30340;&#27969;&#24418;&#19978;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#30340;&#65292;&#35813;&#27169;&#22411;&#23545;UEs&#36827;&#34892;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safeguarding data from unauthorized exploitation is vital for privacy and security, especially in recent rampant research in security breach such as adversarial/membership attacks. To this end, \textit{unlearnable examples} (UEs) have been recently proposed as a compelling protection, by adding imperceptible perturbation to data so that models trained on them cannot classify them accurately on original clean distribution. Unfortunately, we find UEs provide a false sense of security, because they cannot stop unauthorized users from utilizing other unprotected data to remove the protection, by turning unlearnable data into learnable again. Motivated by this observation, we formally define a new threat by introducing \textit{learnable unauthorized examples} (LEs) which are UEs with their protection removed. The core of this approach is a novel purification process that projects UEs onto the manifold of LEs. This is realized by a new joint-conditional diffusion model which denoises UEs con
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#29983;&#25104;&#22810;&#31181;&#39118;&#26684;&#21644;&#35270;&#35282;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#36827;&#19968;&#27493;&#24494;&#35843;&#39592;&#24178;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10226</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#20083;&#33146;X&#32447;&#25668;&#24433;&#22270;&#20687;&#20998;&#26512;&#30340;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization for Mammographic Image Analysis via Contrastive Learning. (arXiv:2304.10226v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10226
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#29983;&#25104;&#22810;&#31181;&#39118;&#26684;&#21644;&#35270;&#35282;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#36827;&#19968;&#27493;&#24494;&#35843;&#39592;&#24178;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;X&#32447;&#25668;&#24433;&#22270;&#20687;&#20998;&#26512;&#26159;&#21307;&#23398;&#24433;&#20687;&#23398;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#20855;&#26377;&#22810;&#26679;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19981;&#21516;&#21378;&#21830;&#30340;&#22270;&#20687;&#39118;&#26684;&#65292;&#36825;&#24448;&#24448;&#38656;&#35201;&#38750;&#24120;&#24222;&#22823;&#30340;&#26679;&#26412;&#38598;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#21040;&#19981;&#21516;&#21378;&#21830;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mammographic image analysis is a fundamental problem in the computer-aided diagnosis scheme, which has recently made remarkable progress with the advance of deep learning. However, the construction of a deep learning model requires training data that are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, mammogram collection from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning models to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor styles. Afterward, the backbone network is then recalibrated to the downstream tasks of mas
&lt;/p&gt;</description></item><item><title>DiFaReli&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#35299;&#30721;&#35299;&#32806;&#30340;&#20809;&#32534;&#30721;&#20197;&#21450;&#20174;&#29616;&#25104;&#30340;&#20272;&#31639;&#22120;&#25512;&#26029;&#20986;&#30340;&#19982;3D&#24418;&#29366;&#21644;&#38754;&#37096;&#36523;&#20221;&#30456;&#20851;&#30340;&#20854;&#20182;&#32534;&#30721;&#65292;&#33021;&#22815;&#22788;&#29702;&#21333;&#35270;&#35282;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;&#20154;&#33080;&#37325;&#29031;&#65292;&#26080;&#38656;&#20809;&#32447;&#33310;&#21488;&#25968;&#25454;&#12289;&#22810;&#35270;&#22270;&#22270;&#20687;&#25110;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09479</link><description>&lt;p&gt;
DiFaReli: &#25193;&#25955;&#20154;&#33080;&#37325;&#29031;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
DiFaReli : Diffusion Face Relighting. (arXiv:2304.09479v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09479
&lt;/p&gt;
&lt;p&gt;
DiFaReli&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#35299;&#30721;&#35299;&#32806;&#30340;&#20809;&#32534;&#30721;&#20197;&#21450;&#20174;&#29616;&#25104;&#30340;&#20272;&#31639;&#22120;&#25512;&#26029;&#20986;&#30340;&#19982;3D&#24418;&#29366;&#21644;&#38754;&#37096;&#36523;&#20221;&#30456;&#20851;&#30340;&#20854;&#20182;&#32534;&#30721;&#65292;&#33021;&#22815;&#22788;&#29702;&#21333;&#35270;&#35282;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;&#20154;&#33080;&#37325;&#29031;&#65292;&#26080;&#38656;&#20809;&#32447;&#33310;&#21488;&#25968;&#25454;&#12289;&#22810;&#35270;&#22270;&#22270;&#20687;&#25110;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;&#21333;&#35270;&#35282;&#20154;&#33080;&#37325;&#29031;&#12290;&#22788;&#29702;&#20840;&#23616;&#29031;&#26126;&#25110;&#25237;&#24433;&#38452;&#24433;&#31561;&#38750;&#28459;&#21453;&#23556;&#25928;&#24212;&#19968;&#30452;&#26159;&#20154;&#33080;&#37325;&#29031;&#39046;&#22495;&#30340;&#38590;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#23450;&#20848;&#20271;&#29305;&#21453;&#23556;&#34920;&#38754;&#65292;&#31616;&#21270;&#20809;&#29031;&#27169;&#22411;&#65292;&#25110;&#32773;&#38656;&#35201;&#20272;&#35745;&#19977;&#32500;&#24418;&#29366;&#12289;&#21453;&#23556;&#29575;&#25110;&#38452;&#24433;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20272;&#35745;&#26159;&#23481;&#26131;&#20986;&#38169;&#30340;&#65292;&#38656;&#35201;&#35768;&#22810;&#20855;&#26377;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#30340;&#35757;&#32451;&#26679;&#26412;&#25165;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32469;&#36807;&#20102;&#20934;&#30830;&#20272;&#35745;&#22266;&#26377;&#32452;&#20214;&#30340;&#38656;&#35201;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;2D&#22270;&#20687;&#35757;&#32451;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#20809;&#32447;&#33310;&#21488;&#25968;&#25454;&#12289;&#22810;&#35270;&#22270;&#22270;&#20687;&#25110;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#65288;DDIM&#65289;&#35299;&#30721;&#35299;&#32806;&#30340;&#20809;&#32534;&#30721;&#20197;&#21450;&#20174;&#29616;&#25104;&#30340;&#20272;&#31639;&#22120;&#25512;&#26029;&#20986;&#30340;&#19982;3D&#24418;&#29366;&#21644;&#38754;&#37096;&#36523;&#20221;&#30456;&#20851;&#30340;&#20854;&#20182;&#32534;&#30721;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#33410;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#31616;&#21270;&#20809;&#19982;&#20960;&#20309;&#20043;&#38388;&#22797;&#26434;&#20114;&#21160;&#30340;&#24314;&#27169;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to single-view face relighting in the wild. Handling non-diffuse effects, such as global illumination or cast shadows, has long been a challenge in face relighting. Prior work often assumes Lambertian surfaces, simplified lighting models or involves estimating 3D shape, albedo, or a shadow map. This estimation, however, is error-prone and requires many training examples with lighting ground truth to generalize well. Our work bypasses the need for accurate estimation of intrinsic components and can be trained solely on 2D images without any light stage data, multi-view images, or lighting ground truth. Our key idea is to leverage a conditional diffusion implicit model (DDIM) for decoding a disentangled light encoding along with other encodings related to 3D shape and facial identity inferred from off-the-shelf estimators. We also propose a novel conditioning technique that eases the modeling of the complex interaction between light and geometry by using a ren
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#32422;&#26463;&#38381;&#29615;&#25511;&#21046;&#31995;&#32479;&#30340;&#22312;&#32447;&#24615;&#33021;&#20248;&#21270;&#65292;&#21516;&#26102;&#28385;&#36275;&#25152;&#38656;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.06104</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24102;&#26102;&#38388;&#24179;&#22343;&#32422;&#26463;&#30340;&#25511;&#21046;&#31995;&#32479;&#22312;&#32447;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Primal-Dual Contextual Bayesian Optimization for Control System Online Optimization with Time-Average Constraints. (arXiv:2304.06104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06104
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#32422;&#26463;&#38381;&#29615;&#25511;&#21046;&#31995;&#32479;&#30340;&#22312;&#32447;&#24615;&#33021;&#20248;&#21270;&#65292;&#21516;&#26102;&#28385;&#36275;&#25152;&#38656;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24102;&#26377;&#22806;&#29983;&#26102;&#38388;&#21464;&#21270;&#19978;&#19979;&#25991;&#24178;&#25200;&#30340;&#26410;&#30693;&#40657;&#30418;&#20989;&#25968;&#30340;&#32422;&#26463;&#38381;&#29615;&#25511;&#21046;&#31995;&#32479;&#22312;&#32447;&#24615;&#33021;&#20248;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22987;-&#23545;&#20598;&#35821;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#26368;&#20248;&#35299;&#30340;&#20122;&#32447;&#24615;&#32047;&#31215;&#36951;&#25022;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#38646;&#26102;&#38388;&#24179;&#22343;&#32422;&#26463;&#36829;&#35268;&#65292;&#30830;&#20445;&#20102;&#32422;&#26463;&#20989;&#25968;&#30340;&#24179;&#22343;&#20540;&#28385;&#36275;&#25152;&#38656;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#37319;&#26679;&#23454;&#20363;&#21644;&#36830;&#32493;&#25605;&#25292;&#27133;&#21453;&#24212;&#22120;&#21442;&#25968;&#35843;&#33410;&#38382;&#39064;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#21644;&#24179;&#22343;&#20445;&#25345;&#32422;&#26463;&#21487;&#34892;&#24615;&#65292;&#36825;&#19982;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#35201;&#20040;&#36973;&#21463;&#22823;&#37327;&#32047;&#31215;&#36951;&#25022;&#65292;&#35201;&#20040;&#23384;&#22312;&#20005;&#37325;&#32422;&#26463;&#36829;&#35268;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of online performance optimization of constrained closed-loop control systems, where both the objective and the constraints are unknown black-box functions affected by exogenous time-varying contextual disturbances. A primal-dual contextual Bayesian optimization algorithm is proposed that achieves sublinear cumulative regret with respect to the dynamic optimal solution under certain regularity conditions. Furthermore, the algorithm achieves zero time-average constraint violation, ensuring that the average value of the constraint function satisfies the desired constraint. The method is applied to both sampled instances from Gaussian processes and a continuous stirred tank reactor parameter tuning problem; simulation results show that the method simultaneously provides close-to-optimal performance and maintains constraint feasibility on average. This contrasts current state-of-the-art methods, which either suffer from large cumulative regret or severe const
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35775;&#35848;19&#20301;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23454;&#36341;&#32773;&#65292;&#21457;&#29616;KG&#26500;&#24314;&#32773;&#38656;&#27714;&#26550;&#26500;&#25191;&#34892;&#31243;&#24207;&#65292;KG&#20998;&#26512;&#24072;&#38656;&#35201;&#21487;&#33258;&#23450;&#20041;&#26597;&#35810;&#26500;&#24314;&#22120;&#65292;KG&#28040;&#36153;&#32773;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#21487;&#35270;&#21270;&#65292;&#24182;&#25351;&#20986;&#22312;&#23454;&#36341;&#20013;&#23454;&#26045;KG&#38656;&#35201;&#25216;&#26415;&#21644;&#31038;&#20132;&#26041;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.01311</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30693;&#35782;&#22270;&#35889;&#29992;&#25143;&#12289;&#25361;&#25112;&#21644;&#21487;&#35270;&#21270;&#38656;&#27714;&#30340;&#29305;&#24449;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Characterizing the Users, Challenges, and Visualization Needs of Knowledge Graphs in Practice. (arXiv:2304.01311v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35775;&#35848;19&#20301;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23454;&#36341;&#32773;&#65292;&#21457;&#29616;KG&#26500;&#24314;&#32773;&#38656;&#27714;&#26550;&#26500;&#25191;&#34892;&#31243;&#24207;&#65292;KG&#20998;&#26512;&#24072;&#38656;&#35201;&#21487;&#33258;&#23450;&#20041;&#26597;&#35810;&#26500;&#24314;&#22120;&#65292;KG&#28040;&#36153;&#32773;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#21487;&#35270;&#21270;&#65292;&#24182;&#25351;&#20986;&#22312;&#23454;&#36341;&#20013;&#23454;&#26045;KG&#38656;&#35201;&#25216;&#26415;&#21644;&#31038;&#20132;&#26041;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;19&#20301;&#26469;&#33258;&#20225;&#19994;&#21644;&#23398;&#26415;&#29615;&#22659;&#19979;&#12289;&#28041;&#21450;&#21508;&#31181;&#29992;&#20363;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23454;&#36341;&#32773;&#30340;&#35775;&#35848;&#65292;&#25552;&#20986;&#20102;KG&#23454;&#36341;&#32773;&#22312;&#21019;&#24314;&#12289;&#25506;&#32034;&#21644;&#20998;&#26512;KG&#26102;&#36935;&#21040;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#36825;&#20123;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#21487;&#35270;&#21270;&#35774;&#35745;&#26469;&#32531;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;KG&#23454;&#36341;&#32773;&#21487;&#20197;&#20998;&#20026;&#19977;&#31867;&#65306;KG&#26500;&#24314;&#32773;&#12289;&#20998;&#26512;&#24072;&#21644;&#28040;&#36153;&#32773;&#65292;&#27599;&#20010;&#20154;&#37117;&#26377;&#33258;&#24049;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#38656;&#27714;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;KG&#26500;&#24314;&#32773;&#21487;&#20197;&#20174;&#26550;&#26500;&#25191;&#34892;&#31243;&#24207;&#20013;&#33719;&#30410;&#65292;&#32780;KG&#20998;&#26512;&#24072;&#38656;&#35201;&#25552;&#20379;&#20013;&#38388;&#26597;&#35810;&#32467;&#26524;&#30340;&#21487;&#33258;&#23450;&#20041;&#26597;&#35810;&#26500;&#24314;&#22120;&#12290;&#23545;&#20110;KG&#28040;&#36153;&#32773;&#65292;&#25105;&#20204;&#30830;&#23450;&#33410;&#28857;&#38142;&#25509;&#22270;&#30340;&#25928;&#21147;&#19981;&#36275;&#65292;&#24182;&#38656;&#35201;&#23450;&#21046;&#30340;&#39046;&#22495;&#29305;&#23450;&#21487;&#35270;&#21270;&#26469;&#20419;&#36827;KG&#30340;&#37319;&#29992;&#21644;&#29702;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23454;&#36341;&#20013;&#26377;&#25928;&#22320;&#23454;&#26045;KG&#38656;&#35201;&#19981;&#20165;&#25216;&#26415;&#19978;&#30340;&#65292;&#36824;&#26377;&#31038;&#20132;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30446;&#21069;&#24182;&#26410;&#34987;&#24403;&#21069;&#30340;&#24037;&#20855;&#12289;&#25216;&#26415;&#21644;&#26368;&#20339;&#23454;&#36341;&#25152;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents insights from interviews with nineteen Knowledge Graph (KG) practitioners who work in both enterprise and academic settings on a wide variety of use cases. Through this study, we identify critical challenges experienced by KG practitioners when creating, exploring, and analyzing KGs that could be alleviated through visualization design. Our findings reveal three major personas among KG practitioners - KG Builders, Analysts, and Consumers - each of whom have their own distinct expertise and needs. We discover that KG Builders would benefit from schema enforcers, while KG Analysts need customizable query builders that provide interim query results. For KG Consumers, we identify a lack of efficacy for node-link diagrams, and the need for tailored domain-specific visualizations to promote KG adoption and comprehension. Lastly, we find that implementing KGs effectively in practice requires both technical and social solutions that are not addressed with current tools, tec
&lt;/p&gt;</description></item><item><title>FixFit&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22797;&#26434;&#38750;&#32447;&#24615;&#27169;&#22411;&#20013;&#30001;&#20110;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#30340;&#22810;&#21442;&#25968;&#38598;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13746</link><description>&lt;p&gt;
FixFit&#65306;&#20351;&#29992;&#21442;&#25968;&#21387;&#32553;&#35299;&#20915;&#36229;&#23450;&#27169;&#22411;&#20013;&#30340;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FixFit: using parameter-compression to solve the inverse problem in overdetermined models. (arXiv:2303.13746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13746
&lt;/p&gt;
&lt;p&gt;
FixFit&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22797;&#26434;&#38750;&#32447;&#24615;&#27169;&#22411;&#20013;&#30001;&#20110;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#30340;&#22810;&#21442;&#25968;&#38598;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#31185;&#23398;&#39046;&#22495;&#37117;&#20381;&#36182;&#20110;&#25968;&#23398;&#27169;&#22411;&#12290;&#20351;&#29992;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#65292;&#22522;&#20110;&#25968;&#25454;&#30340;&#21442;&#25968;&#20272;&#35745;&#32463;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#22810;&#20010;&#21442;&#25968;&#38598;&#21516;&#26679;&#36866;&#21512;&#25968;&#25454;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;FixFit&#65292;&#23558;&#32473;&#23450;&#25968;&#23398;&#27169;&#22411;&#30340;&#21442;&#25968;&#21387;&#32553;&#20026;&#29420;&#29305;&#20110;&#27169;&#22411;&#36755;&#20986;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27169;&#22411;&#21442;&#25968;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#25968;&#25454;&#23545;&#19978;&#35757;&#32451;&#24102;&#26377;&#29942;&#39048;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#33719;&#24471;&#27492;&#34920;&#31034;&#12290;&#29942;&#39048;&#23618;&#33410;&#28857;&#23545;&#24212;&#20110;&#21807;&#19968;&#30340;&#28508;&#22312;&#21442;&#25968;&#65292;&#20854;&#32500;&#24230;&#34920;&#31034;&#27169;&#22411;&#30340;&#20449;&#24687;&#20869;&#23481;&#12290;&#35757;&#32451;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#29942;&#39048;&#23618;&#20998;&#35010;&#25104;&#32534;&#30721;&#22120;&#26469;&#25551;&#36848;&#20887;&#20313;&#20449;&#24687;&#21644;&#35299;&#30721;&#22120;&#26469;&#20174;&#27979;&#37327;&#20013;&#21807;&#19968;&#25512;&#26029;&#28508;&#22312;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#32463;&#20856;&#29289;&#29702;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#20004;&#20010;&#24212;&#29992;&#26696;&#20363;&#20013;&#23637;&#31034;&#20102;FixFit&#12290;
&lt;/p&gt;
&lt;p&gt;
All fields of science depend on mathematical models. One of the fundamental problems with using complex nonlinear models is that data-driven parameter estimation often fails because interactions between model parameters lead to multiple parameter sets fitting the data equally well. Here, we develop a new method to address this problem, FixFit, which compresses a given mathematical model's parameters into a latent representation unique to model outputs. We acquire this representation by training a neural network with a bottleneck layer on data pairs of model parameters and model outputs. The bottleneck layer nodes correspond to the unique latent parameters, and their dimensionality indicates the information content of the model. The trained neural network can be split at the bottleneck layer into an encoder to characterize the redundancies and a decoder to uniquely infer latent parameters from measurements. We demonstrate FixFit in two use cases drawn from classical physics and neurosci
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#35299;&#37322;&#29305;&#24449;&#30340;&#36716;&#31227;&#24615;&#36136;&#26469;&#26816;&#27979;&#20998;&#24067;&#36716;&#31227;&#19979;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#26159;&#21542;&#36234;&#30028;&#65292;&#22312;&#27604;&#36739;&#20013;&#21457;&#29616;&#20854;&#27604;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#26356;&#20026;&#20248;&#31168;&#65292;&#25552;&#20379;&#20102;&#31639;&#27861;&#26041;&#27861;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.08081</link><description>&lt;p&gt;
&#35299;&#37322;&#20301;&#31227;&#65306;&#30740;&#31350;&#27169;&#22411;&#19982;&#36716;&#31227;&#25968;&#25454;&#20998;&#24067;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanation Shift: Investigating Interactions between Models and Shifting Data Distributions. (arXiv:2303.08081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#35299;&#37322;&#29305;&#24449;&#30340;&#36716;&#31227;&#24615;&#36136;&#26469;&#26816;&#27979;&#20998;&#24067;&#36716;&#31227;&#19979;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#26159;&#21542;&#36234;&#30028;&#65292;&#22312;&#27604;&#36739;&#20013;&#21457;&#29616;&#20854;&#27604;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#26356;&#20026;&#20248;&#31168;&#65292;&#25552;&#20379;&#20102;&#31639;&#27861;&#26041;&#27861;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#24448;&#24448;&#20250;&#19979;&#38477;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#26032;&#30340;&#36755;&#20837;&#25968;&#25454;&#24448;&#24448;&#27809;&#26377;&#30446;&#26631;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#27169;&#22411;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#25110;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#35797;&#22270;&#29702;&#35299;&#23398;&#20064;&#27169;&#22411;&#21644;&#36716;&#31227;&#20998;&#24067;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#27169;&#22411;&#22914;&#20309;&#35299;&#37322;&#29305;&#24449;&#30340;&#36716;&#31227;&#24615;&#36136;&#21463;&#21040;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35299;&#37322;&#20301;&#31227;&#30340;&#24314;&#27169;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#26356;&#22909;&#22320;&#25351;&#31034;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;&#21512;&#25104;&#31034;&#20363;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#26041;&#27861;&#65292;&#20801;&#35768;&#25105;&#20204;&#26816;&#26597;&#25968;&#25454;&#38598;&#29305;&#24449;&#21644;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;Python&#21253;&#20013;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#20351;&#29992;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
As input data distributions evolve, the predictive performance of machine learning models tends to deteriorate. In practice, new input data tend to come without target labels. Then, state-of-the-art techniques model input data distributions or model prediction distributions and try to understand issues regarding the interactions between learned models and shifting distributions. We suggest a novel approach that models how explanation characteristics shift when affected by distribution shifts. We find that the modeling of explanation shifts can be a better indicator for detecting out-of-distribution model behaviour than state-of-the-art techniques. We analyze different types of distribution shifts using synthetic examples and real-world data sets. We provide an algorithmic method that allows us to inspect the interaction between data set features and learned models and compare them to the state-of-the-art. We release our methods in an open-source Python package, as well as the code used
&lt;/p&gt;</description></item><item><title>BoundaryCAM&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#30340;&#24369;&#30417;&#30563;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#23545;&#35937;&#20301;&#32622;&#65292;&#23454;&#29616;&#31934;&#32454;&#30340;&#39640;&#31934;&#24230;&#20998;&#21106;&#25513;&#27169;&#12290;</title><link>http://arxiv.org/abs/2303.07853</link><description>&lt;p&gt;
BoundaryCAM&#65306;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#30340;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised Semantic Segmentation of Medical Images. (arXiv:2303.07853v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07853
&lt;/p&gt;
&lt;p&gt;
BoundaryCAM&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#30340;&#24369;&#30417;&#30563;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#23545;&#35937;&#20301;&#32622;&#65292;&#23454;&#29616;&#31934;&#32454;&#30340;&#39640;&#31934;&#24230;&#20998;&#21106;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#21033;&#29992;&#22270;&#20687;&#32423;&#21035;&#30417;&#30563;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#26159;&#35299;&#20915;&#20998;&#21106;&#32593;&#32476;&#38656;&#27714;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22312;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22823;&#37327;&#20687;&#32032;&#32423;&#25513;&#27169;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#32423;WSSS&#25216;&#26415;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#21253;&#21547;&#30340;&#20960;&#20309;&#29305;&#24449;&#30340;&#29702;&#35299;&#65292;&#22240;&#20026;&#32593;&#32476;&#26080;&#27861;&#20174;&#20165;&#22270;&#20687;&#32423;&#21035;&#26631;&#31614;&#20013;&#23548;&#20986;&#20219;&#20309;&#23545;&#35937;&#36793;&#30028;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#26032;&#22411;BoundaryCAM&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#31867;&#28608;&#27963;&#22270;&#32467;&#21512;&#21508;&#31181;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#31934;&#32454;&#30340;&#39640;&#31934;&#24230;&#20998;&#21106;&#25513;&#27169;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#21487;&#29992;&#20110;&#26500;&#24314;&#36793;&#30028;&#22270;&#65292;&#20197;&#20351;BoundaryCAM&#33021;&#22815;&#39640;&#31934;&#24230;&#39044;&#27979;&#23545;&#35937;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision is a promising approach to deal with the need for Segmentation networks, especially for generating a large number of pixel-wise masks in a given dataset. However, most state-of-the-art image-level WSSS techniques lack an understanding of the geometric features embedded in the images since the network cannot derive any object boundary information from just image-level labels. We define a boundary here as the line separating an object and its background, or two different objects. To address this drawback, we propose our novel BoundaryCAM framework, which deploys state-of-the-art class activation maps combined with various post-processing techniques in order to achieve fine-grained higher-accuracy segmentation masks. To achieve this, we investigate a state-of-the-art unsupervised semantic segmentation network that can be used to construct a boundary map, which enables BoundaryCAM to predict object locations w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#29702;&#24615;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#29702;&#24615;&#36924;&#36817;&#22312;&#20915;&#31574;&#21464;&#37327;&#25968;&#30446;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#31070;&#32463;&#32593;&#32476;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#29702;&#24615;&#36924;&#36817;&#30340;&#25928;&#29575;&#65292;&#21363;&#20351;&#36924;&#36817;&#21442;&#25968;&#30340;&#25968;&#30446;&#36739;&#23567;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25913;&#36827;&#20102;&#29702;&#24615;&#36924;&#36817;&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20248;&#21270;&#26041;&#27861;&#26469;&#25511;&#21046;&#32422;&#26463;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.04436</link><description>&lt;p&gt;
&#29702;&#24615;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#20284;&#26041;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A comparison of rational and neural network based approximations. (arXiv:2303.04436v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#29702;&#24615;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#29702;&#24615;&#36924;&#36817;&#22312;&#20915;&#31574;&#21464;&#37327;&#25968;&#30446;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#31070;&#32463;&#32593;&#32476;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#29702;&#24615;&#36924;&#36817;&#30340;&#25928;&#29575;&#65292;&#21363;&#20351;&#36924;&#36817;&#21442;&#25968;&#30340;&#25968;&#30446;&#36739;&#23567;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25913;&#36827;&#20102;&#29702;&#24615;&#36924;&#36817;&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20248;&#21270;&#26041;&#27861;&#26469;&#25511;&#21046;&#32422;&#26463;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#36924;&#36817;&#26041;&#27861;&#20013;&#65292;&#29702;&#24615;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#20284;&#26041;&#27861;&#26159;&#39640;&#25928;&#30340;&#24037;&#20855;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#23545;&#38750;&#20809;&#28369;&#21644;&#38750;Lipschitz&#20989;&#25968;&#65292;&#21253;&#25324;&#22810;&#21464;&#37327;&#22495;&#20989;&#25968;&#65292;&#36827;&#34892;&#31934;&#30830;&#36924;&#36817;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#29702;&#24615;&#36924;&#36817;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#36827;&#34892;&#20989;&#25968;&#36924;&#36817;&#30340;&#25928;&#29575;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#22312;&#30456;&#21516;&#20915;&#31574;&#21464;&#37327;&#25968;&#30446;&#19979;&#65292;&#29702;&#24615;&#36924;&#36817;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#29702;&#24615;&#36924;&#36817;&#30340;&#25928;&#29575;&#65292;&#21363;&#20351;&#36924;&#36817;&#21442;&#25968;&#30340;&#25968;&#30446;&#65288;&#21363;&#30456;&#24212;&#20248;&#21270;&#38382;&#39064;&#30340;&#32500;&#25968;&#65289;&#36739;&#23567;&#20063;&#26159;&#22914;&#27492;&#12290;&#26412;&#25991;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#22312;&#20110;&#29702;&#24615;&#36924;&#36817;&#31639;&#27861;&#30340;&#25913;&#36827;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;&#29702;&#24615;&#36924;&#36817;&#31639;&#27861;&#21487;&#20197;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#25511;&#21046;&#32422;&#26463;&#30697;&#38453;&#30340;&#26465;&#20214;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rational and neural network based approximations are efficient tools in modern approximation. These approaches are able to produce accurate approximations to nonsmooth and non-Lipschitz functions, including multivariate domain functions. In this paper we compare the efficiency of function approximation using rational approximation, neural network and their combinations. It was found that rational approximation is superior to neural network based approaches with the same number of decision variables. Our numerical experiments demonstrate the efficiency of rational approximation, even when the number of approximation parameters (that is, the dimension of the corresponding optimisation problems) is small. Another important contribution of this paper lies in the improvement of rational approximation algorithms. Namely, the optimisation based algorithms for rational approximation can be adjusted to in such a way that the conditioning number of the constraint matrices are controlled. This si
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#38024;&#23545;&#36890;&#29992;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#39640;&#25928;&#37327;&#23376;&#35299;&#20915;&#26041;&#26696;&#65292;&#21482;&#35201;&#27169;&#22411;&#36275;&#22815;&#32791;&#25955;&#21644;&#31232;&#30095;&#65292;&#20855;&#26377;&#23567;&#30340;&#23398;&#20064;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#32553;&#25918;&#33267; $O(T^2 \times \text{polylog}(n))$&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#35777;&#26126;&#20102;&#22312;&#31232;&#30095;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#37327;&#23376;&#35745;&#31639;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.03428</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35777;&#26126;&#39640;&#25928;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards provably efficient quantum algorithms for large-scale machine-learning models. (arXiv:2303.03428v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#30340;&#38024;&#23545;&#36890;&#29992;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#39640;&#25928;&#37327;&#23376;&#35299;&#20915;&#26041;&#26696;&#65292;&#21482;&#35201;&#27169;&#22411;&#36275;&#22815;&#32791;&#25955;&#21644;&#31232;&#30095;&#65292;&#20855;&#26377;&#23567;&#30340;&#23398;&#20064;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#32553;&#25918;&#33267; $O(T^2 \times \text{polylog}(n))$&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#35777;&#26126;&#20102;&#22312;&#31232;&#30095;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#37327;&#23376;&#35745;&#31639;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#38761;&#21629;&#24615;&#25216;&#26415;&#65292;&#20854;&#29942;&#39048;&#21253;&#25324;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12289;&#21151;&#32791;&#21644;&#26102;&#38388;&#65292;&#26082;&#29992;&#20110;&#39044;&#35757;&#32451;&#65292;&#20063;&#29992;&#20110;&#24494;&#35843;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#23481;&#38169;&#37327;&#23376;&#35745;&#31639;&#21487;&#33021;&#20250;&#38024;&#23545;&#36890;&#29992;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#32553;&#25918;&#20026; $\mathcal{O}(T^2 \times \text{polylog}(n))$&#65292;&#20854;&#20013; $n$ &#26159;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;$T$ &#26159;&#35757;&#32451;&#20013;&#30340;&#36845;&#20195;&#27425;&#25968;&#65292;&#21482;&#35201;&#27169;&#22411;&#36275;&#22815;&#32791;&#25955;&#21644;&#31232;&#30095;&#65292;&#24182;&#20855;&#26377;&#36739;&#23567;&#30340;&#23398;&#20064;&#29575;&#12290;&#22522;&#20110;&#26089;&#26399;&#29992;&#20110;&#32791;&#25955;&#24494;&#20998;&#26041;&#31243;&#30340;&#39640;&#25928;&#37327;&#23376;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#35777;&#26126;&#20102;&#31867;&#20284;&#30340;&#31639;&#27861;&#21487;&#29992;&#20110;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#65292;&#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#35201;&#31639;&#27861;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#23545;&#25317;&#26377;&#20174;700&#19975;&#21040;1.03&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#31232;&#30095;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#37327;&#23376;&#35745;&#31639;&#26174;&#28982;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large machine learning models are revolutionary technologies of artificial intelligence whose bottlenecks include huge computational expenses, power, and time used both in the pre-training and fine-tuning process. In this work, we show that fault-tolerant quantum computing could possibly provide provably efficient resolutions for generic (stochastic) gradient descent algorithms, scaling as $\mathcal{O}(T^2 \times \text{polylog}(n))$, where $n$ is the size of the models and $T$ is the number of iterations in the training, as long as the models are both sufficiently dissipative and sparse, with small learning rates. Based on earlier efficient quantum algorithms for dissipative differential equations, we find and prove that similar algorithms work for (stochastic) gradient descent, the primary algorithm for machine learning. In practice, we benchmark instances of large machine learning models from 7 million to 103 million parameters. We find that, in the context of sparse training, a quan
&lt;/p&gt;</description></item><item><title>Internet Explorer&#26159;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#20114;&#32852;&#32593;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#22312;&#32593;&#32476;&#19978;&#25628;&#32034;&#30456;&#20851;&#22270;&#20687;&#24182;&#35757;&#32451;&#23567;&#35268;&#27169;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.14051</link><description>&lt;p&gt;
Internet Explorer:&#24320;&#25918;&#32593;&#32476;&#19978;&#30340;&#26377;&#38024;&#23545;&#24615;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Internet Explorer: Targeted Representation Learning on the Open Web. (arXiv:2302.14051v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14051
&lt;/p&gt;
&lt;p&gt;
Internet Explorer&#26159;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#20114;&#32852;&#32593;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#22312;&#32593;&#32476;&#19978;&#25628;&#32034;&#30456;&#20851;&#22270;&#20687;&#24182;&#35757;&#32451;&#23567;&#35268;&#27169;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#22312;&#22823;&#22411;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#36890;&#29992;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#20123;&#36890;&#29992;&#27169;&#22411;&#21482;&#33021;&#25429;&#25417;&#21040;&#23427;&#20204;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#30693;&#35782;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#38598;&#21482;&#26159;&#20114;&#32852;&#32593;&#30340;&#24494;&#23567;&#12289;&#36807;&#26102;&#30340;&#24555;&#29031;&#8212;&#8212;&#32780;&#20114;&#32852;&#32593;&#19978;&#27599;&#22825;&#37117;&#19978;&#20256;&#25968;&#21313;&#20159;&#24352;&#22270;&#29255;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65306;&#19981;&#26159;&#24076;&#26395;&#25105;&#20204;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21518;&#33021;&#22815;&#36716;&#31227;&#21040;&#25105;&#20204;&#26399;&#26395;&#30340;&#20219;&#21153;&#19978;&#65292;&#32780;&#26159;&#25105;&#20204;&#24314;&#35758;&#21160;&#24577;&#21033;&#29992;&#20114;&#32852;&#32593;&#65292;&#24555;&#36895;&#35757;&#32451;&#19968;&#20010;&#22312;&#24403;&#21069;&#20219;&#21153;&#19978;&#34920;&#29616;&#38750;&#24120;&#22909;&#30340;&#23567;&#35268;&#27169;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21483;&#20570;Internet Explorer&#65292;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#22312;&#32593;&#32476;&#19978;&#25506;&#32034;&#65292;&#36880;&#27493;&#25214;&#21040;&#25913;&#21892;&#25152;&#38656;&#30446;&#26631;&#25968;&#25454;&#38598;&#24615;&#33021;&#30340;&#30456;&#20851;&#31034;&#20363;&#12290;&#23427;&#24490;&#29615;&#22312;&#20114;&#32852;&#32593;&#19978;&#25628;&#32034;&#24102;&#26377;&#25991;&#26412;&#26597;&#35810;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#19979;&#36733;&#30340;&#22270;&#20687;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#65292;&#30830;&#23450;&#21738;&#20123;&#22270;&#20687;&#26159;&#26377;&#29992;&#30340;&#65292;&#24182;&#30830;&#23450;&#19979;&#19968;&#27493;&#35201;&#25628;&#32034;&#30340;&#20248;&#20808;&#32423;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Internet Explorer&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern vision models typically rely on fine-tuning general-purpose models pre-trained on large, static datasets. These general-purpose models only capture the knowledge within their pre-training datasets, which are tiny, out-of-date snapshots of the Internet -- where billions of images are uploaded each day. We suggest an alternate approach: rather than hoping our static datasets transfer to our desired tasks after large-scale pre-training, we propose dynamically utilizing the Internet to quickly train a small-scale model that does extremely well on the task at hand. Our approach, called Internet Explorer, explores the web in a self-supervised manner to progressively find relevant examples that improve performance on a desired target dataset. It cycles between searching for images on the Internet with text queries, self-supervised training on downloaded images, determining which images were useful, and prioritizing what to search for next. We evaluate Internet Explorer across several d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#32771;&#34385;&#20102;&#21307;&#23398;&#22270;&#20687;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#36890;&#36807;&#27604;&#36739;&#22312;ImageNet&#21644;RadImageNet&#19978;&#30340;&#21021;&#22987;&#21270;&#65292;&#21457;&#29616;ImageNet&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;RadImageNet&#19978;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.08272</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#21307;&#23398;&#22270;&#20687;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#38544;&#34255;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Revisiting Hidden Representations in Transfer Learning for Medical Imaging. (arXiv:2302.08272v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#32771;&#34385;&#20102;&#21307;&#23398;&#22270;&#20687;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#36890;&#36807;&#27604;&#36739;&#22312;ImageNet&#21644;RadImageNet&#19978;&#30340;&#21021;&#22987;&#21270;&#65292;&#21457;&#29616;ImageNet&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;RadImageNet&#19978;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#20855;&#26377;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20294;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#36890;&#24120;&#22312;&#22810;&#26679;&#24615;&#21644;&#35268;&#27169;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;&#36801;&#31227;&#23398;&#20064;&#26377;&#21487;&#33021;&#24357;&#21512;&#30456;&#20851;&#20294;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21307;&#23398;&#24212;&#29992;&#32780;&#35328;&#65292;&#39044;&#35757;&#32451;&#33258;&#28982;&#22270;&#20687;&#36824;&#26159;&#21307;&#23398;&#22270;&#20687;&#26356;&#26377;&#30410;&#20173;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23545;&#27604;&#22312;ImageNet&#21644;RadImageNet&#19978;&#30340;&#21021;&#22987;&#21270;&#65292;&#22312;&#19971;&#20010;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21253;&#25324;&#22797;&#21046;&#24615;&#30740;&#31350;&#65292;&#20854;&#32467;&#26524;&#19982;&#20808;&#21069;&#21457;&#34920;&#30340;&#30740;&#31350;&#30456;&#21453;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;ImageNet&#19978;&#39044;&#35757;&#32451;&#30340;ResNet50&#27169;&#22411;&#24448;&#24448;&#20248;&#20110;&#22312;RadImageNet&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20102;&#35299;&#65292;&#25105;&#20204;&#20351;&#29992;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#30740;&#31350;&#20102;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#31034;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#30452;&#35273;&#30456;&#21453;&#65292;ImageNet&#21644;RadImageNet&#21487;&#33021;&#20250;&#25910;&#25947;&#21040;&#19981;&#21516;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a key component to the success of deep learning is the availability of massive amounts of training data, medical image datasets are often limited in diversity and size. Transfer learning has the potential to bridge the gap between related yet different domains. For medical applications, however, it remains unclear whether it is more beneficial to pre-train on natural or medical images. We aim to shed light on this problem by comparing initialization on ImageNet and RadImageNet on seven medical classification tasks. Our work includes a replication study, which yields results contrary to previously published findings. In our experiments, ResNet50 models pre-trained on ImageNet tend to outperform those trained on RadImageNet. To gain further insights, we investigate the learned representations using Canonical Correlation Analysis (CCA) and compare the predictions of the different models. Our results indicate that, contrary to intuition, ImageNet and RadImageNet may converge to disti
&lt;/p&gt;</description></item><item><title>LB-SimTSC&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#30456;&#20284;&#24615;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;LB_Keogh&#20316;&#20026;DTW&#30340;&#19979;&#30028;&#26469;&#35299;&#20915;DTW&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#65292;&#22312;&#23569;&#26631;&#31614;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.04838</link><description>&lt;p&gt;
LB-SimTSC: &#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#39640;&#25928;&#30456;&#20284;&#24615;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LB-SimTSC: An Efficient Similarity-Aware Graph Neural Network for Semi-Supervised Time Series Classification. (arXiv:2301.04838v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04838
&lt;/p&gt;
&lt;p&gt;
LB-SimTSC&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#30456;&#20284;&#24615;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;LB_Keogh&#20316;&#20026;DTW&#30340;&#19979;&#30028;&#26469;&#35299;&#20915;DTW&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#65292;&#22312;&#23569;&#26631;&#31614;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#25968;&#25454;&#25366;&#25496;&#20219;&#21153;&#65292;&#22312;&#36807;&#21435;&#30340;20&#24180;&#37324;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#30001;&#20110;&#23454;&#38469;&#20013;&#26631;&#31614;&#31232;&#32570;&#65292;&#21482;&#26377;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#30340;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21464;&#24471;&#27969;&#34892;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#30456;&#20284;&#24615;&#24863;&#30693;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;SimTSC&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#25209;&#37327;&#25968;&#25454;&#30340;&#20004;&#20004;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#36317;&#31163;&#29983;&#25104;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#27169;&#22411;&#12290;SimTSC&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;&#20960;&#20010;&#23569;&#26631;&#31614;&#35774;&#32622;&#20013;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SimTSC&#20381;&#36182;&#20110;&#20004;&#20004;DTW&#36317;&#31163;&#65292;DTW&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#23427;&#21482;&#33021;&#22312;&#36866;&#24230;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#21322;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#25216;&#26415;LB-SimTSC&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#26032;&#30340;&#22270;&#26500;&#24314;&#27169;&#22359;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;DTW&#30340;&#19979;&#30028;LB_Keogh&#26469;&#36817;&#20284;DTW&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series classification is an important data mining task that has received a lot of interest in the past two decades. Due to the label scarcity in practice, semi-supervised time series classification with only a few labeled samples has become popular. Recently, Similarity-aware Time Series Classification (SimTSC) is proposed to address this problem by using a graph neural network classification model on the graph generated from pairwise Dynamic Time Warping (DTW) distance of batch data. It shows excellent accuracy and outperforms state-of-the-art deep learning models in several few-label settings. However, since SimTSC relies on pairwise DTW distances, the quadratic complexity of DTW limits its usability to only reasonably sized datasets. To address this challenge, we propose a new efficient semi-supervised time series classification technique, LB-SimTSC, with a new graph construction module. Instead of using DTW, we propose to utilize a lower bound of DTW, LB_Keogh, to approximate 
&lt;/p&gt;</description></item><item><title>&#20449;&#24687;&#35770;&#21578;&#35785;&#25105;&#20204;&#65292;&#20026;&#20102;&#26368;&#22823;&#21270;&#27169;&#22411;&#30340;&#20449;&#24687;&#37327;&#65292;&#36873;&#25321;&#21487;&#33021;&#24615;&#26368;&#39640;&#25110;&#34920;&#31034;&#35823;&#24046;&#27604;&#29305;&#26368;&#23569;&#30340;&#23458;&#35266;&#20989;&#25968;&#12290;&#23558;&#19981;&#21516;&#30340;&#23458;&#35266;&#20989;&#25968;&#36716;&#25442;&#20026;&#20284;&#28982;&#20989;&#25968;&#65292;&#23427;&#20204;&#30340;&#30456;&#23545;&#22823;&#23567;&#34920;&#31034;&#25105;&#20204;&#24212;&#35813;&#26356;&#21916;&#27426;&#21738;&#20010;&#23458;&#35266;&#20989;&#25968;&#65292;&#32780;&#20854;&#22823;&#23567;&#30340;&#23545;&#25968;&#34920;&#31034;&#27169;&#22411;&#30340;&#39044;&#26399;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.06566</link><description>&lt;p&gt;
&#22914;&#20309;&#20351;&#29992;&#20449;&#24687;&#35770;&#36873;&#25321;&#23458;&#35266;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
How to select an objective function using information theory. (arXiv:2212.06566v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06566
&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#35770;&#21578;&#35785;&#25105;&#20204;&#65292;&#20026;&#20102;&#26368;&#22823;&#21270;&#27169;&#22411;&#30340;&#20449;&#24687;&#37327;&#65292;&#36873;&#25321;&#21487;&#33021;&#24615;&#26368;&#39640;&#25110;&#34920;&#31034;&#35823;&#24046;&#27604;&#29305;&#26368;&#23569;&#30340;&#23458;&#35266;&#20989;&#25968;&#12290;&#23558;&#19981;&#21516;&#30340;&#23458;&#35266;&#20989;&#25968;&#36716;&#25442;&#20026;&#20284;&#28982;&#20989;&#25968;&#65292;&#23427;&#20204;&#30340;&#30456;&#23545;&#22823;&#23567;&#34920;&#31034;&#25105;&#20204;&#24212;&#35813;&#26356;&#21916;&#27426;&#21738;&#20010;&#23458;&#35266;&#20989;&#25968;&#65292;&#32780;&#20854;&#22823;&#23567;&#30340;&#23545;&#25968;&#34920;&#31034;&#27169;&#22411;&#30340;&#39044;&#26399;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#25110;&#31185;&#23398;&#35745;&#31639;&#20013;&#65292;&#27169;&#22411;&#24615;&#33021;&#26159;&#36890;&#36807;&#23458;&#35266;&#20989;&#25968;&#34913;&#37327;&#30340;&#12290;&#20294;&#26159;&#20026;&#20160;&#20040;&#35201;&#36873;&#25321;&#26576;&#20010;&#23458;&#35266;&#20989;&#25968;&#32780;&#19981;&#26159;&#21478;&#19968;&#20010;&#65311;&#20449;&#24687;&#35770;&#32473;&#20986;&#20102;&#19968;&#20010;&#31572;&#26696;&#65306;&#20026;&#20102;&#26368;&#22823;&#21270;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#37327;&#65292;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#30340;&#23458;&#35266;&#20989;&#25968;&#25110;&#32773;&#20195;&#34920;&#35823;&#24046;&#30340;&#27604;&#29305;&#26368;&#23569;&#30340;&#20989;&#25968;&#12290;&#35201;&#35780;&#20272;&#19981;&#21516;&#30340;&#23458;&#35266;&#20989;&#25968;&#65292;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#20284;&#28982;&#20989;&#25968;&#12290;&#20316;&#20026;&#20284;&#28982;&#20989;&#25968;&#65292;&#23427;&#20204;&#30340;&#30456;&#23545;&#22823;&#23567;&#34920;&#31034;&#25105;&#20204;&#24212;&#35813;&#26356;&#21916;&#27426;&#21738;&#20010;&#23458;&#35266;&#20989;&#25968;&#65292;&#32780;&#20854;&#22823;&#23567;&#30340;&#23545;&#25968;&#34920;&#31034;&#27169;&#22411;&#30340;&#39044;&#26399;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning or scientific computing, model performance is measured with an objective function. But why choose one objective over another? Information theory gives one answer: To maximize the information in the model, select the most likely objective function or whichever represents the error in the fewest bits. To evaluate different objectives, transform them into likelihood functions. As likelihoods, their relative magnitudes represent how much we should prefer one objective versus another, and the log of their magnitude represents the expected uncertainty of the model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#23454;&#29616;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#21644;&#24322;&#24120;&#32531;&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2212.04031</link><description>&lt;p&gt;
&#20851;&#20110;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#21644;&#24322;&#24120;&#32531;&#35299;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On Root Cause Localization and Anomaly Mitigation through Causal Inference. (arXiv:2212.04031v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04031
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#23454;&#29616;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#21644;&#24322;&#24120;&#32531;&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#23433;&#20840;&#12289;&#37329;&#34701;&#30417;&#27979;&#21644;&#20581;&#24247;&#39118;&#38505;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#26377;&#25928;&#20043;&#22806;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20174;&#19994;&#32773;&#36824;&#24076;&#26395;&#36827;&#19968;&#27493;&#20102;&#35299;&#26159;&#20160;&#20040;&#23548;&#33268;&#20102;&#24322;&#24120;&#32467;&#26524;&#20197;&#21450;&#22914;&#20309;&#36827;&#19968;&#27493;&#20462;&#22797;&#23427;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RootCLAM&#65292;&#26088;&#22312;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#23454;&#29616;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#21644;&#24322;&#24120;&#32531;&#35299;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23558;&#30001;&#22806;&#37096;&#24178;&#39044;&#24341;&#36215;&#30340;&#24322;&#24120;&#23450;&#20041;&#20026;&#23545;&#27491;&#24120;&#22240;&#26524;&#26426;&#21046;&#30340;&#22806;&#37096;&#24178;&#39044;&#65292;&#24182;&#26088;&#22312;&#23450;&#20301;&#20855;&#26377;&#22806;&#37096;&#24178;&#39044;&#30340;&#24322;&#24120;&#29305;&#24449;&#20316;&#20026;&#26681;&#26412;&#21407;&#22240;&#12290;&#27492;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#32531;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#25512;&#33616;&#38024;&#23545;&#24322;&#24120;&#29305;&#24449;&#30340;&#32531;&#35299;&#25514;&#26045;&#65292;&#20197;&#24674;&#22797;&#27491;&#24120;&#32467;&#26524;&#65292;&#20351;&#30001;&#22240;&#26524;&#26426;&#21046;&#25351;&#23548;&#30340;&#21453;&#20107;&#23454;&#21464;&#24471;&#27491;&#24120;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23450;&#20301;&#26681;&#26412;&#21407;&#22240;&#21644;&#32531;&#35299;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to a wide spectrum of applications in the real world, such as security, financial surveillance, and health risk, various deep anomaly detection models have been proposed and achieved state-of-the-art performance. However, besides being effective, in practice, the practitioners would further like to know what causes the abnormal outcome and how to further fix it. In this work, we propose RootCLAM, which aims to achieve Root Cause Localization and Anomaly Mitigation from a causal perspective. Especially, we formulate anomalies caused by external interventions on the normal causal mechanism and aim to locate the abnormal features with external interventions as root causes. After that, we further propose an anomaly mitigation approach that aims to recommend mitigation actions on abnormal features to revert the abnormal outcomes such that the counterfactuals guided by the causal mechanism are normal. Experiments on three datasets show that our approach can locate the root causes and fur
&lt;/p&gt;</description></item><item><title>PGFed&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26174;&#24335;&#21644;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#27599;&#20010;&#23458;&#25143;&#30340;&#32463;&#39564;&#39118;&#38505;&#65292;&#20351;&#27599;&#20010;&#23458;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#20854;&#33258;&#24049;&#30340;&#20840;&#23616;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2212.01448</link><description>&lt;p&gt;
PGFed&#65306;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20010;&#24615;&#21270;&#27599;&#20010;&#23458;&#25143;&#30340;&#20840;&#23616;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
PGFed: Personalize Each Client's Global Objective for Federated Learning. (arXiv:2212.01448v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01448
&lt;/p&gt;
&lt;p&gt;
PGFed&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26174;&#24335;&#21644;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#27599;&#20010;&#23458;&#25143;&#30340;&#32463;&#39564;&#39118;&#38505;&#65292;&#20351;&#27599;&#20010;&#23458;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#20854;&#33258;&#24049;&#30340;&#20840;&#23616;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#24179;&#24248;&#65292;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#19982;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#21333;&#20010;&#20840;&#23616;&#20849;&#35782;&#27169;&#22411;&#19981;&#21516;&#65292;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20801;&#35768;&#20026;&#19981;&#21516;&#30340;&#23458;&#25143;&#24314;&#31435;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#21482;&#26159;&#36890;&#36807;&#23558;&#30693;&#35782;&#23884;&#20837;&#21040;&#32858;&#21512;&#27169;&#22411;&#25110;&#27491;&#21017;&#21270;&#20013;&#38544;&#24335;&#22320;&#20256;&#36882;&#21327;&#20316;&#30693;&#35782;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#38544;&#24335;&#30340;&#30693;&#35782;&#20256;&#36882;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;&#27599;&#20010;&#23458;&#25143;&#30340;&#32463;&#39564;&#39118;&#38505;&#23545;&#20854;&#20182;&#23458;&#25143;&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Personalized Global Federated Learning (PGFed)&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26174;&#24335;&#21644;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#33258;&#36523;&#21644;&#20854;&#20182;&#23458;&#25143;&#30340;&#32463;&#39564;&#39118;&#38505;&#65292;&#20351;&#27599;&#20010;&#23458;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#20854;&#33258;&#24049;&#30340;&#20840;&#23616;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized federated learning has received an upsurge of attention due to the mediocre performance of conventional federated learning (FL) over heterogeneous data. Unlike conventional FL which trains a single global consensus model, personalized FL allows different models for different clients. However, existing personalized FL algorithms only implicitly transfer the collaborative knowledge across the federation by embedding the knowledge into the aggregated model or regularization. We observed that this implicit knowledge transfer fails to maximize the potential of each client's empirical risk toward other clients. Based on our observation, in this work, we propose Personalized Global Federated Learning (PGFed), a novel personalized FL framework that enables each client to personalize its own global objective by explicitly and adaptively aggregating the empirical risks of itself and other clients. To avoid massive (O(N^2)) communication overhead and potential privacy leakage while a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#38750;&#23545;&#27604;&#24230;CT&#19978;&#23545;&#24613;&#24615;&#32570;&#34880;&#24615;&#25913;&#21464;&#30340;&#20998;&#21106;&#26159;&#21542;&#33021;&#22815;&#36798;&#21040;&#19982;&#31070;&#32463;&#25918;&#23556;&#21307;&#23398;&#23478;&#30456;&#23218;&#32654;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2211.15341</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#38750;&#23545;&#27604;&#24230;CT&#19978;&#23545;&#24613;&#24615;&#32570;&#34880;&#24615;&#20013;&#39118;&#20998;&#21106;&#30340;&#38750;&#21155;&#24615;&#19982;&#19987;&#23478;&#31070;&#32463;&#25918;&#23556;&#21307;&#23398;&#23478;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Non-inferiority of Deep Learning Acute Ischemic Stroke Segmentation on Non-Contrast CT Compared to Expert Neuroradiologists. (arXiv:2211.15341v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#38750;&#23545;&#27604;&#24230;CT&#19978;&#23545;&#24613;&#24615;&#32570;&#34880;&#24615;&#25913;&#21464;&#30340;&#20998;&#21106;&#26159;&#21542;&#33021;&#22815;&#36798;&#21040;&#19982;&#31070;&#32463;&#25918;&#23556;&#21307;&#23398;&#23478;&#30456;&#23218;&#32654;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#38750;&#23545;&#27604;&#24230;CT&#19978;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#20998;&#21106;&#24613;&#24615;&#32570;&#34880;&#24615;&#25913;&#21464;&#65292;&#19982;&#31070;&#32463;&#25918;&#23556;&#21307;&#23398;&#23478;&#30456;&#27604;&#12290;&#35813;&#30740;&#31350;&#32435;&#20837;&#20102;232&#20363;&#21442;&#19982;DEFUSE 3&#35797;&#39564;&#30340;&#24613;&#24615;&#32570;&#34880;&#24615;&#20013;&#39118;&#24739;&#32773;&#30340;&#38750;&#23545;&#27604;&#24230;CT&#26816;&#26597;&#12290;&#19977;&#21517;&#32463;&#39564;&#20016;&#23500;&#30340;&#31070;&#32463;&#25918;&#23556;&#21307;&#23398;&#23478;&#29420;&#31435;&#22320;&#23545;&#27599;&#20010;&#25195;&#25551;&#22270;&#20687;&#19978;&#21453;&#26144;&#32570;&#34880;&#28790;&#30340;&#20302;&#23494;&#24230;&#36827;&#34892;&#20998;&#21106;&#12290;&#32463;&#39564;&#26368;&#20016;&#23500;&#30340;&#31070;&#32463;&#25918;&#23556;&#21307;&#23398;&#23478;&#65288;&#19987;&#23478;A&#65289;&#30340;&#20998;&#21106;&#32467;&#26524;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#22522;&#20934;&#12290;&#21478;&#22806;&#20004;&#21517;&#31070;&#32463;&#25918;&#23556;&#21307;&#23398;&#23478;&#65288;&#19987;&#23478;B&#21644;C&#65289;&#30340;&#20998;&#21106;&#32467;&#26524;&#29992;&#20110;&#25968;&#25454;&#27979;&#35797;&#12290;232&#20010;&#30740;&#31350;&#34987;&#38543;&#26426;&#20998;&#25104;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#12290;&#35757;&#32451;&#38598;&#36827;&#19968;&#27493;&#38543;&#26426;&#20998;&#25104;5&#25240;&#65292;&#21253;&#25324;&#35757;&#32451;&#38598;&#21644;&#39564;&#35777;&#38598;&#12290;&#20351;&#29992;&#19977;&#32500;CNN&#26550;&#26500;&#35757;&#32451;&#21644;&#20248;&#21270;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#23545;&#38750;&#23545;&#27604;&#24230;CT&#19978;&#19987;&#23478;A&#30340;&#20998;&#21106;&#12290;&#20351;&#29992;&#19968;&#32452;&#23481;&#31215;&#12289;&#37325;&#21472;&#21644;&#36317;&#31163;&#25351;&#26631;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To determine if a convolutional neural network (CNN) deep learning model can accurately segment acute ischemic changes on non-contrast CT compared to neuroradiologists. Non-contrast CT (NCCT) examinations from 232 acute ischemic stroke patients who were enrolled in the DEFUSE 3 trial were included in this study. Three experienced neuroradiologists independently segmented hypodensity that reflected the ischemic core on each scan. The neuroradiologist with the most experience (expert A) served as the ground truth for deep learning model training. Two additional neuroradiologists (experts B and C) segmentations were used for data testing. The 232 studies were randomly split into training and test sets. The training set was further randomly divided into 5 folds with training and validation sets. A 3-dimensional CNN architecture was trained and optimized to predict the segmentations of expert A from NCCT. The performance of the model was assessed using a set of volume, overlap, and distance
&lt;/p&gt;</description></item><item><title>USE-Evaluator&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#30001;&#20110;&#21442;&#32771;&#27880;&#37322;&#19981;&#30830;&#23450;&#12289;&#23567;&#22411;&#25110;&#31354;&#30333;&#23548;&#33268;&#30340;&#24120;&#35268;&#25351;&#26631;&#26080;&#27861;&#36866;&#29992;&#20110;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.13008</link><description>&lt;p&gt;
USE-Evaluator: &#24102;&#26377;&#19981;&#30830;&#23450;&#12289;&#23567;&#22411;&#25110;&#31354;&#30333;&#21442;&#32771;&#27880;&#37322;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
USE-Evaluator: Performance Metrics for Medical Image Segmentation Models with Uncertain, Small or Empty Reference Annotations. (arXiv:2209.13008v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13008
&lt;/p&gt;
&lt;p&gt;
USE-Evaluator&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#30001;&#20110;&#21442;&#32771;&#27880;&#37322;&#19981;&#30830;&#23450;&#12289;&#23567;&#22411;&#25110;&#31354;&#30333;&#23548;&#33268;&#30340;&#24120;&#35268;&#25351;&#26631;&#26080;&#27861;&#36866;&#29992;&#20110;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#24615;&#33021;&#24230;&#37327;&#20027;&#35201;&#29992;&#20110;&#34913;&#37327;&#21442;&#32771;&#27880;&#37322;&#19982;&#39044;&#27979;&#20998;&#21106;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#24120;&#20351;&#29992;&#37325;&#21472;&#24230;&#37327;&#65292;&#20363;&#22914;Dice&#31995;&#25968;&#65292;&#20316;&#20026;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#25351;&#26631;&#65292;&#20197;&#20415;&#32467;&#26524;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;&#28982;&#32780;&#65292;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#30340;&#30149;&#20363;&#20998;&#24067;&#21644;&#20998;&#21106;&#20219;&#21153;&#38590;&#24230;&#19982;&#20020;&#24202;&#23454;&#36341;&#23384;&#22312;&#19981;&#21305;&#37197;&#20043;&#22788;&#12290;&#24120;&#35268;&#25351;&#26631;&#26410;&#33021;&#34913;&#37327;&#21040;&#36825;&#31181;&#19981;&#21305;&#37197;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#21253;&#21547;&#20302;&#20449;&#21495;&#30149;&#29702;&#12289;&#38590;&#24230;&#36739;&#22823;&#30340;&#20998;&#21106;&#20219;&#21153;&#20197;&#21450;&#19981;&#30830;&#23450;&#12289;&#23567;&#22411;&#25110;&#31354;&#30333;&#21442;&#32771;&#27880;&#37322;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#12290;&#36825;&#19968;&#38480;&#21046;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#22312;&#35774;&#35745;&#21644;&#20248;&#21270;&#27169;&#22411;&#26102;&#25928;&#26524;&#19981;&#20339;&#12290;&#35780;&#20272;&#20020;&#24202;&#20215;&#20540;&#30340;&#32500;&#24230;&#21253;&#25324;&#32771;&#34385;&#21442;&#32771;&#27880;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#12289;&#19982;&#21442;&#32771;&#27880;&#37322;&#20307;&#31215;&#22823;&#23567;&#30340;&#29420;&#31435;&#24615;&#20197;&#21450;&#23545;&#31354;&#30333;&#20998;&#31867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance metrics for medical image segmentation models are used to measure the agreement between the reference annotation and the predicted segmentation. Usually, overlap metrics, such as the Dice, are used as a metric to evaluate the performance of these models in order for results to be comparable. However, there is a mismatch between the distributions of cases and difficulty level of segmentation tasks in public data sets compared to clinical practice. Common metrics fail to measure the impact of this mismatch, especially for clinical data sets that include low signal pathologies, a difficult segmentation task, and uncertain, small, or empty reference annotations. This limitation may result in ineffective research of machine learning practitioners in designing and optimizing models. Dimensions of evaluating clinical value include consideration of the uncertainty of reference annotations, independence from reference annotation volume size, and evaluation of classification of empty
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#30340;&#20840;&#23616;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#24102;&#22522;&#25968;&#32422;&#26463;&#30340;&#26368;&#23567;&#24179;&#26041;&#21644;&#32858;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;SDP&#26494;&#24347;&#26041;&#27861;&#21644;&#23450;&#21046;&#30340;&#20998;&#25903;&#31574;&#30053;&#26469;&#25913;&#21892;&#35299;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.08901</link><description>&lt;p&gt;
&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#30340;&#24102;&#22522;&#25968;&#32422;&#26463;&#26368;&#23567;&#24179;&#26041;&#21644;&#32858;&#31867;&#30340;&#20840;&#23616;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Global Optimization for Cardinality-constrained Minimum Sum-of-Squares Clustering via Semidefinite Programming. (arXiv:2209.08901v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#30340;&#20840;&#23616;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#24102;&#22522;&#25968;&#32422;&#26463;&#30340;&#26368;&#23567;&#24179;&#26041;&#21644;&#32858;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;SDP&#26494;&#24347;&#26041;&#27861;&#21644;&#23450;&#21046;&#30340;&#20998;&#25903;&#31574;&#30053;&#26469;&#25913;&#21892;&#35299;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#24179;&#26041;&#21644;&#32858;&#31867;(MSSC)&#25110;k&#22343;&#20540;&#31867;&#22411;&#32858;&#31867;&#26368;&#36817;&#24050;&#32463;&#25193;&#23637;&#20197;&#21033;&#29992;&#23545;&#27599;&#20010;&#31751;&#22522;&#25968;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#36825;&#31181;&#30693;&#35782;&#34987;&#29992;&#20110;&#25552;&#39640;&#24615;&#33021;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25903;&#23450;&#30028;&#25216;&#26415;&#30340;&#20840;&#23616;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#22522;&#25968;&#32422;&#26463;&#30340;MSSC&#38382;&#39064;&#12290;&#23545;&#20110;&#19979;&#30028;&#20363;&#31243;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;Rujeerapaiboon&#31561;&#20154;&#26368;&#36817;&#25552;&#20986;&#30340;&#21322;&#23450;&#35268;&#21010;(SDP)&#26494;&#24347;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35813;&#26494;&#24347;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#23567;&#35268;&#27169;&#23454;&#20363;&#30340;&#20998;&#25903;&#23450;&#30028;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SDP&#26494;&#24347;&#26041;&#27861;&#65292;&#23427;&#33021;&#26356;&#22909;&#22320;&#36866;&#24212;&#23454;&#20363;&#35268;&#27169;&#21644;&#31751;&#30340;&#25968;&#30446;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#22810;&#38754;&#20307;&#21106;&#32447;&#26469;&#21152;&#24378;&#19979;&#30028;&#12290;&#36890;&#36807;&#23450;&#21046;&#30340;&#20998;&#25903;&#31574;&#30053;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#25104;&#23545;&#32422;&#26463;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23376;&#33410;&#28857;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#23545;&#20110;&#19978;&#30028;&#65292;
&lt;/p&gt;
&lt;p&gt;
The minimum sum-of-squares clustering (MSSC), or k-means type clustering, has been recently extended to exploit prior knowledge on the cardinality of each cluster. Such knowledge is used to increase performance as well as solution quality. In this paper, we propose a global optimization approach based on the branch-and-cut technique to solve the cardinality-constrained MSSC. For the lower bound routine, we use the semidefinite programming (SDP) relaxation recently proposed by Rujeerapaiboon et al. [SIAM J. Optim. 29(2), 1211-1239, (2019)]. However, this relaxation can be used in a branch-and-cut method only for small-size instances. Therefore, we derive a new SDP relaxation that scales better with the instance size and the number of clusters. In both cases, we strengthen the bound by adding polyhedral cuts. Benefiting from a tailored branching strategy which enforces pairwise constraints, we reduce the complexity of the problems arising in the children nodes. For the upper bound, inste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#23545;&#25239;&#24615;&#31574;&#30053;&#30340;&#31354;&#38388;&#12290;&#36890;&#36807;&#20998;&#35299;&#25915;&#20987;&#32452;&#20214;&#65292;&#24182;&#24341;&#20837;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#23545;&#25239;&#34892;&#20026;&#65292;&#24182;&#19988;&#34913;&#37327;&#25915;&#20987;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.04521</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#31574;&#30053;&#30340;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
The Space of Adversarial Strategies. (arXiv:2209.04521v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#23545;&#25239;&#24615;&#31574;&#30053;&#30340;&#31354;&#38388;&#12290;&#36890;&#36807;&#20998;&#35299;&#25915;&#20987;&#32452;&#20214;&#65292;&#24182;&#24341;&#20837;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#23545;&#25239;&#34892;&#20026;&#65292;&#24182;&#19988;&#34913;&#37327;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#21363;&#26088;&#22312;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#35825;&#21457;&#26368;&#31967;&#31957;&#34892;&#20026;&#30340;&#36755;&#20837;&#65292;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#29702;&#35299;&#26469;&#33258;&#20110;&#19968;&#29255;&#30456;&#23545;&#38646;&#25955;&#30340;&#30693;&#35782;&#27744;&#65307;&#30446;&#21069;&#65292;&#23384;&#22312;&#30528;&#19968;&#20123;&#25915;&#20987;&#26041;&#27861;&#65292;&#27599;&#31181;&#26041;&#27861;&#22312;&#23041;&#32961;&#27169;&#22411;&#19978;&#37117;&#26377;&#19981;&#21516;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#23450;&#20041;&#30340;&#26368;&#20248;&#24615;&#19981;&#21487;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#21051;&#30011;&#26368;&#22351;&#24773;&#20917;&#65288;&#21363;&#26368;&#20248;&#65289;&#30340;&#23545;&#25239;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;&#20013;&#25915;&#20987;&#30340;&#21487;&#25299;&#23637;&#20998;&#35299;&#65292;&#23558;&#25915;&#20987;&#32452;&#20214;&#20998;&#35299;&#20026;&#34920;&#38754;&#21644;&#26053;&#34892;&#32773;&#12290;&#20511;&#21161;&#20110;&#25105;&#20204;&#30340;&#20998;&#35299;&#65292;&#25105;&#20204;&#26522;&#20030;&#20102;&#21508;&#31181;&#32452;&#20214;&#20197;&#21019;&#24314;&#20102;576&#31181;&#25915;&#20987;&#65288;&#20854;&#20013;568&#31181;&#20197;&#21069;&#23578;&#26410;&#25506;&#32034;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24085;&#32047;&#25176;&#32452;&#21512;&#25915;&#20987;&#65288;PEA&#65289;&#65306;&#19968;&#31181;&#29702;&#35770;&#19978;&#38480;&#21046;&#25915;&#20987;&#24615;&#33021;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#25105;&#20204;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#40065;&#26834;&#21644;&#38750;&#40065;&#26834;&#27169;&#22411;&#12289;&#19971;&#20010;&#25968;&#25454;&#38598;&#20197;&#21450;&#19977;&#20010;&#25193;&#23637;lp&#19978;&#30456;&#23545;&#20110;PEA&#34913;&#37327;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples, inputs designed to induce worst-case behavior in machine learning models, have been extensively studied over the past decade. Yet, our understanding of this phenomenon stems from a rather fragmented pool of knowledge; at present, there are a handful of attacks, each with disparate assumptions in threat models and incomparable definitions of optimality. In this paper, we propose a systematic approach to characterize worst-case (i.e., optimal) adversaries. We first introduce an extensible decomposition of attacks in adversarial machine learning by atomizing attack components into surfaces and travelers. With our decomposition, we enumerate over components to create 576 attacks (568 of which were previously unexplored). Next, we propose the Pareto Ensemble Attack (PEA): a theoretical attack that upper-bounds attack performance. With our new attacks, we measure performance relative to the PEA on: both robust and non-robust models, seven datasets, and three extended lp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;NeurVec&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20462;&#27491;&#22120;&#65292;&#23427;&#33021;&#22815;&#34917;&#20607;&#38598;&#25104;&#35823;&#24046;&#24182;&#22312;&#27169;&#25311;&#20013;&#23454;&#29616;&#26356;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;NeurVec&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21152;&#36895;&#20102;&#20256;&#32479;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.03680</link><description>&lt;p&gt;
&#36890;&#36807;NeurVec&#21152;&#36895;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec. (arXiv:2208.03680v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;NeurVec&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20462;&#27491;&#22120;&#65292;&#23427;&#33021;&#22815;&#34917;&#20607;&#38598;&#25104;&#35823;&#24046;&#24182;&#22312;&#27169;&#25311;&#20013;&#23454;&#29616;&#26356;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;NeurVec&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21152;&#36895;&#20102;&#20256;&#32479;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#30340;&#27169;&#25311;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#22312;&#20272;&#35745;&#31215;&#20998;&#26102;&#30001;&#20110;&#27493;&#38271;&#36873;&#25321;&#30340;&#38480;&#21046;&#65292;&#23384;&#22312;&#30528;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20462;&#27491;&#22120;&#65292;&#31216;&#20026;NeurVec&#65292;&#23427;&#21487;&#20197;&#34917;&#20607;&#38598;&#25104;&#35823;&#24046;&#24182;&#22312;&#27169;&#25311;&#20013;&#23454;&#29616;&#26356;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#20351;&#29992;&#26377;&#38480;&#21644;&#31163;&#25955;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;NeurVec&#22312;&#36830;&#32493;&#30456;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;NeurVec&#26174;&#33879;&#21152;&#36895;&#20102;&#20256;&#32479;&#27714;&#35299;&#22120;&#65292;&#23454;&#29616;&#20102;&#20960;&#21313;&#21040;&#20960;&#30334;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;NeurVec&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35774;&#35745;&#32467;&#21512;&#20102;&#26131;&#20110;&#23454;&#29616;&#30340;&#29305;&#28857;&#65292;&#26377;&#28508;&#21147;&#24314;&#31435;&#36215;&#19968;&#20010;&#26032;&#30340;&#27714;&#35299;&#22120;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large-scale simulation of dynamical systems is critical in numerous scientific and engineering disciplines. However, traditional numerical solvers are limited by the choice of step sizes when estimating integration, resulting in a trade-off between accuracy and computational efficiency. To address this challenge, we introduce a deep learning-based corrector called Neural Vector (NeurVec), which can compensate for integration errors and enable larger time step sizes in simulations. Our extensive experiments on a variety of complex dynamical system benchmarks demonstrate that NeurVec exhibits remarkable generalization capability on a continuous phase space, even when trained using limited and discrete data. NeurVec significantly accelerates traditional solvers, achieving speeds tens to hundreds of times faster while maintaining high levels of accuracy and stability. Moreover, NeurVec's simple-yet-effective design, combined with its ease of implementation, has the potential to establi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#20559;&#22909;&#24314;&#27169;&#20026;&#27599;&#20010;&#36712;&#36857;&#27573;&#30340;&#36951;&#25022;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#21487;&#20197;&#26681;&#25454;&#36825;&#20123;&#36951;&#25022;&#29983;&#25104;&#30340;&#20559;&#22909;&#26469;&#35782;&#21035;&#29983;&#25104;&#36825;&#20123;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#36951;&#25022;&#20559;&#22909;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2206.02231</link><description>&lt;p&gt;
&#20154;&#31867;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#20559;&#22909;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Models of human preference for learning reward functions. (arXiv:2206.02231v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#20559;&#22909;&#24314;&#27169;&#20026;&#27599;&#20010;&#36712;&#36857;&#27573;&#30340;&#36951;&#25022;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#21487;&#20197;&#26681;&#25454;&#36825;&#20123;&#36951;&#25022;&#29983;&#25104;&#30340;&#20559;&#22909;&#26469;&#35782;&#21035;&#29983;&#25104;&#36825;&#20123;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#36951;&#25022;&#20559;&#22909;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29992;&#21463;&#38480;&#20110;&#22870;&#21169;&#20989;&#25968;&#19982;&#20154;&#31867;&#21033;&#30410;&#30340;&#19968;&#33268;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#23545;&#40784;&#26041;&#27861;&#26159;&#20174;&#20154;&#31867;&#29983;&#25104;&#30340;&#36712;&#36857;&#27573;&#23545;&#20043;&#38388;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#24120;&#20551;&#35774;&#36825;&#20123;&#20154;&#31867;&#20559;&#22909;&#20165;&#30001;&#37096;&#20998;&#22238;&#25253;&#26469;&#20915;&#23450;&#65292;&#21363;&#27599;&#20010;&#36712;&#36857;&#27573;&#19978;&#30340;&#22870;&#21169;&#24635;&#21644;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#20551;&#35774;&#23384;&#22312;&#32570;&#38519;&#65292;&#25552;&#20986;&#23558;&#20154;&#31867;&#20559;&#22909;&#24314;&#27169;&#20026;&#30001;&#27599;&#20010;&#36712;&#36857;&#27573;&#30340;&#36951;&#25022;&#26469;&#20915;&#23450;&#65292;&#36951;&#25022;&#26159;&#19968;&#31181;&#34913;&#37327;&#36712;&#36857;&#27573;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#20559;&#31163;&#31243;&#24230;&#30340;&#24230;&#37327;&#12290;&#22312;&#26681;&#25454;&#36951;&#25022;&#29983;&#25104;&#30340;&#26080;&#31351;&#22810;&#20010;&#20559;&#22909;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#35782;&#21035;&#21040;&#19982;&#29983;&#25104;&#36825;&#20123;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#31561;&#20215;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20197;&#21069;&#30340;&#37096;&#20998;&#22238;&#25253;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#32570;&#20047;&#36825;&#31181;&#21487;&#35782;&#21035;&#24615;&#23646;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#36951;&#25022;&#20559;&#22909;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments, a type of reinforcement learning from human feedback (RLHF). These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment. We find this assumption to be flawed and propose modeling human preferences instead as informed by each segment's regret, a measure of a segment's deviation from optimal decision-making. Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences, and we prove that the previous partial return model lacks this identifiability property in multiple contexts. We empirically show that our proposed regret preference model outperf
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39535;&#26381;&#31526;&#21495;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#21453;&#23545;&#31216;&#21270;&#65292;&#20026;&#22312;&#21453;&#23545;&#31216;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#36890;&#29992;&#30340;&#21453;&#23545;&#31216;&#23618;&#20316;&#20026;&#26500;&#24314;&#27169;&#22359;&#25171;&#24320;&#20102;&#22823;&#38376;&#65292;&#24182;&#19988;&#35813;&#36817;&#20284;&#26041;&#27861;&#22312;&#31526;&#21495;&#38382;&#39064;&#21463;&#25511;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2205.12250</link><description>&lt;p&gt;
&#36890;&#36807;&#39535;&#26381;&#31526;&#21495;&#38382;&#39064;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#23618;&#36827;&#34892;&#39640;&#25928;&#30340;&#21453;&#23545;&#31216;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient anti-symmetrization of a neural network layer by taming the sign problem. (arXiv:2205.12250v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12250
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39535;&#26381;&#31526;&#21495;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#21453;&#23545;&#31216;&#21270;&#65292;&#20026;&#22312;&#21453;&#23545;&#31216;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#36890;&#29992;&#30340;&#21453;&#23545;&#31216;&#23618;&#20316;&#20026;&#26500;&#24314;&#27169;&#22359;&#25171;&#24320;&#20102;&#22823;&#38376;&#65292;&#24182;&#19988;&#35813;&#36817;&#20284;&#26041;&#27861;&#22312;&#31526;&#21495;&#38382;&#39064;&#21463;&#25511;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#26174;&#24335;&#21453;&#23545;&#31216;&#21270;&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#21453;&#23545;&#31216;&#20989;&#25968;&#30340;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#65292;&#32780;&#36825;&#31181;&#20989;&#25968;&#22312;&#37327;&#23376;&#29289;&#29702;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#30340;&#23454;&#26045;&#25104;&#26412;&#26497;&#39640;&#65292;&#23545;&#20110;&#22823;&#37327;&#31890;&#23376;&#26469;&#35828;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#35813;&#31574;&#30053;&#36824;&#23384;&#22312;&#31526;&#21495;&#38382;&#39064;&#12290;&#21363;&#30001;&#20110;&#27491;&#36127;&#36129;&#29486;&#30340;&#36817;&#20046;&#23436;&#20840;&#25269;&#28040;&#65292;&#21453;&#23545;&#31216;&#21270;&#20989;&#25968;&#30340;&#24133;&#20540;&#21487;&#33021;&#26126;&#26174;&#23567;&#20110;&#21453;&#23545;&#31216;&#21270;&#20043;&#21069;&#30340;&#24133;&#20540;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#35780;&#20272;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#23545;&#31216;&#25237;&#24433;&#65292;&#20174;&#32780;&#20026;&#22312;&#21453;&#23545;&#31216;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#36890;&#29992;&#30340;&#21453;&#23545;&#31216;&#23618;&#20316;&#20026;&#26500;&#24314;&#27169;&#22359;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;&#36825;&#31181;&#36817;&#20284;&#22312;&#31526;&#21495;&#38382;&#39064;&#21463;&#25511;&#26102;&#26159;&#26377;&#25928;&#30340;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24615;&#36136;&#22312;&#20351;&#29992;&#26631;&#20934;Xavier/He&#21021;&#22987;&#21270;&#26041;&#27861;&#26102;&#65292;&#20851;&#38190;&#21462;&#20915;&#20110;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explicit antisymmetrization of a neural network is a potential candidate for a universal function approximator for generic antisymmetric functions, which are ubiquitous in quantum physics. However, this procedure is a priori factorially costly to implement, making it impractical for large numbers of particles. The strategy also suffers from a sign problem. Namely, due to near-exact cancellation of positive and negative contributions, the magnitude of the antisymmetrized function may be significantly smaller than before anti-symmetrization. We show that the anti-symmetric projection of a two-layer neural network can be evaluated efficiently, opening the door to using a generic antisymmetric layer as a building block in anti-symmetric neural network Ansatzes. This approximation is effective when the sign problem is controlled, and we show that this property depends crucially the choice of activation function under standard Xavier/He initialization methods. As a consequence, using a smoot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-SDE&#30340;&#31639;&#27861;&#26469;&#23398;&#20064;&#24930;-&#24555;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26377;&#25928;&#38477;&#32500;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;&#31163;&#25955;&#21270;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#25429;&#25417;&#20102;&#31995;&#32479;&#30340;&#28436;&#21270;&#29305;&#24615;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.04151</link><description>&lt;p&gt;
Auto-SDE:&#20174;&#25968;&#25454;&#39537;&#21160;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#38477;&#32500;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Auto-SDE: Learning effective reduced dynamics from data-driven stochastic dynamical systems. (arXiv:2205.04151v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.04151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-SDE&#30340;&#31639;&#27861;&#26469;&#23398;&#20064;&#24930;-&#24555;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26377;&#25928;&#38477;&#32500;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;&#31163;&#25955;&#21270;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#25429;&#25417;&#20102;&#31995;&#32479;&#30340;&#28436;&#21270;&#29305;&#24615;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33021;&#22815;&#25551;&#32472;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#29616;&#35937;&#65292;&#22810;&#23610;&#24230;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31185;&#23398;&#21644;&#24037;&#31243;&#38382;&#39064;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#30740;&#31350;&#24930;-&#24555;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26377;&#25928;&#38477;&#32500;&#21160;&#21147;&#23398;&#12290;&#32473;&#23450;&#28385;&#36275;&#26576;&#20123;&#26410;&#30693;&#24930;-&#24555;&#38543;&#26426;&#31995;&#32479;&#30340;&#30701;&#26399;&#35266;&#27979;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#25324;&#33258;&#21160;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;Auto-SDE&#30340;&#26032;&#31639;&#27861;&#26469;&#23398;&#20064;&#19981;&#21464;&#30340;&#24930;&#27969;&#24418;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25429;&#25417;&#20102;&#19968;&#31995;&#21015;&#26102;&#38388;&#30456;&#20851;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#30340;&#28436;&#21270;&#29305;&#24615;&#65292;&#25439;&#22833;&#20989;&#25968;&#36890;&#36807;&#31163;&#25955;&#21270;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26500;&#36896;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19979;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiscale stochastic dynamical systems have been widely adopted to scientific and engineering problems due to their capability of depicting complex phenomena in many real world applications. This work is devoted to investigating the effective reduced dynamics for a slow-fast stochastic dynamical system. Given observation data on a short-term period satisfying some unknown slow-fast stochastic system, we propose a novel algorithm including a neural network called Auto-SDE to learn invariant slow manifold. Our approach captures the evolutionary nature of a series of time-dependent autoencoder neural networks with the loss constructed from a discretized stochastic differential equation. Our algorithm is also proved to be accurate, stable and effective through numerical experiments under various evaluation metrics.
&lt;/p&gt;</description></item><item><title>&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#39044;&#27979;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25972;&#21512;&#20102;&#22810;&#20010;&#38431;&#21015;&#20013;&#30340;&#20020;&#24202;&#21644;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#65292;&#36890;&#36807;&#20998;&#26512;&#39640;&#32500;MRI&#29305;&#24449;&#21644;&#20854;&#20182;&#25968;&#25454;&#27169;&#24577;&#26469;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#30340;&#30149;&#24773;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2203.09096</link><description>&lt;p&gt;
DeepAD:&#19968;&#31181;&#29992;&#20110;&#23454;&#38469;&#20020;&#24202;&#24212;&#29992;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#40065;&#26834;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DeepAD: A Robust Deep Learning Model of Alzheimer's Disease Progression for Real-World Clinical Applications. (arXiv:2203.09096v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09096
&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#23637;&#39044;&#27979;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25972;&#21512;&#20102;&#22810;&#20010;&#38431;&#21015;&#20013;&#30340;&#20020;&#24202;&#21644;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#65292;&#36890;&#36807;&#20998;&#26512;&#39640;&#32500;MRI&#29305;&#24449;&#21644;&#20854;&#20182;&#25968;&#25454;&#27169;&#24577;&#26469;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#30340;&#30149;&#24773;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#30340;&#30149;&#24773;&#36712;&#36857;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31561;&#22797;&#26434;&#30142;&#30149;&#30340;&#33647;&#29289;&#30740;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29992;&#20110;&#39044;&#27979;&#30142;&#30149;&#36827;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35201;&#20040;&#26159;&#21333;&#20219;&#21153;&#35201;&#20040;&#26159;&#21333;&#27169;&#22411;&#65292;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#21253;&#21547;&#22810;&#20219;&#21153;&#23398;&#20064;&#19982;&#39640;&#32500;&#24433;&#20687;&#30340;&#25105;&#20204;&#30340;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#35757;&#32451;&#20110;&#21333;&#20010;&#25968;&#25454;&#38598;&#65288;&#21363;&#38431;&#21015;&#65289;&#65292;&#24456;&#38590;&#25512;&#24191;&#21040;&#20854;&#20182;&#38431;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#22810;&#20010;&#38431;&#21015;&#20013;&#30340;&#32437;&#21521;&#20020;&#24202;&#21644;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#65292;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#32500;MRI&#29305;&#24449;&#19982;&#20020;&#24202;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#20449;&#24687;&#31561;&#20854;&#20182;&#25968;&#25454;&#27169;&#24577;&#25972;&#21512;&#65292;&#26469;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#30340;&#30149;&#24773;&#36712;&#36857;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#24341;&#20837;&#20102;&#23545;&#25239;&#25439;&#22833;&#26469;&#32531;&#35299;&#38431;&#21015;&#29305;&#24322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to predict the future trajectory of a patient is a key step toward the development of therapeutics for complex diseases such as Alzheimer's disease (AD). However, most machine learning approaches developed for prediction of disease progression are either single-task or single-modality models, which can not be directly adopted to our setting involving multi-task learning with high dimensional images. Moreover, most of those approaches are trained on a single dataset (i.e. cohort), which can not be generalized to other cohorts. We propose a novel multimodal multi-task deep learning model to predict AD progression by analyzing longitudinal clinical and neuroimaging data from multiple cohorts. Our proposed model integrates high dimensional MRI features from a 3D convolutional neural network with other data modalities, including clinical and demographic information, to predict the future trajectory of patients. Our model employs an adversarial loss to alleviate the study-specifi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#26356;&#24265;&#20215;&#30340;&#25130;&#26029;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#38544;&#34255;&#22122;&#22768;&#25968;&#25454;&#20998;&#24067;&#24320;&#22987;&#29983;&#25104;&#25968;&#25454;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2202.09671</link><description>&lt;p&gt;
&#25130;&#26029;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders. (arXiv:2202.09671v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09671
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#26356;&#24265;&#20215;&#30340;&#25130;&#26029;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#38544;&#34255;&#22122;&#22768;&#25968;&#25454;&#20998;&#24067;&#24320;&#22987;&#29983;&#25104;&#25968;&#25454;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#27491;&#21521;&#25193;&#25955;&#38142;&#36880;&#27493;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;&#22122;&#22768;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#25512;&#26029;&#21453;&#21521;&#25193;&#25955;&#38142;&#26469;&#23398;&#20064;&#22914;&#20309;&#29983;&#25104;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36895;&#24230;&#24930;&#19988;&#25104;&#26412;&#39640;&#65292;&#22240;&#20026;&#38656;&#35201;&#35768;&#22810;&#27491;&#21521;&#21644;&#21453;&#21521;&#27493;&#39588;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#26356;&#24265;&#20215;&#30340;&#26041;&#27861;&#65292;&#19981;&#26159;&#23558;&#22122;&#22768;&#28155;&#21152;&#21040;&#25968;&#25454;&#21464;&#20026;&#32431;&#38543;&#26426;&#22122;&#22768;&#65292;&#32780;&#26159;&#30452;&#21040;&#36798;&#21040;&#19968;&#20010;&#21487;&#20197;&#33258;&#20449;&#23398;&#20064;&#30340;&#38544;&#34255;&#22122;&#22768;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36739;&#23569;&#30340;&#21453;&#21521;&#27493;&#39588;&#36890;&#36807;&#20174;&#36825;&#20010;&#38544;&#34255;&#20998;&#24067;&#24320;&#22987;&#29983;&#25104;&#31867;&#20284;&#20110;&#22122;&#22768;&#25968;&#25454;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#21644;&#21487;&#23398;&#20064;&#30340;&#38544;&#21547;&#20808;&#39564;&#22686;&#24378;&#30340;&#23545;&#25239;&#24615;&#33258;&#32534;&#30721;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#36739;&#23569;&#30340;&#21453;&#21521;&#25193;&#25955;&#27493;&#39588;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#25130;&#26029;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#20173;&#28982;&#21487;&#20197;&#30456;&#36739;&#20110;&#38750;&#25130;&#26029;&#27169;&#22411;&#25552;&#20379;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Employing a forward diffusion chain to gradually map the data to a noise distribution, diffusion-based generative models learn how to generate the data by inferring a reverse diffusion chain. However, this approach is slow and costly because it needs many forward and reverse steps. We propose a faster and cheaper approach that adds noise not until the data become pure random noise, but until they reach a hidden noisy data distribution that we can confidently learn. Then, we use fewer reverse steps to generate data by starting from this hidden distribution that is made similar to the noisy data. We reveal that the proposed model can be cast as an adversarial auto-encoder empowered by both the diffusion process and a learnable implicit prior. Experimental results show even with a significantly smaller number of reverse diffusion steps, the proposed truncated diffusion probabilistic models can provide consistent improvements over the non-truncated ones in terms of performance in both unco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25968;&#23398;&#23450;&#20041;&#21644;&#30740;&#31350;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#21457;&#29616;&#20351;&#29992;&#25968;&#25454;&#29420;&#31435;&#20989;&#25968;&#36827;&#34892;&#29305;&#24449;&#21270;&#30340;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21160;&#21147;&#23398;&#26426;&#21046;&#20197;&#21450;&#30456;&#24212;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#25968;&#25454;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.12198</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#29420;&#31435;&#20989;&#25968;&#30340;&#38480;&#21046;&#36827;&#34892;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Limitation of Characterizing Implicit Regularization by Data-independent Functions. (arXiv:2201.12198v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25968;&#23398;&#23450;&#20041;&#21644;&#30740;&#31350;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#21457;&#29616;&#20351;&#29992;&#25968;&#25454;&#29420;&#31435;&#20989;&#25968;&#36827;&#34892;&#29305;&#24449;&#21270;&#30340;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21160;&#21147;&#23398;&#26426;&#21046;&#20197;&#21450;&#30456;&#24212;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#25968;&#25454;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#39033;&#26680;&#24515;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#38544;&#24335;&#27491;&#21017;&#21270;&#26412;&#36523;&#24182;&#27809;&#26377;&#23436;&#20840;&#23450;&#20041;&#21644;&#28145;&#20837;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23545;&#38544;&#24335;&#27491;&#21017;&#21270;&#36827;&#34892;&#25968;&#23398;&#23450;&#20041;&#21644;&#30740;&#31350;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#25968;&#25454;&#29420;&#31435;&#20989;&#25968;&#36827;&#34892;&#38544;&#24335;&#27491;&#21017;&#21270;&#29305;&#24449;&#21270;&#30340;&#24120;&#35265;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#21160;&#21147;&#23398;&#26426;&#21046;&#65292;&#21363;&#21452;&#28857;&#21644;&#21333;&#28857;&#37325;&#21472;&#26426;&#21046;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#26426;&#21046;&#25552;&#20379;&#20102;&#20004;&#31181;&#33021;&#22815;&#35777;&#26126;&#19981;&#33021;&#23436;&#20840;&#30001;&#19968;&#31181;&#25110;&#25152;&#26377;&#25968;&#25454;&#29420;&#31435;&#20989;&#25968;&#29305;&#24449;&#21270;&#30340;&#21333;&#38544;&#34255;&#31070;&#32463;&#20803;NNs&#31867;&#30340;&#29983;&#25104;&#26041;&#27861;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#31867;&#20284;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#25968;&#25454;&#20381;&#36182;&#24615;&#65292;&#28608;&#21169;&#25105;&#20204;&#26410;&#26469;&#35814;&#32454;&#30740;&#31350;NN&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#25968;&#25454;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, understanding the implicit regularization of neural networks (NNs) has become a central task in deep learning theory. However, implicit regularization is itself not completely defined and well understood. In this work, we attempt to mathematically define and study implicit regularization. Importantly, we explore the limitations of a common approach to characterizing implicit regularization using data-independent functions. We propose two dynamical mechanisms, i.e., Two-point and One-point Overlapping mechanisms, based on which we provide two recipes for producing classes of one-hidden-neuron NNs that provably cannot be fully characterized by a type of or all data-independent functions. Following the previous works, our results further emphasize the profound data dependency of implicit regularization in general, inspiring us to study in detail the data dependency of NN implicit regularization in the future.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#21270;&#32447;&#24615;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#65292;&#38450;&#27490;&#29305;&#23450;&#38750;&#32447;&#24615;&#23545;&#25163;&#39044;&#27979;&#27010;&#24565;&#65292;&#20294;&#26080;&#27861;&#24443;&#24213;&#35299;&#20915;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#25830;&#38500;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2201.12191</link><description>&lt;p&gt;
Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12191
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#21270;&#32447;&#24615;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#65292;&#38450;&#27490;&#29305;&#23450;&#38750;&#32447;&#24615;&#23545;&#25163;&#39044;&#27979;&#27010;&#24565;&#65292;&#20294;&#26080;&#27861;&#24443;&#24213;&#35299;&#20915;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#25830;&#38500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20986;&#29616;&#12290;&#29702;&#35299;&#36825;&#20123;&#34920;&#31034;&#22914;&#20309;&#32534;&#30721;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#27010;&#24565;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#35782;&#21035;&#31070;&#32463;&#34920;&#31034;&#20013;&#30340;&#27010;&#24565;&#30340;&#19968;&#31181;&#26126;&#26174;&#26041;&#27861;&#26159;&#25628;&#32034;&#19968;&#20010;&#32447;&#24615;&#23376;&#31354;&#38388;&#65292;&#20854;&#25830;&#38500;&#20250;&#38459;&#27490;&#20174;&#34920;&#31034;&#20013;&#39044;&#27979;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35768;&#22810;&#32447;&#24615;&#25830;&#38500;&#31639;&#27861;&#26159;&#21487;&#22788;&#29702;&#21644;&#21487;&#35299;&#37322;&#30340;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#26410;&#24517;&#20197;&#32447;&#24615;&#26041;&#24335;&#34920;&#31034;&#27010;&#24565;&#12290;&#20026;&#20102;&#35782;&#21035;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26680;&#21270;&#30340;&#27010;&#24565;&#25830;&#38500;&#32447;&#24615;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#38450;&#27490;&#29305;&#23450;&#30340;&#38750;&#32447;&#24615;&#23545;&#25163;&#39044;&#27979;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20445;&#25252;&#19981;&#20250;&#36716;&#31227;&#21040;&#19981;&#21516;&#30340;&#38750;&#32447;&#24615;&#23545;&#25163;&#12290;&#22240;&#27492;&#65292;&#24443;&#24213;&#22320;&#25830;&#38500;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation space of neural models for textual data emerges in an unsupervised manner during training. Understanding how those representations encode human-interpretable concepts is a fundamental problem. One prominent approach for the identification of concepts in neural representations is searching for a linear subspace whose erasure prevents the prediction of the concept from the representations. However, while many linear erasure algorithms are tractable and interpretable, neural networks do not necessarily represent concepts in a linear manner. To identify non-linearly encoded concepts, we propose a kernelization of a linear minimax game for concept erasure. We demonstrate that it is possible to prevent specific non-linear adversaries from predicting the concept. However, the protection does not transfer to different nonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded concept remains an open problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;MDPs&#20013;&#20351;&#29992;Q-Learning&#30340;&#25910;&#25947;&#21644;&#36817;&#20284;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#37327;&#21270;&#29366;&#24577;&#21644;&#21160;&#20316;&#65292;&#25105;&#20204;&#35777;&#26126;Quantized Q-Learning&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#65292;&#24182;&#19988;&#36825;&#20010;&#26497;&#38480;&#28385;&#36275;&#19968;&#20010;&#26368;&#20248;&#24615;&#26041;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#36817;&#20284;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.06781</link><description>&lt;p&gt;
Q-Learning&#29992;&#20110;&#20855;&#26377;&#19968;&#33324;&#29366;&#24577;&#31354;&#38388;&#30340;MDPs: &#36890;&#36807;&#24369;&#36830;&#32493;&#24615;&#19979;&#30340;&#37327;&#21270;&#26469;&#23454;&#29616;&#25910;&#25947;&#21644;&#36817;&#20284;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Q-Learning for MDPs with General Spaces: Convergence and Near Optimality via Quantization under Weak Continuity. (arXiv:2111.06781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;MDPs&#20013;&#20351;&#29992;Q-Learning&#30340;&#25910;&#25947;&#21644;&#36817;&#20284;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#37327;&#21270;&#29366;&#24577;&#21644;&#21160;&#20316;&#65292;&#25105;&#20204;&#35777;&#26126;Quantized Q-Learning&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#65292;&#24182;&#19988;&#36825;&#20010;&#26497;&#38480;&#28385;&#36275;&#19968;&#20010;&#26368;&#20248;&#24615;&#26041;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#36817;&#20284;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#35201;&#27714;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDPs)&#20013;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#26159;&#26377;&#38480;&#30340;&#65288;&#20063;&#31216;&#20026;&#21487;&#25511;&#39532;&#23572;&#21487;&#22827;&#38142;&#65289;&#65292;&#25991;&#29486;&#20013;&#24050;&#32463;&#20570;&#20986;&#20102;&#21508;&#31181;&#21162;&#21147;&#65292;&#20197;&#20415;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#36830;&#32493;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#38750;&#24120;&#28201;&#21644;&#30340;&#27491;&#21017;&#24615;&#26465;&#20214;&#19979;&#65288;&#29305;&#21035;&#26159;&#21482;&#28041;&#21450;MDP&#30340;&#36807;&#28193;&#26680;&#30340;&#24369;&#36830;&#32493;&#24615;&#65289;&#65292;&#36890;&#36807;&#23545;&#29366;&#24577;&#21644;&#21160;&#20316;&#36827;&#34892;&#37327;&#21270;&#30340;&#26631;&#20934; Borel MDPs &#30340; Q-Learning&#65288;&#31216;&#20026;Quantized Q-Learning&#65289;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#26497;&#38480;&#65292;&#24182;&#19988;&#36825;&#20010;&#26497;&#38480;&#28385;&#36275;&#19968;&#20010;&#26368;&#20248;&#24615;&#26041;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#36817;&#20284;&#26368;&#20248;&#24615;&#65292;&#35201;&#20040;&#20855;&#26377;&#26174;&#24335;&#30340;&#24615;&#33021;&#30028;&#38480;&#65292;&#35201;&#20040;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#65306;(i) &#23558;&#37327;&#21270;&#35270;&#20026;&#19968;&#20010;&#27979;&#37327;&#26680;&#65292;&#24182;&#23558;&#37327;&#21270;&#30340;MDP&#35270;&#20026;&#19968;&#20010;&#37096;&#20998;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#65292;(ii) &#21033;&#29992; Q-Learning &#23545; POMDP &#30340;&#36817;&#20284;&#26368;&#20248;&#24615;&#21644;&#25910;&#25947;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning algorithms often require finiteness of state and action spaces in Markov decision processes (MDPs) (also called controlled Markov chains) and various efforts have been made in the literature towards the applicability of such algorithms for continuous state and action spaces. In this paper, we show that under very mild regularity conditions (in particular, involving only weak continuity of the transition kernel of an MDP), Q-learning for standard Borel MDPs via quantization of states and actions (called Quantized Q-Learning) converges to a limit, and furthermore this limit satisfies an optimality equation which leads to near optimality with either explicit performance bounds or which are guaranteed to be asymptotically optimal. Our approach builds on (i) viewing quantization as a measurement kernel and thus a quantized MDP as a partially observed Markov decision process (POMDP), (ii) utilizing near optimality and convergence results of Q-learning for POMDPs, and (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#24207;&#36143;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#24207;&#36143;&#25512;&#26029;&#31243;&#24207;&#26469;&#20272;&#35745;&#39044;&#27979;&#24471;&#20998;&#30340;&#26102;&#21464;&#24046;&#24322;&#65292;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#23545;&#39044;&#27979;&#21644;&#32467;&#26524;&#29983;&#25104;&#26041;&#24335;&#30340;&#19981;&#21487;&#39564;&#35777;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2110.00115</link><description>&lt;p&gt;
&#27604;&#36739;&#24207;&#36143;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Comparing Sequential Forecasters. (arXiv:2110.00115v5 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.00115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#24207;&#36143;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#24207;&#36143;&#25512;&#26029;&#31243;&#24207;&#26469;&#20272;&#35745;&#39044;&#27979;&#24471;&#20998;&#30340;&#26102;&#21464;&#24046;&#24322;&#65292;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#23545;&#39044;&#27979;&#21644;&#32467;&#26524;&#29983;&#25104;&#26041;&#24335;&#30340;&#19981;&#21487;&#39564;&#35777;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#20004;&#20010;&#39044;&#27979;&#22120;&#65292;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#23545;&#19968;&#31995;&#21015;&#20107;&#20214;&#36827;&#34892;&#21333;&#27425;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#30456;&#23545;&#22522;&#30784;&#30340;&#38382;&#39064;&#65306;&#22312;&#19981;&#20551;&#35774;&#39044;&#27979;&#21644;&#32467;&#26524;&#29983;&#25104;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22914;&#20309;&#27604;&#36739;&#36825;&#20123;&#39044;&#27979;&#22120;&#65292;&#26080;&#35770;&#26159;&#22312;&#32447;&#36824;&#26159;&#20107;&#21518;&#27604;&#36739;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#29992;&#20110;&#20272;&#35745;&#26102;&#21464;&#39044;&#27979;&#24471;&#20998;&#24046;&#24322;&#30340;&#26032;&#22411;&#24207;&#36143;&#25512;&#26029;&#31243;&#24207;&#65292;&#23545;&#36825;&#20010;&#38382;&#39064;&#32473;&#20986;&#20102;&#20005;&#26684;&#30340;&#31572;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#32622;&#20449;&#24207;&#21015;&#65288;CS&#65289;&#65292;&#23427;&#26159;&#19968;&#31995;&#21015;&#32622;&#20449;&#21306;&#38388;&#65292;&#21487;&#20197;&#36830;&#32493;&#30417;&#27979;&#24182;&#22312;&#20219;&#24847;&#25968;&#25454;&#20381;&#36182;&#20572;&#26102;&#65288;&#8220;anytime-valid&#8221;&#65289;&#19979;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#32622;&#20449;&#24207;&#21015;&#30340;&#23485;&#24230;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#36866;&#24212;&#20102;&#24471;&#20998;&#24046;&#24322;&#30340;&#24213;&#23618;&#26041;&#24046;&#12290;&#23427;&#20204;&#30340;&#26500;&#24314;&#22522;&#20110;&#21338;&#24328;&#35770;&#32479;&#35745;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#29992;&#20110;&#39034;&#24207;&#26816;&#39564;&#24369;&#38646;&#20551;&#35774;&#30340;e&#36807;&#31243;&#21644;p&#36807;&#31243;&#65292;&#21363;&#19968;&#20010;&#39044;&#27979;&#22120;&#24179;&#22343;&#34920;&#29616;&#26159;&#21542;&#20248;&#20110;&#21478;&#19968;&#20010;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider two forecasters, each making a single prediction for a sequence of events over time. We ask a relatively basic question: how might we compare these forecasters, either online or post-hoc, while avoiding unverifiable assumptions on how the forecasts and outcomes were generated? In this paper, we present a rigorous answer to this question by designing novel sequential inference procedures for estimating the time-varying difference in forecast scores. To do this, we employ confidence sequences (CS), which are sequences of confidence intervals that can be continuously monitored and are valid at arbitrary data-dependent stopping times ("anytime-valid"). The widths of our CSs are adaptive to the underlying variance of the score differences. Underlying their construction is a game-theoretic statistical framework, in which we further identify e-processes and p-processes for sequentially testing a weak null hypothesis -- whether one forecaster outperforms another on average (rather tha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38774;&#32593;&#32476;&#19982;&#21151;&#33021;&#27491;&#21017;&#21270;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#38774;&#32593;&#32476;&#20316;&#20026;&#38544;&#24335;&#27491;&#21017;&#21270;&#22120;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26377;&#21033;&#65292;&#20294;&#19981;&#28789;&#27963;&#19988;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26126;&#30830;&#30340;&#21151;&#33021;&#27491;&#21017;&#21270;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#23545;&#20854;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2106.02613</link><description>&lt;p&gt;
&#26550;&#36215;&#38774;&#32593;&#32476;&#19982;&#21151;&#33021;&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#40511;&#27807;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap Between Target Networks and Functional Regularization. (arXiv:2106.02613v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38774;&#32593;&#32476;&#19982;&#21151;&#33021;&#27491;&#21017;&#21270;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#38774;&#32593;&#32476;&#20316;&#20026;&#38544;&#24335;&#27491;&#21017;&#21270;&#22120;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26377;&#21033;&#65292;&#20294;&#19981;&#28789;&#27963;&#19988;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26126;&#30830;&#30340;&#21151;&#33021;&#27491;&#21017;&#21270;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#23545;&#20854;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#23548;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#24341;&#23548;&#23398;&#20064;&#20540;&#20989;&#25968;&#24448;&#24448;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#65292;&#21407;&#22240;&#26159;&#30446;&#26631;&#20540;&#24555;&#36895;&#21464;&#21270;&#12290;&#38774;&#32593;&#32476;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#28382;&#21518;&#21442;&#25968;&#38598;&#21512;&#26469;&#20272;&#35745;&#30446;&#26631;&#20540;&#65292;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;&#23613;&#31649;&#38774;&#32593;&#32476;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#23545;&#20248;&#21270;&#30340;&#24433;&#21709;&#20173;&#26410;&#34987;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38774;&#32593;&#32476;&#20316;&#20026;&#19968;&#20010;&#38544;&#24335;&#27491;&#21017;&#21270;&#22120;&#30340;&#20316;&#29992;&#65292;&#23427;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#22914;&#19981;&#28789;&#27963;&#21644;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#65292;&#21363;&#20351;&#39321;&#33609;TD(0)&#25910;&#25947;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#21151;&#33021;&#27491;&#21017;&#21270;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#26159;&#28789;&#27963;&#21644;&#20984;&#27491;&#21017;&#21270;&#22120;&#65292;&#24182;&#23545;&#20854;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#29615;&#22659;&#12289;&#25240;&#25187;&#22240;&#23376;&#21644;&#38750;&#38543;&#26426;&#25968;&#25454;&#25910;&#38598;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#20197;&#35843;&#26597;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping is behind much of the successes of deep Reinforcement Learning. However, learning the value function via bootstrapping often leads to unstable training due to fast-changing target values. Target Networks are employed to stabilize training by using an additional set of lagging parameters to estimate the target values. Despite the popularity of Target Networks, their effect on the optimization is still misunderstood. In this work, we show that they act as an implicit regularizer which can be beneficial in some cases, but also have disadvantages such as being inflexible and can result in instabilities, even when vanilla TD(0) converges. To overcome these issues, we propose an explicit Functional Regularization alternative that is flexible and a convex regularizer in function space and we theoretically study its convergence. We conduct an experimental study across a range of environments, discount factors, and off-policiness data collections to investigate the effectiveness o
&lt;/p&gt;</description></item><item><title>BoXHED2.0&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#25552;&#21319;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21253;&#25324;&#37325;&#22797;&#20107;&#20214;&#21644;&#31454;&#20105;&#39118;&#38505;&#22312;&#20869;&#30340;&#22810;&#31181;&#29983;&#23384;&#29615;&#22659;&#65292;&#20855;&#26377;&#19982;&#21442;&#25968;&#21270;&#25552;&#21319;&#29983;&#23384;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2103.12591</link><description>&lt;p&gt;
BoXHED2.0&#65306;&#21487;&#25193;&#23637;&#30340;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#25552;&#21319;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BoXHED2.0: Scalable boosting of dynamic survival analysis. (arXiv:2103.12591v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.12591
&lt;/p&gt;
&lt;p&gt;
BoXHED2.0&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#25552;&#21319;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21253;&#25324;&#37325;&#22797;&#20107;&#20214;&#21644;&#31454;&#20105;&#39118;&#38505;&#22312;&#20869;&#30340;&#22810;&#31181;&#29983;&#23384;&#29615;&#22659;&#65292;&#20855;&#26377;&#19982;&#21442;&#25968;&#21270;&#25552;&#21319;&#29983;&#23384;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#23384;&#20998;&#26512;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#22320;&#28041;&#21450;&#21040;&#26102;&#38388;&#20381;&#36182;&#30340;&#21327;&#21464;&#37327;&#12290;Python&#36719;&#20214;&#21253;BoXHED2.0&#26159;&#19968;&#20010;&#23436;&#20840;&#38750;&#21442;&#25968;&#30340;&#26641;&#25552;&#21319;&#29983;&#23384;&#39118;&#38505;&#20272;&#35745;&#22120;&#65292;&#36866;&#29992;&#20110;&#27604;&#21491;&#25130;&#23614;&#26356;&#36890;&#29992;&#30340;&#29983;&#23384;&#29615;&#22659;&#65292;&#21253;&#25324;&#37325;&#22797;&#20107;&#20214;&#21644;&#31454;&#20105;&#39118;&#38505;&#12290;&#30001;&#20110;&#20854;&#26680;&#24515;&#26159;&#29992;C++&#32534;&#20889;&#30340;&#65292;&#36824;&#25903;&#25345;&#20351;&#29992;GPU&#21644;&#22810;&#26680;CPU&#65292;&#22240;&#27492;BoXHED2.0&#30340;&#21487;&#20280;&#32553;&#24615;&#21487;&#19982;&#21442;&#25968;&#21270;&#25552;&#21319;&#29983;&#23384;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;BoXHED2.0&#21487;&#20174;PyPI&#21644;www.github.com/BoXHED&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern applications of survival analysis increasingly involve time-dependent covariates. The Python package BoXHED2.0 is a tree-boosted hazard estimator that is fully nonparametric, and is applicable to survival settings far more general than right-censoring, including recurring events and competing risks. BoXHED2.0 is also scalable to the point of being on the same order of speed as parametric boosted survival models, in part because its core is written in C++ and it also supports the use of GPUs and multicore CPUs. BoXHED2.0 is available from PyPI and also from www.github.com/BoXHED.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22270;&#24418;&#24179;&#28369;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#20351;&#29992;&#36339;&#36291;&#36830;&#25509;&#21644;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2010.10274</link><description>&lt;p&gt;
&#22270;&#24418;&#24179;&#28369;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Fairing Convolutional Networks for Anomaly Detection. (arXiv:2010.10274v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.10274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22270;&#24418;&#24179;&#28369;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#20351;&#29992;&#36339;&#36291;&#36830;&#25509;&#21644;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#26159;&#35768;&#22810;&#22522;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#38750;&#24120;&#26377;&#25928;&#30340;&#24102;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#36880;&#23618;&#20256;&#25773;&#35268;&#21017;&#22312;&#29702;&#35770;&#19978;&#21463;&#21040;&#20960;&#20309;&#22788;&#29702;&#20013;&#38544;&#24335;&#24179;&#28369;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#21253;&#25324;&#29992;&#20110;&#32858;&#21512;&#26469;&#33258;&#30456;&#37051;&#33410;&#28857;&#30340;&#20449;&#24687;&#30340;&#22270;&#21367;&#31215;&#27169;&#22359;&#21644;&#29992;&#20110;&#32452;&#21512;&#36880;&#23618;&#37051;&#23621;&#34920;&#31034;&#30340;&#36339;&#36291;&#36830;&#25509;&#27169;&#22359;&#12290;&#36825;&#20010;&#20256;&#25773;&#35268;&#21017;&#26159;&#36890;&#36807;&#38597;&#21487;&#27604;&#26041;&#27861;&#20174;&#38544;&#24335;&#24179;&#28369;&#26041;&#31243;&#30340;&#36845;&#20195;&#35299;&#23548;&#20986;&#30340;&#12290;&#38500;&#20102;&#36890;&#36807;&#32593;&#32476;&#23618;&#20043;&#38388;&#30340;&#36339;&#36291;&#36830;&#25509;&#25429;&#33719;&#26469;&#33258;&#36828;&#31243;&#22270;&#33410;&#28857;&#30340;&#20449;&#24687;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21033;&#29992;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#26469;&#23398;&#20064;&#26377;&#21306;&#20998;&#24615;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#20123;&#36339;&#36291;&#36830;&#25509;&#26159;&#26681;&#25454;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#26550;&#26500;&#32463;&#36807;&#35774;&#35745;&#25972;&#21512;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolution is a fundamental building block for many deep neural networks on graph-structured data. In this paper, we introduce a simple, yet very effective graph convolutional network with skip connections for semi-supervised anomaly detection. The proposed layerwise propagation rule of our model is theoretically motivated by the concept of implicit fairing in geometry processing, and comprises a graph convolution module for aggregating information from immediate node neighbors and a skip connection module for combining layer-wise neighborhood representations. This propagation rule is derived from the iterative solution of the implicit fairing equation via the Jacobi method. In addition to capturing information from distant graph nodes through skip connections between the network's layers, our approach exploits both the graph structure and node features for learning discriminative node representations. These skip connections are integrated by design in our proposed network archi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36866;&#24403;&#23398;&#20064;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#38750;&#20132;&#25442;&#22810;&#39033;&#24335;&#20248;&#21270;&#65292;&#20445;&#35777;&#20102;&#25968;&#20540;&#35299;&#23545;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2002.01444</link><description>&lt;p&gt;
&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#36866;&#24403;&#23398;&#20064;&#20316;&#20026;&#38750;&#20132;&#25442;&#22810;&#39033;&#24335;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Proper Learning of Linear Dynamical Systems as a Non-Commutative Polynomial Optimisation Problem. (arXiv:2002.01444v5 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.01444
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36866;&#24403;&#23398;&#20064;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#38750;&#20132;&#25442;&#22810;&#39033;&#24335;&#20248;&#21270;&#65292;&#20445;&#35777;&#20102;&#25968;&#20540;&#35299;&#23545;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#39044;&#27979;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;LDS&#65289;&#30340;&#19979;&#19968;&#20010;&#35266;&#27979;&#20540;&#65288;&#31216;&#20026;&#19981;&#36866;&#24403;&#23398;&#20064;&#65289;&#20197;&#21450;&#20272;&#35745;&#20854;&#31995;&#32479;&#30697;&#38453;&#65288;&#31216;&#20026;&#36866;&#24403;&#23398;&#20064;LDS&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24403;&#23398;&#20064;LDS&#30340;&#26041;&#27861;&#65292;&#23613;&#31649;&#38382;&#39064;&#38750;&#20984;&#65292;&#20294;&#33021;&#22815;&#20445;&#35777;&#25968;&#20540;&#35299;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21040;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been much recent progress in forecasting the next observation of a linear dynamical system (LDS), which is known as the improper learning, as well as in the estimation of its system matrices, which is known as the proper learning of LDS. We present an approach to proper learning of LDS, which in spite of the non-convexity of the problem, guarantees global convergence of numerical solutions to a least-squares estimator. We present promising computational results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#36890;&#36807;&#27010;&#29575;&#36923;&#36753;&#21644;&#20851;&#32852;Copula&#20989;&#25968;&#35299;&#20915;&#24322;&#25110;&#34920;&#31034;&#21644;&#36924;&#36817;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#35823;&#24046;&#38754;&#21160;&#24577;&#26469;&#35828;&#26126;&#20854;&#20248;&#21183;&#12290;&#36890;&#36807;&#23558;xor&#34920;&#31034;&#20174;&#24067;&#23572;&#20540;&#25193;&#23637;&#21040;&#23454;&#25968;&#20540;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#28436;&#31034;&#20132;&#21449;&#39564;&#35777;&#27010;&#24565;&#30340;&#26041;&#20415;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/1907.04483</link><description>&lt;p&gt;
Copula&#34920;&#31034;&#21644;&#35823;&#24046;&#38754;&#25237;&#24433;&#23545;&#20110;&#24322;&#25110;&#38382;&#39064;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Copula Representations and Error Surface Projections for the Exclusive Or Problem. (arXiv:1907.04483v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1907.04483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#36890;&#36807;&#27010;&#29575;&#36923;&#36753;&#21644;&#20851;&#32852;Copula&#20989;&#25968;&#35299;&#20915;&#24322;&#25110;&#34920;&#31034;&#21644;&#36924;&#36817;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#19979;&#30340;&#35823;&#24046;&#38754;&#21160;&#24577;&#26469;&#35828;&#26126;&#20854;&#20248;&#21183;&#12290;&#36890;&#36807;&#23558;xor&#34920;&#31034;&#20174;&#24067;&#23572;&#20540;&#25193;&#23637;&#21040;&#23454;&#25968;&#20540;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#28436;&#31034;&#20132;&#21449;&#39564;&#35777;&#27010;&#24565;&#30340;&#26041;&#20415;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#25110;&#65288;xor&#65289;&#20989;&#25968;&#26159;&#23637;&#31034;&#20026;&#20160;&#20040;&#38750;&#32447;&#24615;&#21069;&#39304;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#20248;&#20110;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#31616;&#21333;&#30340;&#31034;&#20363;&#20043;&#19968;&#12290;&#25105;&#20204;&#36890;&#36807;&#27010;&#29575;&#36923;&#36753;&#21644;&#20851;&#32852;Copula&#20989;&#25968;&#35752;&#35770;&#20102;xor&#34920;&#31034;&#21644;&#36924;&#36817;&#38382;&#39064;&#21450;&#20854;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#31616;&#35201;&#22238;&#39038;&#21069;&#39304;&#32593;&#32476;&#35268;&#33539;&#20043;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#32452;&#33394;&#24425;&#20016;&#23500;&#30340;&#19977;&#32500;&#22270;&#34920;&#27604;&#36739;&#20102;&#20351;&#29992;RELU&#21644;tanh&#31561;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;&#23398;&#20064;&#35823;&#24046;&#38754;&#30340;&#21160;&#24577;&#12290;Copula&#34920;&#31034;&#23558;xor&#20174;&#24067;&#23572;&#20540;&#25193;&#23637;&#21040;&#23454;&#25968;&#20540;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#26041;&#24335;&#26469;&#28436;&#31034;&#22312;&#26679;&#26412;&#20869;&#21644;&#26679;&#26412;&#22806;&#25968;&#25454;&#38598;&#19978;&#30340;&#20132;&#21449;&#39564;&#35777;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#25945;&#23398;&#24615;&#30340;&#65292;&#26088;&#22312;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#23548;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exclusive or (xor) function is one of the simplest examples that illustrate why nonlinear feedforward networks are superior to linear regression for machine learning applications. We review the xor representation and approximation problems and discuss their solutions in terms of probabilistic logic and associative copula functions. After briefly reviewing the specification of feedforward networks, we compare the dynamics of learned error surfaces with different activation functions such as RELU and tanh through a set of colorful three-dimensional charts. The copula representations extend xor from Boolean to real values, thereby providing a convenient way to demonstrate the concept of cross-validation on in-sample and out-sample data sets. Our approach is pedagogical and is meant to be a machine learning prolegomenon.
&lt;/p&gt;</description></item></channel></rss>