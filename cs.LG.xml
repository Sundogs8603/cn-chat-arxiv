<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;Human-Regularized PPO (HR-PPO)&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#35757;&#32451;&#20195;&#29702;&#65292;&#23454;&#29616;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#36924;&#30495;&#19988;&#26377;&#25928;&#30340;&#39550;&#39542;&#20249;&#20276;</title><link>https://arxiv.org/abs/2403.19648</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27491;&#21017;&#21270;&#30340;&#33258;&#25105;&#21338;&#24328;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#19982;&#20154;&#31867;&#20860;&#23481;&#30340;&#39550;&#39542;&#20249;&#20276;
&lt;/p&gt;
&lt;p&gt;
Human-compatible driving partners through data-regularized self-play reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19648
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Human-Regularized PPO (HR-PPO)&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#35757;&#32451;&#20195;&#29702;&#65292;&#23454;&#29616;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#36924;&#30495;&#19988;&#26377;&#25928;&#30340;&#39550;&#39542;&#20249;&#20276;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#38754;&#20020;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#19982;&#20154;&#31867;&#36827;&#34892;&#21327;&#35843;&#12290;&#22240;&#27492;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#23558;&#36924;&#30495;&#30340;&#20154;&#31867;&#20195;&#29702;&#32435;&#20837;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#35757;&#32451;&#21644;&#35780;&#20272;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Human-Regularized PPO (HR-PPO)&#30340;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#36827;&#34892;&#35757;&#32451;&#65292;&#23545;&#20559;&#31163;&#20154;&#31867;&#21442;&#32771;&#31574;&#30053;&#30340;&#34892;&#20026;&#36827;&#34892;&#23567;&#24133;&#24809;&#32602;&#65292;&#20197;&#26500;&#24314;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#26082;&#36924;&#30495;&#21448;&#26377;&#25928;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19648v1 Announce Type: cross  Abstract: A central challenge for autonomous vehicles is coordinating with humans. Therefore, incorporating realistic human agents is essential for scalable training and evaluation of autonomous driving systems in simulation. Simulation agents are typically developed by imitating large-scale, high-quality datasets of human driving. However, pure imitation learning agents empirically have high collision rates when executed in a multi-agent closed-loop setting. To build agents that are realistic and effective in closed-loop settings, we propose Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are trained through self-play with a small penalty for deviating from a human reference policy. In contrast to prior work, our approach is RL-first and only uses 30 minutes of imperfect human demonstrations. We evaluate agents in a large set of multi-agent traffic scenes. Results show our HR-PPO agents are highly effective in achieving goa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19647</link><description>&lt;p&gt;
&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65306;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#21457;&#29616;&#21644;&#24212;&#29992;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#30005;&#36335;&#26159;&#20154;&#31867;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#22240;&#26524;&#30456;&#20851;&#23376;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#12290; &#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#30830;&#23450;&#30340;&#30005;&#36335;&#30001;&#22810;&#20041;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#21333;&#20803;&#32452;&#25104;&#65292;&#20363;&#22914;&#27880;&#24847;&#21147;&#22836;&#25110;&#31070;&#32463;&#20803;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23454;&#29616;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#12290; &#30001;&#20110;&#23427;&#20204;&#22522;&#20110;&#32454;&#31890;&#24230;&#21333;&#20803;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23545;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65306;&#25105;&#20204; introduc&#20102;SHIFT&#65292;&#36890;&#36807;&#20999;&#38500;&#20154;&#31867;&#21028;&#26029;&#20026;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21457;&#29616;&#25104;&#21315;&#19978;&#19975;&#20010;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#26469;&#23637;&#31034;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#19988;&#21487;&#25193;&#23637;&#30340;&#21487;&#35299;&#37322;&#24615;&#31649;&#32447;&#65292;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19647v1 Announce Type: cross  Abstract: We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#21644;&#20462;&#21098;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.19631</link><description>&lt;p&gt;
&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#26816;&#32034;&#22686;&#24378;&#30693;&#35782;&#32534;&#36753;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#21644;&#20462;&#21098;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#39640;&#25928;&#33021;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#25972;&#21512;&#23454;&#26102;&#30693;&#35782;&#26356;&#26032;&#65292;&#23548;&#33268;&#21487;&#33021;&#36807;&#26102;&#25110;&#19981;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#24403;&#22788;&#29702;&#22810;&#36339;&#38382;&#39064;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;LLMs&#26356;&#26032;&#21644;&#25972;&#21512;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#22810;&#20010;&#30693;&#35782;&#29255;&#27573;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#23450;&#21046;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#12290;RAE&#39318;&#20808;&#26816;&#32034;&#32534;&#36753;&#21518;&#30340;&#20107;&#23454;&#65292;&#28982;&#21518;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#23436;&#21892;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26816;&#32034;&#26041;&#27861;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#26469;&#35782;&#21035;&#38142;&#24335;&#20107;&#23454;&#65292;&#32780;&#22825;&#30495;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25628;&#32034;&#21487;&#33021;&#20250;&#24573;&#30053;&#36825;&#20123;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#37319;&#29992;&#20102;&#20462;&#21098;&#31574;&#30053;&#65292;&#20174;&#26816;&#32034;&#21040;&#30340;&#20107;&#23454;&#20013;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#65292;&#36825;&#22686;&#24378;&#20102;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19631v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions. To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework tailored for multi-hop question answering. RAE first retrieves edited facts and then refines the language model through in-context learning. Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that na\"ive similarity-based searches might miss. Additionally, our framework incorporates a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the edi
&lt;/p&gt;</description></item><item><title>&#22312;&#26377;&#38480;&#25104;&#23545;&#20559;&#22909;&#27604;&#36739;&#19979;&#30740;&#31350;&#24230;&#37327;&#23398;&#20064;&#65292;&#34920;&#26126;&#34429;&#28982;&#26080;&#27861;&#23398;&#20064;&#21333;&#20010;&#29702;&#24819;&#39033;&#30446;&#65292;&#20294;&#24403;&#27604;&#36739;&#23545;&#35937;&#34920;&#29616;&#20986;&#20302;&#32500;&#32467;&#26500;&#26102;&#65292;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#38480;&#21046;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.19629</link><description>&lt;p&gt;
&#26377;&#38480;&#25104;&#23545;&#20559;&#22909;&#27604;&#36739;&#19979;&#30340;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Metric Learning from Limited Pairwise Preference Comparisons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19629
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#25104;&#23545;&#20559;&#22909;&#27604;&#36739;&#19979;&#30740;&#31350;&#24230;&#37327;&#23398;&#20064;&#65292;&#34920;&#26126;&#34429;&#28982;&#26080;&#27861;&#23398;&#20064;&#21333;&#20010;&#29702;&#24819;&#39033;&#30446;&#65292;&#20294;&#24403;&#27604;&#36739;&#23545;&#35937;&#34920;&#29616;&#20986;&#20302;&#32500;&#32467;&#26500;&#26102;&#65292;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#38480;&#21046;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#29702;&#24819;&#28857;&#27169;&#22411;&#19979;&#30340;&#20559;&#22909;&#27604;&#36739;&#20013;&#30340;&#24230;&#37327;&#23398;&#20064;&#65292;&#20854;&#20013;&#29992;&#25143;&#22914;&#26524;&#19968;&#20010;&#39033;&#30446;&#27604;&#20854;&#28508;&#22312;&#29702;&#24819;&#39033;&#30446;&#26356;&#25509;&#36817;&#65292;&#21017;&#26356;&#21916;&#27426;&#35813;&#39033;&#30446;&#12290;&#36825;&#20123;&#39033;&#30446;&#23884;&#20837;&#21040;&#20855;&#26377;&#26410;&#30693;&#39532;&#27663;&#36317;&#31163;&#30340;$\mathbb{R}^d$&#20013;&#65292;&#35813;&#36317;&#31163;&#22312;&#29992;&#25143;&#38388;&#20849;&#20139;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#27599;&#20010;&#29992;&#25143;$\mathcal{O}(d)$&#20010;&#25104;&#23545;&#27604;&#36739;&#21487;&#20197;&#21516;&#26102;&#24674;&#22797;&#24230;&#37327;&#21644;&#29702;&#24819;&#39033;&#30446;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#26377;$o(d)$&#30340;&#26377;&#38480;&#27604;&#36739;&#39044;&#31639;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21363;&#20351;&#24050;&#30693;&#23398;&#20064;&#21333;&#20010;&#29702;&#24819;&#39033;&#30446;&#29616;&#22312;&#19981;&#20877;&#21487;&#33021;&#65292;&#24230;&#37327;&#26159;&#21542;&#20173;&#28982;&#21487;&#20197;&#24674;&#22797;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#33324;&#26469;&#35828;&#65292;$o(d)$&#27604;&#36739;&#19981;&#20250;&#25581;&#31034;&#26377;&#20851;&#24230;&#37327;&#30340;&#20449;&#24687;&#65292;&#21363;&#20351;&#29992;&#25143;&#25968;&#37327;&#26080;&#38480;&#12290;&#28982;&#32780;&#65292;&#24403;&#27604;&#36739;&#30340;&#39033;&#30446;&#34920;&#29616;&#20986;&#20302;&#32500;&#32467;&#26500;&#26102;&#65292;&#27599;&#20010;&#29992;&#25143;&#37117;&#21487;&#20197;&#26377;&#21161;&#20110;&#23398;&#20064;&#38480;&#21046;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#24230;&#37327;&#65292;&#36825;&#26679;&#24230;&#37327;&#23601;&#21487;&#20197;&#34987;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19629v1 Announce Type: new  Abstract: We study metric learning from preference comparisons under the ideal point model, in which a user prefers an item over another if it is closer to their latent ideal item. These items are embedded into $\mathbb{R}^d$ equipped with an unknown Mahalanobis distance shared across users. While recent work shows that it is possible to simultaneously recover the metric and ideal items given $\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a limited budget of $o(d)$ comparisons. We study whether the metric can still be recovered, even though it is known that learning individual ideal items is now no longer possible. We show that in general, $o(d)$ comparisons reveals no information about the metric, even with infinitely many users. However, when comparisons are made over items that exhibit low-dimensional structure, each user can contribute to learning the metric restricted to a low-dimensional subspace so that the metric
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Top-$k$&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#25968;&#24863;&#30693;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#23454;&#20363;&#30456;&#20851;&#30340;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#25968;&#24863;&#30693;&#31639;&#27861;</title><link>https://arxiv.org/abs/2403.19625</link><description>&lt;p&gt;
&#22522;&#20110;Top-$k$&#20998;&#31867;&#21644;&#22522;&#25968;&#24863;&#30693;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Top-$k$ Classification and Cardinality-Aware Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19625
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Top-$k$&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#25968;&#24863;&#30693;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#23454;&#20363;&#30456;&#20851;&#30340;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#25968;&#24863;&#30693;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35814;&#32454;&#30740;&#31350;&#20102;Top-$k$&#20998;&#31867;&#65292;&#21363;&#39044;&#27979;&#36755;&#20837;&#30340;$k$&#20010;&#26368;&#26377;&#21487;&#33021;&#30340;&#31867;&#21035;&#65292;&#36229;&#36234;&#20102;&#21333;&#31867;&#21035;&#39044;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#20960;&#31181;&#27969;&#34892;&#30340;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#65292;&#22914;comp-sum&#21644;&#32422;&#26463;&#25439;&#22833;&#65292;&#23545;&#20110;Top-$k$&#25439;&#22833;&#20855;&#26377;&#20851;&#20110;$h$-&#19968;&#33268;&#24615;&#36793;&#30028;&#30340;&#25903;&#25345;&#12290;&#36825;&#20123;&#36793;&#30028;&#20445;&#35777;&#20102;&#20851;&#20110;&#20551;&#35774;&#38598;$H$&#30340;&#19968;&#33268;&#24615;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#38750;&#28176;&#36817;&#21644;&#29305;&#23450;&#20551;&#35774;&#38598;&#24615;&#36136;&#65292;&#25552;&#20379;&#27604;&#36125;&#21494;&#26031;&#19968;&#33268;&#24615;&#26356;&#24378;&#30340;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#20934;&#30830;&#24615;&#21644;&#22522;&#25968;$k$&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#23454;&#20363;&#30456;&#20851;&#30340;&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#24341;&#20837;&#20102;&#22522;&#25968;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#12290;&#23545;&#20110;&#36825;&#20123;&#20989;&#25968;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#25104;&#26412;&#25935;&#24863;comp-sum&#21644;&#32422;&#26463;&#26367;&#20195;&#25439;&#22833;&#65292;&#24314;&#31435;&#20102;&#23427;&#20204;&#30340;$H$-&#19968;&#33268;&#24615;&#36793;&#30028;&#21644;&#36125;&#21494;&#26031;&#19968;&#33268;&#24615;&#12290;&#26368;&#23567;&#21270;&#36825;&#20123;&#25439;&#22833;&#23548;&#33268;&#20102;&#26032;&#30340;&#22522;&#25968;&#24863;&#30693;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19625v1 Announce Type: new  Abstract: We present a detailed study of top-$k$ classification, the task of predicting the $k$ most probable classes for an input, extending beyond single-class prediction. We demonstrate that several prevalent surrogate loss functions in multi-class classification, such as comp-sum and constrained losses, are supported by $H$-consistency bounds with respect to the top-$k$ loss. These bounds guarantee consistency in relation to the hypothesis set $H$, providing stronger guarantees than Bayes-consistency due to their non-asymptotic and hypothesis-set specific nature. To address the trade-off between accuracy and cardinality $k$, we further introduce cardinality-aware loss functions through instance-dependent cost-sensitive learning. For these functions, we derive cost-sensitive comp-sum and constrained surrogate losses, establishing their $H$-consistency bounds and Bayes-consistency. Minimizing these losses leads to new cardinality-aware algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#21019;&#24847;&#23545;&#25239;&#32593;&#32476;&#21644;&#28436;&#21270;&#26041;&#27861;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#21327;&#20316;&#20114;&#21160;&#28436;&#21270;&#33402;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#32654;&#23398;&#21644;&#21327;&#20316;&#20132;&#20114;&#24335;&#20154;&#31867;&#35780;&#20272;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.19620</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#21327;&#20316;&#20114;&#21160;&#28436;&#21270;&#33402;&#26415;
&lt;/p&gt;
&lt;p&gt;
Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#21019;&#24847;&#23545;&#25239;&#32593;&#32476;&#21644;&#28436;&#21270;&#26041;&#27861;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#21327;&#20316;&#20114;&#21160;&#28436;&#21270;&#33402;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#32654;&#23398;&#21644;&#21327;&#20316;&#20132;&#20114;&#24335;&#20154;&#31867;&#35780;&#20272;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#22240;&#27492;&#34987;&#29992;&#20316;&#29983;&#25104;&#33402;&#26415;&#22270;&#20687;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#28041;&#21450;&#20174;&#23398;&#20064;&#30340;&#33402;&#26415;&#34920;&#24449;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#24456;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#24050;&#30693;&#20026;&#21019;&#24847;&#23545;&#25239;&#32593;&#32476;&#65288;CANs&#65289;&#30340;&#26550;&#26500;&#35757;&#32451;GANs&#29983;&#25104;&#21019;&#24847;&#22270;&#20687;&#65292;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#28436;&#21270;&#26041;&#27861;&#22312;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20869;&#23548;&#33322;&#20197;&#21457;&#29616;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#32654;&#23398;&#21644;&#21327;&#20316;&#20132;&#20114;&#24335;&#20154;&#31867;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#22312;&#20154;&#31867;&#20114;&#21160;&#35780;&#20272;&#26696;&#20363;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#20301;&#21442;&#19982;&#32773;&#35780;&#20272;&#30340;&#21327;&#20316;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#19968;&#31181;&#26088;&#22312;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#30340;&#26234;&#33021;&#31361;&#21464;&#36816;&#31639;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19620v1 Announce Type: cross  Abstract: Generative Adversarial Networks (GANs) have shown great success in generating high quality images and are thus used as one of the main approaches to generate art images. However, usually the image generation process involves sampling from the latent space of the learned art representations, allowing little control over the output. In this work, we first employ GANs that are trained to produce creative images using an architecture known as Creative Adversarial Networks (CANs), then, we employ an evolutionary approach to navigate within the latent space of the models to discover images. We use automatic aesthetic and collaborative interactive human evaluation metrics to assess the generated images. In the human interactive evaluation case, we propose a collaborative evaluation based on the assessments of several participants. Furthermore, we also experiment with an intelligent mutation operator that aims to improve the quality of the ima
&lt;/p&gt;</description></item><item><title>ILPO-Net&#26159;&#19968;&#31181;&#22788;&#29702;&#20219;&#24847;&#24418;&#29366;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#36816;&#31639;&#23545;&#23616;&#37096;&#31354;&#38388;&#27169;&#24335;&#26041;&#21521;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#22312;&#21508;&#31181;&#20307;&#31215;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.19612</link><description>&lt;p&gt;
ILPO-NET&#65306;&#29992;&#20110;&#19977;&#32500;&#20013;&#20219;&#24847;&#20307;&#31215;&#27169;&#24335;&#19981;&#21464;&#35782;&#21035;&#30340;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ILPO-NET: Network for the invariant recognition of arbitrary volumetric patterns in 3D
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19612
&lt;/p&gt;
&lt;p&gt;
ILPO-Net&#26159;&#19968;&#31181;&#22788;&#29702;&#20219;&#24847;&#24418;&#29366;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#36816;&#31639;&#23545;&#23616;&#37096;&#31354;&#38388;&#27169;&#24335;&#26041;&#21521;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#22312;&#21508;&#31181;&#20307;&#31215;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31354;&#38388;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#26377;&#25928;&#35782;&#21035;&#31354;&#38388;&#27169;&#24335;&#24182;&#23398;&#20064;&#20854;&#23618;&#27425;&#32467;&#26500;&#33267;&#20851;&#37325;&#35201;&#12290;&#20307;&#31215;&#25968;&#25454;&#24212;&#29992;&#23547;&#27714;&#30830;&#20445;&#23545;&#20301;&#31227;&#21644;&#27169;&#24335;&#26059;&#36716;&#22343;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#25216;&#26415;&#12290;ILPO-Net&#65288;Invariant to Local Patterns Orientation Network&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;Wigner&#30697;&#38453;&#23637;&#24320;&#65292;&#22312;&#21367;&#31215;&#25805;&#20316;&#20013;&#22788;&#29702;&#20219;&#24847;&#24418;&#29366;&#30340;&#27169;&#24335;&#65292;&#20174;&#32780;&#26412;&#36136;&#19978;&#23545;&#23616;&#37096;&#31354;&#38388;&#27169;&#24335;&#26041;&#21521;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#26080;&#32541;&#38598;&#25104;&#20102;&#26032;&#30340;&#21367;&#31215;&#36816;&#31639;&#31526;&#65292;&#22312;&#21508;&#31181;&#20307;&#31215;&#25968;&#25454;&#38598;&#65288;&#22914;MedMNIST&#21644;CATH&#65289;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#27604;&#22522;&#20934;&#32447;&#26356;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#20943;&#23569; - &#22312;MedMNIST&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#20102;&#39640;&#36798;1000&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19612v1 Announce Type: cross  Abstract: Effective recognition of spatial patterns and learning their hierarchy is crucial in modern spatial data analysis. Volumetric data applications seek techniques ensuring invariance not only to shifts but also to pattern rotations. While traditional methods can readily achieve translational invariance, rotational invariance possesses multiple challenges and remains an active area of research. Here, we present ILPO-Net (Invariant to Local Patterns Orientation Network), a novel approach that handles arbitrarily shaped patterns with the convolutional operation inherently invariant to local spatial pattern orientations using the Wigner matrix expansions. Our architecture seamlessly integrates the new convolution operator and, when benchmarked on diverse volumetric datasets such as MedMNIST and CATH, demonstrates superior performance over the baselines with significantly reduced parameter counts - up to 1000 times fewer in the case of MedMNIS
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#25968;&#25454;&#33258;&#36866;&#24212;&#36873;&#25321;&#38408;&#20540;&#21644;&#26435;&#34913;&#21442;&#25968;&#26102;&#26377;&#25928;&#25511;&#21046;&#39118;&#38505;&#30340;&#26041;&#27861;&#65292;&#25903;&#25345;&#21333;&#35843;&#21644;&#20960;&#20046;&#21333;&#35843;&#39118;&#38505;&#65292;&#26080;&#38656;&#20998;&#24067;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2403.19605</link><description>&lt;p&gt;
&#25968;&#25454;&#33258;&#36866;&#24212;&#39044;&#27979;&#20013;&#22810;&#37325;&#39118;&#38505;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Data-Adaptive Tradeoffs among Multiple Risks in Distribution-Free Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19605
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#25968;&#25454;&#33258;&#36866;&#24212;&#36873;&#25321;&#38408;&#20540;&#21644;&#26435;&#34913;&#21442;&#25968;&#26102;&#26377;&#25928;&#25511;&#21046;&#39118;&#38505;&#30340;&#26041;&#27861;&#65292;&#25903;&#25345;&#21333;&#35843;&#21644;&#20960;&#20046;&#21333;&#35843;&#39118;&#38505;&#65292;&#26080;&#38656;&#20998;&#24067;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#31649;&#36947;&#36890;&#24120;&#30001;&#21508;&#31181;&#39118;&#38505;&#20989;&#25968;&#20043;&#38388;&#30340;&#26435;&#34913;&#29305;&#24449;&#12290;&#36890;&#24120;&#24076;&#26395;&#20197;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#31649;&#29702;&#36825;&#20123;&#26435;&#34913;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#36825;&#26679;&#20570;&#24471;&#22826;&#24188;&#31258;&#65292;&#26368;&#20808;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#20250;&#23548;&#33268;&#23545;&#20551;&#23450;&#39118;&#38505;&#20445;&#35777;&#30340;&#37325;&#22823;&#36829;&#21453;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#33021;&#22815;&#22312;&#38408;&#20540;&#21644;&#26435;&#34913;&#21442;&#25968;&#33258;&#36866;&#24212;&#36873;&#25321;&#26102;&#20801;&#35768;&#26377;&#25928;&#25511;&#21046;&#39118;&#38505;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25903;&#25345;&#21333;&#35843;&#21644;&#20960;&#20046;&#21333;&#35843;&#30340;&#39118;&#38505;&#65292;&#20294;&#19981;&#20570;&#20219;&#20309;&#20998;&#24067;&#20551;&#35774;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#22823;&#35268;&#27169;&#35270;&#35273;&#25968;&#25454;&#38598;MS-COCO&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19605v1 Announce Type: cross  Abstract: Decision-making pipelines are generally characterized by tradeoffs among various risk functions. It is often desirable to manage such tradeoffs in a data-adaptive manner. As we demonstrate, if this is done naively, state-of-the art uncertainty quantification methods can lead to significant violations of putative risk guarantees.   To address this issue, we develop methods that permit valid control of risk when threshold and tradeoff parameters are chosen adaptively. Our methodology supports monotone and nearly-monotone risks, but otherwise makes no distributional assumptions.   To illustrate the benefits of our approach, we carry out numerical experiments on synthetic data and the large-scale vision dataset MS-COCO.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;GQA-LUT&#31639;&#27861;&#22312;&#21464;&#21387;&#22120;&#20013;&#30340;&#38750;&#32447;&#24615;&#25805;&#20316;&#20013;&#20855;&#26377;&#37327;&#21270;&#24863;&#30693;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;INT8-based LUT&#36924;&#36817;&#30340;&#24212;&#29992;&#65292;&#33410;&#32422;&#20102;&#22823;&#37327;&#30828;&#20214;&#21644;&#21151;&#32791;</title><link>https://arxiv.org/abs/2403.19591</link><description>&lt;p&gt;
&#22522;&#22240;&#37327;&#21270;&#24863;&#30693;&#36924;&#36817;&#29992;&#20110;&#21464;&#21387;&#22120;&#20013;&#30340;&#38750;&#32447;&#24615;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;GQA-LUT&#31639;&#27861;&#22312;&#21464;&#21387;&#22120;&#20013;&#30340;&#38750;&#32447;&#24615;&#25805;&#20316;&#20013;&#20855;&#26377;&#37327;&#21270;&#24863;&#30693;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;INT8-based LUT&#36924;&#36817;&#30340;&#24212;&#29992;&#65292;&#33410;&#32422;&#20102;&#22823;&#37327;&#30828;&#20214;&#21644;&#21151;&#32791;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#21387;&#22120;&#21450;&#20854;&#36731;&#37327;&#32423;&#21464;&#20307;&#20013;&#65292;&#38750;&#32447;&#24615;&#20989;&#25968;&#26222;&#36941;&#23384;&#22312;&#65292;&#23548;&#33268;&#30828;&#20214;&#25104;&#26412;&#26174;&#33879;&#19988;&#32463;&#24120;&#34987;&#20302;&#20272;&#12290;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#20316;&#21697;&#36890;&#36807;&#20998;&#27573;&#32447;&#24615;&#36924;&#36817;&#26469;&#20248;&#21270;&#36825;&#20123;&#25805;&#20316;&#65292;&#24182;&#23558;&#21442;&#25968;&#23384;&#20648;&#22312;&#26597;&#25214;&#34920;&#65288;LUT&#65289;&#20013;&#65292;&#20294;&#22823;&#22810;&#25968;&#38656;&#35201;&#19981;&#21451;&#22909;&#30340;&#39640;&#31934;&#24230;&#31639;&#26415;&#65292;&#22914;FP/INT 32&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#32431;&#25972;&#25968;INT&#37327;&#21270;&#30340;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36951;&#20256;LUT-&#36924;&#36817;&#31639;&#27861;&#65292;&#21363;GQA-LUT&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#30830;&#23450;&#20855;&#26377;&#37327;&#21270;&#24847;&#35782;&#30340;&#21442;&#25968;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#26222;&#36890;&#21644;&#32447;&#24615;Transformer&#27169;&#22411;&#30340;&#25361;&#25112;&#24615;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;GQA-LUT&#23454;&#29616;&#20102;&#21487;&#24573;&#30053;&#30340;&#24615;&#33021;&#38477;&#32423;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#30340;GQA-LUT&#20351;&#24471;&#33021;&#22815;&#20351;&#29992;&#22522;&#20110;INT8&#30340;LUT&#36924;&#36817;&#65292;&#30456;&#27604;&#39640;&#31934;&#24230;FP/INT 32&#65292;&#21487;&#20197;&#23454;&#29616;81.3~81.7%&#30340;&#38754;&#31215;&#33410;&#32422;&#21644;79.3~80.2%&#30340;&#21151;&#32791;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19591v1 Announce Type: new  Abstract: Non-linear functions are prevalent in Transformers and their lightweight variants, incurring substantial and frequently underestimated hardware costs. Previous state-of-the-art works optimize these operations by piece-wise linear approximation and store the parameters in look-up tables (LUT), but most of them require unfriendly high-precision arithmetics such as FP/INT 32 and lack consideration of integer-only INT quantization. This paper proposed a genetic LUT-Approximation algorithm namely GQA-LUT that can automatically determine the parameters with quantization awareness. The results demonstrate that GQA-LUT achieves negligible degradation on the challenging semantic segmentation task for both vanilla and linear Transformer models. Besides, proposed GQA-LUT enables the employment of INT8-based LUT-Approximation that achieves an area savings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the high-precision FP/INT 32 alte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#23494;&#38598;&#36830;&#25509;&#21367;&#31215;&#32593;&#32476;&#65288;DenseNets&#65289;&#65292;&#25581;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;ResNet&#39118;&#26684;&#26550;&#26500;&#30340;&#34987;&#20302;&#20272;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#12289;&#22359;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#24471;DenseNets&#21487;&#20197;&#19982;&#29616;&#20195;&#26550;&#26500;&#31454;&#20105;&#65292;&#24182;&#22312;ImageNet-1K&#19978;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.19588</link><description>&lt;p&gt;
DenseNets&#37325;&#29983;&#65306;&#36229;&#36234;ResNets&#21644;ViTs&#30340;&#33539;&#24335;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#23494;&#38598;&#36830;&#25509;&#21367;&#31215;&#32593;&#32476;&#65288;DenseNets&#65289;&#65292;&#25581;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;ResNet&#39118;&#26684;&#26550;&#26500;&#30340;&#34987;&#20302;&#20272;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#12289;&#22359;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#24471;DenseNets&#21487;&#20197;&#19982;&#29616;&#20195;&#26550;&#26500;&#31454;&#20105;&#65292;&#24182;&#22312;ImageNet-1K&#19978;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22797;&#33487;&#20102;&#23494;&#38598;&#36830;&#25509;&#21367;&#31215;&#32593;&#32476;&#65288;DenseNets&#65289;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30456;&#23545;&#20110;&#20027;&#23548;&#30340;ResNet&#39118;&#26684;&#26550;&#26500;&#34987;&#20302;&#20272;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;DenseNets&#30340;&#28508;&#21147;&#34987;&#24573;&#35270;&#65292;&#26159;&#22240;&#20026;&#26410;&#26366;&#35302;&#21450;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#20256;&#32479;&#35774;&#35745;&#20803;&#32032;&#26410;&#33021;&#23436;&#20840;&#23637;&#29616;&#20854;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36830;&#25509;&#30340;&#23494;&#38598;&#36830;&#25509;&#26159;&#24378;&#22823;&#30340;&#65292;&#34920;&#26126;DenseNets&#21487;&#20197;&#34987;&#37325;&#26032;&#28608;&#27963;&#20197;&#19982;&#29616;&#20195;&#26550;&#26500;&#31454;&#20105;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25913;&#36827;&#20102;&#27425;&#20248;&#32452;&#20214; - &#26550;&#26500;&#35843;&#25972;&#12289;&#22359;&#37325;&#26032;&#35774;&#35745;&#21644;&#25913;&#36827;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#20197;&#25193;&#23637;DenseNets&#24182;&#25552;&#39640;&#20869;&#23384;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#36830;&#25509;&#24555;&#25463;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#31616;&#21333;&#30340;&#26550;&#26500;&#20803;&#32032;&#65292;&#26368;&#32456;&#36229;&#36234;&#20102;Swin Transformer&#12289;ConvNeXt&#21644;DeiT-III - &#27531;&#24046;&#23398;&#20064;&#35889;&#31995;&#20013;&#30340;&#20851;&#38190;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;ImageNet-1K&#19978;&#23637;&#29616;&#20986;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#31454;&#20105;wi
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19588v1 Announce Type: cross  Abstract: This paper revives Densely Connected Convolutional Networks (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures. We believe DenseNets' potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities. Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures. We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts. Our models, employing simple architectural elements, ultimately surpass Swin Transformer, ConvNeXt, and DeiT-III - key architectures in the residual learning lineage. Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing wi
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#65288;KAT&#65289;&#26694;&#26550;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#65288;GPT-4 Turbo&#65289;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#21487;&#23454;&#29616;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#65292;&#23558;&#35270;&#35273;&#35266;&#27979;&#26144;&#23556;&#20026;&#27169;&#25311;&#31034;&#33539;&#32773;&#34892;&#20026;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#29616;&#26377;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19578</link><description>&lt;p&gt;
&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#23454;&#29616;&#19978;&#19979;&#25991;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19578
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#65288;KAT&#65289;&#26694;&#26550;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#65288;GPT-4 Turbo&#65289;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#21487;&#23454;&#29616;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#65292;&#23558;&#35270;&#35273;&#35266;&#27979;&#26144;&#23556;&#20026;&#27169;&#25311;&#31034;&#33539;&#32773;&#34892;&#20026;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#29616;&#26377;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#25104;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21464;&#24418;&#22120;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#23601;&#21487;&#20197;&#25191;&#34892;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#20869;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#65292;&#23558;&#35270;&#35273;&#35266;&#27979;&#26144;&#23556;&#20026;&#27169;&#25311;&#31034;&#33539;&#32773;&#34892;&#20026;&#30340;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35270;&#35273;&#35266;&#27979;&#65288;&#36755;&#20837;&#65289;&#21644;&#21160;&#20316;&#36712;&#36857;&#65288;&#36755;&#20986;&#65289;&#36716;&#25442;&#20026;&#19968;&#31995;&#21015;&#20196;&#29260;&#65292;&#36825;&#20123;&#20196;&#29260;&#21487;&#20197;&#34987;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#65288;GPT-4 Turbo&#65289;&#25509;&#25910;&#21644;&#29983;&#25104;&#65292;&#36890;&#36807;&#25105;&#20204;&#31216;&#20043;&#20026;&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#65288;KAT&#65289;&#30340;&#26694;&#26550;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#23613;&#31649;&#20165;&#22312;&#35821;&#35328;&#19978;&#35757;&#32451;&#65292;&#25105;&#20204;&#23637;&#31034;&#36825;&#20123;&#21464;&#24418;&#22120;&#25797;&#38271;&#23558;&#26631;&#35760;&#21270;&#30340;&#35270;&#35273;&#20851;&#38190;&#28857;&#35266;&#23519;&#32763;&#35793;&#20026;&#34892;&#20026;&#36712;&#36857;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#26085;&#24120;&#20219;&#21153;&#22871;&#20214;&#20013;&#65292;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#34920;&#29616;&#19982;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;&#25193;&#25955;&#31574;&#30053;&#65289;&#12290;KAT&#19981;&#21516;&#20110;&#36890;&#24120;&#22312;&#35821;&#35328;&#39046;&#22495;&#25805;&#20316;&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#25991;&#26412;&#30340;&#21464;&#24418;&#22120;&#22312;&#35270;&#35273;&#21644;&#21160;&#20316;&#39046;&#22495;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19578v1 Announce Type: cross  Abstract: We show that off-the-shelf text-based Transformers, with no additional training, can perform few-shot in-context visual imitation learning, mapping visual observations to action sequences that emulate the demonstrator's behaviour. We achieve this by transforming visual observations (inputs) and trajectories of actions (outputs) into sequences of tokens that a text-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a framework we call Keypoint Action Tokens (KAT). Despite being trained only on language, we show that these Transformers excel at translating tokenised visual keypoint observations into action trajectories, performing on par or better than state-of-the-art imitation learning (diffusion policies) in the low-data regime on a suite of real-world, everyday tasks. Rather than operating in the language domain as is typical, KAT leverages text-based Transformers to operate in the vision and action domains to learn ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;NN TSC&#65289;&#39044;&#27979;&#20891;&#20107;&#32972;&#26223;&#19979;&#32676;&#20307;&#33258;&#20027;&#20307;&#30340;&#20851;&#38190;&#23646;&#24615;&#21644;&#25112;&#26415;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;NN TSC&#22312;&#24555;&#36895;&#25512;&#26029;&#25915;&#20987;&#32676;&#20307;&#24773;&#25253;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19572</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#32676;&#20307;&#29305;&#24615;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Swarm Characteristics Classification Using Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;NN TSC&#65289;&#39044;&#27979;&#20891;&#20107;&#32972;&#26223;&#19979;&#32676;&#20307;&#33258;&#20027;&#20307;&#30340;&#20851;&#38190;&#23646;&#24615;&#21644;&#25112;&#26415;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;NN TSC&#22312;&#24555;&#36895;&#25512;&#26029;&#25915;&#20987;&#32676;&#20307;&#24773;&#25253;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#32676;&#20307;&#33258;&#20027;&#20307;&#30340;&#29305;&#24615;&#23545;&#20110;&#22269;&#38450;&#21644;&#23433;&#20840;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;NN TSC&#65289;&#26469;&#39044;&#27979;&#20891;&#20107;&#29615;&#22659;&#20013;&#32676;&#20307;&#33258;&#20027;&#20307;&#30340;&#20851;&#38190;&#23646;&#24615;&#21644;&#25112;&#26415;&#30340;&#30740;&#31350;&#12290;&#20855;&#20307;&#22320;&#65292;NN TSC&#34987;&#24212;&#29992;&#20110;&#25512;&#26029;&#20004;&#20010;&#20108;&#36827;&#21046;&#23646;&#24615; - &#36890;&#20449;&#21644;&#27604;&#20363;&#23548;&#33322; - &#36825;&#20004;&#32773;&#32467;&#21512;&#23450;&#20041;&#20102;&#22235;&#31181;&#20114;&#26021;&#30340;&#32676;&#20307;&#25112;&#26415;&#12290;&#25105;&#20204;&#21457;&#29616;&#25991;&#29486;&#20013;&#23545;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32676;&#20307;&#20998;&#31867;&#23384;&#22312;&#19968;&#23450;&#30340;&#31354;&#30333;&#65292;&#24182;&#23637;&#31034;&#20102;NN TSC&#22312;&#24555;&#36895;&#25512;&#26029;&#26377;&#20851;&#25915;&#20987;&#32676;&#20307;&#24773;&#25253;&#20197;&#25351;&#23548;&#21453;&#21046;&#21160;&#20316;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#27169;&#25311;&#30340;&#32676;&#20307;&#23545;&#25112;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;NN TSC&#22312;&#35266;&#23519;&#31383;&#21475;&#35201;&#27714;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#23545;&#32676;&#20307;&#35268;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#20851;&#38190;&#21457;&#29616;&#26174;&#31034;NN&#33021;&#22815;&#20351;&#29992;&#36739;&#30701;&#30340;&#35266;&#23519;&#31383;&#21475;&#20197;97%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#32676;&#20307;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19572v1 Announce Type: new  Abstract: Understanding the characteristics of swarming autonomous agents is critical for defense and security applications. This article presents a study on using supervised neural network time series classification (NN TSC) to predict key attributes and tactics of swarming autonomous agents for military contexts. Specifically, NN TSC is applied to infer two binary attributes - communication and proportional navigation - which combine to define four mutually exclusive swarm tactics. We identify a gap in literature on using NNs for swarm classification and demonstrate the effectiveness of NN TSC in rapidly deducing intelligence about attacking swarms to inform counter-maneuvers. Through simulated swarm-vs-swarm engagements, we evaluate NN TSC performance in terms of observation window requirements, noise robustness, and scalability to swarm size. Key findings show NNs can predict swarm behaviors with 97% accuracy using short observation windows of
&lt;/p&gt;</description></item><item><title>GrINd&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#25554;&#20540;&#23618;&#23558;&#31163;&#25955;&#35266;&#27979;&#25968;&#25454;&#26144;&#23556;&#21040;&#39640;&#20998;&#36776;&#29575;&#32593;&#26684;&#65292;&#23454;&#29616;&#20102;&#20174;&#31232;&#30095;&#21644;&#20998;&#25955;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#39044;&#27979;&#26102;&#31354;&#29289;&#29702;&#31995;&#32479;&#28436;&#21464;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19570</link><description>&lt;p&gt;
GrINd&#65306;&#32593;&#26684;&#25554;&#20540;&#32593;&#32476;&#29992;&#20110;&#31163;&#25955;&#35266;&#27979;
&lt;/p&gt;
&lt;p&gt;
GrINd: Grid Interpolation Network for Scattered Observations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19570
&lt;/p&gt;
&lt;p&gt;
GrINd&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#25554;&#20540;&#23618;&#23558;&#31163;&#25955;&#35266;&#27979;&#25968;&#25454;&#26144;&#23556;&#21040;&#39640;&#20998;&#36776;&#29575;&#32593;&#26684;&#65292;&#23454;&#29616;&#20102;&#20174;&#31232;&#30095;&#21644;&#20998;&#25955;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#39044;&#27979;&#26102;&#31354;&#29289;&#29702;&#31995;&#32479;&#28436;&#21464;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31232;&#30095;&#21644;&#20998;&#25955;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#39044;&#27979;&#26102;&#31354;&#29289;&#29702;&#31995;&#32479;&#30340;&#28436;&#21464;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#37117;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#23494;&#38598;&#30340;&#32593;&#26684;&#32467;&#26500;&#25968;&#25454;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#31232;&#30095;&#35266;&#27979;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GrINd&#65288;&#29992;&#20110;&#31163;&#25955;&#35266;&#27979;&#30340;&#32593;&#26684;&#25554;&#20540;&#32593;&#32476;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#20613;&#31435;&#21494;&#25554;&#20540;&#23618;&#23558;&#31163;&#25955;&#35266;&#27979;&#26144;&#23556;&#21040;&#39640;&#20998;&#36776;&#29575;&#32593;&#26684;&#65292;&#21457;&#25381;&#20102;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#30340;&#39640;&#24615;&#33021;&#12290;&#22312;&#39640;&#20998;&#36776;&#29575;&#31354;&#38388;&#20013;&#65292;&#19968;&#20010;NeuralPDE&#27169;&#22411;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;ODE&#27714;&#35299;&#22120;&#21644;&#23436;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#31995;&#32479;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;DynaBench&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;GrINd&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#21253;&#25324;&#20845;&#31181;&#22312;&#20998;&#25955;&#20301;&#32622;&#35266;&#27979;&#21040;&#30340;&#19981;&#21516;&#29289;&#29702;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20854;s&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19570v1 Announce Type: new  Abstract: Predicting the evolution of spatiotemporal physical systems from sparse and scattered observational data poses a significant challenge in various scientific domains. Traditional methods rely on dense grid-structured data, limiting their applicability in scenarios with sparse observations. To address this challenge, we introduce GrINd (Grid Interpolation Network for Scattered Observations), a novel network architecture that leverages the high-performance of grid-based models by mapping scattered observations onto a high-resolution grid using a Fourier Interpolation Layer. In the high-resolution space, a NeuralPDE-class model predicts the system's state at future timepoints using differentiable ODE solvers and fully convolutional neural networks parametrizing the system's dynamics. We empirically evaluate GrINd on the DynaBench benchmark dataset, comprising six different physical systems observed at scattered locations, demonstrating its s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#35774;&#35745;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.19561</link><description>&lt;p&gt;
&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;&#29992;&#20110;&#21487;&#25193;&#23637;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-Improved Learning for Scalable Neural Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#35774;&#35745;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
end-to-end&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;(NCO)&#26041;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#19987;&#23478;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#38382;&#39064;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#25105;&#25913;&#36827;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#19968;&#31181;&#21019;&#26032;&#30340;&#23616;&#37096;&#37325;&#26500;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#33258;&#36523;&#36845;&#20195;&#29983;&#25104;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#20197;&#25351;&#23548;&#26377;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19561v1 Announce Type: cross  Abstract: The end-to-end neural combinatorial optimization (NCO) method shows promising performance in solving complex combinatorial optimization problems without the need for expert design. However, existing methods struggle with large-scale problems, hindering their practical applicability. To overcome this limitation, this work proposes a novel Self-Improved Learning (SIL) method for better scalability of neural combinatorial optimization. Specifically, we develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without any labeled data. Powered by an innovative local reconstruction approach, this method can iteratively generate better solutions by itself as pseudo-labels to guide efficient model training. In addition, we design a linear complexity attention mechanism for the model to efficiently handle large-scale combinatorial problem instances with low computation overhead. Comprehens
&lt;/p&gt;</description></item><item><title>Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19546</link><description>&lt;p&gt;
Croissant&#65306;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
Croissant: A Metadata Format for ML-Ready Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19546
&lt;/p&gt;
&lt;p&gt;
Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20851;&#38190;&#36164;&#28304;&#65292;&#20294;&#22788;&#29702;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25705;&#25830;&#28857;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Croissant&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#34987;ML&#24037;&#20855;&#21644;&#26694;&#26550;&#20351;&#29992;&#30340;&#26041;&#24335;&#12290;Croissant&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;Croissant&#24050;&#24471;&#21040;&#20960;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#24211;&#30340;&#25903;&#25345;&#65292;&#28085;&#30422;&#25968;&#21313;&#19975;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#21152;&#36733;&#21040;&#26368;&#27969;&#34892;&#30340;ML&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19546v1 Announce Type: cross  Abstract: Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by ML tools and frameworks. Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in ML data management and responsible AI. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular ML frameworks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#37329;&#34701;&#26426;&#22120;&#20154;&#22312;&#20197;&#22826;&#22346;&#24179;&#21488;&#19978;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#37329;&#34701;&#26426;&#22120;&#20154;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#20102;&#26816;&#27979;&#24182;&#21019;&#24314;&#20102;&#22320;&#38754;&#23454;&#20917;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.19530</link><description>&lt;p&gt;
&#22312;&#20197;&#22826;&#22346;&#21306;&#22359;&#38142;&#19978;&#26816;&#27979;&#37329;&#34701;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Detecting Financial Bots on the Ethereum Blockchain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#37329;&#34701;&#26426;&#22120;&#20154;&#22312;&#20197;&#22826;&#22346;&#24179;&#21488;&#19978;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#37329;&#34701;&#26426;&#22120;&#20154;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#20102;&#26816;&#27979;&#24182;&#21019;&#24314;&#20102;&#22320;&#38754;&#23454;&#20917;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26426;&#22120;&#20154;&#22312;&#20998;&#24067;&#24335;&#36134;&#26412;&#25216;&#26415;&#65288;DLTs&#65289;&#20013;&#20419;&#36827;&#20102;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20351;&#29992;&#20063;&#19982;&#25504;&#22842;&#24615;&#20132;&#26131;&#21644;&#24066;&#22330;&#25805;&#32437;&#30456;&#20851;&#65292;&#24182;&#21487;&#33021;&#23545;&#31995;&#32479;&#23436;&#25972;&#24615;&#26500;&#25104;&#23041;&#32961;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;DLTs&#20013;&#26426;&#22120;&#20154;&#30340;&#37096;&#32626;&#31243;&#24230;&#33267;&#20851;&#37325;&#35201;&#65307;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#30340;&#26816;&#27979;&#31995;&#32479;&#20027;&#35201;&#22522;&#20110;&#35268;&#21017;&#65292;&#24182;&#19988;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#20197;&#22826;&#22346;&#24179;&#21488;&#19978;&#37329;&#34701;&#26426;&#22120;&#20154;&#30340;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31995;&#32479;&#21270;&#29616;&#26377;&#30340;&#31185;&#23398;&#25991;&#29486;&#24182;&#25910;&#38598;&#36726;&#20107;&#35777;&#25454;&#65292;&#20197;&#24314;&#31435;&#37329;&#34701;&#26426;&#22120;&#20154;&#30340;&#20998;&#31867;&#27861;&#65292;&#21253;&#25324;7&#20010;&#31867;&#21035;&#21644;24&#20010;&#23376;&#31867;&#21035;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;133&#20010;&#20154;&#31867;&#21644;137&#20010;&#26426;&#22120;&#20154;&#22320;&#22336;&#30340;&#22320;&#38754;&#23454;&#20917;&#25968;&#25454;&#38598;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#26816;&#27979;&#37096;&#32626;&#22312;&#20197;&#22826;&#22346;&#19978;&#30340;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19530v1 Announce Type: cross  Abstract: The integration of bots in Distributed Ledger Technologies (DLTs) fosters efficiency and automation. However, their use is also associated with predatory trading and market manipulation, and can pose threats to system integrity. It is therefore essential to understand the extent of bot deployment in DLTs; despite this, current detection systems are predominantly rule-based and lack flexibility. In this study, we present a novel approach that utilizes machine learning for the detection of financial bots on the Ethereum platform. First, we systematize existing scientific literature and collect anecdotal evidence to establish a taxonomy for financial bots, comprising 7 categories and 24 subcategories. Next, we create a ground-truth dataset consisting of 133 human and 137 bot addresses. Third, we employ both unsupervised and supervised machine learning algorithms to detect bots deployed on Ethereum. The highest-performing clustering algori
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21482;&#20351;&#29992;&#23569;&#37327;&#27169;&#22411;&#23601;&#33021;&#33719;&#24471;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26435;&#37325;&#31354;&#38388;&#21644;&#23618;&#27425;&#21152;&#26435;&#24179;&#22343;&#25216;&#26415;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27169;&#22411;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19522</link><description>&lt;p&gt;
&#27169;&#22411;&#24211;&#65306;&#25105;&#20204;&#21482;&#38656;&#35201;&#20960;&#20010;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Model Stock: All we need is just a few fine-tuned models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21482;&#20351;&#29992;&#23569;&#37327;&#27169;&#22411;&#23601;&#33021;&#33719;&#24471;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26435;&#37325;&#31354;&#38388;&#21644;&#23618;&#27425;&#21152;&#26435;&#24179;&#22343;&#25216;&#26415;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27169;&#22411;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20379;&#24378;&#22823;&#30340;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#21644;&#22806;&#20998;&#24067;&#65288;OOD&#65289;&#24615;&#33021;&#12290;&#19982;&#38656;&#35201;&#22823;&#37327;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#30340;&#20256;&#32479;&#20570;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26356;&#23569;&#30340;&#27169;&#22411;&#26469;&#33719;&#24471;&#26368;&#32456;&#26435;&#37325;&#65292;&#21516;&#26102;&#20135;&#29983;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20174;&#24494;&#35843;&#26435;&#37325;&#30340;&#26435;&#37325;&#31354;&#38388;&#20013;&#27762;&#21462;&#20851;&#38190;&#35265;&#35299;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24615;&#33021;&#21644;&#25509;&#36817;&#26435;&#37325;&#31354;&#38388;&#20013;&#24515;&#30340;&#24378;&#36830;&#25509;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#26469;&#36817;&#20284;&#20013;&#24515;&#25509;&#36817;&#30340;&#26435;&#37325;&#65292;&#21487;&#22312;&#35757;&#32451;&#26399;&#38388;&#25110;&#20043;&#21518;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#30340;&#36880;&#23618;&#26435;&#37325;&#24179;&#22343;&#25216;&#26415;&#36229;&#36234;&#20102;Model Soup&#31561;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26041;&#27861;&#65292;&#20165;&#21033;&#29992;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#34987;&#31216;&#20026;&#27169;&#22411;&#24211;&#65292;&#31361;&#20986;&#20102;&#23427;&#20381;&#36182;&#20110;&#36873;&#25321;&#23569;&#37327;&#27169;&#22411;&#26469;&#36827;&#34892;&#32508;&#21512;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19522v1 Announce Type: new  Abstract: This paper introduces an efficient fine-tuning method for large pre-trained models, offering strong in-distribution (ID) and out-of-distribution (OOD) performance. Breaking away from traditional practices that need a multitude of fine-tuned models for averaging, our approach employs significantly fewer models to achieve final weights yet yield superior accuracy. Drawing from key insights in the weight space of fine-tuned weights, we uncover a strong link between the performance and proximity to the center of weight space. Based on this, we introduce a method that approximates a center-close weight using only two fine-tuned models, applicable during or after training. Our innovative layer-wise weight averaging technique surpasses state-of-the-art model methods such as Model Soup, utilizing only two fine-tuned models. This strategy can be aptly coined Model Stock, highlighting its reliance on selecting a minimal number of models to draw a 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.19521</link><description>&lt;p&gt;
&#35299;&#37322;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20013;&#30340;&#20851;&#38190;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#25152;&#37319;&#29992;&#30340;&#26426;&#21046;&#12290;&#22312;&#38646;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#31867;&#20284;&#8220;&#27861;&#22269;&#30340;&#39318;&#37117;&#26159;&#8221;&#30340;&#25552;&#31034;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#24847;&#21147;&#22836;&#20250;&#20174;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20027;&#39064;&#23454;&#20307;&#65292;&#22914;&#8220;&#27861;&#22269;&#8221;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#21518;&#32493;&#30340;MLP&#20197;&#22238;&#24518;&#25152;&#38656;&#30340;&#31572;&#26696;&#65292;&#22914;&#8220;&#24052;&#40654;&#8221;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;MLP&#30340;&#36755;&#20986;&#20998;&#35299;&#20026;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#32452;&#20214;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36319;&#38543;&#36825;&#20123;&#29305;&#23450;&#20219;&#21153;&#22836;&#30340;MLP&#23618;&#30340;&#21151;&#33021;&#12290;&#22312;&#27531;&#24046;&#27969;&#20013;&#65292;&#23427;&#20250;&#25830;&#38500;&#25110;&#25918;&#22823;&#26469;&#33258;&#21508;&#20010;&#22836;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23427;&#20250;&#29983;&#25104;&#19968;&#20010;&#32452;&#20214;&#65292;&#23558;&#27531;&#24046;&#27969;&#37325;&#26032;&#23450;&#21521;&#21040;&#39044;&#26399;&#31572;&#26696;&#30340;&#26041;&#21521;&#12290;&#36825;&#20123;&#38646;&#27425;&#26426;&#21046;&#20063;&#36866;&#29992;&#20110;&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19521v1 Announce Type: cross  Abstract: In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like "The capital of France is," task-specific attention heads extract the topic entity, such as "France," from the context and pass it to subsequent MLPs to recall the required answer such as "Paris." We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#21521;&#22270;&#32858;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#21521;&#32858;&#31867;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19516</link><description>&lt;p&gt;
&#38024;&#23545;&#26377;&#21521;&#22270;&#32858;&#31867;&#38382;&#39064;&#30340;&#38543;&#26426;&#22359;&#27169;&#22411;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Maximum Likelihood Estimation on Stochastic Blockmodels for Directed Graph Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#21521;&#22270;&#32858;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#21521;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32479;&#35745;&#23398;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#26377;&#21521;&#22270;&#32858;&#31867;&#38382;&#39064;&#65292;&#23558;&#32858;&#31867;&#38382;&#39064;&#24314;&#27169;&#20026;&#26377;&#21521;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;DSBM&#65289;&#20013;&#28508;&#22312;&#31038;&#21306;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#23545;DSBM&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#65292;&#20174;&#32780;&#30830;&#23450;&#32473;&#23450;&#35266;&#23519;&#21040;&#30340;&#22270;&#32467;&#26500;&#26102;&#26368;&#21487;&#33021;&#30340;&#31038;&#21306;&#20998;&#37197;&#12290;&#38500;&#20102;&#32479;&#35745;&#35266;&#28857;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#20102;&#36825;&#31181;MLE&#20844;&#24335;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#20248;&#21270;&#21551;&#21457;&#24335;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#35813;&#21551;&#21457;&#24335;&#21516;&#26102;&#32771;&#34385;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#26377;&#21521;&#22270;&#32479;&#35745;&#37327;&#65306;&#36793;&#23494;&#24230;&#21644;&#36793;&#26041;&#21521;&#12290;&#22522;&#20110;&#36825;&#31181;&#26377;&#21521;&#32858;&#31867;&#30340;&#26032;&#20844;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#21521;&#32858;&#31867;&#31639;&#27861;&#65292;&#20998;&#21035;&#26159;&#35889;&#32858;&#31867;&#31639;&#27861;&#21644;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#20026;&#35889;&#32858;&#31867;&#31639;&#27861;&#30340;&#38169;&#35823;&#32858;&#31867;&#39030;&#28857;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19516v1 Announce Type: cross  Abstract: This paper studies the directed graph clustering problem through the lens of statistics, where we formulate clustering as estimating underlying communities in the directed stochastic block model (DSBM). We conduct the maximum likelihood estimation (MLE) on the DSBM and thereby ascertain the most probable community assignment given the observed graph structure. In addition to the statistical point of view, we further establish the equivalence between this MLE formulation and a novel flow optimization heuristic, which jointly considers two important directed graph statistics: edge density and edge orientation. Building on this new formulation of directed clustering, we introduce two efficient and interpretable directed clustering algorithms, a spectral clustering algorithm and a semidefinite programming based clustering algorithm. We provide a theoretical upper bound on the number of misclustered vertices of the spectral clustering algor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CDIMC-net&#65292;&#19968;&#20010;&#35748;&#30693;&#24335;&#28145;&#24230;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#32593;&#32476;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#22270;&#29305;&#23450;&#28145;&#24230;&#32534;&#30721;&#22120;&#21644;&#22270;&#23884;&#20837;&#31574;&#30053;&#65292;&#22312;&#26694;&#26550;&#20013;&#25429;&#33719;&#27599;&#20010;&#35270;&#22270;&#30340;&#39640;&#32423;&#29305;&#24449;&#21644;&#23616;&#37096;&#32467;&#26500;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#27169;&#22411;&#25153;&#24179;&#12289;&#23545;&#22122;&#22768;&#25110;&#24322;&#24120;&#20540;&#25935;&#24863;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19514</link><description>&lt;p&gt;
CDIMC-net: &#35748;&#30693;&#24335;&#28145;&#24230;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CDIMC-net: Cognitive Deep Incomplete Multi-view Clustering Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19514
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CDIMC-net&#65292;&#19968;&#20010;&#35748;&#30693;&#24335;&#28145;&#24230;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#32593;&#32476;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#22270;&#29305;&#23450;&#28145;&#24230;&#32534;&#30721;&#22120;&#21644;&#22270;&#23884;&#20837;&#31574;&#30053;&#65292;&#22312;&#26694;&#26550;&#20013;&#25429;&#33719;&#27599;&#20010;&#35270;&#22270;&#30340;&#39640;&#32423;&#29305;&#24449;&#21644;&#23616;&#37096;&#32467;&#26500;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#27169;&#22411;&#25153;&#24179;&#12289;&#23545;&#22122;&#22768;&#25110;&#24322;&#24120;&#20540;&#25935;&#24863;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20851;&#20110;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#30740;&#31350;&#27491;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#32570;&#22833;&#35270;&#22270;&#19978;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#20197;&#19979;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;1&#65289;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#27973;&#23618;&#27169;&#22411;&#65292;&#24456;&#38590;&#33719;&#24471;&#26377;&#21306;&#20998;&#21147;&#30340;&#20849;&#21516;&#34920;&#31034;&#12290;2&#65289;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23545;&#22122;&#22768;&#25110;&#24322;&#24120;&#20540;&#25935;&#24863;&#65292;&#22240;&#20026;&#36127;&#26679;&#26412;&#34987;&#35270;&#20026;&#19982;&#37325;&#35201;&#26679;&#26412;&#21516;&#31561;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#32593;&#32476;&#65292;&#31216;&#20026;&#35748;&#30693;&#24335;&#28145;&#24230;&#19981;&#23436;&#25972;&#22810;&#35270;&#22270;&#32858;&#31867;&#32593;&#32476;&#65288;CDIMC-net&#65289;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#36890;&#36807;&#23558;&#35270;&#22270;&#29305;&#23450;&#30340;&#28145;&#24230;&#32534;&#30721;&#22120;&#21644;&#22270;&#23884;&#20837;&#31574;&#30053;&#32435;&#20837;&#26694;&#26550;&#26469;&#25429;&#33719;&#27599;&#20010;&#35270;&#22270;&#30340;&#39640;&#32423;&#29305;&#24449;&#21644;&#23616;&#37096;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#20154;&#31867;&#35748;&#30693;&#65292;&#21363;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19514v1 Announce Type: cross  Abstract: In recent years, incomplete multi-view clustering, which studies the challenging multi-view clustering problem on missing views, has received growing research interests. Although a series of methods have been proposed to address this issue, the following problems still exist: 1) Almost all of the existing methods are based on shallow models, which is difficult to obtain discriminative common representations. 2) These methods are generally sensitive to noise or outliers since the negative samples are treated equally as the important samples. In this paper, we propose a novel incomplete multi-view clustering network, called Cognitive Deep Incomplete Multi-view Clustering Network (CDIMC-net), to address these issues. Specifically, it captures the high-level features and local structure of each view by incorporating the view-specific deep encoders and graph embedding strategy into a framework. Moreover, based on the human cognition, i.e., 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20351;&#29992;&#21463;&#25511;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#28040;&#38500;&#24515;&#33039;&#25104;&#20687;&#20013;&#30340;&#20559;&#24046;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19508</link><description>&lt;p&gt;
&#29992;&#21463;&#25511;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#28040;&#38500;&#24515;&#33039;&#25104;&#20687;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Debiasing Cardiac Imaging with Controlled Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19508
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20351;&#29992;&#21463;&#25511;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#28040;&#38500;&#24515;&#33039;&#25104;&#20687;&#20013;&#30340;&#20559;&#24046;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#23545;&#22522;&#20110;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#36827;&#34892;&#30142;&#30149;&#35786;&#26029;&#21644;&#39044;&#21518;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#36827;&#23637;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#39640;&#24230;&#19981;&#24179;&#34913;&#21644;&#20559;&#24046;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#20307;&#37325;&#25351;&#25968;&#21644;&#20581;&#24247;&#29366;&#20917;&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#20943;&#36731;&#25968;&#25454;&#38598;&#20013;&#22266;&#26377;&#30340;&#19981;&#24179;&#34913;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;ControlNet&#26469;&#20197;&#24739;&#32773;&#20803;&#25968;&#25454;&#21644;&#20351;&#29992;&#22823;&#22411;&#38431;&#21015;&#30740;&#31350;&#65288;&#20855;&#20307;&#26469;&#35828;&#26159;UK Biobank&#65289;&#20013;&#20998;&#21106;&#25513;&#27169;&#23548;&#20986;&#30340;&#24515;&#33039;&#20960;&#20309;&#24418;&#29366;&#20026;&#26465;&#20214;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;&#23450;&#37327;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#30340;&#36924;&#30495;&#31243;&#24230;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#30340;&#26679;&#26412;&#32416;&#27491;&#20195;&#34920;&#24615;&#19981;&#36275;&#32676;&#20307;&#20869;&#30340;&#19981;&#24179;&#34913;&#26469;&#25913;&#27491;&#20998;&#31867;&#22120;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#31034;&#33539;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19508v1 Announce Type: cross  Abstract: The progress in deep learning solutions for disease diagnosis and prognosis based on cardiac magnetic resonance imaging is hindered by highly imbalanced and biased training data. To address this issue, we propose a method to alleviate imbalances inherent in datasets through the generation of synthetic data based on sensitive attributes such as sex, age, body mass index, and health condition. We adopt ControlNet based on a denoising diffusion probabilistic model to condition on text assembled from patient metadata and cardiac geometry derived from segmentation masks using a large-cohort study, specifically, the UK Biobank. We assess our method by evaluating the realism of the generated images using established quantitative metrics. Furthermore, we conduct a downstream classification task aimed at debiasing a classifier by rectifying imbalances within underrepresented groups through synthetically generated samples. Our experiments demons
&lt;/p&gt;</description></item><item><title>SineNet&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#20010;&#20381;&#27425;&#36830;&#25509;&#30340;U-shaped&#32593;&#32476;&#22359;&#65288;&#27874;&#65289;&#22312;&#35299;&#20915;&#26102;&#38388;&#20381;&#36182;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#20943;&#23569;&#38169;&#20301;&#29305;&#24449;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.19507</link><description>&lt;p&gt;
SineNet&#65306;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#38388;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19507
&lt;/p&gt;
&lt;p&gt;
SineNet&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#20010;&#20381;&#27425;&#36830;&#25509;&#30340;U-shaped&#32593;&#32476;&#22359;&#65288;&#27874;&#65289;&#22312;&#35299;&#20915;&#26102;&#38388;&#20381;&#36182;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#20943;&#23569;&#38169;&#20301;&#29305;&#24449;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#26102;&#38388;&#20381;&#36182;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#65292;&#20854;&#20013;&#22810;&#23610;&#24230;&#22788;&#29702;&#23545;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#38388;&#28436;&#21464;&#21160;&#24577;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#20351;&#29992;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;U-Net&#26550;&#26500;&#26469;&#23454;&#29616;&#22810;&#23610;&#24230;&#22788;&#29702;&#65292;&#20294;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#29305;&#24449;&#38656;&#35201;&#36328;&#23618;&#28436;&#21464;&#23548;&#33268;&#36339;&#36291;&#36830;&#25509;&#20013;&#23384;&#22312;&#26102;&#38388;&#38169;&#20301;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SineNet&#65292;&#30001;&#22810;&#20010;&#20381;&#27425;&#36830;&#25509;&#30340;U&#24418;&#32593;&#32476;&#22359;&#32452;&#25104;&#65292;&#31216;&#20026;&#27874;&#12290;&#22312;SineNet&#20013;&#65292;&#39640;&#20998;&#36776;&#29575;&#29305;&#24449;&#36890;&#36807;&#22810;&#20010;&#38454;&#27573;&#36880;&#28176;&#28436;&#21464;&#65292;&#20174;&#32780;&#20943;&#23569;&#27599;&#20010;&#38454;&#27573;&#20869;&#30340;&#38169;&#20301;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#36339;&#36291;&#36830;&#25509;&#22312;&#23454;&#29616;&#22810;&#23610;&#24230;&#20449;&#24687;&#30340;&#24182;&#34892;&#21644;&#36830;&#32493;&#22788;&#29702;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;PDE&#19978;&#32463;&#36807;&#20005;&#26684;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19507v1 Announce Type: new  Abstract: We consider using deep neural networks to solve time-dependent partial differential equations (PDEs), where multi-scale processing is crucial for modeling complex, time-evolving dynamics. While the U-Net architecture with skip connections is commonly used by prior studies to enable multi-scale processing, our analysis shows that the need for features to evolve across layers results in temporally misaligned features in skip connections, which limits the model's performance. To address this limitation, we propose SineNet, consisting of multiple sequentially connected U-shaped network blocks, referred to as waves. In SineNet, high-resolution features are evolved progressively through multiple stages, thereby reducing the amount of misalignment within each stage. We furthermore analyze the role of skip connections in enabling both parallel and sequential processing of multi-scale information. Our method is rigorously tested on multiple PDE d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;Canonical Polyadic Decomposition&#21644;Tensor Train&#32422;&#26463;&#30340;&#26680;&#26426;&#22120;&#30340;&#36755;&#20986;&#20250;&#22312;&#23545;&#21442;&#25968;&#36827;&#34892;i.i.d.&#20808;&#39564;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#20026;&#39640;&#26031;&#36807;&#31243;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;Tensor Train&#27169;&#22411;&#30456;&#23545;&#20110;Canonical Polyadic Decomposition&#27169;&#22411;&#20855;&#26377;&#26356;&#22810;&#39640;&#26031;&#36807;&#31243;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.19500</link><description>&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#32422;&#26463;&#30340;&#26680;&#26426;&#22120;&#20316;&#20026;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Tensor Network-Constrained Kernel Machines as Gaussian Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;Canonical Polyadic Decomposition&#21644;Tensor Train&#32422;&#26463;&#30340;&#26680;&#26426;&#22120;&#30340;&#36755;&#20986;&#20250;&#22312;&#23545;&#21442;&#25968;&#36827;&#34892;i.i.d.&#20808;&#39564;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#20026;&#39640;&#26031;&#36807;&#31243;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;Tensor Train&#27169;&#22411;&#30456;&#23545;&#20110;Canonical Polyadic Decomposition&#27169;&#22411;&#20855;&#26377;&#26356;&#22810;&#39640;&#26031;&#36807;&#31243;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#65288;TNs&#65289;&#26368;&#36817;&#34987;&#29992;&#26469;&#36890;&#36807;&#32422;&#26463;&#27169;&#22411;&#26435;&#37325;&#21152;&#24555;&#26680;&#26426;&#22120;&#30340;&#36895;&#24230;&#65292;&#20135;&#29983;&#20102;&#25351;&#25968;&#32423;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#33410;&#32422;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;Canonical Polyadic Decomposition&#65288;CPD&#65289;&#21644;Tensor Train&#65288;TT&#65289;&#32422;&#26463;&#30340;&#26680;&#26426;&#22120;&#30340;&#36755;&#20986;&#20250;&#22312;&#23545;&#21442;&#25968;&#36827;&#34892;i.i.d.&#20808;&#39564;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#20026;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#65292;&#25105;&#20204;&#23436;&#20840;&#34920;&#24449;&#20102;&#36825;&#19968;&#36807;&#31243;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;CPD&#21644;TT&#32422;&#26463;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;TT&#30456;&#23545;&#20110;CPD&#20855;&#26377;&#26356;&#22810;GP&#34892;&#20026;&#30340;&#27169;&#22411;&#65292;&#32780;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#30456;&#21516;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#25968;&#20540;&#23454;&#39564;&#22312;&#20004;&#20010;&#26041;&#38754;&#23454;&#35777;&#35266;&#23519;&#20102;&#36825;&#19968;&#34892;&#20026;&#65292;&#20998;&#21035;&#26159;&#20998;&#26512;&#21040;GP&#30340;&#25910;&#25947;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24352;&#37327;&#32593;&#32476;&#32422;&#26463;&#30340;&#26680;&#26426;&#22120;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19500v1 Announce Type: new  Abstract: Tensor Networks (TNs) have recently been used to speed up kernel machines by constraining the model weights, yielding exponential computational and storage savings. In this paper we prove that the outputs of Canonical Polyadic Decomposition (CPD) and Tensor Train (TT)-constrained kernel machines recover a Gaussian Process (GP), which we fully characterize, when placing i.i.d. priors over their parameters. We analyze the convergence of both CPD and TT-constrained models, and show how TT yields models exhibiting more GP behavior compared to CPD, for the same number of model parameters. We empirically observe this behavior in two numerical experiments where we respectively analyze the convergence to the GP and the performance at prediction. We thereby establish a connection between TN-constrained kernel machines and GPs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23458;&#25143;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#19982;&#20010;&#24615;&#21270;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19499</link><description>&lt;p&gt;
&#23458;&#25143;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#36808;&#21521;&#19968;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Client-supervised Federated Learning: Towards One-model-for-all Personalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19499
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23458;&#25143;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#23398;&#20064;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#19982;&#20010;&#24615;&#21270;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PerFL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#19979;&#20026;&#19981;&#21516;&#23458;&#25143;&#20132;&#20184;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#22823;&#22810;&#25968;PerFL&#26041;&#27861;&#38656;&#35201;&#22312;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#39069;&#22806;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#20351;&#29992;&#20854;&#33258;&#24049;&#30340;&#26412;&#22320;&#25968;&#25454;&#35843;&#25972;&#20840;&#29699;&#20849;&#20139;&#27169;&#22411;&#20197;&#36866;&#24212;&#23458;&#25143;&#29305;&#23450;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;PerFL&#20013;&#30340;&#27169;&#22411;&#36866;&#24212;&#36807;&#31243;&#22312;&#27169;&#22411;&#37096;&#32626;&#21644;&#27979;&#35797;&#26102;&#38388;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#24037;&#20316;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#23398;&#20064;&#21482;&#19968;&#20010;&#24378;&#22823;&#20840;&#29699;&#27169;&#22411;&#65292;&#20197;&#22312;FL&#31995;&#32479;&#20013;&#23454;&#29616;&#19982;&#37027;&#20123;&#20010;&#24615;&#21270;&#27169;&#22411;&#22312;&#26410;&#30693;/&#27979;&#35797;&#23458;&#25143;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#23458;&#25143;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#65288;FedCS&#65289;&#26469;&#25581;&#31034;&#23458;&#25143;&#23545;&#23454;&#20363;&#28508;&#22312;&#34920;&#31034;&#30340;&#20559;&#35265;&#65292;&#20197;&#20415;&#20840;&#29699;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#23458;&#25143;&#29305;&#23450;&#21644;&#23458;&#25143;&#19981;&#21487;&#30693;&#30340;&#30693;&#35782;&#12290;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;FedCS&#21487;&#20197;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19499v1 Announce Type: new  Abstract: Personalized Federated Learning (PerFL) is a new machine learning paradigm that delivers personalized models for diverse clients under federated learning settings. Most PerFL methods require extra learning processes on a client to adapt a globally shared model to the client-specific personalized model using its own local data. However, the model adaptation process in PerFL is still an open challenge in the stage of model deployment and test time. This work tackles the challenge by proposing a novel federated learning framework to learn only one robust global model to achieve competitive performance to those personalized models on unseen/test clients in the FL system. Specifically, we design a new Client-Supervised Federated Learning (FedCS) to unravel clients' bias on instances' latent representations so that the global model can learn both client-specific and client-agnostic knowledge. Experimental study shows that the FedCS can learn a
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#22238;&#24402;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#28041;&#21450;&#23558;&#39044;&#27979;&#25512;&#36831;&#32473;&#22810;&#20010;&#19987;&#23478;&#65292;&#25552;&#20986;&#20102;&#21333;&#38454;&#27573;&#21644;&#21452;&#38454;&#27573;&#24773;&#26223;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#21450;&#20854;&#25903;&#25345;&#30340;&#19968;&#33268;&#24615;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.19494</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#19987;&#23478;&#25512;&#36831;&#30340;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Regression with Multi-Expert Deferral
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19494
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#22238;&#24402;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#28041;&#21450;&#23558;&#39044;&#27979;&#25512;&#36831;&#32473;&#22810;&#20010;&#19987;&#23478;&#65292;&#25552;&#20986;&#20102;&#21333;&#38454;&#27573;&#21644;&#21452;&#38454;&#27573;&#24773;&#26223;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#21450;&#20854;&#25903;&#25345;&#30340;&#19968;&#33268;&#24615;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19982;&#22810;&#20010;&#19987;&#23478;&#25512;&#36831;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#21487;&#20197;&#36873;&#25321;&#23558;&#39044;&#27979;&#25512;&#36831;&#32473;&#22810;&#20010;&#19987;&#23478;&#12290;&#34429;&#28982;&#22312;&#20998;&#31867;&#24773;&#22659;&#20013;&#35813;&#38382;&#39064;&#24471;&#21040;&#20102;&#37325;&#35270;&#65292;&#20294;&#30001;&#20110;&#26631;&#31614;&#31354;&#38388;&#30340;&#26080;&#38480;&#21644;&#36830;&#32493;&#29305;&#24615;&#65292;&#23427;&#22312;&#22238;&#24402;&#20013;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#25512;&#36831;&#30340;&#22238;&#24402;&#26032;&#26694;&#26550;&#65292;&#20854;&#20013;&#28041;&#21450;&#23558;&#39044;&#27979;&#25512;&#36831;&#32473;&#22810;&#20010;&#19987;&#23478;&#12290;&#25105;&#20204;&#38024;&#23545;&#21333;&#38454;&#27573;&#24773;&#26223;&#21644;&#21452;&#38454;&#27573;&#24773;&#26223;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21069;&#32773;&#28041;&#21450;&#39044;&#27979;&#22120;&#21644;&#25512;&#36831;&#20989;&#25968;&#30340;&#21516;&#26102;&#23398;&#20064;&#65292;&#21518;&#32773;&#28041;&#21450;&#20855;&#26377;&#24050;&#35757;&#32451;&#39044;&#27979;&#22120;&#21644;&#23398;&#20064;&#25512;&#36831;&#20989;&#25968;&#12290;&#25105;&#20204;&#20026;&#20004;&#31181;&#24773;&#26223;&#24341;&#20837;&#20102;&#26032;&#30340;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21463;&#21040;$H$-&#19968;&#33268;&#24615;&#30028;&#38480;&#30340;&#25903;&#25345;&#12290;&#36825;&#20123;&#30028;&#38480;&#25552;&#20379;&#20102;&#27604;&#36125;&#21494;&#26031;&#19968;&#33268;&#24615;&#26356;&#24378;&#30340;&#19968;&#33268;&#24615;&#20445;&#35777;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#38750;&#28176;&#36817;&#30340;&#65292;&#19988;&#20551;&#35774;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19494v1 Announce Type: new  Abstract: Learning to defer with multiple experts is a framework where the learner can choose to defer the prediction to several experts. While this problem has received significant attention in classification contexts, it presents unique challenges in regression due to the infinite and continuous nature of the label space. In this work, we introduce a novel framework of regression with deferral, which involves deferring the prediction to multiple experts. We present a comprehensive analysis for both the single-stage scenario, where there is simultaneous learning of predictor and deferral functions, and the two-stage scenario, which involves a pre-trained predictor with a learned deferral function. We introduce new surrogate loss functions for both scenarios and prove that they are supported by $H$-consistency bounds. These bounds provide consistency guarantees that are stronger than Bayes consistency, as they are non-asymptotic and hypothesis set
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;$H$&#19968;&#33268;&#24615;&#30028;&#24037;&#20855;&#26469;&#20998;&#26512;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#24179;&#26041;&#25439;&#22833;&#25512;&#23548;&#20986;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;$H$&#19968;&#33268;&#24615;&#30028;&#65307;&#22522;&#20110;&#23545;$H$&#19968;&#33268;&#24615;&#30340;&#20998;&#26512;&#65292;&#20026;&#23545;&#25239;&#24615;&#22238;&#24402;&#25552;&#20379;&#20102;&#26377;&#21407;&#21017;&#30340;&#20195;&#29702;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2403.19480</link><description>&lt;p&gt;
&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;$H$&#19968;&#33268;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
$H$-Consistency Guarantees for Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19480
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;$H$&#19968;&#33268;&#24615;&#30028;&#24037;&#20855;&#26469;&#20998;&#26512;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#24179;&#26041;&#25439;&#22833;&#25512;&#23548;&#20986;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;$H$&#19968;&#33268;&#24615;&#30028;&#65307;&#22522;&#20110;&#23545;$H$&#19968;&#33268;&#24615;&#30340;&#20998;&#26512;&#65292;&#20026;&#23545;&#25239;&#24615;&#22238;&#24402;&#25552;&#20379;&#20102;&#26377;&#21407;&#21017;&#30340;&#20195;&#29702;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;$H$&#19968;&#33268;&#24615;&#30028;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#23450;&#29702;&#65292;&#36825;&#20123;&#23450;&#29702;&#25512;&#24191;&#20102;&#20808;&#21069;&#29992;&#20110;&#24314;&#31435;$H$&#19968;&#33268;&#24615;&#30028;&#30340;&#24037;&#20855;&#12290;&#36825;&#31181;&#27010;&#25324;&#23545;&#20110;&#20998;&#26512;&#38024;&#23545;&#22238;&#24402;&#38382;&#39064;&#29305;&#23450;&#30340;$H$&#19968;&#33268;&#24615;&#30028;&#33267;&#20851;&#37325;&#35201;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#23545;&#31216;&#20998;&#24067;&#21644;&#26377;&#30028;&#20551;&#35774;&#38598;&#30340;&#26465;&#20214;&#19979;&#65292;&#35777;&#26126;&#20102;&#19968;&#31995;&#21015;&#20851;&#20110;&#24179;&#26041;&#25439;&#22833;&#30340;&#26032;&#39062;$H$&#19968;&#33268;&#24615;&#30028;&#65292;&#21253;&#25324;Huber&#25439;&#22833;&#12289;&#25152;&#26377;$\ell_p$&#25439;&#22833;&#65288;$p \geq 1$&#65289;&#12289;&#24179;&#26041;$\epsilon$-&#19981;&#25935;&#24863;&#25439;&#22833;&#30340;&#27491;&#32467;&#26524;&#65292;&#20197;&#21450;&#23545;&#20110;&#22312;&#24179;&#26041;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#20013;&#20351;&#29992;&#30340;$\epsilon$-&#19981;&#25935;&#24863;&#25439;&#22833;&#30340;&#36127;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#23545;&#22238;&#24402;&#38382;&#39064;&#20013;$H$&#19968;&#33268;&#24615;&#30340;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#23545;&#25239;&#22238;&#24402;&#30340;&#26377;&#21407;&#21017;&#30340;&#20195;&#29702;&#25439;&#22833;&#65288;&#31532;5&#33410;&#65289;&#12290;&#36825;&#20026;&#23545;&#25239;&#24615;&#22238;&#24402;&#24314;&#31435;&#20102;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#26377;&#21033;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19480v1 Announce Type: new  Abstract: We present a detailed study of $H$-consistency bounds for regression. We first present new theorems that generalize the tools previously given to establish $H$-consistency bounds. This generalization proves essential for analyzing $H$-consistency bounds specific to regression. Next, we prove a series of novel $H$-consistency bounds for surrogate loss functions of the squared loss, under the assumption of a symmetric distribution and a bounded hypothesis set. This includes positive results for the Huber loss, all $\ell_p$ losses, $p \geq 1$, the squared $\epsilon$-insensitive loss, as well as a negative result for the $\epsilon$-insensitive loss used in squared Support Vector Regression (SVR). We further leverage our analysis of $H$-consistency for regression and derive principled surrogate losses for adversarial regression (Section 5). This readily establishes novel algorithms for adversarial regression, for which we report favorable exp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#38480;&#23380;&#24452;&#36870;&#38556;&#30861;&#25955;&#23556;&#38382;&#39064;&#30340;&#28145;&#24230;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25552;&#20379;&#19982;&#25955;&#23556;&#27169;&#22411;&#30456;&#20851;&#32852;&#30340;&#29289;&#29702;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#22312;&#36870;&#38382;&#39064;&#19978;&#24037;&#20316;&#65292;&#24182;&#36991;&#20813;&#25197;&#26354;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.19470</link><description>&lt;p&gt;
&#26377;&#38480;&#23380;&#24452;&#36870;&#38556;&#30861;&#25955;&#23556;&#38382;&#39064;&#30340;&#28145;&#24230;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep decomposition method for the limited aperture inverse obstacle scattering problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19470
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#38480;&#23380;&#24452;&#36870;&#38556;&#30861;&#25955;&#23556;&#38382;&#39064;&#30340;&#28145;&#24230;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25552;&#20379;&#19982;&#25955;&#23556;&#27169;&#22411;&#30456;&#20851;&#32852;&#30340;&#29289;&#29702;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#22312;&#36870;&#38382;&#39064;&#19978;&#24037;&#20316;&#65292;&#24182;&#36991;&#20813;&#25197;&#26354;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#38024;&#23545;&#26377;&#38480;&#23380;&#24452;&#36870;&#38556;&#30861;&#25955;&#23556;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#20165;&#20381;&#36182;&#25968;&#25454;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#65292;&#24403;&#21482;&#26377;&#38388;&#25509;&#35266;&#27979;&#25968;&#25454;&#21644;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#21487;&#29992;&#26102;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20854;&#22312;&#36870;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#38754;&#23545;&#36825;&#20123;&#23616;&#38480;&#24615;&#26102;&#65292;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#26159;&#21542;&#21487;&#33021;&#20351;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#36870;&#38382;&#39064;&#65292;&#24182;&#19988;&#20102;&#35299;&#23427;&#27491;&#22312;&#23398;&#20064;&#30340;&#20869;&#23481;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#36825;&#20123;&#30446;&#30340;&#30340;&#28145;&#24230;&#20998;&#35299;&#26041;&#27861;&#65288;DDM&#65289;&#65292;&#23427;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#12290;&#23427;&#36890;&#36807;&#21521;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25552;&#20379;&#19982;&#25955;&#23556;&#27169;&#22411;&#30456;&#20851;&#32852;&#30340;&#29289;&#29702;&#36816;&#31639;&#31526;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;DDM&#20013;&#36824;&#23454;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#23436;&#25972;&#24615;&#26041;&#26696;&#65292;&#20197;&#38450;&#27490;&#25197;&#26354;&#26377;&#38480;&#23380;&#24452;&#25968;&#25454;&#30340;&#36870;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#38500;&#20102;&#35299;&#20915;i
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19470v1 Announce Type: cross  Abstract: In this paper, we consider a deep learning approach to the limited aperture inverse obstacle scattering problem. It is well known that traditional deep learning relies solely on data, which may limit its performance for the inverse problem when only indirect observation data and a physical model are available. A fundamental question arises in light of these limitations: is it possible to enable deep learning to work on inverse problems without labeled data and to be aware of what it is learning? This work proposes a deep decomposition method (DDM) for such purposes, which does not require ground truth labels. It accomplishes this by providing physical operators associated with the scattering model to the neural network architecture. Additionally, a deep learning based data completion scheme is implemented in DDM to prevent distorting the solution of the inverse problem for limited aperture data. Furthermore, apart from addressing the i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31163;&#32447;&#22810;&#22522;&#32447;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#32534;&#35793;&#22120;&#20248;&#21270;&#39046;&#22495;&#23454;&#29616;&#20102;&#36229;&#36234;&#26631;&#20934;RL&#30340;&#31574;&#30053;&#23398;&#20064;&#25928;&#26524;</title><link>https://arxiv.org/abs/2403.19462</link><description>&lt;p&gt;
&#31163;&#32447;&#22810;&#22522;&#32447;&#27169;&#20223;&#23398;&#20064;&#21450;&#20854;&#22312;&#32534;&#35793;&#22120;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Offline Imitation Learning from Multiple Baselines with Applications to Compiler Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19462
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#22810;&#22522;&#32447;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#32534;&#35793;&#22120;&#20248;&#21270;&#39046;&#22495;&#23454;&#29616;&#20102;&#36229;&#36234;&#26631;&#20934;RL&#30340;&#31574;&#30053;&#23398;&#20064;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#20854;&#20013;&#32473;&#23450;&#20102;&#20351;&#29992;K&#20010;&#22522;&#32447;&#31574;&#30053;&#25910;&#38598;&#30340;&#19968;&#32452;&#36712;&#36857;&#12290;&#36825;&#20123;&#31574;&#30053;&#20013;&#30340;&#27599;&#19968;&#20010;&#22312;&#21333;&#29420;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#30456;&#24403;&#27425;&#20248;&#65292;&#24182;&#19988;&#22312;&#29366;&#24577;&#31354;&#38388;&#30340;&#20114;&#34917;&#37096;&#20998;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#24615;&#33021;&#12290;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#65292;&#20351;&#20854;&#22312;&#25972;&#20010;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340;&#34920;&#29616;&#19982;&#26368;&#20339;&#32452;&#21512;&#22522;&#32447;&#30456;&#24403;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#27169;&#20223;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#20934;&#30830;&#24615;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#23637;&#31034;&#19968;&#31181;&#21305;&#37197;&#30340;&#19979;&#30028;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26497;&#23567;-&#26497;&#22823;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#24341;&#23548;&#32534;&#35793;&#22120;&#20248;&#21270;&#30340;&#35774;&#32622;&#20013;&#24212;&#29992;&#35813;&#31639;&#27861;&#65292;&#23398;&#20064;&#29992;&#20110;&#20869;&#32852;&#31243;&#24207;&#30340;&#31574;&#30053;&#65292;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#23567;&#30340;&#20108;&#36827;&#21046;&#25991;&#20214;&#12290;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20960;&#27425;&#36845;&#20195;&#23398;&#20064;&#21040;&#19968;&#20010;&#20248;&#20110;&#36890;&#36807;&#26631;&#20934;RL&#23398;&#21040;&#30340;&#21021;&#22987;&#31574;&#30053;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19462v1 Announce Type: new  Abstract: This work studies a Reinforcement Learning (RL) problem in which we are given a set of trajectories collected with K baseline policies. Each of these policies can be quite suboptimal in isolation, and have strong performance in complementary parts of the state space. The goal is to learn a policy which performs as well as the best combination of baselines on the entire state space. We propose a simple imitation learning based algorithm, show a sample complexity bound on its accuracy and prove that the the algorithm is minimax optimal by showing a matching lower bound. Further, we apply the algorithm in the setting of machine learning guided compiler optimization to learn policies for inlining programs with the objective of creating a small binary. We demonstrate that we can learn a policy that outperforms an initial policy learned via standard RL through a few iterations of our approach.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22522;&#20110;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#33258;&#28982;&#26799;&#24230;&#26041;&#27861;&#22312;&#32447;&#24615;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#29616;&#26377;&#32467;&#26524;&#30340;&#29109;&#27491;&#21017;&#21270;&#35823;&#24046;&#20272;&#35745;&#65292;&#24182;&#23545;&#25200;&#21160;&#30340;Fisher-Rao&#26799;&#24230;&#27969;&#21644;&#33258;&#28982;&#26799;&#24230;&#27969;&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.19448</link><description>&lt;p&gt;
Fisher-Rao&#32447;&#24615;&#35268;&#21010;&#21644;&#29366;&#24577;-&#21160;&#20316;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#30340;&#26799;&#24230;&#27969;
&lt;/p&gt;
&lt;p&gt;
Fisher-Rao Gradient Flows of Linear Programs and State-Action Natural Policy Gradients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19448
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22522;&#20110;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#33258;&#28982;&#26799;&#24230;&#26041;&#27861;&#22312;&#32447;&#24615;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#29616;&#26377;&#32467;&#26524;&#30340;&#29109;&#27491;&#21017;&#21270;&#35823;&#24046;&#20272;&#35745;&#65292;&#24182;&#23545;&#25200;&#21160;&#30340;Fisher-Rao&#26799;&#24230;&#27969;&#21644;&#33258;&#28982;&#26799;&#24230;&#27969;&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kakade&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36817;&#24180;&#26469;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#34920;&#26126;&#22312;&#26377;&#25110;&#26080;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21478;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;-&#21160;&#20316;&#20998;&#24067;&#30340;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#33258;&#28982;&#26799;&#24230;&#26041;&#27861;&#65292;&#20294;&#22312;&#29702;&#35770;&#26041;&#38754;&#25509;&#21463;&#24230;&#36739;&#20302;&#12290;&#22312;&#36825;&#37324;&#65292;&#29366;&#24577;-&#21160;&#20316;&#20998;&#24067;&#22312;&#29366;&#24577;-&#21160;&#20316;&#22810;&#38754;&#20307;&#20869;&#36981;&#24490;Fisher-Rao&#26799;&#24230;&#27969;&#65292;&#30456;&#23545;&#20110;&#32447;&#24615;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26356;&#20840;&#38754;&#22320;&#30740;&#31350;&#32447;&#24615;&#35268;&#21010;&#30340;Fisher-Rao&#26799;&#24230;&#27969;&#65292;&#24182;&#26174;&#31034;&#20102;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#20854;&#36895;&#29575;&#21462;&#20915;&#20110;&#32447;&#24615;&#35268;&#21010;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36825;&#25552;&#20379;&#20102;&#32447;&#24615;&#35268;&#21010;&#30340;&#29109;&#27491;&#21017;&#21270;&#24341;&#36215;&#30340;&#35823;&#24046;&#20272;&#35745;&#65292;&#36825;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#12290;&#25105;&#20204;&#25299;&#23637;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#25200;&#21160;&#30340;Fisher-Rao&#26799;&#24230;&#27969;&#21644;&#33258;&#28982;&#26799;&#24230;&#27969;&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#30452;&#21040;&#36924;&#36817;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19448v1 Announce Type: cross  Abstract: Kakade's natural policy gradient method has been studied extensively in the last years showing linear convergence with and without regularization. We study another natural gradient method which is based on the Fisher information matrix of the state-action distributions and has received little attention from the theoretical side. Here, the state-action distributions follow the Fisher-Rao gradient flow inside the state-action polytope with respect to a linear potential. Therefore, we study Fisher-Rao gradient flows of linear programs more generally and show linear convergence with a rate that depends on the geometry of the linear program. Equivalently, this yields an estimate on the error induced by entropic regularization of the linear program which improves existing results. We extend these results and show sublinear convergence for perturbed Fisher-Rao gradient flows and natural gradient flows up to an approximation error. In particul
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;ante-hoc&#26041;&#27861;&#23558;&#20020;&#24202;&#27010;&#24565;&#24341;&#20837;&#21040;&#20998;&#31867;&#31649;&#36947;&#20013;&#65292;&#25552;&#20379;&#20102;&#32954;&#30284;&#26816;&#27979;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#65292;&#30456;&#36739;&#20110;&#22522;&#32447;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65288;F1 &gt; 0.9&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.19444</link><description>&lt;p&gt;
&#36879;&#26126;&#19988;&#20020;&#24202;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#32954;&#30284;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transparent and Clinically Interpretable AI for Lung Cancer Detection in Chest X-Rays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19444
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;ante-hoc&#26041;&#27861;&#23558;&#20020;&#24202;&#27010;&#24565;&#24341;&#20837;&#21040;&#20998;&#31867;&#31649;&#36947;&#20013;&#65292;&#25552;&#20379;&#20102;&#32954;&#30284;&#26816;&#27979;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#65292;&#30456;&#36739;&#20110;&#22522;&#32447;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65288;F1 &gt; 0.9&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19444v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#31616;&#35201;&#25688;&#35201;&#65306;&#36879;&#26126;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#26088;&#22312;&#35299;&#20915;&#22797;&#26434;&#40657;&#21283;&#23376;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#20449;&#20219;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#20107;&#21518;XAI&#25216;&#26415;&#26368;&#36817;&#24050;&#34987;&#35777;&#26126;&#22312;&#21307;&#30103;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#35299;&#37322;&#65292;&#19981;&#36866;&#21512;&#20020;&#24202;&#20351;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;ante-hoc&#26041;&#27861;&#65292;&#39318;&#27425;&#23558;&#20020;&#24202;&#27010;&#24565;&#24341;&#20837;&#20998;&#31867;&#31649;&#36947;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#28145;&#20837;&#20102;&#35299;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#19968;&#20010;&#22823;&#22411;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;&#33016;&#37096;X&#23556;&#32447;&#21644;&#30456;&#20851;&#21307;&#30103;&#25253;&#21578;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#21363;&#32954;&#30284;&#30340;&#26816;&#27979;&#12290;&#19982;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32954;&#30284;&#26816;&#27979;&#20013;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65288;F1 &gt; 0.9&#65289;&#65292;&#21516;&#26102;&#29983;&#25104;&#20102;&#20020;&#24202;&#30456;&#20851;&#19988;&#26356;&#21487;&#38752;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19444v1 Announce Type: new  Abstract: The rapidly advancing field of Explainable Artificial Intelligence (XAI) aims to tackle the issue of trust regarding the use of complex black-box deep learning models in real-world applications. Existing post-hoc XAI techniques have recently been shown to have poor performance on medical data, producing unreliable explanations which are infeasible for clinical use. To address this, we propose an ante-hoc approach based on concept bottleneck models which introduces for the first time clinical concepts into the classification pipeline, allowing the user valuable insight into the decision-making process. On a large public dataset of chest X-rays and associated medical reports, we focus on the binary classification task of lung cancer detection. Our approach yields improved classification performance in lung cancer detection when compared to baseline deep learning models (F1 &gt; 0.9), while also generating clinically relevant and more reliable
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#24490;&#29615;&#21644;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#32467;&#21512;&#22270;&#20869;&#37096;&#20851;&#31995;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#24182;&#25552;&#21319;&#20102;&#29983;&#24577;&#30636;&#26102;&#35780;&#20272;&#65288;EMA&#65289;&#25968;&#25454;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19442</link><description>&lt;p&gt;
&#21033;&#29992;&#20010;&#20307;&#22270;&#32467;&#26500;&#25552;&#21319;&#29983;&#24577;&#30636;&#26102;&#35780;&#20272;&#65288;EMA&#65289;&#30340;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploiting Individual Graph Structures to Enhance Ecological Momentary Assessment (EMA) Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19442
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#24490;&#29615;&#21644;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#32467;&#21512;&#22270;&#20869;&#37096;&#20851;&#31995;&#20449;&#24687;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#24182;&#25552;&#21319;&#20102;&#29983;&#24577;&#30636;&#26102;&#35780;&#20272;&#65288;EMA&#65289;&#25968;&#25454;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#24515;&#29702;&#30149;&#29702;&#23398;&#39046;&#22495;&#20013;&#65292;&#20934;&#30830;&#35780;&#20272;&#21644;&#39044;&#27979;&#20174;&#29983;&#24577;&#30636;&#26102;&#35780;&#20272;&#65288;EMA&#65289;&#33719;&#21462;&#30340;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;EMA&#25552;&#20379;&#38543;&#26102;&#38388;&#20016;&#23500;&#30340;&#24773;&#22659;&#24615;&#24515;&#29702;&#30149;&#29702;&#27979;&#37327;&#65292;&#23454;&#38469;&#19978;&#20250;&#23548;&#33268;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#20998;&#26512;&#20013;&#20250;&#20986;&#29616;&#35768;&#22810;&#25361;&#25112;&#65292;&#28304;&#20110;&#24773;&#32490;&#12289;&#34892;&#20026;&#21644;&#24773;&#22659;EMA&#25968;&#25454;&#22266;&#26377;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#26041;&#38754;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#24490;&#29615;&#21644;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#34920;&#29616;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21512;&#24182;&#21453;&#26144;&#21464;&#37327;&#20043;&#38388;&#20869;&#37096;&#20851;&#31995;&#30340;&#22270;&#20013;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;GNN&#26174;&#30528;&#25552;&#39640;&#20102;&#32467;&#26524;&#65292;&#23558;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20174;1.02&#38477;&#20302;&#21040;0.84&#65292;&#19982;&#22522;&#32447;LSTM&#27169;&#22411;&#30456;&#27604;&#12290;&#22240;&#27492;&#65292;&#36824;&#25506;&#35752;&#20102;&#21033;&#29992;&#19981;&#21516;&#29305;&#24449;&#26500;&#24314;&#22270;&#23545;GNN&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19442v1 Announce Type: new  Abstract: In the evolving field of psychopathology, the accurate assessment and forecasting of data derived from Ecological Momentary Assessment (EMA) is crucial. EMA offers contextually-rich psychopathological measurements over time, that practically lead to Multivariate Time Series (MTS) data. Thus, many challenges arise in analysis from the temporal complexities inherent in emotional, behavioral, and contextual EMA data as well as their inter-dependencies. To address both of these aspects, this research investigates the performance of Recurrent and Temporal Graph Neural Networks (GNNs). Overall, GNNs, by incorporating additional information from graphs reflecting the inner relationships between the variables, notably enhance the results by decreasing the Mean Squared Error (MSE) to 0.84 compared to the baseline LSTM model at 1.02. Therefore, the effect of constructing graphs with different characteristics on GNN performance is also explored. Ad
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#38543;&#26426;Transformer&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20020;&#24202;&#38754;&#35848;&#30340;&#38899;&#39057;&#35760;&#24405;&#23454;&#29616;&#20102;&#26368;&#26032;&#27700;&#24179;&#30340;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.19441</link><description>&lt;p&gt;
&#22522;&#20110;&#26032;&#22411;&#38543;&#26426;Transformer&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20020;&#24202;&#38754;&#35848;&#30340;&#38899;&#39057;&#35760;&#24405;&#26816;&#27979;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
A Novel Stochastic Transformer-based Approach for Post-Traumatic Stress Disorder Detection using Audio Recording of Clinical Interviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19441
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#38543;&#26426;Transformer&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20020;&#24202;&#38754;&#35848;&#30340;&#38899;&#39057;&#35760;&#24405;&#23454;&#29616;&#20102;&#26368;&#26032;&#27700;&#24179;&#30340;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#65288;PTSD&#65289;&#26159;&#19968;&#31181;&#22312;&#30446;&#30585;&#25110;&#20307;&#39564;&#26497;&#31471;&#21019;&#20260;&#20107;&#20214;&#21518;&#21487;&#33021;&#20135;&#29983;&#30340;&#31934;&#31070;&#38556;&#30861;&#12290;PTSD&#21487;&#20197;&#24433;&#21709;&#20219;&#20309;&#20154;&#65292;&#26080;&#35770;&#31181;&#26063;&#25110;&#25991;&#21270;&#32972;&#26223;&#22914;&#20309;&#12290;&#34987;&#20272;&#35745;&#27599;&#21313;&#19968;&#20154;&#20013;&#23601;&#26377;&#19968;&#20154;&#22312;&#20182;&#20204;&#30340;&#19968;&#29983;&#20013;&#20250;&#32463;&#21382;PTSD&#12290;&#20020;&#24202;&#26045;&#29992;&#30340;PTSD&#37327;&#34920;&#65288;CAPS&#65289;&#21644;&#27665;&#29992;PTSD&#37327;&#34920;&#65288;PCL-C&#65289;&#30340;&#38754;&#35848;&#26159;PTSD&#35786;&#26029;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;&#36825;&#20123;&#38382;&#21367;&#21487;&#20197;&#34987;&#21463;&#35797;&#32773;&#30340;&#22238;&#31572;&#25152;&#27450;&#39575;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20020;&#24202;&#38754;&#35848;&#30340;&#38899;&#39057;&#35760;&#24405;&#22312;PTSD&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#27700;&#24179;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20174;&#20020;&#24202;&#38754;&#35848;&#30340;&#38899;&#39057;&#35760;&#24405;&#20013;&#25552;&#21462;&#30340;MFCC&#20302;&#32423;&#29305;&#24449;&#65292;&#25509;&#30528;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;Transformer&#36827;&#34892;&#28145;&#24230;&#39640;&#32423;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;eDAIC&#25968;&#25454;&#38598;&#19978;&#20197;2.92&#30340;RMSE&#23454;&#29616;&#20102;&#26368;&#26032;&#27700;&#24179;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19441v1 Announce Type: cross  Abstract: Post-traumatic stress disorder (PTSD) is a mental disorder that can be developed after witnessing or experiencing extremely traumatic events. PTSD can affect anyone, regardless of ethnicity, or culture. An estimated one in every eleven people will experience PTSD during their lifetime. The Clinician-Administered PTSD Scale (CAPS) and the PTSD Check List for Civilians (PCL-C) interviews are gold standards in the diagnosis of PTSD. These questionnaires can be fooled by the subject's responses. This work proposes a deep learning-based approach that achieves state-of-the-art performances for PTSD detection using audio recordings during clinical interviews. Our approach is based on MFCC low-level features extracted from audio recordings of clinical interviews, followed by deep high-level learning using a Stochastic Transformer. Our proposed approach achieves state-of-the-art performances with an RMSE of 2.92 on the eDAIC dataset thanks to t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#24182;&#34892;&#21270;&#25216;&#26415;&#65292;&#20197;&#32553;&#30701;&#23725;&#22238;&#24402;&#33041;&#32534;&#30721;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;fMRI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2403.19421</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#20010;&#20307;fMRI&#25968;&#25454;&#38598;&#20013;&#25193;&#23637;&#23725;&#22238;&#24402;&#36827;&#34892;&#33041;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Scaling up ridge regression for brain encoding in a massive individual fMRI dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19421
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#24182;&#34892;&#21270;&#25216;&#26415;&#65292;&#20197;&#32553;&#30701;&#23725;&#22238;&#24402;&#33041;&#32534;&#30721;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;fMRI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#36827;&#34892;&#22823;&#33041;&#32534;&#30721;&#26159;&#19968;&#31181;&#26088;&#22312;&#30452;&#25509;&#20174;&#22797;&#26434;&#21050;&#28608;&#29305;&#24449;&#65288;&#22914;&#30005;&#24433;&#24103;&#65289;&#39044;&#27979;&#20154;&#31867;&#22823;&#33041;&#27963;&#21160;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#23725;&#22238;&#24402;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#33041;&#32534;&#30721;&#39044;&#27979;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#33391;&#22909;&#30340;&#26679;&#26412;&#22806;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#21253;&#21547;&#35768;&#22810;&#22823;&#35268;&#27169;&#28145;&#24230;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#25968;&#25454;&#38598;&#26102;&#65292;&#35757;&#32451;&#23725;&#22238;&#24402;&#27169;&#22411;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#25324;&#35768;&#22810;&#33041;&#27963;&#21160;&#30340;&#31354;&#38388;-&#26102;&#38388;&#26679;&#26412;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#24182;&#34892;&#21270;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#22312;CNeuroMod Friends&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#23725;&#22238;&#24402;&#36827;&#34892;&#33041;&#32534;&#30721;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#30446;&#21069;&#21487;&#29992;&#30340;&#26368;&#22823;&#30340;&#28145;&#24230;fMRI&#36164;&#28304;&#20043;&#19968;&#12290;&#36890;&#36807;&#22810;&#32447;&#31243;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Intel Math Kernel&#24211;&#65288;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19421v1 Announce Type: cross  Abstract: Brain encoding with neuroimaging data is an established analysis aimed at predicting human brain activity directly from complex stimuli features such as movie frames. Typically, these features are the latent space representation from an artificial neural network, and the stimuli are image, audio, or text inputs. Ridge regression is a popular prediction model for brain encoding due to its good out-of-sample generalization performance. However, training a ridge regression model can be highly time-consuming when dealing with large-scale deep functional magnetic resonance imaging (fMRI) datasets that include many space-time samples of brain activity. This paper evaluates different parallelization techniques to reduce the training time of brain encoding with ridge regression on the CNeuroMod Friends dataset, one of the largest deep fMRI resource currently available. With multi-threading, our results show that the Intel Math Kernel Library (
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#21518;&#22788;&#29702;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#26080;&#38656;&#21463;&#20445;&#25252;&#23646;&#24615;&#65292;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#26174;&#31034;&#20102;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#25490;&#21517;&#30340;P-&#20844;&#24179;&#24615;&#21644;&#30456;&#23545;&#20110;&#24402;&#19968;&#21270;&#25240;&#25187;&#32047;&#35745;&#22686;&#30410;(NDCG)&#30340;&#26377;&#25928;&#24615;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19419</link><description>&lt;p&gt;
&#25490;&#21517;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#36890;&#36807;&#38543;&#26426;&#21270;&#23454;&#29616;&#25239;&#24178;&#25200;&#32780;&#26080;&#38656;&#21463;&#20445;&#25252;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness in Ranking: Robustness through Randomization without the Protected Attribute
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19419
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#21518;&#22788;&#29702;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#26080;&#38656;&#21463;&#20445;&#25252;&#23646;&#24615;&#65292;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#26174;&#31034;&#20102;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#25490;&#21517;&#30340;P-&#20844;&#24179;&#24615;&#21644;&#30456;&#23545;&#20110;&#24402;&#19968;&#21270;&#25240;&#25187;&#32047;&#35745;&#22686;&#30410;(NDCG)&#30340;&#26377;&#25928;&#24615;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#23588;&#20854;&#26159;&#19982;&#20998;&#31867;&#38382;&#39064;&#26377;&#20851;&#30340;&#20844;&#24179;&#24615;&#65292;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#22312;&#28041;&#21450;&#25490;&#21517;&#30340;&#38382;&#39064;&#20013;&#65292;&#22914;&#22312;&#32447;&#24191;&#21578;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#20154;&#21147;&#36164;&#28304;&#33258;&#21160;&#21270;&#20013;&#65292;&#20173;&#28982;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#24037;&#20316;&#12290;&#20004;&#20010;&#22797;&#26434;&#20043;&#22788;&#22312;&#20110;&#65306;&#39318;&#20808;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#21463;&#20445;&#25252;&#23646;&#24615;&#12290;&#20854;&#27425;&#65292;&#25490;&#21517;&#30340;&#20844;&#24179;&#24615;&#23384;&#22312;&#22810;&#20010;&#34913;&#37327;&#26631;&#20934;&#65292;&#22522;&#20110;&#21333;&#20010;&#34913;&#37327;&#26631;&#20934;&#30340;&#20248;&#21270;&#26041;&#27861;&#21487;&#33021;&#20250;&#20135;&#29983;&#30456;&#23545;&#20854;&#20182;&#34913;&#37327;&#26631;&#20934;&#19981;&#20844;&#24179;&#30340;&#25490;&#21517;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#21518;&#22788;&#29702;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#21487;&#29992;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#20540;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#25490;&#21517;&#30340;P-&#20844;&#24179;&#24615;&#21644;&#30456;&#23545;&#20110;&#24402;&#19968;&#21270;&#25240;&#25187;&#32047;&#35745;&#22686;&#30410;(NDCG)&#30340;&#25928;&#26524;&#30340;&#31283;&#20581;&#24615;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19419v1 Announce Type: cross  Abstract: There has been great interest in fairness in machine learning, especially in relation to classification problems. In ranking-related problems, such as in online advertising, recommender systems, and HR automation, much work on fairness remains to be done. Two complications arise: first, the protected attribute may not be available in many applications. Second, there are multiple measures of fairness of rankings, and optimization-based methods utilizing a single measure of fairness of rankings may produce rankings that are unfair with respect to other measures. In this work, we propose a randomized method for post-processing rankings, which do not require the availability of the protected attribute. In an extensive numerical study, we show the robustness of our methods with respect to P-Fairness and effectiveness with respect to Normalized Discounted Cumulative Gain (NDCG) from the baseline ranking, improving on previously proposed meth
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24471;&#21040;&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#32467;&#21512;&#26446;&#23545;&#31216;&#25216;&#26415;&#20998;&#26512;&#24471;&#21040;&#20102;&#23432;&#24658;&#21644;&#38750;&#23432;&#24658;&#24773;&#20917;&#19979;1D&#21644;2D&#35856;&#25391;&#23376;&#30340;&#36816;&#21160;&#31215;&#20998;&#65292;&#23637;&#31034;&#20102;&#38750;&#23432;&#24658;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#33021;&#37327;&#23432;&#24658;&#24120;&#25968;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#39057;&#29575;&#27604;&#20363;&#24773;&#20917;&#19979;&#25512;&#24191;&#20102;&#35282;&#21160;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.19418</link><description>&lt;p&gt;
&#36816;&#21160;&#30340;&#23432;&#24658;&#21644;&#38750;&#23432;&#24658;&#21160;&#21147;&#23398;&#30340;&#36816;&#21160;&#31215;&#20998;
&lt;/p&gt;
&lt;p&gt;
Constants of Motion for Conserved and Non-conserved Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19418
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24471;&#21040;&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#32467;&#21512;&#26446;&#23545;&#31216;&#25216;&#26415;&#20998;&#26512;&#24471;&#21040;&#20102;&#23432;&#24658;&#21644;&#38750;&#23432;&#24658;&#24773;&#20917;&#19979;1D&#21644;2D&#35856;&#25391;&#23376;&#30340;&#36816;&#21160;&#31215;&#20998;&#65292;&#23637;&#31034;&#20102;&#38750;&#23432;&#24658;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#33021;&#37327;&#23432;&#24658;&#24120;&#25968;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#39057;&#29575;&#27604;&#20363;&#24773;&#20917;&#19979;&#25512;&#24191;&#20102;&#35282;&#21160;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;FJet&#65289;&#21040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#33719;&#24471;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#65307;&#28982;&#21518;&#21033;&#29992;&#26446;&#23545;&#31216;&#25216;&#26415;&#23545;&#35813;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#20197;&#33719;&#24471;&#36816;&#21160;&#31215;&#20998;&#12290;&#35813;&#20998;&#26512;&#38024;&#23545;1D&#21644;2D&#35856;&#25391;&#23376;&#30340;&#23432;&#24658;&#21644;&#38750;&#23432;&#24658;&#24773;&#20917;&#36827;&#34892;&#12290;&#23545;&#20110;1D&#35856;&#25391;&#23376;&#65292;&#22312;&#27424;&#38459;&#23612;&#12289;&#36807;&#38459;&#23612;&#21644;&#20020;&#30028;&#38459;&#23612;&#24773;&#20917;&#19979;&#25214;&#21040;&#20102;&#36816;&#21160;&#31215;&#20998;&#12290;&#23545;&#20110;&#38750;&#23432;&#24658;&#27169;&#22411;&#30340;&#23384;&#22312;&#36825;&#26679;&#30340;&#24120;&#25968;&#26159;&#23545;&#25972;&#20010;&#31995;&#32479;&#65288;&#21363;&#25391;&#33633;&#22120;&#21152;&#32791;&#25955;&#29615;&#22659;&#65289;&#33021;&#37327;&#23432;&#24658;&#30340;&#19968;&#31181;&#34920;&#29616;&#30340;&#26032;&#39062;&#35299;&#37322;&#12290;&#23545;&#20110;2D&#35856;&#25391;&#23376;&#65292;&#22312;&#31561;&#21521;&#21644;&#38750;&#31561;&#21521;&#24773;&#20917;&#19979;&#25214;&#21040;&#20102;&#36816;&#21160;&#31215;&#20998;&#65292;&#21253;&#25324;&#39057;&#29575;&#19981;&#21487;&#20849;&#36717;&#30340;&#24773;&#20917;&#65307;&#36824;&#25512;&#24191;&#21040;&#20219;&#24847;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#19968;&#20010;&#24120;&#25968;&#65292;&#23427;&#23558;&#35282;&#21160;&#37327;&#25512;&#24191;&#21040;&#25152;&#26377;&#39057;&#29575;&#27604;&#20363;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19418v1 Announce Type: new  Abstract: This paper begins with a dynamical model that was obtained by applying a machine learning technique (FJet) to time-series data; this dynamical model is then analyzed with Lie symmetry techniques to obtain constants of motion. This analysis is performed on both the conserved and non-conserved cases of the 1D and 2D harmonic oscillators. For the 1D oscillator, constants are found in the cases where the system is underdamped, overdamped, and critically damped. The novel existence of such a constant for a non-conserved model is interpreted as a manifestation of the conservation of energy of the {\em total} system (i.e., oscillator plus dissipative environment). For the 2D oscillator, constants are found for the isotropic and anisotropic cases, including when the frequencies are incommensurate; it is also generalized to arbitrary dimensions. In addition, a constant is identified which generalizes angular momentum for all ratios of the frequen
&lt;/p&gt;</description></item><item><title>&#25361;&#25112;&#24120;&#29992;&#30340;&#24207;&#25968;&#32534;&#30721;&#65292;&#25552;&#20986;&#22522;&#20110;&#23383;&#31526;&#20018;&#30456;&#20284;&#24615;&#32534;&#30721;&#30340;&#34920;&#26684;&#23398;&#20064;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#25928;&#26524;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.19405</link><description>&lt;p&gt;
&#34920;&#26684;&#23398;&#20064;&#65306;&#23454;&#20307;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Tabular Learning: Encoding for Entity and Context Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19405
&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#24120;&#29992;&#30340;&#24207;&#25968;&#32534;&#30721;&#65292;&#25552;&#20986;&#22522;&#20110;&#23383;&#31526;&#20018;&#30456;&#20284;&#24615;&#32534;&#30721;&#30340;&#34920;&#26684;&#23398;&#20064;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#25928;&#26524;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#32534;&#30721;&#25216;&#26415;&#23545;&#23454;&#20307;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#25361;&#25112;&#24120;&#29992;&#30340;&#24207;&#25968;&#32534;&#30721;&#22312;&#34920;&#26684;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#19981;&#21516;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#21644;&#32593;&#32476;&#26550;&#26500;&#65292;&#23545;&#32534;&#30721;&#22120;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#23398;&#20064;&#32467;&#26524;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36890;&#36807;&#20445;&#25345;&#27979;&#35797;&#12289;&#39564;&#35777;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#20998;&#31867;&#25968;&#25454;&#65292;&#24207;&#25968;&#32534;&#30721;&#24182;&#19981;&#26159;&#26368;&#21512;&#36866;&#30340;&#32534;&#30721;&#22120;&#65292;&#26080;&#27861;&#27491;&#30830;&#39044;&#22788;&#29702;&#25968;&#25454;&#24182;&#20998;&#31867;&#30446;&#26631;&#21464;&#37327;&#12290;&#36890;&#36807;&#22522;&#20110;&#23383;&#31526;&#20018;&#30456;&#20284;&#24615;&#23545;&#29305;&#24449;&#36827;&#34892;&#32534;&#30721;&#65292;&#35745;&#31639;&#30456;&#20284;&#24615;&#30697;&#38453;&#20316;&#20026;&#32593;&#32476;&#36755;&#20837;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#36866;&#29992;&#20110;&#23454;&#20307;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#65292;&#21464;&#25442;&#22120;&#26550;&#26500;&#22312;&#24207;&#25968;&#32534;&#30721;&#21644;&#30456;&#20284;&#24615;&#32534;&#30721;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19405v1 Announce Type: cross  Abstract: Examining the effect of different encoding techniques on entity and context embeddings, the goal of this work is to challenge commonly used Ordinal encoding for tabular learning. Applying different preprocessing methods and network architectures over several datasets resulted in a benchmark on how the encoders influence the learning outcome of the networks. By keeping the test, validation and training data consistent, results have shown that ordinal encoding is not the most suited encoder for categorical data in terms of preprocessing the data and thereafter, classifying the target variable correctly. A better outcome was achieved, encoding the features based on string similarities by computing a similarity matrix as input for the network. This is the case for both, entity and context embeddings, where the transformer architecture showed improved performance for Ordinal and Similarity encoding with regard to multi-label classification 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20174;&#26631;&#31614;&#27604;&#20363;&#23398;&#20064;&#24067;&#23572;&#20989;&#25968;&#30340;&#38590;&#39064;&#24615;&#23637;&#24320;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#23547;&#25214;&#28385;&#36275;&#23376;&#38598;&#21512;&#20013;&#24120;&#25968;&#27604;&#20363;&#30340;&#24067;&#23572;&#20989;&#25968;&#30340;&#23376;&#21477;&#26159;NP&#38590;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.19401</link><description>&lt;p&gt;
&#20174;&#26631;&#31614;&#27604;&#20363;&#23398;&#20064;&#24067;&#23572;&#20989;&#25968;&#30340;&#22256;&#38590;&#24615;
&lt;/p&gt;
&lt;p&gt;
Hardness of Learning Boolean Functions from Label Proportions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20174;&#26631;&#31614;&#27604;&#20363;&#23398;&#20064;&#24067;&#23572;&#20989;&#25968;&#30340;&#38590;&#39064;&#24615;&#23637;&#24320;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#23547;&#25214;&#28385;&#36275;&#23376;&#38598;&#21512;&#20013;&#24120;&#25968;&#27604;&#20363;&#30340;&#24067;&#23572;&#20989;&#25968;&#30340;&#23376;&#21477;&#26159;NP&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20174;&#26631;&#31614;&#27604;&#20363;&#36827;&#34892;&#23398;&#20064;(LLP)&#30340;&#26694;&#26550;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#31034;&#20363;&#34987;&#32858;&#21512;&#21040;&#23376;&#38598;&#25110;&#34955;&#20013;&#65292;&#21482;&#26377;&#27599;&#20010;&#34955;&#30340;&#24179;&#22343;&#26631;&#31614;&#21487;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#22522;&#20110;&#31034;&#20363;&#30340;&#39044;&#27979;&#22120;&#12290;&#36825;&#26159;&#23545;&#20256;&#32479;&#30340;PAC&#23398;&#20064;&#30340;&#25512;&#24191;&#65292;&#21518;&#32773;&#26159;&#21333;&#20301;&#22823;&#23567;&#34955;&#30340;&#29305;&#20363;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23545;LLP&#30340;&#35745;&#31639;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#30740;&#31350;(Saket, NeurIPS'21; Saket, NeurIPS'22)&#65292;&#23637;&#31034;&#20102;&#22312;LLP&#35774;&#32622;&#20013;&#23398;&#20064;&#21322;&#31354;&#38388;&#30340;&#31639;&#27861;&#21644;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;LLP&#23398;&#20064;&#24067;&#23572;&#20989;&#25968;&#30340;&#38590;&#39064;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#32771;&#34385;&#21040;&#22823;&#23567;&#19981;&#36229;&#36807;$2$&#30340;&#34955;&#23376;&#38598;&#21512;&#65292;&#36825;&#20123;&#34955;&#23376;&#38598;&#21512;&#19982;OR&#20989;&#25968;&#19968;&#33268;&#26102;&#65292;&#35201;&#25214;&#21040;&#19968;&#20010;&#28385;&#36275;&#20219;&#20309;&#24120;&#25968;&#37096;&#20998;&#34955;&#23376;&#30340;&#24120;&#25968;&#22810;&#23376;&#21477;CNF&#26159;NP&#38590;&#30340;&#12290;&#36825;&#19982;(Saket, NeurIPS'21)&#30340;&#24037;&#20316;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;$(2/5)$-approx&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19401v1 Announce Type: cross  Abstract: In recent years the framework of learning from label proportions (LLP) has been gaining importance in machine learning. In this setting, the training examples are aggregated into subsets or bags and only the average label per bag is available for learning an example-level predictor. This generalizes traditional PAC learning which is the special case of unit-sized bags. The computational learning aspects of LLP were studied in recent works (Saket, NeurIPS'21; Saket, NeurIPS'22) which showed algorithms and hardness for learning halfspaces in the LLP setting. In this work we focus on the intractability of LLP learning Boolean functions. Our first result shows that given a collection of bags of size at most $2$ which are consistent with an OR function, it is NP-hard to find a CNF of constantly many clauses which satisfies any constant-fraction of the bags. This is in contrast with the work of (Saket, NeurIPS'21) which gave a $(2/5)$-approx
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#26368;&#20248;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#30001;&#26410;&#30693;&#20219;&#21153;&#20998;&#24067;&#23450;&#20041;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19381</link><description>&lt;p&gt;
&#20851;&#20110;&#36817;&#36125;&#21494;&#26031;&#26368;&#20248;&#31639;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Uncertainty Quantification for Near-Bayes Optimal Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19381
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#26368;&#20248;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#30001;&#26410;&#30693;&#20219;&#21153;&#20998;&#24067;&#23450;&#20041;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#24314;&#27169;&#20801;&#35768;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#65292;&#26500;&#24314;&#25110;&#23454;&#29616;&#23427;&#20204;&#30340;&#36125;&#21494;&#26031;&#23545;&#24212;&#26159;&#22256;&#38590;&#30340;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#24120;&#29992;&#30340;ML&#31639;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#39640;&#25928;&#65292;&#24182;&#19988;&#21487;&#33021;&#22312;&#26410;&#30693;&#20219;&#21153;&#20998;&#24067;&#19979;&#25509;&#36817;&#36125;&#21494;&#26031;&#26368;&#20248;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20351;&#29992;&#35813;&#31639;&#27861;&#26500;&#24314;&#19968;&#20010;&#38789;&#21518;&#39564;&#65292;&#21487;&#20197;&#24674;&#22797;&#30001;&#20219;&#21153;&#20998;&#24067;&#23450;&#20041;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#26159;&#26410;&#30693;&#20294;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36890;&#29992;ML&#31639;&#27861;&#30340;&#23454;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;&#22522;&#20110;&#21508;&#31181;&#38750;NN&#21644;NN&#31639;&#27861;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19381v1 Announce Type: cross  Abstract: Bayesian modelling allows for the quantification of predictive uncertainty which is crucial in safety-critical applications. Yet for many machine learning (ML) algorithms, it is difficult to construct or implement their Bayesian counterpart. In this work we present a promising approach to address this challenge, based on the hypothesis that commonly used ML algorithms are efficient across a wide variety of tasks and may thus be near Bayes-optimal w.r.t. an unknown task distribution. We prove that it is possible to recover the Bayesian posterior defined by the task distribution, which is unknown but optimal in this setting, by building a martingale posterior using the algorithm. We further propose a practical uncertainty quantification method that apply to general ML algorithms. Experiments based on a variety of non-NN and NN algorithms demonstrate the efficacy of our method.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#20061;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#21450;&#20004;&#31181;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#39044;&#27979;&#26032;&#20896;&#32954;&#28814;&#24739;&#32773;&#27515;&#20129;&#29575;&#12289;ICU&#38656;&#27714;&#21644;&#36890;&#27668;&#22825;&#25968;&#30340;&#26368;&#21518;&#29366;&#24577;&#20013;&#65292;&#20165;&#26377;10&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#20855;&#26377;&#29992;&#22788;&#65292;&#20854;&#20013;&#24613;&#24615;&#32958;&#25439;&#20260;&#29305;&#24449;&#26368;&#20026;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.19355</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26032;&#20896;&#32954;&#28814;&#24739;&#32773;&#27515;&#20129;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) Based Prediction of Mortality, for COVID-19 Patients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19355
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#20061;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#21450;&#20004;&#31181;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30340;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#39044;&#27979;&#26032;&#20896;&#32954;&#28814;&#24739;&#32773;&#27515;&#20129;&#29575;&#12289;ICU&#38656;&#27714;&#21644;&#36890;&#27668;&#22825;&#25968;&#30340;&#26368;&#21518;&#29366;&#24577;&#20013;&#65292;&#20165;&#26377;10&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#20855;&#26377;&#29992;&#22788;&#65292;&#20854;&#20013;&#24613;&#24615;&#32958;&#25439;&#20260;&#29305;&#24449;&#26368;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20005;&#37325;&#24863;&#26579;&#30340;&#26032;&#20896;&#32954;&#28814;&#24739;&#32773;&#65292;&#35782;&#21035;&#39640;&#39118;&#38505;&#24739;&#32773;&#24182;&#39044;&#27979;&#29983;&#23384;&#21644;&#26159;&#21542;&#38656;&#35201;&#37325;&#30151;&#30417;&#25252;&#65288;ICU&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20061;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#19982;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#32467;&#21512;&#30340;&#24615;&#33021;&#65292;&#20197;&#39044;&#27979;&#20195;&#34920;&#27515;&#20129;&#12289;ICU&#38656;&#27714;&#21644;&#36890;&#27668;&#22825;&#25968;&#30340;&#26368;&#21518;&#29366;&#24577;&#12290;&#37319;&#29992;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#20026;&#20943;&#23569;&#20559;&#24046;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#26681;&#25454;&#30456;&#20284;&#20998;&#24067;&#25286;&#20998;&#12290;&#39044;&#27979;&#24314;&#27169;&#20013;&#21457;&#29616;&#20165;&#26377;10&#20010;&#29305;&#24449;&#22312;&#39044;&#27979;&#20013;&#26377;&#29992;&#65292;&#20854;&#20013;&#22312;&#20303;&#38498;&#26399;&#38388;&#21457;&#29983;&#24613;&#24615;&#32958;&#25439;&#20260;&#30340;&#29305;&#24449;&#26368;&#20026;&#37325;&#35201;&#12290;&#31639;&#27861;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#29305;&#24449;&#25968;&#37327;&#21644;&#25968;&#25454;&#39044;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19355v1 Announce Type: new  Abstract: For severely affected COVID-19 patients, it is crucial to identify high-risk patients and predict survival and need for intensive care (ICU). Most of the proposed models are not well reported making them less reproducible and prone to high risk of bias particularly in presence of imbalance data/class. In this study, the performances of nine machine and deep learning algorithms in combination with two widely used feature selection methods were investigated to predict last status representing mortality, ICU requirement, and ventilation days. Fivefold cross-validation was used for training and validation purposes. To minimize bias, the training and testing sets were split maintaining similar distributions. Only 10 out of 122 features were found to be useful in prediction modelling with Acute kidney injury during hospitalization feature being the most important one. The algorithms performances depend on feature numbers and data pre-processin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#20132;&#20114;&#24335;&#20154;&#26426;&#23398;&#20064;&#30028;&#38754;&#65292;&#20801;&#35768;&#20154;&#31867;&#27880;&#37322;&#32773;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#26469;&#34917;&#20805;&#26631;&#20934;&#20108;&#20803;&#26631;&#31614;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#24310;&#20280;&#24037;&#20316;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19339</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25910;&#38598;&#21644;&#23398;&#20064;&#22797;&#26434;&#27880;&#37322;&#30340;&#20132;&#20114;&#24335;&#20154;&#26426;&#23398;&#20064;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
An Interactive Human-Machine Learning Interface for Collecting and Learning from Complex Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#20132;&#20114;&#24335;&#20154;&#26426;&#23398;&#20064;&#30028;&#38754;&#65292;&#20801;&#35768;&#20154;&#31867;&#27880;&#37322;&#32773;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#26469;&#34917;&#20805;&#26631;&#20934;&#20108;&#20803;&#26631;&#31614;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#24310;&#20280;&#24037;&#20316;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12289;&#21152;&#36895;&#23398;&#20064;&#24182;&#22686;&#24378;&#29992;&#25143;&#20449;&#24515;&#26469;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20943;&#36731;&#20154;&#31867;&#27880;&#37322;&#32773;&#38656;&#36866;&#24212;&#20256;&#32479;&#26631;&#31614;&#25152;&#26045;&#21152;&#30340;&#38480;&#21046;&#30340;&#26399;&#26395;&#65292;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#20154;&#26426;&#23398;&#20064;&#30028;&#38754;&#65292;&#20351;&#20154;&#31867;&#27880;&#37322;&#32773;&#21487;&#20197;&#21033;&#29992;&#21453;&#20107;&#23454;&#20363;&#26469;&#34917;&#20805;&#26631;&#20934;&#30340;&#20108;&#20803;&#26631;&#31614;&#20316;&#20026;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#26469;&#24310;&#20280;&#27492;&#24037;&#20316;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19339v1 Announce Type: new  Abstract: Human-Computer Interaction has been shown to lead to improvements in machine learning systems by boosting model performance, accelerating learning and building user confidence. In this work, we aim to alleviate the expectation that human annotators adapt to the constraints imposed by traditional labels by allowing for extra flexibility in the form that supervision information is collected. For this, we propose a human-machine learning interface for binary classification tasks which enables human annotators to utilise counterfactual examples to complement standard binary labels as annotations for a dataset. Finally we discuss the challenges in future extensions of this work.
&lt;/p&gt;</description></item><item><title>MedBN&#26159;&#19968;&#31181;&#38024;&#23545;&#24694;&#24847;&#27979;&#35797;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20013;&#20540;&#25209;&#27425;&#24402;&#19968;&#21270;&#65288;MedBN&#65289;&#22312;&#27979;&#35797;&#26102;&#38388;&#25512;&#26029;&#26399;&#38388;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#25915;&#20987;&#65292;&#24182;&#25104;&#21151;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;TTA&#26694;&#26550;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.19326</link><description>&lt;p&gt;
MedBN: &#38024;&#23545;&#24694;&#24847;&#27979;&#35797;&#26679;&#26412;&#30340;&#40065;&#26834;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
MedBN: Robust Test-Time Adaptation against Malicious Test Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19326
&lt;/p&gt;
&lt;p&gt;
MedBN&#26159;&#19968;&#31181;&#38024;&#23545;&#24694;&#24847;&#27979;&#35797;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20013;&#20540;&#25209;&#27425;&#24402;&#19968;&#21270;&#65288;MedBN&#65289;&#22312;&#27979;&#35797;&#26102;&#38388;&#25512;&#26029;&#26399;&#38388;&#20445;&#25252;&#27169;&#22411;&#20813;&#21463;&#25915;&#20987;&#65292;&#24182;&#25104;&#21151;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;TTA&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Test-time adaptation (TTA)&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#26410;&#39044;&#26009;&#21040;&#30340;&#20998;&#24067;&#36716;&#31227;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;TTA&#26041;&#27861;&#22312;&#36866;&#24212;&#20110;&#27979;&#35797;&#25968;&#25454;&#30340;&#21464;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36825;&#31181;&#36866;&#24212;&#24615;&#20351;&#27169;&#22411;&#26292;&#38706;&#20110;&#24694;&#24847;&#31034;&#20363;&#30340;&#25915;&#20987;&#20013;&#65292;&#36825;&#19968;&#26041;&#38754;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#21457;&#29616;&#65292;&#21363;&#20351;&#27979;&#35797;&#25209;&#27425;&#30340;&#19968;&#23567;&#37096;&#20998;&#21463;&#21040;&#24694;&#24847;&#31713;&#25913;&#65292;&#20063;&#20250;&#26292;&#38706;&#20986;TTA&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#20316;&#20026;&#23545;&#26032;&#20852;&#23041;&#32961;&#30340;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20013;&#20540;&#25209;&#27425;&#24402;&#19968;&#21270;&#65288;MedBN&#65289;&#65292;&#21033;&#29992;&#20102;&#20013;&#20540;&#23545;&#22312;&#27979;&#35797;&#26102;&#38388;&#25512;&#26029;&#26399;&#38388;&#25209;&#27425;&#24402;&#19968;&#21270;&#23618;&#20869;&#30340;&#32479;&#35745;&#20272;&#35745;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31639;&#27861;&#19981;&#21487;&#30693;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;TTA&#26694;&#26550;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;CIFAR10-C&#12289;CIFAR100-C&#21644;ImageNet-C&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;MedBN&#24615;&#33021;&#20248;&#36234;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19326v1 Announce Type: new  Abstract: Test-time adaptation (TTA) has emerged as a promising solution to address performance decay due to unforeseen distribution shifts between training and test data. While recent TTA methods excel in adapting to test data variations, such adaptability exposes a model to vulnerability against malicious examples, an aspect that has received limited attention. Previous studies have uncovered security vulnerabilities within TTA even when a small proportion of the test batch is maliciously manipulated. In response to the emerging threat, we propose median batch normalization (MedBN), leveraging the robustness of the median for statistics estimation within the batch normalization layer during test-time inference. Our method is algorithm-agnostic, thus allowing seamless integration with existing TTA frameworks. Our experimental results on benchmark datasets, including CIFAR10-C, CIFAR100-C and ImageNet-C, consistently demonstrate that MedBN outperf
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyperMV&#30340;&#22810;&#35270;&#35282;&#20107;&#20214;&#21160;&#20316;&#35782;&#21035;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23558;&#31163;&#25955;&#20107;&#20214;&#25968;&#25454;&#36716;&#25442;&#25104;&#24103;&#29366;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#20849;&#20139;&#21367;&#31215;&#32593;&#32476;&#25552;&#21462;&#35270;&#35282;&#30456;&#20851;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.19316</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#22270;&#30340;&#22810;&#35270;&#35282;&#20107;&#20214;&#30456;&#26426;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Hypergraph-based Multi-View Action Recognition using Event Cameras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19316
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyperMV&#30340;&#22810;&#35270;&#35282;&#20107;&#20214;&#21160;&#20316;&#35782;&#21035;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23558;&#31163;&#25955;&#20107;&#20214;&#25968;&#25454;&#36716;&#25442;&#25104;&#24103;&#29366;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#20849;&#20139;&#21367;&#31215;&#32593;&#32476;&#25552;&#21462;&#35270;&#35282;&#30456;&#20851;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25968;&#25454;&#30340;&#21160;&#20316;&#35782;&#21035;&#26159;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#22522;&#30707;&#12290;&#21333;&#35270;&#35282;&#21160;&#20316;&#35782;&#21035;&#30001;&#20110;&#20381;&#36182;&#21333;&#19968;&#35270;&#35282;&#32780;&#38754;&#20020;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22810;&#35270;&#35282;&#26041;&#27861;&#20174;&#19981;&#21516;&#35270;&#35282;&#25429;&#33719;&#20114;&#34917;&#20449;&#24687;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#65292;&#20107;&#20214;&#30456;&#26426;&#20316;&#20026;&#21019;&#26032;&#30340;&#20223;&#29983;&#20256;&#24863;&#22120;&#23853;&#38706;&#22836;&#35282;&#65292;&#20026;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#20316;&#35782;&#21035;&#24102;&#26469;&#20102;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#20851;&#27880;&#21333;&#19968;&#35270;&#35282;&#22330;&#26223;&#65292;&#22312;&#22810;&#35270;&#35282;&#20107;&#20214;&#25968;&#25454;&#21033;&#29992;&#26041;&#38754;&#23384;&#22312;&#31354;&#30333;&#65292;&#29305;&#21035;&#26159;&#22312;&#20449;&#24687;&#19981;&#36275;&#21644;&#35821;&#20041;&#38169;&#37197;&#31561;&#25361;&#25112;&#26041;&#38754;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HyperMV&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#20316;&#35782;&#21035;&#26694;&#26550;&#12290;HyperMV&#23558;&#31163;&#25955;&#20107;&#20214;&#25968;&#25454;&#36716;&#25442;&#25104;&#31867;&#20284;&#24103;&#30340;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#20849;&#20139;&#30340;&#21367;&#31215;&#32593;&#32476;&#25552;&#21462;&#19982;&#35270;&#35282;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#23558;&#27573;&#35270;&#20026;&#39030;&#28857;&#24182;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26500;&#24314;&#36229;&#36793;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19316v1 Announce Type: cross  Abstract: Action recognition from video data forms a cornerstone with wide-ranging applications. Single-view action recognition faces limitations due to its reliance on a single viewpoint. In contrast, multi-view approaches capture complementary information from various viewpoints for improved accuracy. Recently, event cameras have emerged as innovative bio-inspired sensors, leading to advancements in event-based action recognition. However, existing works predominantly focus on single-view scenarios, leaving a gap in multi-view event data exploitation, particularly in challenges like information deficit and semantic misalignment. To bridge this gap, we introduce HyperMV, a multi-view event-based action recognition framework. HyperMV converts discrete event data into frame-like representations and extracts view-related features using a shared convolutional network. By treating segments as vertices and constructing hyperedges using rule-based and
&lt;/p&gt;</description></item><item><title>FlowDepth&#25552;&#20986;&#20102;&#36890;&#36807;Dynamic Motion Flow Module&#65288;DMFM&#65289;&#35299;&#32806;&#20809;&#27969;&#65292;&#36890;&#36807;&#36816;&#29992;&#22522;&#20110;&#26426;&#21046;&#30340;&#26041;&#27861;&#35299;&#20915;&#31227;&#21160;&#29289;&#20307;&#24341;&#36215;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;Depth-Cue-Aware Blur&#65288;DCABlur&#65289;&#21644;Cost-Volume&#31232;&#30095;&#25439;&#22833;&#26469;&#35299;&#20915;&#39640;&#39057;&#21644;&#20302;&#32441;&#29702;&#21306;&#22495;&#30340;&#20809;&#24230;&#35823;&#24046;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19294</link><description>&lt;p&gt;
FlowDepth: &#35299;&#32806;&#20809;&#27969;&#29992;&#20110;&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19294
&lt;/p&gt;
&lt;p&gt;
FlowDepth&#25552;&#20986;&#20102;&#36890;&#36807;Dynamic Motion Flow Module&#65288;DMFM&#65289;&#35299;&#32806;&#20809;&#27969;&#65292;&#36890;&#36807;&#36816;&#29992;&#22522;&#20110;&#26426;&#21046;&#30340;&#26041;&#27861;&#35299;&#20915;&#31227;&#21160;&#29289;&#20307;&#24341;&#36215;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;Depth-Cue-Aware Blur&#65288;DCABlur&#65289;&#21644;Cost-Volume&#31232;&#30095;&#25439;&#22833;&#26469;&#35299;&#20915;&#39640;&#39057;&#21644;&#20302;&#32441;&#29702;&#21306;&#22495;&#30340;&#20809;&#24230;&#35823;&#24046;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#24103;&#26041;&#27861;&#30446;&#21069;&#22312;&#28145;&#24230;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#22240;&#20026;&#31227;&#21160;&#29289;&#20307;&#32780;&#36973;&#21463;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#36825;&#25171;&#30772;&#20102;&#38745;&#24577;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#22312;&#35745;&#31639;&#22270;&#20687;&#30340;&#39640;&#39057;&#25110;&#20302;&#32441;&#29702;&#21306;&#22495;&#30340;&#20809;&#24230;&#35823;&#24046;&#26102;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#39069;&#22806;&#30340;&#35821;&#20041;&#20808;&#39564;&#40657;&#30418;&#32593;&#32476;&#26469;&#20998;&#31163;&#31227;&#21160;&#29289;&#20307;&#65292;&#24182;&#20165;&#22312;&#25439;&#22833;&#27700;&#24179;&#19978;&#25913;&#36827;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FlowDepth&#65292;&#20854;&#20013;&#21160;&#24577;&#36816;&#21160;&#20809;&#27969;&#27169;&#22359;&#65288;DMFM&#65289;&#36890;&#36807;&#22522;&#20110;&#26426;&#21046;&#30340;&#26041;&#27861;&#35299;&#32806;&#20809;&#27969;&#65292;&#24182;&#23545;&#21160;&#24577;&#21306;&#22495;&#36827;&#34892;&#21464;&#24418;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#30001;&#39640;&#39057;&#21644;&#20302;&#32441;&#29702;&#21306;&#22495;&#24341;&#36215;&#30340;&#20809;&#24230;&#35823;&#24046;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#25105;&#20204;&#20998;&#21035;&#22312;&#36755;&#20837;&#21644;&#25439;&#22833;&#27700;&#24179;&#19978;&#20351;&#29992;Depth-Cue-Aware Blur&#65288;DCABlur&#65289;&#21644;Cost-Volume&#31232;&#30095;&#25439;&#22833;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19294v1 Announce Type: cross  Abstract: Self-supervised multi-frame methods have currently achieved promising results in depth estimation. However, these methods often suffer from mismatch problems due to the moving objects, which break the static assumption. Additionally, unfairness can occur when calculating photometric errors in high-freq or low-texture regions of the images. To address these issues, existing approaches use additional semantic priori black-box networks to separate moving objects and improve the model only at the loss level. Therefore, we propose FlowDepth, where a Dynamic Motion Flow Module (DMFM) decouples the optical flow by a mechanism-based approach and warps the dynamic regions thus solving the mismatch problem. For the unfairness of photometric errors caused by high-freq and low-texture regions, we use Depth-Cue-Aware Blur (DCABlur) and Cost-Volume sparsity loss respectively at the input and the loss level to solve the problem. Experimental results 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.19289</link><description>&lt;p&gt;
&#29992;&#20110;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Treatment Effect Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19289
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24448;&#24448;&#28041;&#21450;&#26114;&#36149;&#30340;&#27835;&#30103;&#20998;&#37197;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#35774;&#32622;&#20013;&#21487;&#33021;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#36825;&#31181;&#27835;&#30103;&#25928;&#26524;&#32780;&#26080;&#38656;&#23454;&#38469;&#24178;&#39044;&#26159;&#20943;&#23569;&#39118;&#38505;&#30340;&#19968;&#31181;&#26631;&#20934;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#23454;&#39564;&#26500;&#24314;&#30340;&#35757;&#32451;&#38598;&#65292;&#22240;&#27492;&#20174;&#26681;&#26412;&#19978;&#23384;&#22312;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#20381;&#36182;&#20110;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#22270;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#35270;&#20026;&#20855;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#35760;&#23454;&#20363;&#30340;&#33410;&#28857;&#22238;&#24402;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#20808;&#21069;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#21452;&#27169;&#22411;&#31070;&#32463;&#26550;&#26500;&#65292;&#24182;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#39069;&#22806;&#27493;&#39588;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#19982;&#33719;&#21462;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#20197;&#24341;&#23548;&#20449;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19289v1 Announce Type: cross  Abstract: Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#31574;&#30053;&#19978;&#30340;&#22870;&#21169;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#31574;&#30053;&#26679;&#26412;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#20998;&#24067;&#19978;&#30340;&#19968;&#33268;&#24615;</title><link>https://arxiv.org/abs/2403.19279</link><description>&lt;p&gt;
&#20351;&#29992;&#22870;&#21169;&#23398;&#20064;&#22312;&#31574;&#30053;&#19978;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models with Reward Learning on Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19279
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#31574;&#30053;&#19978;&#30340;&#22870;&#21169;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#31574;&#30053;&#26679;&#26412;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#20998;&#24067;&#19978;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#20986;&#29616;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;RLHF&#21253;&#21547;&#19977;&#20010;&#27493;&#39588;&#65292;&#21363;&#25910;&#38598;&#20154;&#31867;&#20559;&#22909;&#12289;&#22870;&#21169;&#23398;&#20064;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#36890;&#24120;&#26159;&#20018;&#34892;&#25191;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#65288;&#22266;&#23450;&#30340;&#65289;&#22870;&#21169;&#27169;&#22411;&#21487;&#33021;&#20250;&#22240;&#20026;&#31574;&#30053;&#20248;&#21270;&#19981;&#26029;&#25913;&#21464;LLMs&#30340;&#25968;&#25454;&#20998;&#24067;&#32780;&#36973;&#21463;&#19981;&#20934;&#30830;&#30340;&#31163;&#20998;&#24067;&#24773;&#20917;&#12290;&#20174;&#26368;&#26032;&#30340;LLMs&#37325;&#22797;&#25910;&#38598;&#26032;&#30340;&#20559;&#22909;&#25968;&#25454;&#21487;&#33021;&#20250;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20250;&#20351;&#24471;&#32467;&#26524;&#31995;&#32479;&#26356;&#21152;&#22797;&#26434;&#21644;&#38590;&#20197;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#31574;&#30053;&#19978;&#30340;&#22870;&#21169;&#23398;&#20064;&#65288;RLP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#31574;&#30053;&#26679;&#26412;&#26469;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#20998;&#24067;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#31574;&#30053;&#26679;&#26412;&#30340;&#31283;&#20581;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19279v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences. RLHF contains three steps, i.e., human preference collecting, reward learning, and policy optimization, which are usually performed serially. Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs' data distribution. Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize. In this paper, we propose reward learning on policy (RLP), an unsupervised framework that refines a reward model using policy samples to keep it on-distribution. Specifically, an unsupervised multi-view learning method is introduced to learn robust representations of policy samples. Meanwhile, a synthetic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#22303;&#22756;&#33829;&#20859;&#21644;&#27668;&#35937;&#22240;&#32032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20316;&#29289;&#20135;&#37327;&#21644;&#30149;&#23475;&#39044;&#27979;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#23391;&#21152;&#25289;&#22269;&#20892;&#19994;&#20013;&#20316;&#29289;&#36873;&#25321;&#21644;&#30149;&#23475;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19273</link><description>&lt;p&gt;
&#19968;&#31181;&#25972;&#21512;&#22303;&#22756;&#33829;&#20859;&#21644;&#27668;&#35937;&#22240;&#32032;&#30340;&#20316;&#29289;&#20135;&#37327;&#21644;&#30149;&#23475;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Approach for Crop Yield and Disease Prediction Integrating Soil Nutrition and Weather Factors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#22303;&#22756;&#33829;&#20859;&#21644;&#27668;&#35937;&#22240;&#32032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20316;&#29289;&#20135;&#37327;&#21644;&#30149;&#23475;&#39044;&#27979;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#23391;&#21152;&#25289;&#22269;&#20892;&#19994;&#20013;&#20316;&#29289;&#36873;&#25321;&#21644;&#30149;&#23475;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#26234;&#33021;&#30340;&#20892;&#19994;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#23391;&#21152;&#25289;&#22269;&#30340;&#20316;&#29289;&#36873;&#25321;&#21644;&#30149;&#23475;&#39044;&#27979;&#12290;&#35813;&#22269;&#23478;&#30340;&#32463;&#27982;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20892;&#19994;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#29983;&#20135;&#29575;&#26356;&#39640;&#30340;&#20316;&#29289;&#20197;&#21450;&#26377;&#25928;&#25511;&#21046;&#20316;&#29289;&#30149;&#23475;&#26159;&#20892;&#27665;&#24517;&#39035;&#38754;&#23545;&#30340;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25512;&#33616;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#22810;&#31181;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20316;&#29289;&#29983;&#20135;&#12289;&#22303;&#22756;&#26465;&#20214;&#12289;&#20892;&#19994;&#27668;&#35937;&#21306;&#22495;&#12289;&#20316;&#29289;&#30149;&#23475;&#21644;&#27668;&#35937;&#22240;&#32032;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#20851;&#20110;&#30149;&#23475;&#36235;&#21183;&#12289;&#20316;&#29289;&#23545;&#22303;&#22756;&#33829;&#20859;&#38656;&#27714;&#20197;&#21450;&#20892;&#19994;&#29983;&#20135;&#21382;&#21490;&#30340;&#26377;&#35265;&#22320;&#20449;&#24687;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#30693;&#35782;&#65292;&#35813;&#27169;&#22411;&#39318;&#20808;&#26681;&#25454;&#29305;&#23450;&#29992;&#25143;&#20301;&#32622;&#30340;&#22303;&#22756;&#33829;&#20859;&#25512;&#33616;&#20027;&#35201;&#36873;&#23450;&#20316;&#29289;&#30340;&#21015;&#34920;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#36827;&#34892;&#25972;&#21512;&#65292;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#20316;&#29289;&#30340;&#20135;&#37327;&#21644;&#30149;&#23475;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19273v1 Announce Type: cross  Abstract: The development of an intelligent agricultural decision-supporting system for crop selection and disease forecasting in Bangladesh is the main objective of this work. The economy of the nation depends heavily on agriculture. However, choosing crops with better production rates and efficiently controlling crop disease are obstacles that farmers have to face. These issues are addressed in this research by utilizing machine learning methods and real-world datasets. The recommended approach uses a variety of datasets on the production of crops, soil conditions, agro-meteorological regions, crop disease, and meteorological factors. These datasets offer insightful information on disease trends, soil nutrition demand of crops, and agricultural production history. By incorporating this knowledge, the model first recommends the list of primarily selected crops based on the soil nutrition of a particular user location. Then the predictions of me
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#33258;&#30417;&#30563;&#27979;&#36317;&#35823;&#24046;&#26657;&#27491;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#25910;&#38598;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#19982;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;</title><link>https://arxiv.org/abs/2403.19262</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#25910;&#38598;&#65306;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#33258;&#30417;&#30563;&#27979;&#36317;&#35823;&#24046;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Removing the need for ground truth UWB data collection: self-supervised ranging error correction using deep reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#33258;&#30417;&#30563;&#27979;&#36317;&#35823;&#24046;&#26657;&#27491;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#25910;&#38598;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#19982;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#23450;&#20301;&#21033;&#29992;UWB&#25216;&#26415;&#22240;&#20854;&#21400;&#31859;&#32423;&#20934;&#30830;&#24230;&#28508;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22810;&#24452;&#25928;&#24212;&#21644;&#38750;&#30452;&#23556;&#26465;&#20214;&#23548;&#33268;&#20102;&#22522;&#31449;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#27979;&#36317;&#35823;&#24046;&#12290;&#29616;&#26377;&#30340;&#32531;&#35299;&#36825;&#20123;&#27979;&#36317;&#35823;&#24046;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#25910;&#38598;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#19981;&#20999;&#23454;&#38469;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#12290;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23558;&#36890;&#36947;&#33033;&#20914;&#21709;&#24212;&#20316;&#20026;&#29366;&#24577;&#65292;&#24182;&#39044;&#27979;&#26657;&#27491;&#20197;&#20943;&#23567;&#26657;&#27491;&#21644;&#20272;&#35745;&#27979;&#36317;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;&#35813;&#20195;&#29702;&#36890;&#36807;&#32467;&#21512;&#36712;&#36857;&#30340;&#21487;&#39044;&#27979;&#24615;&#19982;&#36807;&#28388;&#21644;&#24179;&#28369;&#22788;&#29702;&#29983;&#25104;&#30340;&#26657;&#27491;&#65292;&#33258;&#30417;&#30563;&#22320;&#23398;&#20064;&#21644;&#36845;&#20195;&#25913;&#36827;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;UWB&#27979;&#37327;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20854;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19262v1 Announce Type: cross  Abstract: Indoor positioning using UWB technology has gained interest due to its centimeter-level accuracy potential. However, multipath effects and non-line-of-sight conditions cause ranging errors between anchors and tags. Existing approaches for mitigating these ranging errors rely on collecting large labeled datasets, making them impractical for real-world deployments. This paper proposes a novel self-supervised deep reinforcement learning approach that does not require labeled ground truth data. A reinforcement learning agent uses the channel impulse response as a state and predicts corrections to minimize the error between corrected and estimated ranges. The agent learns, self-supervised, by iteratively improving corrections that are generated by combining the predictability of trajectories with filtering and smoothening. Experiments on real-world UWB measurements demonstrate comparable performance to state-of-the-art supervised methods, o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28508;&#22312;&#26102;&#38388;&#31232;&#30095;&#21327;&#35843;&#22270;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#20851;&#31995;&#24182;&#21033;&#29992;&#21382;&#21490;&#35266;&#27979;&#26469;&#36827;&#34892;&#30693;&#35782;&#20132;&#25442;</title><link>https://arxiv.org/abs/2403.19253</link><description>&lt;p&gt;
&#25512;&#26029;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28508;&#22312;&#26102;&#38388;&#31232;&#30095;&#21327;&#35843;&#22270;
&lt;/p&gt;
&lt;p&gt;
Inferring Latent Temporal Sparse Coordination Graph for Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19253
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28508;&#22312;&#26102;&#38388;&#31232;&#30095;&#21327;&#35843;&#22270;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#20851;&#31995;&#24182;&#21033;&#29992;&#21382;&#21490;&#35266;&#27979;&#26469;&#36827;&#34892;&#30693;&#35782;&#20132;&#25442;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#26234;&#33021;&#20307;&#21327;&#35843;&#23545;&#20110;&#21512;&#20316;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;MARL&#20013;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;&#23616;&#38480;&#24615;&#36739;&#22823;&#65292;&#20165;&#20165;&#20381;&#36182;&#19968;&#27493;&#35266;&#23519;&#65292;&#24573;&#30053;&#20102;&#37325;&#35201;&#30340;&#21382;&#21490;&#32463;&#39564;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#22270;&#23384;&#22312;&#32570;&#38519;&#65292;&#20419;&#36827;&#20102;&#20887;&#20313;&#25110;&#26377;&#23475;&#20449;&#24687;&#20132;&#25442;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25512;&#26029;&#29992;&#20110;MARL&#30340;&#28508;&#22312;&#26102;&#38388;&#31232;&#30095;&#21327;&#35843;&#22270;&#65288;LTS-CG&#65289;&#12290;LTS-CG&#21033;&#29992;&#26234;&#33021;&#20307;&#30340;&#21382;&#21490;&#35266;&#27979;&#26469;&#35745;&#31639;&#26234;&#33021;&#20307;&#23545;&#27010;&#29575;&#30697;&#38453;&#65292;&#20174;&#20013;&#25277;&#21462;&#31232;&#30095;&#22270;&#24182;&#29992;&#20110;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#25442;&#65292;&#20174;&#32780;&#21516;&#26102;&#25429;&#25417;&#26234;&#33021;&#20307;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20851;&#31995;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#36807;&#31243;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20165;&#19982;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19253v1 Announce Type: new  Abstract: Effective agent coordination is crucial in cooperative Multi-Agent Reinforcement Learning (MARL). While agent cooperation can be represented by graph structures, prevailing graph learning methods in MARL are limited. They rely solely on one-step observations, neglecting crucial historical experiences, leading to deficient graphs that foster redundant or detrimental information exchanges. Additionally, high computational demands for action-pair calculations in dense graphs impede scalability. To address these challenges, we propose inferring a Latent Temporal Sparse Coordination Graph (LTS-CG) for MARL. The LTS-CG leverages agents' historical observations to calculate an agent-pair probability matrix, where a sparse graph is sampled from and used for knowledge exchange between agents, thereby simultaneously capturing agent dependencies and relation uncertainty. The computational complexity of this procedure is only related to the number o
&lt;/p&gt;</description></item><item><title>MPXGAT&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#37325;&#22270;&#23884;&#20837;&#65292;&#36890;&#36807;&#32467;&#21512;GATs&#22312;&#23618;&#20869;&#21644;&#23618;&#38388;&#36830;&#25509;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#39044;&#27979;&#22810;&#37325;&#32593;&#32476;&#20013;&#21508;&#23618;&#20869;&#21644;&#23618;&#38388;&#30340;&#38142;&#25509;&#12290;</title><link>https://arxiv.org/abs/2403.19246</link><description>&lt;p&gt;
MPXGAT&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#37325;&#22270;&#23884;&#20837;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MPXGAT: An Attention based Deep Learning Model for Multiplex Graphs Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19246
&lt;/p&gt;
&lt;p&gt;
MPXGAT&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#37325;&#22270;&#23884;&#20837;&#65292;&#36890;&#36807;&#32467;&#21512;GATs&#22312;&#23618;&#20869;&#21644;&#23618;&#38388;&#36830;&#25509;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#39044;&#27979;&#22810;&#37325;&#32593;&#32476;&#20013;&#21508;&#23618;&#20869;&#21644;&#23618;&#38388;&#30340;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#36805;&#36895;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#23613;&#31649;&#20854;&#26085;&#30410;&#26222;&#21450;&#65292;&#20294;&#22823;&#37096;&#20998;&#30740;&#31350;&#20173;&#20165;&#38480;&#20110;&#23884;&#20837;&#21333;&#23618;&#22270;&#65292;&#36825;&#22312;&#34920;&#31034;&#20855;&#26377;&#22810;&#26041;&#38754;&#20851;&#31995;&#30340;&#22797;&#26434;&#31995;&#32479;&#26102;&#19981;&#36275;&#20197;&#32988;&#20219;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MPXGAT&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#22810;&#37325;&#22270;&#23884;&#20837;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GATs&#65289;&#30340;&#31283;&#20581;&#24615;&#65292;MPXGAT&#36890;&#36807;&#21033;&#29992;&#23618;&#20869;&#21644;&#23618;&#38388;&#36830;&#25509;&#25429;&#25417;&#22810;&#37325;&#32593;&#32476;&#30340;&#32467;&#26500;&#12290;&#36825;&#31181;&#21033;&#29992;&#26377;&#21161;&#20110;&#20934;&#30830;&#39044;&#27979;&#32593;&#32476;&#22810;&#20010;&#23618;&#20869;&#21644;&#23618;&#38388;&#30340;&#38142;&#25509;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#23454;&#65292;MPXGAT&#22987;&#32456;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31454;&#20105;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19246v1 Announce Type: new  Abstract: Graph representation learning has rapidly emerged as a pivotal field of study. Despite its growing popularity, the majority of research has been confined to embedding single-layer graphs, which fall short in representing complex systems with multifaceted relationships. To bridge this gap, we introduce MPXGAT, an innovative attention-based deep learning model tailored to multiplex graph embedding. Leveraging the robustness of Graph Attention Networks (GATs), MPXGAT captures the structure of multiplex networks by harnessing both intra-layer and inter-layer connections. This exploitation facilitates accurate link prediction within and across the network's multiple layers. Our comprehensive experimental evaluation, conducted on various benchmark datasets, confirms that MPXGAT consistently outperforms state-of-the-art competing algorithms.
&lt;/p&gt;</description></item><item><title>&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19243</link><description>&lt;p&gt;
&#29992;&#27491;&#24358;&#28608;&#27963;&#30340;&#20302;&#31209;&#30697;&#38453;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sine Activated Low-Rank Matrices for Parameter Efficient Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19243
&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#20998;&#35299;&#24050;&#32463;&#25104;&#20026;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#22686;&#24378;&#21442;&#25968;&#25928;&#29575;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20123;&#25216;&#26415;&#26174;&#33879;&#38477;&#20302;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#21462;&#24471;&#20102;&#31616;&#27905;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#26159;&#22312;&#21442;&#25968;&#25928;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#20570;&#20986;&#22949;&#21327;&#65292;&#21442;&#25968;&#20943;&#23569;&#24448;&#24448;&#23548;&#33268;&#20934;&#30830;&#24615;&#19981;&#21450;&#23436;&#25972;&#31209;&#23545;&#24212;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#22312;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#25972;&#21512;&#20102;&#19968;&#20010;&#27491;&#24358;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20445;&#30041;&#20102;&#20302;&#31209;&#26041;&#27861;&#30340;&#21442;&#25968;&#25928;&#29575;&#29305;&#24615;&#30340;&#22909;&#22788;&#65292;&#36824;&#22686;&#21152;&#20102;&#20998;&#35299;&#30340;&#31209;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#29616;&#26377;&#20302;&#31209;&#27169;&#22411;&#30340;&#19968;&#31181;&#36866;&#24212;&#24615;&#22686;&#24378;&#65292;&#27491;&#22914;&#20854;&#25104;&#21151;&#35777;&#23454;&#30340;&#37027;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19243v1 Announce Type: new  Abstract: Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model accuracy. Our method proves to be an adaptable enhancement for existing low-rank models, as evidenced by its successful 
&lt;/p&gt;</description></item><item><title>AZ-NAS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#21512;&#21508;&#31181;&#38646;&#25104;&#26412;&#20195;&#29702;&#26469;&#22686;&#24378;&#32593;&#32476;&#25490;&#21517;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#34920;&#36798;&#24615;&#12289;&#36827;&#27493;&#24615;&#12289;&#21487;&#35757;&#32451;&#24615;&#21644;&#22797;&#26434;&#24615;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.19232</link><description>&lt;p&gt;
AZ-NAS: &#38024;&#23545;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#30340;&#38646;&#25104;&#26412;&#20195;&#29702;&#32452;&#35013;
&lt;/p&gt;
&lt;p&gt;
AZ-NAS: Assembling Zero-Cost Proxies for Network Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19232
&lt;/p&gt;
&lt;p&gt;
AZ-NAS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#21512;&#21508;&#31181;&#38646;&#25104;&#26412;&#20195;&#29702;&#26469;&#22686;&#24378;&#32593;&#32476;&#25490;&#21517;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#34920;&#36798;&#24615;&#12289;&#36827;&#27493;&#24615;&#12289;&#21487;&#35757;&#32451;&#24615;&#21644;&#22797;&#26434;&#24615;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#20813;&#36153;&#30340;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26088;&#22312;&#21457;&#29616;&#20855;&#26377;&#38646;&#25104;&#26412;&#20195;&#29702;&#30340;&#34920;&#29616;&#20248;&#24322;&#30340;&#32593;&#32476;&#65292;&#25429;&#33719;&#19982;&#26368;&#32456;&#24615;&#33021;&#30456;&#20851;&#30340;&#32593;&#32476;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#35757;&#32451;&#20813;&#36153;&#30340;NAS&#26041;&#27861;&#20272;&#35745;&#30340;&#32593;&#32476;&#25490;&#21517;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36739;&#24369;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AZ-NAS&#65292;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21508;&#31181;&#38646;&#25104;&#26412;&#20195;&#29702;&#30340;&#38598;&#21512;&#26469;&#26174;&#33879;&#22686;&#24378;&#39044;&#27979;&#30340;&#32593;&#32476;&#25490;&#21517;&#19982;&#24615;&#33021;&#22320;&#38754;&#23454;&#20917;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22235;&#31181;&#26032;&#30340;&#38646;&#25104;&#26412;&#20195;&#29702;&#65292;&#36825;&#20123;&#20195;&#29702;&#22312;&#34920;&#36798;&#24615;&#12289;&#36827;&#27493;&#24615;&#12289;&#21487;&#35757;&#32451;&#24615;&#21644;&#22797;&#26434;&#24615;&#31561;&#26041;&#38754;&#30456;&#36741;&#30456;&#25104;&#65292;&#20998;&#26512;&#20102;&#26550;&#26500;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;&#20195;&#29702;&#35780;&#20998;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#20013;&#21516;&#26102;&#33719;&#24471;&#65292;&#20351;&#25972;&#20010;NAS&#36807;&#31243;&#26497;&#20854;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19232v1 Announce Type: cross  Abstract: Training-free network architecture search (NAS) aims to discover high-performing networks with zero-cost proxies, capturing network characteristics related to the final performance. However, network rankings estimated by previous training-free NAS methods have shown weak correlations with the performance. To address this issue, we propose AZ-NAS, a novel approach that leverages the ensemble of various zero-cost proxies to enhance the correlation between a predicted ranking of networks and the ground truth substantially in terms of the performance. To achieve this, we introduce four novel zero-cost proxies that are complementary to each other, analyzing distinct traits of architectures in the views of expressivity, progressivity, trainability, and complexity. The proxy scores can be obtained simultaneously within a single forward and backward pass, making an overall NAS process highly efficient. In order to integrate the rankings predic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#19981;&#20165;&#20851;&#27880;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;</title><link>https://arxiv.org/abs/2403.19211</link><description>&lt;p&gt;
&#20026;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#25552;&#20379;&#21452;&#37325;&#20010;&#24615;&#21270;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dual-Personalizing Adapter for Federated Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19211
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#19981;&#20165;&#20851;&#27880;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24494;&#35843;&#22823;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#23637;&#29616;&#20986;&#20102;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#65288;FL&#65289;&#29615;&#22659;&#19979;&#36890;&#36807;&#21033;&#29992;&#35768;&#22810;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#36827;&#34892;&#21327;&#20316;&#24494;&#35843;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#38750;IID&#25968;&#25454;&#12290;&#20026;&#20102;&#20943;&#36731;&#36890;&#20449;&#21644;&#35745;&#31639;&#24320;&#38144;&#65292;&#24341;&#20837;&#20102;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#19988;&#19968;&#20123;&#30740;&#31350;&#23558;&#20010;&#24615;&#21270;&#26041;&#27861;&#35843;&#25972;&#20026;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#21475;&#26159;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#24573;&#30053;&#20102;&#27979;&#35797;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#23427;&#19981;&#20165;&#19987;&#27880;&#20110;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19211v1 Announce Type: cross  Abstract: Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning large amounts of instruction data. Notably, federated foundation models emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to federated foundation models for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications. Therefore, to bridge this gap, we propose a new setting, termed test-time personalization, which not only concentrates on the targeted local task but also extends to other tasks that exhibit test-t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#22330;&#30340;&#21021;&#22987;&#21270;&#21644;&#28608;&#27963;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#32593;&#32476;&#21021;&#22987;&#21270;&#12289;&#26550;&#26500;&#36873;&#25321;&#21644;&#20248;&#21270;&#36807;&#31243;&#20043;&#38388;&#30340;&#28145;&#23618;&#32852;&#31995;&#65292;&#20026;&#31070;&#32463;&#22330;&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.19205</link><description>&lt;p&gt;
&#20174;&#28608;&#27963;&#21040;&#21021;&#22987;&#21270;&#65306;&#20248;&#21270;&#31070;&#32463;&#22330;&#30340;&#25193;&#23637;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
From Activation to Initialization: Scaling Insights for Optimizing Neural Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#22330;&#30340;&#21021;&#22987;&#21270;&#21644;&#28608;&#27963;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#32593;&#32476;&#21021;&#22987;&#21270;&#12289;&#26550;&#26500;&#36873;&#25321;&#21644;&#20248;&#21270;&#36807;&#31243;&#20043;&#38388;&#30340;&#28145;&#23618;&#32852;&#31995;&#65292;&#20026;&#31070;&#32463;&#22330;&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#31070;&#32463;&#22330;&#20316;&#20026;&#19968;&#31181;&#24403;&#20195;&#24037;&#20855;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20449;&#21495;&#34920;&#31034;&#32780;&#22791;&#21463;&#37325;&#35270;&#12290;&#23613;&#31649;&#24050;&#32463;&#22312;&#20351;&#36825;&#20123;&#32593;&#32476;&#36866;&#24212;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#35813;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#28145;&#20837;&#25506;&#35752;&#21021;&#22987;&#21270;&#21644;&#28608;&#27963;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20026;&#31070;&#32463;&#22330;&#30340;&#24378;&#21270;&#20248;&#21270;&#25552;&#20379;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#35265;&#35299;&#25581;&#31034;&#20102;&#32593;&#32476;&#21021;&#22987;&#21270;&#12289;&#26550;&#26500;&#36873;&#25321;&#21644;&#20248;&#21270;&#36807;&#31243;&#20043;&#38388;&#30340;&#28145;&#23618;&#32852;&#31995;&#65292;&#24378;&#35843;&#22312;&#35774;&#35745;&#23574;&#31471;&#31070;&#32463;&#22330;&#26102;&#38656;&#35201;&#37319;&#29992;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19205v1 Announce Type: cross  Abstract: In the realm of computer vision, Neural Fields have gained prominence as a contemporary tool harnessing neural networks for signal representation. Despite the remarkable progress in adapting these networks to solve a variety of problems, the field still lacks a comprehensive theoretical framework. This article aims to address this gap by delving into the intricate interplay between initialization and activation, providing a foundational basis for the robust optimization of Neural Fields. Our theoretical insights reveal a deep-seated connection among network initialization, architectural choices, and the optimization process, emphasizing the need for a holistic approach when designing cutting-edge Neural Fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.19181</link><description>&lt;p&gt;
&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#25490;&#21517;&#22120;
&lt;/p&gt;
&lt;p&gt;
Make Large Language Model a Better Ranker
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#26174;&#33879;&#22686;&#24378;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#25512;&#33616;&#31995;&#32479;&#65288;RSs&#65289;&#27010;&#24565;&#21644;&#24320;&#21457;&#26041;&#24335;&#21457;&#29983;&#20102;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#23545;&#28857;&#21644;&#25104;&#23545;&#25512;&#33616;&#33539;&#24335;&#19978;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#22120;&#20013;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#19968;&#20123;&#30740;&#31350;&#34429;&#28982;&#28145;&#20837;&#30740;&#31350;&#20102;&#21015;&#34920;&#22411;&#26041;&#27861;&#65292;&#20294;&#22312;&#25490;&#21517;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#19968;&#19981;&#36275;&#24402;&#22240;&#20110;&#25490;&#21517;&#21644;&#35821;&#35328;&#29983;&#25104;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#12290;ALRO&#26088;&#22312;&#24357;&#21512;LLMs&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#24494;&#22937;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;ALRO&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#24341;&#20837;&#20102;&#36719;lambda&#20540;lo
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19181v1 Announce Type: cross  Abstract: The evolution of Large Language Models (LLMs) has significantly enhanced capabilities across various fields, leading to a paradigm shift in how Recommender Systems (RSs) are conceptualized and developed. However, existing research primarily focuses on point-wise and pair-wise recommendation paradigms. These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models. While some studies have delved into list-wise approaches, they fall short in ranking tasks. This shortfall is attributed to the misalignment between the objectives of ranking and language generation. To this end, this paper introduces the Language Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks within recommender systems. A key feature of ALRO is the introduction of soft lambda lo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;BCFL&#65289;&#65292;&#31361;&#20986;&#20102;&#21306;&#22359;&#38142;&#30340;&#23433;&#20840;&#21151;&#33021;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#27169;&#24335;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.19178</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#22686;&#24378;&#20449;&#20219;&#21644;&#38544;&#31169;&#65306;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Enhancing Trust and Privacy in Distributed Networks: A Comprehensive Survey on Blockchain-based Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;BCFL&#65289;&#65292;&#31361;&#20986;&#20102;&#21306;&#22359;&#38142;&#30340;&#23433;&#20840;&#21151;&#33021;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#27169;&#24335;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#23384;&#22312;&#21333;&#28857;&#25925;&#38556;&#39118;&#38505;&#26102;&#65292;&#20687;&#21306;&#22359;&#38142;&#36825;&#26679;&#30340;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#36890;&#36807;&#22312;&#22810;&#20010;&#23454;&#20307;&#20043;&#38388;&#23454;&#26045;&#20849;&#35782;&#26426;&#21046;&#25552;&#20379;&#20102;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23558;&#20998;&#24067;&#24335;&#35745;&#31639;&#19982;&#21152;&#23494;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#21435;&#20013;&#24515;&#21270;&#25216;&#26415;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#33539;&#24335;&#12290;&#21306;&#22359;&#38142;&#36890;&#36807;&#22312;&#32593;&#32476;&#33410;&#28857;&#20043;&#38388;&#32463;&#36807;&#20849;&#35782;&#39564;&#35777;&#21644;&#35760;&#24405;&#20132;&#26131;&#26469;&#30830;&#20445;&#23433;&#20840;&#12289;&#36879;&#26126;&#21644;&#38450;&#31713;&#25913;&#30340;&#25968;&#25454;&#31649;&#29702;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#21442;&#19982;&#32773;&#33021;&#22815;&#22312;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#30340;&#21516;&#26102;&#36890;&#36807;&#36991;&#20813;&#30452;&#25509;&#21407;&#22987;&#25968;&#25454;&#20132;&#25442;&#26469;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#23613;&#31649;&#23545;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#20294;&#23427;&#20204;&#22312;FL&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#28145;&#20837;&#35843;&#26597;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;FL&#65288;BCFL&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#21306;&#22359;&#38142;&#30340;&#23433;&#20840;&#29305;&#24615;&#19982;FL&#30340;&#38544;&#31169;&#20445;&#25252;&#27169;&#24335;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19178v1 Announce Type: cross  Abstract: While centralized servers pose a risk of being a single point of failure, decentralized approaches like blockchain offer a compelling solution by implementing a consensus mechanism among multiple entities. Merging distributed computing with cryptographic techniques, decentralized technologies introduce a novel computing paradigm. Blockchain ensures secure, transparent, and tamper-proof data management by validating and recording transactions via consensus across network nodes. Federated Learning (FL), as a distributed machine learning framework, enables participants to collaboratively train models while safeguarding data privacy by avoiding direct raw data exchange. Despite the growing interest in decentralized methods, their application in FL remains underexplored. This paper presents a thorough investigation into Blockchain-based FL (BCFL), spotlighting the synergy between blockchain's security features and FL's privacy-preserving mo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#23545;&#25152;&#26377;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#22343;&#31561;&#37325;&#35201;&#24615;&#30340;&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#35780;&#20272;&#31639;&#27861;&#20844;&#24179;&#24615;&#65292;&#30830;&#20445;&#22312;&#20943;&#23569;&#20559;&#35265;&#21644;&#20840;&#23616;&#20998;&#31867;&#38169;&#35823;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.19165</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Evaluating Fair Feature Selection in Machine Learning for Healthcare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19165
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#23545;&#25152;&#26377;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#22343;&#31561;&#37325;&#35201;&#24615;&#30340;&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#35780;&#20272;&#31639;&#27861;&#20844;&#24179;&#24615;&#65292;&#30830;&#20445;&#22312;&#20943;&#23569;&#20559;&#35265;&#21644;&#20840;&#23616;&#20998;&#31867;&#38169;&#35823;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#33258;&#21160;&#21270;&#31038;&#20250;&#20559;&#35265;&#36827;&#19968;&#27493;&#21152;&#21095;&#20581;&#24247;&#24046;&#36317;&#30340;&#28508;&#21147;&#26500;&#25104;&#20102;&#37325;&#22823;&#39118;&#38505;&#12290;&#25105;&#20204;&#20174;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#25506;&#35752;&#31639;&#27861;&#20844;&#24179;&#24615;&#12290;&#20256;&#32479;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#36890;&#36807;&#21435;&#38500;&#36164;&#28304;&#23494;&#38598;&#12289;&#30456;&#20851;&#25110;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#26469;&#35782;&#21035;&#29992;&#20110;&#26356;&#22909;&#20915;&#31574;&#30340;&#29305;&#24449;&#65292;&#20294;&#24573;&#30053;&#20102;&#36825;&#20123;&#22240;&#32032;&#22312;&#19981;&#21516;&#23376;&#32676;&#20307;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31181;&#32771;&#34385;&#23545;&#25152;&#26377;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#22343;&#31561;&#37325;&#35201;&#24615;&#30340;&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20013;&#21516;&#26102;&#32771;&#34385;&#20102;&#20844;&#24179;&#24615;&#24230;&#37327;&#21644;&#38169;&#35823;&#24230;&#37327;&#65292;&#20197;&#30830;&#20445;&#22312;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#20559;&#35265;&#21644;&#20840;&#23616;&#20998;&#31867;&#38169;&#35823;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#25152;&#26377;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20844;&#24179;&#24615;&#25351;&#26631;&#24471;&#21040;&#25913;&#21892;&#65292;&#21516;&#26102;&#20998;&#31867;&#38169;&#35823;&#20165;&#26377;&#36731;&#24494;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19165v1 Announce Type: new  Abstract: With the universal adoption of machine learning in healthcare, the potential for the automation of societal biases to further exacerbate health disparities poses a significant risk. We explore algorithmic fairness from the perspective of feature selection. Traditional feature selection methods identify features for better decision making by removing resource-intensive, correlated, or non-relevant features but overlook how these factors may differ across subgroups. To counter these issues, we evaluate a fair feature selection method that considers equal importance to all demographic groups. We jointly considered a fairness metric and an error metric within the feature selection process to ensure a balance between minimizing both bias and global classification error. We tested our approach on three publicly available healthcare datasets. On all three datasets, we observed improvements in fairness metrics coupled with a minimal degradation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20165;&#36816;&#34892;&#26102;&#35299;&#30721;&#22120;&#30340;&#36229;&#32593;&#32476;&#65292;&#19981;&#20381;&#36182;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36328;&#23618;&#21442;&#25968;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2403.19163</link><description>&lt;p&gt;
D'OH: &#20165;&#35299;&#30721;&#22120;&#38543;&#26426;&#36229;&#32593;&#32476;&#29992;&#20110;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
D'OH: Decoder-Only random Hypernetworks for Implicit Neural Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20165;&#36816;&#34892;&#26102;&#35299;&#30721;&#22120;&#30340;&#36229;&#32593;&#32476;&#65292;&#19981;&#20381;&#36182;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36328;&#23618;&#21442;&#25968;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38544;&#24335;&#20989;&#25968;&#34987;&#21457;&#29616;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#32534;&#30721;&#21508;&#31181;&#33258;&#28982;&#20449;&#21495;&#12290;&#23427;&#20204;&#30340;&#21560;&#24341;&#21147;&#22312;&#20110;&#33021;&#22815;&#32039;&#20945;&#22320;&#34920;&#31034;&#20449;&#21495;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#30340;&#38544;&#24335;&#20559;&#24046;&#26469;&#35299;&#32806;&#20449;&#21495;&#20013;&#30340;&#38544;&#34255;&#20887;&#20313;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#26679;&#19968;&#20010;&#20551;&#35774;&#65306;&#36890;&#36807;&#21033;&#29992;&#23618;&#20043;&#38388;&#23384;&#22312;&#30340;&#20887;&#20313;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#21387;&#32553;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20165;&#36816;&#34892;&#26102;&#35299;&#30721;&#22120;&#30340;&#36229;&#32593;&#32476; - &#23427;&#19981;&#20351;&#29992;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454; - &#26469;&#26356;&#22909;&#22320;&#24314;&#27169;&#36328;&#23618;&#21442;&#25968;&#20887;&#20313;&#12290;&#20808;&#21069;&#22312;&#28145;&#24230;&#38544;&#24335;&#20989;&#25968;&#20013;&#24212;&#29992;&#36229;&#32593;&#32476;&#30340;&#24212;&#29992;&#37117;&#37319;&#29992;&#20102;&#20381;&#36182;&#22823;&#37327;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#21069;&#39304;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26080;&#27861;&#27867;&#21270;&#21040;&#35757;&#32451;&#20449;&#21495;&#20043;&#22806;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#21021;&#22987;&#21270;&#36816;&#34892;&#26102;&#28145;&#24230;&#38544;&#24335;&#20989;&#25968;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19163v1 Announce Type: new  Abstract: Deep implicit functions have been found to be an effective tool for efficiently encoding all manner of natural signals. Their attractiveness stems from their ability to compactly represent signals with little to no off-line training data. Instead, they leverage the implicit bias of deep networks to decouple hidden redundancies within the signal. In this paper, we explore the hypothesis that additional compression can be achieved by leveraging the redundancies that exist between layers. We propose to use a novel run-time decoder-only hypernetwork - that uses no offline training data - to better model this cross-layer parameter redundancy. Previous applications of hyper-networks with deep implicit functions have applied feed-forward encoder/decoder frameworks that rely on large offline datasets that do not generalize beyond the signals they were trained on. We instead present a strategy for the initialization of run-time deep implicit func
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#20013;&#30340;&#38271;&#24230;&#38382;&#39064;&#23637;&#24320;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;DPO&#20013;&#26174;&#33879;&#30340;&#21033;&#29992;&#24773;&#20917;&#65292;&#24182;&#23558;&#20854;&#19982;&#20998;&#24067;&#22806;&#24341;&#23548;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>https://arxiv.org/abs/2403.19159</link><description>&lt;p&gt;
&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#20013;&#23558;&#38271;&#24230;&#19982;&#36136;&#37327;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Disentangling Length from Quality in Direct Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19159
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#20013;&#30340;&#38271;&#24230;&#38382;&#39064;&#23637;&#24320;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;DPO&#20013;&#26174;&#33879;&#30340;&#21033;&#29992;&#24773;&#20917;&#65292;&#24182;&#23558;&#20854;&#19982;&#20998;&#24067;&#22806;&#24341;&#23548;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF)&#26159;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;RLHF&#34987;&#35748;&#20026;&#21033;&#29992;&#20102;&#20154;&#31867;&#20559;&#22909;&#20013;&#30340;&#20559;&#35265;&#65292;&#27604;&#22914;&#20887;&#38271;&#24615;&#12290;&#31934;&#24515;&#26684;&#24335;&#21270;&#21644;&#38596;&#36777;&#30340;&#31572;&#26696;&#36890;&#24120;&#20250;&#34987;&#29992;&#25143;&#26356;&#39640;&#35780;&#20215;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#24110;&#21161;&#24615;&#21644;&#23458;&#35266;&#24615;&#19978;&#36739;&#20302;&#12290;&#19968;&#20123;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#26469;&#25511;&#21046;&#36825;&#20123;&#20559;&#35265;&#65292;&#22312;&#21476;&#20856;RLHF&#25991;&#29486;&#20013;&#36825;&#20010;&#38382;&#39064;&#24050;&#26377;&#25152;&#25506;&#35752;&#65292;&#20294;&#23545;&#20110;&#30452;&#25509;&#23545;&#40784;&#31639;&#27861;&#22914;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#36825;&#20010;&#38382;&#39064;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#19982;&#21476;&#20856;RLHF&#19981;&#21516;&#65292;DPO&#19981;&#35757;&#32451;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#25110;&#30452;&#25509;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#22240;&#27492;&#20043;&#21069;&#29992;&#26469;&#25511;&#21046;&#20887;&#38271;&#24615;&#30340;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20570;&#20986;&#20102;&#20960;&#28857;&#36129;&#29486;&#12290;&#39318;&#27425;&#22312;DPO&#29615;&#22659;&#20013;&#30740;&#31350;&#38271;&#24230;&#38382;&#39064;&#65292;&#26174;&#31034;DPO&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#21033;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;&#20998;&#24067;&#22806;&#24341;&#23548;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19159v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has been a crucial component in the recent success of Large Language Models. However, RLHF is know to exploit biases in human preferences, such as verbosity. A well-formatted and eloquent answer is often more highly rated by users, even when it is less helpful and objective. A number of approaches have been developed to control those biases in the classical RLHF literature, but the problem remains relatively under-explored for Direct Alignment Algorithms such as Direct Preference Optimization (DPO). Unlike classical RLHF, DPO does not train a separate reward model or use reinforcement learning directly, so previous approaches developed to control verbosity cannot be directly applied to this setting. Our work makes several contributions. For the first time, we study the length problem in the DPO setting, showing significant exploitation in DPO and linking it to out-of-distribution bootstra
&lt;/p&gt;</description></item><item><title>&#22312;&#28151;&#21512;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#20998;&#31163;&#20223;&#23556;&#21442;&#25968;&#27604;&#20998;&#31163;&#32479;&#35745;&#25968;&#25454;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#21457;&#25381;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.19150</link><description>&lt;p&gt;
&#25506;&#32034;&#28151;&#21512;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#21452;&#37325;&#25209;&#37327;&#24402;&#19968;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Dual BN In Hybrid Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19150
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28151;&#21512;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#20998;&#31163;&#20223;&#23556;&#21442;&#25968;&#27604;&#20998;&#31163;&#32479;&#35745;&#25968;&#25454;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#21457;&#25381;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20851;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#20013;&#24212;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#30340;&#20851;&#27880;&#26085;&#30410;&#22686;&#38271;&#65292;&#23588;&#20854;&#26159;&#24403;&#27169;&#22411;&#21516;&#26102;&#22312;&#23545;&#25239;&#26679;&#26412;&#21644;&#24178;&#20928;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65288;&#31216;&#20026;&#28151;&#21512;-AT&#65289;&#26102;&#12290;&#19968;&#20010;&#20808;&#21069;&#30740;&#31350;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#20551;&#35774;&#23545;&#25239;&#26679;&#26412;&#21644;&#24178;&#20928;&#26679;&#26412;&#26469;&#33258;&#20004;&#20010;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#37319;&#29992;&#21452;&#37325;BN&#65292;&#20998;&#21035;&#29992;&#20110;&#23545;&#25239;&#20998;&#25903;&#21644;&#24178;&#20928;&#20998;&#25903;&#12290;&#28608;&#21169;&#21452;&#37325;BN&#30340;&#19968;&#31181;&#27969;&#34892;&#35266;&#24565;&#26159;&#65292;&#20272;&#35745;&#36825;&#31181;&#28151;&#21512;&#20998;&#24067;&#30340;&#35268;&#33539;&#21270;&#32479;&#35745;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#27492;&#20026;&#35268;&#33539;&#21270;&#32780;&#23558;&#20854;&#20998;&#24320;&#21487;&#20197;&#23454;&#29616;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#19982;&#36825;&#19968;&#35266;&#24565;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#65292;&#20998;&#31163;&#32479;&#35745;&#25968;&#25454;&#30340;&#20316;&#29992;&#27604;&#20998;&#31163;&#20223;&#23556;&#21442;&#25968;&#30340;&#20316;&#29992;&#36739;&#23567;&#12290;&#36825;&#19968;&#21457;&#29616;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#65288;Rebuffi&#31561;&#20154;&#65292;2023&#65289;&#19968;&#33268;&#65292;&#25105;&#20204;&#22312;&#20854;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19150v1 Announce Type: cross  Abstract: There is a growing concern about applying batch normalization (BN) in adversarial training (AT), especially when the model is trained on both adversarial samples and clean samples (termed Hybrid-AT). With the assumption that adversarial and clean samples are from two different domains, a common practice in prior works is to adopt Dual BN, where BN and BN are used for adversarial and clean branches, respectively. A popular belief for motivating Dual BN is that estimating normalization statistics of this mixture distribution is challenging and thus disentangling it for normalization achieves stronger robustness. In contrast to this belief, we reveal that disentangling statistics plays a less role than disentangling affine parameters in model training. This finding aligns with prior work (Rebuffi et al., 2023), and we build upon their research for further investigations. We demonstrate that the domain gap between adversarial and clean sam
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25299;&#25169;&#24490;&#29615;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;CycGAT&#65289;&#65292;&#36890;&#36807;&#24490;&#29615;&#20851;&#32852;&#30697;&#38453;&#24314;&#31435;&#29420;&#31435;&#24490;&#29615;&#22522;&#30784;&#65292;&#32467;&#21512;&#24490;&#29615;&#22270;&#21367;&#31215;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#23545;&#22823;&#33041;&#21151;&#33021;&#36830;&#25509;&#20013;&#20851;&#38190;&#36335;&#24452;&#30340;&#23450;&#20301;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19149</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#24490;&#29615;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#22823;&#33041;&#21151;&#33021;&#36830;&#25509;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Topological Cycle Graph Attention Network for Brain Functional Connectivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19149
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25299;&#25169;&#24490;&#29615;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;CycGAT&#65289;&#65292;&#36890;&#36807;&#24490;&#29615;&#20851;&#32852;&#30697;&#38453;&#24314;&#31435;&#29420;&#31435;&#24490;&#29615;&#22522;&#30784;&#65292;&#32467;&#21512;&#24490;&#29615;&#22270;&#21367;&#31215;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#23545;&#22823;&#33041;&#21151;&#33021;&#36830;&#25509;&#20013;&#20851;&#38190;&#36335;&#24452;&#30340;&#23450;&#20301;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25299;&#25169;&#24490;&#29615;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;CycGAT&#65289;&#65292;&#26088;&#22312;&#25551;&#32472;&#22823;&#33041;&#21151;&#33021;&#22270;&#20013;&#30340;&#21151;&#33021;&#39592;&#24178;&#8212;&#8212;&#20449;&#21495;&#20256;&#36755;&#30340;&#20851;&#38190;&#36335;&#24452;&#8212;&#8212;&#19982;&#26080;&#20851;&#32039;&#35201;&#30340;&#20887;&#20313;&#36830;&#25509;&#65288;&#22260;&#32469;&#35813;&#26680;&#24515;&#32467;&#26500;&#24418;&#25104;&#24490;&#29615;&#65289;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#24490;&#29615;&#20851;&#32852;&#30697;&#38453;&#65292;&#24314;&#31435;&#20102;&#22270;&#20013;&#30340;&#29420;&#31435;&#24490;&#29615;&#22522;&#30784;&#65292;&#23558;&#20854;&#19982;&#36793;&#30340;&#20851;&#31995;&#36827;&#34892;&#26144;&#23556;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24490;&#29615;&#22270;&#21367;&#31215;&#65292;&#21033;&#29992;&#20174;&#24490;&#29615;&#20851;&#32852;&#30697;&#38453;&#23548;&#20986;&#30340;&#24490;&#29615;&#37051;&#25509;&#30697;&#38453;&#65292;&#20197;&#29305;&#23450;&#22320;&#36807;&#28388;&#24490;&#29615;&#22495;&#20013;&#30340;&#36793;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#36793;&#20301;&#32622;&#32534;&#30721;&#22686;&#24378;&#24490;&#29615;&#20013;&#30340;&#25299;&#25169;&#24847;&#35782;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24490;&#29615;&#22270;&#21367;&#31215;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#35777;&#26126;&#20102;CycGAT&#30340;&#23450;&#20301;&#33021;&#21147;&#65292;&#24182;&#22312;ABCD&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19149v1 Announce Type: new  Abstract: This study, we introduce a novel Topological Cycle Graph Attention Network (CycGAT), designed to delineate a functional backbone within brain functional graph--key pathways essential for signal transmissio--from non-essential, redundant connections that form cycles around this core structure. We first introduce a cycle incidence matrix that establishes an independent cycle basis within a graph, mapping its relationship with edges. We propose a cycle graph convolution that leverages a cycle adjacency matrix, derived from the cycle incidence matrix, to specifically filter edge signals in a domain of cycles. Additionally, we strengthen the representation power of the cycle graph convolution by adding an attention mechanism, which is further augmented by the introduction of edge positional encodings in cycles, to enhance the topological awareness of CycGAT. We demonstrate CycGAT's localization through simulation and its efficacy on an ABCD s
&lt;/p&gt;</description></item><item><title>LR-MPGNN&#27169;&#22411;&#37319;&#29992;&#20302;&#31209;&#36817;&#20284;&#25216;&#26415;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#21442;&#25968;&#25968;&#37327;&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.19143</link><description>&lt;p&gt;
&#29992;&#20110;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#30340;&#24494;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Tiny Graph Neural Networks for Radio Resource Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19143
&lt;/p&gt;
&lt;p&gt;
LR-MPGNN&#27169;&#22411;&#37319;&#29992;&#20302;&#31209;&#36817;&#20284;&#25216;&#26415;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#21442;&#25968;&#25968;&#37327;&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#38656;&#27714;&#28608;&#22686;&#65292;&#36843;&#20351;&#25105;&#20204;&#24320;&#21457;&#22797;&#26434;&#32780;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#26412;&#25991;&#36890;&#36807;&#20171;&#32461;&#19968;&#31181;&#38024;&#23545;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#23450;&#21046;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#8212;&#8212;&#20302;&#31209;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;LR-MPGNN&#65289;&#26469;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;LR-MPGNN&#30340;&#26680;&#24515;&#26159;&#37319;&#29992;&#20302;&#31209;&#36817;&#20284;&#25216;&#26415;&#65292;&#23558;&#20256;&#32479;&#30340;&#32447;&#24615;&#23618;&#26367;&#25442;&#20026;&#20854;&#20302;&#31209;&#23545;&#24212;&#23618;&#12290;&#36825;&#31181;&#21019;&#26032;&#35774;&#35745;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#21442;&#25968;&#25968;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#20851;&#38190;&#25351;&#26631;&#23545;LR-MPGNN&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#65306;&#27169;&#22411;&#22823;&#23567;&#12289;&#21442;&#25968;&#25968;&#37327;&#12289;&#36890;&#20449;&#31995;&#32479;&#30340;&#21152;&#26435;&#24635;&#36895;&#29575;&#20197;&#21450;&#26435;&#37325;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;LR-MPGNN&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#20943;&#23569;&#20102;60&#20493;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19143v1 Announce Type: new  Abstract: The surge in demand for efficient radio resource management has necessitated the development of sophisticated yet compact neural network architectures. In this paper, we introduce a novel approach to Graph Neural Networks (GNNs) tailored for radio resource management by presenting a new architecture: the Low Rank Message Passing Graph Neural Network (LR-MPGNN). The cornerstone of LR-MPGNN is the implementation of a low-rank approximation technique that substitutes the conventional linear layers with their low-rank counterparts. This innovative design significantly reduces the model size and the number of parameters. We evaluate the performance of the proposed LR-MPGNN model based on several key metrics: model size, number of parameters, weighted sum rate of the communication system, and the distribution of eigenvalues of weight matrices. Our extensive evaluations demonstrate that the LR-MPGNN model achieves a sixtyfold decrease in model 
&lt;/p&gt;</description></item><item><title>EvoEval&#36890;&#36807;&#23558;&#29616;&#26377;&#22522;&#20934;&#28436;&#21270;&#20026;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31243;&#24207;&#21512;&#25104;&#22522;&#20934;&#22871;&#20214;&#65292;&#20197;&#20805;&#20998;&#35780;&#20272;LLM&#32534;&#30721;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19114</link><description>&lt;p&gt;
&#39030;&#32423;&#25490;&#34892;&#27036; = &#39030;&#32423;&#32534;&#31243;&#33021;&#21147;&#65292;&#27704;&#36828;&#21527;&#65311;EvoEval: &#36890;&#36807;LLM&#28436;&#21270;&#32534;&#30721;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19114
&lt;/p&gt;
&lt;p&gt;
EvoEval&#36890;&#36807;&#23558;&#29616;&#26377;&#22522;&#20934;&#28436;&#21270;&#20026;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31243;&#24207;&#21512;&#25104;&#22522;&#20934;&#22871;&#20214;&#65292;&#20197;&#20805;&#20998;&#35780;&#20272;LLM&#32534;&#30721;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#24050;&#25104;&#20026;&#29983;&#25104;&#20195;&#30721;&#20219;&#21153;&#30340;&#39318;&#36873;&#65292;LLM&#30340;&#35757;&#32451;&#12289;&#24320;&#21457;&#21644;&#20351;&#29992;&#38543;&#30528;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#20195;&#30721;&#30340;LLM&#30340;&#25351;&#25968;&#22686;&#38271;&#32780;&#22686;&#21152;&#12290;&#20026;&#20102;&#35780;&#20272;LLM&#22312;&#32534;&#30721;&#19978;&#30340;&#33021;&#21147;&#65292;&#23398;&#26415;&#30028;&#21644;&#34892;&#19994;&#20174;&#19994;&#32773;&#20381;&#36182;&#20110;&#27969;&#34892;&#30340;&#20154;&#24037;&#21046;&#23450;&#30340;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#22522;&#20934;&#21482;&#21253;&#21547;&#20102;&#25968;&#37327;&#21644;&#31181;&#31867;&#38750;&#24120;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#27969;&#34892;&#24230;&#21644;&#24180;&#40836;&#65292;&#35768;&#22810;&#22522;&#20934;&#23481;&#26131;&#21457;&#29983;&#25968;&#25454;&#27844;&#28431;&#65292;&#20854;&#20013;&#31034;&#20363;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#22312;&#32593;&#32476;&#19978;&#25214;&#21040;&#65292;&#22240;&#27492;&#21487;&#33021;&#20986;&#29616;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#36825;&#20123;&#38480;&#21046;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#25105;&#20204;&#35201;&#25506;&#35752;&#65306;&#29616;&#26377;&#22522;&#20934;&#30340;&#25490;&#34892;&#27036;&#34920;&#29616;&#26159;&#21542;&#21487;&#38752;&#19988;&#20840;&#38754;&#36275;&#20197;&#34913;&#37327;LLM&#30340;&#31243;&#24207;&#21512;&#25104;&#33021;&#21147;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;EvoEval--&#19968;&#20010;&#36890;&#36807;&#23558;&#29616;&#26377;&#22522;&#20934;&#28436;&#21270;&#20026;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#32780;&#21019;&#24314;&#30340;&#31243;&#24207;&#21512;&#25104;&#22522;&#20934;&#22871;&#20214;&#65292;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;LLM&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19114v1 Announce Type: cross  Abstract: LLMs have become the go-to choice for code generation tasks, with an exponential increase in the training, development, and usage of LLMs specifically for code generation. To evaluate the ability of LLMs on code, both academic and industry practitioners rely on popular handcrafted benchmarks. However, prior benchmarks contain only a very limited set of problems, both in quantity and variety. Further, due to popularity and age, many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data. Such limitations inevitably lead us to inquire: Is the leaderboard performance on existing benchmarks reliable and comprehensive enough to measure the program synthesis ability of LLMs? To address this, we introduce EvoEval -- a program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains for a comprehensive evaluation of LLM coding a
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#24320;&#28304;&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#24037;&#20855;&#65288;GIST&#65289;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#65292;&#25913;&#36827;&#20102;&#25968;&#23383;&#20581;&#24247;AI&#31639;&#27861;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#12289;&#24739;&#32773;&#25252;&#29702;&#65292;&#24182;&#20943;&#23569;&#21307;&#30103;&#32416;&#32439;&#32034;&#36180;&#12290;</title><link>https://arxiv.org/abs/2403.19107</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#29992;&#20110;&#26222;&#36890;X&#23556;&#32447;&#30340;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Synthetic Medical Imaging Generation with Generative Adversarial Networks For Plain Radiographs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19107
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#24320;&#28304;&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#24037;&#20855;&#65288;GIST&#65289;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#65292;&#25913;&#36827;&#20102;&#25968;&#23383;&#20581;&#24247;AI&#31639;&#27861;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#12289;&#24739;&#32773;&#25252;&#29702;&#65292;&#24182;&#20943;&#23569;&#21307;&#30103;&#32416;&#32439;&#32034;&#36180;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#24433;&#20687;&#23398;&#20013;&#65292;&#30001;&#20110;&#24739;&#32773;&#38544;&#31169;&#38480;&#21046;&#20197;&#21450;&#22312;&#32597;&#35265;&#30142;&#30149;&#24773;&#20917;&#19979;&#33719;&#21462;&#36275;&#22815;&#25968;&#25454;&#21487;&#33021;&#20250;&#22256;&#38590;&#65292;&#36890;&#24120;&#20250;&#38480;&#21046;&#23545;&#25968;&#25454;&#30340;&#35775;&#38382;&#12290;&#35813;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#24320;&#21457;&#19968;&#31181;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#24320;&#28304;&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#24037;&#20855;&#65288;GAN Image Synthesis Tool&#65292;GIST&#65289;&#65292;&#26131;&#20110;&#20351;&#29992;&#19988;&#26131;&#20110;&#37096;&#32626;&#12290;&#35813;&#24037;&#20855;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#19982;&#29305;&#23450;&#24739;&#32773;&#26080;&#20851;&#30340;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#65292;&#26377;&#21161;&#20110;&#25913;&#36827;&#21644;&#26631;&#20934;&#21270;&#25968;&#23383;&#20581;&#24247;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#12290;&#20854;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#21253;&#25324;&#33021;&#22815;&#29983;&#25104;&#21457;&#30149;&#29575;&#36739;&#20302;&#30340;&#30149;&#21464;&#25110;&#21463;&#20260;&#30340;&#24433;&#20687;&#12290;&#25913;&#21892;&#25968;&#23383;&#20581;&#24247;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#24110;&#21161;&#24739;&#32773;&#25252;&#29702;&#65292;&#20943;&#23569;&#21307;&#30103;&#32416;&#32439;&#32034;&#36180;&#65292;&#24182;&#26368;&#32456;&#38477;&#20302;&#25972;&#20307;&#21307;&#30103;&#20445;&#20581;&#25104;&#26412;&#12290;&#35813;&#24037;&#20855;&#22312;&#29616;&#26377;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#31639;&#27861;&#22522;&#30784;&#19978;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19107v1 Announce Type: cross  Abstract: In medical imaging, access to data is commonly limited due to patient privacy restrictions and the issue that it can be difficult to acquire enough data in the case of rare diseases.[1] The purpose of this investigation was to develop a reusable open-source synthetic image generation pipeline, the GAN Image Synthesis Tool (GIST), that is easy to use as well as easy to deploy. The pipeline helps to improve and standardize AI algorithms in the digital health space by generating high quality synthetic image data that is not linked to specific patients. Its image generation capabilities include the ability to generate imaging of pathologies or injuries with low incidence rates. This improvement of digital health AI algorithms could improve diagnostic accuracy, aid in patient care, decrease medicolegal claims, and ultimately decrease the overall cost of healthcare. The pipeline builds on existing Generative Adversarial Networks (GANs) algor
&lt;/p&gt;</description></item><item><title>PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19103</link><description>&lt;p&gt;
&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#40657;&#30418;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19103
&lt;/p&gt;
&lt;p&gt;
PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#25552;&#31034;&#32780;&#23548;&#33268;&#24037;&#20316;&#32321;&#37325;&#12290;&#36825;&#19968;&#25361;&#25112;&#20419;&#20351;&#20102;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;T2I&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#20256;&#36882;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#38656;&#35201;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#30333;&#30418;&#35775;&#38382;&#65292;&#24182;&#20135;&#29983;&#38750;&#30452;&#35266;&#30340;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PRISM&#65292;&#36825;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#23601;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#29425;&#30340;&#21551;&#21457;&#65292;PRISM&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#32473;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#20505;&#36873;&#25552;&#31034;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;PRISM&#22312;&#20026;&#23545;&#35937;&#12289;&#26679;&#24335;&#31561;&#29983;&#25104;&#20934;&#30830;&#25552;&#31034;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19103v1 Announce Type: cross  Abstract: Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, sty
&lt;/p&gt;</description></item><item><title>&#20248;&#21270;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20351;&#20854;&#33021;&#22788;&#29702;&#20219;&#24847;&#36755;&#20837;&#25968;&#25454;&#32500;&#24230;&#65292;&#24182;&#21516;&#26102;&#20248;&#21270;&#20102;&#36741;&#21161;&#37327;&#23376;&#36164;&#28304;&#20998;&#37197;</title><link>https://arxiv.org/abs/2403.19099</link><description>&lt;p&gt;
&#20026;&#20219;&#24847;&#25968;&#25454;&#32500;&#24230;&#20248;&#21270;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Optimizing Quantum Convolutional Neural Network Architectures for Arbitrary Data Dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19099
&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20351;&#20854;&#33021;&#22788;&#29702;&#20219;&#24847;&#36755;&#20837;&#25968;&#25454;&#32500;&#24230;&#65292;&#24182;&#21516;&#26102;&#20248;&#21270;&#20102;&#36741;&#21161;&#37327;&#23376;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Quantum Convolutional Neural Networks (QCNNs)&#20195;&#34920;&#20102;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20026;&#37327;&#23376;&#21644;&#32463;&#20856;&#25968;&#25454;&#20998;&#26512;&#24320;&#36767;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#23558;QCNNs&#24212;&#29992;&#20110;&#32463;&#20856;&#25968;&#25454;&#26102;&#20250;&#20986;&#29616;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#36755;&#20837;&#25968;&#25454;&#32500;&#24230;&#30340;QCNN&#26550;&#26500;&#65292;&#21516;&#26102;&#20248;&#21270;&#20102;&#35832;&#22914;&#36741;&#21161;&#37327;&#23376;&#36164;&#28304;&#20998;&#37197;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19099v1 Announce Type: cross  Abstract: Quantum convolutional neural networks (QCNNs) represent a promising approach in quantum machine learning, paving new directions for both quantum and classical data analysis. This approach is particularly attractive due to the absence of the barren plateau problem, a fundamental challenge in training quantum neural networks (QNNs), and its feasibility. However, a limitation arises when applying QCNNs to classical data. The network architecture is most natural when the number of input qubits is a power of two, as this number is reduced by a factor of two in each pooling layer. The number of input qubits determines the dimensions (i.e. the number of features) of the input data that can be processed, restricting the applicability of QCNN algorithms to real-world data. To address this issue, we propose a QCNN architecture capable of handling arbitrary input data dimensions while optimizing the allocation of quantum resources such as ancilla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#26368;&#23567;&#21270;&#21508;&#33258;&#30340;&#21155;&#21183;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#20581;&#24247;&#20135;&#19994;&#20013;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.19083</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#25913;&#36827;&#30284;&#30151;&#25104;&#20687;&#35786;&#26029;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Cancer Imaging Diagnosis with Bayesian Networks and Deep Learning: A Bayesian Deep Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#26368;&#23567;&#21270;&#21508;&#33258;&#30340;&#21155;&#21183;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#20581;&#24247;&#20135;&#19994;&#20013;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#21487;&#20197;&#21019;&#24314;&#35768;&#22810;&#31934;&#30830;&#30340;&#27169;&#22411;&#26469;&#35757;&#32451;&#21644;&#39044;&#27979;&#32473;&#23450;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#32972;&#21518;&#30340;&#29702;&#35770;&#65292;&#26681;&#25454;&#27599;&#20010;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#26500;&#24314;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20248;&#21183;&#21516;&#26102;&#26368;&#23567;&#21270;&#21155;&#21183;&#12290;&#26368;&#32456;&#65292;&#23558;&#20998;&#26512;&#32467;&#26524;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20581;&#24247;&#20135;&#19994;&#20013;&#20998;&#31867;&#22270;&#20687;&#30340;&#24212;&#29992;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19083v1 Announce Type: cross  Abstract: With recent advancements in the development of artificial intelligence applications using theories and algorithms in machine learning, many accurate models can be created to train and predict on given datasets. With the realization of the importance of imaging interpretation in cancer diagnosis, this article aims to investigate the theory behind Deep Learning and Bayesian Network prediction models. Based on the advantages and drawbacks of each model, different approaches will be used to construct a Bayesian Deep Learning Model, combining the strengths while minimizing the weaknesses. Finally, the applications and accuracy of the resulting Bayesian Deep Learning approach in the health industry in classifying images will be analyzed.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;E-&#26816;&#39564;&#32479;&#35745;&#24341;&#20837;BB-predictor&#65292;&#22686;&#24378;&#31526;&#21512;&#24615;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19082</link><description>&lt;p&gt;
&#21033;&#29992;E-&#26816;&#39564;&#32479;&#35745;&#22686;&#24378;&#31526;&#21512;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Conformal Prediction Using E-Test Statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19082
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;E-&#26816;&#39564;&#32479;&#35745;&#24341;&#20837;BB-predictor&#65292;&#22686;&#24378;&#31526;&#21512;&#24615;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#20316;&#20026;&#19968;&#31181;&#31283;&#20581;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#25152;&#20570;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#28857;&#39044;&#27979;&#22120;&#19981;&#21516;&#65292;CP&#22522;&#20110;&#25968;&#25454;&#21487;&#20132;&#25442;&#24615;&#30340;&#20551;&#35774;&#29983;&#25104;&#32479;&#35745;&#19978;&#26377;&#25928;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#20063;&#31216;&#20026;&#39044;&#27979;&#21306;&#38388;&#12290;&#36890;&#24120;&#65292;&#26500;&#24314;&#31526;&#21512;&#24615;&#39044;&#27979;&#20381;&#36182;&#20110;p&#20540;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#36208;&#19978;&#20102;&#21478;&#19968;&#26465;&#36335;&#24452;&#65292;&#21033;&#29992;E-&#26816;&#39564;&#32479;&#35745;&#30340;&#21147;&#37327;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#19979;&#30028;&#39044;&#27979;&#22120;&#65288;BB-predictor&#65289;&#26469;&#22686;&#24378;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#25928;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19082v1 Announce Type: cross  Abstract: Conformal Prediction (CP) serves as a robust framework that quantifies uncertainty in predictions made by Machine Learning (ML) models. Unlike traditional point predictors, CP generates statistically valid prediction regions, also known as prediction intervals, based on the assumption of data exchangeability. Typically, the construction of conformal predictions hinges on p-values. This paper, however, ventures down an alternative path, harnessing the power of e-test statistics to augment the efficacy of conformal predictions by introducing a BB-predictor (bounded from the below predictor).
&lt;/p&gt;</description></item><item><title>TinyML&#26159;&#19968;&#31181;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#21040;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24494;&#25511;&#21046;&#22120;&#20013;&#23454;&#29616;&#26080;&#22788;&#19981;&#22312;&#26234;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#38656;&#35201;&#20849;&#21516;&#35774;&#35745;&#31639;&#27861;&#21644;&#31995;&#32479;&#22534;&#26632;&#20197;&#20811;&#26381;&#30828;&#20214;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.19076</link><description>&lt;p&gt;
&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#65306;&#36827;&#23637;&#19982;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Tiny Machine Learning: Progress and Futures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19076
&lt;/p&gt;
&lt;p&gt;
TinyML&#26159;&#19968;&#31181;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#21040;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24494;&#25511;&#21046;&#22120;&#20013;&#23454;&#29616;&#26080;&#22788;&#19981;&#22312;&#26234;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#38656;&#35201;&#20849;&#21516;&#35774;&#35745;&#31639;&#27861;&#21644;&#31995;&#32479;&#22534;&#26632;&#20197;&#20811;&#26381;&#30828;&#20214;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tiny Machine Learning&#65288;TinyML&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#26032;&#39046;&#22495;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#21040;&#25968;&#21313;&#20159;&#20010;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24494;&#25511;&#21046;&#22120;&#65288;MCUs&#65289;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#33539;&#22260;&#65292;&#23454;&#29616;&#20102;&#26080;&#22788;&#19981;&#22312;&#30340;&#26234;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30828;&#20214;&#38480;&#21046;&#65292;TinyML&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#26377;&#38480;&#30340;&#20869;&#23384;&#36164;&#28304;&#20351;&#24471;&#38590;&#20197;&#23481;&#32435;&#20026;&#20113;&#21644;&#31227;&#21160;&#24179;&#21488;&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#23545;&#20110;&#35064;&#26426;&#35774;&#22791;&#65292;&#32534;&#35793;&#22120;&#21644;&#25512;&#26029;&#24341;&#25806;&#25903;&#25345;&#20063;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#20849;&#21516;&#35774;&#35745;&#31639;&#27861;&#21644;&#31995;&#32479;&#22534;&#26632;&#20197;&#23454;&#29616;TinyML&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19076v1 Announce Type: cross  Abstract: Tiny Machine Learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of AI applications and enable ubiquitous intelligence. However, TinyML is challenging due to hardware constraints: the tiny memory resource makes it difficult to hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. Therefore, we need to co-design the algorithm and system stack to enable TinyML. In this review, we will first discuss the definition, challenges, and applications of TinyML. We then survey the recent progress in TinyML and deep learning on MCUs. Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design. We will further extend the solution from inference to training and in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#20013;&#24515;&#30340;&#24314;&#31569;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;&#65292;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.19060</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#26045;&#24037;&#26426;&#22120;&#20154;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;
&lt;/p&gt;
&lt;p&gt;
Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#20013;&#24515;&#30340;&#24314;&#31569;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;&#65292;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#24314;&#31569;&#34892;&#19994;&#20013;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#20154;&#38598;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#21270;&#29305;&#23450;&#20219;&#21153;&#65292;&#36890;&#24120;&#24573;&#30053;&#20102;&#24314;&#31569;&#24037;&#20316;&#27969;&#31243;&#20013;&#20154;&#31867;&#22240;&#32032;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#21270;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#20026;&#26412;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#8220;&#24037;&#20316;&#20276;&#20387;&#28459;&#28216;&#22120;&#8221;&#65292;&#26088;&#22312;&#21327;&#21161;&#24314;&#31569;&#24037;&#20154;&#23436;&#25104;&#20854;&#29616;&#26377;&#23454;&#36341;&#65292;&#26088;&#22312;&#22686;&#24378;&#23433;&#20840;&#24615;&#21644;&#24037;&#20316;&#27969;&#31243;&#30340;&#27969;&#30021;&#24615;&#65292;&#21516;&#26102;&#23562;&#37325;&#24314;&#31569;&#21171;&#21160;&#30340;&#25216;&#26415;&#24615;&#36136;&#12290;&#25105;&#20204;&#23545;&#22312;&#26408;&#24037;&#27169;&#26495;&#24037;&#31243;&#20013;&#37096;&#32626;&#26426;&#22120;&#20154;&#31995;&#32479;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#21407;&#22411;&#65292;&#36890;&#36807;&#29615;&#22659;&#30456;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39537;&#21160;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#26426;&#21160;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#30340;&#24037;&#20154;-&#26426;&#22120;&#20154;&#21327;&#20316;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#65292;&#20513;&#23548;&#21327;&#20316;&#27169;&#22411;&#65292;&#20854;&#20013;&#33258;&#36866;&#24212;&#26426;&#22120;&#20154;&#25903;&#25345;&#32780;&#19981;&#26159;&#21462;&#20195;&#20154;&#31867;&#65292;&#24378;&#35843;&#20102;&#20132;&#20114;&#24335;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19060v1 Announce Type: cross  Abstract: In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a ``work companion rover" designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor's skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#31958;&#23615;&#30149;&#24739;&#32773;&#20877;&#20837;&#38498;&#26041;&#38754;&#23454;&#29616;&#20102;&#20844;&#24179;&#21644;&#20934;&#30830;&#65292;&#20854;&#20013;GBM&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#36328;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#19979;&#23454;&#29616;&#20102;&#24179;&#34913;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19057</link><description>&lt;p&gt;
&#22312;&#21355;&#29983;&#20445;&#20581;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#23545;&#31958;&#23615;&#30149;&#24739;&#32773;&#20877;&#20837;&#38498;&#30340;&#39044;&#27979;&#20013;&#30340;&#19981;&#24179;&#31561;
&lt;/p&gt;
&lt;p&gt;
Equity in Healthcare: Analyzing Disparities in Machine Learning Predictions of Diabetic Patient Readmissions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19057
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#31958;&#23615;&#30149;&#24739;&#32773;&#20877;&#20837;&#38498;&#26041;&#38754;&#23454;&#29616;&#20102;&#20844;&#24179;&#21644;&#20934;&#30830;&#65292;&#20854;&#20013;GBM&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#36328;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#19979;&#23454;&#29616;&#20102;&#24179;&#34913;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#22914;&#20309;&#33021;&#22815;&#20844;&#24179;&#12289;&#20934;&#30830;&#22320;&#39044;&#27979;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#65292;&#36328;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#23398;&#22240;&#32032;&#65288;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#31181;&#26063;&#65289;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102; Deep Learning&#12289;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12289;&#26799;&#24230;&#19978;&#21319;&#26426;&#65288;GBM&#65289;&#21644;&#26420;&#32032;&#36125;&#21494;&#26031;&#31561;&#27169;&#22411;&#12290;GBM&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377; 84.3% &#30340; F1 &#20998;&#25968;&#21644; 82.2% &#30340;&#20934;&#30830;&#24230;&#65292;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#36328;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#23398;&#22240;&#32032;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;&#23545;&#25152;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#20844;&#24179;&#24615;&#20998;&#26512;&#12290;GBM&#26368;&#23567;&#21270;&#20102;&#39044;&#27979;&#20013;&#30340;&#24046;&#24322;&#65292;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#20043;&#38388;&#33719;&#24471;&#20102;&#24179;&#34913;&#30340;&#32467;&#26524;&#12290;&#23545;&#20110;&#20004;&#24615;&#26469;&#35828;&#65292;&#23427;&#26174;&#31034;&#20986;&#36739;&#20302;&#30340;&#20551;&#38452;&#24615;&#29575;&#65288;FDR&#65289;&#65288;6-7%&#65289;&#21644;&#20551;&#38451;&#24615;&#29575;&#65288;FPR&#65289;&#65288;5%&#65289;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#31181;&#26063;&#32676;&#20307;&#65292;&#22914;&#38750;&#35028;&#32654;&#22269;&#20154;&#65288;8%&#65289;&#21644;&#20122;&#35028;&#65288;7%&#65289;&#65292;FDR&#20445;&#25345;&#36739;&#20302;&#27700;&#24179;&#12290;&#31867;&#20284;&#22320;&#65292;&#23545;&#20110;&#24180;&#40836;&#32676;&#20307;&#65288;40 &#23681;&#20197;&#19979;&#21644;40 &#23681;&#20197;&#19978;&#24739;&#32773;&#65289;&#65292;FPR&#22312; 4% &#30340;&#27700;&#24179;&#19978;&#20445;&#25345;&#19968;&#33268;&#65292;&#34920;&#26126;&#20854;&#31934;&#30830;&#24615;&#21644;&#20943;&#23569;&#20559;&#35265;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19057v1 Announce Type: new  Abstract: This study investigates how machine learning (ML) models can predict hospital readmissions for diabetic patients fairly and accurately across different demographics (age, gender, race). We compared models like Deep Learning, Generalized Linear Models, Gradient Boosting Machines (GBM), and Naive Bayes. GBM stood out with an F1-score of 84.3% and accuracy of 82.2%, accurately predicting readmissions across demographics. A fairness analysis was conducted across all the models. GBM minimized disparities in predictions, achieving balanced results across genders and races. It showed low False Discovery Rates (FDR) (6-7%) and False Positive Rates (FPR) (5%) for both genders. Additionally, FDRs remained low for racial groups, such as African Americans (8%) and Asians (7%). Similarly, FPRs were consistent across age groups (4%) for both patients under 40 and those above 40, indicating its precision and ability to reduce bias. These findings empha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26816;&#27979;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#29983;&#25104;&#24615;&#27169;&#20223;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#25439;&#22833;&#30340;&#26816;&#27979;&#38408;&#20540;&#65292;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#21644;&#25552;&#21319;&#20854;&#27861;&#24459;&#21512;&#35268;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19050</link><description>&lt;p&gt;
&#36890;&#36807;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#26816;&#27979;&#29983;&#25104;&#24615;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Detecting Generative Parroting through Overfitting Masked Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26816;&#27979;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#29983;&#25104;&#24615;&#27169;&#20223;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#25439;&#22833;&#30340;&#26816;&#27979;&#38408;&#20540;&#65292;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#21644;&#25552;&#21319;&#20854;&#27861;&#24459;&#21512;&#35268;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#25968;&#23383;&#20869;&#23481;&#21019;&#24314;&#30340;&#26041;&#24335;&#65292;&#28982;&#32780;&#30001;&#20110;&#29983;&#25104;&#24615;&#27169;&#20223;&#38382;&#39064;&#65292;&#27169;&#22411;&#36807;&#20110;&#27169;&#20223;&#20854;&#35757;&#32451;&#25968;&#25454;&#32780;&#32473;&#29256;&#26435;&#23436;&#25972;&#24615;&#24102;&#26469;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#21033;&#29992;&#19968;&#20010;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26377;&#25928;&#22320;&#26816;&#27979;&#36825;&#31181;&#27169;&#20223;&#26679;&#26412;&#12290;&#25105;&#20204;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#25439;&#22833;&#24314;&#31435;&#19968;&#20010;&#26816;&#27979;&#38408;&#20540;&#65292;&#20174;&#32780;&#31934;&#30830;&#23450;&#20301;&#20462;&#25913;&#21518;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#20223;&#20869;&#23481;&#12290;&#21021;&#27493;&#35780;&#20272;&#34920;&#26126;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#24182;&#21152;&#24378;&#27861;&#24459;&#21512;&#35268;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19050v1 Announce Type: cross  Abstract: The advent of generative AI models has revolutionized digital content creation, yet it introduces challenges in maintaining copyright integrity due to generative parroting, where models mimic their training data too closely. Our research presents a novel approach to tackle this issue by employing an overfitted Masked Autoencoder (MAE) to detect such parroted samples effectively. We establish a detection threshold based on the mean loss across the training dataset, allowing for the precise identification of parroted content in modified datasets. Preliminary evaluations demonstrate promising results, suggesting our method's potential to ensure ethical use and enhance the legal compliance of generative models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#20010;&#20114;&#34917;&#30340;&#26041;&#21521;&#24863;&#30693;&#25439;&#22833;&#39033;&#65292;&#24378;&#35843;&#25968;&#25454;&#30340;&#26102;&#38388;&#26041;&#38754;&#65292;&#24341;&#23548;&#20248;&#21270;&#21644;&#32467;&#26524;&#23884;&#20837;&#20197;&#23637;&#31034;&#21487;&#33021;&#34987;&#24573;&#30053;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.19040</link><description>&lt;p&gt;
&#20351;&#29992;&#26041;&#21521;&#24863;&#30693;t-SNE&#21487;&#35270;&#21270;&#39640;&#32500;&#26102;&#38388;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Visualizing High-Dimensional Temporal Data Using Direction-Aware t-SNE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19040
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#20010;&#20114;&#34917;&#30340;&#26041;&#21521;&#24863;&#30693;&#25439;&#22833;&#39033;&#65292;&#24378;&#35843;&#25968;&#25454;&#30340;&#26102;&#38388;&#26041;&#38754;&#65292;&#24341;&#23548;&#20248;&#21270;&#21644;&#32467;&#26524;&#23884;&#20837;&#20197;&#23637;&#31034;&#21487;&#33021;&#34987;&#24573;&#30053;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21253;&#21547;&#26102;&#38388;&#32452;&#20214;&#25110;&#28041;&#21450;&#20174;&#19968;&#20010;&#29366;&#24577;&#21040;&#21478;&#19968;&#20010;&#29366;&#24577;&#30340;&#36716;&#21464;&#12290;&#20026;&#20102;&#36827;&#34892;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#36825;&#20123;&#39640;&#32500;&#25968;&#25454;&#38598;&#34920;&#31034;&#20026;&#20108;&#32500;&#22320;&#22270;&#65292;&#20351;&#29992;&#25968;&#25454;&#23545;&#35937;&#30340;&#23884;&#20837;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#29992;&#26377;&#21521;&#36793;&#34920;&#31034;&#23427;&#20204;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38477;&#32500;&#25216;&#26415;&#65292;&#22914;t-SNE&#21644;UMAP&#65292;&#22312;&#26500;&#24314;&#23884;&#20837;&#26102;&#26410;&#32771;&#34385;&#25968;&#25454;&#30340;&#26102;&#38388;&#24615;&#25110;&#20851;&#31995;&#24615;&#65292;&#23548;&#33268;&#26102;&#38388;&#19978;&#26434;&#20081;&#30340;&#21487;&#35270;&#21270;&#65292;&#20351;&#28508;&#22312;&#26377;&#36259;&#30340;&#27169;&#24335;&#21464;&#24471;&#27169;&#31946;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;t-SNE&#30340;&#20248;&#21270;&#20989;&#25968;&#20013;&#25552;&#20986;&#20102;&#20004;&#20010;&#20114;&#34917;&#30340;&#26041;&#21521;&#24863;&#30693;&#25439;&#22833;&#39033;&#65292;&#24378;&#35843;&#25968;&#25454;&#30340;&#26102;&#38388;&#26041;&#38754;&#65292;&#24341;&#23548;&#20248;&#21270;&#21644;&#32467;&#26524;&#23884;&#20837;&#20197;&#23637;&#31034;&#21487;&#33021;&#34987;&#24573;&#30053;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#23450;&#21521;&#19968;&#33268;&#25439;&#22833;&#65288;DCL&#65289;&#40723;&#21169;&#38468;&#36817;&#30340;&#31661;&#22836;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19040v1 Announce Type: new  Abstract: Many real-world data sets contain a temporal component or involve transitions from state to state. For exploratory data analysis, we can represent these high-dimensional data sets in two-dimensional maps, using embeddings of the data objects under exploration and representing their temporal relationships with directed edges. Most existing dimensionality reduction techniques, such as t-SNE and UMAP, do not take into account the temporal or relational nature of the data when constructing the embeddings, resulting in temporally cluttered visualizations that obscure potentially interesting patterns. To address this problem, we propose two complementary, direction-aware loss terms in the optimization function of t-SNE that emphasize the temporal aspects of the data, guiding the optimization and the resulting embedding to reveal temporal patterns that might otherwise go unnoticed. The Directional Coherence Loss (DCL) encourages nearby arrows c
&lt;/p&gt;</description></item><item><title>&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;...</title><link>https://arxiv.org/abs/2403.19031</link><description>&lt;p&gt;
&#20351;&#29992;&#20844;&#20849;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#30456;&#20851;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19031
&lt;/p&gt;
&lt;p&gt;
&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#35797;&#22270;&#35780;&#20272;&#23427;&#20204;&#22312;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340;&#20581;&#24247;&#30456;&#20851;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#20219;&#21153;&#20256;&#32479;&#19978;&#24456;&#38590;&#33719;&#24471;&#39640;&#20998;&#12290;&#25105;&#20204;&#22312;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#23545;&#19968;&#20010;&#22522;&#20110;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVMs&#65289;&#30340;&#30417;&#30563;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#19977;&#20010;&#22522;&#20110;RoBERTa&#12289;BERTweet&#21644;SocBERT&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#20197;&#21450;&#20004;&#20010;&#22522;&#20110;GPT3.5&#21644;GPT4&#30340;LLM&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#21033;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65306;&#23558;LLMs&#29992;&#20316;&#38646;&#27425;&#20998;&#31867;&#22120;&#65292;&#23558;LLMs&#29992;&#20316;&#27880;&#37322;&#22120;&#20026;&#30417;&#30563;&#20998;&#31867;&#22120;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#21033;&#29992;LLMs&#36827;&#34892;&#23569;&#37327;&#31034;&#20363;&#26469;&#22686;&#21152;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19031v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable success in NLP tasks. However, there is a paucity of studies that attempt to evaluate their performances on social media-based health-related natural language processing tasks, which have traditionally been difficult to achieve high scores in. We benchmarked one supervised classic machine learning model based on Support Vector Machines (SVMs), three supervised pretrained language models (PLMs) based on RoBERTa, BERTweet, and SocBERT, and two LLM based classifiers (GPT3.5 and GPT4), across 6 text classification tasks. We developed three approaches for leveraging LLMs for text classification: employing LLMs as zero-shot classifiers, us-ing LLMs as annotators to annotate training data for supervised classifiers, and utilizing LLMs with few-shot examples for augmentation of manually annotated data. Our comprehensive experiments demonstrate that employ-ing data augmentation using LLM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#23545;&#31216;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#32780;&#19981;&#35201;&#27714;&#22870;&#21169;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19024</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#36827;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#38750;&#23545;&#31216;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#23545;&#31216;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#32780;&#19981;&#35201;&#27714;&#22870;&#21169;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#27169;&#22411;&#20013;&#30340;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#31574;&#30053;&#35757;&#32451;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#19968;&#20010;&#24120;&#29992;&#30340;&#31616;&#21270;&#20551;&#35774;&#26159;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#37117;&#34920;&#29616;&#20986;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#21160;&#21147;&#23398;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#22870;&#21169;&#27169;&#22411;&#29420;&#31435;&#30340;&#23545;&#31216;&#24615;&#65306;&#22870;&#21169;&#21487;&#33021;&#19981;&#28385;&#36275;&#19982;&#21160;&#21147;&#23398;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21482;&#20551;&#23450;&#21160;&#21147;&#23398;&#34920;&#29616;&#20986;&#23545;&#31216;&#24615;&#30340;&#24773;&#20917;&#65292;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#23398;&#20064;&#20013;&#21487;&#24212;&#29992;&#23545;&#31216;&#25216;&#26415;&#30340;&#38382;&#39064;&#33539;&#22260;&#12290;&#25105;&#20204;&#21033;&#29992;&#21345;&#22612;&#24681;&#31227;&#21160;&#26694;&#26550;&#26041;&#27861;&#24341;&#20837;&#19968;&#31181;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26500;&#36896;&#65292;&#36825;&#31181;&#21160;&#21147;&#23398;&#34920;&#29616;&#20986;&#25351;&#23450;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#21040;&#20102;&#26356;&#20934;&#30830;&#30340;&#21160;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19024v1 Announce Type: cross  Abstract: Recent work in reinforcement learning has leveraged symmetries in the model to improve sample efficiency in training a policy. A commonly used simplifying assumption is that the dynamics and reward both exhibit the same symmetry. However, in many real-world environments, the dynamical model exhibits symmetry independent of the reward model: the reward may not satisfy the same symmetries as the dynamics. In this paper, we investigate scenarios where only the dynamics are assumed to exhibit symmetry, extending the scope of problems in reinforcement learning and learning in control theory where symmetry techniques can be applied. We use Cartan's moving frame method to introduce a technique for learning dynamics which, by construction, exhibit specified symmetries. We demonstrate through numerical experiments that the proposed method learns a more accurate dynamical model.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;IDGen&#65292;&#23558;&#27599;&#20010;&#25512;&#33616;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#25991;&#26412;ID&#65292;&#20174;&#32780;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26356;&#22909;&#22320;&#19982;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.19021</link><description>&lt;p&gt;
&#26397;&#21521;LLM-RecSys&#23545;&#40784;&#19982;&#25991;&#26412;ID&#23398;&#20064;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Towards LLM-RecSys Alignment with Textual ID Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19021
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;IDGen&#65292;&#23558;&#27599;&#20010;&#25512;&#33616;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#25991;&#26412;ID&#65292;&#20174;&#32780;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26356;&#22909;&#22320;&#19982;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#24050;&#32463;&#23558;&#20256;&#32479;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#25512;&#33616;&#26041;&#24335;&#36716;&#21464;&#20026;&#25991;&#26412;&#29983;&#25104;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#19982;&#22266;&#26377;&#25805;&#20316;&#20154;&#31867;&#35789;&#27719;&#30340;&#26631;&#20934;NLP&#20219;&#21153;&#30456;&#21453;&#65292;&#30446;&#21069;&#29983;&#25104;&#24335;&#25512;&#33616;&#39046;&#22495;&#30340;&#30740;&#31350;&#22312;&#22914;&#20309;&#22312;&#25991;&#26412;&#29983;&#25104;&#33539;&#24335;&#20013;&#20197;&#31616;&#27905;&#32780;&#26377;&#24847;&#20041;&#30340;ID&#34920;&#31034;&#26377;&#25928;&#32534;&#30721;&#25512;&#33616;&#39033;&#30446;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23545;&#40784;LLMs&#19982;&#25512;&#33616;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IDGen&#65292;&#20351;&#29992;&#20154;&#31867;&#35821;&#35328;&#26631;&#35760;&#23558;&#27599;&#20010;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#12289;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#25991;&#26412;ID&#12290;&#36825;&#36890;&#36807;&#22312;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#26049;&#35757;&#32451;&#25991;&#26412;ID&#29983;&#25104;&#22120;&#26469;&#23454;&#29616;&#65292;&#20351;&#20010;&#24615;&#21270;&#25512;&#33616;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21040;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#24182;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#35299;&#32806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19021v1 Announce Type: cross  Abstract: Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potenti
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30643;&#23380;&#27979;&#37327;&#26469;&#35782;&#21035; VR &#20013;&#24773;&#32490;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#24037;&#31243;&#21644;&#26799;&#24230;&#25552;&#21319;&#27169;&#22411;&#65292;&#22312; Thelxino\"e &#26694;&#26550;&#20013;&#21462;&#24471;&#20102;98.8%&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#20026;&#21457;&#23637;&#26356;&#20855;&#27785;&#28024;&#24863;&#21644;&#20114;&#21160;&#24615;&#30340; VR &#29615;&#22659;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.19014</link><description>&lt;p&gt;
Thelxino\"e:&#20351;&#29992;&#30643;&#23380;&#27979;&#37327;&#21644;&#26426;&#22120;&#23398;&#20064;&#35782;&#21035;&#20154;&#31867;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
Thelxino\"e: Recognizing Human Emotions Using Pupillometry and Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19014
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30643;&#23380;&#27979;&#37327;&#26469;&#35782;&#21035; VR &#20013;&#24773;&#32490;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#24037;&#31243;&#21644;&#26799;&#24230;&#25552;&#21319;&#27169;&#22411;&#65292;&#22312; Thelxino\"e &#26694;&#26550;&#20013;&#21462;&#24471;&#20102;98.8%&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#20026;&#21457;&#23637;&#26356;&#20855;&#27785;&#28024;&#24863;&#21644;&#20114;&#21160;&#24615;&#30340; VR &#29615;&#22659;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30643;&#23380;&#27979;&#37327;&#26469;&#35782;&#21035;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#20013;&#24773;&#32490;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;VR&#22836;&#26174;&#20998;&#26512;&#23545;&#35270;&#35273;&#21644;&#21548;&#35273;&#21050;&#28608;&#30340;&#30643;&#23380;&#30452;&#24452;&#21709;&#24212;&#65292;&#24182;&#19987;&#27880;&#20110;&#20174;VR&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26102;&#22495;&#12289;&#39057;&#22495;&#21644;&#26102;&#39057;&#22495;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29305;&#24449;&#36873;&#25321;&#65292;&#36890;&#36807;&#26368;&#22823;&#30456;&#20851;&#24615;&#26368;&#23567;&#20887;&#20313;&#24615;&#65288;mRMR&#65289;&#35782;&#21035;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#24212;&#29992;&#26799;&#24230;&#25552;&#21319;&#27169;&#22411;&#65292;&#19968;&#31181;&#20351;&#29992;&#21472;&#21152;&#20915;&#31574;&#26641;&#30340;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#22312;&#36827;&#34892;&#29305;&#24449;&#24037;&#31243;&#26102;&#23454;&#29616;&#20102;98.8%&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#26410;&#36827;&#34892;&#29305;&#24449;&#24037;&#31243;&#26102;&#20026;84.9%&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;Thelxino\"e&#26694;&#26550;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#23454;&#29616;&#26356;&#21152;&#36924;&#30495;&#21644;&#24773;&#24863;&#20849;&#40483;&#30340;&#35302;&#35273;&#20132;&#20114;&#65292;&#20026;&#24320;&#21457;&#26356;&#20855;&#27785;&#28024;&#24863;&#21644;&#20114;&#21160;&#24615;&#30340;VR&#29615;&#22659;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#20026;&#26410;&#26469;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19014v1 Announce Type: new  Abstract: In this study, we present a method for emotion recognition in Virtual Reality (VR) using pupillometry. We analyze pupil diameter responses to both visual and auditory stimuli via a VR headset and focus on extracting key features in the time-domain, frequency-domain, and time-frequency domain from VR generated data. Our approach utilizes feature selection to identify the most impactful features using Maximum Relevance Minimum Redundancy (mRMR). By applying a Gradient Boosting model, an ensemble learning technique using stacked decision trees, we achieve an accuracy of 98.8% with feature engineering, compared to 84.9% without it. This research contributes significantly to the Thelxino\"e framework, aiming to enhance VR experiences by integrating multiple sensor data for realistic and emotionally resonant touch interactions. Our findings open new avenues for developing more immersive and interactive VR environments, paving the way for futur
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35774;&#35745;&#20102;&#19968;&#20010;&#27010;&#29575;&#26080;&#30417;&#30563;&#27169;&#22411;&#65292;&#29992;&#20110;&#25512;&#26029;&#21307;&#38498;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#22810;&#20010;&#20219;&#24847;&#38271;&#24230;&#24207;&#21015;&#65292;&#21487;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21407;&#22987;&#25968;&#25454;&#19978;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.19011</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#29575;&#27169;&#22411;&#39034;&#24207;&#25512;&#26029;&#21307;&#38498;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Sequential Inference of Hospitalization ElectronicHealth Records Using Probabilistic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19011
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35774;&#35745;&#20102;&#19968;&#20010;&#27010;&#29575;&#26080;&#30417;&#30563;&#27169;&#22411;&#65292;&#29992;&#20110;&#25512;&#26029;&#21307;&#38498;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#22810;&#20010;&#20219;&#24847;&#38271;&#24230;&#24207;&#21015;&#65292;&#21487;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21407;&#22987;&#25968;&#25454;&#19978;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#30340;&#21307;&#38498;&#29615;&#22659;&#20013;&#65292;&#20915;&#31574;&#25903;&#25345;&#21487;&#20197;&#25104;&#20026;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#22312;&#36825;&#31181;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#22522;&#20110;&#25968;&#25454;&#30340;&#25512;&#26029;&#26410;&#26469;&#32467;&#26524;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38271;&#24207;&#21015;&#65288;&#22914;&#23454;&#39564;&#23460;&#26816;&#27979;&#21644;&#33647;&#29289;&#65289;&#32463;&#24120;&#26356;&#26032;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#21307;&#38498;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#20013;&#30340;&#22810;&#20010;&#20219;&#24847;&#38271;&#24230;&#24207;&#21015;&#30340;&#27010;&#29575;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#28508;&#22312;&#21464;&#37327;&#32467;&#26500;&#65292;&#25429;&#25417;&#20102;&#33647;&#29289;&#12289;&#35786;&#26029;&#12289;&#23454;&#39564;&#23460;&#26816;&#27979;&#12289;&#31070;&#32463;&#35780;&#20272;&#21644;&#33647;&#29289;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#23427;&#21487;&#20197;&#22312;&#21407;&#22987;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#25439;&#22833;&#36716;&#25442;&#25110;&#26102;&#38388;&#20998;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19011v1 Announce Type: cross  Abstract: In the dynamic hospital setting, decision support can be a valuable tool for improving patient outcomes. Data-driven inference of future outcomes is challenging in this dynamic setting, where long sequences such as laboratory tests and medications are updated frequently. This is due in part to heterogeneity of data types and mixed-sequence types contained in variable length sequences. In this work we design a probabilistic unsupervised model for multiple arbitrary-length sequences contained in hospitalization Electronic Health Record (EHR) data. The model uses a latent variable structure and captures complex relationships between medications, diagnoses, laboratory tests, neurological assessments, and medications. It can be trained on original data, without requiring any lossy transformations or time binning. Inference algorithms are derived that use partial data to infer properties of the complete sequences, including their length and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#31350;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#30899;&#36275;&#36857;&#65292;&#24182;&#24341;&#20837;&#20102;Robustness Carbon Trade-off Index&#65288;RCTI&#65289;&#65292;&#35813;&#25351;&#26631;&#25429;&#25417;&#20102;&#30899;&#25490;&#25918;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19009</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#25345;&#32493;&#30340;SecureML: &#37327;&#21270;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#30899;&#36275;&#36857;
&lt;/p&gt;
&lt;p&gt;
Towards Sustainable SecureML: Quantifying Carbon Footprint of Adversarial Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#31350;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#30899;&#36275;&#36857;&#65292;&#24182;&#24341;&#20837;&#20102;Robustness Carbon Trade-off Index&#65288;RCTI&#65289;&#65292;&#35813;&#25351;&#26631;&#25429;&#25417;&#20102;&#30899;&#25490;&#25918;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#22312;&#21508;&#34892;&#21508;&#19994;&#30340;&#24191;&#27867;&#24212;&#29992;&#24341;&#36215;&#20102;&#21487;&#25345;&#32493;&#24615;&#25285;&#24551;&#65292;&#22240;&#20026;&#20854;&#24040;&#22823;&#30340;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#25490;&#25918;&#12290;&#22312;&#23545;&#25239;&#24615;ML&#20013;&#65292;&#36825;&#19968;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#32039;&#36843;&#65292;&#22240;&#20026;&#23427;&#30528;&#37325;&#20110;&#22686;&#24378;&#27169;&#22411;&#23545;&#25239;&#19981;&#21516;&#22522;&#20110;&#32593;&#32476;&#30340;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#12290;&#22312;ML&#31995;&#32479;&#20013;&#23454;&#26045;&#38450;&#24481;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#65292;&#21152;&#21095;&#20102;&#23427;&#20204;&#30340;&#29615;&#22659;&#24433;&#21709;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23545;&#25239;&#24615;ML&#30340;&#30899;&#36275;&#36857;&#65292;&#25552;&#20379;&#23454;&#35777;&#35777;&#25454;&#23558;&#26356;&#22823;&#30340;&#27169;&#22411;&#31283;&#20581;&#24615;&#19982;&#26356;&#39640;&#30340;&#25490;&#25918;&#32852;&#31995;&#36215;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#37327;&#21270;&#36825;&#31181;&#26435;&#34913;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Robustness Carbon Trade-off Index (RCTI)&#12290;&#36825;&#19968;&#26032;&#39062;&#30340;&#24230;&#37327;&#25351;&#26631;&#21463;&#32463;&#27982;&#24377;&#24615;&#21407;&#29702;&#21551;&#21457;&#65292;&#25429;&#25417;&#20102;&#30899;&#25490;&#25918;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19009v1 Announce Type: new  Abstract: The widespread adoption of machine learning (ML) across various industries has raised sustainability concerns due to its substantial energy usage and carbon emissions. This issue becomes more pressing in adversarial ML, which focuses on enhancing model security against different network-based attacks. Implementing defenses in ML systems often necessitates additional computational resources and network security measures, exacerbating their environmental impacts. In this paper, we pioneer the first investigation into adversarial ML's carbon footprint, providing empirical evidence connecting greater model robustness to higher emissions. Addressing the critical need to quantify this trade-off, we introduce the Robustness Carbon Trade-off Index (RCTI). This novel metric, inspired by economic elasticity principles, captures the sensitivity of carbon emissions to changes in adversarial robustness. We demonstrate the RCTI through an experiment i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.18998</link><description>&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#36328;&#31995;&#32479;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18998
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#65288;MSS&#65289;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#21160;&#24577;&#30340;&#29305;&#24615;&#21487;&#33021;&#22312;&#21508;&#31181;&#25925;&#38556;&#31867;&#21035;&#20013;&#20986;&#29616;&#25925;&#38556;&#12290;&#20026;&#20102;&#26377;&#25928;&#22788;&#29702;&#25925;&#38556;&#65292;AIOps&#24037;&#20855;&#21033;&#29992;&#22522;&#20110;&#36319;&#36394;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#65288;2&#65289;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;MSS&#65292;Trainticket&#21644;OnlineBoutique&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20351;&#29992;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35843;&#25972;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20197;&#23545;&#26032;&#30340;&#12289;&#26410;&#35265;&#30340;&#26032;&#39062;&#25925;&#38556;&#31867;&#21035;&#30340;&#24322;&#24120;&#36319;&#36394;&#36827;&#34892;&#20998;&#31867;&#65292;&#26080;&#35770;&#26159;&#22312;&#26368;&#21021;&#35757;&#32451;&#30340;&#21516;&#19968;&#31995;&#32479;&#20869;&#65292;&#36824;&#26159;&#22312;&#20854;&#20182;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18998v1 Announce Type: cross  Abstract: Microservice-based systems (MSS) may experience failures in various fault categories due to their complex and dynamic nature. To effectively handle failures, AIOps tools utilize trace-based anomaly detection and root cause analysis. In this paper, we propose a novel framework for few-shot abnormal trace classification for MSS. Our framework comprises two main components: (1) Multi-Head Attention Autoencoder for constructing system-specific trace representations, which enables (2) Transformer Encoder-based Model-Agnostic Meta-Learning to perform effective and efficient few-shot learning for abnormal trace classification. The proposed framework is evaluated on two representative MSS, Trainticket and OnlineBoutique, with open datasets. The results show that our framework can adapt the learned knowledge to classify new, unseen abnormal traces of novel fault categories both within the same system it was initially trained on and even in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#39062;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39640;&#32500;&#22797;&#26434;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.18994</link><description>&lt;p&gt;
Causal-StoNet: &#39640;&#32500;&#22797;&#26434;&#25968;&#25454;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal-StoNet: Causal Inference for High-Dimensional Complex Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#39062;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39640;&#32500;&#22797;&#26434;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#31185;&#23398;&#30340;&#21457;&#23637;&#65292;&#25910;&#38598;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#24050;&#32463;&#21464;&#24471;&#21496;&#31354;&#35265;&#24815;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#65292;&#25968;&#25454;&#32500;&#24230;&#21487;&#33021;&#38750;&#24120;&#39640;&#65292;&#24182;&#19988;&#28508;&#22312;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21487;&#33021;&#26159;&#26410;&#30693;&#30340;&#65292;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#22914;&#21307;&#23398;&#12289;&#35745;&#37327;&#32463;&#27982;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#65292;&#23545;&#39640;&#32500;&#22797;&#26434;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#30340;&#20219;&#21153;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#36890;&#24120;&#26159;&#22312;&#20551;&#35774;&#25968;&#25454;&#32500;&#24230;&#36739;&#20302;&#25110;&#28508;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20026;&#32447;&#24615;&#25110;&#36817;&#20284;&#32447;&#24615;&#30340;&#24773;&#20917;&#19979;&#24320;&#21457;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39640;&#32500;&#22797;&#26434;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#26368;&#36817;&#21457;&#23637;&#30340;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#21644;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18994v1 Announce Type: cross  Abstract: With the advancement of data science, the collection of increasingly complex datasets has become commonplace. In such datasets, the data dimension can be extremely high, and the underlying data generation process can be unknown and highly nonlinear. As a result, the task of making causal inference with high-dimensional complex data has become a fundamental problem in many disciplines, such as medicine, econometrics, and social science. However, the existing methods for causal inference are frequently developed under the assumption that the data dimension is low or that the underlying data generation process is linear or approximately linear. To address these challenges, this paper proposes a novel causal inference approach for dealing with high-dimensional complex data. The proposed approach is based on deep learning techniques, including sparse deep learning theory and stochastic neural networks, that have been developed in recent lit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#26512;&#21040;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#30340;&#19981;&#21516;&#27169;&#22411;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#35782;&#21035;&#25935;&#24863;&#21306;&#22495;&#24182;&#22312;&#26368;&#23567;&#31243;&#24230;&#25197;&#26354;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#65292;&#29983;&#25104;&#20248;&#36234;&#30340;&#23450;&#20301;&#25513;&#27169;&#65292;&#24182;&#22312;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.18985</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#40657;&#30418;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#31867;&#30340;&#40065;&#26834;&#24615;&#21644;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal Classification with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18985
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#26512;&#21040;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#30340;&#19981;&#21516;&#27169;&#22411;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#35782;&#21035;&#25935;&#24863;&#21306;&#22495;&#24182;&#22312;&#26368;&#23567;&#31243;&#24230;&#25197;&#26354;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#65292;&#29983;&#25104;&#20248;&#36234;&#30340;&#23450;&#20301;&#25513;&#27169;&#65292;&#24182;&#22312;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#38024;&#23545;&#20174;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#26512;&#65288;1D&#65289;&#12289;&#22270;&#20687;&#20998;&#31867;&#65288;2D&#65289;&#21040;&#35270;&#39057;&#20998;&#31867;&#65288;3D&#65289;&#31561;&#19981;&#21516;&#27169;&#22411;&#31867;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#20248;&#21270;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#35782;&#21035;&#25935;&#24863;&#21306;&#22495;&#24182;&#22312;&#26368;&#23567;&#31243;&#24230;&#25197;&#26354;&#21644;&#21508;&#31181;&#25197;&#26354;&#31867;&#22411;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#12290;&#26032;&#39062;&#30340;RL&#26041;&#27861;&#22312;&#25152;&#26377;&#19977;&#20010;&#24212;&#29992;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;RL&#26041;&#27861;&#29983;&#25104;&#20102;&#20248;&#36234;&#30340;&#23450;&#20301;&#25513;&#27169;&#65292;&#22686;&#24378;&#20102;&#22270;&#20687;&#20998;&#31867;&#21644;&#24515;&#30005;&#22270;&#20998;&#26512;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23545;&#20110;&#24515;&#30005;&#22270;&#20998;&#26512;&#31561;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#24179;&#21488;&#31361;&#20986;&#20102;&#20020;&#24202;&#21307;&#29983;&#20851;&#27880;&#30340;&#20851;&#38190;&#24515;&#30005;&#22270;&#29255;&#27573;&#65292;&#21516;&#26102;&#30830;&#20445;&#23545;&#27969;&#34892;&#25197;&#26354;&#30340;&#38887;&#24615;&#12290;&#36825;&#19968;&#20840;&#38754;&#30340;&#24037;&#20855;&#26088;&#22312;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#36879;&#26126;&#24230;&#25552;&#39640;&#21508;&#31181;&#24212;&#29992;&#21644;&#25968;&#25454;&#31867;&#22411;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18985v1 Announce Type: cross  Abstract: We present a generic Reinforcement Learning (RL) framework optimized for crafting adversarial attacks on different model types spanning from ECG signal analysis (1D), image classification (2D), and video classification (3D). The framework focuses on identifying sensitive regions and inducing misclassifications with minimal distortions and various distortion types. The novel RL method outperforms state-of-the-art methods for all three applications, proving its efficiency. Our RL approach produces superior localization masks, enhancing interpretability for image classification and ECG analysis models. For applications such as ECG analysis, our platform highlights critical ECG segments for clinicians while ensuring resilience against prevalent distortions. This comprehensive tool aims to bolster both resilience with adversarial training and transparency across varied applications and data types.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24494;&#35843;&#25991;&#26412;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18978</link><description>&lt;p&gt;
TextCraftor&#65306;&#24744;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#21487;&#20197;&#25104;&#20026;&#22270;&#20687;&#36136;&#37327;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
TextCraftor: Your Text Encoder Can be Image Quality Controller
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18978
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#25991;&#26412;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20869;&#23481;&#29983;&#25104;&#39046;&#22495;&#65292;&#20351;&#24471;&#22312;&#35832;&#22914;&#22270;&#20687;&#32534;&#36753;&#21644;&#35270;&#39057;&#21512;&#25104;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#24182;&#38750;&#27809;&#26377;&#23616;&#38480;&#24615;&#12290;&#21512;&#25104;&#19982;&#36755;&#20837;&#25991;&#26412;&#30456;&#22865;&#21512;&#30340;&#22270;&#20687;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#22810;&#27425;&#36816;&#34892;&#20197;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#25165;&#33021;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#35768;&#22810;&#30740;&#31350;&#21162;&#21147;&#23545;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21363;UNet&#65292;&#36827;&#34892;&#24494;&#35843;&#65292;&#21033;&#29992;&#21508;&#31181;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#21162;&#21147;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#36827;&#34892;&#24494;&#35843;&#20197;&#25913;&#21892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21487;&#33021;&#21644;&#21487;&#34892;&#65292;&#20173;&#28982;&#22823;&#22810;&#26410;&#34987;&#25506;&#35752;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20854;&#26367;&#25442;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#26356;&#22909;&#30340;&#26041;&#27861;&#26159;&#24494;&#35843;&#25991;&#26412;&#32534;&#30721;&#22120;&#20197;&#25552;&#21319;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18978v1 Announce Type: cross  Abstract: Diffusion-based text-to-image generative models, e.g., Stable Diffusion, have revolutionized the field of content generation, enabling significant advancements in areas like image editing and video synthesis. Despite their formidable capabilities, these models are not without their limitations. It is still challenging to synthesize an image that aligns well with the input text, and multiple runs with carefully crafted prompts are required to achieve satisfactory results. To mitigate these limitations, numerous studies have endeavored to fine-tune the pre-trained diffusion models, i.e., UNet, utilizing various technologies. Yet, amidst these efforts, a pivotal question of text-to-image diffusion model training has remained largely unexplored: Is it possible and feasible to fine-tune the text encoder to improve the performance of text-to-image diffusion models? Our findings reveal that, instead of replacing the CLIP text encoder used in 
&lt;/p&gt;</description></item><item><title>Transformer&#27169;&#22411;&#22312;&#25913;&#38761;&#20256;&#32479;&#20219;&#21153;&#21644;&#25512;&#36827;&#36328;&#34892;&#19994;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.18969</link><description>&lt;p&gt;
&#20174;&#27010;&#24565;&#21040;&#23454;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Models from Concept to Implementation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18969
&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#25913;&#38761;&#20256;&#32479;&#20219;&#21153;&#21644;&#25512;&#36827;&#36328;&#34892;&#19994;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;Transformer&#26550;&#26500;&#26500;&#24314;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#21457;&#23637;&#26497;&#22823;&#25299;&#23485;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#30340;&#33539;&#22260;&#65292;&#36229;&#36234;&#20102;&#26368;&#21021;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#26041;&#38754;&#24212;&#29992;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;GPT&#31995;&#21015;&#12290;&#36825;&#39033;&#25506;&#32034;&#32858;&#28966;&#20110;&#20154;&#24037;&#26234;&#33021;(AI)&#39537;&#21160;&#24037;&#20855;&#22312;&#25913;&#38761;&#20256;&#32479;&#32534;&#30721;&#21644;&#38382;&#39064;&#35299;&#20915;&#31561;&#20219;&#21153;&#19978;&#30340;&#38761;&#21629;&#24615;&#24433;&#21709;&#65292;&#21516;&#26102;&#22312;&#36328;&#36234;&#19981;&#21516;&#34892;&#19994;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#24320;&#36767;&#26032;&#36335;&#24452;&#12290;&#20174;&#20195;&#30721;&#35299;&#37322;&#21644;&#22270;&#20687;&#25551;&#36848;&#21040;&#20419;&#36827;&#20132;&#20114;&#24335;&#31995;&#32479;&#30340;&#25645;&#24314;&#21644;&#25512;&#36827;&#35745;&#31639;&#39046;&#22495;&#65292;Transformer&#27169;&#22411;&#20307;&#29616;&#20102;&#28145;&#24230;&#23398;&#20064;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#26412;&#32508;&#36848;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#31361;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18969v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs), particularly those built on Transformer architectures, have significantly broadened the scope of natural language processing (NLP) applications, transcending their initial use in chatbot technology. This paper investigates the multifaceted applications of these models, with an emphasis on the GPT series. This exploration focuses on the transformative impact of artificial intelligence (AI) driven tools in revolutionizing traditional tasks like coding and problem-solving, while also paving new paths in research and development across diverse industries. From code interpretation and image captioning to facilitating the construction of interactive systems and advancing computational domains, Transformer models exemplify a synergy of deep learning, data analysis, and neural network design. This survey provides an in-depth look at the latest research in Transformer models, highlighting the
&lt;/p&gt;</description></item><item><title>LORD&#36890;&#36807;&#19981;&#26399;&#26395;&#30340;&#35821;&#35328;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#27169;&#22411;&#30340;&#30456;&#21453;&#22870;&#21169;&#35774;&#35745;&#65292;&#20197;&#20415;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#22870;&#21169;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18965</link><description>&lt;p&gt;
LORD&#65306;&#22522;&#20110;&#22823;&#27169;&#22411;&#30340;&#30456;&#21453;&#22870;&#21169;&#35774;&#35745;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
LORD: Large Models based Opposite Reward Design for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18965
&lt;/p&gt;
&lt;p&gt;
LORD&#36890;&#36807;&#19981;&#26399;&#26395;&#30340;&#35821;&#35328;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#27169;&#22411;&#30340;&#30456;&#21453;&#22870;&#21169;&#35774;&#35745;&#65292;&#20197;&#20415;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39537;&#21160;&#30340;&#33258;&#21160;&#39550;&#39542;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#25968;&#25454;&#39537;&#21160;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#20026;RL&#21046;&#23450;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#35201;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#23450;&#20041;&#21644;&#37327;&#21270;&#33391;&#22909;&#30340;&#39550;&#39542;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#22870;&#21169;&#27169;&#22411;&#65292;&#20026;&#25351;&#23450;&#20855;&#26377;&#26399;&#26395;&#35821;&#35328;&#30446;&#26631;&#30340;&#20219;&#21153;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#26399;&#26395;&#35821;&#35328;&#30446;&#26631;&#65292;&#22914;&#8220;&#23433;&#20840;&#39550;&#39542;&#8221;&#65292;&#23545;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35828;&#26159;&#27169;&#31946;&#19988;&#38590;&#20197;&#29702;&#35299;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19981;&#26399;&#26395;&#30340;&#35821;&#35328;&#30446;&#26631;&#65292;&#27604;&#22914;&#8220;&#30896;&#25758;&#8221;&#65292;&#26356;&#21152;&#20855;&#20307;&#19988;&#21487;&#36319;&#36394;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LORD&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22823;&#27169;&#22411;&#30340;&#30456;&#21453;&#22870;&#21169;&#35774;&#35745;&#65292;&#36890;&#36807;&#19981;&#26399;&#26395;&#30340;&#35821;&#35328;&#30446;&#26631;&#26469;&#23454;&#29616;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#20351;&#29992;&#65292;&#20316;&#20026;&#38646;-shot&#22870;&#21169;&#27169;&#22411;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18965v1 Announce Type: cross  Abstract: Reinforcement learning (RL) based autonomous driving has emerged as a promising alternative to data-driven imitation learning approaches. However, crafting effective reward functions for RL poses challenges due to the complexity of defining and quantifying good driving behaviors across diverse scenarios. Recently, large pretrained models have gained significant attention as zero-shot reward models for tasks specified with desired linguistic goals. However, the desired linguistic goals for autonomous driving such as "drive safely" are ambiguous and incomprehensible by pretrained models. On the other hand, undesired linguistic goals like "collision" are more concrete and tractable. In this work, we introduce LORD, a novel large models based opposite reward design through undesired linguistic goals to enable the efficient use of large pretrained models as zero-shot reward models. Through extensive experiments, our proposed framework shows
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#19981;&#23433;&#20840;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#28216;&#25103;&#20013;&#30340;&#36829;&#27861;&#25512;&#24191;&#23041;&#32961;&#65292;&#25910;&#38598;&#20102;&#19968;&#32452;&#21253;&#21547;&#24615;&#26292;&#21147;&#21644;&#26292;&#21147;&#20869;&#23481;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.18957</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35843;&#33410;&#19981;&#23433;&#20840;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#28216;&#25103;&#20013;&#30340;&#36829;&#27861;&#22312;&#32447;&#22270;&#29255;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18957
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#19981;&#23433;&#20840;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#28216;&#25103;&#20013;&#30340;&#36829;&#27861;&#25512;&#24191;&#23041;&#32961;&#65292;&#25910;&#38598;&#20102;&#19968;&#32452;&#21253;&#21547;&#24615;&#26292;&#21147;&#21644;&#26292;&#21147;&#20869;&#23481;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#28216;&#25103;&#65288;UGCGs&#65289;&#22312;&#20799;&#31461;&#21644;&#38738;&#23569;&#24180;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29992;&#20110;&#31038;&#20132;&#20114;&#21160;&#21644;&#26356;&#26377;&#21019;&#24847;&#30340;&#22312;&#32447;&#23089;&#20048;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#30528;&#26356;&#39640;&#30340;&#26292;&#38706;&#19981;&#33391;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20799;&#31461;&#21644;&#38738;&#23569;&#24180;&#22312;&#32447;&#23433;&#20840;&#30340;&#26085;&#30410;&#20851;&#27880;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#31532;&#19968;&#27493;&#30740;&#31350;&#23545;&#19981;&#23433;&#20840;UGCGs&#30340;&#36829;&#27861;&#25512;&#24191;&#36827;&#34892;&#23041;&#32961;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#32452;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;2,924&#24352;&#23637;&#31034;&#19981;&#21516;&#24615;&#26292;&#21147;&#21644;&#26292;&#21147;&#20869;&#23481;&#30340;&#22270;&#20687;&#65292;&#36825;&#20123;&#20869;&#23481;&#34987;&#28216;&#25103;&#21019;&#24314;&#32773;&#29992;&#20110;&#25512;&#24191;UGCGs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18957v1 Announce Type: cross  Abstract: Online user-generated content games (UGCGs) are increasingly popular among children and adolescents for social interaction and more creative online entertainment. However, they pose a heightened risk of exposure to explicit content, raising growing concerns for the online safety of children and adolescents. Despite these concerns, few studies have addressed the issue of illicit image-based promotions of unsafe UGCGs on social media, which can inadvertently attract young users. This challenge arises from the difficulty of obtaining comprehensive training data for UGCG images and the unique nature of these images, which differ from traditional unsafe content. In this work, we take the first step towards studying the threat of illicit promotions of unsafe UGCGs. We collect a real-world dataset comprising 2,924 images that display diverse sexually explicit and violent content used to promote UGCGs by their game creators. Our in-depth studi
&lt;/p&gt;</description></item><item><title>SPA&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#32467;&#26500;&#21098;&#26525;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#20219;&#20309;&#26550;&#26500;&#12289;&#20219;&#20309;&#26694;&#26550;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21098;&#26525;&#65292;&#32780;&#19988;&#21487;&#20197;&#22312;&#20219;&#20309;&#35757;&#32451;&#38454;&#27573;&#36827;&#34892;&#21098;&#26525;&#12290;</title><link>https://arxiv.org/abs/2403.18955</link><description>&lt;p&gt;
&#20219;&#20309;&#32467;&#26500;&#35009;&#21098;&#65306;&#20219;&#20309;&#26550;&#26500;&#65292;&#20219;&#20309;&#26694;&#26550;&#65292;&#20219;&#20309;&#26102;&#20505;
&lt;/p&gt;
&lt;p&gt;
Structurally Prune Anything: Any Architecture, Any Framework, Any Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18955
&lt;/p&gt;
&lt;p&gt;
SPA&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#32467;&#26500;&#21098;&#26525;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#20219;&#20309;&#26550;&#26500;&#12289;&#20219;&#20309;&#26694;&#26550;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21098;&#26525;&#65292;&#32780;&#19988;&#21487;&#20197;&#22312;&#20219;&#20309;&#35757;&#32451;&#38454;&#27573;&#36827;&#34892;&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18955v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26159;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25928;&#29575;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#19982;&#26080;&#32467;&#26500;&#21098;&#26525;&#19981;&#21516;&#65292;&#21518;&#32773;&#20165;&#23558;&#29305;&#23450;&#21442;&#25968;&#35774;&#32622;&#20026;&#38646;&#65292;&#32467;&#26500;&#21098;&#26525;&#28040;&#38500;&#20102;&#25972;&#20010;&#36890;&#36947;&#65292;&#20174;&#32780;&#20135;&#29983;&#30452;&#25509;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#32806;&#21512;&#21442;&#25968;&#27169;&#24335;&#65292;&#22914;&#27531;&#24046;&#36830;&#25509;&#21644;&#32452;&#21367;&#31215;&#65292;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#21450;&#21487;&#20197;&#25191;&#34892;&#21098;&#26525;&#30340;&#21508;&#31181;&#26102;&#38388;&#38454;&#27573;&#20351;&#24471;&#29616;&#26377;&#30340;&#21098;&#26525;&#26041;&#27861;&#23545;&#19981;&#21516;&#26550;&#26500;&#12289;&#26694;&#26550;&#21644;&#21098;&#26525;&#20934;&#21017; less adaptable&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Structurally Prune Anything&#65288;SPA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#32467;&#26500;&#21098;&#26525;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#20219;&#20309;&#26550;&#26500;&#65292;&#20219;&#20309;&#26694;&#26550;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21098;&#26525;&#65292;&#32780;&#19988;&#21487;&#20197;&#22312;&#35757;&#32451;&#30340;&#20219;&#20309;&#38454;&#27573;&#36827;&#34892;&#21098;&#26525;&#12290;SPA&#21033;&#29992;&#26631;&#20934;&#21270;&#30340;&#35745;&#31639;&#22270;&#21644;ONNX&#34920;&#31034;&#26469;&#23545;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#21098;&#26525;&#65292;&#32780;&#26080;&#38656;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18955v1 Announce Type: new  Abstract: Neural network pruning serves as a critical technique for enhancing the efficiency of deep learning models. Unlike unstructured pruning, which only sets specific parameters to zero, structured pruning eliminates entire channels, thus yielding direct computational and storage benefits. However, the diverse patterns for coupling parameters, such as residual connections and group convolutions, the diverse deep learning frameworks, and the various time stages at which pruning can be performed make existing pruning methods less adaptable to different architectures, frameworks, and pruning criteria. To address this, we introduce Structurally Prune Anything (SPA), a versatile structured pruning framework that can prune neural networks with any architecture, from any framework, and at any stage of training. SPA leverages a standardized computational graph and ONNX representation to prune diverse neural network architectures without the need for 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20256;&#32479;&#27700;&#24211;&#35745;&#31639;&#21644;&#19979;&#19968;&#20195;&#27700;&#24211;&#35745;&#31639;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#39044;&#27979;&#22797;&#26434;&#21644;&#28151;&#27788;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#24403;&#21333;&#29420;&#20351;&#29992;RC&#21644;NGRC&#32452;&#20214;&#19981;&#36275;&#20197;&#28385;&#36275;&#26102;&#65292;&#28151;&#21512;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.18953</link><description>&lt;p&gt;
&#23558;&#20256;&#32479;&#21644;&#19979;&#19968;&#20195;&#27700;&#24211;&#35745;&#31639;&#28151;&#21512;&#20197;&#31934;&#30830;&#39640;&#25928;&#22320;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Hybridizing Traditional and Next-Generation Reservoir Computing to Accurately and Efficiently Forecast Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18953
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20256;&#32479;&#27700;&#24211;&#35745;&#31639;&#21644;&#19979;&#19968;&#20195;&#27700;&#24211;&#35745;&#31639;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#39044;&#27979;&#22797;&#26434;&#21644;&#28151;&#27788;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#24403;&#21333;&#29420;&#20351;&#29992;RC&#21644;NGRC&#32452;&#20214;&#19981;&#36275;&#20197;&#28385;&#36275;&#26102;&#65292;&#28151;&#21512;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#24211;&#35745;&#31639;&#65288;RCs&#65289;&#26159;&#24378;&#22823;&#30340;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#19979;&#19968;&#20195;&#27700;&#24211;&#35745;&#31639;&#65288;NGRCs&#65289;&#65292;&#30456;&#27604;RCs&#65292;&#23427;&#20204;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#22914;&#38477;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#21644;&#36739;&#20302;&#30340;&#25968;&#25454;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;NGRCs&#19982;RCs&#23384;&#22312;&#30528;&#33258;&#36523;&#29420;&#29305;&#30340;&#23454;&#38469;&#22256;&#38590;&#65292;&#21253;&#25324;&#23545;&#25968;&#25454;&#20013;&#30340;&#37319;&#26679;&#26102;&#38388;&#21644;&#38750;&#32447;&#24615;&#31867;&#22411;&#30340;&#25935;&#24863;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#21644;&#28151;&#27788;&#21160;&#21147;&#31995;&#32479;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#28151;&#21512;RC-NGRC&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#28151;&#21512;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20934;&#30830;&#30340;&#30701;&#26399;&#39044;&#27979;&#65292;&#24182;&#22312;RC&#21644;NGRC&#21508;&#33258;&#29420;&#31435;&#26102;&#33021;&#22815;&#25429;&#25417;&#21160;&#21147;&#31995;&#32479;&#30340;&#38271;&#26399;&#32479;&#35745;&#24773;&#20917;&#12290;&#24403;&#20004;&#20010;&#32452;&#20214;&#30340;&#39044;&#27979;&#33021;&#21147;&#37117;&#21463;&#38480;&#26102;&#65292;&#20363;&#22914;&#23545;&#20110;&#23567;&#30340;RC&#21644;&#22823;&#30340;&#37319;&#26679;&#26102;&#38388;&#26102;&#65292;&#28151;&#21512;RC-NGRC&#26041;&#27861;&#30340;&#20248;&#21183;&#26368;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18953v1 Announce Type: new  Abstract: Reservoir computers (RCs) are powerful machine learning architectures for time series prediction. Recently, next generation reservoir computers (NGRCs) have been introduced, offering distinct advantages over RCs, such as reduced computational expense and lower data requirements. However, NGRCs have their own practical difficulties distinct from those of RCs, including sensitivity to sampling time and type of nonlinearities in the data. Here, we introduce a hybrid RC-NGRC approach for time series forecasting of complex and chaotic dynamical systems. We show that our hybrid approach can produce accurate short term predictions and capture the long term statistics of dynamical systems in situations where the RC and NGRC components alone are insufficient. The advantage of the hybrid RC-NGRC approach is most pronounced when both components are limited in their prediction capabilities, e.g. for a small RC and a large sampling time in the traini
&lt;/p&gt;</description></item><item><title>MoNet&#26159;&#19968;&#31181;&#32467;&#21512;&#31471;&#21040;&#31471;&#23398;&#20064;&#21644;&#27169;&#22359;&#21270;&#32593;&#32476;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35748;&#30693;&#24341;&#23548;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#32780;&#26080;&#38656;&#20219;&#21153;&#32423;&#21035;&#30340;&#30417;&#30563;&#65292;&#21516;&#26102;&#25552;&#39640;&#31471;&#21040;&#31471;&#25512;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#35270;&#35273;&#33258;&#20027;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2403.18947</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21487;&#35299;&#37322;&#30340;&#24863;&#30693;&#21160;&#20316;&#23398;&#20064;&#36890;&#36807;&#28508;&#22312;&#21151;&#33021;&#27169;&#22359;&#24615;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Interpretable Sensorimotor Learning via Latent Functional Modularity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18947
&lt;/p&gt;
&lt;p&gt;
MoNet&#26159;&#19968;&#31181;&#32467;&#21512;&#31471;&#21040;&#31471;&#23398;&#20064;&#21644;&#27169;&#22359;&#21270;&#32593;&#32476;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35748;&#30693;&#24341;&#23548;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#32780;&#26080;&#38656;&#20219;&#21153;&#32423;&#21035;&#30340;&#30417;&#30563;&#65292;&#21516;&#26102;&#25552;&#39640;&#31471;&#21040;&#31471;&#25512;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#35270;&#35273;&#33258;&#20027;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MoNet&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#31471;&#21040;&#31471;&#23398;&#20064;&#19982;&#27169;&#22359;&#21270;&#32593;&#32476;&#26550;&#26500;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;&#21644;&#21487;&#35299;&#37322;&#30340;&#24863;&#30693;&#21160;&#20316;&#23398;&#20064;&#12290;MoNet&#30001;&#19977;&#20010;&#21151;&#33021;&#19978;&#19981;&#21516;&#30340;&#31070;&#32463;&#27169;&#22359;&#32452;&#25104;&#65306;&#24863;&#30693;&#12289;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#36890;&#36807;&#35748;&#30693;&#24341;&#23548;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;MoNet&#26377;&#25928;&#22320;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#29305;&#23450;&#20219;&#21153;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#21153;&#32423;&#21035;&#30340;&#30417;&#30563;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#34701;&#20837;&#20102;&#19968;&#31181;&#22312;&#32447;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#31471;&#21040;&#31471;&#25512;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#19981;&#24433;&#21709;&#24863;&#30693;&#21160;&#20316;&#24615;&#33021;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#23460;&#20869;&#29615;&#22659;&#20013;&#65292;MoNet&#23637;&#31034;&#20102;&#26377;&#25928;&#30340;&#35270;&#35273;&#33258;&#20027;&#23548;&#33322;&#65292;&#22312;&#20219;&#21153;&#29305;&#24322;&#24615;&#20998;&#26512;&#20013;&#36229;&#36234;&#20102;&#22522;&#32447;&#27169;&#22411;11%&#33267;47%&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#24863;&#30693;&#26174;&#33879;&#24615;&#22320;&#22270;&#30340;&#20107;&#21518;&#20998;&#26512;&#25506;&#35752;&#20102;&#25105;&#20204;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18947v1 Announce Type: new  Abstract: We introduce MoNet, a novel method that combines end-to-end learning with modular network architectures for self-supervised and interpretable sensorimotor learning. MoNet is composed of three functionally distinct neural modules: Perception, Planning, and Control. Leveraging its inherent modularity through a cognition-guided contrastive loss function, MoNet efficiently learns task-specific decision-making processes in latent space, without requiring task-level supervision. Moreover, our method incorporates an online post-hoc explainability approach, which enhances the interpretability of the end-to-end inferences without a trade-off in sensorimotor performance. In real-world indoor environments, MoNet demonstrates effective visual autonomous navigation, surpassing baseline models by 11% to 47% in task specificity analysis. We further delve into the interpretability of our network through the post-hoc analysis of perceptual saliency maps 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#32858;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#38543;&#26426;&#25277;&#26679;&#29983;&#25104;&#32858;&#21512;&#22120;&#27874;&#26463;&#24418;&#25104;&#30690;&#37327;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#30340;&#32858;&#21512;&#35823;&#24046;&#26368;&#23567;&#21270;&#21644;&#35774;&#22791;&#36873;&#25321;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.18946</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#35268;&#27169;&#32593;&#32476;&#30340;&#38543;&#26426;&#32858;&#21512;&#27874;&#26463;&#25104;&#24418;&#29992;&#20110;&#31354;&#20013;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Random Aggregate Beamforming for Over-the-Air Federated Learning in Large-Scale Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#32858;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#38543;&#26426;&#25277;&#26679;&#29983;&#25104;&#32858;&#21512;&#22120;&#27874;&#26463;&#24418;&#25104;&#30690;&#37327;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#30340;&#32858;&#21512;&#35823;&#24046;&#26368;&#23567;&#21270;&#21644;&#35774;&#22791;&#36873;&#25321;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#37096;&#32626;&#26222;&#36941;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#20110;&#32593;&#32476;&#36793;&#32536;&#30340;&#36235;&#21183;&#26085;&#30410;&#26126;&#26174;&#12290;&#20316;&#20026;&#19968;&#31181;&#26377;&#26395;&#23454;&#29616;&#23433;&#20840;&#36793;&#32536;&#26234;&#33021;&#30340;&#26694;&#26550;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#31354;&#20013;&#35745;&#31639;&#65288;AirComp&#65289;&#24050;&#32463;&#34987;&#38598;&#25104;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#32852;&#21512;&#35774;&#22791;&#36873;&#25321;&#21644;&#32858;&#21512;&#27874;&#26463;&#25104;&#24418;&#35774;&#35745;&#65292;&#20854;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#32858;&#21512;&#35823;&#24046;&#24182;&#26368;&#22823;&#21270;&#36873;&#23450;&#35774;&#22791;&#30340;&#25968;&#37327;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#23588;&#20854;&#38590;&#20197;&#35299;&#20915;&#12290;&#20026;&#20102;&#20197;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#26041;&#24335;&#35299;&#20915;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#32858;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#38543;&#26426;&#25277;&#26679;&#29983;&#25104;&#32858;&#21512;&#27874;&#26463;&#24418;&#25104;&#30690;&#37327;&#65292;&#32780;&#38750;&#20248;&#21270;&#12290;&#35813;&#26041;&#26696;&#30340;&#23454;&#26045;&#19981;&#38656;&#35201;&#36827;&#34892;&#20449;&#36947;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18946v1 Announce Type: cross  Abstract: At present, there is a trend to deploy ubiquitous artificial intelligence (AI) applications at the edge of the network. As a promising framework that enables secure edge intelligence, federated learning (FL) has received widespread attention, and over-the-air computing (AirComp) has been integrated to further improve the communication efficiency. In this paper, we consider a joint device selection and aggregate beamforming design with the objectives of minimizing the aggregate error and maximizing the number of selected devices. This yields a combinatorial problem, which is difficult to solve especially in large-scale networks. To tackle the problems in a cost-effective manner, we propose a random aggregate beamforming-based scheme, which generates the aggregator beamforming vector via random sampling rather than optimization. The implementation of the proposed scheme does not require the channel estimation. We additionally use asympto
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#20851;&#20110;&#20004;&#31181;&#28145;&#24230;&#23637;&#24320;&#26426;&#21046;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#23637;&#24320;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#19968;&#20010;&#23436;&#20840;&#23637;&#24320;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39640;&#25928;&#36827;&#34892;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#21151;&#29575;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.18930</link><description>&lt;p&gt;
&#20248;&#21270;&#26080;&#32447;&#32593;&#32476;&#19982;&#28145;&#24230;&#23637;&#24320;&#65306;&#20004;&#31181;&#28145;&#24230;&#23637;&#24320;&#26426;&#21046;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimizing Wireless Networks with Deep Unfolding: Comparative Study on Two Deep Unfolding Mechanisms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#20851;&#20110;&#20004;&#31181;&#28145;&#24230;&#23637;&#24320;&#26426;&#21046;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#23637;&#24320;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#19968;&#20010;&#23436;&#20840;&#23637;&#24320;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39640;&#25928;&#36827;&#34892;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#21151;&#29575;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#20004;&#31181;&#28145;&#24230;&#23637;&#24320;&#26426;&#21046;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#65292;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#19979;&#19968;&#20195;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#21151;&#29575;&#25511;&#21046;&#12290;&#21151;&#29575;&#25511;&#21046;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#22810;&#20010;&#24178;&#25200;&#38142;&#25509;&#19978;&#30340;&#33021;&#37327;&#25928;&#29575;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#26159;&#38750;&#20984;&#30340;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#25968;&#35268;&#21010;&#36716;&#25442;&#20026;&#35813;&#38382;&#39064;&#35774;&#35745;&#20102;&#20004;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#31532;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#26159;&#25968;&#20540;&#35299;&#65292;&#32780;&#31532;&#20108;&#20010;&#35299;&#20915;&#26041;&#26696;&#26159;&#38381;&#24335;&#35299;&#12290;&#22522;&#20110;&#31532;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21322;&#23637;&#24320;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#25105;&#20204;&#32467;&#21512;&#20102;&#26080;&#32447;&#36890;&#20449;&#39046;&#22495;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#38381;&#24335;&#35299;&#30340;&#20142;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23436;&#20840;&#23637;&#24320;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#20805;&#20998;&#21033;&#29992;&#20102;&#34920;&#36798;&#21147;&#24378;&#30340;&#38381;&#24335;&#21151;&#29575;&#25511;&#21046;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#12290;&#22312;&#20223;&#30495;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18930v1 Announce Type: cross  Abstract: In this work, we conduct a comparative study on two deep unfolding mechanisms to efficiently perform power control in the next generation wireless networks. The power control problem is formulated as energy efficiency over multiple interference links. The problem is nonconvex. We employ fractional programming transformation to design two solutions for the problem. The first solution is a numerical solution while the second solution is a closed-form solution. Based on the first solution, we design a semi-unfolding deep learning model where we combine the domain knowledge of the wireless communications and the recent advances in the data-driven deep learning. Moreover, on the highlights of the closed-form solution, fully deep unfolded deep learning model is designed in which we fully leveraged the expressive closed-form power control solution and deep learning advances. In the simulation results, we compare the performance of the propose
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#36890;&#36807;&#29983;&#29289;&#21487;&#20449;&#30340;&#20449;&#29992;&#25351;&#27966;&#26041;&#26696;&#36827;&#34892;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#20860;&#23481;&#24615;&#24191;&#27867;&#12289;&#33021;&#25928;&#39640;&#12289;&#36866;&#29992;&#20110;&#21508;&#31181;&#23398;&#20064;&#26465;&#20214;&#21644;&#30828;&#20214;&#12289;&#20197;&#21450;&#23454;&#26102;&#33258;&#36866;&#24212;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.18929</link><description>&lt;p&gt;
&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Neuroscience-Inspired Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18929
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#36890;&#36807;&#29983;&#29289;&#21487;&#20449;&#30340;&#20449;&#29992;&#25351;&#27966;&#26041;&#26696;&#36827;&#34892;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#20860;&#23481;&#24615;&#24191;&#27867;&#12289;&#33021;&#25928;&#39640;&#12289;&#36866;&#29992;&#20110;&#21508;&#31181;&#23398;&#20064;&#26465;&#20214;&#21644;&#30828;&#20214;&#12289;&#20197;&#21450;&#23454;&#26102;&#33258;&#36866;&#24212;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#22823;&#25209;&#35780;&#22312;&#20110;&#29992;&#20110;&#23398;&#20064;&#30340;&#20449;&#29992;&#25351;&#27966;&#26041;&#26696;&#30340;&#29983;&#29289;&#19981;&#21487;&#20449;&#24615; - &#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#12290;&#36825;&#31181;&#19981;&#21487;&#20449;&#24615;&#36716;&#21270;&#20026;&#23454;&#38469;&#38480;&#21046;&#65292;&#28085;&#30422;&#31185;&#23398;&#39046;&#22495;&#65292;&#21253;&#25324;&#19982;&#30828;&#20214;&#19981;&#20860;&#23481;&#21644;&#38750;&#21487;&#24494;&#23454;&#29616;&#65292;&#20174;&#32780;&#23548;&#33268;&#26114;&#36149;&#30340;&#33021;&#28304;&#38656;&#27714;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20855;&#26377;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#30340;&#20449;&#29992;&#25351;&#27966;&#19982;&#23454;&#38469;&#19978;&#30340;&#20219;&#20309;&#23398;&#20064;&#26465;&#20214;&#20860;&#23481;&#65292;&#24182;&#19988;&#33410;&#33021;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#36866;&#24212;&#30828;&#20214;&#21644;&#31185;&#23398;&#24314;&#27169;&#65292;&#20363;&#22914;&#23398;&#20064;&#29289;&#29702;&#31995;&#32479;&#21644;&#38750;&#21487;&#24494;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#23548;&#33268;&#23454;&#26102;&#30340;&#33258;&#36866;&#24212;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#36328;&#23398;&#31185;&#20998;&#25903;&#65292;&#20301;&#20110;&#31070;&#32463;&#31185;&#23398;&#12289;&#35748;&#30693;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#38598;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18929v1 Announce Type: cross  Abstract: One major criticism of deep learning centers around the biological implausibility of the credit assignment schema used for learning -- backpropagation of errors. This implausibility translates into practical limitations, spanning scientific fields, including incompatibility with hardware and non-differentiable implementations, thus leading to expensive energy requirements. In contrast, biologically plausible credit assignment is compatible with practically any learning condition and is energy-efficient. As a result, it accommodates hardware and scientific modeling, e.g. learning with physical systems and non-differentiable behavior. Furthermore, it can lead to the development of real-time, adaptive neuromorphic processing systems. In addressing this problem, an interdisciplinary branch of artificial intelligence research that lies at the intersection of neuroscience, cognitive science, and machine learning has emerged. In this paper, w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MoE&#27169;&#22411;\tool&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#22411;&#19987;&#23478;&#21644;&#22522;&#20110;&#38408;&#20540;&#30340;&#36335;&#30001;&#22120;&#65292;&#20351;&#26631;&#35760;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20165;&#28041;&#21450;&#21040;&#24517;&#35201;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#22312;&#20943;&#23569;MoE&#23618;&#35745;&#31639;&#36127;&#36733;50%&#20197;&#19978;&#30340;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18926</link><description>&lt;p&gt;
&#29992;&#26356;&#31232;&#30095;&#30340;&#36873;&#25321;&#25552;&#39640;&#31232;&#30095;&#27169;&#22411;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Enhancing Efficiency in Sparse Models with Sparser Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18926
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MoE&#27169;&#22411;\tool&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#22411;&#19987;&#23478;&#21644;&#22522;&#20110;&#38408;&#20540;&#30340;&#36335;&#30001;&#22120;&#65292;&#20351;&#26631;&#35760;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20165;&#28041;&#21450;&#21040;&#24517;&#35201;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#22312;&#20943;&#23569;MoE&#23618;&#35745;&#31639;&#36127;&#36733;50%&#20197;&#19978;&#30340;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#27169;&#22411;&#65292;&#21253;&#25324;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#27169;&#22411;&#65292;&#24050;&#32463;&#25104;&#20026;&#32553;&#25918;Transformer&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#23384;&#22312;&#35745;&#31639;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#22823;&#37327;&#21442;&#25968;&#36890;&#36807;&#23558;&#20540;&#20056;&#20197;&#38646;&#25110;&#20302;&#28608;&#27963;&#20540;&#26080;&#35859;&#21442;&#19982;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\tool &#30340;&#26032;&#39062;MoE&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#21319;&#31232;&#30095;MoE&#27169;&#22411;&#30340;&#21151;&#25928;&#21644;&#25928;&#29575;&#12290; \tool &#21033;&#29992;&#23567;&#22411;&#19987;&#23478;&#21644;&#22522;&#20110;&#38408;&#20540;&#30340;&#36335;&#30001;&#22120;&#65292;&#20351;&#26631;&#35760;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#20165;&#28041;&#21450;&#21040;&#24517;&#35201;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;\tool &#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;MoE&#23618;&#30340;&#35745;&#31639;&#36127;&#36733;&#20943;&#23569;50\%&#20197;&#19978;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;\tool &#30340;&#36890;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#23494;&#38598;&#27169;&#22411;&#65292;&#22312;&#25512;&#26029;&#26399;&#38388;&#23454;&#29616;&#31232;&#30095;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18926v1 Announce Type: cross  Abstract: Sparse models, including sparse Mixture-of-Experts (MoE) models, have emerged as an effective approach for scaling Transformer models. However, they often suffer from computational inefficiency since a significant number of parameters are unnecessarily involved in computations via multiplying values by zero or low activation values. To address this issue, we present \tool, a novel MoE designed to enhance both the efficacy and efficiency of sparse MoE models. \tool leverages small experts and a threshold-based router to enable tokens to selectively engage only essential parameters. Our extensive experiments on language modeling and machine translation tasks demonstrate that \tool can enhance model performance while decreasing the computation load at MoE layers by over 50\% without sacrificing performance. Furthermore, we present the versatility of \tool by applying it to dense models, enabling sparse computation during inference. We pro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#23618;&#34701;&#21512;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#33258;&#28982;&#36807;&#31243;&#65292;&#26377;&#25928;&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#27987;&#24230;</title><link>https://arxiv.org/abs/2403.18923</link><description>&lt;p&gt;
&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#29992;&#20110;&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#27987;&#24230;
&lt;/p&gt;
&lt;p&gt;
Nature-Guided Cognitive Evolution for Predicting Dissolved Oxygen Concentrations in North Temperate Lakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18923
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#23618;&#34701;&#21512;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#33258;&#28982;&#36807;&#31243;&#65292;&#26377;&#25928;&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#27987;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#65288;DO&#65289;&#27987;&#24230;&#38656;&#35201;&#23545;&#19981;&#21516;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#29289;&#20505;&#27169;&#24335;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;&#36825;&#20984;&#26174;&#20102;&#36873;&#25321;&#29289;&#20505;&#29305;&#24449;&#21644;&#29305;&#24449;&#20132;&#20114;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#36807;&#31243;&#30340;&#27169;&#22411;&#21463;&#37096;&#20998;&#36807;&#31243;&#30693;&#35782;&#38480;&#21046;&#25110;&#29305;&#24449;&#34920;&#31034;&#36807;&#20110;&#31616;&#21270;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26377;&#25928;&#36873;&#25321;&#19981;&#21516;&#28246;&#27850;&#31867;&#22411;&#21644;&#20219;&#21153;&#30340;&#30456;&#20851;&#29305;&#24449;&#20132;&#20114;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;DO&#25968;&#25454;&#25910;&#38598;&#19981;&#39057;&#32321;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#65288;NGCE&#65289;&#31574;&#30053;&#65292;&#36825;&#20195;&#34920;&#20102;&#33258;&#36866;&#24212;&#23398;&#20064;&#19982;&#33258;&#28982;&#36807;&#31243;&#22810;&#23618;&#34701;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20195;&#35874;&#36807;&#31243;&#20026;&#22522;&#30784;&#30340;&#27169;&#22411;&#29983;&#25104;&#27169;&#25311;DO&#26631;&#31614;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#25311;&#26631;&#31614;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#22810;&#31181;&#32676;&#35748;&#30693;&#36827;&#21270;&#25628;&#32034;&#65292;&#27169;&#22411;&#21453;&#26144;&#33258;&#28982;&#26377;&#26426;&#20307;&#65292;&#36866;&#24212;&#24615;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18923v1 Announce Type: cross  Abstract: Predicting dissolved oxygen (DO) concentrations in north temperate lakes requires a comprehensive study of phenological patterns across various ecosystems, which highlights the significance of selecting phenological features and feature interactions. Process-based models are limited by partial process knowledge or oversimplified feature representations, while machine learning models face challenges in efficiently selecting relevant feature interactions for different lake types and tasks, especially under the infrequent nature of DO data collection. In this paper, we propose a Nature-Guided Cognitive Evolution (NGCE) strategy, which represents a multi-level fusion of adaptive learning with natural processes. Specifically, we utilize metabolic process-based models to generate simulated DO labels. Using these simulated labels, we implement a multi-population cognitive evolutionary search, where models, mirroring natural organisms, adaptiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;FPGA&#19978;&#20351;&#29992;&#26234;&#33021;&#31163;&#29255;&#39537;&#36880;&#27969;&#27169;&#24335;CNN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;&#39537;&#36880;&#26426;&#21046;&#26469;&#35299;&#20915;&#29616;&#20195;&#25299;&#25169;&#32467;&#26500;&#22312;&#33455;&#29255;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.18921</link><description>&lt;p&gt;
SMOF&#65306;&#22312;FPGA&#19978;&#20351;&#29992;&#26234;&#33021;&#31163;&#29255;&#39537;&#36880;&#27969;&#27169;&#24335;CNN
&lt;/p&gt;
&lt;p&gt;
SMOF: Streaming Modern CNNs on FPGAs with Smart Off-Chip Eviction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;FPGA&#19978;&#20351;&#29992;&#26234;&#33021;&#31163;&#29255;&#39537;&#36880;&#27969;&#27169;&#24335;CNN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;&#39537;&#36880;&#26426;&#21046;&#26469;&#35299;&#20915;&#29616;&#20195;&#25299;&#25169;&#32467;&#26500;&#22312;&#33455;&#29255;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;&#20247;&#22810;&#35270;&#35273;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#39640;&#30340;&#22788;&#29702;&#35201;&#27714;&#38656;&#35201;&#26377;&#25928;&#30340;&#30828;&#20214;&#21152;&#36895;&#20197;&#28385;&#36275;&#24212;&#29992;&#30340;&#24615;&#33021;&#30446;&#26631;&#12290;&#22312;FPGAs&#39046;&#22495;&#65292;&#29992;&#25143;&#36890;&#24120;&#37319;&#29992;&#22522;&#20110;&#27969;&#24335;&#25968;&#25454;&#27969;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#23618;&#27969;&#27700;&#32447;&#21644;&#22312;&#33455;&#29255;&#19978;&#20445;&#30041;&#25968;&#25454;&#20197;&#38477;&#20302;&#31163;&#29255;&#20869;&#23384;&#35775;&#38382;&#26469;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#25299;&#25169;&#32467;&#26500;&#65292;&#22914;UNet&#12289;YOLO&#21644;X3D&#27169;&#22411;&#65292;&#21033;&#29992;&#38271;&#36339;&#36291;&#36830;&#25509;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#33455;&#29255;&#20869;&#23384;&#31354;&#38388;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#36825;&#31181;&#31995;&#32479;&#26550;&#26500;&#25152;&#36798;&#21040;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#35745;&#31639;&#27969;&#31243;&#20013;&#27839;&#30528;&#26435;&#37325;&#21644;&#28608;&#27963;&#39537;&#36880;&#26426;&#21046;&#33267;&#31163;&#29255;&#20869;&#23384;&#65292;&#32771;&#34385;&#21040;&#21487;&#29992;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18921v1 Announce Type: cross  Abstract: Convolutional Neural Networks (CNNs) have demonstrated their effectiveness in numerous vision tasks. However, their high processing requirements necessitate efficient hardware acceleration to meet the application's performance targets. In the space of FPGAs, streaming-based dataflow architectures are often adopted by users, as significant performance gains can be achieved through layer-wise pipelining and reduced off-chip memory access by retaining data on-chip. However, modern topologies, such as the UNet, YOLO, and X3D models, utilise long skip connections, requiring significant on-chip storage and thus limiting the performance achieved by such system architectures. The paper addresses the above limitation by introducing weight and activation eviction mechanisms to off-chip memory along the computational pipeline, taking into account the available compute and memory resources. The proposed mechanism is incorporated into an existing t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#23569;&#26679;&#26412;&#26102;&#24207;&#21160;&#20316;&#23450;&#20301;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#21644;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#30340;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#36890;&#29992;&#29305;&#24449;&#21644;&#20943;&#36731;&#36807;&#25311;&#21512;&#39118;&#38505;</title><link>https://arxiv.org/abs/2403.18915</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#23569;&#26679;&#26412;&#26102;&#24207;&#21160;&#20316;&#23450;&#20301;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PLOT-TAL -- Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18915
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#23569;&#26679;&#26412;&#26102;&#24207;&#21160;&#20316;&#23450;&#20301;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#21644;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#30340;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#36890;&#29992;&#29305;&#24449;&#21644;&#20943;&#36731;&#36807;&#25311;&#21512;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#26102;&#24207;&#21160;&#20316;&#23450;&#20301;&#65288;TAL&#65289;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#20256;&#32479;&#21333;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#30001;&#20110;&#26080;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#39057;&#20013;&#36328;&#19981;&#21516;&#19978;&#19979;&#25991;&#36827;&#34892;&#27867;&#21270;&#32780;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#37492;&#20110;&#35270;&#39057;&#20013;&#25668;&#20687;&#26426;&#35270;&#35282;&#12289;&#32972;&#26223;&#21644;&#29289;&#20307;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#20102;&#26368;&#20248;&#20256;&#36755;&#30340;&#22810;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#36825;&#20010;&#35774;&#35745;&#20801;&#35768;&#27169;&#22411;&#20026;&#27599;&#20010;&#21160;&#20316;&#23398;&#20064;&#19968;&#32452;&#22810;&#26679;&#30340;&#25552;&#31034;&#65292;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#36890;&#29992;&#29305;&#24449;&#24182;&#20998;&#24067;&#34920;&#31034;&#20197;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#37319;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#25552;&#31034;&#19982;&#21160;&#20316;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#65292;&#20248;&#21270;&#20197;&#33719;&#24471;&#36866;&#24212;&#35270;&#39057;&#25968;&#25454;&#22810;&#38754;&#24615;&#30340;&#32508;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21160;&#20316;&#23450;&#20301;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18915v1 Announce Type: cross  Abstract: This paper introduces a novel approach to temporal action localization (TAL) in few-shot learning. Our work addresses the inherent limitations of conventional single-prompt learning methods that often lead to overfitting due to the inability to generalize across varying contexts in real-world videos. Recognizing the diversity of camera views, backgrounds, and objects in videos, we propose a multi-prompt learning framework enhanced with optimal transport. This design allows the model to learn a set of diverse prompts for each action, capturing general characteristics more effectively and distributing the representation to mitigate the risk of overfitting. Furthermore, by employing optimal transport theory, we efficiently align these prompts with action features, optimizing for a comprehensive representation that adapts to the multifaceted nature of video data. Our experiments demonstrate significant improvements in action localization a
&lt;/p&gt;</description></item><item><title>&#39640;&#20284;&#28982;&#21306;&#22495;&#23558;&#19981;&#20250;&#34987;&#29983;&#25104;&#22914;&#26524;&#23427;&#20204;&#21253;&#21547;&#26368;&#23567;&#27010;&#29575;&#36136;&#37327;&#65292;&#22522;&#20110;&#27492;&#35266;&#23519;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26412;&#22320;&#22266;&#26377;&#32500;&#24230;&#20272;&#35745;&#36827;&#34892;&#31163;&#32676;&#26816;&#27979;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.18910</link><description>&lt;p&gt;
&#23545;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#24726;&#35770;&#30340;&#20284;&#28982;&#20960;&#20309;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
A Geometric Explanation of the Likelihood OOD Detection Paradox
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18910
&lt;/p&gt;
&lt;p&gt;
&#39640;&#20284;&#28982;&#21306;&#22495;&#23558;&#19981;&#20250;&#34987;&#29983;&#25104;&#22914;&#26524;&#23427;&#20204;&#21253;&#21547;&#26368;&#23567;&#27010;&#29575;&#36136;&#37327;&#65292;&#22522;&#20110;&#27492;&#35266;&#23519;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26412;&#22320;&#22266;&#26377;&#32500;&#24230;&#20272;&#35745;&#36827;&#34892;&#31163;&#32676;&#26816;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#36890;&#24120;&#34920;&#29616;&#20986;&#20196;&#20154;&#22256;&#24785;&#30340;&#34892;&#20026;&#65306;&#24403;&#22312;&#30456;&#23545;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#20250;&#32473;&#26469;&#33258;&#26356;&#31616;&#21333;&#26469;&#28304;&#30340;&#31163;&#32676;&#25968;&#25454;&#36171;&#20104;&#26356;&#39640;&#30340;&#20284;&#28982;&#20540;&#12290;&#26356;&#20351;&#20154;&#24863;&#21040;&#31070;&#31192;&#30340;&#26159;&#65292;&#23613;&#31649;&#20855;&#26377;&#26356;&#39640;&#30340;&#20284;&#28982;&#20540;&#65292;&#20294;&#36825;&#20123;DGMs&#20174;&#26410;&#29983;&#25104;&#36807;&#31163;&#32676;&#26679;&#26412;&#12290;&#36825;&#20010;&#21452;&#31649;&#40784;&#19979;&#30340;&#24726;&#35770;&#23578;&#26410;&#24471;&#21040;&#26368;&#32456;&#35299;&#37322;&#65292;&#20351;&#24471;&#22522;&#20110;&#20284;&#28982;&#30340;&#31163;&#32676;&#26816;&#27979;&#19981;&#21487;&#38752;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#26159;&#65292;&#22914;&#26524;&#39640;&#20284;&#28982;&#21306;&#22495;&#20013;&#21253;&#21547;&#20102;&#26368;&#23567;&#27010;&#29575;&#36136;&#37327;&#65292;&#37027;&#20040;&#36825;&#20123;&#21306;&#22495;&#23558;&#19981;&#20250;&#34987;&#29983;&#25104;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#22260;&#32469;&#20302;&#32500;&#27969;&#24418;&#25968;&#25454;&#30340;&#22320;&#26041;&#21487;&#33021;&#20986;&#29616;&#22823;&#23494;&#24230;&#20294;&#20302;&#27010;&#29575;&#36136;&#37327;&#30340;&#30475;&#20284;&#30683;&#30462;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26412;&#22320;&#22266;&#26377;&#32500;&#24230;(LID)&#20272;&#35745;&#21487;&#20197;&#35782;&#21035;&#36825;&#31181;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;DGM&#33719;&#24471;&#30340;&#20284;&#28982;&#21644;LID&#20272;&#35745;&#30456;&#37197;&#23545;&#30340;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18910v1 Announce Type: cross  Abstract: Likelihood-based deep generative models (DGMs) commonly exhibit a puzzling behaviour: when trained on a relatively complex dataset, they assign higher likelihood values to out-of-distribution (OOD) data from simpler sources. Adding to the mystery, OOD samples are never generated by these DGMs despite having higher likelihoods. This two-pronged paradox has yet to be conclusively explained, making likelihood-based OOD detection unreliable. Our primary observation is that high-likelihood regions will not be generated if they contain minimal probability mass. We demonstrate how this seeming contradiction of large densities yet low probability mass can occur around data confined to low-dimensional manifolds. We also show that this scenario can be identified through local intrinsic dimension (LID) estimation, and propose a method for OOD detection which pairs the likelihoods and LID estimates obtained from a pre-trained DGM. Our method can b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEMA&#30340;&#26032;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#25105;&#25193;&#23637;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#27169;&#22359;&#21270;&#36866;&#37197;&#65292;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#26368;&#23567;&#36951;&#24536;&#65292;&#35299;&#20915;&#20808;&#21069;&#38024;&#23545;&#38745;&#24577;&#27169;&#22411;&#26550;&#26500;&#24773;&#20917;&#19979;&#23384;&#22312;&#30340;&#36807;&#22810;&#21442;&#25968;&#20998;&#37197;&#25110;&#36866;&#24212;&#24615;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.18886</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#36866;&#37197;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#25105;&#25193;&#23637;&#20197;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18886
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEMA&#30340;&#26032;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#25105;&#25193;&#23637;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#27169;&#22359;&#21270;&#36866;&#37197;&#65292;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#26368;&#23567;&#36951;&#24536;&#65292;&#35299;&#20915;&#20808;&#21069;&#38024;&#23545;&#38745;&#24577;&#27169;&#22411;&#26550;&#26500;&#24773;&#20917;&#19979;&#23384;&#22312;&#30340;&#36807;&#22810;&#21442;&#25968;&#20998;&#37197;&#25110;&#36866;&#24212;&#24615;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#20174;&#36830;&#32493;&#21040;&#36798;&#30340;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#36951;&#24536;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEMA&#30340;&#26032;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#25105;&#25193;&#23637;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#27169;&#22359;&#21270;&#36866;&#37197;&#65292;&#33258;&#21160;&#20915;&#23450;...&#65288;&#25688;&#35201;&#26410;&#23436;&#25972;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18886v1 Announce Type: new  Abstract: Continual learning aims to learn from a stream of continuously arriving data with minimum forgetting of previously learned knowledge. While previous works have explored the effectiveness of leveraging the generalizable knowledge from pre-trained models in continual learning, existing parameter-efficient fine-tuning approaches focus on the use of a predetermined or task-wise set of adapters or prompts. However, these approaches still suffer from forgetting due to task interference on jointly used parameters or restricted flexibility. The reliance on a static model architecture may lead to the allocation of excessive parameters that are not essential or, conversely, inadequate adaptation for downstream tasks, given that the scale and distribution of incoming data are unpredictable in continual learning. We propose Self-Expansion of pre-trained models with Modularized Adaptation (SEMA), a novel fine-tuning approach which automatically decid
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#20219;&#20309;&#29616;&#26377;&#32534;&#30721;-&#35299;&#30721;&#20998;&#21106;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#26465;&#20214;&#21270;&#27169;&#22411;&#39044;&#27979;&#19982;&#21487;&#23398;&#20064;&#30340;&#35299;&#21078;&#20808;&#39564;&#26469;&#26045;&#21152;&#35299;&#21078;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.18878</link><description>&lt;p&gt;
AIC-UNet: &#29992;&#20110;&#20581;&#22766;&#22810;&#22120;&#23448;&#20998;&#21106;&#30340;&#35299;&#21078;&#20449;&#24687;&#39537;&#21160;&#32423;&#32852;UNet
&lt;/p&gt;
&lt;p&gt;
AIC-UNet: Anatomy-informed Cascaded UNet for Robust Multi-Organ Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18878
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#20219;&#20309;&#29616;&#26377;&#32534;&#30721;-&#35299;&#30721;&#20998;&#21106;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#26465;&#20214;&#21270;&#27169;&#22411;&#39044;&#27979;&#19982;&#21487;&#23398;&#20064;&#30340;&#35299;&#21078;&#20808;&#39564;&#26469;&#26045;&#21152;&#35299;&#21078;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21152;&#20851;&#38190;&#35299;&#21078;&#29305;&#24449;&#65292;&#20363;&#22914;&#22120;&#23448;&#25968;&#37327;&#12289;&#24418;&#29366;&#12289;&#22823;&#23567;&#21644;&#30456;&#23545;&#20301;&#32622;&#65292;&#23545;&#20110;&#26500;&#24314;&#20581;&#22766;&#30340;&#22810;&#22120;&#23448;&#20998;&#21106;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#29616;&#26377;&#30340;&#32534;&#30721;-&#35299;&#30721;&#20998;&#21106;&#27169;&#22411;&#19978;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#35299;&#21078;&#20808;&#39564;&#65292;&#26469;&#23454;&#26045;&#35299;&#21078;&#32422;&#26463;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#33145;&#37096;&#25195;&#25551;&#26102;&#65292;&#32534;&#30721;&#22120;&#30340;&#19968;&#37096;&#20998;&#36890;&#36807;&#34180;&#26495;&#26679;&#26465;&#65288;TPS&#65289;&#32593;&#26684;&#25554;&#20540;&#23558;&#21487;&#23398;&#20064;&#30340;&#20808;&#39564;&#31354;&#38388;&#23545;&#20934;&#32473;&#23450;&#30340;&#36755;&#20837;&#25195;&#25551;&#12290;&#28982;&#21518;&#22312;&#35299;&#30721;&#38454;&#27573;&#25972;&#21512;&#21464;&#24418;&#30340;&#20808;&#39564;&#20197;&#25351;&#23548;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18878v1 Announce Type: cross  Abstract: Imposing key anatomical features, such as the number of organs, their shapes, sizes, and relative positions, is crucial for building a robust multi-organ segmentation model. Current attempts to incorporate anatomical features include broadening effective receptive fields (ERF) size with resource- and data-intensive modules such as self-attention or introducing organ-specific topology regularizers, which may not scale to multi-organ segmentation problems where inter-organ relation also plays a huge role. We introduce a new approach to impose anatomical constraints on any existing encoder-decoder segmentation model by conditioning model prediction with learnable anatomy prior. More specifically, given an abdominal scan, a part of the encoder spatially warps a learnable prior to align with the given input scan using thin plate spline (TPS) grid interpolation. The warped prior is then integrated during the decoding phase to guide the model
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#20316;&#20026;&#39069;&#22806;&#25104;&#20687;&#25216;&#26415;&#26469;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#21644;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#32467;&#21512;&#30340;&#26041;&#27861;&#25104;&#21151;&#21306;&#20998;&#20102;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#21644;&#38750;&#39118;&#38505;&#24739;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.18873</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#32593;&#33180;OCT&#25104;&#20687;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Predicting risk of cardiovascular disease using retinal OCT imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18873
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#20316;&#20026;&#39069;&#22806;&#25104;&#20687;&#25216;&#26415;&#26469;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#21644;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#32467;&#21512;&#30340;&#26041;&#27861;&#25104;&#21151;&#21306;&#20998;&#20102;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#21644;&#38750;&#39118;&#38505;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#20316;&#20026;&#19968;&#31181;&#39069;&#22806;&#25104;&#20687;&#25216;&#26415;&#26469;&#39044;&#27979;&#26410;&#26469;&#24515;&#34880;&#31649;&#30142;&#30149;&#65288;CVD&#65289;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#20102;&#39640;&#32500;3D OCT&#22270;&#20687;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#25429;&#25417;&#20102;OCT&#22270;&#20687;&#20013;&#19981;&#21516;&#35270;&#32593;&#33180;&#23618;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#38543;&#21518;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#29305;&#24449;&#21644;&#21442;&#19982;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20197;&#21450;&#20020;&#24202;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#20998;&#31867;&#22120;&#65292;&#20197;&#21306;&#20998;&#22788;&#20110;CVD&#20107;&#20214;&#39118;&#38505;&#65288;&#24515;&#26775;&#25110;&#20013;&#39118;&#65289;&#21644;&#38750;CVD&#30149;&#20363;&#30340;&#24739;&#32773;&#12290;&#25105;&#20204;&#30340;&#39044;&#27979;&#27169;&#22411;&#22522;&#20110;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#35780;&#20272;&#20854;&#33021;&#21147;&#26469;&#27491;&#30830;&#35782;&#21035;&#22312;&#22270;&#20687;&#33719;&#21462;&#21518;&#30340;5&#24180;&#20869;&#21487;&#33021;&#24739;&#26377;CVD&#20107;&#20214;&#65288;&#24515;&#26775;&#25110;&#20013;&#39118;&#65289;&#30340;&#20010;&#20307;&#12290;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;VAE&#29305;&#24449;&#36873;&#25321;&#21644;&#22810;&#27169;&#24577;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18873v1 Announce Type: cross  Abstract: We investigated the potential of optical coherence tomography (OCT) as an additional imaging technique to predict future cardiovascular disease (CVD). We utilised a self-supervised deep learning approach based on Variational Autoencoders (VAE) to learn low-dimensional representations of high-dimensional 3D OCT images and to capture distinct characteristics of different retinal layers within the OCT image. A Random Forest (RF) classifier was subsequently trained using the learned latent features and participant demographic and clinical data, to differentiate between patients at risk of CVD events (MI or stroke) and non-CVD cases. Our predictive model, trained on multimodal data, was assessed based on its ability to correctly identify individuals likely to suffer from a CVD event(MI or stroke), within a 5-year interval after image acquisition. Our self-supervised VAE feature selection and multimodal Random Forest classifier differentiate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;DeepView&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#20197;&#20943;&#23569;&#32534;&#30721;&#22120;&#27169;&#22411;&#23384;&#22312;&#30340;&#39118;&#38505;&#24182;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.18872</link><description>&lt;p&gt;
&#32534;&#30721;&#22120;LLMs&#39592;&#24178;&#30340;&#23450;&#21521;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Targeted Visualization of the Backbone of Encoder LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;DeepView&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#20197;&#20943;&#23569;&#32534;&#30721;&#22120;&#27169;&#22411;&#23384;&#22312;&#30340;&#39118;&#38505;&#24182;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#20854;&#20013;&#26368;&#24120;&#35265;&#30340;&#20004;&#31181;&#26550;&#26500;&#26159;&#32534;&#30721;&#22120;&#65292;&#22914;BERT&#65292;&#21644;&#35299;&#30721;&#22120;&#65292;&#22914;GPT&#27169;&#22411;&#12290;&#23613;&#31649;&#32534;&#30721;&#22120;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#23384;&#22312;&#19968;&#20123;&#39118;&#38505;&#65292;&#21253;&#25324;&#20559;&#35265;&#38382;&#39064;&#25110;&#26131;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#34920;&#26126;&#20102;&#38656;&#35201;&#21487;&#35299;&#37322;&#30340;AI&#26469;&#26816;&#27979;&#36825;&#20123;&#38382;&#39064;&#12290;&#34429;&#28982;&#30446;&#21069;&#23384;&#22312;&#21508;&#31181;&#20851;&#27880;&#39044;&#27979;&#21333;&#20010;&#36755;&#20837;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20294;&#22522;&#20110;&#38477;&#32500;&#30340;&#29992;&#20110;&#20998;&#31867;&#26816;&#26597;&#30340;&#20840;&#23616;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#20854;&#20182;&#39046;&#22495;&#20986;&#29616;&#24182;&#36229;&#36234;&#20165;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#20351;&#29992;t-SNE&#30340;&#26041;&#27861;&#65292;&#22312;NLP&#20013;&#24182;&#19981;&#21313;&#20998;&#24191;&#27867;&#20256;&#25773;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DeepView&#26041;&#27861;&#22312;NLP&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#22312;&#20108;&#32500;&#20013;&#21487;&#35270;&#21270;&#20915;&#31574;&#20989;&#25968;&#30340;&#19968;&#37096;&#20998;&#20197;&#21450;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18872v1 Announce Type: cross  Abstract: Attention based Large Language Models (LLMs) are the state-of-the-art in natural language processing (NLP). The two most common architectures are encoders such as BERT, and decoders like the GPT models. Despite the success of encoder models, on which we focus in this work, they also bear several risks, including issues with bias or their susceptibility for adversarial attacks, signifying the necessity for explainable AI to detect such issues. While there does exist various local explainability methods focusing on the prediction of single inputs, global methods based on dimensionality reduction for classification inspection, which have emerged in other domains and that go further than just using t-SNE in the embedding space, are not widely spread in NLP.   To reduce this gap, we investigate the application of DeepView, a method for visualizing a part of the decision function together with a data set in two dimensions, to the NLP domain.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#24341;&#23548;&#26041;&#27861;&#65292;&#23558;&#27668;&#33016;&#30340;&#20020;&#24202;&#30693;&#35782;&#34701;&#20837;XAI&#26041;&#27861;&#65292;&#20197;&#36807;&#28388;&#25481;&#33853;&#22312;&#27169;&#26495;&#20043;&#22806;&#30340;&#19981;&#30456;&#20851;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.18871</link><description>&lt;p&gt;
&#36890;&#36807;&#20020;&#24202;&#39046;&#22495;&#30693;&#35782;&#34893;&#29983;&#30340;&#27169;&#26495;&#25552;&#39640;&#20102;&#21518;&#32493;AI&#35299;&#37322;&#22312;&#27668;&#33016;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18871
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#24341;&#23548;&#26041;&#27861;&#65292;&#23558;&#27668;&#33016;&#30340;&#20020;&#24202;&#30693;&#35782;&#34701;&#20837;XAI&#26041;&#27861;&#65292;&#20197;&#36807;&#28388;&#25481;&#33853;&#22312;&#27169;&#26495;&#20043;&#22806;&#30340;&#19981;&#30456;&#20851;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#33016;&#26159;&#19968;&#31181;&#24613;&#24615;&#33016;&#37096;&#30142;&#30149;&#65292;&#30001;&#20110;&#32954;&#37096;&#21644;&#33016;&#22721;&#20043;&#38388;&#24322;&#24120;&#31215;&#32858;&#27668;&#20307;&#24341;&#36215;&#12290;&#20026;&#20102;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24120;&#35265;&#30340;&#19981;&#36879;&#26126;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#21246;&#30011;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27668;&#33016;&#35786;&#26029;&#30456;&#20851;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#37322;&#26377;&#26102;&#20250;&#20559;&#31163;&#23454;&#38469;&#30149;&#21464;&#21306;&#22495;&#65292;&#31361;&#26174;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#24341;&#23548;&#26041;&#27861;&#65292;&#23558;&#27668;&#33016;&#30340;&#20020;&#24202;&#30693;&#35782;&#34701;&#20837;XAI&#26041;&#27861;&#29983;&#25104;&#30340;&#27169;&#22411;&#35299;&#37322;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#36825;&#20123;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;&#21033;&#29992;&#25918;&#23556;&#31185;&#21307;&#29983;&#21019;&#24314;&#30340;&#19968;&#31181;&#30149;&#21464;&#21010;&#20998;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#20195;&#34920;&#21487;&#33021;&#21457;&#29983;&#27668;&#33016;&#21306;&#22495;&#30340;&#27169;&#26495;&#12290;&#28982;&#21518;&#23558;&#27492;&#27169;&#26495;&#21472;&#21152;&#21040;&#27169;&#22411;&#35299;&#37322;&#19978;&#65292;&#20197;&#36807;&#28388;&#25481;&#33853;&#22312;&#27169;&#26495;&#20043;&#22806;&#30340;&#19981;&#30456;&#20851;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18871v1 Announce Type: cross  Abstract: Background: Pneumothorax is an acute thoracic disease caused by abnormal air collection between the lungs and chest wall. To address the opaqueness often associated with deep learning (DL) models, explainable artificial intelligence (XAI) methods have been introduced to outline regions related to pneumothorax diagnoses made by DL models. However, these explanations sometimes diverge from actual lesion areas, highlighting the need for further improvement. Method: We propose a template-guided approach to incorporate the clinical knowledge of pneumothorax into model explanations generated by XAI methods, thereby enhancing the quality of these explanations. Utilizing one lesion delineation created by radiologists, our approach first generates a template that represents potential areas of pneumothorax occurrence. This template is then superimposed on model explanations to filter out extraneous explanations that fall outside the template's b
&lt;/p&gt;</description></item><item><title>SugarcaneNet2024&#26159;&#36890;&#36807;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24555;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18870</link><description>&lt;p&gt;
SugarcaneNet2024: LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SugarcaneNet2024: An Optimized Weighted Average Ensemble Approach of LASSO Regularized Pre-trained Models for Sugarcane Disease Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18870
&lt;/p&gt;
&lt;p&gt;
SugarcaneNet2024&#26159;&#36890;&#36807;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24555;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29976;&#34071;&#20316;&#20026;&#19990;&#30028;&#31958;&#19994;&#30340;&#20851;&#38190;&#20316;&#29289;&#65292;&#23481;&#26131;&#21463;&#22810;&#31181;&#30149;&#23475;&#20405;&#23475;&#65292;&#36825;&#20123;&#30149;&#23475;&#23545;&#20854;&#20135;&#37327;&#21644;&#36136;&#37327;&#37117;&#26377;&#37325;&#22823;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#26377;&#25928;&#31649;&#29702;&#21644;&#23454;&#26045;&#39044;&#38450;&#25514;&#26045;&#65292;&#24517;&#39035;&#21450;&#26102;&#20934;&#30830;&#22320;&#26816;&#27979;&#30149;&#23475;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SugarcaneNet2024&#30340;&#29420;&#29305;&#27169;&#22411;&#65292;&#36890;&#36807;&#21494;&#29255;&#22270;&#20687;&#22788;&#29702;&#65292;&#33021;&#22815;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#33258;&#21160;&#24555;&#36895;&#26816;&#27979;&#29976;&#34071;&#30149;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#27719;&#24635;&#20102;&#19971;&#20010;&#23450;&#21046;&#30340;&#12289;&#32463;&#36807;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;&#65292;&#29305;&#21035;&#26159;InceptionV3&#12289;InceptionResNetV2&#12289;DenseNet201&#12289;DenseNet169&#12289;Xception&#21644;ResNet152V2&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#24213;&#37096;&#28155;&#21152;&#20102;&#19977;&#23618;&#26356;&#23494;&#38598;&#23618;&#65292;&#20855;&#26377;0.0001&#30340;LASSO&#27491;&#21017;&#21270;&#65292;&#19977;&#20010;30%&#30340;dropout&#23618;&#21644;&#19977;&#20010;&#21551;&#29992;renorm&#30340;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18870v1 Announce Type: cross  Abstract: Sugarcane, a key crop for the world's sugar industry, is prone to several diseases that have a substantial negative influence on both its yield and quality. To effectively manage and implement preventative initiatives, diseases must be detected promptly and accurately. In this study, we present a unique model called sugarcaneNet2024 that outperforms previous methods for automatically and quickly detecting sugarcane disease through leaf image processing. Our proposed model consolidates an optimized weighted average ensemble of seven customized and LASSO-regularized pre-trained models, particularly InceptionV3, InceptionResNetV2, DenseNet201, DenseNet169, Xception, and ResNet152V2. Initially, we added three more dense layers with 0.0001 LASSO regularization, three 30% dropout layers, and three batch normalizations with renorm enabled at the bottom of these pre-trained models to improve the performance. The accuracy of sugarcane leaf dise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#23618;&#25193;&#25955;&#27169;&#22411;&#21644;&#20449;&#24687;&#20851;&#32852;&#26426;&#21046;&#35299;&#20915;&#22810;&#37325;&#24433;&#21709;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.18866</link><description>&lt;p&gt;
&#22810;&#23618;&#27425;&#22270;&#36125;&#21494;&#26031;&#20248;&#21270;&#29992;&#20110;&#22810;&#37325;&#24433;&#21709;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph Bayesian Optimization for Multiplex Influence Maximization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#23618;&#25193;&#25955;&#27169;&#22411;&#21644;&#20449;&#24687;&#20851;&#32852;&#26426;&#21046;&#35299;&#20915;&#22810;&#37325;&#24433;&#21709;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#21709;&#26368;&#22823;&#21270;(IM)&#26159;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#35782;&#21035;&#26377;&#38480;&#25968;&#37327;&#30340;&#21021;&#22987;&#26377;&#24433;&#21709;&#21147;&#30340;&#29992;&#25143;&#20197;&#26368;&#22823;&#21270;&#21463;&#24433;&#21709;&#29992;&#25143;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20010;&#20307;&#20449;&#24687;&#20256;&#25773;&#19978;&#65292;&#24573;&#30053;&#20102;&#22810;&#20010;&#20449;&#24687;&#39033;&#30340;&#21516;&#26102;&#21644;&#20114;&#21160;&#20256;&#25773;&#12290;&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;&#22810;&#23618;&#25193;&#25955;&#27169;&#22411;&#21644;&#20449;&#24687;&#20851;&#32852;&#26426;&#21046;&#23545;&#22810;&#37325;&#24433;&#21709;&#26368;&#22823;&#21270;(Multi-IM)&#38382;&#39064;&#36827;&#34892;&#20102;&#35268;&#21010;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#31181;&#23376;&#38598;&#26159;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#29992;&#25143;&#21644;&#20449;&#24687;&#20851;&#32852;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18866v1 Announce Type: cross  Abstract: Influence maximization (IM) is the problem of identifying a limited number of initial influential users within a social network to maximize the number of influenced users. However, previous research has mostly focused on individual information propagation, neglecting the simultaneous and interactive dissemination of multiple information items. In reality, when users encounter a piece of information, such as a smartphone product, they often associate it with related products in their minds, such as earphones or computers from the same brand. Additionally, information platforms frequently recommend related content to users, amplifying this cascading effect and leading to multiplex influence diffusion.   This paper first formulates the Multiplex Influence Maximization (Multi-IM) problem using multiplex diffusion models with an information association mechanism. In this problem, the seed set is a combination of influential users and inform
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#20110;&#22686;&#24378;&#22825;&#27668;&#21644;&#27668;&#20505;&#24314;&#27169;&#30340;&#21487;&#20449;&#24230;&#21644;&#23454;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#21518;&#39564;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#21644;&#20174;&#22836;&#35774;&#35745;&#30340;&#22266;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18864</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Interpretable Machine Learning for Weather and Climate Prediction: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18864
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#20110;&#22686;&#24378;&#22825;&#27668;&#21644;&#27668;&#20505;&#24314;&#27169;&#30340;&#21487;&#20449;&#24230;&#21644;&#23454;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#21518;&#39564;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#21644;&#20174;&#22836;&#35774;&#35745;&#30340;&#22266;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#22266;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#34920;&#29616;&#20026;&#8220;&#40657;&#21283;&#23376;&#8221;&#65292;&#38459;&#30861;&#20102;&#29992;&#25143;&#20449;&#20219;&#65292;&#20063;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#30340;&#27169;&#22411;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#22686;&#24378;&#22825;&#27668;&#21644;&#27668;&#20505;&#24314;&#27169;&#30340;&#21487;&#20449;&#24230;&#21644;&#23454;&#29992;&#24615;&#26041;&#38754;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#24212;&#29992;&#20110;&#27668;&#35937;&#39044;&#27979;&#30340;&#24403;&#21069;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#26041;&#27861;&#20998;&#31867;&#20026;&#20004;&#20010;&#20027;&#35201;&#33539;&#20363;&#65306;1&#65289;&#21518;&#39564;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#35299;&#37322;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#25200;&#21160;&#12289;&#22522;&#20110;&#21338;&#24328;&#35770;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#24402;&#22240;&#26041;&#27861;&#12290;2&#65289;&#20174;&#22836;&#24320;&#22987;&#35774;&#35745;&#22266;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26641;&#38598;&#25104;&#21644;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#31561;&#26550;&#26500;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#27599;&#31181;&#25216;&#26415;&#22914;&#20309;&#25552;&#20379;&#23545;&#39044;&#27979;&#27169;&#22411;&#20869;&#37096;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18864v1 Announce Type: cross  Abstract: Advanced machine learning models have recently achieved high predictive accuracy for weather and climate prediction. However, these complex models often lack inherent transparency and interpretability, acting as "black boxes" that impede user trust and hinder further model improvements. As such, interpretable machine learning techniques have become crucial in enhancing the credibility and utility of weather and climate modeling. In this survey, we review current interpretable machine learning approaches applied to meteorological predictions. We categorize methods into two major paradigms: 1) Post-hoc interpretability techniques that explain pre-trained models, such as perturbation-based, game theory based, and gradient-based attribution methods. 2) Designing inherently interpretable models from scratch using architectures like tree ensembles and explainable neural networks. We summarize how each technique provides insights into the pre
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#38142;&#25509;&#39044;&#27979;&#20316;&#20026;&#33258;&#21160;&#23637;&#31034;&#30456;&#20851;&#25991;&#29486;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#29983;&#25104;&#30340;&#35821;&#20041;&#34920;&#31034;&#22312;&#25512;&#33616;&#21644;&#25490;&#21517;&#20219;&#21153;&#19978;&#21462;&#24471;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18855</link><description>&lt;p&gt;
&#36890;&#36807;&#38142;&#25509;&#39044;&#27979;&#36827;&#34892;&#23450;&#21521;&#26631;&#20934;&#24341;&#25991;&#25512;&#33616;&#21644;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Directed Criteria Citation Recommendation and Ranking Through Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18855
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#38142;&#25509;&#39044;&#27979;&#20316;&#20026;&#33258;&#21160;&#23637;&#31034;&#30456;&#20851;&#25991;&#29486;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#29983;&#25104;&#30340;&#35821;&#20041;&#34920;&#31034;&#22312;&#25512;&#33616;&#21644;&#25490;&#21517;&#20219;&#21153;&#19978;&#21462;&#24471;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#38142;&#25509;&#39044;&#27979;&#20316;&#20026;&#33258;&#21160;&#23637;&#31034;&#19982;&#26032;&#25991;&#26723;&#22312;&#20027;&#39064;&#25110;&#19978;&#19979;&#25991;&#19978;&#21487;&#33021;&#30456;&#20851;&#30340;&#29616;&#26377;&#25991;&#29486;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#22270;&#23884;&#20837;&#26469;&#32534;&#30721;&#27599;&#20010;&#25991;&#26723;&#30340;&#21547;&#20041;&#65292;&#21576;&#29616;&#20026;&#24341;&#25991;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#35821;&#20041;&#34920;&#31034;&#33021;&#22815;&#22312;&#25512;&#33616;&#21644;&#25490;&#21517;&#20219;&#21153;&#20013;&#32988;&#36807;&#20854;&#20182;&#22522;&#20110;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#36825;&#20026;&#25506;&#32034;&#24341;&#25991;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#25972;&#20307;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#37027;&#20123;&#36825;&#20123;&#25991;&#26723;&#27491;&#30830;&#20114;&#30456;&#24341;&#29992;&#33267;&#20851;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#20197;&#20415;&#26368;&#23567;&#21270;&#20219;&#20309;&#19981;&#19968;&#33268;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18855v1 Announce Type: cross  Abstract: We explore link prediction as a proxy for automatically surfacing documents from existing literature that might be topically or contextually relevant to a new document. Our model uses transformer-based graph embeddings to encode the meaning of each document, presented as a node within a citation network. We show that the semantic representations that our model generates can outperform other content-based methods in recommendation and ranking tasks. This provides a holistic approach to exploring citation graphs in domains where it is critical that these documents properly cite each other, so as to minimize the possibility of any inconsistencies
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#20998;&#26512;&#20102;&#22312;&#39640;&#32824;&#29289;&#20307;&#22788;&#27979;&#24471;&#30340;&#19978;&#21319;&#38378;&#30005;&#19982;35&#20010;&#36739;&#22823;&#23610;&#24230;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#36739;&#22823;&#23610;&#24230;&#30340;&#21521;&#19978;&#36895;&#24230;&#12289;10&#31859;&#39640;&#24230;&#30340;&#39118;&#36895;&#21644;&#39118;&#21521;&#20197;&#21450;&#20113;&#29289;&#29702;&#21464;&#37327;&#23545;UL&#39118;&#38505;&#35780;&#20272;&#36129;&#29486;&#26368;&#22823;&#65292;&#36827;&#32780;&#39044;&#27979;&#20102;&#30740;&#31350;&#21306;&#22495;UL&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.18853</link><description>&lt;p&gt;
&#21033;&#29992;&#27668;&#35937;&#20877;&#20998;&#26512;&#25968;&#25454;&#23545;&#39640;&#32824;&#29289;&#20307;&#19978;&#21319;&#38378;&#30005;&#30340;&#26102;&#31354;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Spatio-seasonal risk assessment of upward lightning at tall objects using meteorological reanalysis data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18853
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#20998;&#26512;&#20102;&#22312;&#39640;&#32824;&#29289;&#20307;&#22788;&#27979;&#24471;&#30340;&#19978;&#21319;&#38378;&#30005;&#19982;35&#20010;&#36739;&#22823;&#23610;&#24230;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#36739;&#22823;&#23610;&#24230;&#30340;&#21521;&#19978;&#36895;&#24230;&#12289;10&#31859;&#39640;&#24230;&#30340;&#39118;&#36895;&#21644;&#39118;&#21521;&#20197;&#21450;&#20113;&#29289;&#29702;&#21464;&#37327;&#23545;UL&#39118;&#38505;&#35780;&#20272;&#36129;&#29486;&#26368;&#22823;&#65292;&#36827;&#32780;&#39044;&#27979;&#20102;&#30740;&#31350;&#21306;&#22495;UL&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39640;&#32824;&#29289;&#20307;&#22788;&#30340;&#38378;&#30005;&#24773;&#20917;&#65292;&#24182;&#35780;&#20272;&#20102;&#19996;&#38463;&#23572;&#21329;&#26031;&#23665;&#21450;&#20854;&#21608;&#36793;&#22320;&#21306;&#19978;&#21319;&#38378;&#30005;&#65288;UL&#65289;&#30340;&#39118;&#38505;&#12290;&#34429;&#28982;&#19981;&#24120;&#35265;&#65292;&#20294;UL&#23545;&#39118;&#21147;&#28065;&#36718;&#26426;&#23588;&#20854;&#26500;&#25104;&#23041;&#32961;&#65292;&#22240;&#20026;&#20854;&#38271;&#26102;&#38388;&#30005;&#27969;&#21487;&#33021;&#20250;&#36896;&#25104;&#37325;&#22823;&#30772;&#22351;&#12290;&#24403;&#21069;&#30340;&#39118;&#38505;&#35780;&#20272;&#26041;&#27861;&#24573;&#35270;&#20102;&#27668;&#35937;&#26465;&#20214;&#30340;&#24433;&#21709;&#65292;&#21487;&#33021;&#20302;&#20272;&#20102;UL&#30340;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992;&#38543;&#26426;&#26862;&#26519;&#65292;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20998;&#26512;&#20102;&#22312;&#22885;&#22320;&#21033;Gaisberg Tower&#27979;&#37327;&#30340;UL&#19982;35&#20010;&#36739;&#22823;&#23610;&#24230;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20854;&#20013;&#65292;&#36739;&#22823;&#23610;&#24230;&#30340;&#21521;&#19978;&#36895;&#24230;&#12289;10&#31859;&#39640;&#24230;&#30340;&#39118;&#36895;&#21644;&#39118;&#21521;&#20197;&#21450;&#20113;&#29289;&#29702;&#21464;&#37327;&#25552;&#20379;&#20102;&#26368;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#38543;&#26426;&#26862;&#26519;&#20197;1 km^2&#30340;&#20998;&#36776;&#29575;&#39044;&#27979;&#20102;&#30740;&#31350;&#21306;&#22495;&#30340;UL&#39118;&#38505;&#12290;&#24378;&#28872;&#30340;&#22320;&#34920;&#20020;&#36817;&#39118;&#19982;&#21463;&#39640;&#22320;&#24418;&#19978;&#21319;&#30340;&#20559;&#36716;&#30456;&#32467;&#21512;&#20250;&#22686;&#21152;UL&#39118;&#38505;&#12290;UL&#30340;&#26085;&#21464;&#21270;&#21608;&#26399;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18853v1 Announce Type: cross  Abstract: This study investigates lightning at tall objects and evaluates the risk of upward lightning (UL) over the eastern Alps and its surrounding areas. While uncommon, UL poses a threat, especially to wind turbines, as the long-duration current of UL can cause significant damage. Current risk assessment methods overlook the impact of meteorological conditions, potentially underestimating UL risks. Therefore, this study employs random forests, a machine learning technique, to analyze the relationship between UL measured at Gaisberg Tower (Austria) and $35$ larger-scale meteorological variables. Of these, the larger-scale upward velocity, wind speed and direction at 10 meters and cloud physics variables contribute most information. The random forests predict the risk of UL across the study area at a 1 km$^2$ resolution. Strong near-surface winds combined with upward deflection by elevated terrain increase UL risk. The diurnal cycle of the UL 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;&#20013;&#30340;&#21069;&#23548;&#30896;&#25758;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#25913;&#36827;Hadamard&#21464;&#25442;&#21644;&#35774;&#35745;&#22359;MHT&#23618;&#26469;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18846</link><description>&lt;p&gt;
&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
The Blind Normalized Stein Variational Gradient Descent-Based Detection for Intelligent Massive Random Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;&#20013;&#30340;&#21069;&#23548;&#30896;&#25758;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#25913;&#36827;Hadamard&#21464;&#25442;&#21644;&#35774;&#35745;&#22359;MHT&#23618;&#26469;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#39640;&#25928;&#30340;&#21069;&#23548;&#26816;&#27979;&#31639;&#27861;&#20173;&#28982;&#26159;&#35299;&#20915;&#23454;&#38469;&#36890;&#20449;&#22330;&#26223;&#20013;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;(RA)&#20013;&#21069;&#23548;&#30896;&#25758;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;(MLE)&#27169;&#22411;&#30340;&#26089;&#26399;&#21069;&#23548;&#26816;&#27979;&#26041;&#26696;&#65292;&#22312;&#25480;&#20104;&#24335;RA&#27969;&#31243;&#30340;&#31532;&#19968;&#27493;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;(SVGD)&#30340;&#26816;&#27979;&#22120;&#65292;&#20197;&#33719;&#24471;MLE&#27169;&#22411;&#30340;&#36817;&#20284;&#35299;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#25506;&#32034;Hadamard&#21464;&#25442;&#21644;&#23567;&#27874;&#21464;&#25442;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25913;&#36827;Hadamard&#21464;&#25442;(MHT)&#65292;&#20351;&#29992;&#20108;&#38454;&#23548;&#25968;&#28388;&#27874;&#22120;&#23558;&#39640;&#39057;&#20998;&#31163;&#20986;&#37325;&#35201;&#37096;&#20998;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#28040;&#38500;SVGD&#26816;&#27979;&#22120;&#20013;&#30340;&#22122;&#22768;&#24182;&#20943;&#36731;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#22522;&#20110;MHT&#30340;&#22359;MHT&#23618;&#65292;&#35813;&#23618;&#22522;&#20110;MHT&#12289;&#32553;&#25918;&#23618;&#12289;&#36719;&#38408;&#20540;&#23618;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18846v1 Announce Type: cross  Abstract: The lack of an efficient preamble detection algorithm remains a challenge for solving preamble collision problems in intelligent massive random access (RA) in practical communication scenarios. To solve this problem, we present a novel early preamble detection scheme based on a maximum likelihood estimation (MLE) model at the first step of the grant-based RA procedure. A novel blind normalized Stein variational gradient descent (SVGD)-based detector is proposed to obtain an approximate solution to the MLE model. First, by exploring the relationship between the Hadamard transform and wavelet transform, a new modified Hadamard transform (MHT) is developed to separate high-frequencies from important components using the second-order derivative filter. Next, to eliminate noise and mitigate the vanishing gradients problem in the SVGD-based detectors, the block MHT layer is designed based on the MHT, scaling layer, soft-thresholding layer, i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;JEP-KD&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#32593;&#32476;&#22312;&#23884;&#20837;&#23618;&#20869;&#22686;&#24378;&#35270;&#39057;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#38899;&#39057;&#29305;&#24449;&#65292;&#36880;&#27493;&#20943;&#23569;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#19982;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.18843</link><description>&lt;p&gt;
JEP-KD&#65306;&#22522;&#20110;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#30340;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
JEP-KD: Joint-Embedding Predictive Architecture Based Knowledge Distillation for Visual Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18843
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;JEP-KD&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#32593;&#32476;&#22312;&#23884;&#20837;&#23618;&#20869;&#22686;&#24378;&#35270;&#39057;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#38899;&#39057;&#29305;&#24449;&#65292;&#36880;&#27493;&#20943;&#23569;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#19982;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65288;VSR&#65289;&#20219;&#21153;&#36890;&#24120;&#34987;&#35748;&#20026;&#20855;&#26377;&#27604;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26356;&#20302;&#30340;&#29702;&#35770;&#24615;&#33021;&#19978;&#38480;&#65292;&#36825;&#26159;&#30001;&#20110;&#36890;&#36807;&#35270;&#35273;&#26041;&#24335;&#20256;&#36798;&#35821;&#20041;&#20449;&#24687;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;JEP-KD&#30340;Joint-Embedding Predictive Architecture&#65288;JEPA&#65289;&#65292;&#26088;&#22312;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#38899;&#39057;&#29305;&#24449;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;JEP-KD&#30340;&#26680;&#24515;&#22312;&#20110;&#22312;&#23884;&#20837;&#23618;&#20869;&#21253;&#21547;&#19968;&#20010;&#29983;&#25104;&#32593;&#32476;&#65292;&#22686;&#24378;&#20102;&#35270;&#39057;&#32534;&#30721;&#22120;&#23545;&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#20854;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#32534;&#30721;&#22120;&#30340;&#38899;&#39057;&#29305;&#24449;&#26356;&#21152;&#25509;&#36817;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#36880;&#28176;&#20943;&#23569;VSR&#21644;ASR&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#36824;&#24314;&#31435;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#12289;&#22810;&#38454;&#27573;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;JEP-KD&#26694;&#26550;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18843v1 Announce Type: cross  Abstract: Visual Speech Recognition (VSR) tasks are generally recognized to have a lower theoretical performance ceiling than Automatic Speech Recognition (ASR), owing to the inherent limitations of conveying semantic information visually. To mitigate this challenge, this paper introduces an advanced knowledge distillation approach using a Joint-Embedding Predictive Architecture (JEPA), named JEP-KD, designed to more effectively utilize audio features during model training. Central to JEP-KD is the inclusion of a generative network within the embedding layer, which enhances the video encoder's capacity for semantic feature extraction and brings it into closer alignment with the audio features from a pre-trained ASR model's encoder. This approach aims to progressively reduce the performance gap between VSR and ASR. Moreover, a comprehensive multimodal, multistage training regimen for the JEP-KD framework is established, bolstering the robustness 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#23376;&#22330;&#35770;&#20013;&#39640;&#38454;&#36153;&#26364;&#22270;&#30340;&#35745;&#31639;&#22270;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#32455;&#25104;&#24352;&#37327;&#25805;&#20316;&#30340;&#20998;&#24418;&#32467;&#26500;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#20887;&#20313;&#65292;&#38598;&#25104;&#20102;Taylor-mode&#33258;&#21160;&#24494;&#20998;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#36153;&#26364;&#22270;&#32534;&#35793;&#22120;&#20197;&#20248;&#21270;&#35745;&#31639;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.18840</link><description>&lt;p&gt;
&#36153;&#26364;&#22270;&#20316;&#20026;&#35745;&#31639;&#22270;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Feynman Diagrams as Computational Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18840
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#23376;&#22330;&#35770;&#20013;&#39640;&#38454;&#36153;&#26364;&#22270;&#30340;&#35745;&#31639;&#22270;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#32455;&#25104;&#24352;&#37327;&#25805;&#20316;&#30340;&#20998;&#24418;&#32467;&#26500;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#20887;&#20313;&#65292;&#38598;&#25104;&#20102;Taylor-mode&#33258;&#21160;&#24494;&#20998;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#36153;&#26364;&#22270;&#32534;&#35793;&#22120;&#20197;&#20248;&#21270;&#35745;&#31639;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#31354;&#38388;&#12289;&#26102;&#38388;&#12289;&#21160;&#37327;&#21644;&#39057;&#29575;&#39046;&#22495;&#30340;&#39640;&#38454;&#36153;&#26364;&#22270;&#30340;&#35745;&#31639;&#22270;&#34920;&#31034;&#65292;&#36866;&#29992;&#20110;&#37327;&#23376;&#22330;&#35770;&#65288;QFT&#65289;&#12290;&#21033;&#29992;&#25140;&#26862;-&#26045;&#28201;&#26684;&#26041;&#31243;&#21644;&#26641;&#22270;&#26041;&#31243;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#22270;&#32452;&#32455;&#25104;&#24352;&#37327;&#25805;&#20316;&#30340;&#20998;&#24418;&#32467;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#20887;&#20313;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#31616;&#21270;&#20102;&#22797;&#26434;&#22270;&#30340;&#35780;&#20272;&#65292;&#36824;&#20419;&#36827;&#20102;&#22330;&#35770;&#37325;&#25972;&#21270;&#26041;&#26696;&#30340;&#39640;&#25928;&#23454;&#26045;&#65292;&#23545;&#22686;&#24378;&#24494;&#25200;QFT&#35745;&#31639;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#19968;&#36827;&#23637;&#30340;&#20851;&#38190;&#22312;&#20110;&#38598;&#25104;&#20102;Taylor-mode&#33258;&#21160;&#24494;&#20998;&#65292;&#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#21253;&#20013;&#29992;&#20110;&#22312;&#35745;&#31639;&#22270;&#19978;&#39640;&#25928;&#35745;&#31639;&#39640;&#38454;&#23548;&#25968;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#25805;&#20316;&#21270;&#36825;&#20123;&#27010;&#24565;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36153;&#26364;&#22270;&#32534;&#35793;&#22120;&#65292;&#20248;&#21270;&#20102;&#21508;&#31181;&#35745;&#31639;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18840v1 Announce Type: cross  Abstract: We propose a computational graph representation of high-order Feynman diagrams in Quantum Field Theory (QFT), applicable to any combination of spatial, temporal, momentum, and frequency domains. Utilizing the Dyson-Schwinger and parquet equations, our approach effectively organizes these diagrams into a fractal structure of tensor operations, significantly reducing computational redundancy. This approach not only streamlines the evaluation of complex diagrams but also facilitates an efficient implementation of the field-theoretic renormalization scheme, crucial for enhancing perturbative QFT calculations. Key to this advancement is the integration of Taylor-mode automatic differentiation, a key technique employed in machine learning packages to compute higher-order derivatives efficiently on computational graphs. To operationalize these concepts, we develop a Feynman diagram compiler that optimizes diagrams for various computational pl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#37329;&#34701;&#24066;&#22330;&#30340;Wyckoff&#38454;&#27573;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#32047;&#31215;&#27169;&#24335;&#21644;&#20132;&#26131;&#33539;&#22260;&#31561;&#38454;&#27573;&#30340;&#37325;&#35201;&#24615;&#65292;&#25581;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LSTM&#27169;&#22411;&#20998;&#26512;&#24066;&#22330;&#25968;&#25454;&#20197;&#39044;&#27979;&#20215;&#26684;&#36208;&#21183;&#21644;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.18839</link><description>&lt;p&gt;
&#36135;&#24065;&#20132;&#26131;&#20013;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#24335;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Long Short-Term Memory Pattern Recognition in Currency Trading
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#37329;&#34701;&#24066;&#22330;&#30340;Wyckoff&#38454;&#27573;&#26694;&#26550;&#65292;&#25506;&#35752;&#20102;&#32047;&#31215;&#27169;&#24335;&#21644;&#20132;&#26131;&#33539;&#22260;&#31561;&#38454;&#27573;&#30340;&#37325;&#35201;&#24615;&#65292;&#25581;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LSTM&#27169;&#22411;&#20998;&#26512;&#24066;&#22330;&#25968;&#25454;&#20197;&#39044;&#27979;&#20215;&#26684;&#36208;&#21183;&#21644;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#36890;&#36807;Richard D. Wyckoff&#22312;20&#19990;&#32426;&#26089;&#26399;&#35774;&#35745;&#30340;Wyckoff&#38454;&#27573;&#26694;&#26550;&#30340;&#37329;&#34701;&#24066;&#22330;&#12290;&#37325;&#28857;&#20851;&#27880;Wyckoff&#26694;&#26550;&#20013;&#30340;&#32047;&#31215;&#27169;&#24335;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#20132;&#26131;&#33539;&#22260;&#21644;&#27425;&#32423;&#27979;&#35797;&#38454;&#27573;&#65292;&#38416;&#26126;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#24066;&#22330;&#21160;&#24577;&#21644;&#35782;&#21035;&#28508;&#22312;&#20132;&#26131;&#26426;&#20250;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21078;&#26512;&#36825;&#20123;&#38454;&#27573;&#30340;&#22797;&#26434;&#24615;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#24066;&#22330;&#32467;&#26500;&#36890;&#36807;&#21019;&#36896;&#27969;&#21160;&#24615;&#65292;&#20026;&#20132;&#26131;&#32773;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#39044;&#27979;&#20215;&#26684;&#36208;&#21183;&#21644;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#26377;&#25928;&#26816;&#27979;&#21644;&#20998;&#26512;Wyckoff&#27169;&#24335;&#38656;&#35201;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#24066;&#22330;&#25968;&#25454;&#30340;&#24378;&#22823;&#35745;&#31639;&#27169;&#22411;&#65292;&#20854;&#20013;&#31354;&#38388;&#25968;&#25454;&#26368;&#22909;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#36827;&#34892;&#20998;&#26512;&#65292;&#32780;&#26102;&#38388;&#25968;&#25454;&#21017;&#36890;&#36807;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18839v1 Announce Type: cross  Abstract: This study delves into the analysis of financial markets through the lens of Wyckoff Phases, a framework devised by Richard D. Wyckoff in the early 20th century. Focusing on the accumulation pattern within the Wyckoff framework, the research explores the phases of trading range and secondary test, elucidating their significance in understanding market dynamics and identifying potential trading opportunities. By dissecting the intricacies of these phases, the study sheds light on the creation of liquidity through market structure, offering insights into how traders can leverage this knowledge to anticipate price movements and make informed decisions. The effective detection and analysis of Wyckoff patterns necessitate robust computational models capable of processing complex market data, with spatial data best analyzed using Convolutional Neural Networks (CNNs) and temporal data through Long Short-Term Memory (LSTM) models. The creation
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#26080;&#21047;&#30452;&#27969;&#30005;&#26426;&#20013;&#24320;&#21457;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#30005;&#27969;&#32441;&#27874;&#36827;&#34892;&#36895;&#24230;&#21644;&#20301;&#32622;&#20272;&#35745;&#65292;&#36890;&#36807;&#26816;&#27979;&#33033;&#20914;&#26469;&#20272;&#35745;&#36895;&#24230;&#24182;&#35745;&#25968;&#26469;&#20272;&#35745;&#20301;&#32622;&#65292;&#33021;&#22815;&#26816;&#27979;&#39740;&#33033;&#20914;&#21644;&#20002;&#24323;&#34394;&#20551;&#33033;&#20914;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18833</link><description>&lt;p&gt;
&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#26080;&#21047;&#30452;&#27969;&#30005;&#26426;&#20013;&#26080;&#20256;&#24863;&#22120;&#20272;&#35745;&#36895;&#24230;&#21644;&#20301;&#32622;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Method for Sensorless Estimation of the Speed and Position in Brushed DC Motors Using Support Vector Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18833
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#26080;&#21047;&#30452;&#27969;&#30005;&#26426;&#20013;&#24320;&#21457;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#30005;&#27969;&#32441;&#27874;&#36827;&#34892;&#36895;&#24230;&#21644;&#20301;&#32622;&#20272;&#35745;&#65292;&#36890;&#36807;&#26816;&#27979;&#33033;&#20914;&#26469;&#20272;&#35745;&#36895;&#24230;&#24182;&#35745;&#25968;&#26469;&#20272;&#35745;&#20301;&#32622;&#65292;&#33021;&#22815;&#26816;&#27979;&#39740;&#33033;&#20914;&#21644;&#20002;&#24323;&#34394;&#20551;&#33033;&#20914;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#26469;&#35828;&#65292;&#20102;&#35299;&#30005;&#26426;&#30340;&#36895;&#24230;&#21644;&#20301;&#32622;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#23558;&#26426;&#26800;&#20256;&#24863;&#22120;&#32806;&#21512;&#21040;&#30005;&#26426;&#36724;&#19978;&#25110;&#20351;&#29992;&#26080;&#20256;&#24863;&#22120;&#25216;&#26415;&#26469;&#23454;&#29616;&#12290;&#26080;&#21047;&#30452;&#27969;&#30005;&#26426;&#20013;&#30340;&#26080;&#20256;&#24863;&#22120;&#25216;&#26415;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;1&#65289;&#22522;&#20110;&#21160;&#24577;&#26080;&#21047;&#30452;&#27969;&#30005;&#26426;&#27169;&#22411;&#30340;&#25216;&#26415;&#21644;2&#65289;&#22522;&#20110;&#30005;&#27969;&#32441;&#27874;&#20998;&#37327;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30005;&#27969;&#32441;&#27874;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#26080;&#21047;&#30452;&#27969;&#30005;&#26426;&#20013;&#36827;&#34892;&#36895;&#24230;&#21644;&#20301;&#32622;&#20272;&#35745;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20165;&#27979;&#37327;&#30005;&#27969;&#24182;&#26816;&#27979;&#35813;&#20449;&#21495;&#20013;&#30340;&#33033;&#20914;&#12290;&#36890;&#36807;&#20351;&#29992;&#26816;&#27979;&#21040;&#30340;&#33033;&#20914;&#20043;&#38388;&#30340;&#21453;&#36317;&#31163;&#26469;&#20272;&#35745;&#30005;&#26426;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#25152;&#26377;&#26816;&#27979;&#21040;&#30340;&#33033;&#20914;&#26469;&#20272;&#35745;&#20301;&#32622;&#12290;&#19982;&#20854;&#20182;&#26080;&#20256;&#24863;&#22120;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#28857;&#26159;&#33021;&#22815;&#26816;&#27979;&#39740;&#33033;&#20914;&#24182;&#20002;&#24323;&#34394;&#20551;&#33033;&#20914;&#12290;&#22312;&#20004;&#20010;fra&#36827;&#34892;&#20102;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18833v1 Announce Type: cross  Abstract: Currently, for many applications, it is necessary to know the speed and position of motors. This can be achieved using mechanical sensors coupled to the motor shaft or using sensorless techniques. The sensorless techniques in brushed dc motors can be classified into two types: 1) techniques based on the dynamic brushed dc motor model and 2) techniques based on the ripple component of the current. This paper presents a new method, based on the ripple component, for speed and position estimation in brushed dc motors, using support vector machines. The proposed method only measures the current and detects the pulses in this signal. The motor speed is estimated by using the inverse distance between the detected pulses, and the position is estimated by counting all detected pulses. The ability to detect ghost pulses and to discard false pulses is the main advantage of this method over other sensorless methods. The performed tests on two fra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#27169;&#22359;&#37325;&#26500;&#20026;&#24433;&#23376;&#29983;&#25104;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#35748;&#30693;&#26550;&#26500;&#19982;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#32541;&#36830;&#25509;</title><link>https://arxiv.org/abs/2403.18827</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;&#32593;&#32476;&#19982;&#35748;&#30693;&#20849;&#21516;&#27169;&#22411;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Bridging Generative Networks with the Common Model of Cognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18827
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#27169;&#22359;&#37325;&#26500;&#20026;&#24433;&#23376;&#29983;&#25104;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#35748;&#30693;&#26550;&#26500;&#19982;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#32541;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#35748;&#30693;&#20849;&#21516;&#27169;&#22411;&#35843;&#25972;&#20026;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#22823;&#22411;&#29983;&#25104;&#32593;&#32476;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#20849;&#21516;&#27169;&#22411;&#20013;&#30340;&#27169;&#22359;&#37325;&#26500;&#20026;&#21608;&#36793;&#30340;&#24433;&#23376;&#29983;&#25104;&#31995;&#32479;&#65292;&#36825;&#20123;&#24433;&#23376;&#29983;&#25104;&#31995;&#32479;&#36741;&#21161;&#22788;&#29702;&#39640;&#23618;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#35748;&#30693;&#26550;&#26500;&#19982;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#32541;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18827v1 Announce Type: new  Abstract: This article presents a theoretical framework for adapting the Common Model of Cognition to large generative network models within the field of artificial intelligence. This can be accomplished by restructuring modules within the Common Model into shadow production systems that are peripheral to a central production system, which handles higher-level reasoning based on the shadow productions' output. Implementing this novel structure within the Common Model allows for a seamless connection between cognitive architectures and generative neural networks.
&lt;/p&gt;</description></item><item><title>LSTM&#32593;&#32476;&#22312;&#37329;&#34701;&#25968;&#25454;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#22810;&#20010;&#29305;&#24449;&#25552;&#21319;&#27169;&#22411;&#25429;&#25417;&#22797;&#26434;&#27169;&#24335;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#20851;&#38190;&#23646;&#24615;&#25581;&#31034;&#24494;&#22937;&#24046;&#24322;&#65292;&#37319;&#29992;25&#22825;&#26102;&#38388;&#27493;&#38271;&#20869;&#30340;&#36755;&#20837;&#32467;&#26500;&#25429;&#25417;&#26102;&#38388;&#19978;&#30340;&#22797;&#26434;&#24615;</title><link>https://arxiv.org/abs/2403.18822</link><description>&lt;p&gt;
&#25552;&#21319;&#37329;&#34701;&#25968;&#25454;&#21487;&#35270;&#21270;&#20197;&#36741;&#21161;&#25237;&#36164;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Enhancing Financial Data Visualization for Investment Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18822
&lt;/p&gt;
&lt;p&gt;
LSTM&#32593;&#32476;&#22312;&#37329;&#34701;&#25968;&#25454;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#22810;&#20010;&#29305;&#24449;&#25552;&#21319;&#27169;&#22411;&#25429;&#25417;&#22797;&#26434;&#27169;&#24335;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#20851;&#38190;&#23646;&#24615;&#25581;&#31034;&#24494;&#22937;&#24046;&#24322;&#65292;&#37319;&#29992;25&#22825;&#26102;&#38388;&#27493;&#38271;&#20869;&#30340;&#36755;&#20837;&#32467;&#26500;&#25429;&#25417;&#26102;&#38388;&#19978;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#39044;&#27979;&#32929;&#31080;&#21160;&#24577;&#30340;&#28508;&#21147;&#65292;&#30528;&#37325;&#20110;&#35782;&#21035;&#24494;&#22937;&#30340;&#28072;&#36300;&#27169;&#24335;&#12290;&#21033;&#29992;&#32445;&#32422;&#35777;&#21048;&#20132;&#26131;&#25152;&#65288;NYSE&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#34701;&#21512;&#20102;&#22810;&#31181;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;LSTM&#22312;&#25429;&#25417;&#22797;&#26434;&#27169;&#24335;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#20851;&#38190;&#23646;&#24615;&#65288;&#22914;&#24320;&#30424;&#20215;&#12289;&#25910;&#30424;&#20215;&#12289;&#26368;&#20302;&#20215;&#21644;&#26368;&#39640;&#20215;&#65289;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;&#23545;&#20840;&#38754;&#24066;&#22330;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#35813;&#30740;&#31350;&#31934;&#24515;&#35774;&#35745;&#30340;LSTM&#36755;&#20837;&#32467;&#26500;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#24050;&#24314;&#31435;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;&#32467;&#21512;&#20102;25&#22825;&#26102;&#38388;&#27493;&#38271;&#20869;&#30340;&#20215;&#26684;&#21644;&#20132;&#26131;&#37327;&#23646;&#24615;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#26102;&#38388;&#19978;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18822v1 Announce Type: cross  Abstract: Navigating the intricate landscape of financial markets requires adept forecasting of stock price movements. This paper delves into the potential of Long Short-Term Memory (LSTM) networks for predicting stock dynamics, with a focus on discerning nuanced rise and fall patterns. Leveraging a dataset from the New York Stock Exchange (NYSE), the study incorporates multiple features to enhance LSTM's capacity in capturing complex patterns. Visualization of key attributes, such as opening, closing, low, and high prices, aids in unraveling subtle distinctions crucial for comprehensive market understanding. The meticulously crafted LSTM input structure, inspired by established guidelines, incorporates both price and volume attributes over a 25-day time step, enabling the model to capture temporal intricacies. A comprehensive methodology, including hyperparameter tuning with Grid Search, Early Stopping, and Callback mechanisms, leads to a remar
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20026;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#19988;&#21463;ViT&#23884;&#20837;&#26465;&#20214;&#32422;&#26463;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18807</link><description>&lt;p&gt;
ECoDepth: &#26377;&#25928;&#35843;&#25972;&#25193;&#25955;&#27169;&#22411;&#20197;&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20026;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#19988;&#21463;ViT&#23884;&#20837;&#26465;&#20214;&#32422;&#26463;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#20047;&#35270;&#24046;&#32447;&#32034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#65288;SIDE&#65289;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#22270;&#20687;&#20013;&#30340;&#38452;&#24433;&#21644;&#19978;&#19979;&#25991;&#32447;&#32034;&#12290;&#25105;&#20204;&#20174;&#24050;&#26377;&#30740;&#31350;&#30340;&#21551;&#21457;&#20013;&#25506;&#35752;&#20351;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20197;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#30340;SIDE&#27169;&#22411;&#65292;&#20854;&#21463;&#21040;ViT&#23884;&#20837;&#30340;&#26465;&#20214;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18807v1 Announce Type: cross  Abstract: In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embedding
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558; FPGA &#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20197;&#25552;&#39640;&#26080;&#20154;&#26426;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.18703</link><description>&lt;p&gt;
&#22522;&#20110; FPGA &#30340;&#26080;&#20154;&#26426;&#31070;&#32463;&#25512;&#21147;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fpga-Based Neural Thrust Controller for UAVs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558; FPGA &#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20197;&#25552;&#39640;&#26080;&#20154;&#26426;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#65288;UAVs&#65289;&#30340;&#20986;&#29616;&#36890;&#36807;&#25552;&#20379;&#22810;&#21151;&#33021;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#21487;&#35775;&#38382;&#24179;&#21488;&#65292;&#25913;&#21892;&#20102;&#21508;&#31181;&#39046;&#22495;&#23454;&#26045;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#23436;&#25104;&#26356;&#24191;&#27867;&#30340;&#20219;&#21153;&#65292;&#36234;&#26469;&#36234;&#38656;&#35201;&#22686;&#24378;&#26426;&#36733;&#35745;&#31639;&#24615;&#33021;&#20197;&#24212;&#23545;&#19981;&#26029;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#29615;&#22659;&#26465;&#20214;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#30475;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#19982;&#24378;&#21270;&#23398;&#20064;&#19968;&#36215;&#65292;&#20197;&#25552;&#39640;&#26080;&#20154;&#26426;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#12290;&#28982;&#32780;&#65292;DNNs &#30340;&#35745;&#31639;&#35201;&#27714;&#23545;&#35768;&#22810; UAVs &#19978;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#26500;&#25104;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#29616;&#22330;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGAs&#65289;&#20316;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#12289;&#39640;&#24615;&#33021;&#12289;&#33021;&#37327;&#21644;&#26102;&#38388;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37197;&#22791; Artix-7 FPGA &#30340;&#26032;&#22411;&#30828;&#20214;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18703v1 Announce Type: cross  Abstract: The advent of unmanned aerial vehicles (UAVs) has improved a variety of fields by providing a versatile, cost-effective and accessible platform for implementing state-of-the-art algorithms. To accomplish a broader range of tasks, there is a growing need for enhanced on-board computing to cope with increasing complexity and dynamic environmental conditions. Recent advances have seen the application of Deep Neural Networks (DNNs), particularly in combination with Reinforcement Learning (RL), to improve the adaptability and performance of UAVs, especially in unknown environments. However, the computational requirements of DNNs pose a challenge to the limited computing resources available on many UAVs. This work explores the use of Field Programmable Gate Arrays (FPGAs) as a viable solution to this challenge, offering flexibility, high performance, energy and time efficiency. We propose a novel hardware board equipped with an Artix-7 FPGA 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;ov-freeze&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.18159</link><description>&lt;p&gt;
&#22114;&#65281;&#25105;&#20204;&#20919;&#20923;&#65306;&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18159
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;ov-freeze&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#20998;&#21035;&#22312;NLP&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#36895;&#24230;&#24930;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#39640;&#65292;&#36825;&#20351;&#24471;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#23427;&#20204;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#37327;&#21270;&#24863;&#30693;&#24494;&#35843;&#25216;&#26415;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD-QAT&#65289;&#26469;&#25913;&#21892;&#20351;&#29992;&#24120;&#29992;&#25968;&#25454;&#38598;&#25913;&#36827;4&#20301;&#37325;&#37327;&#37327;&#21270;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#23454;&#29616;&#27969;&#34892;&#30340;&#35821;&#35328;&#20351;&#29992;&#26696;&#20363;&#65292;&#22312;&#35774;&#22791;&#32842;&#22825;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#31181;&#24494;&#35843;&#33539;&#24335;&#65292;&#20316;&#20026;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#20256;&#25773;&#65292;&#25552;&#20379;&#23545;KD-QAT&#31283;&#23450;&#24615;&#30340;&#27934;&#23519;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;KD-QAT&#30340;&#26041;&#27861;&#23545;&#20302;&#20301;&#37327;&#21270;&#35823;&#24046;&#30340;&#33030;&#24369;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ov-freeze&#65292;&#19968;&#31181;&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#31616;&#21333;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18159v1 Announce Type: cross  Abstract: Large generative models, such as large language models (LLMs) and diffusion models have as revolutionized the fields of NLP and computer vision respectively. However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices. In this study, we propose a light-weight quantization aware fine tuning technique using knowledge distillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs using commonly available datasets to realize a popular language use case, on device chat applications. To improve this paradigm of finetuning, as main contributions, we provide insights into stability of KD-QAT by empirically studying the gradient propagation during training to better understand the vulnerabilities of KD-QAT based approaches to low-bit quantization errors. Based on our insights, we propose ov-freeze, a simple technique to stabilize the KD-QAT process. Finally, we expe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#37319;&#29992;&#21355;&#26143;&#22270;&#20687;&#21644;&#20854;&#20182;&#29289;&#31181;&#20986;&#29616;&#20449;&#24687;&#26469;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;R-Tran&#65292;&#21487;&#20197;&#21033;&#29992;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18028</link><description>&lt;p&gt;
&#20174;&#37096;&#20998;&#35266;&#27979;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Predicting species occurrence patterns from partial observations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18028
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#37319;&#29992;&#21355;&#26143;&#22270;&#20687;&#21644;&#20854;&#20182;&#29289;&#31181;&#20986;&#29616;&#20449;&#24687;&#26469;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;R-Tran&#65292;&#21487;&#20197;&#21033;&#29992;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#29983;&#29289;&#22810;&#26679;&#24615;&#21644;&#27668;&#20505;&#21361;&#26426;&#65292;&#25105;&#20204;&#38656;&#35201;&#20102;&#35299;&#29289;&#31181;&#20998;&#24067;&#30340;&#20301;&#32622;&#20197;&#21450;&#36825;&#20123;&#27169;&#24335;&#22914;&#20309;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29289;&#31181;&#30340;&#35266;&#27979;&#25968;&#25454;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#65292;&#21487;&#29992;&#25968;&#25454;&#30340;&#37327;&#22312;&#19981;&#21516;&#20998;&#31867;&#32676;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#32473;&#23450;&#21355;&#26143;&#22270;&#20687;&#21644;&#20854;&#20182;&#29289;&#31181;&#20986;&#29616;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#12290;&#20026;&#20102;&#22312;&#27492;&#20219;&#21153;&#19978;&#35780;&#20272;&#31639;&#27861;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SatButterfly&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#34676;&#34678;&#30340;&#21355;&#26143;&#22270;&#20687;&#12289;&#29615;&#22659;&#25968;&#25454;&#21644;&#35266;&#27979;&#25968;&#25454;&#65292;&#26088;&#22312;&#19982;&#29616;&#26377;&#30340;&#40479;&#31867;&#35266;&#27979;&#25968;&#25454;&#38598;SatBird&#37197;&#23545;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;R-Tran&#65292;&#29992;&#20110;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#22320;&#26041;&#20351;&#29992;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;R-Tran&#22312;&#39044;&#27979;&#29289;&#31181;&#36973;&#36935;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18028v1 Announce Type: cross  Abstract: To address the interlinked biodiversity and climate crises, we need an understanding of where species occur and how these patterns are changing. However, observational data on most species remains very limited, and the amount of data available varies greatly between taxonomic groups. We introduce the problem of predicting species occurrence patterns given (a) satellite imagery, and (b) known information on the occurrence of other species. To evaluate algorithms on this task, we introduce SatButterfly, a dataset of satellite images, environmental data and observational data for butterflies, which is designed to pair with the existing SatBird dataset of bird observational data. To address this task, we propose a general model, R-Tran, for predicting species occurrence patterns that enables the use of partial observational data wherever found. We find that R-Tran outperforms other methods in predicting species encounter rates with partial
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;LM&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.18025</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#23450;&#25513;&#30721;&#25439;&#22833;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#65306;&#20197;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18025
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;LM&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35843;&#25972;&#21040;&#26032;&#39046;&#22495;&#36890;&#24120;&#36890;&#36807;&#22312;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;LM&#65288;PLM&#65289;&#26469;&#23454;&#29616;&#12290;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#24341;&#20837;LM&#65292;&#20351;&#23427;&#33021;&#22815;&#29702;&#35299;&#21644;&#26377;&#25928;&#25191;&#34892;&#30446;&#26631;&#22495;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#21464;&#24471;&#19981;&#22815;&#25935;&#24863;&#65292;&#22914;&#26524;&#23427;&#24573;&#35270;&#20102;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#24191;&#27867;&#24046;&#24322;&#65288;&#20363;&#22914;&#22312;&#35789;&#20041;&#19978;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#24494;&#35843;&#19981;&#25935;&#24863;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36866;&#24403;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65288;DS-terms&#65289;&#30340;&#37325;&#35201;&#24615;&#26469;&#26377;&#25928;&#33719;&#21462;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;MSLM&#21516;&#26102;&#23631;&#34109;DS&#26415;&#35821;&#21644;&#36890;&#29992;&#35789;&#65292;&#28982;&#21518;&#36890;&#36807;&#30830;&#20445;LM&#21463;&#21040;&#26356;&#22823;&#24809;&#32602;&#26469;&#23398;&#20064;&#29305;&#23450;&#20110;&#25513;&#30721;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18025v1 Announce Type: cross  Abstract: Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for in
&lt;/p&gt;</description></item><item><title>DORE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#23450;&#20041;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#21253;&#21547;&#36229;&#36807;10&#19975;&#20010;&#23450;&#20041;&#65292;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18018</link><description>&lt;p&gt;
DORE&#65306;&#19968;&#20221;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#23450;&#20041;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DORE: A Dataset For Portuguese Definition Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18018
&lt;/p&gt;
&lt;p&gt;
DORE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#23450;&#20041;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#21253;&#21547;&#36229;&#36807;10&#19975;&#20010;&#23450;&#20041;&#65292;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18018v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#23450;&#20041;&#24314;&#27169;&#65288;DM&#65289;&#26159;&#33258;&#21160;&#20026;&#29305;&#23450;&#21333;&#35789;&#29983;&#25104;&#35789;&#20856;&#23450;&#20041;&#30340;&#20219;&#21153;&#12290;&#20855;&#26377;DM&#33021;&#21147;&#30340;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#22312;&#22810;&#20010;&#21463;&#20247;&#20013;&#21463;&#30410;&#65292;&#22240;&#20026;DM&#34987;&#35270;&#20026;&#30417;&#30563;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#38382;&#39064;&#65292;&#36825;&#20123;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#24050;&#32463;&#21457;&#24067;&#20102;&#19968;&#20123;&#29992;&#20110;&#33521;&#35821;&#21644;&#20854;&#20182;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;DM&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#33889;&#33796;&#29273;&#35821;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20013;/&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#19988;&#34987;2&#20159;&#22810;&#27597;&#35821;&#20154;&#21475;&#20351;&#29992;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#33889;&#33796;&#29273;&#35821;&#30340;DM&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;DORE&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65307;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#23450;&#20041;&#24314;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;10&#19975;&#20010;&#23450;&#20041;&#12290;&#25105;&#20204;&#36824;&#22312;DORE&#19978;&#35780;&#20272;&#20102;&#20960;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;DM&#27169;&#22411;&#65292;&#24182;&#25253;&#21578;&#20102;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18018v1 Announce Type: new  Abstract: Definition modelling (DM) is the task of automatically generating a dictionary definition for a specific word. Computational systems that are capable of DM can have numerous applications benefiting a wide range of audiences. As DM is considered a supervised natural language generation problem, these systems require large annotated datasets to train the machine learning (ML) models. Several DM datasets have been released for English and other high-resource languages. While Portuguese is considered a mid/high-resource language in most natural language processing tasks and is spoken by more than 200 million native speakers, there is no DM dataset available for Portuguese. In this research, we fill this gap by introducing DORE; the first dataset for Definition MOdelling for PoRtuguEse containing more than 100,000 definitions. We also evaluate several deep learning based DM models on DORE and report the results. The dataset and the findings o
&lt;/p&gt;</description></item><item><title>&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17919</link><description>&lt;p&gt;
LISA&#65306;&#29992;&#20110;&#39640;&#25928;&#20869;&#23384;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17919
&lt;/p&gt;
&lt;p&gt;
&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39318;&#27425;&#20986;&#29616;&#20197;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#24040;&#22823;&#30340;&#20869;&#23384;&#28040;&#32791;&#24050;&#25104;&#20026;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35832;&#22914;&#20302;&#31209;&#35843;&#25972;&#65288;LoRA&#65289;&#20043;&#31867;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#22823;&#35268;&#27169;&#24494;&#35843;&#35774;&#32622;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#26080;&#27861;&#19982;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#30456;&#21305;&#37197;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LoRA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#30340;&#36880;&#23618;&#29305;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#21516;&#23618;&#20043;&#38388;&#26435;&#37325;&#33539;&#25968;&#30340;&#24322;&#24120;&#20559;&#26012;&#12290;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#31616;&#21333;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#35760;&#24518;&#25104;&#26412;&#20302;&#20110;LoRA&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#24191;&#27867;&#30340;&#35774;&#32622;&#20013;&#20248;&#20110;LoRA&#21644;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;Layerwise Importance Sampled AdamW&#65288;LISA&#65289;&#65292;&#36825;&#26159;LoRA&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24212;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17919v1 Announce Type: cross  Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of
&lt;/p&gt;</description></item><item><title>&#35768;&#22810;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#25968;&#25454;&#38598;&#23384;&#22312;&#19982;JPEG&#21387;&#32553;&#21644;&#22270;&#20687;&#22823;&#23567;&#30456;&#20851;&#30340;&#20559;&#35265;&#65292;&#21435;&#38500;&#36825;&#20123;&#20559;&#35265;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;JPEG&#21387;&#32553;&#30340;&#31283;&#20581;&#24615;&#24182;&#26174;&#33879;&#25913;&#21464;&#26816;&#27979;&#22120;&#30340;&#36328;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17608</link><description>&lt;p&gt;
&#20266;&#36896;&#36824;&#26159;JPEG&#65311;&#25581;&#31034;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#24120;&#35265;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17608
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#25968;&#25454;&#38598;&#23384;&#22312;&#19982;JPEG&#21387;&#32553;&#21644;&#22270;&#20687;&#22823;&#23567;&#30456;&#20851;&#30340;&#20559;&#35265;&#65292;&#21435;&#38500;&#36825;&#20123;&#20559;&#35265;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;JPEG&#21387;&#32553;&#30340;&#31283;&#20581;&#24615;&#24182;&#26174;&#33879;&#25913;&#21464;&#26816;&#27979;&#22120;&#30340;&#36328;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#20984;&#26174;&#20102;&#26816;&#27979;&#20154;&#36896;&#20869;&#23481;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#36825;&#26159;&#25171;&#20987;&#24191;&#27867;&#25805;&#32437;&#21644;&#35823;&#23548;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#26816;&#27979;&#22120;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#24050;&#32463;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#25968;&#25454;&#38598;&#19981;&#32463;&#24847;&#22320;&#24341;&#20837;&#20102;&#19981;&#33391;&#20559;&#35265;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#26816;&#27979;&#22120;&#30340;&#25928;&#26524;&#21644;&#35780;&#20272;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#35768;&#22810;&#29992;&#20110;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;JPEG&#21387;&#32553;&#21644;&#22270;&#20687;&#22823;&#23567;&#26377;&#20851;&#30340;&#20559;&#35265;&#12290;&#20351;&#29992;GenImage&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#26816;&#27979;&#22120;&#30830;&#23454;&#20174;&#36825;&#20123;&#19981;&#21463;&#27426;&#36814;&#30340;&#22240;&#32032;&#20013;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#21435;&#38500;&#36825;&#20123;&#21629;&#21517;&#20559;&#35265;&#20250;&#26174;&#33879;&#22686;&#21152;&#38024;&#23545;JPEG&#21387;&#32553;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#26174;&#33879;&#25913;&#21464;&#35780;&#20272;&#26816;&#27979;&#22120;&#30340;&#36328;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;ResNet50&#21644;S
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17608v1 Announce Type: cross  Abstract: The widespread adoption of generative image models has highlighted the urgent need to detect artificial content, which is a crucial step in combating widespread manipulation and misinformation. Consequently, numerous detectors and associated datasets have emerged. However, many of these datasets inadvertently introduce undesirable biases, thereby impacting the effectiveness and evaluation of detectors. In this paper, we emphasize that many datasets for AI-generated image detection contain biases related to JPEG compression and image size. Using the GenImage dataset, we demonstrate that detectors indeed learn from these undesired factors. Furthermore, we show that removing the named biases substantially increases robustness to JPEG compression and significantly alters the cross-generator performance of evaluated detectors. Specifically, it leads to more than 11 percentage points increase in cross-generator performance for ResNet50 and S
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#19981;&#21516;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#21457;&#29616;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#21462;&#20915;&#20110;&#22806;&#37096;&#21464;&#37327;&#65292;&#22914;&#25915;&#20987;&#31867;&#22411;&#12289;&#22797;&#26434;&#24615;&#21644;&#32593;&#32476;&#29615;&#22659;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#24182;&#38750;&#22987;&#32456;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2403.17458</link><description>&lt;p&gt;
&#26399;&#26395;&#19982;&#29616;&#23454;&#65306;&#23454;&#36341;&#20013;&#35780;&#20272;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Expectations Versus Reality: Evaluating Intrusion Detection Systems in Practice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17458
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#19981;&#21516;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#21457;&#29616;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#21462;&#20915;&#20110;&#22806;&#37096;&#21464;&#37327;&#65292;&#22914;&#25915;&#20987;&#31867;&#22411;&#12289;&#22797;&#26434;&#24615;&#21644;&#32593;&#32476;&#29615;&#22659;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#24182;&#38750;&#22987;&#32456;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#26368;&#36817;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23458;&#35266;&#27604;&#36739;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#26681;&#25454;&#20854;&#38656;&#27714;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#27809;&#26377;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#26368;&#22909;&#30340;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#22806;&#37096;&#21464;&#37327;&#65292;&#22914;&#25915;&#20987;&#31867;&#22411;&#12289;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#38598;&#20013;&#30340;&#32593;&#32476;&#29615;&#22659;&#12290;&#20363;&#22914;&#65292;BoT_IoT&#21644;Stratosphere IoT&#25968;&#25454;&#38598;&#37117;&#25429;&#33719;&#20102;&#19982;&#29289;&#32852;&#32593;&#30456;&#20851;&#30340;&#25915;&#20987;&#65292;&#20294;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;BoT_IoT&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#26102;&#34920;&#29616;&#26368;&#20339;&#65292;&#32780;&#22312;&#20351;&#29992;Stratosphere IoT&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#26102;HELAD&#34920;&#29616;&#26368;&#20339;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#25105;&#20204;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26368;&#39640;&#30340;&#24179;&#22343;F1&#20998;&#25968;&#65292;&#20294;&#24182;&#19981;&#24635;&#26159;&#34920;&#29616;&#26368;&#22909;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#20351;&#29992;&#25991;&#29486;&#21644;&#39033;&#30446;&#23384;&#20648;&#24211;&#20013;&#30340;IDS&#30340;&#22256;&#38590;&#65292;&#36825;&#20351;&#24471;&#23601;IDS&#36873;&#25321;&#24471;&#20986;&#26126;&#30830;&#32467;&#35770;&#21464;&#24471;&#22797;&#26434;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17458v1 Announce Type: cross  Abstract: Our paper provides empirical comparisons between recent IDSs to provide an objective comparison between them to help users choose the most appropriate solution based on their requirements. Our results show that no one solution is the best, but is dependent on external variables such as the types of attacks, complexity, and network environment in the dataset. For example, BoT_IoT and Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural network performed the best when tested using the BoT_IoT dataset while HELAD performed the best when tested using the Stratosphere IoT dataset. So although we found that a deep neural network solution had the highest average F1 scores on tested datasets, it is not always the best-performing one. We further discuss difficulties in using IDS from literature and project repositories, which complicated drawing definitive conclusions regarding IDS selection.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;CADGL&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#27867;&#21270;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#29616;&#23454;&#24212;&#29992;&#26041;&#38754;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.17210</link><description>&lt;p&gt;
CADGL: &#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#29992;&#20110;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17210
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;CADGL&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#27867;&#21270;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#29616;&#23454;&#24212;&#29992;&#26041;&#38754;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDIs&#65289;&#30340;&#30740;&#31350;&#26159;&#33647;&#29289;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20803;&#32032;&#12290;DDIs&#21457;&#29983;&#22312;&#19968;&#20010;&#33647;&#29289;&#30340;&#24615;&#36136;&#21463;&#20854;&#20182;&#33647;&#29289;&#21253;&#21547;&#30340;&#24433;&#21709;&#26102;&#12290;&#26816;&#27979;&#26377;&#21033;&#30340;DDIs&#26377;&#21487;&#33021;&#20026;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#24212;&#29992;&#30340;&#21019;&#26032;&#33647;&#29289;&#30340;&#21019;&#36896;&#21644;&#25512;&#36827;&#38138;&#24179;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#12289;&#31283;&#20581;&#29305;&#24449;&#25552;&#21462;&#20197;&#21450;&#29616;&#23454;&#24212;&#29992;&#21487;&#33021;&#24615;&#26041;&#38754;&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;CADGL&#30340;&#26032;&#39062;&#26694;&#26550;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#22522;&#20110;&#23450;&#21046;&#30340;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;VGAE&#65289;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#19978;&#19979;&#25991;&#39044;&#22788;&#29702;&#22120;&#20174;&#20004;&#20010;&#19981;&#21516;&#35270;&#35282;&#65306;&#23616;&#37096;&#37051;&#22495;&#21644;&#20998;&#23376;&#19978;&#19979;&#25991;&#65292;&#22312;&#24322;&#36136;&#22270;&#32467;&#26500;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#25429;&#33719;&#20851;&#38190;&#30340;&#32467;&#26500;&#21644;&#29983;&#29702;&#21270;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17210v1 Announce Type: cross  Abstract: Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process of drug development. DDIs occur when one drug's properties are affected by the inclusion of other drugs. Detecting favorable DDIs has the potential to pave the way for creating and advancing innovative medications applicable in practical settings. However, existing DDI prediction models continue to face challenges related to generalization in extreme cases, robust feature extraction, and real-life application possibilities. We aim to address these challenges by leveraging the effectiveness of context-aware deep graph learning by introducing a novel framework named CADGL. Based on a customized variational graph autoencoder (VGAE), we capture critical structural and physio-chemical information using two context preprocessors for feature extraction from two different perspectives: local neighborhood and molecular context, in a heterogeneous graphical structure. Ou
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16591</link><description>&lt;p&gt;
&#25581;&#31034;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16591
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#36805;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#38544;&#31169;&#23450;&#20041;&#30340;&#22810;&#26679;&#21270;&#65292;&#30001;&#20110;&#23545;&#38544;&#31169;&#26500;&#25104;&#30340;&#23041;&#32961;&#65292;&#21253;&#25324;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#30340;&#27010;&#24565;&#12290;&#34429;&#28982;&#34987;&#24191;&#27867;&#25509;&#21463;&#24182;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34987;&#21033;&#29992;&#65292;&#20294;&#36825;&#31181;&#20256;&#32479;&#30340;&#38544;&#31169;&#27979;&#37327;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#20174;&#26080;&#27861;&#38450;&#27490;&#25512;&#26029;&#25259;&#38706;&#21040;&#32570;&#20047;&#23545;&#23545;&#25163;&#32972;&#26223;&#30693;&#35782;&#30340;&#32771;&#34385;&#12290;&#22312;&#36825;&#39033;&#20840;&#38754;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#36125;&#21494;&#26031;&#38544;&#31169;&#24182;&#28145;&#20837;&#25506;&#35752;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#21644;&#20854;&#36125;&#21494;&#26031;&#23545;&#24212;&#29289;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#27010;&#25324;&#20102;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#31361;&#20986;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#22522;&#20110;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#65288;ABP&#65289;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#20005;&#26684;&#23450;&#20041;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16591v1 Announce Type: cross  Abstract: The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Max
&lt;/p&gt;</description></item><item><title>DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;</title><link>https://arxiv.org/abs/2403.16451</link><description>&lt;p&gt;
DeepMachining: &#38115;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#22312;&#32447;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeepMachining: Online Prediction of Machining Errors of Lathe Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16451
&lt;/p&gt;
&lt;p&gt;
DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;DeepMachining&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#21152;&#24037;&#35823;&#24046;&#12290;&#25105;&#20204;&#22522;&#20110;&#24037;&#21378;&#30340;&#21046;&#36896;&#25968;&#25454;&#26500;&#24314;&#24182;&#35780;&#20272;&#20102;DeepMachining&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29305;&#23450;&#36710;&#24202;&#26426;&#24202;&#25805;&#20316;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#21152;&#24037;&#29366;&#24577;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#29305;&#23450;&#21152;&#24037;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DeepMachining&#22312;&#28041;&#21450;&#19981;&#21516;&#24037;&#20214;&#21644;&#20992;&#20855;&#30340;&#22810;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#39318;&#25209;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16451v1 Announce Type: cross  Abstract: We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#32791;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#26694;&#26550;&#30446;&#26631;&#22359;&#24494;&#35843;&#65292;&#26681;&#25454;&#25968;&#25454;&#28418;&#31227;&#31867;&#22411;&#24494;&#35843;&#19981;&#21516;&#27169;&#22359;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#21644;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2403.15905</link><description>&lt;p&gt;
&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#37327;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15905
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#32791;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#26694;&#26550;&#30446;&#26631;&#22359;&#24494;&#35843;&#65292;&#26681;&#25454;&#25968;&#25454;&#28418;&#31227;&#31867;&#22411;&#24494;&#35843;&#19981;&#21516;&#27169;&#22359;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#21644;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20010;&#24615;&#21270;&#20197;&#35299;&#20915;&#25968;&#25454;&#28418;&#31227;&#38382;&#39064;&#22312;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20391;&#37325;&#20110;&#24494;&#35843;&#23436;&#25972;&#22522;&#30784;&#27169;&#22411;&#25110;&#20854;&#26368;&#21518;&#20960;&#23618;&#20197;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#33021;&#28304;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#32791;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#26694;&#26550;&#8212;&#8212;&#30446;&#26631;&#22359;&#24494;&#35843;&#65288;TBFT&#65289;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#28418;&#31227;&#21644;&#20010;&#24615;&#21270;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#36755;&#20837;&#32423;&#21035;&#12289;&#29305;&#24449;&#32423;&#21035;&#21644;&#36755;&#20986;&#32423;&#21035;&#12290;&#38024;&#23545;&#27599;&#31181;&#31867;&#22411;&#65292;&#25105;&#20204;&#24494;&#35843;&#19981;&#21516;&#27169;&#22411;&#22359;&#20197;&#23454;&#29616;&#22312;&#38477;&#20302;&#33021;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36755;&#20837;&#32423;&#12289;&#29305;&#24449;&#32423;&#21644;&#36755;&#20986;&#32423;&#23545;&#24212;&#20110;&#24494;&#35843;&#27169;&#22411;&#30340;&#21069;&#31471;&#12289;&#20013;&#27573;&#21644;&#21518;&#31471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15905v1 Announce Type: new  Abstract: The personalization of machine learning (ML) models to address data drift is a significant challenge in the context of Internet of Things (IoT) applications. Presently, most approaches focus on fine-tuning either the full base model or its last few layers to adapt to new data, while often neglecting energy costs. However, various types of data drift exist, and fine-tuning the full base model or the last few layers may not result in optimal performance in certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy adaptive personalization framework designed for resource-constrained devices. We categorize data drift and personalization into three types: input-level, feature-level, and output-level. For each type, we fine-tune different blocks of the model to achieve optimal performance with reduced energy costs. Specifically, input-, feature-, and output-level correspond to fine-tuning the front, middle, and rear blocks of 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#36807;&#31243;&#20013;&#19981;&#21516;&#38454;&#27573;&#33719;&#24471;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20307;&#31215;/&#20960;&#20309;&#21644;&#25439;&#22833;&#26223;&#35266;&#29305;&#24449;&#36827;&#34892;&#32463;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#35797;&#22270;&#27934;&#23519;&#36208;&#21183;&#24425;&#31080;&#20551;&#35774;&#21644;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#20013;&#30340;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.15022</link><description>&lt;p&gt;
&#36208;&#21183;&#24425;&#31080;&#20551;&#35774;&#21644;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Insights into the Lottery Ticket Hypothesis and the Iterative Magnitude Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15022
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#36807;&#31243;&#20013;&#19981;&#21516;&#38454;&#27573;&#33719;&#24471;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20307;&#31215;/&#20960;&#20309;&#21644;&#25439;&#22833;&#26223;&#35266;&#29305;&#24449;&#36827;&#34892;&#32463;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#35797;&#22270;&#27934;&#23519;&#36208;&#21183;&#24425;&#31080;&#20551;&#35774;&#21644;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#20013;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15022v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36208;&#21183;&#24425;&#31080;&#20551;&#35774;&#24378;&#35843;&#20102;&#37325;&#26032;&#35757;&#32451;&#21033;&#29992;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#36807;&#31243;&#33719;&#24471;&#30340;&#26356;&#31232;&#30095;&#32593;&#32476;&#26102;&#25152;&#20351;&#29992;&#30340;&#21021;&#22987;&#21270;&#30340;&#37325;&#35201;&#24615;&#12290;&#33267;&#20170;&#23578;&#32570;&#20047;&#20851;&#20110;&#36208;&#21183;&#24425;&#31080;&#20551;&#35774;&#20013;&#25552;&#20986;&#30340;&#29305;&#23450;&#21021;&#22987;&#21270;&#20026;&#20309;&#26356;&#26377;&#21033;&#20110;&#27867;&#21270;&#65288;&#21644;&#35757;&#32451;&#65289;&#24615;&#33021;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#20013;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#22914;&#21098;&#26525;&#36739;&#23567;&#24133;&#24230;&#26435;&#37325;&#21644;&#36845;&#20195;&#36807;&#31243;&#30340;&#20316;&#29992;&#65292;&#23578;&#32570;&#20047;&#23436;&#20840;&#29702;&#35299;&#21644;&#35299;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#23545;&#22312;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#36807;&#31243;&#30340;&#21508;&#20010;&#38454;&#27573;&#33719;&#24471;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20307;&#31215;/&#20960;&#20309;&#21644;&#25439;&#22833;&#26223;&#35266;&#29305;&#24449;&#36827;&#34892;&#32463;&#39564;&#30740;&#31350;&#65292;&#20197;&#27934;&#23519;&#36825;&#20123;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15022v1 Announce Type: new  Abstract: Lottery ticket hypothesis for deep neural networks emphasizes the importance of initialization used to re-train the sparser networks obtained using the iterative magnitude pruning process. An explanation for why the specific initialization proposed by the lottery ticket hypothesis tends to work better in terms of generalization (and training) performance has been lacking. Moreover, the underlying principles in iterative magnitude pruning, like the pruning of smaller magnitude weights and the role of the iterative process, lack full understanding and explanation. In this work, we attempt to provide insights into these phenomena by empirically studying the volume/geometry and loss landscape characteristics of the solutions obtained at various stages of the iterative magnitude pruning process.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14472</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Large Language Models via Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SafeEdit&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#23433;&#20840;&#31867;&#21035;&#65292;&#20855;&#26377;&#21508;&#31181;&#24378;&#22823;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#30693;&#35782;&#32534;&#36753;&#26377;&#28508;&#21147;&#22312;&#23545;LLMs&#36827;&#34892;&#21435;&#27602;&#21270;&#26102;&#65292;&#22312;&#23545;&#19968;&#33324;&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;&#65292;&#31216;&#20026;&#36890;&#36807;&#26415;&#20013;&#31070;&#32463;&#30417;&#27979;&#21435;&#27602;&#21270;&#65288;DINM&#65289;&#65292;&#36890;&#36807;&#20165;&#19968;&#27425;&#23454;&#20363;&#30340;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;LLMs&#30340;&#27602;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;SFT&#21644;DPO&#21487;&#33021;&#20165;&#25233;&#21046;&#26377;&#27602;&#21442;&#25968;&#30340;&#28608;&#27963;&#65292;&#32780;DINM&#21017;&#20943;&#36731;&#26377;&#27602;&#21442;&#25968;&#30340;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#33033;&#20914;&#33258;&#27880;&#24847;&#26426;&#21046;DSSA&#20197;&#21450;&#32467;&#21512;ResNet&#30340;&#22810;&#38454;&#27573;&#26550;&#26500;&#30340;SpikingResformer&#26550;&#26500;&#65292;&#26088;&#22312;&#25913;&#21892;&#24615;&#33021;&#21644;&#33021;&#25928;&#65292;&#24182;&#20943;&#23569;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.14302</link><description>&lt;p&gt;
SpikingResformer: &#23558;ResNet&#21644;Vision Transformer&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#26725;&#25509;
&lt;/p&gt;
&lt;p&gt;
SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14302
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#33033;&#20914;&#33258;&#27880;&#24847;&#26426;&#21046;DSSA&#20197;&#21450;&#32467;&#21512;ResNet&#30340;&#22810;&#38454;&#27573;&#26550;&#26500;&#30340;SpikingResformer&#26550;&#26500;&#65292;&#26088;&#22312;&#25913;&#21892;&#24615;&#33021;&#21644;&#33021;&#25928;&#65292;&#24182;&#20943;&#23569;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformer&#22312;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#20013;&#32467;&#21512;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#22522;&#20110;Transformer&#30340;&#32467;&#26500;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Dual Spike Self-Attention (DSSA)&#30340;&#26032;&#22411;&#33033;&#20914;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24102;&#26377;&#21512;&#29702;&#30340;&#25193;&#23637;&#26041;&#27861;&#12290;&#22522;&#20110;DSSA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpikingResformer&#30340;&#26032;&#22411;&#33033;&#20914;Vision Transformer&#26550;&#26500;&#65292;&#23558;&#22522;&#20110;ResNet&#30340;&#22810;&#38454;&#27573;&#26550;&#26500;&#19982;&#25105;&#20204;&#25552;&#20986;&#30340;DSSA&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#33021;&#25928;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14302v1 Announce Type: cross  Abstract: The remarkable success of Vision Transformers in Artificial Neural Networks (ANNs) has led to a growing interest in incorporating the self-attention mechanism and transformer-based architecture into Spiking Neural Networks (SNNs). While existing methods propose spiking self-attention mechanisms that are compatible with SNNs, they lack reasonable scaling methods, and the overall architectures proposed by these methods suffer from a bottleneck in effectively extracting local features. To address these challenges, we propose a novel spiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a reasonable scaling method. Based on DSSA, we propose a novel spiking Vision Transformer architecture called SpikingResformer, which combines the ResNet-based multi-stage architecture with our proposed DSSA to improve both performance and energy efficiency while reducing parameters. Experimental results show that SpikingResformer ach
&lt;/p&gt;</description></item><item><title>&#23558;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#35270;&#20026;&#21477;&#23376;&#24182;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#65292;&#35777;&#23454;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.12984</link><description>&lt;p&gt;
&#24403;SMILES&#25317;&#26377;&#35821;&#35328;&#65306;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#23545;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12984
&lt;/p&gt;
&lt;p&gt;
&#23558;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#35270;&#20026;&#21477;&#23376;&#24182;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#65292;&#35777;&#23454;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#21270;&#23398;&#32467;&#26500;&#65292;&#22914;&#33647;&#29289;&#65292;&#36890;&#24120;&#30001;SMILES&#23383;&#31526;&#20018;&#26469;&#23450;&#20041;&#65292;&#20316;&#20026;&#20998;&#23376;&#21644;&#38190;&#30340;&#24207;&#21015;&#12290;&#36825;&#20123;SMILES&#23383;&#31526;&#20018;&#22312;&#19981;&#21516;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20851;&#30740;&#31350;&#21644;&#34920;&#31034;&#24037;&#20316;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25670;&#33073;&#22797;&#26434;&#30340;&#34920;&#31034;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22914;&#26524;&#25105;&#20204;&#23558;&#33647;&#29289;SMILES&#35270;&#20026;&#24120;&#35268;&#21477;&#23376;&#65292;&#24182;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#20197;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#20250;&#24590;&#26679;&#65311;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#33719;&#24471;&#20102;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#25968;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#27599;&#20010;&#21407;&#23376;&#21644;&#38190;&#35270;&#20026;&#21477;&#23376;&#32452;&#20214;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#22522;&#26412;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23545;&#33647;&#29289;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#34920;&#26126;&#22797;&#26434;&#30340;&#38382;&#39064;&#20063;&#21487;&#20197;&#29992;&#26356;&#31616;&#21333;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#22312;&#27492;&#22788;&#25214;&#21040;&#65306;https://github.com/azminewasi/Drug-Classification-NLP&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12984v1 Announce Type: cross  Abstract: Complex chemical structures, like drugs, are usually defined by SMILES strings as a sequence of molecules and bonds. These SMILES strings are used in different complex machine learning-based drug-related research and representation works. Escaping from complex representation, in this work, we pose a single question: What if we treat drug SMILES as conventional sentences and engage in text classification for drug classification? Our experiments affirm the possibility with very competitive scores. The study explores the notion of viewing each atom and bond as sentence components, employing basic NLP methods to categorize drug types, proving that complex problems can also be solved with simpler perspectives. The data and code are available here: https://github.com/azminewasi/Drug-Classification-NLP.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#21487;&#33021;&#23384;&#22312;&#31574;&#30053;&#20998;&#21449;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#25299;&#25169;&#20998;&#26512;&#20197;&#35777;&#26126;&#22312;&#19968;&#20123;&#24773;&#26223;&#19979;&#65292;&#31574;&#30053;&#38656;&#35201;&#20855;&#26377;&#19981;&#36830;&#32493;&#24615;&#25110;&#22810;&#20540;&#24615;&#65292;&#36825;&#23545;&#24212;&#20110;&#38556;&#30861;&#29289;&#33258;&#30001;&#29366;&#24577;&#31354;&#38388;&#20026;&#38750;&#21333;&#36830;&#36890;&#26102;&#38656;&#35201;&#31574;&#30053;&#20998;&#21449;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.12847</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20998;&#21449;
&lt;/p&gt;
&lt;p&gt;
Policy Bifurcation in Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12847
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#21487;&#33021;&#23384;&#22312;&#31574;&#30053;&#20998;&#21449;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#25299;&#25169;&#20998;&#26512;&#20197;&#35777;&#26126;&#22312;&#19968;&#20123;&#24773;&#26223;&#19979;&#65292;&#31574;&#30053;&#38656;&#35201;&#20855;&#26377;&#19981;&#36830;&#32493;&#24615;&#25110;&#22810;&#20540;&#24615;&#65292;&#36825;&#23545;&#24212;&#20110;&#38556;&#30861;&#29289;&#33258;&#30001;&#29366;&#24577;&#31354;&#38388;&#20026;&#38750;&#21333;&#36830;&#36890;&#26102;&#38656;&#35201;&#31574;&#30053;&#20998;&#21449;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20026;&#21463;&#38480;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#38544;&#21547;&#22320;&#20551;&#35774;&#31574;&#30053;&#20989;&#25968;&#20855;&#26377;&#36830;&#32493;&#24615;&#65292;&#21363;&#31574;&#30053;&#20197;&#24179;&#31283;&#12289;&#36830;&#32493;&#30340;&#26041;&#24335;&#23558;&#29366;&#24577;&#26144;&#23556;&#21040;&#21160;&#20316;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#34892;&#31574;&#30053;&#24212;&#35813;&#26159;&#19981;&#36830;&#32493;&#25110;&#22810;&#20540;&#30340;&#65292;&#32780;&#22312;&#19981;&#36830;&#32493;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#20043;&#38388;&#25554;&#20540;&#21487;&#33021;&#20250;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#32422;&#26463;&#36829;&#35268;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#35782;&#21035;&#20986;&#36825;&#31181;&#29616;&#35937;&#29983;&#25104;&#26426;&#21046;&#30340;&#30740;&#31350;&#65292;&#24182;&#37319;&#29992;&#25299;&#25169;&#20998;&#26512;&#20005;&#35880;&#22320;&#35777;&#26126;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#20998;&#21449;&#30340;&#23384;&#22312;&#65292;&#36825;&#23545;&#24212;&#20110;&#21487;&#36798;&#20803;&#32452;&#30340;&#21487;&#25910;&#32553;&#24615;&#12290;&#25105;&#20204;&#30340;&#23450;&#29702;&#25581;&#31034;&#20102;&#22312;&#38556;&#30861;&#29289;&#33258;&#30001;&#29366;&#24577;&#31354;&#38388;&#20026;&#38750;&#21333;&#36830;&#36890;&#30340;&#24773;&#26223;&#20013;&#65292;&#38656;&#35201;&#31574;&#30053;&#20998;&#21449;&#65292;&#24847;&#21619;&#30528;&#20854;&#36755;&#20986;&#21160;&#20316;&#38656;&#35201;&#36805;&#36895;&#21709;&#24212;&#29366;&#24577;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12847v1 Announce Type: new  Abstract: Safe reinforcement learning (RL) offers advanced solutions to constrained optimal control problems. Existing studies in safe RL implicitly assume continuity in policy functions, where policies map states to actions in a smooth, uninterrupted manner; however, our research finds that in some scenarios, the feasible policy should be discontinuous or multi-valued, interpolating between discontinuous local optima can inevitably lead to constraint violations. We are the first to identify the generating mechanism of such a phenomenon, and employ topological analysis to rigorously prove the existence of policy bifurcation in safe RL, which corresponds to the contractibility of the reachable tuple. Our theorem reveals that in scenarios where the obstacle-free state space is non-simply connected, a feasible policy is required to be bifurcated, meaning its output action needs to change abruptly in response to the varying state. To train such a bifu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27604;&#20256;&#32479;&#20998;&#23376;&#27169;&#25311;&#24555;4&#21040;5&#20010;&#25968;&#37327;&#32423;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#27832;&#30707;&#30340;&#21560;&#38468;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#21560;&#38468;&#20301;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.12659</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27832;&#30707;&#30340;&#21560;&#38468;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Zeolite Adsorption Property Prediction using Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12659
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27604;&#20256;&#32479;&#20998;&#23376;&#27169;&#25311;&#24555;4&#21040;5&#20010;&#25968;&#37327;&#32423;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#27832;&#30707;&#30340;&#21560;&#38468;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#21560;&#38468;&#20301;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#27832;&#30707;&#30340;&#21560;&#38468;&#24615;&#33021;&#23545;&#20110;&#21152;&#36895;&#26032;&#26448;&#26009;&#35774;&#35745;&#36807;&#31243;&#26377;&#24456;&#22823;&#30340;&#30410;&#22788;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27604;&#20998;&#23376;&#27169;&#25311;&#24555;4&#21040;5&#20010;&#25968;&#37327;&#32423;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#21560;&#38468;&#24615;&#33021;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#24471;&#21040;&#30340;&#32467;&#26524;&#19982;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#24471;&#21040;&#30340;&#25968;&#20540;&#19968;&#33268;&#65292;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#21487;&#29992;&#20110;&#24615;&#33021;&#39044;&#27979;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#21560;&#38468;&#20301;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12659v1 Announce Type: cross  Abstract: The ability to efficiently predict adsorption properties of zeolites can be of large benefit in accelerating the design process of novel materials. The existing configuration space for these materials is wide, while existing molecular simulation methods are computationally expensive. In this work, we propose a model which is 4 to 5 orders of magnitude faster at adsorption properties compared to molecular simulations. To validate the model, we generated datasets containing various aluminium configurations for the MOR, MFI, RHO and ITW zeolites along with their heat of adsorptions and Henry coefficients for CO$_2$, obtained from Monte Carlo simulations. The predictions obtained from the Machine Learning model are in agreement with the values obtained from the Monte Carlo simulations, confirming that the model can be used for property prediction. Furthermore, we show that the model can be used for identifying adsorption sites. Finally, we
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ROUTERBENCH&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#36335;&#30001;&#31995;&#32479;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#25324;&#36229;&#36807;405k&#25512;&#29702;&#32467;&#26524;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#36335;&#30001;&#31574;&#30053;&#30340;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2403.12031</link><description>&lt;p&gt;
ROUTERBENCH&#65306;&#29992;&#20110;&#22810;LLM&#36335;&#30001;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ROUTERBENCH: A Benchmark for Multi-LLM Routing System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12031
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ROUTERBENCH&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#36335;&#30001;&#31995;&#32479;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#25324;&#36229;&#36807;405k&#25512;&#29702;&#32467;&#26524;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#36335;&#30001;&#31574;&#30053;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#33539;&#22260;&#19981;&#26029;&#25193;&#22823;&#65292;&#23545;&#26377;&#25928;&#30340;&#26381;&#21153;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#21464;&#24471;&#26085;&#30410;&#20851;&#38190;&#12290;&#23613;&#31649;LLMs&#20855;&#26377;&#22810;&#26679;&#24615;&#65292;&#20294;&#27809;&#26377;&#21333;&#19968;&#27169;&#22411;&#21487;&#20197;&#26368;&#20248;&#22320;&#35299;&#20915;&#25152;&#26377;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#24179;&#34913;&#24615;&#33021;&#21644;&#25104;&#26412;&#20043;&#38388;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#38480;&#21046;&#65292;&#21457;&#23637;&#20102;LLM&#36335;&#30001;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#32467;&#21512;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#20197;&#20811;&#26381;&#21333;&#20010;LLMs&#30340;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#29992;&#20110;&#35780;&#20272;LLM&#36335;&#30001;&#22120;&#24615;&#33021;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#27979;&#35797;&#65292;&#38459;&#30861;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ROUTERBENCH&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#31995;&#32479;&#35780;&#20272;LLM&#36335;&#30001;&#31995;&#32479;&#30340;&#21151;&#25928;&#65292;&#20197;&#21450;&#19968;&#20010;&#21253;&#25324;&#26469;&#33258;&#20195;&#34920;&#24615;LLMs&#30340;&#36229;&#36807;405k&#25512;&#29702;&#32467;&#26524;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#36335;&#30001;&#31574;&#30053;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#36335;&#30001;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12031v1 Announce Type: cross  Abstract: As the range of applications for Large Language Models (LLMs) continues to grow, the demand for effective serving solutions becomes increasingly critical. Despite the versatility of LLMs, no single model can optimally address all tasks and applications, particularly when balancing performance with cost. This limitation has led to the development of LLM routing systems, which combine the strengths of various models to overcome the constraints of individual LLMs. Yet, the absence of a standardized benchmark for evaluating the performance of LLM routers hinders progress in this area. To bridge this gap, we present ROUTERBENCH, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies. We further propose a theoretical framework for LLM routing, and del
&lt;/p&gt;</description></item><item><title>&#22312;&#38750;&#20809;&#28369;&#35774;&#32622;&#19979;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#20869;&#26144;&#23556;&#30340;&#22806;&#26144;&#23556;&#22266;&#23450;&#28857;&#30340;&#38544;&#24335;&#23548;&#25968;&#30340;&#26032;&#26041;&#27861;NSID&#65292;&#24182;&#25552;&#20379;&#20102;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#36845;&#20195;&#24494;&#20998;&#65288;ITD&#65289;&#21644;&#36817;&#20284;&#38544;&#24335;&#24494;&#20998;&#65288;AID&#65289;&#30340;&#25913;&#36827;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.11687</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#38544;&#24335;&#24494;&#20998;&#65306;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11687
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#20809;&#28369;&#35774;&#32622;&#19979;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#20869;&#26144;&#23556;&#30340;&#22806;&#26144;&#23556;&#22266;&#23450;&#28857;&#30340;&#38544;&#24335;&#23548;&#25968;&#30340;&#26032;&#26041;&#27861;NSID&#65292;&#24182;&#25552;&#20379;&#20102;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#36845;&#20195;&#24494;&#20998;&#65288;ITD&#65289;&#21644;&#36817;&#20284;&#38544;&#24335;&#24494;&#20998;&#65288;AID&#65289;&#30340;&#25913;&#36827;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#25928;&#35745;&#31639;&#21442;&#25968;&#21270;&#19981;&#21487;&#24494;&#25910;&#32553;&#26144;&#23556;&#22266;&#23450;&#28857;&#23548;&#25968;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#20803;&#23398;&#20064;&#21644;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65306;&#36845;&#20195;&#24494;&#20998;&#65288;ITD&#65289;&#21644;&#36817;&#20284;&#38544;&#24335;&#24494;&#20998;&#65288;AID&#65289;&#12290;&#22312;&#38750;&#20809;&#28369;&#35774;&#32622;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#38142;&#35268;&#21017;&#19981;&#20877;&#25104;&#31435;&#12290;&#22312;Bolte&#31561;&#20154;&#65288;2022&#65289;&#26368;&#36817;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#20182;&#20204;&#35777;&#26126;&#20102;&#19981;&#21487;&#24494;&#20998;ITD&#30340;&#32447;&#24615;&#25910;&#25947;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;ITD&#21644;AID&#30340;&#25913;&#36827;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;NSID&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22266;&#23450;&#28857;&#34987;&#23450;&#20041;&#20026;&#21482;&#36890;&#36807;&#38543;&#26426;&#26080;&#20559;&#20272;&#35745;&#22120;&#35775;&#38382;&#30340;&#22806;&#26144;&#23556;&#21644;&#20869;&#26144;&#23556;&#30340;&#32452;&#21512;&#26102;&#35745;&#31639;&#38544;&#24335;&#23548;&#25968;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11687v1 Announce Type: cross  Abstract: We study the problem of efficiently computing the derivative of the fixed-point of a parametric non-differentiable contraction map. This problem has wide applications in machine learning, including hyperparameter optimization, meta-learning and data poisoning attacks. We analyze two popular approaches: iterative differentiation (ITD) and approximate implicit differentiation (AID). A key challenge behind the nonsmooth setting is that the chain rule does not hold anymore. Building upon the recent work by Bolte et al. (2022), who proved the linear convergence of non-differentiable ITD, we provide refined linear convergence rates for both ITD and AID in the deterministic case. We further introduce NSID, a new method to compute the implicit derivative when the fixed point is defined as the composition of an outer map and an inner map which is accessible only through a stochastic unbiased estimator. We establish rates for the convergence of 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11624</link><description>&lt;p&gt;
&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Dual-Channel Multiplex Graph Neural Networks for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#20934;&#30830;&#25429;&#25417;&#21453;&#26144;&#20010;&#20154;&#20559;&#22909;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#23646;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19968;&#20123;&#29616;&#26377;&#30340;&#25512;&#33616;&#25216;&#26415;&#24050;&#32463;&#24320;&#22987;&#23558;&#37325;&#28857;&#36716;&#21521;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#22330;&#26223;&#20013;&#23545;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#21508;&#31181;&#31867;&#22411;&#20132;&#20114;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#22312;&#32447;&#36141;&#29289;&#24179;&#21488;&#19978;&#30340;&#28857;&#20987;&#12289;&#26631;&#35760;&#25910;&#34255;&#21644;&#36141;&#20080;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#30340;&#32570;&#28857;&#65306;(1) &#19981;&#36275;&#30340;&#24314;&#27169;&#21644;&#21033;&#29992;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#22810;&#36890;&#36335;&#20851;&#31995;&#24418;&#25104;&#30340;&#21508;&#31181;&#34892;&#20026;&#27169;&#24335;&#23545;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;(2) &#24573;&#30053;&#20102;&#34892;&#20026;&#27169;&#24335;&#20013;&#19981;&#21516;&#20851;&#31995;&#23545;&#25512;&#33616;&#31995;&#32479;&#22330;&#26223;&#20013;&#30446;&#26631;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11624v1 Announce Type: cross  Abstract: Efficient recommender systems play a crucial role in accurately capturing user and item attributes that mirror individual preferences. Some existing recommendation techniques have started to shift their focus towards modeling various types of interaction relations between users and items in real-world recommendation scenarios, such as clicks, marking favorites, and purchases on online shopping platforms. Nevertheless, these approaches still grapple with two significant shortcomings: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on representation learning, and (2) ignoring the effect of different relations in the behavior patterns on the target relation in recommender system scenarios. In this study, we introduce a novel recommendation framework, Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the aforementioned challenges
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31649;&#36947;&#65292;&#32467;&#21512;EfficientNet&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#20174;&#32954;&#37096;CT&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;COVID-19&#65292;&#24182;&#22312;&#31454;&#36187;&#25968;&#25454;&#38598;&#39564;&#35777;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;</title><link>https://arxiv.org/abs/2403.11505</link><description>&lt;p&gt;
&#20351;&#29992;EfficientNet&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;CT&#25195;&#25551;&#20013;&#26816;&#27979;Covid-19
&lt;/p&gt;
&lt;p&gt;
Covid-19 detection from CT scans using EfficientNet and Attention mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11505
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31649;&#36947;&#65292;&#32467;&#21512;EfficientNet&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#20174;&#32954;&#37096;CT&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;COVID-19&#65292;&#24182;&#22312;&#31454;&#36187;&#25968;&#25454;&#38598;&#39564;&#35777;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#31649;&#36947;&#30340;COVID-19&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32954;&#37096;CT&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;COVID-19&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;EfficientNet&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#21435;&#24180;&#31454;&#36187;&#25968;&#25454;&#38598;&#39564;&#35777;&#38598;&#19978;&#30340;&#20854;&#20182;&#22242;&#38431;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11505v1 Announce Type: cross  Abstract: Manual diagnosis and analysis of COVID-19 through the examination of lung Computed Tomography (CT) scan images by physicians tends to result in inefficiency, especially with high patient volumes and numerous images per patient. We address the need for automation by developing a deep learning model-based pipeline for COVID-19 detection from CT scan images of the lungs. The Domain adaptation, Explainability, and Fairness in AI for Medical Image Analysis Workshop and COVID-19 Diagnosis Competition (DEF-AI-MIA COV19D) provides an opportunity to assess our designed pipeline for COVID-19 detection from CT scan images. The proposed pipeline incorporates EfficientNet with an Attention mechanism with a pre-processing step. Our pipeline outperforms last year's teams on the validation set of the competition dataset.
&lt;/p&gt;</description></item><item><title>FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.08059</link><description>&lt;p&gt;
FluoroSAM: &#29992;&#20110;X&#20809;&#22270;&#20687;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08059
&lt;/p&gt;
&lt;p&gt;
FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;X&#20809;&#22270;&#20687;&#20998;&#21106;&#23558;&#21152;&#36895;&#35786;&#26029;&#21644;&#20171;&#20837;&#31934;&#20934;&#21307;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#22270;&#20687;&#20998;&#26512;&#38382;&#39064;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#25928;&#29992;&#21463;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#39046;&#22495;&#65292;&#35201;&#25299;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#21017;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#12289;&#26631;&#31614;&#21644;&#37325;&#26032;&#35757;&#32451;&#24037;&#20316;&#12290;&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289; - &#35757;&#32451;&#22312;&#22823;&#37327;&#39640;&#24230;&#21464;&#21270;&#25968;&#25454;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22240;&#27492;&#20351;&#24471;&#24191;&#27867;&#36866;&#29992;&#24615;&#25104;&#20026;&#21487;&#33021; - &#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#30340;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;FMs&#32858;&#28966;&#20110;&#23545;&#35937;&#34987;&#26126;&#26174;&#21487;&#35265;&#36793;&#30028;&#28165;&#26224;&#23450;&#20041;&#30340;&#22330;&#26223;&#21644;&#27169;&#24335;&#65292;&#22914;&#20869;&#31397;&#38236;&#25163;&#26415;&#24037;&#20855;&#20998;&#21106;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;X&#20809;&#25104;&#20687;&#36890;&#24120;&#27809;&#26377;&#25552;&#20379;&#36825;&#31181;&#28165;&#26224;&#30340;&#36793;&#30028;&#25110;&#32467;&#26500;&#20808;&#39564;&#12290;&#22312;X&#20809;&#22270;&#20687;&#24418;&#25104;&#26399;&#38388;&#65292;&#22797;&#26434;&#30340;&#19977;&#32500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08059v1 Announce Type: cross  Abstract: Automated X-ray image segmentation would accelerate research and development in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving specific image analysis problems, but the utility of these models is restricted to their particular task domain, and expanding to broader use requires additional data, labels, and retraining efforts. Recently, foundation models (FMs) -- machine learning models trained on large amounts of highly variable data thus enabling broad applicability -- have emerged as promising tools for automated image analysis. Existing FMs for medical image analysis focus on scenarios and modalities where objects are clearly defined by visually apparent boundaries, such as surgical tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally offer such clearly delineated boundaries or structure priors. During X-ray image formation, complex 3D
&lt;/p&gt;</description></item><item><title>CAS&#26694;&#26550;&#20801;&#35768;&#22312;&#22312;&#32447;&#36873;&#25321;&#24615;&#39044;&#27979;&#20013;&#25511;&#21046;FCR&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#21644;&#26657;&#20934;&#38598;&#26500;&#36896;&#36755;&#20986;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;</title><link>https://arxiv.org/abs/2403.07728</link><description>&lt;p&gt;
CAS: &#19968;&#31181;&#20855;&#26377;FCR&#25511;&#21046;&#30340;&#22312;&#32447;&#36873;&#25321;&#24615;&#31526;&#21512;&#39044;&#27979;&#30340;&#36890;&#29992;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07728
&lt;/p&gt;
&lt;p&gt;
CAS&#26694;&#26550;&#20801;&#35768;&#22312;&#22312;&#32447;&#36873;&#25321;&#24615;&#39044;&#27979;&#20013;&#25511;&#21046;FCR&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#21644;&#26657;&#20934;&#38598;&#26500;&#36896;&#36755;&#20986;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#26041;&#24335;&#19979;&#21518;&#36873;&#25321;&#39044;&#27979;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#23558;&#36164;&#28304;&#32791;&#36153;&#22312;&#19981;&#37325;&#35201;&#30340;&#21333;&#20301;&#19978;&#65292;&#22312;&#25253;&#21578;&#20854;&#39044;&#27979;&#21306;&#38388;&#20043;&#21069;&#23545;&#24403;&#21069;&#20010;&#20307;&#36827;&#34892;&#21021;&#27493;&#36873;&#25321;&#22312;&#22312;&#32447;&#39044;&#27979;&#20219;&#21153;&#20013;&#26159;&#24120;&#35265;&#19988;&#26377;&#24847;&#20041;&#30340;&#12290;&#30001;&#20110;&#22312;&#32447;&#36873;&#25321;&#23548;&#33268;&#25152;&#36873;&#39044;&#27979;&#21306;&#38388;&#20013;&#23384;&#22312;&#26102;&#38388;&#22810;&#37325;&#24615;&#65292;&#22240;&#27492;&#25511;&#21046;&#23454;&#26102;&#35823;&#35206;&#30422;&#38472;&#36848;&#29575;&#65288;FCR&#65289;&#26469;&#27979;&#37327;&#24179;&#22343;&#35823;&#35206;&#30422;&#35823;&#24046;&#26159;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;CAS&#65288;&#36866;&#24212;&#24615;&#36873;&#25321;&#21518;&#26657;&#20934;&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#21253;&#35065;&#20219;&#20309;&#39044;&#27979;&#27169;&#22411;&#21644;&#22312;&#32447;&#36873;&#25321;&#35268;&#21017;&#65292;&#20197;&#36755;&#20986;&#21518;&#36873;&#25321;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#22914;&#26524;&#36873;&#25321;&#20102;&#24403;&#21069;&#20010;&#20307;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#33258;&#36866;&#24212;&#36873;&#25321;&#26469;&#26500;&#24314;&#26657;&#20934;&#38598;&#65292;&#28982;&#21518;&#20026;&#26410;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#36755;&#20986;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#20026;&#26657;&#20934;&#38598;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26500;&#36896;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07728v1 Announce Type: cross  Abstract: We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) to measure the averaged miscoverage error. We develop a general framework named CAS (Calibration after Adaptive Selection) that can wrap around any prediction model and online selection rule to output post-selection prediction intervals. If the current individual is selected, we first perform an adaptive selection on historical data to construct a calibration set, then output a conformal prediction interval for the unobserved label. We provide tractable constructions for the calibration set for 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36880;&#27493;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65288;SLIM&#65289;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#36164;&#28304;&#38656;&#27714;&#30340;&#38590;&#39064;&#65292;&#20351;&#20854;&#33021;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20139;&#21463;LLMs&#30340;&#20986;&#33394;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04260</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#25104;&#20026;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#33391;&#22909;&#25512;&#29702;&#32773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Small Language Models be Good Reasoners for Sequential Recommendation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36880;&#27493;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65288;SLIM&#65289;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#36164;&#28304;&#38656;&#27714;&#30340;&#38590;&#39064;&#65292;&#20351;&#20854;&#33021;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20139;&#21463;LLMs&#30340;&#20986;&#33394;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#24320;&#25299;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#35201;&#25104;&#21151;&#23454;&#29616;&#30001;LLMs&#36171;&#33021;&#30340;&#39034;&#24207;&#25512;&#33616;&#36824;&#26377;&#35768;&#22810;&#25361;&#25112;&#38656;&#35201;&#35299;&#20915;&#12290;&#39318;&#20808;&#65292;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#36890;&#24120;&#22797;&#26434;&#65292;&#20165;&#20165;&#20381;&#38752;LLMs&#30340;&#19968;&#27493;&#25512;&#29702;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#25110;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#21709;&#24212;&#12290;&#20854;&#27425;&#65292;LLMs&#65288;&#20363;&#22914;ChatGPT-175B&#65289;&#26497;&#39640;&#30340;&#36164;&#28304;&#38656;&#27714;&#26159;&#38590;&#20197;&#25215;&#21463;&#19988;&#22312;&#23454;&#38469;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#36880;&#27493;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#29992;&#20110;&#25512;&#33616;&#65288;SLIM&#65289;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#22120;&#20197;&#8220;&#30246;&#8221;&#65288;&#21363;&#36164;&#28304;&#39640;&#25928;&#65289;&#30340;&#26041;&#24335;&#20139;&#21463;LLMs&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#38138;&#24179;&#20102;&#19968;&#26465;&#26377;&#21069;&#36884;&#30340;&#36947;&#36335;&#12290;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#30340;CoT&#25552;&#31034;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04260v1 Announce Type: cross  Abstract: Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larg
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.01121</link><description>&lt;p&gt;
OpenGraph: &#36808;&#21521;&#24320;&#25918;&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OpenGraph: Towards Open Graph Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#20132;&#20114;   &#25688;&#35201;: &#22270;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#37322;&#21644;&#21033;&#29992;&#21508;&#39046;&#22495;&#30340;&#20851;&#31995;&#25968;&#25454;&#30340;&#19981;&#21487;&#25110;&#32570;&#37096;&#20998;&#65292;&#20174;&#25512;&#33616;&#31995;&#32479;&#21040;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#21508;&#31181;GNN&#24050;&#32463;&#25104;&#20026;&#32534;&#30721;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#36825;&#20123;GNN&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#22686;&#24378;&#22270;&#23398;&#20064;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20363;&#22914;&#38142;&#25509;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;: &#36825;&#20123;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#22312;&#23558;&#26174;&#33879;&#19981;&#21516;&#20110;&#35757;&#32451;&#23454;&#20363;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#27867;&#21270;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#26469;&#25512;&#36827;&#22270;&#23398;&#20064;&#33539;&#24335;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#29702;&#35299;&#22810;&#26679;&#22270;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#25299;&#25169;&#27169;&#24335;&#65292;&#20351;&#20854;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 Announce Type: cross  Abstract: Graph learning has become indispensable for interpreting and harnessing relational data in diverse fields, ranging from recommendation systems to social network analysis. In this context, a variety of GNNs have emerged as promising methodologies for encoding the structural information of graphs. By effectively capturing the graph's underlying structure, these GNNs have shown great potential in enhancing performance in graph learning tasks, such as link prediction and node classification. However, despite their successes, a significant challenge persists: these advanced methods often face difficulties in generalizing to unseen graph data that significantly differs from the training instances. In this work, our aim is to advance the graph learning paradigm by developing a general graph foundation model. This model is designed to understand the complex topological patterns present in diverse graph data, enabling it to excel in zero-shot g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#27599;&#38598;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26368;&#20248;$Q$-&#20989;&#25968;&#65292;&#30830;&#20445;&#25910;&#25947;&#21442;&#25968;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.19212</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#20010;&#20984;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning: A Convex Optimization Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#27599;&#38598;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26368;&#20248;$Q$-&#20989;&#25968;&#65292;&#30830;&#20445;&#25910;&#25947;&#21442;&#25968;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#38598;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;&#27599;&#20010;&#38598;&#20013;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#25214;&#21040;&#26368;&#20248;$Q$-&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#12290;&#20984;&#20248;&#21270;&#26041;&#27861;&#30830;&#20445;&#27599;&#20010;&#38598;&#21512;&#20013;&#35745;&#31639;&#30340;&#26435;&#37325;&#26159;&#26368;&#20248;&#30340;&#65292;&#20851;&#20110;&#24403;&#21069;&#38598;&#21512;&#30340;&#37319;&#26679;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#23545;&#20110;&#31283;&#23450;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#25910;&#25947;&#65292;&#24182;&#19988;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#21442;&#25968;&#21487;&#20197;&#19982;&#26368;&#20248;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26080;&#38480;&#25509;&#36817;&#12290;&#29305;&#21035;&#26159;&#65292;&#22914;&#26524;&#27491;&#21017;&#21270;&#21442;&#25968;&#20026;$\rho$&#65292;&#26102;&#38388;&#38271;&#24230;&#20026;$T$&#65292;&#37027;&#20040;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#25910;&#25947;&#21040;$w$&#65292;&#20854;&#20013;$w$&#19982;&#26368;&#20248;&#21442;&#25968;$w^\star$&#20043;&#38388;&#30340;&#36317;&#31163;&#21463;&#21040;$\mathcal{O}(\rho T^{-1})$&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19212v1 Announce Type: cross  Abstract: In this paper, we consider reinforcement learning of nonlinear systems with continuous state and action spaces. We present an episodic learning algorithm, where we for each episode use convex optimization to find a two-layer neural network approximation of the optimal $Q$-function. The convex optimization approach guarantees that the weights calculated at each episode are optimal, with respect to the given sampled states and actions of the current episode. For stable nonlinear systems, we show that the algorithm converges and that the converging parameters of the trained neural network can be made arbitrarily close to the optimal neural network parameters. In particular, if the regularization parameter is $\rho$ and the time horizon is $T$, then the parameters of the trained neural network converge to $w$, where the distance between $w$ from the optimal parameters $w^\star$ is bounded by $\mathcal{O}(\rho T^{-1})$. That is, when the nu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#36825;&#19968;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30693;&#35782;&#20849;&#20139;&#65292;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25269;&#24481;&#35266;&#27979;&#22122;&#22768;&#12290;</title><link>https://arxiv.org/abs/2402.16105</link><description>&lt;p&gt;
&#36890;&#30693;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Informed Meta-Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#36825;&#19968;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30693;&#35782;&#20849;&#20139;&#65292;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25269;&#24481;&#35266;&#27979;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#30427;&#34892;&#30340;&#22024;&#26434;&#21644;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#31361;&#20986;&#30340;&#25361;&#25112;&#22312;&#20110;&#26377;&#25928;&#22320;&#34701;&#21512;&#20419;&#36827;&#25968;&#25454;&#25928;&#29575;&#21644;&#31283;&#20581;&#24615;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#20803;&#23398;&#20064;&#21644;&#36890;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#20004;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#26041;&#27861;&#12290;&#21069;&#32773;&#20381;&#36182;&#20110;&#19968;&#31181;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#20808;&#39564;&#26469;&#28304;&#65292;&#32780;&#21518;&#32773;&#21463;&#19987;&#23478;&#30693;&#35782;&#30340;&#24418;&#24335;&#21270;&#34920;&#31034;&#24341;&#23548;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#33539;&#24335;&#65292;&#36890;&#30693;&#20803;&#23398;&#20064;&#65292;&#26088;&#22312;&#23454;&#29616;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#36328;&#20219;&#21153;&#30693;&#35782;&#20849;&#20139;&#30340;&#20114;&#34917;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#19968;&#26694;&#26550;&#30340;&#20855;&#20307;&#23454;&#20363;--&#36890;&#30693;&#31070;&#32463;&#36807;&#31243;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35828;&#26126;&#24615;&#21644;&#26356;&#22823;&#35268;&#27169;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#22312;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25269;&#24481;&#35266;&#27979;&#22122;&#22768;&#26041;&#38754;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16105v1 Announce Type: new  Abstract: In noisy and low-data regimes prevalent in real-world applications, an outstanding challenge of machine learning lies in effectively incorporating inductive biases that promote data efficiency and robustness. Meta-learning and informed ML stand out as two approaches for incorporating prior knowledge into the ML pipeline. While the former relies on a purely data-driven source of priors, the latter is guided by a formal representation of expert knowledge. This paper introduces a novel hybrid paradigm, informed meta-learning, seeking complementarity in cross-task knowledge sharing of humans and machines. We establish the foundational components of informed meta-learning and present a concrete instantiation of this framework--the Informed Neural Process. Through a series of illustrative and larger-scale experiments, we demonstrate the potential benefits of informed meta-learning in improving data efficiency and robustness to observational no
&lt;/p&gt;</description></item><item><title>Equilibrium K-Means&#65288;EKM&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#19988;&#31616;&#21333;&#30340;K&#22343;&#20540;&#31867;&#22411;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#32858;&#31867;&#20013;&#24515;&#22312;&#22823;&#31867;&#31751;&#20013;&#24515;&#32858;&#38598;&#30340;&#20542;&#21521;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14490</link><description>&lt;p&gt;
&#20351;&#29992;Equilibrium K-Means&#36827;&#34892;&#19981;&#24179;&#34913;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Imbalanced Data Clustering using Equilibrium K-Means
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14490
&lt;/p&gt;
&lt;p&gt;
Equilibrium K-Means&#65288;EKM&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#19988;&#31616;&#21333;&#30340;K&#22343;&#20540;&#31867;&#22411;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#32858;&#31867;&#20013;&#24515;&#22312;&#22823;&#31867;&#31751;&#20013;&#24515;&#32858;&#38598;&#30340;&#20542;&#21521;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#25968;&#25454;&#25351;&#30340;&#26159;&#25968;&#25454;&#28857;&#22312;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#20998;&#24067;&#19981;&#22343;&#34913;&#65292;&#36825;&#32473;&#20256;&#32479;&#30340;&#30828;&#32858;&#31867;&#31639;&#27861;&#21644;&#27169;&#31946;&#32858;&#31867;&#31639;&#27861;&#65288;&#22914;&#30828;K&#22343;&#20540;&#65288;HKM&#65292;&#25110;&#32773;Lloyd&#31639;&#27861;&#65289;&#21644;&#27169;&#31946;K&#22343;&#20540;&#65288;FKM&#65292;&#25110;&#32773;Bezdek&#31639;&#27861;&#65289;&#65289;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#31616;&#21333;&#30340;K&#22343;&#20540;&#31867;&#22411;&#31639;&#27861;&#8212;&#8212;Equilibrium K-Means&#65288;EKM&#65289;&#65292;&#23427;&#22312;&#20004;&#20010;&#27493;&#39588;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#32858;&#31867;&#32467;&#26524;&#65292;&#20943;&#23569;&#20102;&#32858;&#31867;&#20013;&#24515;&#21521;&#22823;&#31867;&#31751;&#20013;&#24515;&#32858;&#38598;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23545;HKM&#12289;FKM&#21644;EKM&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#34920;&#26126;&#23427;&#20204;&#26412;&#36136;&#19978;&#26159;&#20855;&#26377;&#26126;&#30830;&#20851;&#31995;&#30340;&#29275;&#39039;&#26041;&#27861;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;EKM&#20855;&#26377;&#19982;FKM&#30456;&#21516;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#20294;&#23545;&#20854;&#25104;&#21592;&#23450;&#20041;&#25552;&#20379;&#20102;&#26356;&#28165;&#26224;&#30340;&#29289;&#29702;&#24847;&#20041;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#21313;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;EKM&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#21508;&#31181;&#32858;&#31867;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14490v1 Announce Type: new  Abstract: Imbalanced data, characterized by an unequal distribution of data points across different clusters, poses a challenge for traditional hard and fuzzy clustering algorithms, such as hard K-means (HKM, or Lloyd's algorithm) and fuzzy K-means (FKM, or Bezdek's algorithm). This paper introduces equilibrium K-means (EKM), a novel and simple K-means-type algorithm that alternates between just two steps, yielding significantly improved clustering results for imbalanced data by reducing the tendency of centroids to crowd together in the center of large clusters. We also present a unifying perspective for HKM, FKM, and EKM, showing they are essentially gradient descent algorithms with an explicit relationship to Newton's method. EKM has the same time and space complexity as FKM but offers a clearer physical meaning for its membership definition. We illustrate the performance of EKM on two synthetic and ten real datasets, comparing it to various cl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11815</link><description>&lt;p&gt;
HU&#22312;SemEval-2024&#20219;&#21153;8A&#20013;&#30340;&#34920;&#29616;&#65306;&#23545;&#27604;&#23398;&#20064;&#33021;&#21542;&#23398;&#20064;&#23884;&#20837;&#20197;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11815
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;SemEval-2024&#20219;&#21153;8&#8220;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#8221;&#24320;&#21457;&#30340;&#31995;&#32479;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#34394;&#20551;&#25991;&#26412;&#29983;&#25104;&#12289;&#32593;&#32476;&#38035;&#40060;&#12289;&#32771;&#35797;&#20316;&#24330;&#29978;&#33267;&#25220;&#34989;&#29256;&#26435;&#26448;&#26009;&#20013;&#30340;&#20351;&#29992;&#65292;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19968;&#30452;&#26159;&#20027;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#35768;&#22810;&#31995;&#32479;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#20999;&#23454;&#38469;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#36890;&#24120;&#19981;&#21487;&#33021;&#30693;&#36947;&#29992;&#25143;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#20855;&#20307;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#20854;&#20351;&#29992;&#22522;&#32447;&#21442;&#25968;&#30340;&#22823;&#32422;40%&#65288;149M&#27604;355M&#65289;&#65292;&#20294;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21487;&#27604;&#30340;&#24615;&#33021;&#65288;&#22312;137&#20010;&#21442;&#19982;&#32773;&#20013;&#25490;&#21517;&#31532;21&#65289;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#26159;&#65292;&#21363;&#20351;&#27809;&#26377;&#22810;&#20010;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11815v1 Announce Type: cross  Abstract: This paper describes our system developed for SemEval-2024 Task 8, "Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection." Machine-generated texts have been one of the main concerns due to the use of large language models (LLM) in fake text generation, phishing, cheating in exams, or even plagiarizing copyright materials. A lot of systems have been developed to detect machine-generated text. Nonetheless, the majority of these systems rely on the text-generating model, a limitation that is impractical in real-world scenarios, as it's often impossible to know which specific model the user has used for text generation. In this work, we propose a single model based on contrastive learning, which uses ~40% of the baseline's parameters (149M vs. 355M) but shows a comparable performance on the test dataset (21st out of 137 participants). Our key finding is that even without an ensemble of multiple models, a
&lt;/p&gt;</description></item><item><title>Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.10251</link><description>&lt;p&gt;
Brant-2&#65306;&#33041;&#20449;&#21495;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Brant-2: Foundation Model for Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10251
&lt;/p&gt;
&lt;p&gt;
Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#21463;&#30410;&#20110;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#31181;&#27169;&#22411;&#22312;&#20998;&#26512;&#33041;&#20449;&#21495;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#65292;&#22240;&#20026;&#36825;&#19968;&#39046;&#22495;&#28085;&#30422;&#20102;&#20247;&#22810;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#19988;&#36827;&#34892;&#22823;&#35268;&#27169;&#27880;&#37322;&#26159;&#25104;&#26412;&#39640;&#26114;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;Brant-2&#12290;&#19982;&#29992;&#20110;&#39045;&#20869;&#31070;&#32463;&#20449;&#21495;&#30340;&#22522;&#30784;&#27169;&#22411;Brant&#30456;&#27604;&#65292;Brant-2&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#22823;&#37327;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Brant-2&#23545;&#33041;&#20449;&#21495;&#20013;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#30340;&#36866;&#24212;&#24615;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#25581;&#31034;&#20102;Brant-2&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#39564;&#35777;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#20445;&#25345;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10251v1 Announce Type: cross  Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRDP&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#40657;&#30418;&#22870;&#21169;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#25552;&#31034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08714</link><description>&lt;p&gt;
PRDP&#65306;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#29992;&#20110;&#22870;&#21169;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRDP&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#40657;&#30418;&#22870;&#21169;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#25552;&#31034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#24494;&#35843;&#24050;&#25104;&#20026;&#23558;&#22522;&#30784;&#27169;&#22411;&#19982;&#19979;&#28216;&#30446;&#26631;&#23545;&#40784;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#22312;&#35821;&#35328;&#39046;&#22495;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#26368;&#22823;&#21270;&#21453;&#26144;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#24050;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#35270;&#35273;&#39046;&#22495;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;RL&#30340;&#22870;&#21169;&#24494;&#35843;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#20013;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#65292;&#20351;&#23427;&#20204;&#26080;&#27861;&#25512;&#24191;&#21040;&#22797;&#26434;&#30340;&#12289;&#26410;&#30693;&#30340;&#25552;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#65288;PRDP&#65289;&#65292;&#39318;&#27425;&#22312;&#36229;&#36807;100K&#20010;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25552;&#31034;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#40657;&#30418;&#22870;&#21169;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#65288;RDP&#65289;&#30446;&#26631;&#65292;&#35813;&#30446;&#26631;&#19982;RL&#30446;&#26631;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#20139;&#21463;&#26356;&#22909;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08714v1 Announce Type: cross Abstract: Reward finetuning has emerged as a promising approach to aligning foundation models with downstream objectives. Remarkable success has been achieved in the language domain by using reinforcement learning (RL) to maximize rewards that reflect human preference. However, in the vision domain, existing RL-based reward finetuning methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen prompts. In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable black-box reward finetuning for diffusion models for the first time on large-scale prompt datasets with over 100K prompts. Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability. Specifically, the RDP objective is a supervised regression objective that tasks the diffusion model with pred
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#26500;&#24819;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#38656;&#35201;&#38754;&#23545;&#26356;&#22797;&#26434;&#21644;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#30340;&#24895;&#26223;&#12290;&#36825;&#20010;&#24895;&#26223;&#30340;&#26680;&#24515;&#26159;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.07946</link><description>&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#25351;&#25381;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Re-Envisioning Command and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07946
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#38656;&#35201;&#38754;&#23545;&#26356;&#22797;&#26434;&#21644;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#30340;&#24895;&#26223;&#12290;&#36825;&#20010;&#24895;&#26223;&#30340;&#26680;&#24515;&#26159;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#25112;&#20105;&#23558;&#35201;&#27714;&#22312;&#26356;&#22797;&#26434;&#12289;&#24555;&#33410;&#22863;&#12289;&#19981;&#32467;&#26500;&#21270;&#21644;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#12290;C2&#23558;&#22240;&#34987;&#25298;&#32477;&#12289;&#36864;&#21270;&#12289;&#38388;&#27463;&#21644;&#26377;&#38480;&#30340;&#36890;&#20449;&#20197;&#21450;&#38656;&#35201;&#32771;&#34385;&#21040;&#22810;&#20010;&#20316;&#25112;&#39046;&#22495;&#20013;&#30340;&#35768;&#22810;&#25968;&#25454;&#27969;&#32780;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;C2&#23454;&#36341;&#8212;&#8212;&#28304;&#33258;&#24037;&#19994;&#26102;&#20195;&#32780;&#38750;&#26032;&#20852;&#30340;&#26234;&#33021;&#26102;&#20195;&#8212;&#8212;&#26159;&#32447;&#24615;&#30340;&#19988;&#32791;&#26102;&#12290;&#32780;&#19988;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#26410;&#26469;&#25112;&#22330;&#19978;&#19982;&#23545;&#25163;&#20445;&#25345;&#20248;&#21183;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#19982;&#20154;&#31867;&#20043;&#38388;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#24895;&#26223;&#12290;&#36825;&#20010;&#26410;&#26469;&#24895;&#26223;&#20307;&#29616;&#22312;&#19977;&#20010;&#36816;&#33829;&#24433;&#21709;&#19978;&#65306;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#20197;&#21450;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25152;&#35774;&#24819;&#30340;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future warfare will require Command and Control (C2) decision-making to occur in more complex, fast-paced, ill-structured, and demanding conditions. C2 will be further complicated by operational challenges such as Denied, Degraded, Intermittent, and Limited (DDIL) communications and the need to account for many data streams, potentially across multiple domains of operation. Yet, current C2 practices -- which stem from the industrial era rather than the emerging intelligence era -- are linear and time-consuming. Critically, these approaches may fail to maintain overmatch against adversaries on the future battlefield. To address these challenges, we propose a vision for future C2 based on robust partnerships between humans and artificial intelligence (AI) systems. This future vision is encapsulated in three operational impacts: streamlining the C2 operations process, maintaining unity of effort, and developing adaptive collective knowledge systems. This paper illustrates the envisaged fu
&lt;/p&gt;</description></item><item><title>&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#65292;&#20197;&#25552;&#39640;C2&#36816;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06501</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Scalable Interactive Machine Learning for Future Command and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06501
&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#65292;&#20197;&#25552;&#39640;C2&#36816;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#37492;&#20110;&#38656;&#35201;&#24378;&#22823;&#30340;&#20915;&#31574;&#36807;&#31243;&#21644;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#30340;&#38598;&#25104;&#20855;&#26377;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;C2&#36816;&#20316;&#27969;&#31243;&#30340;&#28508;&#21147;&#65292;&#20197;&#30830;&#20445;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#26368;&#36817;&#22312;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#30340;&#31361;&#30772;&#65292;&#20154;&#31867;&#21487;&#20197;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21512;&#20316;&#20197;&#25351;&#23548;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#30446;&#21069;&#31185;&#25216;&#21457;&#23637;&#20013;&#23384;&#22312;&#30340;&#20960;&#20010;&#24046;&#36317;&#65292;&#26410;&#26469;&#30340;&#24037;&#20316;&#24212;&#35813;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#65292;&#20197;&#25193;&#23637;&#36825;&#20123;&#26041;&#27861;&#22312;&#22797;&#26434;&#30340;C2&#29615;&#22659;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19977;&#20010;&#30740;&#31350;&#37325;&#28857;&#39046;&#22495;&#65292;&#20849;&#21516;&#26088;&#22312;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;SIML&#65289;&#65306;1&#65289;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#31639;&#27861;&#20197;&#23454;&#29616;&#21327;&#21516;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future warfare will require Command and Control (C2) personnel to make decisions at shrinking timescales in complex and potentially ill-defined situations. Given the need for robust decision-making processes and decision-support tools, integration of artificial and human intelligence holds the potential to revolutionize the C2 operations process to ensure adaptability and efficiency in rapidly changing operational environments. We propose to leverage recent promising breakthroughs in interactive machine learning, in which humans can cooperate with machine learning algorithms to guide machine learning algorithm behavior. This paper identifies several gaps in state-of-the-art science and technology that future work should address to extend these approaches to function in complex C2 contexts. In particular, we describe three research focus areas that together, aim to enable scalable interactive machine learning (SIML): 1) developing human-AI interaction algorithms to enable planning in co
&lt;/p&gt;</description></item><item><title>TopoNav&#26159;&#19968;&#31181;&#25299;&#25169;&#23548;&#33322;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20027;&#21160;&#25299;&#25169;&#26144;&#23556;&#12289;&#20869;&#37096;&#22870;&#21169;&#26426;&#21046;&#21644;&#23618;&#27425;&#21270;&#30446;&#26631;&#20248;&#20808;&#32423;&#30340;&#32452;&#21512;&#26469;&#23454;&#29616;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#39640;&#25928;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.04061</link><description>&lt;p&gt;
TopoNav&#65306;&#33410;&#32422;&#22870;&#21169;&#29615;&#22659;&#20013;&#39640;&#25928;&#25506;&#32034;&#30340;&#25299;&#25169;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
TopoNav: Topological Navigation for Efficient Exploration in Sparse Reward Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04061
&lt;/p&gt;
&lt;p&gt;
TopoNav&#26159;&#19968;&#31181;&#25299;&#25169;&#23548;&#33322;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20027;&#21160;&#25299;&#25169;&#26144;&#23556;&#12289;&#20869;&#37096;&#22870;&#21169;&#26426;&#21046;&#21644;&#23618;&#27425;&#21270;&#30446;&#26631;&#20248;&#20808;&#32423;&#30340;&#32452;&#21512;&#26469;&#23454;&#29616;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#39640;&#25928;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#21306;&#22495;&#30340;&#25506;&#32034;&#38754;&#20020;&#30528;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#8212;&#8212;&#22312;&#27809;&#26377;&#20808;&#21069;&#22320;&#22270;&#21644;&#26377;&#38480;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#23548;&#33322;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#65292;&#36825;&#20010;&#25361;&#25112;&#26356;&#21152;&#20005;&#23803;&#65292;&#20256;&#32479;&#30340;&#25506;&#32034;&#25216;&#26415;&#24448;&#24448;&#22833;&#36133;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TopoNav&#65292;&#19968;&#31181;&#20840;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#23454;&#29616;&#39640;&#25928;&#12289;&#36866;&#24212;&#24615;&#24378;&#19988;&#30446;&#26631;&#23548;&#21521;&#30340;&#25506;&#32034;&#12290;TopoNav&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#26159;&#20027;&#21160;&#25299;&#25169;&#26144;&#23556;&#12289;&#20869;&#37096;&#22870;&#21169;&#26426;&#21046;&#21644;&#23618;&#27425;&#21270;&#30446;&#26631;&#20248;&#20808;&#32423;&#12290;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#65292;TopoNav&#26500;&#24314;&#20102;&#21160;&#24577;&#25299;&#25169;&#22320;&#22270;&#65292;&#25429;&#33719;&#20851;&#38190;&#20301;&#32622;&#21644;&#36335;&#24452;&#12290;&#23427;&#21033;&#29992;&#20869;&#37096;&#22870;&#21169;&#26469;&#25351;&#23548;&#26426;&#22120;&#20154;&#26397;&#30528;&#22320;&#22270;&#20013;&#25351;&#23450;&#30340;&#23376;&#30446;&#26631;&#21069;&#36827;&#65292;&#20419;&#36827;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#32467;&#26500;&#21270;&#25506;&#32034;&#12290;&#20026;&#20102;&#30830;&#20445;&#39640;&#25928;&#23548;&#33322;&#65292;TopoNav&#37319;&#29992;&#20102;&#20998;&#23618;&#30446;&#26631;&#39537;&#21160;&#30340;&#20027;&#21160;&#25299;&#25169;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20248;&#20808;&#32771;&#34385;&#26368;&#32039;&#24613;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous robots exploring unknown areas face a significant challenge -- navigating effectively without prior maps and with limited external feedback. This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail. In this paper, we introduce TopoNav, a novel framework that empowers robots to overcome these constraints and achieve efficient, adaptable, and goal-oriented exploration. TopoNav's fundamental building blocks are active topological mapping, intrinsic reward mechanisms, and hierarchical objective prioritization. Throughout its exploration, TopoNav constructs a dynamic topological map that captures key locations and pathways. It utilizes intrinsic rewards to guide the robot towards designated sub-goals within this map, fostering structured exploration even in sparse reward settings. To ensure efficient navigation, TopoNav employs the Hierarchical Objective-Driven Active Topologies framework, enabling the robot to prioritize immed
&lt;/p&gt;</description></item><item><title>COA-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;&#34892;&#21160;&#26041;&#26696;&#30340;&#31639;&#27861;&#65292;&#23427;&#34701;&#21512;&#20102;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#22312;&#20891;&#20107;&#28216;&#25103;&#20013;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#24555;&#36895;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;COAs&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.01786</link><description>&lt;p&gt;
COA-GPT&#65306;&#29992;&#20110;&#20891;&#20107;&#34892;&#21160;&#20013;&#21152;&#36895;&#34892;&#21160;&#26041;&#26696;&#24320;&#21457;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01786
&lt;/p&gt;
&lt;p&gt;
COA-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;&#34892;&#21160;&#26041;&#26696;&#30340;&#31639;&#27861;&#65292;&#23427;&#34701;&#21512;&#20102;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#22312;&#20891;&#20107;&#28216;&#25103;&#20013;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#24555;&#36895;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;COAs&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20891;&#20107;&#34892;&#21160;&#20013;&#34892;&#21160;&#26041;&#26696;&#65288;COAs&#65289;&#30340;&#24320;&#21457;&#20256;&#32479;&#19978;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#22797;&#26434;&#30340;&#36807;&#31243;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;COA-GPT&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;COAs&#30340;&#26032;&#31639;&#27861;&#12290;COA-GPT&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23558;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#34701;&#20837;&#21040;LLMs&#20013;&#65292;&#20801;&#35768;&#25351;&#25381;&#23448;&#36755;&#20837;&#20219;&#21153;&#20449;&#24687;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#20687;&#26684;&#24335;&#65289;&#65292;&#24182;&#33719;&#24471;&#19982;&#25112;&#30053;&#23545;&#40784;&#30340;COAs&#20197;&#20379;&#23457;&#26597;&#21644;&#25209;&#20934;&#12290;&#29420;&#29305;&#30340;&#26159;&#65292;COA-GPT&#19981;&#20165;&#21152;&#36895;&#20102;COA&#30340;&#24320;&#21457;&#65292;&#22312;&#20960;&#31186;&#38047;&#20869;&#29983;&#25104;&#21021;&#22987;COAs&#65292;&#36824;&#33021;&#26681;&#25454;&#25351;&#25381;&#23448;&#30340;&#21453;&#39304;&#23454;&#26102;&#31934;&#32454;&#21270;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#22312;&#12298;&#26143;&#38469;&#20105;&#38712;II&#12299;&#28216;&#25103;&#30340;&#20891;&#20107;&#30456;&#20851;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;COA-GPT&#65292;&#23558;&#20854;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;COA-GPT&#22312;&#26356;&#24555;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;&#30340;COAs&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, 
&lt;/p&gt;</description></item><item><title>HQ-VAE&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#22312;&#20998;&#23618;&#32467;&#26500;&#20013;&#38543;&#26426;&#23398;&#20064;&#31163;&#25955;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;VQ-VAE&#20013;&#30340;&#30721;&#20070;/&#23618;&#22349;&#22604;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.00365</link><description>&lt;p&gt;
HQ-VAE&#65306;&#20855;&#26377;&#21464;&#20998;&#36125;&#21494;&#26031;&#30340;&#20998;&#23618;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HQ-VAE: Hierarchical Discrete Representation Learning with Variational Bayes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00365
&lt;/p&gt;
&lt;p&gt;
HQ-VAE&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#22312;&#20998;&#23618;&#32467;&#26500;&#20013;&#38543;&#26426;&#23398;&#20064;&#31163;&#25955;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;VQ-VAE&#20013;&#30340;&#30721;&#20070;/&#23618;&#22349;&#22604;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#26159;&#19968;&#31181;&#30830;&#23450;&#24615;&#23398;&#20064;&#20855;&#26377;&#31163;&#25955;&#30721;&#20070;&#34920;&#31034;&#30340;&#29305;&#24449;&#30340;&#25216;&#26415;&#12290;&#36890;&#24120;&#36890;&#36807;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#27169;&#22411; VQ-VAE &#26469;&#25191;&#34892;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25193;&#23637;&#21040;&#20998;&#23618;&#32467;&#26500;&#20197;&#36827;&#34892;&#39640;&#20445;&#30495;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;VQ-VAE &#30340;&#36825;&#31181;&#20998;&#23618;&#25193;&#23637;&#32463;&#24120;&#21463;&#21040;&#30721;&#20070;/&#23618;&#22349;&#22604;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#20854;&#20013;&#30721;&#20070;&#26410;&#34987;&#26377;&#25928;&#22320;&#29992;&#26469;&#34920;&#36798;&#25968;&#25454;&#65292;&#20174;&#32780;&#38477;&#20302;&#37325;&#24314;&#31934;&#24230;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#38543;&#26426;&#23398;&#20064;&#20998;&#23618;&#31163;&#25955;&#34920;&#31034;&#65292;&#31216;&#20026;&#20998;&#23618;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;HQ-VAE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00365v2 Announce Type: replace-cross  Abstract: Vector quantization (VQ) is a technique to deterministically learn features with discrete codebook representations. It is commonly performed with a variational autoencoding model, VQ-VAE, which can be further extended to hierarchical structures for making high-fidelity reconstructions. However, such hierarchical extensions of VQ-VAE often suffer from the codebook/layer collapse issue, where the codebook is not efficiently used to express the data, and hence degrades reconstruction accuracy. To mitigate this problem, we propose a novel unified framework to stochastically learn hierarchical discrete representation on the basis of the variational Bayes framework, called hierarchically quantized variational autoencoder (HQ-VAE). HQ-VAE naturally generalizes the hierarchical variants of VQ-VAE, such as VQ-VAE-2 and residual-quantized VAE (RQ-VAE), and provides them with a Bayesian training scheme. Our comprehensive experiments on im
&lt;/p&gt;</description></item><item><title>SkillDiffuser&#36890;&#36807;&#23558;&#21487;&#35299;&#37322;&#30340;&#25216;&#33021;&#23398;&#20064;&#19982;&#26465;&#20214;&#25193;&#25955;&#35268;&#21010;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#39640;&#23618;&#25351;&#20196;&#19979;&#29983;&#25104;&#36830;&#36143;&#36712;&#36857;&#30340;&#20998;&#23618;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2312.11598</link><description>&lt;p&gt;
SkillDiffuser: &#36890;&#36807;&#25216;&#33021;&#25277;&#35937;&#22312;&#22522;&#20110;&#25193;&#25955;&#30340;&#20219;&#21153;&#25191;&#34892;&#20013;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20998;&#23618;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11598
&lt;/p&gt;
&lt;p&gt;
SkillDiffuser&#36890;&#36807;&#23558;&#21487;&#35299;&#37322;&#30340;&#25216;&#33021;&#23398;&#20064;&#19982;&#26465;&#20214;&#25193;&#25955;&#35268;&#21010;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#39640;&#23618;&#25351;&#20196;&#19979;&#29983;&#25104;&#36830;&#36143;&#36712;&#36857;&#30340;&#20998;&#23618;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#20154;&#36712;&#36857;&#35268;&#21010;&#26041;&#38754;&#30340;&#24378;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20174;&#39640;&#23618;&#25351;&#20196;&#29983;&#25104;&#36830;&#36143;&#30340;&#36712;&#36857;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38656;&#35201;&#22810;&#20010;&#39034;&#24207;&#25216;&#33021;&#30340;&#38271;&#36317;&#31163;&#32452;&#21512;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SkillDiffuser&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20998;&#23618;&#35268;&#21010;&#26694;&#26550;&#65292;&#23558;&#21487;&#35299;&#37322;&#30340;&#25216;&#33021;&#23398;&#20064;&#19982;&#26465;&#20214;&#25193;&#25955;&#35268;&#21010;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#22312;&#36739;&#39640;&#23618;&#27425;&#65292;&#25216;&#33021;&#25277;&#35937;&#27169;&#22359;&#20174;&#35270;&#35273;&#35266;&#23519;&#21644;&#35821;&#35328;&#25351;&#20196;&#20013;&#23398;&#20064;&#31163;&#25955;&#30340;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#25216;&#33021;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#23884;&#20837;&#34987;&#29992;&#26469;&#26465;&#20214;&#21270;&#25193;&#25955;&#27169;&#22411;&#65292;&#29983;&#25104;&#19982;&#25216;&#33021;&#23545;&#40784;&#30340;&#23450;&#21046;&#28508;&#22312;&#36712;&#36857;&#12290;&#36825;&#20801;&#35768;&#29983;&#25104;&#31526;&#21512;&#21487;&#23398;&#20064;&#25216;&#33021;&#30340;&#22810;&#26679;&#29366;&#24577;&#36712;&#36857;&#12290;&#36890;&#36807;&#23558;&#25216;&#33021;&#23398;&#20064;&#19982;&#26465;&#20214;&#36712;&#36857;&#29983;&#25104;&#30456;&#32467;&#21512;&#65292;SkillDiffuser&#20135;&#29983;&#36830;&#36143;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11598v2 Announce Type: replace-cross  Abstract: Diffusion models have demonstrated strong potential for robotic trajectory planning. However, generating coherent trajectories from high-level instructions remains challenging, especially for long-range composition tasks requiring multiple sequential skills. We propose SkillDiffuser, an end-to-end hierarchical planning framework integrating interpretable skill learning with conditional diffusion planning to address this problem. At the higher level, the skill abstraction module learns discrete, human-understandable skill representations from visual observations and language instructions. These learned skill embeddings are then used to condition the diffusion model to generate customized latent trajectories aligned with the skills. This allows generating diverse state trajectories that adhere to the learnable skills. By integrating skill learning with conditional trajectory generation, SkillDiffuser produces coherent behavior fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25361;&#25112;&#20102;&#23454;&#20307;&#30456;&#20284;&#24615;&#22312;&#22270;&#20013;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#33258;&#28982;&#21453;&#26144;&#30340;&#20027;&#27969;&#20551;&#35774;&#65292;&#36890;&#36807;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#34913;&#37327;&#36825;&#31181;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2312.10370</link><description>&lt;p&gt;
&#30456;&#20284;&#30340;&#23454;&#20307;&#26159;&#21542;&#20855;&#26377;&#30456;&#20284;&#30340;&#23884;&#20837;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Similar Entities have Similar Embeddings?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;&#23454;&#20307;&#30456;&#20284;&#24615;&#22312;&#22270;&#20013;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#33258;&#28982;&#21453;&#26144;&#30340;&#20027;&#27969;&#20551;&#35774;&#65292;&#36890;&#36807;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#34913;&#37327;&#36825;&#31181;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#32780;&#24320;&#21457;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#65288;KGEMs&#65289;&#23398;&#20064;&#30693;&#35782;&#22270;&#20013;&#23454;&#20307;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#21363;&#23884;&#20837;&#12290;&#19968;&#20010;&#26222;&#36941;&#30340;&#40664;&#35748;&#20551;&#35774;&#26159;KGE&#23454;&#20307;&#30456;&#20284;&#24615;&#20551;&#35774;&#65292;&#21363;&#36825;&#20123;KGEMs&#22312;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#20445;&#30041;&#22270;&#30340;&#32467;&#26500;&#65292;&#21363;&#23558;&#30456;&#20284;&#30340;&#23454;&#20307;&#25918;&#22312;&#22270;&#20013;&#24444;&#27492;&#38752;&#36817;&#12290;&#36825;&#31181;&#29702;&#24819;&#30340;&#24615;&#36136;&#20351;&#24471;KGEMs&#22312;&#25512;&#33616;&#31995;&#32479;&#25110;&#33647;&#29289;&#20877;&#21033;&#29992;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23454;&#20307;&#30340;&#30456;&#20284;&#24615;&#19982;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#24456;&#23569;&#34987;&#27491;&#24335;&#35780;&#20272;&#12290;&#36890;&#24120;&#65292;KGEMs&#26159;&#22522;&#20110;&#20854;&#21807;&#19968;&#30340;&#38142;&#25509;&#39044;&#27979;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#30340;&#65292;&#20351;&#29992;&#31867;&#20284;Hits@K&#25110;Mean Rank&#30340;&#25490;&#21517;&#25351;&#26631;&#12290;&#26412;&#25991;&#36136;&#30097;&#20102;&#22270;&#20013;&#30340;&#23454;&#20307;&#30456;&#20284;&#24615;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#22825;&#28982;&#21453;&#26144;&#36825;&#19968;&#27969;&#34892;&#20551;&#35774;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#34913;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10370v2 Announce Type: replace  Abstract: Knowledge graph embedding models (KGEMs) developed for link prediction learn vector representations for entities in a knowledge graph, known as embeddings. A common tacit assumption is the KGE entity similarity assumption, which states that these KGEMs retain the graph's structure within their embedding space, \textit{i.e.}, position similar entities within the graph close to one another. This desirable property make KGEMs widely used in downstream tasks such as recommender systems or drug repurposing. Yet, the relation of entity similarity and similarity in the embedding space has rarely been formally evaluated. Typically, KGEMs are assessed based on their sole link prediction capabilities, using ranked-based metrics such as Hits@K or Mean Rank. This paper challenges the prevailing assumption that entity similarity in the graph is inherently mirrored in the embedding space. Therefore, we conduct extensive experiments to measure the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#24320;&#25918;&#25968;&#25454;&#38598;&#21644;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#30340;&#26426;&#22120;&#21487;&#35835;&#25991;&#26723;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#29992;&#24615;&#65292;&#31616;&#21270;&#25968;&#25454;&#38598;&#35780;&#20272;&#36807;&#31243;&#65292;&#20419;&#36827;&#26356;&#21487;&#38752;&#30340;&#25968;&#25454;&#24212;&#29992;&#65292;&#20174;&#32780;&#22521;&#32946;&#26356;&#36127;&#36131;&#20219;&#21644;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2312.06153</link><description>&lt;p&gt;
&#24320;&#25918;&#25968;&#25454;&#34920;&#65306;&#38754;&#21521;&#24320;&#25918;&#25968;&#25454;&#38598;&#21644;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#30340;&#26426;&#22120;&#21487;&#35835;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
Open Datasheets: Machine-readable Documentation for Open Datasets and Responsible AI Assessments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#24320;&#25918;&#25968;&#25454;&#38598;&#21644;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#30340;&#26426;&#22120;&#21487;&#35835;&#25991;&#26723;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#29992;&#24615;&#65292;&#31616;&#21270;&#25968;&#25454;&#38598;&#35780;&#20272;&#36807;&#31243;&#65292;&#20419;&#36827;&#26356;&#21487;&#38752;&#30340;&#25968;&#25454;&#24212;&#29992;&#65292;&#20174;&#32780;&#22521;&#32946;&#26356;&#36127;&#36131;&#20219;&#21644;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#20195;&#30721;&#30340;&#12289;&#38754;&#21521;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#26426;&#22120;&#21487;&#35835;&#25991;&#26723;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#65288;RAI&#65289;&#32771;&#34385;&#22240;&#32032;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#25552;&#39640;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#29992;&#24615;&#65292;&#20419;&#36827;&#26356;&#23481;&#26131;&#22320;&#21457;&#29616;&#21644;&#20351;&#29992;&#25968;&#25454;&#38598;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#20869;&#23481;&#21644;&#19978;&#19979;&#25991;&#65292;&#20197;&#21450;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26088;&#22312;&#31616;&#21270;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20854;&#20182;&#24320;&#25918;&#25968;&#25454;&#29992;&#25143;&#24555;&#36895;&#35782;&#21035;&#31526;&#21512;&#20854;&#38656;&#27714;&#21644;&#32452;&#32455;&#25919;&#31574;&#25110;&#27861;&#35268;&#30340;&#25968;&#25454;&#38598;&#12290;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#26694;&#26550;&#30340;&#23454;&#26045;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#22823;&#21270;&#20854;&#28508;&#21147;&#30340;&#24314;&#35758;&#12290;&#39044;&#26399;&#35813;&#26694;&#26550;&#23558;&#22686;&#24378;&#22312;&#30740;&#31350;&#21644;&#20915;&#31574;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#20419;&#36827;&#26356;&#36127;&#36131;&#20219;&#12289;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06153v2 Announce Type: replace-cross  Abstract: This paper introduces a no-code, machine-readable documentation framework for open datasets, with a focus on responsible AI (RAI) considerations. The framework aims to improve comprehensibility, and usability of open datasets, facilitating easier discovery and use, better understanding of content and context, and evaluation of dataset quality and accuracy. The proposed framework is designed to streamline the evaluation of datasets, helping researchers, data scientists, and other open data users quickly identify datasets that meet their needs and organizational policies or regulations. The paper also discusses the implementation of the framework and provides recommendations to maximize its potential. The framework is expected to enhance the quality and reliability of data used in research and decision-making, fostering the development of more responsible and trustworthy AI systems.
&lt;/p&gt;</description></item><item><title>MMM &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#36816;&#21160;&#27169;&#22411;&#30340;&#26032;&#39062;&#36816;&#21160;&#29983;&#25104;&#33539;&#24335;&#65292;&#36890;&#36807;&#36816;&#21160;&#26631;&#35760;&#22120;&#21644;&#26465;&#20214;&#36974;&#34109;&#36816;&#21160;&#21464;&#25442;&#22120;&#65292;&#22312;&#23454;&#26102;&#24615;&#33021;&#12289;&#39640;&#20445;&#30495;&#24230;&#21644;&#36816;&#21160;&#21487;&#32534;&#36753;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2312.03596</link><description>&lt;p&gt;
MMM&#65306;&#29983;&#25104;&#24335;&#36974;&#34109;&#36816;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MMM: Generative Masked Motion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03596
&lt;/p&gt;
&lt;p&gt;
MMM &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#36816;&#21160;&#27169;&#22411;&#30340;&#26032;&#39062;&#36816;&#21160;&#29983;&#25104;&#33539;&#24335;&#65292;&#36890;&#36807;&#36816;&#21160;&#26631;&#35760;&#22120;&#21644;&#26465;&#20214;&#36974;&#34109;&#36816;&#21160;&#21464;&#25442;&#22120;&#65292;&#22312;&#23454;&#26102;&#24615;&#33021;&#12289;&#39640;&#20445;&#30495;&#24230;&#21644;&#36816;&#21160;&#21487;&#32534;&#36753;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#20351;&#29992;&#25193;&#25955;&#21644;&#33258;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#36816;&#21160;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#22312;&#23454;&#26102;&#24615;&#33021;&#12289;&#39640;&#20445;&#30495;&#24230;&#21644;&#36816;&#21160;&#21487;&#32534;&#36753;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MMM&#65292;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#36816;&#21160;&#27169;&#22411;&#30340;&#26032;&#39062;&#32780;&#31616;&#21333;&#30340;&#36816;&#21160;&#29983;&#25104;&#33539;&#24335;&#12290;MMM&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;&#65288;1&#65289;&#36816;&#21160;&#26631;&#35760;&#22120;&#65292;&#23558;3D&#20154;&#20307;&#36816;&#21160;&#36716;&#21270;&#20026;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#19968;&#31995;&#21015;&#31163;&#25955;&#26631;&#35760;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26465;&#20214;&#36974;&#34109;&#36816;&#21160;&#21464;&#25442;&#22120;&#65292;&#23398;&#20064;&#39044;&#27979;&#22312;&#39044;&#20808;&#35745;&#31639;&#30340;&#25991;&#26412;&#26631;&#35760;&#30340;&#26465;&#20214;&#19979;&#38543;&#26426;&#36974;&#34109;&#30340;&#36816;&#21160;&#26631;&#35760;&#12290;&#36890;&#36807;&#22312;&#25152;&#26377;&#26041;&#21521;&#19978;&#20851;&#27880;&#36816;&#21160;&#21644;&#25991;&#26412;&#26631;&#35760;&#65292;MMM&#26126;&#30830;&#22320;&#25429;&#33719;&#20102;&#36816;&#21160;&#26631;&#35760;&#20043;&#38388;&#30340;&#22266;&#26377;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#36816;&#21160;&#21644;&#25991;&#26412;&#26631;&#35760;&#20043;&#38388;&#30340;&#35821;&#20041;&#26144;&#23556;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#36825;&#20801;&#35768;&#23545;&#19982;fi&#39640;&#24230;&#19968;&#33268;&#30340;&#22810;&#20010;&#36816;&#21160;&#26631;&#35760;&#36827;&#34892;&#24182;&#34892;&#21644;&#36845;&#20195;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03596v2 Announce Type: replace-cross  Abstract: Recent advances in text-to-motion generation using diffusion and autoregressive models have shown promising results. However, these models often suffer from a trade-off between real-time performance, high fidelity, and motion editability. To address this gap, we introduce MMM, a novel yet simple motion generation paradigm based on Masked Motion Model. MMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into a sequence of discrete tokens in latent space, and (2) a conditional masked motion transformer that learns to predict randomly masked motion tokens, conditioned on the pre-computed text tokens. By attending to motion and text tokens in all directions, MMM explicitly captures inherent dependency among motion tokens and semantic mapping between motion and text tokens. During inference, this allows parallel and iterative decoding of multiple motion tokens that are highly consistent with fi
&lt;/p&gt;</description></item><item><title>HybridNeRF&#26041;&#27861;&#23558;&#22823;&#22810;&#25968;&#23545;&#35937;&#21576;&#29616;&#20026;&#34920;&#38754;&#65292;&#20165;&#23545;&#23569;&#37096;&#20998;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21306;&#22495;&#36827;&#34892;&#20307;&#31215;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#28210;&#26579;&#30340;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.03160</link><description>&lt;p&gt;
HybridNeRF&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#20307;&#31215;&#34920;&#38754;&#23454;&#29616;&#39640;&#25928;&#31070;&#32463;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;
HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03160
&lt;/p&gt;
&lt;p&gt;
HybridNeRF&#26041;&#27861;&#23558;&#22823;&#22810;&#25968;&#23545;&#35937;&#21576;&#29616;&#20026;&#34920;&#38754;&#65292;&#20165;&#23545;&#23569;&#37096;&#20998;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21306;&#22495;&#36827;&#34892;&#20307;&#31215;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#28210;&#26579;&#30340;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#22270;&#21512;&#25104;&#36136;&#37327;&#65292;&#20294;&#28210;&#26579;&#36895;&#24230;&#36739;&#24930;&#12290;&#19968;&#20010;&#21407;&#22240;&#26159;&#23427;&#20204;&#21033;&#29992;&#20307;&#32032;&#28210;&#26579;&#65292;&#22312;&#28210;&#26579;&#26102;&#38656;&#35201;&#27599;&#20010;&#20809;&#32447;&#36827;&#34892;&#35768;&#22810;&#37319;&#26679;&#65288;&#21644;&#27169;&#22411;&#26597;&#35810;&#65289;&#12290;&#23613;&#31649;&#36825;&#31181;&#34920;&#31034;&#28789;&#27963;&#19988;&#26131;&#20110;&#20248;&#21270;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#23454;&#19990;&#30028;&#30340;&#23545;&#35937;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#29992;&#34920;&#38754;&#32780;&#19981;&#26159;&#20307;&#31215;&#24314;&#27169;&#65292;&#20174;&#32780;&#27599;&#20010;&#20809;&#32447;&#38656;&#35201;&#26356;&#23569;&#30340;&#37319;&#26679;&#12290;&#36825;&#19968;&#35266;&#23519;&#20419;&#25104;&#20102;&#23545;&#34920;&#38754;&#34920;&#31034;&#30340;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#22914;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65292;&#20294;&#36825;&#20123;&#21487;&#33021;&#38590;&#20197;&#24314;&#27169;&#21322;&#36879;&#26126;&#21644;&#34180;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;HybridNeRF&#65292;&#36890;&#36807;&#23558;&#22823;&#22810;&#25968;&#23545;&#35937;&#21576;&#29616;&#20026;&#34920;&#38754;&#65292;&#21516;&#26102;&#23545;&#65288;&#36890;&#24120;&#65289;&#23567;&#37096;&#20998;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21306;&#22495;&#36827;&#34892;&#20307;&#31215;&#24314;&#27169;&#65292;&#20174;&#32780;&#21033;&#29992;&#36825;&#20004;&#31181;&#34920;&#31034;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;HybridNeRF&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;Eyeful Tower&#25968;&#25454;&#38598;&#20197;&#21450;&#20854;&#20182;&#24120;&#29992;&#35270;&#22270;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03160v2 Announce Type: replace-cross  Abstract: Neural radiance fields provide state-of-the-art view synthesis quality but tend to be slow to render. One reason is that they make use of volume rendering, thus requiring many samples (and model queries) per ray at render time. Although this representation is flexible and easy to optimize, most real-world objects can be modeled more efficiently with surfaces instead of volumes, requiring far fewer samples per ray. This observation has spurred considerable progress in surface representations such as signed distance functions, but these may struggle to model semi-opaque and thin structures. We propose a method, HybridNeRF, that leverages the strengths of both representations by rendering most objects as surfaces while modeling the (typically) small fraction of challenging regions volumetrically. We evaluate HybridNeRF against the challenging Eyeful Tower dataset along with other commonly used view synthesis datasets. When compari
&lt;/p&gt;</description></item><item><title>sGMC&#27169;&#22411;&#20316;&#20026;LASSO&#30340;&#38750;&#20984;&#27491;&#21017;&#21270;&#26367;&#20195;&#21697;&#65292;&#22312;&#20445;&#30041;LASSO&#27169;&#22411;&#20248;&#21183;&#30340;&#21516;&#26102;&#65292;&#20854;&#35299;&#38598;&#20960;&#20309;&#12289;&#35299;&#21807;&#19968;&#24615;&#21644;&#31232;&#30095;&#24615;&#19982;LASSO&#27169;&#22411;&#20855;&#26377;&#30456;&#20284;&#19988;&#20248;&#38597;&#30340;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.18438</link><description>&lt;p&gt;
&#19968;&#20010;&#38750;&#20984;&#27491;&#21017;&#21270;&#20984;&#31232;&#30095;&#27169;&#22411;&#30340;&#35299;&#38598;&#20960;&#20309;&#19982;&#27491;&#21017;&#21270;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Solution-Set Geometry and Regularization Path of a Nonconvexly Regularized Convex Sparse Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18438
&lt;/p&gt;
&lt;p&gt;
sGMC&#27169;&#22411;&#20316;&#20026;LASSO&#30340;&#38750;&#20984;&#27491;&#21017;&#21270;&#26367;&#20195;&#21697;&#65292;&#22312;&#20445;&#30041;LASSO&#27169;&#22411;&#20248;&#21183;&#30340;&#21516;&#26102;&#65292;&#20854;&#35299;&#38598;&#20960;&#20309;&#12289;&#35299;&#21807;&#19968;&#24615;&#21644;&#31232;&#30095;&#24615;&#19982;LASSO&#27169;&#22411;&#20855;&#26377;&#30456;&#20284;&#19988;&#20248;&#38597;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#26497;&#23567;&#26497;&#22823;&#20985;&#65288;GMC&#65289;&#24809;&#32602;&#26159;&#19968;&#31181;&#38750;&#20984;&#31232;&#30095;&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#20197;&#20445;&#25345;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#30340;&#25972;&#20307;&#20984;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;GMC&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#23454;&#20363;&#65292;&#31216;&#20026;&#32553;&#25918;GMC&#65288;sGMC&#65289;&#65292;&#24182;&#23601;&#20854;&#35299;&#38598;&#20960;&#20309;&#21644;&#27491;&#21017;&#21270;&#36335;&#24452;&#25552;&#20986;&#21508;&#31181;&#26174;&#33879;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#34429;&#28982;sGMC&#24809;&#32602;&#26159;LASSO&#24809;&#32602;&#30340;&#38750;&#20984;&#25193;&#23637;&#65288;&#21363;$\ell_1$&#33539;&#25968;&#65289;&#65292;&#20294;sGMC&#27169;&#22411;&#20445;&#30041;&#20102;LASSO&#27169;&#22411;&#30340;&#35768;&#22810;&#33879;&#21517;&#29305;&#24615;&#65292;&#22240;&#27492;&#21487;&#20197;&#20316;&#20026;LASSO&#30340;&#19968;&#20010;&#20559;&#24046;&#36739;&#23567;&#30340;&#26367;&#20195;&#21697;&#32780;&#19981;&#20250;&#22833;&#21435;&#20854;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#22266;&#23450;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;$\lambda$&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;sGMC&#27169;&#22411;&#30340;&#35299;&#38598;&#20960;&#20309;&#12289;&#35299;&#21807;&#19968;&#24615;&#21644;&#31232;&#30095;&#24615;&#21487;&#20197;&#20197;&#19968;&#31181;&#31867;&#20284;&#20248;&#38597;&#30340;&#26041;&#24335;&#21051;&#30011;&#20026;LASSO&#27169;&#22411;&#65288;&#21442;&#35265;&#65292;&#20363;&#22914;&#65292;Osborne&#31561;&#20154;2000&#24180;&#65292;R. J. Tibshirani 2013&#24180;&#65289;&#12290;&#23545;&#20110;&#21464;&#21270;&#30340;$\lambda$&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18438v2 Announce Type: cross  Abstract: The generalized minimax concave (GMC) penalty is a nonconvex sparse regularizer which can preserve the overall-convexity of the regularized least-squares problem. In this paper, we focus on a significant instance of the GMC model termed scaled GMC (sGMC), and present various notable findings on its solution-set geometry and regularization path. Our investigation indicates that while the sGMC penalty is a nonconvex extension of the LASSO penalty (i.e., the $\ell_1$-norm), the sGMC model preserves many celebrated properties of the LASSO model, hence can serve as a less biased surrogate of LASSO without losing its advantages. Specifically, for a fixed regularization parameter $\lambda$, we show that the solution-set geometry, solution uniqueness and sparseness of the sGMC model can be characterized in a similar elegant way to the LASSO model (see, e.g., Osborne et al. 2000, R. J. Tibshirani 2013). For a varying $\lambda$, we prove that th
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#39118;&#38505;&#25511;&#21046;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#20449;&#24687;&#39118;&#38505;&#24230;&#37327;&#26063;&#30340;&#19978;&#38480;&#36873;&#21462;&#25552;&#31034;&#65292;&#24110;&#21161;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36127;&#36131;&#37096;&#32626;&#36807;&#31243;&#20013;&#20135;&#29983;&#24847;&#22806;&#31967;&#31957;&#21709;&#24212;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2311.13628</link><description>&lt;p&gt;
&#25552;&#31034;&#39118;&#38505;&#25511;&#21046;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36127;&#36131;&#37096;&#32626;&#30340;&#20005;&#26684;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13628
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#39118;&#38505;&#25511;&#21046;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#20449;&#24687;&#39118;&#38505;&#24230;&#37327;&#26063;&#30340;&#19978;&#38480;&#36873;&#21462;&#25552;&#31034;&#65292;&#24110;&#21161;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36127;&#36131;&#37096;&#32626;&#36807;&#31243;&#20013;&#20135;&#29983;&#24847;&#22806;&#31967;&#31957;&#21709;&#24212;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#29190;&#28856;&#24335;&#22686;&#38271;&#24341;&#21457;&#20102;&#23545;&#22914;&#20309;&#26368;&#22909;&#22320;&#25552;&#31034;&#27169;&#22411;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#20852;&#36259;&#28010;&#28526;&#12290;&#36873;&#25321;&#19968;&#20010;&#22522;&#20110;&#39564;&#35777;&#38598;&#19978;&#24179;&#22343;&#24615;&#33021;&#30340;&#25552;&#31034;&#21487;&#33021;&#24456;&#35825;&#20154;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#29983;&#25104;&#20986;&#20046;&#24847;&#26009;&#30340;&#31967;&#31957;&#21709;&#24212;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22788;&#22659;&#26368;&#22256;&#38590;&#30340;&#29992;&#25143;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#25552;&#31034;&#39118;&#38505;&#25511;&#21046;&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#26681;&#25454;&#20449;&#24687;&#39118;&#38505;&#24230;&#37327;&#26063;&#30340;&#20005;&#26684;&#19978;&#38480;&#36873;&#25321;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#20135;&#29983;&#22810;&#31181;&#24230;&#37327;&#19978;&#38480;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#34913;&#37327;&#26368;&#22351;&#24773;&#20917;&#21709;&#24212;&#21644;&#29992;&#25143;&#32676;&#20307;&#29983;&#25104;&#36136;&#37327;&#19981;&#22343;&#34913;&#30340;&#37327;&#65292;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22522;&#30784;&#32479;&#35745;&#30028;&#23450;&#25216;&#26415;&#65292;&#20197;&#36866;&#24212;&#37096;&#32626;&#20013;&#20998;&#24067;&#21464;&#21270;&#21487;&#33021;&#24615;&#30340;&#24773;&#20917;&#12290;&#22312;&#24320;&#25918;&#24335;&#32842;&#22825;&#12289;&#21307;&#23398;&#38382;&#39064;&#31561;&#24212;&#29992;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13628v2 Announce Type: replace-cross  Abstract: The recent explosion in the capabilities of large language models has led to a wave of interest in how best to prompt a model to perform a given task. While it may be tempting to simply choose a prompt based on average performance on a validation set, this can lead to a deployment where unexpectedly poor responses are generated, especially for the worst-off users. To mitigate this prospect, we propose Prompt Risk Control, a lightweight framework for selecting a prompt based on rigorous upper bounds on families of informative risk measures. We offer methods for producing bounds on a diverse set of metrics, including quantities that measure worst-case responses and disparities in generation quality across the population of users. In addition, we extend the underlying statistical bounding techniques to accommodate the possibility of distribution shifts in deployment. Experiments on applications such as open-ended chat, medical que
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#29289;&#29702;&#23398;&#27169;&#25311;&#19982; NeRF &#32467;&#21512;&#65292;&#26080;&#38656;&#20013;&#38388;&#24418;&#24577;&#20195;&#29702;&#65292;&#36890;&#36807; Q-GMLS &#25429;&#25417;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#22823;&#24418;&#21464;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#24377;&#24615;&#21160;&#21147;&#23398;&#29983;&#25104;&#65292;&#24182;&#36866;&#24212; NeRF &#23494;&#24230;&#22330;&#35843;&#25972;&#26368;&#23567;&#20108;&#20056;&#26680;&#65292;&#20174;&#32780;&#39640;&#25928;&#21512;&#25104;&#21508;&#31181;&#39640;&#24377;&#24615;&#26448;&#26009;&#30340;&#29289;&#29702;&#36924;&#30495;&#21160;&#30011;&#12290;</title><link>https://arxiv.org/abs/2311.13099</link><description>&lt;p&gt;
PIE-NeRF: &#20351;&#29992; NeRF &#36827;&#34892;&#22522;&#20110;&#29289;&#29702;&#30340;&#20132;&#20114;&#24377;&#24615;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#29289;&#29702;&#23398;&#27169;&#25311;&#19982; NeRF &#32467;&#21512;&#65292;&#26080;&#38656;&#20013;&#38388;&#24418;&#24577;&#20195;&#29702;&#65292;&#36890;&#36807; Q-GMLS &#25429;&#25417;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#22823;&#24418;&#21464;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#24377;&#24615;&#21160;&#21147;&#23398;&#29983;&#25104;&#65292;&#24182;&#36866;&#24212; NeRF &#23494;&#24230;&#22330;&#35843;&#25972;&#26368;&#23567;&#20108;&#20056;&#26680;&#65292;&#20174;&#32780;&#39640;&#25928;&#21512;&#25104;&#21508;&#31181;&#39640;&#24377;&#24615;&#26448;&#26009;&#30340;&#29289;&#29702;&#36924;&#30495;&#21160;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#29289;&#29702;&#23398;&#27169;&#25311;&#19982; NeRF &#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#65292;&#29992;&#20110;&#29983;&#25104;&#30495;&#23454;&#29289;&#20307;&#30340;&#39640;&#36136;&#37327;&#24377;&#24615;&#21160;&#21147;&#23398;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20197;&#26080;&#32593;&#26684;&#30340;&#26041;&#24335;&#31163;&#25955;&#21270;&#38750;&#32447;&#24615;&#36229;&#24377;&#24615;&#65292;&#36991;&#20813;&#20102;&#20013;&#38388;&#36741;&#21161;&#24418;&#24577;&#20195;&#29702;&#29289;&#22914;&#22235;&#38754;&#20307;&#32593;&#26684;&#25110;&#20307;&#32032;&#32593;&#26684;&#30340;&#24517;&#35201;&#24615;&#12290;&#37319;&#29992;&#20108;&#27425;&#24191;&#20041;&#31227;&#21160;&#26368;&#23567;&#20108;&#20056;&#65288;Q-GMLS&#65289;&#26469;&#25429;&#25417;&#38544;&#24335;&#27169;&#22411;&#19978;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#21644;&#22823;&#24418;&#21464;&#12290;&#36825;&#31181;&#26080;&#32593;&#26684;&#38598;&#25104;&#20351;&#22797;&#26434;&#21644;&#20849;&#32500;&#24230;&#24418;&#29366;&#30340;&#22810;&#21151;&#33021;&#27169;&#25311;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#26681;&#25454; NeRF &#23494;&#24230;&#22330;&#33258;&#36866;&#24212;&#22320;&#25918;&#32622;&#26368;&#23567;&#20108;&#20056;&#26680;&#65292;&#26174;&#33879;&#38477;&#20302;&#38750;&#32447;&#24615;&#27169;&#25311;&#30340;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#20132;&#20114;&#36895;&#29575;&#21512;&#25104;&#21508;&#31181;&#39640;&#24377;&#24615;&#26448;&#26009;&#30340;&#29289;&#29702;&#36924;&#30495;&#21160;&#30011;&#12290;&#26377;&#20851;&#26356;&#22810;&#20449;&#24687;&#65292;&#35831;&#35775;&#38382;&#25105;&#20204;&#30340;&#39033;&#30446;&#39029;&#38754;https://fytalo
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13099v2 Announce Type: replace-cross  Abstract: We show that physics-based simulations can be seamlessly integrated with NeRF to generate high-quality elastodynamics of real-world objects. Unlike existing methods, we discretize nonlinear hyperelasticity in a meshless way, obviating the necessity for intermediate auxiliary shape proxies like a tetrahedral mesh or voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed to capture nonlinear dynamics and large deformation on the implicit model. Such meshless integration enables versatile simulations of complex and codimensional shapes. We adaptively place the least-square kernels according to the NeRF density field to significantly reduce the complexity of the nonlinear simulation. As a result, physically realistic animations can be conveniently synthesized using our method for a wide range of hyperelastic materials at an interactive rate. For more information, please visit our project page at https://fytalo
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#23376;&#39046;&#22495;&#65292;&#33268;&#21147;&#20110;&#35753;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#25968;&#25454;&#19978;&#19981;&#26029;&#23398;&#20064;&#65292;&#32780;&#19981;&#24536;&#35760;&#36807;&#21435;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#20869;&#23384;&#38480;&#21046;&#22330;&#26223;&#30340;&#20027;&#23548;&#22320;&#20301;&#65292;&#24182;&#35752;&#35770;&#20102;&#36830;&#32493;&#23398;&#20064;&#22312;&#35299;&#20915;&#27169;&#22411;&#32534;&#36753;&#12289;&#20010;&#24615;&#21270;&#12289;&#19987;&#19994;&#21270;&#12289;&#35774;&#22791;&#31471;&#23398;&#20064;&#12289;&#24555;&#36895;&#65288;&#37325;&#26032;&#65289;&#35757;&#32451;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.11908</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65306;&#24212;&#29992;&#19982;&#26410;&#26469;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Continual Learning: Applications and the Road Forward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11908
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#23376;&#39046;&#22495;&#65292;&#33268;&#21147;&#20110;&#35753;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#25968;&#25454;&#19978;&#19981;&#26029;&#23398;&#20064;&#65292;&#32780;&#19981;&#24536;&#35760;&#36807;&#21435;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#20869;&#23384;&#38480;&#21046;&#22330;&#26223;&#30340;&#20027;&#23548;&#22320;&#20301;&#65292;&#24182;&#35752;&#35770;&#20102;&#36830;&#32493;&#23398;&#20064;&#22312;&#35299;&#20915;&#27169;&#22411;&#32534;&#36753;&#12289;&#20010;&#24615;&#21270;&#12289;&#19987;&#19994;&#21270;&#12289;&#35774;&#22791;&#31471;&#23398;&#20064;&#12289;&#24555;&#36895;&#65288;&#37325;&#26032;&#65289;&#35757;&#32451;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#26088;&#22312;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#26032;&#25968;&#25454;&#19978;&#19981;&#26029;&#23398;&#20064;&#65292;&#36890;&#36807;&#31215;&#32047;&#30693;&#35782;&#32780;&#19981;&#36951;&#24536;&#36807;&#21435;&#25152;&#23398;&#12290;&#26412;&#30740;&#31350;&#36864;&#19968;&#27493;&#24605;&#32771;&#65292;&#24182;&#25552;&#20986;&#38382;&#39064;&#65306;&#8220;&#20026;&#20160;&#20040;&#39318;&#20808;&#35201;&#20851;&#27880;&#36830;&#32493;&#23398;&#20064;&#65311;&#8221;&#12290;&#25105;&#20204;&#36890;&#36807;&#23457;&#35270;&#36817;&#26399;&#22312;&#22235;&#20010;&#20027;&#35201;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#21457;&#34920;&#30340;&#36830;&#32493;&#23398;&#20064;&#35770;&#25991;&#26469;&#38138;&#22443;&#65292;&#23637;&#31034;&#20102;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#22330;&#26223;&#20027;&#23548;&#20102;&#35813;&#39046;&#22495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20116;&#20010;&#26410;&#35299;&#38382;&#39064;&#65292;&#23613;&#31649;&#20045;&#30475;&#36215;&#26469;&#21487;&#33021;&#19982;&#36830;&#32493;&#23398;&#20064;&#26080;&#20851;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#36830;&#32493;&#23398;&#20064;&#23558;&#24517;&#28982;&#25104;&#20026;&#23427;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#19968;&#37096;&#20998;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#27169;&#22411;&#32534;&#36753;&#12289;&#20010;&#24615;&#21270;&#21644;&#19987;&#19994;&#21270;&#12289;&#35774;&#22791;&#31471;&#23398;&#20064;&#12289;&#26356;&#24555;&#30340;&#65288;&#37325;&#26032;&#65289;&#35757;&#32451;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#27604;&#36739;&#36825;&#20123;&#26410;&#35299;&#38382;&#39064;&#30340;&#26399;&#26395;&#21644;&#24403;&#21069;&#30340;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11908v3 Announce Type: replace-cross  Abstract: Continual learning is a subfield of machine learning, which aims to allow machine learning models to continuously learn on new data, by accumulating knowledge without forgetting what was learned in the past. In this work, we take a step back, and ask: "Why should one care about continual learning in the first place?". We set the stage by examining recent continual learning papers published at four major machine learning conferences, and show that memory-constrained settings dominate the field. Then, we discuss five open problems in machine learning, and even though they might seem unrelated to continual learning at first sight, we show that continual learning will inevitably be part of their solution. These problems are model editing, personalization and specialization, on-device learning, faster (re-)training and reinforcement learning. Finally, by comparing the desiderata from these unsolved problems and the current assumptio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21270;&#24402;&#19968;&#21270;&#27969;&#30340;&#27010;&#29575;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#22815;&#25429;&#25417;&#21644;&#33391;&#22909;&#22806;&#25512;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.06958</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#21270;&#31354;&#38388;-&#26102;&#38388;&#24402;&#19968;&#21270;&#27969;&#30340;&#27010;&#29575;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards probabilistic Weather Forecasting with Conditioned Spatio-Temporal Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06958
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21270;&#24402;&#19968;&#21270;&#27969;&#30340;&#27010;&#29575;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#22815;&#25429;&#25417;&#21644;&#33391;&#22909;&#22806;&#25512;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#24402;&#19968;&#21270;&#27969;&#33021;&#22815;&#24314;&#27169;&#22810;&#27169;&#24577;&#31354;&#38388;&#20998;&#24067;&#65292;&#24050;&#32463;&#25104;&#21151;&#22320;&#27169;&#25311;&#20102;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#30001;&#20110;&#20854;&#35757;&#32451;&#31283;&#23450;&#24615;&#12289;&#21487;&#36870;&#24615;&#20197;&#21450;&#22312;&#37319;&#26679;&#21644;&#25512;&#26029;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#27604;&#20854;&#20182;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#20960;&#39033;&#22909;&#22788;&#12290;&#36825;&#20351;&#23427;&#20204;&#25104;&#20026;&#38543;&#26426;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#38382;&#39064;&#30340;&#21512;&#36866;&#20505;&#36873;&#32773;&#65292;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#37117;&#26222;&#36941;&#23384;&#22312;&#65292;&#22914;&#22320;&#29699;&#31185;&#23398;&#12289;&#22825;&#20307;&#29289;&#29702;&#23398;&#25110;&#20998;&#23376;&#31185;&#23398;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#38543;&#26426;&#31354;&#38388;-&#26102;&#38388;&#24314;&#27169;&#30340;&#26465;&#20214;&#21270;&#24402;&#19968;&#21270;&#27969;&#12290;&#35813;&#26041;&#27861;&#22312;&#20174;ERA5&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#26085;&#28201;&#24230;&#21644;&#23567;&#26102;&#31561;&#21387;&#22270;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30340;&#26102;&#38388;&#33539;&#22260;&#20043;&#22806;&#36827;&#34892;&#33391;&#22909;&#30340;&#22806;&#25512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06958v2 Announce Type: replace-cross  Abstract: Generative normalizing flows are able to model multimodal spatial distributions, and they have been shown to model temporal correlations successfully as well. These models provide several benefits over other types of generative models due to their training stability, invertibility and efficiency in sampling and inference. This makes them a suitable candidate for stochastic spatio-temporal prediction problems, which are omnipresent in many fields of sciences, such as earth sciences, astrophysics or molecular sciences. In this paper, we present conditional normalizing flows for stochastic spatio-temporal modelling. The method is evaluated on the task of daily temperature and hourly geopotential map prediction from ERA5 datasets. Experiments show that our method is able to capture spatio-temporal correlations and extrapolates well beyond the time horizon used during training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23454;&#26102;&#36882;&#24402;&#24378;&#21270;&#23398;&#20064;&#65288;RTRRL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20803;-&#24378;&#21270;&#23398;&#20064;RNN&#26550;&#26500;&#12289;&#22806;&#37096;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;RFLO&#23616;&#37096;&#22312;&#32447;&#23398;&#20064;&#65292;&#25104;&#21151;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;BPTT&#25110;RTRL&#26367;&#20195;RTRRL&#20013;&#30340;&#20248;&#21270;&#31639;&#27861;&#24182;&#19981;&#33021;&#25552;&#39640;&#22238;&#25253;&#12290;</title><link>https://arxiv.org/abs/2311.04830</link><description>&lt;p&gt;
&#23454;&#26102;&#36882;&#24402;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Real-Time Recurrent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23454;&#26102;&#36882;&#24402;&#24378;&#21270;&#23398;&#20064;&#65288;RTRRL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20803;-&#24378;&#21270;&#23398;&#20064;RNN&#26550;&#26500;&#12289;&#22806;&#37096;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;RFLO&#23616;&#37096;&#22312;&#32447;&#23398;&#20064;&#65292;&#25104;&#21151;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;BPTT&#25110;RTRL&#26367;&#20195;RTRRL&#20013;&#30340;&#20248;&#21270;&#31639;&#27861;&#24182;&#19981;&#33021;&#25552;&#39640;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#26102;&#36882;&#24402;&#24378;&#21270;&#23398;&#20064;&#65288;RTRRL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#20013;&#30340;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#36827;&#34892;&#27714;&#35299;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#26041;&#27861;&#12290;RTRRL&#30001;&#19977;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;1&#65289;&#19968;&#20010;&#20803;-&#24378;&#21270;&#23398;&#20064;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#26550;&#26500;&#65292;&#29420;&#31435;&#23454;&#29616;&#20102;&#19968;&#20010;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65307;&#65288;2&#65289;&#19968;&#20010;&#22806;&#37096;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#21644;&#33655;&#20848;&#36164;&#26684;&#36861;&#36394;&#26469;&#35757;&#32451;&#20803;-&#24378;&#21270;&#23398;&#20064;&#32593;&#32476;&#65307;&#21644;&#65288;3&#65289;&#38543;&#26426;&#21453;&#39304;&#23616;&#37096;&#22312;&#32447;&#65288;RFLO&#65289;&#23398;&#20064;&#65292;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#32593;&#32476;&#21442;&#25968;&#26799;&#24230;&#30340;&#22312;&#32447;&#33258;&#21160;&#24494;&#20998;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;RTRRL&#20013;&#30340;&#20248;&#21270;&#31639;&#27861;&#26367;&#25442;&#20026;&#29983;&#29289;&#19981;&#21512;&#29702;&#30340;&#26102;&#24310;&#21453;&#21521;&#20256;&#25773;&#65288;BPTT&#65289;&#25110;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#65292;&#24182;&#19981;&#33021;&#25913;&#21892;&#22238;&#25253;&#65292;&#21516;&#26102;&#22312;&#21305;&#37197;BPTT&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#29978;&#33267;&#20250;&#22686;&#21152;&#36820;&#22238;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04830v2 Announce Type: replace  Abstract: In this paper we propose real-time recurrent reinforcement learning (RTRRL), a biologically plausible approach to solving discrete and continuous control tasks in partially-observable markov decision processes (POMDPs). RTRRL consists of three parts: (1) a Meta-RL RNN architecture, implementing on its own an actor-critic algorithm; (2) an outer reinforcement learning algorithm, exploiting temporal difference learning and dutch eligibility traces to train the Meta-RL network; and (3) random-feedback local-online (RFLO) learning, an online automatic differentiation algorithm for computing the gradients with respect to parameters of the network.Our experimental results show that by replacing the optimization algorithm in RTRRL with the biologically implausible back propagation through time (BPTT), or real-time recurrent learning (RTRL), one does not improve returns, while matching the computational complexity for BPTT, and even increasi
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#21457;&#23637;&#23398;&#20064;&#30340;&#36817;&#31471;&#32593;&#32476;&#65292;&#35777;&#26126;&#23427;&#20204;&#25552;&#20379;&#20102;&#30830;&#20999;&#30340;proximal&#25805;&#20316;&#31526;&#12290;</title><link>https://arxiv.org/abs/2310.14344</link><description>&lt;p&gt;
&#20808;&#39564;&#20013;&#30340;&#20869;&#23481;&#26159;&#20160;&#20040;&#65311;&#29992;&#20110;&#36870;&#38382;&#39064;&#30340;&#23398;&#20064;&#36817;&#31471;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
What's in a Prior? Learned Proximal Networks for Inverse Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14344
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#21457;&#23637;&#23398;&#20064;&#30340;&#36817;&#31471;&#32593;&#32476;&#65292;&#35777;&#26126;&#23427;&#20204;&#25552;&#20379;&#20102;&#30830;&#20999;&#30340;proximal&#25805;&#20316;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Proximal operators are ubiquitous in inverse problems, commonly appearing as part of algorithmic strategies to regularize problems that are otherwise ill-posed. Modern deep learning models have been brought to bear for these tasks too, as in the framework of plug-and-play or deep unrolling, where they loosely resemble proximal operators. Yet, something essential is lost in employing these purely data-driven approaches: there is no guarantee that a general deep network represents the proximal operator of any function, nor is there any characterization of the function for which the network might provide some approximate proximal. This not only makes guaranteeing convergence of iterative schemes challenging but, more fundamentally, complicates the analysis of what has been learned by these networks about their training data.
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14344v2 Announce Type: replace-cross  Abstract: Proximal operators are ubiquitous in inverse problems, commonly appearing as part of algorithmic strategies to regularize problems that are otherwise ill-posed. Modern deep learning models have been brought to bear for these tasks too, as in the framework of plug-and-play or deep unrolling, where they loosely resemble proximal operators. Yet, something essential is lost in employing these purely data-driven approaches: there is no guarantee that a general deep network represents the proximal operator of any function, nor is there any characterization of the function for which the network might provide some approximate proximal. This not only makes guaranteeing convergence of iterative schemes challenging but, more fundamentally, complicates the analysis of what has been learned by these networks about their training data. Herein we provide a framework to develop learned proximal networks (LPN), prove that they provide exact pro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31070;&#32463;-ODE&#25972;&#21512;&#30340;&#26041;&#27861;&#29992;&#20110;&#32959;&#30244;&#21160;&#24577;&#39044;&#27979;&#65292;&#26377;&#25928;&#25972;&#21512;&#20102;&#22810;&#31181;&#25968;&#25454;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#22686;&#24378;&#20010;&#24615;&#21270;&#32959;&#30244;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2310.00926</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31070;&#32463;-ODE&#30340;&#25972;&#21512;&#29992;&#20110;&#32959;&#30244;&#21160;&#24577;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Integration of Graph Neural Network and Neural-ODEs for Tumor Dynamic Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00926
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31070;&#32463;-ODE&#25972;&#21512;&#30340;&#26041;&#27861;&#29992;&#20110;&#32959;&#30244;&#21160;&#24577;&#39044;&#27979;&#65292;&#26377;&#25928;&#25972;&#21512;&#20102;&#22810;&#31181;&#25968;&#25454;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#22686;&#24378;&#20010;&#24615;&#21270;&#32959;&#30244;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25239;&#30284;&#33647;&#29289;&#24320;&#21457;&#20013;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#31185;&#23398;&#25361;&#25112;&#26159;&#35299;&#24320;&#24739;&#32773;&#32959;&#30244;&#26679;&#26412;&#30340;&#39640;&#32500;&#22522;&#22240;&#32452;&#25968;&#25454;&#12289;&#30456;&#24212;&#32959;&#30244;&#30340;&#22120;&#23448;&#26469;&#28304;&#12289;&#19982;&#32473;&#23450;&#27835;&#30103;&#30456;&#20851;&#30340;&#33647;&#29289;&#38774;&#28857;&#20197;&#21450; resulting treatment response &#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#31934;&#20934;&#21307;&#23398;&#22312;&#26681;&#25454;&#27835;&#30103;&#21453;&#24212;&#35782;&#21035;&#21644;&#35843;&#25972;&#24739;&#32773;&#27835;&#30103;&#26041;&#26696;&#30340;&#24895;&#26223;&#65292;&#26377;&#24517;&#35201;&#26500;&#24314;&#33021;&#22815;&#25972;&#21512;&#32437;&#21521;&#32959;&#30244;&#22823;&#23567;&#21644;&#22810;&#27169;&#24577;&#12289;&#39640;&#20869;&#23481;&#25968;&#25454;&#30340;&#32959;&#30244;&#21160;&#24577;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#24322;&#26500;&#22270;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#21452;&#20998;&#22270;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#21644;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;&#31070;&#32463;-ODEs&#65289;&#65292;&#36808;&#21521;&#22686;&#24378;&#20010;&#24615;&#21270;&#32959;&#30244;&#21160;&#24577;&#39044;&#27979;&#30340;&#19968;&#27493;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#37327;&#26469;&#28304;&#20110;&#24739;&#32773;&#30340;&#24322;&#31181;&#31227;&#26893;&#27169;&#22411;&#65288;PDX&#65289;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00926v2 Announce Type: replace  Abstract: In anti-cancer drug development, a major scientific challenge is disentangling the complex relationships between high-dimensional genomics data from patient tumor samples, the corresponding tumor's organ of origin, the drug targets associated with given treatments and the resulting treatment response. Furthermore, to realize the aspirations of precision medicine in identifying and adjusting treatments for patients depending on the therapeutic response, there is a need for building tumor dynamic models that can integrate both longitudinal tumor size as well as multimodal, high-content data. In this work, we take a step towards enhancing personalized tumor dynamic predictions by proposing a heterogeneous graph encoder that utilizes a bipartite Graph Convolutional Neural network (GCN) combined with Neural Ordinary Differential Equations (Neural-ODEs). We applied the methodology to a large collection of patient-derived xenograft (PDX) da
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#31038;&#20132;&#26426;&#22120;&#20154;&#22312;&#29983;&#24577;&#22330;&#26223;&#20013;&#29702;&#35299;&#35828;&#35805;&#32773;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2308.10757</link><description>&lt;p&gt;
&#35841;&#22312;&#19982;&#20320;&#20132;&#35848;&#65311;&#19968;&#31181;&#36171;&#20104;&#31038;&#20132;&#26426;&#22120;&#20154;&#23450;&#20301;&#33021;&#21147;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
To Whom are You Talking? A Deep Learning Model to Endow Social Robots with Addressee Estimation Skills
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10757
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#31038;&#20132;&#26426;&#22120;&#20154;&#22312;&#29983;&#24577;&#22330;&#26223;&#20013;&#29702;&#35299;&#35828;&#35805;&#32773;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#27969;&#22609;&#36896;&#20102;&#25105;&#20204;&#30340;&#31038;&#20132;&#19990;&#30028;&#12290;&#20026;&#20102;&#35753;&#26426;&#22120;&#20154;&#34987;&#35270;&#20026;&#31038;&#20132;&#30340;&#65292;&#24182;&#34987;&#32435;&#20837;&#25105;&#20204;&#30340;&#31038;&#20132;&#29615;&#22659;&#20013;&#65292;&#29702;&#35299;&#39537;&#20351;&#20154;&#38469;&#20132;&#27969;&#30340;&#19968;&#20123;&#21160;&#24577;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23450;&#20301;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;&#21363;&#29702;&#35299;&#35805;&#35821;&#30340;&#23545;&#35937;&#65292;&#36890;&#36807;&#35299;&#37322;&#21644;&#21033;&#29992;&#35828;&#35805;&#32773;&#30340;&#38750;&#35821;&#35328;&#36523;&#20307;&#20449;&#21495;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#35813;&#27169;&#22411;&#30001;&#21367;&#31215;&#23618;&#21644;LSTM&#21333;&#20803;&#32452;&#25104;&#65292;&#20197;&#25551;&#32472;&#35828;&#35805;&#32773;&#33080;&#37096;&#30340;&#22270;&#20687;&#21644;&#35828;&#35805;&#32773;&#36523;&#20307;&#23039;&#21183;&#30340;2D&#21521;&#37327;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#36873;&#25321;&#26159;&#20026;&#20102;&#24320;&#21457;&#19968;&#20010;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#19978;&#21487;&#20197;&#37096;&#32626;&#24182;&#33021;&#22312;&#29983;&#24577;&#22330;&#26223;&#20013;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20174;&#26426;&#22120;&#20154;&#30340;&#33258;&#25105;&#20013;&#24515;&#35266;&#28857;&#35299;&#20915;&#23450;&#20301;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10757v2 Announce Type: replace-cross  Abstract: Communicating shapes our social word. For a robot to be considered social and being consequently integrated in our social environment it is fundamental to understand some of the dynamics that rule human-human communication. In this work, we tackle the problem of Addressee Estimation, the ability to understand an utterance's addressee, by interpreting and exploiting non-verbal bodily cues from the speaker. We do so by implementing an hybrid deep learning model composed of convolutional layers and LSTM cells taking as input images portraying the face of the speaker and 2D vectors of the speaker's body posture. Our implementation choices were guided by the aim to develop a model that could be deployed on social robots and be efficient in ecological scenarios. We demonstrate that our model is able to solve the Addressee Estimation problem in terms of addressee localisation in space, from a robot ego-centric point of view.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#32452;&#21512;&#26102;&#38047;&#25293;&#21334;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#38656;&#27714;&#26597;&#35810;&#32780;&#19981;&#26159;&#20215;&#20540;&#26597;&#35810;&#26469;&#33719;&#21462;&#25237;&#26631;&#20154;&#30340;&#20559;&#22909;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2308.10226</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#32452;&#21512;&#26102;&#38047;&#25293;&#21334;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Powered Combinatorial Clock Auction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#32452;&#21512;&#26102;&#38047;&#25293;&#21334;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#38656;&#27714;&#26597;&#35810;&#32780;&#19981;&#26159;&#20215;&#20540;&#26597;&#35810;&#26469;&#33719;&#21462;&#25237;&#26631;&#20154;&#30340;&#20559;&#22909;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#65288;ICA&#65289;&#30340;&#35774;&#35745;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#26463;&#31354;&#38388;&#38543;&#30528;&#29289;&#21697;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#26377;&#20960;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20559;&#22909;&#35843;&#26597;&#31639;&#27861;&#65292;&#26088;&#22312;&#20165;&#20174;&#25237;&#26631;&#20154;&#37027;&#37324;&#33719;&#21462;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20174;&#23454;&#38469;&#35282;&#24230;&#30475;&#65292;&#36825;&#20123;&#20808;&#21069;&#24037;&#20316;&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#36890;&#36807;&#20215;&#20540;&#26597;&#35810;&#65288;&#21363;&#65292;&#8220;&#23545;&#20110;&#25414;&#32465;&#21253;$\{A,B\}$&#65292;&#24744;&#30340;&#20215;&#20540;&#26159;&#22810;&#23569;&#65311;&#8221;&#65289;&#24341;&#20986;&#25237;&#26631;&#20154;&#30340;&#20559;&#22909;&#12290;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;ICA&#39046;&#22495;&#20013;&#65292;&#20215;&#20540;&#26597;&#35810;&#34987;&#35748;&#20026;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#32473;&#25237;&#26631;&#20154;&#24102;&#26469;&#20102;&#19981;&#20999;&#23454;&#38469;&#30340;&#39640;&#35748;&#30693;&#36127;&#25285;&#65292;&#36825;&#23601;&#26159;&#20026;&#20160;&#20040;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#19981;&#34987;&#20351;&#29992;&#30340;&#21407;&#22240;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#32452;&#21512;&#26102;&#38047;&#25293;&#21334;&#26469;&#35299;&#20915;&#36825;&#19968;&#32570;&#28857;&#65292;&#35813;&#25293;&#21334;&#21482;&#36890;&#36807;&#38656;&#27714;&#26597;&#35810;&#65288;&#21363;&#65292;&#8220;&#22312;&#20215;&#26684;$p$&#19979;&#65292;&#24744;&#23545;&#25414;&#32465;&#21253;$\{A,B\}$&#30340;&#38656;&#27714;&#26377;&#22810;&#23569;&#65311;&#8221;&#65289;&#20174;&#25237;&#26631;&#20154;&#37027;&#37324;&#24341;&#36215;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10226v2 Announce Type: replace-cross  Abstract: We study the design of iterative combinatorial auctions (ICAs). The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, several papers have recently proposed machine learning (ML)-based preference elicitation algorithms that aim to elicit only the most important information from bidders. However, from a practical point of view, the main shortcoming of this prior work is that those designs elicit bidders' preferences via value queries (i.e., ``What is your value for the bundle $\{A,B\}$?''). In most real-world ICA domains, value queries are considered impractical, since they impose an unrealistically high cognitive burden on bidders, which is why they are not used in practice. In this paper, we address this shortcoming by designing an ML-powered combinatorial clock auction that elicits information from the bidders only via demand queries (i.e., ``At prices $p$, what
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#30697;&#35282;&#22797;&#21512;&#20307;&#30340;&#21516;&#35843;&#27010;&#24565;&#23450;&#20041;&#20102;&#21452;&#20998;&#32423;&#25345;&#20037;&#24615;&#26465;&#30721;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#31283;&#23450;&#24615;&#23450;&#29702;</title><link>https://arxiv.org/abs/2303.14694</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#21452;&#20998;&#32423;&#25345;&#20037;&#24615;&#26465;&#30721;&#30340;&#31283;&#23450;&#24615;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
A stability theorem for bigraded persistence barcodes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.14694
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30697;&#35282;&#22797;&#21512;&#20307;&#30340;&#21516;&#35843;&#27010;&#24565;&#23450;&#20041;&#20102;&#21452;&#20998;&#32423;&#25345;&#20037;&#24615;&#26465;&#30721;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#31283;&#23450;&#24615;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#36234;&#25176;&#37324;&#26031;-&#37324;&#26222;&#26031;&#28388;&#27874;&#30340;&#30697;&#35282;&#22797;&#21512;&#20307;&#30340;&#26222;&#36890;&#21644;&#21452;&#37325;&#21516;&#35843;&#23450;&#20041;&#20102;&#26377;&#38480;&#20266;&#24230;&#37327;&#31354;&#38388;X&#30340;&#21452;&#20998;&#32423;&#25345;&#20037;&#21516;&#35843;&#27169;&#21644;&#21452;&#20998;&#32423;&#26465;&#30721;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20851;&#20110;&#21452;&#20998;&#32423;&#25345;&#20037;&#24615;&#21452;&#21516;&#35843;&#27169;&#21644;&#26465;&#30721;&#30340;&#31283;&#23450;&#24615;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.14694v2 Announce Type: replace-cross  Abstract: We define bigraded persistent homology modules and bigraded barcodes of a finite pseudo-metric space X using the ordinary and double homology of the moment-angle complex associated with the Vietoris-Rips filtration of X. We prove a stability theorem for the bigraded persistent double homology modules and barcodes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2302.03788</link><description>&lt;p&gt;
&#38754;&#21521;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#22240;&#26524;&#35770;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Toward a Theory of Causation for Interpreting Neural Code Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03788
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Language Models of Code&#65292;&#25110;&#32773;&#31216;&#20026;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#65288;NCMs&#65289;&#65292;&#27491;&#22312;&#36805;&#36895;&#20174;&#30740;&#31350;&#21407;&#22411;&#21457;&#23637;&#20026;&#21830;&#19994;&#24320;&#21457;&#32773;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#36890;&#24120;&#26159;&#20351;&#29992;&#33258;&#21160;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#30340;&#65292;&#36825;&#20123;&#25351;&#26631;&#36890;&#24120;&#21482;&#33021;&#25581;&#31034;&#23427;&#20204;&#30495;&#23454;&#24615;&#33021;&#30340;&#19968;&#37096;&#20998;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;NCMs&#30340;&#24615;&#33021;&#20284;&#20046;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#30446;&#21069;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#20570;&#20986;&#20915;&#31574;&#20173;&#26377;&#24456;&#22810;&#26410;&#30693;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#38376;&#38024;&#23545;NCMs&#65292;&#33021;&#22815;&#35299;&#37322;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;$do_{code}$&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#20197;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;&#34429;&#28982;$do_{code}$&#30340;&#29702;&#35770;&#22522;&#30784;&#21487;&#25193;&#23637;&#21040;&#25506;&#32034;&#19981;&#21516;&#30340;&#27169;&#22411;&#23646;&#24615;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#23454;&#20363;&#65292;&#26088;&#22312;&#20943;&#23569;&#24433;&#21709;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03788v2 Announce Type: replace-cross  Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22351;&#26679;&#26412;&#25366;&#25496;&#65288;DBPM&#65289;&#31639;&#27861;&#65292;&#21487;&#38752;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#22122;&#22768;&#27491;&#26679;&#26412;&#23545;&#21644;&#38169;&#35823;&#27491;&#26679;&#26412;&#23545;&#12290;</title><link>https://arxiv.org/abs/2302.03357</link><description>&lt;p&gt;
&#20248;&#21270;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#65306;&#19968;&#31181;&#21160;&#24577;&#22351;&#26679;&#26412;&#25366;&#25496;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Enhancing Time Series Contrastive Learning: A Dynamic Bad Pair Mining Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03357
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22351;&#26679;&#26412;&#25366;&#25496;&#65288;DBPM&#65289;&#31639;&#27861;&#65292;&#21487;&#38752;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#22122;&#22768;&#27491;&#26679;&#26412;&#23545;&#21644;&#38169;&#35823;&#27491;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#27491;&#26679;&#26412;&#23545;&#23545;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#37117;&#26377;&#30410;&#22788;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#21487;&#33021;&#24433;&#21709;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#36136;&#37327;&#30340;&#22351;&#27491;&#26679;&#26412;&#23545;&#65306;&#22122;&#22768;&#27491;&#26679;&#26412;&#23545;&#21644;&#38169;&#35823;&#27491;&#26679;&#26412;&#23545;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#23384;&#22312;&#22122;&#22768;&#27491;&#26679;&#26412;&#23545;&#26102;&#65292;&#27169;&#22411;&#24448;&#24448;&#21482;&#23398;&#20064;&#22122;&#22768;&#30340;&#27169;&#24335;&#65288;&#22122;&#22768;&#23545;&#40784;&#65289;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24403;&#20986;&#29616;&#38169;&#35823;&#27491;&#26679;&#26412;&#23545;&#26102;&#65292;&#27169;&#22411;&#20250;&#28010;&#36153;&#22823;&#37327;&#31934;&#21147;&#26469;&#23545;&#40784;&#38750;&#20195;&#34920;&#24615;&#27169;&#24335;&#65288;&#38169;&#35823;&#23545;&#40784;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22351;&#26679;&#26412;&#25366;&#25496;&#65288;DBPM&#65289;&#31639;&#27861;&#65292;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#22351;&#27491;&#26679;&#26412;&#23545;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DBPM&#21033;&#29992;&#20869;&#23384;&#27169;&#22359;&#21160;&#24577;&#36319;&#36394;&#27599;&#20010;&#27491;&#26679;&#26412;&#23545;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35757;&#32451;&#34892;&#20026;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#22522;&#20110;&#27599;&#20010;&#26102;&#26399;&#35782;&#21035;&#28508;&#22312;&#30340;&#22351;&#27491;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03357v2 Announce Type: replace  Abstract: Not all positive pairs are beneficial to time series contrastive learning. In this paper, we study two types of bad positive pairs that can impair the quality of time series representation learned through contrastive learning: the noisy positive pair and the faulty positive pair. We observe that, with the presence of noisy positive pairs, the model tends to simply learn the pattern of noise (Noisy Alignment). Meanwhile, when faulty positive pairs arise, the model wastes considerable amount of effort aligning non-representative patterns (Faulty Alignment). To address this problem, we propose a Dynamic Bad Pair Mining (DBPM) algorithm, which reliably identifies and suppresses bad positive pairs in time series contrastive learning. Specifically, DBPM utilizes a memory module to dynamically track the training behavior of each positive pair along training process. This allows us to identify potential bad positive pairs at each epoch based
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#25200;&#21160;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#26368;&#22351;&#24773;&#20917;&#30340;&#34394;&#25311;&#29366;&#24577;&#36716;&#25442;&#20197;&#25552;&#21319;&#40065;&#26834;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2301.13375</link><description>&lt;p&gt;
&#20351;&#29992;&#24102;&#26377;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#26368;&#20248;&#36755;&#36816;&#25200;&#21160;&#36827;&#34892;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport Perturbations for Safe Reinforcement Learning with Robustness Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13375
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#25200;&#21160;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#26368;&#22351;&#24773;&#20917;&#30340;&#34394;&#25311;&#29366;&#24577;&#36716;&#25442;&#20197;&#25552;&#21319;&#40065;&#26834;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#25104;&#26412;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#26368;&#20248;&#36755;&#36816;&#25200;&#21160;&#26469;&#26500;&#24314;&#26368;&#22351;&#24773;&#20917;&#30340;&#34394;&#25311;&#29366;&#24577;&#36716;&#25442;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#23454;&#29616;&#26041;&#27861;&#12290;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#40065;&#26834;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#37096;&#32626;&#26102;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13375v2 Announce Type: replace-cross  Abstract: Robustness and safety are critical for the trustworthy deployment of deep reinforcement learning. Real-world decision making applications require algorithms that can guarantee robust performance and safety in the presence of general environment disturbances, while making limited assumptions on the data collection process during training. In order to accomplish this goal, we introduce a safe reinforcement learning framework that incorporates robustness through the use of an optimal transport cost uncertainty set. We provide an efficient implementation based on applying Optimal Transport Perturbations to construct worst-case virtual state transitions, which does not impact data collection during training and does not require detailed simulator access. In experiments on continuous control tasks with safety constraints, our approach demonstrates robust performance while significantly improving safety at deployment time compared to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#8212;&#8212;&#20711;&#20285;&#32599;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#25968;&#25454;&#38598;(SOLD)&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#30740;&#31350;&#23616;&#38480;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2212.00851</link><description>&lt;p&gt;
SOLD&#65306;&#20711;&#20285;&#32599;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SOLD: Sinhala Offensive Language Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.00851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#8212;&#8212;&#20711;&#20285;&#32599;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#25968;&#25454;&#38598;(SOLD)&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#30740;&#31350;&#23616;&#38480;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#27604;&#22914;&#20167;&#24680;&#35328;&#35770;&#21644;&#32593;&#32476;&#27450;&#20940;&#65292;&#24050;&#25104;&#20026;&#20840;&#29699;&#24615;&#29616;&#35937;&#12290;&#36825;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#30340;&#20852;&#36259;&#65292;&#20419;&#20351;&#24320;&#21457;&#21508;&#31181;&#31995;&#32479;&#65292;&#33021;&#22815;&#33258;&#21160;&#26816;&#27979;&#28508;&#22312;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#23569;&#25968;&#20960;&#20010;&#20363;&#22806;&#24773;&#20917;&#22806;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#36825;&#19968;&#20027;&#39064;&#30340;&#25968;&#25454;&#38598;&#37117;&#22788;&#29702;&#33521;&#35821;&#21644;&#23569;&#25968;&#20854;&#20182;&#39640;&#36164;&#28304;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#30740;&#31350;&#19968;&#30452;&#23616;&#38480;&#20110;&#36825;&#20123;&#35821;&#35328;&#12290;&#26412;&#25991;&#36890;&#36807;&#22788;&#29702;&#20711;&#20285;&#32599;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#20711;&#20285;&#32599;&#35821;&#26159;&#26031;&#37324;&#20848;&#21345;&#26377;&#36229;&#36807;1700&#19975;&#20154;&#21475;&#20351;&#29992;&#30340;&#20302;&#36164;&#28304;&#21360;&#27431;&#35821;&#35328;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20711;&#20285;&#32599;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;&#65288;SOLD&#65289;&#65292;&#24182;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#22810;&#20010;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.00851v2 Announce Type: replace-cross  Abstract: The widespread of offensive content online, such as hate speech and cyber-bullying, is a global phenomenon. This has sparked interest in the artificial intelligence (AI) and natural language processing (NLP) communities, motivating the development of various systems trained to detect potentially harmful content automatically. These systems require annotated datasets to train the machine learning (ML) models. However, with a few notable exceptions, most datasets on this topic have dealt with English and a few other high-resource languages. As a result, the research in offensive language identification has been limited to these languages. This paper addresses this gap by tackling offensive language identification in Sinhala, a low-resource Indo-Aryan language spoken by over 17 million people in Sri Lanka. We introduce the Sinhala Offensive Language Dataset (SOLD) and present multiple experiments on this dataset. SOLD is a manuall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#20811;&#37324;&#37329;&#30340;&#36125;&#21494;&#26031;&#21452;&#30446;&#26631;&#25490;&#24207;&#21644;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22312;&#35782;&#21035;&#20855;&#26377;&#26368;&#20339;&#26399;&#26395;&#24615;&#33021;&#35299;&#26102;&#30340;&#38169;&#35823;&#20998;&#31867;</title><link>https://arxiv.org/abs/2209.03919</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#20811;&#37324;&#37329;&#30340;&#21452;&#30446;&#26631;&#25490;&#24207;&#21644;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Bi-objective Ranking and Selection Using Stochastic Kriging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.03919
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#20811;&#37324;&#37329;&#30340;&#36125;&#21494;&#26031;&#21452;&#30446;&#26631;&#25490;&#24207;&#21644;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22312;&#35782;&#21035;&#20855;&#26377;&#26368;&#20339;&#26399;&#26395;&#24615;&#33021;&#35299;&#26102;&#30340;&#38169;&#35823;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#21452;&#30446;&#26631;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#22312;&#35266;&#23519;&#21040;&#20004;&#20010;&#30446;&#26631;&#32467;&#26524;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#20505;&#36873;&#38598;&#20013;&#27491;&#30830;&#35782;&#21035;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#65292;&#20363;&#22914;&#65292;&#22312;&#36816;&#34892;&#22810;&#30446;&#26631;&#38543;&#26426;&#27169;&#25311;&#20248;&#21270;&#36807;&#31243;&#20043;&#21518;&#12290;&#22312;&#35782;&#21035;&#36825;&#20123;&#35299;&#26102;&#65292;&#35266;&#27979;&#24615;&#33021;&#30340;&#22122;&#22768;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#20004;&#31181;&#38169;&#35823;&#65306;&#30495;&#27491;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#35299;&#21487;&#33021;&#34987;&#38169;&#35823;&#22320;&#35748;&#20026;&#26159;&#34987;&#25903;&#37197;&#30340;&#65292;&#32780;&#30495;&#27491;&#34987;&#25903;&#37197;&#30340;&#35299;&#21487;&#33021;&#34987;&#38169;&#35823;&#22320;&#35748;&#20026;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#21452;&#30446;&#26631;&#25490;&#24207;&#21644;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#39034;&#24207;&#20998;&#37197;&#39069;&#22806;&#26679;&#26412;&#32473;&#31454;&#20105;&#35299;&#65292;&#20197;&#20943;&#23569;&#22312;&#35782;&#21035;&#20855;&#26377;&#26368;&#20339;&#26399;&#26395;&#24615;&#33021;&#30340;&#35299;&#26102;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#38543;&#26426;&#20811;&#37324;&#37329;&#26500;&#24314;&#23458;&#35266;&#39044;&#27979;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.03919v3 Announce Type: replace-cross  Abstract: We consider bi-objective ranking and selection problems, where the goal is to correctly identify the Pareto optimal solutions among a finite set of candidates for which the two objective outcomes have been observed with uncertainty (e.g., after running a multiobjective stochastic simulation optimization procedure). When identifying these solutions, the noise perturbing the observed performance may lead to two types of errors: solutions that are truly Pareto-optimal can be wrongly considered dominated, and solutions that are truly dominated can be wrongly considered Pareto-optimal. We propose a novel Bayesian bi-objective ranking and selection method that sequentially allocates extra samples to competitive solutions, in view of reducing the misclassification errors when identifying the solutions with the best expected performance. The approach uses stochastic kriging to build reliable predictive distributions of the objective ou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#39640;&#25928;&#20272;&#35745;&#29983;&#21629;&#20307;&#24449;&#65292;&#28040;&#38500;&#20102;&#32321;&#29712;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;</title><link>https://arxiv.org/abs/2204.08989</link><description>&lt;p&gt;
&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#39640;&#25928;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#21629;&#20307;&#24449;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Deep Learning-based Estimation of the Vital Signs on Smartphones
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.08989
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#39640;&#25928;&#20272;&#35745;&#29983;&#21629;&#20307;&#24449;&#65292;&#28040;&#38500;&#20102;&#32321;&#29712;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#25163;&#26426;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#36825;&#20123;&#35774;&#22791;&#24050;&#32463;&#33021;&#22815;&#25191;&#34892;&#35768;&#22810;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#38024;&#23545;&#23545;&#29983;&#21629;&#20307;&#24449;&#30340;&#25345;&#32493;&#30417;&#27979;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#32769;&#24180;&#20154;&#25110;&#24739;&#26377;&#26576;&#20123;&#31867;&#22411;&#30142;&#30149;&#30340;&#20154;&#32676;&#65292;&#24320;&#21457;&#33021;&#22815;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20272;&#35745;&#29983;&#21629;&#20307;&#24449;&#30340;&#31639;&#27861;&#24341;&#36215;&#20102;&#20840;&#29699;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#22312;&#25506;&#32034;&#20351;&#29992;&#21487;&#20197;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#36816;&#34892;&#30340;&#31639;&#27861;&#26469;&#20272;&#35745;&#29983;&#21629;&#20307;&#24449;&#65292;&#20363;&#22914;&#24515;&#29575;&#12289;&#34880;&#27687;&#39281;&#21644;&#24230;&#27700;&#24179;&#21644;&#21628;&#21560;&#29575;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#22810;&#20010;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#36825;&#21487;&#33021;&#20250;&#24341;&#20837;&#19968;&#20123;&#23454;&#29616;&#24320;&#38144;&#25110;&#38656;&#35201;&#35774;&#35745;&#20960;&#20010;&#25163;&#24037;&#38454;&#27573;&#25165;&#33021;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31227;&#21160;&#35774;&#22791;&#29983;&#21629;&#20307;&#24449;&#20272;&#35745;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#28040;&#38500;&#20102;&#23545;&#39044;&#22788;&#29702;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.08989v3 Announce Type: replace-cross  Abstract: With the increasing use of smartphones in our daily lives, these devices have become capable of performing many complex tasks. Concerning the need for continuous monitoring of vital signs, especially for the elderly or those with certain types of diseases, the development of algorithms that can estimate vital signs using smartphones has attracted researchers worldwide. In particular, researchers have been exploring ways to estimate vital signs, such as heart rate, oxygen saturation levels, and respiratory rate, using algorithms that can be run on smartphones. However, many of these algorithms require multiple pre-processing steps that might introduce some implementation overheads or require the design of a couple of hand-crafted stages to obtain an optimal result. To address this issue, this research proposes a novel end-to-end solution to mobile-based vital sign estimation using deep learning that eliminates the need for pre-p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#30340;&#39057;&#22495;&#20449;&#36947;&#20272;&#35745;&#21435;&#22122;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20449;&#36947;&#21435;&#22122;&#36807;&#31243;&#21644;&#20449;&#36947;&#26354;&#29575;&#35745;&#31639;&#26469;&#35782;&#21035;&#19981;&#21487;&#38752;&#20449;&#36947;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2101.10300</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;MIMO OFDM&#31995;&#32479;&#20013;&#30340;&#36880;&#27493;&#21435;&#22122;&#20449;&#36947;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Channel Estimation via Successive Denoising in MIMO OFDM Systems: A Reinforcement Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2101.10300
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#30340;&#39057;&#22495;&#20449;&#36947;&#20272;&#35745;&#21435;&#22122;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20449;&#36947;&#21435;&#22122;&#36807;&#31243;&#21644;&#20449;&#36947;&#26354;&#29575;&#35745;&#31639;&#26469;&#35782;&#21035;&#19981;&#21487;&#38752;&#20449;&#36947;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22312;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#27491;&#20132;&#39057;&#20998;&#22797;&#29992;&#65288;OFDM&#65289;&#31995;&#32479;&#20013;&#23454;&#29616;&#21487;&#38752;&#36890;&#20449;&#38656;&#35201;&#25509;&#25910;&#31471;&#36827;&#34892;&#20934;&#30830;&#30340;&#20449;&#36947;&#20272;&#35745;&#12290;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#20381;&#36182;&#20110;&#26102;&#38388;&#22495;&#20449;&#36947;&#20998;&#26512;&#25110;&#38656;&#35201;&#22823;&#37327;&#39044;&#20808;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#20449;&#36947;&#20272;&#35745;&#21435;&#22122;&#26041;&#27861;&#19978;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#30340;&#39057;&#22495;&#21435;&#22122;&#26041;&#27861;&#65292;&#26080;&#38656;&#20808;&#39564;&#20449;&#36947;&#30693;&#35782;&#21644;&#39044;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#22522;&#20110;&#20449;&#36947;&#26354;&#29575;&#35745;&#31639;&#30340;&#26032;&#22411;&#36880;&#27493;&#20449;&#36947;&#21435;&#22122;&#36807;&#31243;&#65292;&#20854;&#20013;&#25105;&#20204;&#33719;&#24471;&#19968;&#20010;&#20449;&#36947;&#26354;&#29575;&#24133;&#24230;&#38408;&#20540;&#26469;&#35782;&#21035;&#19981;&#21487;&#38752;&#30340;&#20449;&#36947;&#20272;&#35745;&#12290;&#22522;&#20110;&#36825;&#19968;&#36807;&#31243;&#65292;&#25105;&#20204;&#23558;&#21435;&#22122;&#26426;&#21046;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2101.10300v5 Announce Type: replace-cross  Abstract: In general, reliable communication via multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) requires accurate channel estimation at the receiver. The existing literature largely focuses on denoising methods for channel estimation that depend on either (i) channel analysis in the time-domain with prior channel knowledge or (ii) supervised learning techniques which require large pre-labeled datasets for training. To address these limitations, we present a frequency-domain denoising method based on a reinforcement learning framework that does not need a priori channel knowledge and pre-labeled data. Our methodology includes a new successive channel denoising process based on channel curvature computation, for which we obtain a channel curvature magnitude threshold to identify unreliable channel estimates. Based on this process, we formulate the denoising mechanism as a Markov decision process, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#36830;&#32493;&#20989;&#25968;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#30340;&#20005;&#26684;&#35777;&#26126;&#65292;&#22635;&#34917;&#20102;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#25104;&#21151;&#30340;&#29702;&#35770;&#32570;&#21475;</title><link>https://arxiv.org/abs/2007.15776</link><description>&lt;p&gt;
&#29992;&#20110;&#27969;&#24418;&#19978;&#20989;&#25968;&#36924;&#36817;&#30340;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Random Vector Functional Link Networks for Function Approximation on Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2007.15776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#36830;&#32493;&#20989;&#25968;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#30340;&#20005;&#26684;&#35777;&#26126;&#65292;&#22635;&#34917;&#20102;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#25104;&#21151;&#30340;&#29702;&#35770;&#32570;&#21475;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
feed-forward&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36895;&#24230;&#22240;&#24930;&#32780;&#33879;&#21517;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#24050;&#32463;&#25104;&#20026;&#29942;&#39048;&#25968;&#21313;&#24180;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#23581;&#35797;&#24341;&#20837;&#38543;&#26426;&#24615;&#26469;&#20943;&#23569;&#23398;&#20064;&#38656;&#27714;&#12290;&#22522;&#20110;Igelnik&#21644;Pao&#30340;&#21407;&#22987;&#26500;&#36896;&#65292;&#20855;&#26377;&#38543;&#26426;&#36755;&#20837;&#21040;&#38544;&#34255;&#23618;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#32570;&#20047;&#24517;&#35201;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#29702;&#35770;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#65288;&#26356;&#27491;&#30340;&#65289;&#20005;&#26684;&#35777;&#26126;&#65292;&#35777;&#26126;Igelnik&#21644;Pao&#30340;&#26500;&#36896;&#26159;&#19968;&#20010;&#36830;&#32493;&#20989;&#25968;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#36924;&#36817;&#35823;&#24046;&#20687;&#28176;&#36817;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2007.15776v3 Announce Type: replace-cross  Abstract: The learning speed of feed-forward neural networks is notoriously slow and has presented a bottleneck in deep learning applications for several decades. For instance, gradient-based learning algorithms, which are used extensively to train neural networks, tend to work slowly when all of the network parameters must be iteratively tuned. To counter this, both researchers and practitioners have tried introducing randomness to reduce the learning requirement. Based on the original construction of Igelnik and Pao, single layer neural-networks with random input-to-hidden layer weights and biases have seen success in practice, but the necessary theoretical justification is lacking. In this paper, we begin to fill this theoretical gap. We provide a (corrected) rigorous proof that the Igelnik and Pao construction is a universal approximator for continuous functions on compact domains, with approximation error decaying asymptotically lik
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.10746</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#33041;&#30005;&#35299;&#30721;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding. (arXiv:2401.10746v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#32463;&#24120;&#29992;&#20110;&#21508;&#31181;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20219;&#21153;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#22823;&#37327;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#65292;&#36801;&#31227;&#23398;&#20064;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;DL&#27169;&#22411;&#12290;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#26159;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#65288;EA&#65289;&#65292;&#22240;&#20026;&#23427;&#26131;&#20110;&#20351;&#29992;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#20302;&#24182;&#19988;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20860;&#23481;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20854;&#23545;&#20849;&#20139;&#21644;&#20010;&#20307;DL&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;EA&#19982;DL&#30456;&#32467;&#21512;&#22312;&#35299;&#30721;BCI&#20449;&#21495;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;EA&#26469;&#35757;&#32451;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#20849;&#20139;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#26032;&#21463;&#35797;&#32773;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#23558;&#30446;&#26631;&#21463;&#35797;&#32773;&#30340;&#35299;&#30721;&#29575;&#25552;&#39640;&#20102;4.33&#65285;&#65292;&#24182;&#19988;&#25910;&#25947;&#26102;&#38388;&#32553;&#30701;&#20102;&#36229;&#36807;70&#65285;&#12290;&#25105;&#20204;&#36824;&#20026;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) signals are frequently used for various Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have shown promising results, they are hindered by the substantial data requirements. By leveraging data from multiple subjects, transfer learning enables more effective training of DL models. A technique that is gaining popularity is Euclidean Alignment (EA) due to its ease of use, low computational complexity, and compatibility with Deep Learning models. However, few studies evaluate its impact on the training performance of shared and individual DL models. In this work, we systematically evaluate the effect of EA combined with DL for decoding BCI signals. We used EA to train shared models with data from multiple subjects and evaluated its transferability to new subjects. Our experimental results show that it improves decoding in the target subject by 4.33% and decreases convergence time by more than 70%. We also trained individual models for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01286</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#19982;&#20154;&#31867;&#20132;&#27969;&#32039;&#23494;&#30456;&#20284;&#30340;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26174;&#33879;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#36896;&#25104;&#30340;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#19990;&#30028;&#30340;&#21160;&#24577;&#24615;&#65292;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;LLM&#20197;&#20462;&#27491;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#38598;&#25104;&#26032;&#30693;&#35782;&#65292;&#20174;&#32780;&#30830;&#20445;&#20854;&#25345;&#32493;&#30340;&#30456;&#20851;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#25345;&#32493;&#30340;&#27169;&#22411;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#32570;&#38519;&#25110;&#19981;&#33391;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;LLM&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#39640;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#26377;&#25928;&#22320;&#20462;&#25913;LLM&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#30693;&#35782;&#32534;&#36753;&#30340;&#30446;&#26631;&#21644;&#25361;&#25112;&#65292;&#28982;&#21518;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24212;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#35782;&#21035;&#21644;&#32416;&#27491;&#38169;&#35823;&#26631;&#35760;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21512;&#20316;&#20249;&#20276;&#20998;&#31867;&#22120;&#21644;&#20114;&#30456;&#30417;&#30563;&#33539;&#24335;&#65292;&#20197;&#21450;&#20351;&#29992;&#27169;&#31946;&#26426;&#21046;&#23454;&#29616;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2312.11034</link><description>&lt;p&gt;
&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#21512;&#20316;&#20249;&#20276;
&lt;/p&gt;
&lt;p&gt;
Partial Label Learning with a Partner. (arXiv:2312.11034v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#35782;&#21035;&#21644;&#32416;&#27491;&#38169;&#35823;&#26631;&#35760;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21512;&#20316;&#20249;&#20276;&#20998;&#31867;&#22120;&#21644;&#20114;&#30456;&#30417;&#30563;&#33539;&#24335;&#65292;&#20197;&#21450;&#20351;&#29992;&#27169;&#31946;&#26426;&#21046;&#23454;&#29616;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#65292;&#27599;&#20010;&#23454;&#20363;&#19982;&#19968;&#32452;&#20505;&#36873;&#26631;&#31614;&#30456;&#20851;&#32852;&#65292;&#20854;&#20013;&#21482;&#26377;&#19968;&#20010;&#26159;&#30495;&#23454;&#26631;&#31614;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#22823;&#22810;&#20851;&#27880;&#26500;&#24314;&#24378;&#22823;&#30340;&#20998;&#31867;&#22120;&#26469;&#20272;&#35745;&#20505;&#36873;&#26631;&#31614;&#30340;&#26631;&#31614;&#32622;&#20449;&#24230;&#65292;&#20197;&#35782;&#21035;&#27491;&#30830;&#30340;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#24456;&#38590;&#32416;&#27491;&#38169;&#35823;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;&#20026;&#20102;&#24110;&#21161;&#29616;&#26377;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#35782;&#21035;&#21644;&#32416;&#27491;&#38169;&#35823;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21512;&#20316;&#20249;&#20276;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#20114;&#30456;&#30417;&#30563;&#8221;&#33539;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26681;&#25454;&#38544;&#21547;&#30340;&#20107;&#23454;&#23454;&#20363;&#21270;&#20102;&#21512;&#20316;&#20249;&#20276;&#20998;&#31867;&#22120;&#65292;&#21363;&#19968;&#20010;&#26679;&#26412;&#30340;&#38750;&#20505;&#36873;&#26631;&#31614;&#19981;&#24212;&#35813;&#34987;&#20998;&#37197;&#32473;&#23427;&#65292;&#36825;&#22312;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#20855;&#26377;&#20869;&#22312;&#30340;&#20934;&#30830;&#24615;&#24182;&#19988;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#20316;&#39033;&#26469;&#23558;&#22522;&#30784;&#20998;&#31867;&#22120;&#21644;&#21512;&#20316;&#20249;&#20276;&#20998;&#31867;&#22120;&#38142;&#25509;&#36215;&#26469;&#12290;&#22312;&#27599;&#20010;&#20114;&#30456;&#30417;&#30563;&#30340;&#38454;&#27573;&#20013;&#65292;&#20004;&#20010;&#20998;&#31867;&#22120;&#23558;&#36890;&#36807;&#19968;&#20010;&#27169;&#31946;&#26426;&#21046;&#20114;&#30456;&#27169;&#31946;&#24444;&#27492;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In partial label learning (PLL), each instance is associated with a set of candidate labels among which only one is ground-truth. The majority of the existing works focuses on constructing robust classifiers to estimate the labeling confidence of candidate labels in order to identify the correct one. However, these methods usually struggle to rectify mislabeled samples. To help existing PLL methods identify and rectify mislabeled samples, in this paper, we introduce a novel partner classifier and propose a novel ``mutual supervision'' paradigm. Specifically, we instantiate the partner classifier predicated on the implicit fact that non-candidate labels of a sample should not be assigned to it, which is inherently accurate and has not been fully investigated in PLL. Furthermore, a novel collaborative term is formulated to link the base classifier and the partner one. During each stage of mutual supervision, both classifiers will blur each other's predictions through a blurring mechanism
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#30340;&#26657;&#20934;&#20250;&#20808;&#22686;&#21152;&#32780;&#21518;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#20027;&#35201;&#20986;&#29616;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#31561;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#25552;&#31034;&#38656;&#35201;&#38024;&#23545;&#21487;&#38752;&#24615;&#22330;&#26223;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.04021</link><description>&lt;p&gt;
&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26657;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on the Calibration of In-context Learning. (arXiv:2312.04021v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#30340;&#26657;&#20934;&#20250;&#20808;&#22686;&#21152;&#32780;&#21518;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#20027;&#35201;&#20986;&#29616;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#31561;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#25552;&#31034;&#38656;&#35201;&#38024;&#23545;&#21487;&#38752;&#24615;&#22330;&#26223;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#23433;&#20840;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#29616;&#20195;LMs&#26657;&#20934;&#24615;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;LMs&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#22312;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#26368;&#21021;&#20250;&#20986;&#29616;&#22686;&#21152;&#30340;&#26657;&#20934;&#35823;&#24046;&#65292;&#28982;&#21518;&#25165;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#26657;&#20934;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#24448;&#24448;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#20986;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20197;&#25552;&#39640;&#21487;&#29992;&#24615;&#20026;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#22914;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#65292;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#36825;&#34920;&#26126;&#22312;&#26399;&#26395;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#22330;&#26223;&#20013;&#21487;&#33021;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty quantification is crucial for the safe deployment of language models (LMs), and prior research has demonstrated improvements in the calibration of modern LMs. Our study focuses on in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examines the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations, suggesting that new methods may be required for scenarios where models are expected to be reliable.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#30452;&#25509;&#20559;&#22909;&#36807;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20559;&#22909;&#30340;&#24207;&#32467;&#26500;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20445;&#35777;&#26368;&#20248;&#31574;&#30053;&#23384;&#22312;&#30340;&#26465;&#20214;&#12290;&#36825;&#20010;&#30740;&#31350;&#32553;&#23567;&#20102;&#23398;&#20064;&#20559;&#22909;&#21453;&#39304;&#31639;&#27861;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20248;&#31574;&#30053;&#23384;&#22312;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2311.01990</link><description>&lt;p&gt;
&#26465;&#20214;&#23545;&#20559;&#22909;&#20851;&#31995;&#36827;&#34892;&#38480;&#21046;&#20197;&#20445;&#35777;&#26368;&#20248;&#31574;&#30053;&#30340;&#23384;&#22312;
&lt;/p&gt;
&lt;p&gt;
Conditions on Preference Relations that Guarantee the Existence of Optimal Policies. (arXiv:2311.01990v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01990
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#30452;&#25509;&#20559;&#22909;&#36807;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20559;&#22909;&#30340;&#24207;&#32467;&#26500;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20445;&#35777;&#26368;&#20248;&#31574;&#30053;&#23384;&#22312;&#30340;&#26465;&#20214;&#12290;&#36825;&#20010;&#30740;&#31350;&#32553;&#23567;&#20102;&#23398;&#20064;&#20559;&#22909;&#21453;&#39304;&#31639;&#27861;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20248;&#31574;&#30053;&#23384;&#22312;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20559;&#22909;&#21453;&#39304; (LfPF) &#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26576;&#20123;&#31867;&#22411;&#30340;&#20132;&#20114;&#24335;&#23398;&#20064;&#20195;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;LfPF&#31639;&#27861;&#30340;&#29702;&#35770;&#21644;&#24212;&#29992;&#20043;&#38388;&#23384;&#22312;&#30528;&#23454;&#36136;&#24615;&#30340;&#24046;&#36317;&#12290;&#30446;&#21069;&#20445;&#35777;&#22312;LfPF&#38382;&#39064;&#20013;&#23384;&#22312;&#26368;&#20248;&#31574;&#30053;&#30340;&#32467;&#26524;&#20551;&#35774;&#20559;&#22909;&#21644;&#36716;&#31227;&#21160;&#24577;&#37117;&#30001;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30830;&#23450;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#30452;&#25509;&#20559;&#22909;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#12289;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#20013;&#20998;&#26512;LfPF&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#20559;&#22909;&#30340;&#24207;&#32467;&#26500;&#24314;&#31435;&#20102;&#20445;&#35777;&#26368;&#20248;&#31574;&#30053;&#23384;&#22312;&#30340;&#26465;&#20214;&#12290;&#21033;&#29992;&#20911;&#183;&#35834;&#20381;&#26364;-&#25705;&#26681;&#26031;&#29305;&#24681;&#26399;&#26395;&#25928;&#29992;&#23450;&#29702;&#65292;&#25105;&#20204;&#34920;&#26126;&#30452;&#25509;&#20559;&#22909;&#36807;&#31243;&#25512;&#24191;&#20102;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#32553;&#23567;&#20102;LfPF&#31639;&#27861;&#22312;&#32463;&#39564;&#25104;&#21151;&#21644;&#29702;&#35770;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#20248;&#31574;&#30053;&#23384;&#22312;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Preferential Feedback (LfPF) plays an essential role in training Large Language Models, as well as certain types of interactive learning agents. However, a substantial gap exists between the theory and application of LfPF algorithms. Current results guaranteeing the existence of optimal policies in LfPF problems assume that both the preferences and transition dynamics are determined by a Markov Decision Process. We introduce the Direct Preference Process, a new framework for analyzing LfPF problems in partially-observable, non-Markovian environments. Within this framework, we establish conditions that guarantee the existence of optimal policies by considering the ordinal structure of the preferences. Using the von Neumann-Morgenstern Expected Utility Theorem, we show that the Direct Preference Process generalizes the standard reinforcement learning problem. Our findings narrow the gap between the empirical success and theoretical understanding of LfPF algorithms and provi
&lt;/p&gt;</description></item><item><title>ADMarker&#26159;&#19968;&#20010;&#22810;&#27169;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#33258;&#28982;&#29983;&#27963;&#29615;&#22659;&#20013;&#30417;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#23427;&#20855;&#26377;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#20986;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#24182;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#23637;&#31034;&#20986;&#39640;&#20934;&#30830;&#29575;&#21644;&#26089;&#26399;AD&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.15301</link><description>&lt;p&gt;
ADMarker: &#19968;&#31181;&#22810;&#27169;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#30417;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;
&lt;/p&gt;
&lt;p&gt;
ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital Biomarkers of Alzheimer's Disease. (arXiv:2310.15301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15301
&lt;/p&gt;
&lt;p&gt;
ADMarker&#26159;&#19968;&#20010;&#22810;&#27169;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#33258;&#28982;&#29983;&#27963;&#29615;&#22659;&#20013;&#30417;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#23427;&#20855;&#26377;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#20986;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#24182;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#23637;&#31034;&#20986;&#39640;&#20934;&#30830;&#29575;&#21644;&#26089;&#26399;AD&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#21450;&#30456;&#20851;&#30196;&#21574;&#30151;&#30001;&#20110;&#20154;&#21475;&#32769;&#40836;&#21270;&#32780;&#25104;&#20026;&#20840;&#29699;&#26085;&#30410;&#20005;&#37325;&#30340;&#20581;&#24247;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ADMarker&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#21644;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25972;&#21512;&#36215;&#26469;&#65292;&#20197;&#22312;&#33258;&#28982;&#29983;&#27963;&#29615;&#22659;&#20013;&#26816;&#27979;&#22810;&#32500;AD&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;ADMarker&#20855;&#26377;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#22810;&#27169;&#24335;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#20197;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#20934;&#30830;&#26816;&#27979;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20849;&#21516;&#35299;&#20915;&#20102;&#25968;&#25454;&#26631;&#31614;&#26377;&#38480;&#12289;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#31561;&#20960;&#20010;&#20027;&#35201;&#30340;&#29616;&#23454;&#19990;&#30028;&#25361;&#25112;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32039;&#20945;&#30340;&#22810;&#27169;&#24335;&#30828;&#20214;&#31995;&#32479;&#65292;&#24182;&#22312;&#19968;&#20010;&#20026;&#26399;&#22235;&#21608;&#30340;&#20020;&#24202;&#35797;&#39564;&#20013;&#23558;&#20854;&#37096;&#32626;&#22312;91&#21517;&#32769;&#24180;&#21442;&#19982;&#32773;&#36523;&#19978;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ADMarker&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#20986;&#20840;&#38754;&#30340;&#25968;&#23383;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;93.8&#65285;&#65292;&#24182;&#20197;&#24179;&#22343;88.9&#65285;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26089;&#26399;AD&#12290;ADMarker&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#22312;&#30417;&#27979;AD&#24739;&#32773;&#26102;&#25552;&#20379;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's Disease (AD) and related dementia are a growing global health challenge due to the aging population. In this paper, we present ADMarker, the first end-to-end system that integrates multi-modal sensors and new federated learning algorithms for detecting multidimensional AD digital biomarkers in natural living environments. ADMarker features a novel three-stage multi-modal federated learning architecture that can accurately detect digital biomarkers in a privacy-preserving manner. Our approach collectively addresses several major real-world challenges, such as limited data labels, data heterogeneity, and limited computing resources. We built a compact multi-modality hardware system and deployed it in a four-week clinical trial involving 91 elderly participants. The results indicate that ADMarker can accurately detect a comprehensive set of digital biomarkers with up to 93.8% accuracy and identify early AD with an average of 88.9% accuracy. ADMarker offers a new platform that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#25913;&#36827;&#20256;&#24863;&#22120;&#24067;&#25918;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12387</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#23398;&#20064;&#35299;&#20915;&#27668;&#20505;&#20256;&#24863;&#22120;&#24067;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning to Solve Climate Sensor Placement Problems with a Transformer. (arXiv:2310.12387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#25913;&#36827;&#20256;&#24863;&#22120;&#24067;&#25918;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;NP&#38590;&#24615;&#36136;&#65292;&#29615;&#22659;&#30417;&#27979;&#21644;&#28798;&#23475;&#31649;&#29702;&#20013;&#30340;&#20256;&#24863;&#22120;&#24067;&#25918;&#20248;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20256;&#24863;&#22120;&#24067;&#25918;&#26041;&#27861;&#21253;&#25324;&#31934;&#30830;&#12289;&#36817;&#20284;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20854;&#20013;&#21551;&#21457;&#24335;&#26041;&#27861;&#26159;&#26368;&#24120;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#21551;&#21457;&#24335;&#26041;&#27861;&#21463;&#21040;&#19987;&#23478;&#30452;&#35273;&#21644;&#32463;&#39564;&#30340;&#38480;&#21046;&#12290;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#29983;&#25104;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#24863;&#22120;&#24067;&#25918;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#23398;&#20064;&#25913;&#36827;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#20844;&#24335;&#26469;&#23398;&#20064;&#25913;&#36827;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#35757;&#32451;&#31574;&#30053;&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#20840;&#38754;&#30340;&#23454;&#39564;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#27668;&#20505;&#20256;&#24863;&#22120;&#24067;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal placement of sensors for environmental monitoring and disaster management is a challenging problem due to its NP-hard nature. Traditional methods for sensor placement involve exact, approximation, or heuristic approaches, with the latter being the most widely used. However, heuristic methods are limited by expert intuition and experience. Deep learning (DL) has emerged as a promising approach for generating heuristic algorithms automatically. In this paper, we introduce a novel sensor placement approach focused on learning improvement heuristics using deep reinforcement learning (RL) methods. Our approach leverages an RL formulation for learning improvement heuristics, driven by an actor-critic algorithm for training the policy network. We compare our method with several state-of-the-art approaches by conducting comprehensive experiments, demonstrating the effectiveness and superiority of our proposed approach in producing high-quality solutions. Our work presents a promisi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#20013;&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#36235;&#21183;&#36319;&#36394;&#39044;&#27979;&#22120;&#65292;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#24066;&#22330;&#26465;&#20214;&#65292;&#24182;&#33719;&#24471;&#36739;&#39640;&#30340;&#22799;&#26222;&#27604;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.10500</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#27169;&#24335;&#29992;&#20110;&#36235;&#21183;&#36319;&#36394;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Learning Patterns in Financial Time-Series for Trend-Following Strategies. (arXiv:2310.10500v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10500
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#20013;&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#36235;&#21183;&#36319;&#36394;&#39044;&#27979;&#22120;&#65292;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#24066;&#22330;&#26465;&#20214;&#65292;&#24182;&#33719;&#24471;&#36739;&#39640;&#30340;&#22799;&#26222;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#31995;&#32479;&#21270;&#20132;&#26131;&#31574;&#30053;&#30340;&#39044;&#27979;&#27169;&#22411;&#22312;&#37329;&#34701;&#24066;&#22330;&#26465;&#20214;&#21457;&#29983;&#21464;&#21270;&#26102;&#26080;&#27861;&#36805;&#36895;&#36866;&#24212;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#36235;&#21183;&#36319;&#36394;&#39044;&#27979;&#22120;&#65292;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#24066;&#22330;&#26465;&#20214;&#12290;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37319;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20132;&#21449;&#27880;&#24847;&#21147;&#26102;&#38388;&#24207;&#21015;&#36235;&#21183;&#32593;&#32476; - X-Trend&#65292;&#36890;&#36807;&#23545;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20851;&#27880;&#65292;&#20174;&#19978;&#19979;&#25991;&#26679;&#26412;&#38598;&#20013;&#36716;&#31227;&#30456;&#20284;&#27169;&#24335;&#30340;&#36235;&#21183;&#65292;&#23545;&#26032;&#30340;&#30446;&#26631;&#24773;&#22659;&#36827;&#34892;&#39044;&#27979;&#21644;&#23450;&#20301;&#12290;X-Trend&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#37329;&#34701;&#24773;&#22659;&#65292;&#20854;&#22799;&#26222;&#27604;&#29575;&#30456;&#23545;&#20110;&#31070;&#32463;&#39044;&#27979;&#22120;&#25552;&#39640;&#20102;18.9%&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#25552;&#39640;&#20102;10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting models for systematic trading strategies do not adapt quickly when financial market conditions change, as was seen in the advent of the COVID-19 pandemic in 2020, when market conditions changed dramatically causing many forecasting models to take loss-making positions. To deal with such situations, we propose a novel time-series trend-following forecaster that is able to quickly adapt to new market conditions, referred to as regimes. We leverage recent developments from the deep learning community and use few-shot learning. We propose the Cross Attentive Time-Series Trend Network - X-Trend which takes positions attending over a context set of financial time-series regimes. X-Trend transfers trends from similar patterns in the context set to make predictions and take positions for a new distinct target regime. X-Trend is able to quickly adapt to new financial regimes with a Sharpe ratio increase of 18.9% over a neural forecaster and 10-fold over a conventional Time-series 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20887;&#20313;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25913;&#36827;&#34920;&#36798;&#33021;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#37051;&#22495;&#26641;&#30340;&#32858;&#21512;&#26041;&#26696;&#20943;&#23569;&#20887;&#20313;&#65292;&#25552;&#39640;&#20102;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#23545;&#20110;&#20943;&#36731;&#36807;&#24230;&#21387;&#32553;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04190</link><description>&lt;p&gt;
&#38750;&#20887;&#20313;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25913;&#36827;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Non-Redundant Graph Neural Networks with Improved Expressiveness. (arXiv:2310.04190v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20887;&#20313;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25913;&#36827;&#34920;&#36798;&#33021;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#37051;&#22495;&#26641;&#30340;&#32858;&#21512;&#26041;&#26696;&#20943;&#23569;&#20887;&#20313;&#65292;&#25552;&#39640;&#20102;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#23545;&#20110;&#20943;&#36731;&#36807;&#24230;&#21387;&#32553;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#25152;&#26377;&#37051;&#23621;&#33410;&#28857;&#30340;&#28040;&#24687;&#26469;&#36845;&#20195;&#35745;&#31639;&#33410;&#28857;&#23884;&#20837;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#34987;&#35270;&#20026;Weisfeiler-Leman&#26041;&#27861;&#30340;&#31070;&#32463;&#21464;&#20307;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38480;&#21046;&#20102;&#36825;&#20123;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30340;&#23618;&#25968;&#12290;&#28040;&#24687;&#20256;&#36882;&#20013;&#30456;&#21516;&#20449;&#24687;&#30340;&#37325;&#22797;&#20132;&#25442;&#21644;&#32534;&#30721;&#20250;&#25918;&#22823;&#36807;&#24230;&#21387;&#32553;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#22495;&#26641;&#30340;&#26032;&#30340;&#32858;&#21512;&#26041;&#26696;&#65292;&#20801;&#35768;&#36890;&#36807;&#20462;&#21098;&#26631;&#20934;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#23637;&#24320;&#26641;&#30340;&#20998;&#25903;&#26469;&#25511;&#21046;&#20887;&#20313;&#12290;&#25105;&#20204;&#35777;&#26126;&#20943;&#23569;&#20887;&#20313;&#21487;&#20197;&#25552;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23427;&#20943;&#36731;&#20102;&#36807;&#24230;&#21387;&#32553;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#28040;&#24687;&#20256;&#36882;&#20013;&#30340;&#20887;&#20313;&#19982;&#35745;&#31639;&#20013;&#30340;&#20887;&#20313;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#37051;&#22495;&#26641;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#36890;&#36807;&#31070;&#32463;&#26641;&#35268;&#33539;&#21270;&#25216;&#26415;&#35745;&#31639;&#33410;&#28857;&#21644;&#22270;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing graph neural networks iteratively compute node embeddings by aggregating messages from all neighbors. This procedure can be viewed as a neural variant of the Weisfeiler-Leman method, which limits their expressive power. Moreover, oversmoothing and oversquashing restrict the number of layers these networks can effectively utilize. The repeated exchange and encoding of identical information in message passing amplifies oversquashing. We propose a novel aggregation scheme based on neighborhood trees, which allows for controlling the redundancy by pruning branches of the unfolding trees underlying standard message passing. We prove that reducing redundancy improves expressivity and experimentally show that it alleviates oversquashing. We investigate the interaction between redundancy in message passing and redundancy in computation and propose a compact representation of neighborhood trees, from which we compute node and graph embeddings via a neural tree canonization techn
&lt;/p&gt;</description></item><item><title>&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.02861</link><description>&lt;p&gt;
Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection. (arXiv:2310.02861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02861
&lt;/p&gt;
&lt;p&gt;
&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#37238;&#39044;&#27979;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#22270;&#24322;&#24120;&#30340;&#28508;&#22312;&#23646;&#24615;&#65292;&#23548;&#33268;&#26694;&#26550;&#35774;&#35745;&#19981;&#21487;&#35299;&#37322;&#21644;&#24615;&#33021;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#37325;&#26032;&#30740;&#31350;&#20102;&#24322;&#24120;&#21644;&#27491;&#24120;&#22270;&#20043;&#38388;&#30340;&#20809;&#35889;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#31867;&#20043;&#38388;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#20449;&#21495;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#21487;&#20197;&#29992;&#20854;&#29790;&#21033;&#21830;&#34920;&#31034;&#65292;&#36825;&#34920;&#26126;&#29790;&#21033;&#21830;&#26159;&#22270;&#24322;&#24120;&#23646;&#24615;&#30340;&#19968;&#20010;&#39537;&#21160;&#22240;&#32032;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Rayleigh Quotient Graph Neural Network&#65288;RQGNN&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#20809;&#35889;GNN&#65292;&#20026;&#25506;&#32034;&#24322;&#24120;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graph
&lt;/p&gt;</description></item><item><title>SWoTTeD&#26159;&#19968;&#31181;&#25193;&#23637;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#19979;&#30340;&#38544;&#34255;&#34920;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SWoTTeD&#19981;&#20165;&#33021;&#19982;&#26368;&#26032;&#30340;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#26041;&#27861;&#19968;&#26679;&#20934;&#30830;&#22320;&#37325;&#24314;&#25968;&#25454;&#65292;&#36824;&#33021;&#25552;&#21462;&#20986;&#23545;&#20020;&#24202;&#21307;&#29983;&#26377;&#24847;&#20041;&#30340;&#26102;&#38388;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.01201</link><description>&lt;p&gt;
SWoTTeD:&#24352;&#37327;&#20998;&#35299;&#22312;&#26102;&#38388;&#34920;&#24449;&#20013;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping. (arXiv:2310.01201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01201
&lt;/p&gt;
&lt;p&gt;
SWoTTeD&#26159;&#19968;&#31181;&#25193;&#23637;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#19979;&#30340;&#38544;&#34255;&#34920;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SWoTTeD&#19981;&#20165;&#33021;&#19982;&#26368;&#26032;&#30340;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#26041;&#27861;&#19968;&#26679;&#20934;&#30830;&#22320;&#37325;&#24314;&#25968;&#25454;&#65292;&#36824;&#33021;&#25552;&#21462;&#20986;&#23545;&#20020;&#24202;&#21307;&#29983;&#26377;&#24847;&#20041;&#30340;&#26102;&#38388;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#20998;&#35299;&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#23545;&#20110;&#20010;&#20307;&#36861;&#36394;&#25968;&#25454;&#30340;&#20998;&#26512;&#65292;&#22914;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#65292;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#36981;&#24490;&#22797;&#26434;&#30340;&#26102;&#38388;&#27169;&#24335;&#26102;&#65292;&#36825;&#20010;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26102;&#38388;&#34920;&#24449;&#30340;&#27010;&#24565;&#65292;&#21363;&#19968;&#32452;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;SWoTTeD (Sliding Window for Temporal Tensor Decomposition)&#26041;&#27861;&#65292;&#19968;&#31181;&#21457;&#29616;&#38544;&#34255;&#26102;&#38388;&#27169;&#24335;&#30340;&#26032;&#26041;&#27861;&#12290;SWoTTeD&#38598;&#25104;&#20102;&#22810;&#31181;&#32422;&#26463;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#25552;&#21462;&#21040;&#30340;&#34920;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#24052;&#40654;&#22823;&#23398;&#21307;&#38498;&#30340;&#25968;&#25454;&#30340;&#21407;&#22987;&#29992;&#20363;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SWoTTeD&#33021;&#22815;&#33267;&#23569;&#19982;&#26368;&#26032;&#30340;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#27169;&#22411;&#19968;&#26679;&#20934;&#30830;&#22320;&#37325;&#24314;&#25968;&#25454;&#65292;&#24182;&#25552;&#21462;&#21040;&#23545;&#20020;&#24202;&#21307;&#29983;&#26377;&#24847;&#20041;&#30340;&#26102;&#38388;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor decomposition has recently been gaining attention in the machine learning community for the analysis of individual traces, such as Electronic Health Records (EHR). However, this task becomes significantly more difficult when the data follows complex temporal patterns. This paper introduces the notion of a temporal phenotype as an arrangement of features over time and it proposes SWoTTeD (Sliding Window for Temporal Tensor Decomposition), a novel method to discover hidden temporal patterns. SWoTTeD integrates several constraints and regularizations to enhance the interpretability of the extracted phenotypes. We validate our proposal using both synthetic and real-world datasets, and we present an original usecase using data from the Greater Paris University Hospital. The results show that SWoTTeD achieves at least as accurate reconstruction as recent state-of-the-art tensor decomposition models, and extracts temporal phenotypes that are meaningful for clinicians.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#34920;&#36798;&#33021;&#21147;&#21644;$k$&#32500;Weisfeiler-Leman ($k$WL)&#27979;&#35797;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#21457;&#29616;&#20102;$k$WL&#27979;&#35797;&#21487;&#20197;&#26377;&#25928;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#20986;&#29616;&#27425;&#25968;&#30340;&#27169;&#24335;&#22270;$P$&#30340;&#22270;&#24418;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#24335;&#22270;&#35745;&#25968;&#38382;&#39064;&#30340;&#26368;&#23567;&#32500;&#24230;$k$&#12290;</title><link>http://arxiv.org/abs/2309.17053</link><description>&lt;p&gt;
&#20851;&#20110;Weisfeiler-Leman&#27979;&#35797;&#22312;&#22270;&#24418;&#27169;&#24335;&#21442;&#25968;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters. (arXiv:2309.17053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#34920;&#36798;&#33021;&#21147;&#21644;$k$&#32500;Weisfeiler-Leman ($k$WL)&#27979;&#35797;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#21457;&#29616;&#20102;$k$WL&#27979;&#35797;&#21487;&#20197;&#26377;&#25928;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#20986;&#29616;&#27425;&#25968;&#30340;&#27169;&#24335;&#22270;$P$&#30340;&#22270;&#24418;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#24335;&#22270;&#35745;&#25968;&#38382;&#39064;&#30340;&#26368;&#23567;&#32500;&#24230;$k$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;$k$&#32500;Weisfeiler-Leman&#65288;$k$WL&#65289;&#27979;&#35797;&#20043;&#38388;&#30340;&#30452;&#25509;&#23545;&#24212;&#20851;&#31995;&#65292;$k$WL&#27979;&#35797;&#26159;&#19968;&#31181;&#24191;&#20026;&#35748;&#21487;&#30340;&#29992;&#20110;&#39564;&#35777;&#22270;&#21516;&#26500;&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#36830;&#25509;&#37325;&#26032;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;$k$WL&#27979;&#35797;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#30340;&#29305;&#23450;&#22270;&#23646;&#24615;&#30340;&#20852;&#36259;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#30340;&#20013;&#24515;&#26159;&#30830;&#23450;&#26368;&#23567;&#32500;&#24230;$k$&#65292;&#20351;&#24471;$k$WL&#21487;&#20197;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#20986;&#29616;&#27425;&#25968;&#30340;&#27169;&#24335;&#22270;$P$&#30340;&#22270;&#24418;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26368;&#23567;$k$&#31216;&#20026;&#36825;&#20010;&#27169;&#24335;&#35745;&#25968;&#38382;&#39064;&#30340;WL&#32500;&#24230;&#12290;&#36825;&#20010;&#35843;&#26597;&#20256;&#32479;&#19978;&#25506;&#35752;&#19982;&#22270;&#26696;&#30456;&#20851;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#35745;&#25968;&#38382;&#39064;&#65306;&#23376;&#22270;&#35745;&#25968;&#21644;&#35825;&#23548;&#23376;&#22270;&#35745;&#25968;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#23613;&#31649;&#23427;&#20204;&#26368;&#21021;&#30475;&#36215;&#26469;&#26159;&#20855;&#26377;&#30475;&#20284;&#19981;&#21516;&#26041;&#27861;&#30340;&#29420;&#31435;&#25361;&#25112;&#65292;&#20294;&#36825;&#20004;&#20010;&#38382;&#39064;&#37117;&#26159;&#19968;&#20010;&#26356;&#20840;&#38754;&#38382;&#39064;&#30340;&#30456;&#20114;&#20851;&#32852;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seminal research in the field of graph neural networks (GNNs) has revealed a direct correspondence between the expressive capabilities of GNNs and the $k$-dimensional Weisfeiler-Leman ($k$WL) test, a widely-recognized method for verifying graph isomorphism. This connection has reignited interest in comprehending the specific graph properties effectively distinguishable by the $k$WL test. A central focus of research in this field revolves around determining the least dimensionality $k$, for which $k$WL can discern graphs with different number of occurrences of a pattern graph $P$. We refer to such a least $k$ as the WL-dimension of this pattern counting problem. This inquiry traditionally delves into two distinct counting problems related to patterns: subgraph counting and induced subgraph counting. Intriguingly, despite their initial appearance as separate challenges with seemingly divergent approaches, both of these problems are interconnected components of a more comprehensive proble
&lt;/p&gt;</description></item><item><title>GPSINDy&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#19982;SINDy&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#27169;&#22411;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#31995;&#32479;&#21160;&#24577;&#21644;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#26041;&#38754;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11076</link><description>&lt;p&gt;
GPSINDy: &#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#26041;&#31243;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GPSINDy: Data-Driven Discovery of Equations of Motion. (arXiv:2309.11076v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11076
&lt;/p&gt;
&lt;p&gt;
GPSINDy&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#19982;SINDy&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#27169;&#22411;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#31995;&#32479;&#21160;&#24577;&#21644;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#26041;&#38754;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32771;&#34385;&#20174;&#26377;&#22122;&#22768;&#25968;&#25454;&#20013;&#21457;&#29616;&#21160;&#21147;&#23398;&#31995;&#32479;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#24050;&#30693;&#22122;&#22768;&#23384;&#22312;&#23545;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;&#19968;&#31181;&#38750;&#21442;&#25968;&#23398;&#20064;&#26041;&#27861;&#65289;&#19982;SINDy&#65288;&#19968;&#31181;&#21442;&#25968;&#23398;&#20064;&#26041;&#27861;&#65289;&#30456;&#32467;&#21512;&#65292;&#20174;&#25968;&#25454;&#20013;&#35782;&#21035;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#19982;SINDy&#30456;&#27604;&#22312;&#26377;&#22122;&#22768;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#22312;Lotka-Volterra&#27169;&#22411;&#21644;&#20223;&#30495;&#20013;&#30340;&#21333;&#36718;&#36710;&#21160;&#24577;&#27169;&#22411;&#19978;&#20197;&#21450;&#22312;&#20351;&#29992;&#30828;&#20214;&#25968;&#25454;&#30340;NVIDIA JetRacer&#31995;&#32479;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;SINDy&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21457;&#29616;&#31995;&#32479;&#21160;&#24577;&#21644;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#26041;&#38754;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of discovering dynamical system models from noisy data. The presence of noise is known to be a significant problem for symbolic regression algorithms. We combine Gaussian process regression, a nonparametric learning method, with SINDy, a parametric learning approach, to identify nonlinear dynamical systems from data. The key advantages of our proposed approach are its simplicity coupled with the fact that it demonstrates improved robustness properties with noisy data over SINDy. We demonstrate our proposed approach on a Lotka-Volterra model and a unicycle dynamic model in simulation and on an NVIDIA JetRacer system using hardware data. We demonstrate improved performance over SINDy for discovering the system dynamics and predicting future trajectories.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#19982;&#29289;&#29702;&#21551;&#21457;&#24335;&#36873;&#25321;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#21487;&#24494;&#20998;&#28237;&#27969;&#27169;&#22411;&#65292;&#22312;&#22823;&#28065;&#27169;&#25311;&#20013;&#26377;&#25928;&#22320;&#24314;&#27169;&#23376;&#32593;&#26684;&#23610;&#24230;&#28237;&#27969;&#65292;&#24182;&#33021;&#25512;&#24191;&#21040;&#22810;&#31181;&#27969;&#21160;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2307.03683</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#28237;&#27969;
&lt;/p&gt;
&lt;p&gt;
Differentiable Turbulence. (arXiv:2307.03683v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03683
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#29289;&#29702;&#21551;&#21457;&#24335;&#36873;&#25321;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#21487;&#24494;&#20998;&#28237;&#27969;&#27169;&#22411;&#65292;&#22312;&#22823;&#28065;&#27169;&#25311;&#20013;&#26377;&#25928;&#22320;&#24314;&#27169;&#23376;&#32593;&#26684;&#23610;&#24230;&#28237;&#27969;&#65292;&#24182;&#33021;&#25512;&#24191;&#21040;&#22810;&#31181;&#27969;&#21160;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#25913;&#36827;&#22823;&#28065;&#27169;&#25311;(SGS)&#20013;&#23376;&#32593;&#26684;&#23610;&#24230;&#28237;&#27969;&#38381;&#21512;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#36234;&#26469;&#36234;&#20855;&#26377;&#28508;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#21487;&#24494;&#20998;&#28237;&#27969;&#30340;&#27010;&#24565;&#65292;&#22312;&#20351;&#29992;&#29289;&#29702;&#21551;&#21457;&#24335;&#36873;&#25321;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#19982;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#20102;&#36866;&#29992;&#20110;&#20108;&#32500;&#28237;&#27969;&#27969;&#21160;&#30340;&#39640;&#25928;&#19988;&#22810;&#21151;&#33021;&#30340;SGS&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#25152;&#36873;&#25321;&#30340;&#26550;&#26500;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#21457;&#29616;&#21253;&#21547;&#23567;&#23610;&#24230;&#30340;&#38750;&#23616;&#37096;&#29305;&#24449;&#23545;&#20110;&#26377;&#25928;&#30340;SGS&#24314;&#27169;&#26368;&#20026;&#20851;&#38190;&#65292;&#32780;&#22823;&#23610;&#24230;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#20107;&#21518;&#35299;&#22330;&#30340;&#28857;&#31934;&#30830;&#24230;&#12290;&#28388;&#27874;&#36895;&#24230;&#26799;&#24230;&#24352;&#37327;&#21487;&#20197;&#36890;&#36807;&#23558;&#36755;&#20837;&#21644;&#36755;&#20986;&#20998;&#35299;&#20026;&#21508;&#21521;&#21516;&#24615;&#12289;&#20559;&#31163;&#21516;&#24615;&#21644;&#21453;&#23545;&#31216;&#20998;&#37327;&#26469;&#30452;&#25509;&#26144;&#23556;&#21040;SGS&#24212;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#22810;&#31181;&#27969;&#21160;&#37197;&#32622;&#65292;&#21253;&#25324;&#36739;&#39640;&#21644;&#36739;&#20302;&#30340;&#36895;&#24230; Reynolds &#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is increasingly becoming a promising pathway to improving the accuracy of sub-grid scale (SGS) turbulence closure models for large eddy simulations (LES). We leverage the concept of differentiable turbulence, whereby an end-to-end differentiable solver is used in combination with physics-inspired choices of deep learning architectures to learn highly effective and versatile SGS models for two-dimensional turbulent flow. We perform an in-depth analysis of the inductive biases in the chosen architectures, finding that the inclusion of small-scale non-local features is most critical to effective SGS modeling, while large-scale features can improve pointwise accuracy of the a-posteriori solution field. The filtered velocity gradient tensor can be mapped directly to the SGS stress via decomposition of the inputs and outputs into isotropic, deviatoric, and anti-symmetric components. We see that the model can generalize to a variety of flow configurations, including higher and l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#21644;&#35823;&#24046;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#27668;&#27873;&#24341;&#36215;&#30340;&#30913;&#22330;&#27874;&#21160;&#65292;&#37325;&#24314;&#30005;&#35299;&#36807;&#31243;&#20013;&#30340;&#27668;&#27873;&#20998;&#24067;&#21644;&#30005;&#23548;&#29575;&#22270;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02496</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#21644;&#35823;&#24046;&#25193;&#25955;&#23398;&#20064;&#21033;&#29992;&#23548;&#30005;&#22270;&#37325;&#24314;&#27668;&#27873;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Learning to reconstruct the bubble distribution with conductivity maps using Invertible Neural Networks and Error Diffusion. (arXiv:2307.02496v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#21644;&#35823;&#24046;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#27668;&#27873;&#24341;&#36215;&#30340;&#30913;&#22330;&#27874;&#21160;&#65292;&#37325;&#24314;&#30005;&#35299;&#36807;&#31243;&#20013;&#30340;&#27668;&#27873;&#20998;&#24067;&#21644;&#30005;&#23548;&#29575;&#22270;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#35299;&#26159;&#29615;&#20445;&#30340;&#27682;&#27668;&#29983;&#20135;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#65292;&#20294;&#26159;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#27668;&#27873;&#20250;&#38459;&#30861;&#21453;&#24212;&#65292;&#38477;&#20302;&#30005;&#27744;&#25928;&#29575;&#65292;&#22686;&#21152;&#33021;&#37327;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27668;&#27873;&#20250;&#23548;&#33268;&#30005;&#27744;&#20869;&#37096;&#30340;&#30005;&#23548;&#29575;&#21457;&#29983;&#21464;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#21608;&#22260;&#20135;&#29983;&#24863;&#24212;&#30913;&#22330;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#22806;&#37096;&#30913;&#20256;&#24863;&#22120;&#27979;&#37327;&#36825;&#20123;&#27668;&#27873;&#24341;&#36215;&#30340;&#30913;&#22330;&#27874;&#21160;&#65292;&#24182;&#27714;&#35299;Biot-Savart&#23450;&#24459;&#30340;&#21453;&#38382;&#39064;&#65292;&#21487;&#20197;&#20272;&#35745;&#30005;&#27744;&#20869;&#30340;&#30005;&#23548;&#29575;&#65292;&#20174;&#32780;&#24471;&#21040;&#27668;&#27873;&#30340;&#22823;&#23567;&#21644;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#20165;&#20973;&#20960;&#20010;&#24863;&#24212;&#30913;&#22330;&#27979;&#37327;&#20540;&#30830;&#23450;&#39640;&#20998;&#36776;&#29575;&#30005;&#23548;&#29575;&#22270;&#26159;&#19968;&#20010;&#30149;&#24577;&#21453;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#37325;&#24314;&#30005;&#23548;&#29575;&#22330;&#12290;&#25105;&#20204;&#30340;&#23450;&#24615;&#32467;&#26524;&#21644;&#20351;&#29992;&#38543;&#26426;&#35823;&#24046;&#25193;&#25955;&#30340;&#23450;&#37327;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;Tikhonov&#27491;&#21017;&#21270;&#30456;&#27604;&#65292;INN&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrolysis is crucial for eco-friendly hydrogen production, but gas bubbles generated during the process hinder reactions, reduce cell efficiency, and increase energy consumption. Additionally, these gas bubbles cause changes in the conductivity inside the cell, resulting in corresponding variations in the induced magnetic field around the cell. Therefore, measuring these gas bubble-induced magnetic field fluctuations using external magnetic sensors and solving the inverse problem of Biot-Savart Law allows for estimating the conductivity in the cell and, thus, bubble size and location. However, determining high-resolution conductivity maps from only a few induced magnetic field measurements is an ill-posed inverse problem. To overcome this, we exploit Invertible Neural Networks (INNs) to reconstruct the conductivity field. Our qualitative results and quantitative evaluation using random error diffusion show that INN achieves far superior performance compared to Tikhonov regularizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRP&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;&#26469;&#26174;&#33879;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36127;&#25285;&#65292;&#24182;&#39318;&#27425;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17563</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#25991;&#26412;&#25490;&#24207;&#22120;&#65292;&#20855;&#26377;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. (arXiv:2306.17563v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRP&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;&#26469;&#26174;&#33879;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36127;&#25285;&#65292;&#24182;&#39318;&#27425;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#30452;&#25509;&#23558;&#26597;&#35810;&#21644;&#20505;&#36873;&#25991;&#26723;&#36755;&#20837;&#25552;&#31034;&#36827;&#34892;&#25991;&#26723;&#25490;&#24207;&#26159;&#19968;&#20010;&#26377;&#36259;&#19988;&#23454;&#29992;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#24456;&#38590;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#31934;&#35843;&#22522;&#20934;&#25490;&#24207;&#22120;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#30340;&#28857;&#23545;&#28857;&#21644;&#21015;&#34920;&#25490;&#24207;&#25552;&#31034;&#65292;&#24182;&#35748;&#20026;&#29616;&#25104;&#30340;LLM&#27809;&#26377;&#23436;&#20840;&#29702;&#35299;&#36825;&#20123;&#25490;&#24207;&#20844;&#24335;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;LLM&#30340;&#35757;&#32451;&#26041;&#24335;&#30340;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20004;&#20004;&#25490;&#21517;&#25552;&#31034;&#65288;PRP&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;LLM&#30340;&#36127;&#25285;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#25991;&#29486;&#20013;&#39318;&#27425;&#20351;&#29992;&#20013;&#31561;&#35268;&#27169;&#30340;&#24320;&#28304;LLM&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25490;&#21517;&#24615;&#33021;&#12290;&#22312;TREC-DL2020&#19978;&#65292;&#22522;&#20110;20B&#21442;&#25968;&#30340;Flan-UL2&#27169;&#22411;&#30340;PRP&#36229;&#36807;&#20102;&#25991;&#29486;&#20013;&#22522;&#20110;&#21830;&#19994;&#40657;&#30418;GPT-4&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, there has been limited success so far, as researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these ranking formulations, possibly due to the nature of how LLMs are trained. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL2020, PRP based on the Flan-UL2 model with 20B parameters outperforms the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#25442;&#31169;&#26377;&#35266;&#27979;&#20449;&#24687;&#65292;&#20195;&#29702;&#21487;&#20197;&#38598;&#20307;&#20272;&#35745;&#26410;&#30693;&#25968;&#37327;&#65292;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#32447;&#24615;&#32858;&#21512;&#26041;&#26696;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35843;&#25972;&#30340;&#38543;&#26426;&#21270;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#39640;&#25928;&#32452;&#21512;&#35266;&#27979;&#25968;&#25454;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15865</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Distributed Estimation and Learning. (arXiv:2306.15865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#20132;&#25442;&#31169;&#26377;&#35266;&#27979;&#20449;&#24687;&#65292;&#20195;&#29702;&#21487;&#20197;&#38598;&#20307;&#20272;&#35745;&#26410;&#30693;&#25968;&#37327;&#65292;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#32447;&#24615;&#32858;&#21512;&#26041;&#26696;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35843;&#25972;&#30340;&#38543;&#26426;&#21270;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#39640;&#25928;&#32452;&#21512;&#35266;&#27979;&#25968;&#25454;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#20132;&#25442;&#20449;&#24687;&#26469;&#20272;&#35745;&#20174;&#20854;&#31169;&#19979;&#35266;&#23519;&#30340;&#26679;&#26412;&#20013;&#26410;&#30693;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;&#36890;&#36807;&#20132;&#25442;&#31169;&#26377;&#35266;&#27979;&#20449;&#24687;&#65292;&#20195;&#29702;&#21487;&#20197;&#38598;&#20307;&#20272;&#35745;&#26410;&#30693;&#25968;&#37327;&#65292;&#20294;&#20182;&#20204;&#20063;&#38754;&#20020;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#32858;&#21512;&#26041;&#26696;&#30340;&#30446;&#26631;&#26159;&#22312;&#26102;&#38388;&#21644;&#32593;&#32476;&#20013;&#39640;&#25928;&#22320;&#32452;&#21512;&#35266;&#27979;&#25968;&#25454;&#65292;&#21516;&#26102;&#28385;&#36275;&#20195;&#29702;&#30340;&#38544;&#31169;&#38656;&#27714;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#36229;&#36234;&#20182;&#20204;&#26412;&#22320;&#38468;&#36817;&#30340;&#21327;&#35843;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#21442;&#19982;&#30340;&#20195;&#29702;&#33021;&#22815;&#20174;&#31163;&#32447;&#25110;&#38543;&#26102;&#38388;&#22312;&#32447;&#33719;&#21462;&#30340;&#31169;&#26377;&#20449;&#21495;&#20013;&#20272;&#35745;&#23436;&#25972;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#24182;&#20445;&#25252;&#20854;&#20449;&#21495;&#21644;&#32593;&#32476;&#38468;&#36817;&#30340;&#38544;&#31169;&#12290;&#36825;&#26159;&#36890;&#36807;&#32447;&#24615;&#32858;&#21512;&#26041;&#26696;&#21644;&#35843;&#25972;&#30340;&#38543;&#26426;&#21270;&#26041;&#26696;&#23454;&#29616;&#30340;&#65292;&#23558;&#22122;&#22768;&#28155;&#21152;&#21040;&#20132;&#25442;&#30340;&#20272;&#35745;&#25968;&#25454;&#20013;&#20197;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distributed estimation and learning problems in a networked environment in which agents exchange information to estimate unknown statistical properties of random variables from their privately observed samples. By exchanging information about their private observations, the agents can collectively estimate the unknown quantities, but they also face privacy risks. The goal of our aggregation schemes is to combine the observed data efficiently over time and across the network, while accommodating the privacy needs of the agents and without any coordination beyond their local neighborhoods. Our algorithms enable the participating agents to estimate a complete sufficient statistic from private signals that are acquired offline or online over time, and to preserve the privacy of their signals and network neighborhoods. This is achieved through linear aggregation schemes with adjusted randomization schemes that add noise to the exchanged estimates subject to differential privacy (DP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#32422;&#26463;&#39033;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#19981;&#36275;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12627</link><description>&lt;p&gt;
&#38024;&#23545;&#24322;&#24120;&#26816;&#27979;&#30340;&#30446;&#26631;&#22604;&#32553;&#27491;&#21017;&#21270;&#33258;&#32534;&#30721;&#22120;&#65306;&#20013;&#24515;&#30340;&#40657;&#27934;
&lt;/p&gt;
&lt;p&gt;
Targeted collapse regularized autoencoder for anomaly detection: black hole at the center. (arXiv:2306.12627v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#32422;&#26463;&#39033;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#19981;&#36275;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#24050;&#24191;&#27867;&#29992;&#20110;&#26368;&#36817;&#30340;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#24320;&#21457;&#20013;&#12290;&#23427;&#20204;&#30340;&#24212;&#29992;&#21069;&#25552;&#26159;&#22312;&#27491;&#24120;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#21518;&#65292;&#24322;&#24120;&#36755;&#20837;&#23558;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#37325;&#26500;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#36825;&#20351;&#24471;&#27491;&#24120;&#21644;&#24322;&#24120;&#26679;&#26412;&#20043;&#38388;&#26377;&#20102;&#26126;&#26174;&#30340;&#21306;&#21035;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#65292;&#33258;&#32534;&#30721;&#22120;&#21487;&#20197;&#19968;&#23450;&#31243;&#24230;&#19978;&#27867;&#21270;&#21040;&#27491;&#24120;&#31867;&#20043;&#22806;&#65292;&#24182;&#22312;&#19968;&#20123;&#24322;&#24120;&#26679;&#26412;&#19978;&#23454;&#29616;&#36739;&#23567;&#30340;&#37325;&#26500;&#35823;&#24046;&#12290;&#20026;&#20102;&#25913;&#21892;&#24615;&#33021;&#65292;&#21508;&#31181;&#25216;&#26415;&#25552;&#20986;&#20102;&#20854;&#20182;&#32452;&#20214;&#21644;&#26356;&#22797;&#26434;&#30340;&#35757;&#32451;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#19981;&#26159;&#28155;&#21152;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12289;&#28041;&#21450;&#35745;&#31639;&#21644;&#32321;&#29712;&#30340;&#35757;&#32451;&#65292;&#32780;&#26159;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35843;&#33410;&#34920;&#31034;&#30340;&#33539;&#25968;&#65292;&#29992;&#19968;&#20010;&#35745;&#31639;&#31616;&#21333;&#30340;&#39033;&#26469;&#34917;&#20805;&#37325;&#26500;&#25439;&#22833;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#26368;&#23567;&#21270;&#20102;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoencoders have been extensively used in the development of recent anomaly detection techniques. The premise of their application is based on the notion that after training the autoencoder on normal training data, anomalous inputs will exhibit a significant reconstruction error. Consequently, this enables a clear differentiation between normal and anomalous samples. In practice, however, it is observed that autoencoders can generalize beyond the normal class and achieve a small reconstruction error on some of the anomalous samples. To improve the performance, various techniques propose additional components and more sophisticated training procedures. In this work, we propose a remarkably straightforward alternative: instead of adding neural network components, involved computations, and cumbersome training, we complement the reconstruction loss with a computationally light term that regulates the norm of representations in the latent space. The simplicity of our approach minimizes th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;&#24182;&#34892;&#37327;&#23376;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#20010;&#20855;&#26377;&#37327;&#23376;&#21367;&#31215;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20854;&#20013;&#19968;&#20010;&#28151;&#21512;&#37327;&#23376;&#26041;&#27861;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#36229;&#36807;99%&#30340;&#24778;&#20154;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09224</link><description>&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning for image classification. (arXiv:2304.09224v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;&#24182;&#34892;&#37327;&#23376;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#20010;&#20855;&#26377;&#37327;&#23376;&#21367;&#31215;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20854;&#20013;&#19968;&#20010;&#28151;&#21512;&#37327;&#23376;&#26041;&#27861;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#36229;&#36807;99%&#30340;&#24778;&#20154;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#35782;&#21035;&#21644;&#20998;&#31867;&#26159;&#21508;&#34892;&#21508;&#19994;&#20013;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#26159;&#29616;&#20195;&#19990;&#30028;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#39046;&#22495;&#12290;&#36817;&#24180;&#26469;&#65292;&#23588;&#20854;&#26159;&#31070;&#32463;&#32593;&#32476;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26041;&#27861;&#21033;&#29992;&#37327;&#23376;&#25928;&#24212;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20256;&#32479;&#32463;&#20856;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20004;&#31181;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#27169;&#22411;&#65306;&#19968;&#20010;&#20855;&#26377;&#24182;&#34892;&#37327;&#23376;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#20010;&#20855;&#26377;&#37327;&#23376;&#21367;&#31215;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#20854;&#20013;&#19968;&#20010;&#28151;&#21512;&#37327;&#23376;&#26041;&#27861;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#36229;&#36807;99%&#30340;&#24778;&#20154;&#20934;&#30830;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#65292;&#25152;&#26377;&#21487;&#21464;&#21442;&#25968;&#37117;&#26159;&#21487;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23558;&#37327;&#23376;&#37096;&#20998;&#20998;&#25104;&#22810;&#20010;&#24182;&#34892;&#21487;&#21464;&#37327;&#23376;&#30005;&#36335;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image recognition and classification are fundamental tasks with diverse practical applications across various industries, making them critical in the modern world. Recently, machine learning models, particularly neural networks, have emerged as powerful tools for solving these problems. However, the utilization of quantum effects through hybrid quantum-classical approaches can further enhance the capabilities of traditional classical models. Here, we propose two hybrid quantum-classical models: a neural network with parallel quantum layers and a neural network with a quanvolutional layer, which address image classification problems. One of our hybrid quantum approaches demonstrates remarkable accuracy of more than 99% on the MNIST dataset. Notably, in the proposed quantum circuits all variational parameters are trainable, and we divide the quantum part into multiple parallel variational quantum circuits for efficient neural network learning. In summary, our study contributes to the ong
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GENEA Challenge 2022&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#35813;&#27604;&#36187;&#26088;&#22312;&#22522;&#20934;&#27979;&#35797;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33258;&#21160;&#20849;&#21516;&#35821;&#35328;&#25163;&#21183;&#29983;&#25104;&#12290;&#20351;&#29992;&#20855;&#26377;&#30456;&#21516;&#35821;&#38899;&#21644;&#21160;&#20316;&#30340;&#25968;&#25454;&#38598;&#65292;&#20247;&#22810;&#21442;&#36187;&#22242;&#38431;&#30340;&#25163;&#21183;&#29983;&#25104;&#31995;&#32479;&#22312;&#20960;&#20010;&#22823;&#22411;&#29992;&#25143;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#22240;&#27492;&#33021;&#22815;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2303.08737</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24320;&#25918;&#25361;&#25112;&#20013;&#35780;&#20272;&#25163;&#21183;&#29983;&#25104;&#65306;GENEA Challenge 2022&#30340;&#30740;&#31350;&#25253;&#21578;(arXiv:2303.08737v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
Evaluating gesture-generation in a large-scale open challenge: The GENEA Challenge 2022. (arXiv:2303.08737v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GENEA Challenge 2022&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#35813;&#27604;&#36187;&#26088;&#22312;&#22522;&#20934;&#27979;&#35797;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33258;&#21160;&#20849;&#21516;&#35821;&#35328;&#25163;&#21183;&#29983;&#25104;&#12290;&#20351;&#29992;&#20855;&#26377;&#30456;&#21516;&#35821;&#38899;&#21644;&#21160;&#20316;&#30340;&#25968;&#25454;&#38598;&#65292;&#20247;&#22810;&#21442;&#36187;&#22242;&#38431;&#30340;&#25163;&#21183;&#29983;&#25104;&#31995;&#32479;&#22312;&#20960;&#20010;&#22823;&#22411;&#29992;&#25143;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#22240;&#27492;&#33021;&#22815;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#31532;&#20108;&#23626;GENEA Challenge&#65292;&#23545;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33258;&#21160;&#20849;&#21516;&#35821;&#35328;&#25163;&#21183;&#29983;&#25104;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#21442;&#36187;&#22242;&#38431;&#20351;&#29992;&#30456;&#21516;&#30340;&#35821;&#38899;&#21644;&#36816;&#21160;&#25968;&#25454;&#38598;&#26500;&#24314;&#25163;&#21183;&#29983;&#25104;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#29983;&#25104;&#30340;&#21160;&#20316;&#20351;&#29992;&#26631;&#20934;&#21270;&#30340;&#21487;&#35270;&#21270;&#31649;&#36947;&#28210;&#26579;&#20026;&#35270;&#39057;&#65292;&#24182;&#22312;&#20960;&#20010;&#22823;&#22411;&#20247;&#21253;&#29992;&#25143;&#30740;&#31350;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#19982;&#27604;&#36739;&#19981;&#21516;&#30740;&#31350;&#35770;&#25991;&#26102;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#37324;&#30340;&#32467;&#26524;&#24046;&#24322;&#20165;&#30001;&#20110;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31995;&#32479;&#20043;&#38388;&#30340;&#30452;&#25509;&#27604;&#36739;&#12290;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;18&#23567;&#26102;&#30340;&#20840;&#36523;&#21160;&#20316;&#25429;&#25417;&#65292;&#21253;&#25324;&#25163;&#25351;&#65292;&#24182;&#35760;&#24405;&#20102;&#19981;&#21516;&#20154;&#29289;&#21442;&#19982;&#21452;&#20154; &#23545;&#35805;&#12290;&#21313;&#20010;&#22242;&#38431;&#21442;&#21152;&#20102;&#20998;&#20026;&#20840;&#36523;&#21644;&#19978;&#21322;&#36523;&#32930;&#20307;&#34920;&#36798;&#30340;&#20004;&#20010;&#23618;&#27425;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#27599;&#20010;&#23618;&#27425;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25163;&#21183;&#36816;&#21160;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#21644;&#20854;&#23545;&#29305;&#23450;&#35821;&#38899;&#20449;&#21495;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#23558;&#20154;&#31867;&#30456;&#20284;&#24230;&#19982;&#25163;&#21183;&#36866;&#29992;&#24615;&#35299;&#34261;&#24320;&#65292;&#36825;&#19968;&#28857;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reports on the second GENEA Challenge to benchmark data-driven automatic co-speech gesture generation. Participating teams used the same speech and motion dataset to build gesture-generation systems. Motion generated by all these systems was rendered to video using a standardised visualisation pipeline and evaluated in several large, crowdsourced user studies. Unlike when comparing different research papers, differences in results are here only due to differences between methods, enabling direct comparison between systems. The dataset was based on 18 hours of full-body motion capture, including fingers, of different persons engaging in a dyadic conversation. Ten teams participated in the challenge across two tiers: full-body and upper-body gesticulation. For each tier, we evaluated both the human-likeness of the gesture motion and its appropriateness for the specific speech signal. Our evaluations decouple human-likeness from gesture appropriateness, which has been a difficu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#30340;GAN&#21644;VAE&#27169;&#22411;&#20013;&#28040;&#38500;&#29305;&#23450;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05699</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;GAN&#21644;VAE&#30340;&#29305;&#24449;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Feature Unlearning for Pre-trained GANs and VAEs. (arXiv:2303.05699v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#30340;GAN&#21644;VAE&#27169;&#22411;&#20013;&#28040;&#38500;&#29305;&#23450;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;GAN&#21644;VAE&#65289;&#20013;&#28040;&#38500;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;&#19982;&#24120;&#35265;&#30340;&#28040;&#38500;&#20219;&#21153;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#28040;&#38500;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#38754;&#37096;&#22270;&#20687;&#20013;&#30340;&#21457;&#22411;&#12290;&#30001;&#20110;&#30446;&#26631;&#29305;&#24449;&#20165;&#20986;&#29616;&#22312;&#22270;&#20687;&#30340;&#23616;&#37096;&#21306;&#22495;&#20013;&#65292;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#28040;&#38500;&#25972;&#20010;&#22270;&#20687;&#21487;&#33021;&#23548;&#33268;&#22833;&#21435;&#22270;&#20687;&#21097;&#20313;&#21306;&#22495;&#20013;&#30340;&#20854;&#20182;&#32454;&#33410;&#12290;&#20026;&#20102;&#25351;&#23450;&#35201;&#28040;&#38500;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#25910;&#38598;&#21253;&#21547;&#30446;&#26631;&#29305;&#24449;&#30340;&#38543;&#26426;&#29983;&#25104;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35782;&#21035;&#19982;&#30446;&#26631;&#29305;&#24449;&#23545;&#24212;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#34920;&#31034;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MNIST&#21644;CelebA&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25104;&#21151;&#21024;&#38500;&#30446;&#26631;&#29305;&#24449;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#36827;&#19968;&#27493;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#23454;&#39564;&#35777;&#26126;&#20102;&#28040;&#38500;&#21518;&#30340;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of feature unlearning from a pre-trained image generative model: GANs and VAEs. Unlike a common unlearning task where an unlearning target is a subset of the training set, we aim to unlearn a specific feature, such as hairstyle from facial images, from the pre-trained generative models. As the target feature is only presented in a local region of an image, unlearning the entire image from the pre-trained model may result in losing other details in the remaining region of the image. To specify which features to unlearn, we collect randomly generated images that contain the target features. We then identify a latent representation corresponding to the target feature and then use the representation to fine-tune the pre-trained model. Through experiments on MNIST and CelebA datasets, we show that target features are successfully removed while keeping the fidelity of the original models. Further experiments with an adversarial attack show that the unlearned model is mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#24773;&#20917;&#19979;&#23545;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#38450;&#24481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#31363;&#21462;&#25216;&#26415;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#32593;&#32476;&#65292;&#21516;&#26102;&#37319;&#29992;&#23567;&#27874;&#22122;&#22768;&#21435;&#38500;&#22120;&#65288;WNR&#65289;&#20943;&#23569;&#23545;&#25239;&#24615;&#27745;&#26579;&#12290;</title><link>http://arxiv.org/abs/2211.01579</link><description>&lt;p&gt;
&#26080;&#25968;&#25454;&#24773;&#20917;&#19979;&#23545;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#38450;&#24481;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-free Defense of Black Box Models Against Adversarial Attacks. (arXiv:2211.01579v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#24773;&#20917;&#19979;&#23545;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#38450;&#24481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#31363;&#21462;&#25216;&#26415;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#32593;&#32476;&#65292;&#21516;&#26102;&#37319;&#29992;&#23567;&#27874;&#22122;&#22768;&#21435;&#38500;&#22120;&#65288;WNR&#65289;&#20943;&#23569;&#23545;&#25239;&#24615;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20844;&#21496;&#36890;&#36807;API&#20165;&#23558;&#35757;&#32451;&#22909;&#30340;&#28145;&#24230;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#26292;&#38706;&#32473;&#31532;&#19977;&#26041;&#29992;&#25143;&#65292;&#20197;&#20445;&#25252;&#27169;&#22411;&#30340;&#32454;&#33410;&#65288;&#22914;&#26550;&#26500;&#12289;&#23398;&#20064;&#26435;&#37325;&#12289;&#35757;&#32451;&#32454;&#33410;&#31561;&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#40657;&#30418;&#27169;&#22411;&#22312;&#26080;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#26032;&#22411;&#38450;&#24481;&#26426;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#31363;&#21462;&#25216;&#26415;&#35757;&#32451;&#26367;&#20195;&#27169;&#22411;&#32593;&#32476;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#25200;&#21160;&#26679;&#26412;&#19978;&#30340;&#23545;&#25239;&#24615;&#27745;&#26579;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#23567;&#27874;&#22122;&#22768;&#21435;&#38500;&#22120;&#8221;(WNR)&#65292;&#23427;&#22312;&#36755;&#20837;&#22270;&#20687;&#19978;&#25191;&#34892;&#31163;&#25955;&#23567;&#27874;&#20998;&#35299;&#65292;&#24182;&#20165;&#36873;&#25321;&#25105;&#20204;&#30340;&#8220;&#23567;&#27874;&#31995;&#25968;&#36873;&#25321;&#27169;&#22359;&#8221;(WCSM)&#30830;&#23450;&#30340;&#23569;&#25968;&#37325;&#35201;&#31995;&#25968;&#12290;&#20026;&#20102;&#22312;&#36890;&#36807;WNR&#21435;&#38500;&#22122;&#22768;&#21518;&#24674;&#22797;&#22270;&#20687;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35757;&#32451;&#20102;&#19968;&#20010;&#8220;&#20877;&#29983;&#22120;&#8221;&#32593;&#32476;&#65292;&#30446;&#26631;&#26159;&#24674;&#22797;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several companies often safeguard their trained deep models (i.e., details of architecture, learnt weights, training details etc.) from third-party users by exposing them only as black boxes through APIs. Moreover, they may not even provide access to the training data due to proprietary reasons or sensitivity concerns. In this work, we propose a novel defense mechanism for black box models against adversarial attacks in a data-free set up. We construct synthetic data via generative model and train surrogate network using model stealing techniques. To minimize adversarial contamination on perturbed samples, we propose 'wavelet noise remover' (WNR) that performs discrete wavelet decomposition on input images and carefully select only a few important coefficients determined by our 'wavelet coefficient selection module' (WCSM). To recover the high-frequency content of the image after noise removal via WNR, we further train a 'regenerator' network with the objective of retrieving the coeffi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;</title><link>http://arxiv.org/abs/2204.11970</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#23545;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#36827;&#34892;&#35270;&#21147;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System. (arXiv:2204.11970v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#30524;&#31185;&#23398;&#20013;&#30340;&#29627;&#29827;&#20307;&#25163;&#26415;&#33647;&#29289;&#27835;&#30103;&#26159;&#27835;&#30103;&#24180;&#40836;&#30456;&#20851;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#12289;&#31958;&#23615;&#30149;&#24615;&#40644;&#26001;&#27700;&#32959;&#65288;DME&#65289;&#21644;&#35270;&#32593;&#33180;&#38745;&#33033;&#38459;&#22622;&#65288;RVO&#65289;&#30456;&#20851;&#30142;&#30149;&#30340;&#19968;&#31181;&#26222;&#36941;&#27835;&#30103;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#65292;&#24739;&#32773;&#24448;&#24448;&#20250;&#22312;&#22810;&#24180;&#26102;&#38388;&#20869;&#22833;&#21435;&#35270;&#21147;&#65292;&#23613;&#31649;&#25509;&#21463;&#27835;&#30103;&#12290;&#26412;&#25991;&#37319;&#29992;&#22810;&#31181;IT&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#25104;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#34701;&#21512;&#20102;&#24503;&#22269;&#19968;&#23478;&#26368;&#20339;&#21307;&#30103;&#20445;&#20581;&#21307;&#38498;&#30340;&#30524;&#31185;&#37096;&#38376;&#30340;&#19981;&#21516;IT&#31995;&#32479;&#12290;&#32463;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#24739;&#32773;&#35270;&#21147;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#19977;&#31181;&#30142;&#30149;&#30340;&#39044;&#27979;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#24037;&#20855;&#65292;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In ophthalmology, intravitreal operative medication therapy (IVOM) is a widespread treatment for diseases related to the age-related macular degeneration (AMD), the diabetic macular edema (DME), as well as the retinal vein occlusion (RVO). However, in real-world settings, patients often suffer from loss of vision on time scales of years despite therapy, whereas the prediction of the visual acuity (VA) and the earliest possible detection of deterioration under real-life conditions is challenging due to heterogeneous and incomplete data. In this contribution, we present a workflow for the development of a research-compatible data corpus fusing different IT systems of the department of ophthalmology of a German maximum care hospital. The extensive data corpus allows predictive statements of the expected progression of a patient and his or her VA in each of the three diseases. We found out for the disease AMD a significant deterioration of the visual acuity over time. Within our proposed m
&lt;/p&gt;</description></item></channel></rss>