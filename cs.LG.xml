<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#27169;&#22411;&#23545;&#22495;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#38598;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#23454;&#29616;&#12290;&#22312;3D&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#38450;&#27490;&#20102;&#23545;&#38750;&#26631;&#20934;&#23545;&#35937;&#28857;&#30340;&#38169;&#35823;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#21521;&#37327;&#26469;&#23545;&#23545;&#35937;&#36827;&#34892;&#23545;&#25239;&#24615;&#21464;&#24418;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#20445;&#25345;&#23427;&#20204;&#30340;&#21512;&#29702;&#24615;&#21644;&#24179;&#28369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15479</link><description>&lt;p&gt;
&#29992;&#20110;&#40065;&#26834;&#24615;&#22495;&#22806;&#39044;&#27979;&#30340;3D&#23545;&#25239;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
3D Adversarial Augmentations for Robust Out-of-Domain Predictions. (arXiv:2308.15479v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#27169;&#22411;&#23545;&#22495;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#38598;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#23454;&#29616;&#12290;&#22312;3D&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#38450;&#27490;&#20102;&#23545;&#38750;&#26631;&#20934;&#23545;&#35937;&#28857;&#30340;&#38169;&#35823;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#21521;&#37327;&#26469;&#23545;&#23545;&#35937;&#36827;&#34892;&#23545;&#25239;&#24615;&#21464;&#24418;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#20445;&#25345;&#23427;&#20204;&#30340;&#21512;&#29702;&#24615;&#21644;&#24179;&#28369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#26080;&#27861;&#27491;&#30830;&#37319;&#26679;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#38271;&#23614;&#37096;&#20998;&#65292;&#35282;&#33853;&#26696;&#20363;&#21644;&#31232;&#26377;&#30340;&#22495;&#22806;&#26679;&#26412;&#20250;&#20005;&#37325;&#24433;&#21709;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#31264;&#23494;&#20219;&#21153;&#65292;&#20363;&#22914;3D&#35821;&#20041;&#20998;&#21106;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#38750;&#26631;&#20934;&#23545;&#35937;&#30340;&#28857;&#21487;&#20197;&#34987;&#38169;&#35823;&#22320;&#20851;&#32852;&#21040;&#20854;&#20182;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#25913;&#21892;&#23545;&#22495;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#38598;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#32452;&#21521;&#37327;&#65292;&#20197;&#23545;&#25239;&#26041;&#24335;&#25913;&#21464;&#23545;&#35937;&#12290;&#20026;&#20102;&#38450;&#27490;&#23545;&#25239;&#26679;&#26412;&#36828;&#31163;&#29616;&#26377;&#25968;&#25454;&#20998;&#24067;&#36807;&#36828;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#32422;&#26463;&#26469;&#20445;&#25345;&#23427;&#20204;&#30340;&#21512;&#29702;&#24615;&#65292;&#30830;&#20445;&#20256;&#24863;&#22120;&#24863;&#30693;&#21644;&#24418;&#29366;&#24179;&#28369;&#24615;&#12290;&#28982;&#21518;&#65292;&#22312;&#35757;&#32451;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#23558;&#23398;&#20064;&#21040;&#30340;&#29420;&#31435;&#20110;&#26679;&#26412;&#30340;&#21521;&#37327;&#24212;&#29992;&#20110;&#21487;&#29992;&#30340;&#23545;&#35937;&#65292;&#36827;&#34892;&#23545;&#25239;&#22686;&#24378;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since real-world training datasets cannot properly sample the long tail of the underlying data distribution, corner cases and rare out-of-domain samples can severely hinder the performance of state-of-the-art models. This problem becomes even more severe for dense tasks, such as 3D semantic segmentation, where points of non-standard objects can be confidently associated to the wrong class. In this work, we focus on improving the generalization to out-of-domain data. We achieve this by augmenting the training set with adversarial examples. First, we learn a set of vectors that deform the objects in an adversarial fashion. To prevent the adversarial examples from being too far from the existing data distribution, we preserve their plausibility through a series of constraints, ensuring sensor-awareness and shapes smoothness. Then, we perform adversarial augmentation by applying the learned sample-independent vectors to the available objects when training a model. We conduct extensive expe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20999;&#21521;&#29305;&#24449;&#35270;&#35282;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32447;&#24615;&#21464;&#25442;&#21644;&#32467;&#26500;&#27491;&#21017;&#21270;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26680;&#23545;&#40784;&#29616;&#35937;&#30340;&#32454;&#33268;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.15478</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#20999;&#21521;&#29305;&#24449;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Tangent Feature Perspective of Neural Networks. (arXiv:2308.15478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20999;&#21521;&#29305;&#24449;&#35270;&#35282;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32447;&#24615;&#21464;&#25442;&#21644;&#32467;&#26500;&#27491;&#21017;&#21270;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26680;&#23545;&#40784;&#29616;&#35937;&#30340;&#32454;&#33268;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#29702;&#35299;&#22312;&#20999;&#21521;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#20854;&#20013;&#29305;&#24449;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#36827;&#34892;&#36716;&#25442;&#12290;&#25105;&#20204;&#32771;&#34385;&#29305;&#24449;&#30340;&#32447;&#24615;&#21464;&#25442;&#65292;&#20174;&#32780;&#36890;&#36807;&#21452;&#32447;&#24615;&#25554;&#20540;&#32422;&#26463;&#22312;&#21442;&#25968;&#21644;&#21464;&#25442;&#19978;&#36827;&#34892;&#32852;&#21512;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#19982;&#20855;&#26377;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#30340;&#31561;&#20215;&#32447;&#24615;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#20855;&#26377;&#36817;&#20284;&#20302;&#31209;&#35299;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25105;&#20204;&#23545;&#29305;&#24449;&#20197;&#21450;&#26680;&#20989;&#25968;&#30340;&#21464;&#21270;&#33719;&#24471;&#20102;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#20026;&#24403;&#30446;&#26631;&#20989;&#25968;&#22312;&#20999;&#21521;&#29305;&#24449;&#19978;&#34920;&#24449;&#19981;&#22909;&#26102;&#30340;&#26680;&#23545;&#40784;&#29616;&#35937;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#38500;&#20102;&#22312;&#31616;&#21333;&#22238;&#24402;&#38382;&#39064;&#19978;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#35266;&#23519;&#32467;&#26524;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20999;&#21521;&#29305;&#24449;&#20998;&#31867;&#30340;&#33258;&#36866;&#24212;&#29305;&#24449;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to better understand feature learning in neural networks, we propose a framework for understanding linear models in tangent feature space where the features are allowed to be transformed during training. We consider linear transformations of features, resulting in a joint optimization over parameters and transformations with a bilinear interpolation constraint. We show that this optimization problem has an equivalent linearly constrained optimization with structured regularization that encourages approximately low rank solutions. Specializing to neural network structure, we gain insights into how the features and thus the kernel function change, providing additional nuance to the phenomenon of kernel alignment when the target function is poorly represented using tangent features. In addition to verifying our theoretical observations in real neural networks on a simple regression problem, we empirically show that an adaptive feature implementation of tangent feature classificat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#30446;&#26631;&#25919;&#31574;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#39044;&#20808;&#23384;&#22312;&#30340;&#25945;&#24072;&#31574;&#30053;&#24341;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#35777;&#26126;&#20102;&#25945;&#24072;&#31574;&#30053;&#22312;&#26080;&#24418;&#29366;&#22870;&#21169;&#19979;&#33021;&#22815;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#25104;&#21151;&#22320;&#32452;&#21512;&#25945;&#24072;&#31574;&#30053;&#24182;&#25193;&#23637;&#25945;&#24072;&#30340;&#31574;&#30053;&#20197;&#35299;&#20915;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.15470</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25919;&#31574;&#32452;&#21512;&#36890;&#36807;&#22810;&#30446;&#26631;&#25919;&#31574;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy composition in reinforcement learning via multi-objective policy optimization. (arXiv:2308.15470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15470
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#30446;&#26631;&#25919;&#31574;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#39044;&#20808;&#23384;&#22312;&#30340;&#25945;&#24072;&#31574;&#30053;&#24341;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#35777;&#26126;&#20102;&#25945;&#24072;&#31574;&#30053;&#22312;&#26080;&#24418;&#29366;&#22870;&#21169;&#19979;&#33021;&#22815;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#25104;&#21151;&#22320;&#32452;&#21512;&#25945;&#24072;&#31574;&#30053;&#24182;&#25193;&#23637;&#25945;&#24072;&#30340;&#31574;&#30053;&#20197;&#35299;&#20915;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30456;&#20851;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#25945;&#24072;&#31574;&#30053;&#65292;&#20351;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#25104;&#21151;&#30340;&#34892;&#20026;&#31574;&#30053;&#12290;&#25945;&#24072;&#31574;&#30053;&#34987;&#24341;&#20837;&#20316;&#20026;&#30446;&#26631;&#65292;&#38500;&#20102;&#20219;&#21153;&#30446;&#26631;&#20197;&#22806;&#65292;&#22312;&#22810;&#30446;&#26631;&#25919;&#31574;&#20248;&#21270;&#30340;&#35774;&#32622;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#30446;&#26631;&#26368;&#22823;&#21518;&#39564;&#25919;&#31574;&#20248;&#21270;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#25945;&#24072;&#31574;&#30053;&#33021;&#22815;&#21152;&#36895;&#23398;&#20064;&#30340;&#36807;&#31243;&#65292;&#23588;&#20854;&#26159;&#22312;&#32570;&#23569;&#24418;&#29366;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#20855;&#26377;&#36830;&#32493;&#35266;&#23519;&#21644;&#34892;&#21160;&#31354;&#38388;&#30340;&#20004;&#20010;&#22495;&#20013;&#65292;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#25104;&#21151;&#22320;&#25353;&#39034;&#24207;&#21644;&#24182;&#34892;&#22320;&#32452;&#21512;&#25945;&#24072;&#31574;&#30053;&#65292;&#24182;&#19988;&#36824;&#33021;&#22815;&#36827;&#19968;&#27493;&#25193;&#23637;&#25945;&#24072;&#30340;&#31574;&#30053;&#20197;&#35299;&#20915;&#20219;&#21153;&#12290;&#26681;&#25454;&#20219;&#21153;&#21644;&#25945;&#24072;&#30340;&#25351;&#23450;&#32452;&#21512;&#65292;&#25945;&#24072;&#21487;&#33021;&#33258;&#28982;&#22320;&#38480;&#21046;&#26234;&#33021;&#20307;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;&#26234;&#33021;&#20307;&#38656;&#35201;&#36981;&#23432;&#25945;&#24072;&#31574;&#30053;&#30340;&#31243;&#24230;&#30001;&#36229;&#21442;&#25968;&#20915;&#23450;&#65292;&#36825;&#20123;&#36229;&#21442;&#25968;&#30830;&#23450;&#20102;&#25945;&#24072;&#31574;&#30053;&#30340;&#24433;&#21709;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We enable reinforcement learning agents to learn successful behavior policies by utilizing relevant pre-existing teacher policies. The teacher policies are introduced as objectives, in addition to the task objective, in a multi-objective policy optimization setting. Using the Multi-Objective Maximum a Posteriori Policy Optimization algorithm \citep{abdolmaleki2020distributional}, we show that teacher policies can help speed up learning, particularly in the absence of shaping rewards. In two domains with continuous observation and action spaces, our agents successfully compose teacher policies in sequence and in parallel, and are also able to further extend the policies of the teachers in order to solve the task.  Depending on the specified combination of task and teacher(s), teacher(s) may naturally act to limit the final performance of an agent. The extent to which agents are required to adhere to teacher policies are determined by hyperparameters which determine both the effect of te
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#24403;&#25628;&#32034;&#31354;&#38388;&#36866;&#24403;&#32422;&#26463;&#26102;&#65292;&#36755;&#20837;&#30028;&#38480;&#21487;&#39044;&#27979;&#27867;&#21270;&#24615;&#33021;&#65292;&#19988;&#19982;&#38544;&#34255;&#34920;&#31034;&#30028;&#38480;&#30456;&#27604;&#21462;&#24471;&#20102;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15466</link><description>&lt;p&gt;
&#36755;&#20837;&#30028;&#38480;&#20063;&#21487;&#20197;&#39044;&#27979;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Input margins can predict generalization too. (arXiv:2308.15466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#24403;&#25628;&#32034;&#31354;&#38388;&#36866;&#24403;&#32422;&#26463;&#26102;&#65292;&#36755;&#20837;&#30028;&#38480;&#21487;&#39044;&#27979;&#27867;&#21270;&#24615;&#33021;&#65292;&#19988;&#19982;&#38544;&#34255;&#34920;&#31034;&#30028;&#38480;&#30456;&#27604;&#21462;&#24471;&#20102;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#24615;&#22914;&#20309;&#29702;&#35299;&#26159;&#19968;&#20010;&#31215;&#26497;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25506;&#32034;&#26041;&#27861;&#26159;&#30028;&#38480;&#27979;&#37327;&#65306;&#32473;&#23450;&#26679;&#26412;&#25110;&#20854;&#22312;&#32593;&#32476;&#20869;&#30340;&#34920;&#31034;&#21040;&#20915;&#31574;&#36793;&#30028;&#30340;&#26368;&#30701;&#36317;&#31163;&#12290;&#34429;&#28982;&#24050;&#32463;&#26174;&#31034;&#20102;&#22312;&#38544;&#34255;&#34920;&#31034;&#65288;&#38544;&#34255;&#30028;&#38480;&#65289;&#20013;&#27979;&#37327;&#26102;&#30028;&#38480;&#19982;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30456;&#20851;&#65292;&#20294;&#23578;&#26410;&#24314;&#31435;&#36215;&#36755;&#20837;&#30028;&#38480;&#19982;&#27867;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#34429;&#28982;&#36755;&#20837;&#30028;&#38480;&#36890;&#24120;&#19981;&#33021;&#39044;&#27979;&#27867;&#21270;&#65292;&#20294;&#22914;&#26524;&#36866;&#24403;&#22320;&#32422;&#26463;&#25628;&#32034;&#31354;&#38388;&#65292;&#23427;&#20204;&#21487;&#20197;&#36215;&#21040;&#39044;&#27979;&#20316;&#29992;&#12290;&#25105;&#20204;&#22522;&#20110;&#36755;&#20837;&#30028;&#38480;&#24320;&#21457;&#20102;&#36825;&#26679;&#19968;&#31181;&#24230;&#37327;&#65292;&#31216;&#20043;&#20026;&#8220;&#21463;&#38480;&#30028;&#38480;&#8221;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26032;&#24230;&#37327;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#19982;&#38544;&#34255;&#34920;&#31034;&#30028;&#38480;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#24182;&#22312;&#8220;&#28145;&#24230;&#23398;&#20064;&#20013;&#39044;&#27979;&#27867;&#21270;&#24615;&#8221;&#65288;PGDL&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21463;&#38480;&#30028;&#38480;&#21462;&#24471;&#20102;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#20998;&#25968;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding generalization in deep neural networks is an active area of research. A promising avenue of exploration has been that of margin measurements: the shortest distance to the decision boundary for a given sample or its representation internal to the network. While margins have been shown to be correlated with the generalization ability of a model when measured at its hidden representations (hidden margins), no such link between large margins and generalization has been established for input margins. We show that while input margins are not generally predictive of generalization, they can be if the search space is appropriately constrained. We develop such a measure based on input margins, which we refer to as `constrained margins'. The predictive power of this new measure is demonstrated on the 'Predicting Generalization in Deep Learning' (PGDL) dataset and contrasted with hidden representation margins. We find that constrained margins achieve highly competitive scores and ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22810;&#31181;&#21463;&#37325;&#23614;&#20998;&#26512;&#21644;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#21551;&#21457;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#20132;&#36890;&#39044;&#27979;&#20013;&#25317;&#22581;&#24773;&#20917;&#30340;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#21035;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#20248;&#21270;&#30446;&#26631;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.15464</link><description>&lt;p&gt;
&#25439;&#22833;&#20989;&#25968;&#30340;&#27604;&#36739;&#30740;&#31350;&#65306;&#24120;&#35268;&#21644;&#25317;&#22581;&#24773;&#26223;&#19979;&#30340;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Loss Functions: Traffic Predictions in Regular and Congestion Scenarios. (arXiv:2308.15464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22810;&#31181;&#21463;&#37325;&#23614;&#20998;&#26512;&#21644;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#21551;&#21457;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#20132;&#36890;&#39044;&#27979;&#20013;&#25317;&#22581;&#24773;&#20917;&#30340;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#21035;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#20248;&#21270;&#30446;&#26631;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20132;&#36890;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20256;&#32479;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#24448;&#24448;&#38590;&#20197;&#20934;&#30830;&#39044;&#27979;&#25317;&#22581;&#24773;&#20917;&#12290;&#20934;&#30830;&#39044;&#27979;&#24120;&#35268;&#20132;&#36890;&#26465;&#20214;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36824;&#24517;&#39035;&#20934;&#30830;&#39044;&#27979;&#25317;&#22581;&#24773;&#26223;&#65292;&#20197;&#32500;&#25345;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#20132;&#36890;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21463;&#37325;&#23614;&#20998;&#26512;&#21644;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#21551;&#21457;&#30340;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#26041;&#38754;&#35780;&#20272;&#20102;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#25317;&#22581;&#24773;&#20917;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20248;&#21270;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#26102;&#65292;MAE-Focal Loss&#20989;&#25968;&#34920;&#29616;&#26368;&#20026;&#26377;&#25928;&#12290;&#22312;&#20248;&#21270;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#26102;&#65292;Gumbel Loss&#34987;&#35777;&#26126;&#26159;&#26356;&#20248;&#30340;&#36873;&#25321;&#12290;&#36825;&#20123;&#36873;&#25321;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;&#20132;&#36890;&#25317;&#22581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal graph neural networks have achieved state-of-the-art performance in traffic forecasting. However, they often struggle to forecast congestion accurately due to the limitations of traditional loss functions. While accurate forecasting of regular traffic conditions is crucial, a reliable AI system must also accurately forecast congestion scenarios to maintain safe and efficient transportation. In this paper, we explore various loss functions inspired by heavy tail analysis and imbalanced classification problems to address this issue. We evaluate the efficacy of these loss functions in forecasting traffic speed, with an emphasis on congestion scenarios. Through extensive experiments on real-world traffic datasets, we discovered that when optimizing for Mean Absolute Error (MAE), the MAE-Focal Loss function stands out as the most effective. When optimizing Mean Squared Error (MSE), Gumbel Loss proves to be the superior choice. These choices effectively forecast traffic conges
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#30340;&#35268;&#33539;&#22240;&#32032;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22240;&#23376;&#29305;&#24449;&#23481;&#37327;&#34429;&#28982;&#31616;&#21333;&#39640;&#25928;&#65292;&#20294;&#23384;&#22312;&#19981;&#33391;&#20559;&#24046;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#35268;&#33539;&#21270;&#21464;&#25442;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#28040;&#38500;&#20559;&#24046;&#65292;&#24182;&#22312;&#22270;&#20687;&#12289;&#36317;&#31163;&#21644;&#36752;&#23556;&#22330;&#37325;&#24314;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36136;&#37327;&#12289;&#40065;&#26834;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.15461</link><description>&lt;p&gt;
&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#30340;&#35268;&#33539;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Canonical Factors for Hybrid Neural Fields. (arXiv:2308.15461v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15461
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#30340;&#35268;&#33539;&#22240;&#32032;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22240;&#23376;&#29305;&#24449;&#23481;&#37327;&#34429;&#28982;&#31616;&#21333;&#39640;&#25928;&#65292;&#20294;&#23384;&#22312;&#19981;&#33391;&#20559;&#24046;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#35268;&#33539;&#21270;&#21464;&#25442;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#28040;&#38500;&#20559;&#24046;&#65292;&#24182;&#22312;&#22270;&#20687;&#12289;&#36317;&#31163;&#21644;&#36752;&#23556;&#22330;&#37325;&#24314;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36136;&#37327;&#12289;&#40065;&#26834;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#23376;&#29305;&#24449;&#23481;&#37327;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#26356;&#32039;&#20945;&#12289;&#39640;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#19981;&#19968;&#23450;&#26377;&#30410;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#20559;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#65288;1&#65289;&#23545;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#23545;&#40784;&#36724;&#20449;&#21495;&#30340;&#19981;&#33391;&#20559;&#24046;&#36827;&#34892;&#20102;&#34920;&#24449; - &#23427;&#20204;&#21487;&#20197;&#23548;&#33268;&#36752;&#23556;&#22330;&#37325;&#24314;&#30340;&#24046;&#24322;&#39640;&#36798;2 PSNR - &#24182;&#65288;2&#65289;&#25506;&#32034;&#20102;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#35268;&#33539;&#21270;&#21464;&#25442;&#26469;&#25552;&#39640;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#28040;&#38500;&#36825;&#20123;&#20559;&#24046;&#12290;&#22312;&#19968;&#20010;&#20108;&#32500;&#27169;&#22411;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#21516;&#26102;&#23398;&#20064;&#36825;&#20123;&#21464;&#25442;&#20197;&#21450;&#22330;&#26223;&#22806;&#35266;&#21487;&#20197;&#20197;&#22823;&#22823;&#25552;&#39640;&#30340;&#25928;&#29575;&#25104;&#21151;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#20687;&#12289;&#26377;&#31526;&#21495;&#36317;&#31163;&#21644;&#36752;&#23556;&#22330;&#37325;&#24314;&#20219;&#21153;&#39564;&#35777;&#20102;&#25152;&#24471;&#21040;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36136;&#37327;&#12289;&#40065;&#26834;&#24615;&#12289;&#32039;&#20945;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TILTED&#21487;&#20197;&#23454;&#29616;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#33021;&#21147;&#65292;&#32780;&#22522;&#32447;&#26159;2&#20493;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factored feature volumes offer a simple way to build more compact, efficient, and intepretable neural fields, but also introduce biases that are not necessarily beneficial for real-world data. In this work, we (1) characterize the undesirable biases that these architectures have for axis-aligned signals -- they can lead to radiance field reconstruction differences of as high as 2 PSNR -- and (2) explore how learning a set of canonicalizing transformations can improve representations by removing these biases. We prove in a two-dimensional model problem that simultaneously learning these transformations together with scene appearance succeeds with drastically improved efficiency. We validate the resulting architectures, which we call TILTED, using image, signed distance, and radiance field reconstruction tasks, where we observe improvements across quality, robustness, compactness, and runtime. Results demonstrate that TILTED can enable capabilities comparable to baselines that are 2x lar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;SMOTE&#21040;Mixup&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;&#12290;&#36890;&#36807;&#23545;SMOTE&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#32467;&#21512;Mixup&#25216;&#26415;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;Mixup&#25216;&#26415;&#36890;&#36807;&#23454;&#29616;&#22810;&#25968;&#31867;&#21644;&#23569;&#25968;&#31867;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38388;&#38553;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36793;&#30028;&#30340;Mixup&#25216;&#26415;&#65292;&#26356;&#26126;&#30830;&#22320;&#23454;&#29616;&#20102;&#19981;&#24179;&#34913;&#38388;&#38553;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15457</link><description>&lt;p&gt;
&#20174;SMOTE&#21040;Mixup&#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
From SMOTE to Mixup for Deep Imbalanced Classification. (arXiv:2308.15457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;SMOTE&#21040;Mixup&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;&#12290;&#36890;&#36807;&#23545;SMOTE&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#32467;&#21512;Mixup&#25216;&#26415;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;Mixup&#25216;&#26415;&#36890;&#36807;&#23454;&#29616;&#22810;&#25968;&#31867;&#21644;&#23569;&#25968;&#31867;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38388;&#38553;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36793;&#30028;&#30340;Mixup&#25216;&#26415;&#65292;&#26356;&#26126;&#30830;&#22320;&#23454;&#29616;&#20102;&#19981;&#24179;&#34913;&#38388;&#38553;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#22909;&#30340;&#20998;&#31867;&#22120;&#22240;&#20026;&#23569;&#25968;&#31867;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#32780;&#22256;&#38590;&#37325;&#37325;&#12290;&#20256;&#32479;&#19978;&#65292;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#30693;&#21517;&#23569;&#25968;&#31867;&#21512;&#25104;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#38754;&#21521;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#34987;&#29992;&#26469;&#25913;&#21892;&#36825;&#31181;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;SMOTE&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#26159;&#21542;&#20063;&#26377;&#30410;&#22788;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#21407;&#22987;&#30340;SMOTE&#23545;&#28145;&#24230;&#23398;&#20064;&#26469;&#35828;&#26159;&#19981;&#36275;&#30340;&#65292;&#24182;&#20351;&#29992;&#36719;&#26631;&#31614;&#22686;&#24378;&#20102;SMOTE&#12290;&#23558;&#24471;&#21040;&#30340;&#36719;SMOTE&#19982;Mixup&#65292;&#19968;&#31181;&#29616;&#20195;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#20256;&#32479;&#21644;&#29616;&#20195;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#32435;&#20837;&#21516;&#19968;&#20010;&#33539;&#30068;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#34920;&#26126;&#65292;Mixup&#36890;&#36807;&#38544;&#24335;&#22320;&#23454;&#29616;&#22810;&#25968;&#31867;&#21644;&#23569;&#25968;&#31867;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38388;&#38553;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36793;&#30028;&#30340;Mixup&#25216;&#26415;&#65292;&#26356;&#26126;&#30830;&#22320;&#23454;&#29616;&#20102;&#19981;&#24179;&#34913;&#38388;&#38553;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#37117;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given imbalanced data, it is hard to train a good classifier using deep learning because of the poor generalization of minority classes. Traditionally, the well-known synthetic minority oversampling technique (SMOTE) for data augmentation, a data mining approach for imbalanced learning, has been used to improve this generalization. However, it is unclear whether SMOTE also benefits deep learning. In this work, we study why the original SMOTE is insufficient for deep learning, and enhance SMOTE using soft labels. Connecting the resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to a unified framework that puts traditional and modern data augmentation techniques under the same umbrella. A careful study within this framework shows that Mixup improves generalization by implicitly achieving uneven margins between majority and minority classes. We then propose a novel margin-aware Mixup technique that more explicitly achieves uneven margins. Extensive experimental r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#29305;&#24449;&#36924;&#36817;&#22312;&#19968;&#33324;&#35889;&#26041;&#27861;&#20013;&#30340;&#24212;&#29992;&#65292;&#20998;&#26512;&#20102;&#19982;&#38543;&#26426;&#29305;&#24449;&#30456;&#32467;&#21512;&#30340;&#35889;&#27491;&#21017;&#21270;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#24182;&#33719;&#24471;&#20102;&#22312;&#19981;&#21516;&#27491;&#21017;&#24615;&#31867;&#21035;&#20013;&#30340;&#26368;&#20339;&#23398;&#20064;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.15434</link><description>&lt;p&gt;
&#38024;&#23545;&#19968;&#33324;&#35889;&#26041;&#27861;&#30340;&#38543;&#26426;&#29305;&#24449;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Random feature approximation for general spectral methods. (arXiv:2308.15434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#29305;&#24449;&#36924;&#36817;&#22312;&#19968;&#33324;&#35889;&#26041;&#27861;&#20013;&#30340;&#24212;&#29992;&#65292;&#20998;&#26512;&#20102;&#19982;&#38543;&#26426;&#29305;&#24449;&#30456;&#32467;&#21512;&#30340;&#35889;&#27491;&#21017;&#21270;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#24182;&#33719;&#24471;&#20102;&#22312;&#19981;&#21516;&#27491;&#21017;&#24615;&#31867;&#21035;&#20013;&#30340;&#26368;&#20339;&#23398;&#20064;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#29305;&#24449;&#36924;&#36817;&#34987;&#35748;&#20026;&#26159;&#22312;&#22823;&#35268;&#27169;&#31639;&#27861;&#20013;&#21152;&#36895;&#26680;&#26041;&#27861;&#30340;&#26368;&#27969;&#34892;&#25216;&#26415;&#20043;&#19968;&#65292;&#24182;&#19988;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19982;&#38543;&#26426;&#29305;&#24449;&#30456;&#32467;&#21512;&#30340;&#19968;&#22823;&#31867;&#35889;&#27491;&#21017;&#21270;&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#38544;&#24335;&#27491;&#21017;&#21270;&#65288;&#22914;&#26799;&#24230;&#19979;&#38477;&#65289;&#25110;&#26174;&#24335;&#26041;&#27861;&#65288;&#22914;Tikhonov&#27491;&#21017;&#21270;&#65289;&#30340;&#26680;&#26041;&#27861;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#22312;&#36866;&#24403;&#30340;&#28304;&#26465;&#20214;&#19979;&#23450;&#20041;&#20102;&#27491;&#21017;&#24615;&#31867;&#21035;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#23398;&#20064;&#36895;&#29575;&#12290;&#36825;&#25913;&#36827;&#25110;&#23436;&#21892;&#20102;&#20197;&#21069;&#22312;&#29305;&#23450;&#26680;&#31639;&#27861;&#30456;&#20851;&#35774;&#32622;&#20013;&#33719;&#24471;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random feature approximation is arguably one of the most popular techniques to speed up kernel methods in large scale algorithms and provides a theoretical approach to the analysis of deep neural networks. We analyze generalization properties for a large class of spectral regularization methods combined with random features, containing kernel methods with implicit regularization such as gradient descent or explicit methods like Tikhonov regularization. For our estimators we obtain optimal learning rates over regularity classes (even for classes that are not included in the reproducing kernel Hilbert space), which are defined through appropriate source conditions. This improves or completes previous results obtained in related settings for specific kernel algorithms.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#21382;&#26102;&#22826;&#38451;&#30913;&#22330;&#22270;&#25968;&#25454;&#65292;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#65292;&#36827;&#34892;&#20102;&#27010;&#29575;&#24615;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#65292;&#21457;&#29616;&#21253;&#21547;&#21382;&#21490;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15410</link><description>&lt;p&gt;
&#20351;&#29992;&#21382;&#21490;&#30913;&#22330;&#22270;&#25968;&#25454;&#36827;&#34892;&#27010;&#29575;&#24615;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic solar flare forecasting using historical magnetogram data. (arXiv:2308.15410v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15410
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#21382;&#26102;&#22826;&#38451;&#30913;&#22330;&#22270;&#25968;&#25454;&#65292;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#65292;&#36827;&#34892;&#20102;&#27010;&#29575;&#24615;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#65292;&#21457;&#29616;&#21253;&#21547;&#21382;&#21490;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;SDO/HMI&#26102;&#20195;&#35206;&#30422;&#22826;&#38451;&#31532;24&#20010;&#21608;&#26085;&#21644;&#31532;25&#20010;&#21608;&#26085;&#24320;&#22987;&#30340;&#39640;&#20998;&#36776;&#29575;&#30913;&#22330;&#22270;&#25968;&#25454;&#19978;&#65292;&#19968;&#20123;&#30740;&#31350;&#36824;&#22238;&#39038;&#20102;&#26469;&#33258;&#22826;&#38451;&#31532;23&#20010;&#21608;&#26085;&#30340;SOHO/MDI&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;&#22810;&#20010;&#20202;&#22120;&#30340;&#21382;&#26102;4&#20010;&#22826;&#38451;&#21608;&#26399;&#30340;&#27599;&#26085;&#21382;&#21490;&#30913;&#22330;&#22270;&#25968;&#25454;&#12290;&#36825;&#26159;&#39318;&#27425;&#23581;&#35797;&#21033;&#29992;&#36825;&#20123;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32768;&#26001;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20174;&#20840;&#29699;&#30913;&#22330;&#22270;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#23558;&#22522;&#20110;&#30913;&#22330;&#22270;&#21644;&#32768;&#26001;&#21382;&#21490;&#30340;&#26631;&#37327;&#29305;&#24449;&#32435;&#20837;&#27169;&#22411;&#12290;&#25105;&#20204;&#37319;&#29992;&#38598;&#25104;&#26041;&#27861;&#29983;&#25104;&#22312;&#25509;&#19979;&#26469;&#30340;24&#23567;&#26102;&#20869;M&#32423;&#25110;&#26356;&#22823;&#32768;&#26001;&#30340;&#26657;&#20934;&#27010;&#29575;&#39044;&#27979;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#21253;&#21547;&#21382;&#21490;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#32768;&#26001;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21333;&#24103;&#30913;&#22330;&#22270;&#24182;&#19981;&#21253;&#21547;&#27604;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#30340;&#26356;&#22810;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solar flare forecasting research using machine learning (ML) has focused on high resolution magnetogram data from the SDO/HMI era covering Solar Cycle 24 and the start of Solar Cycle 25, with some efforts looking back to SOHO/MDI for data from Solar Cycle 23. In this paper, we consider over 4 solar cycles of daily historical magnetogram data from multiple instruments. This is the first attempt to take advantage of this historical data for ML-based flare forecasting. We apply a convolutional neural network (CNN) to extract features from full-disk magnetograms together with a logistic regression model to incorporate scalar features based on magnetograms and flaring history. We use an ensemble approach to generate calibrated probabilistic forecasts of M-class or larger flares in the next 24 hours. Overall, we find that including historical data improves forecasting skill and reliability. We show that single frame magnetograms do not contain significantly more relevant information than can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;CVaR&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#38271;&#23614;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#22791;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#31614;&#24863;&#30693;&#26377;&#30028;CVaR&#65288;LAB-CVaR&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#21407;&#22987;CVaR&#30340;&#24754;&#35266;&#32467;&#26524;&#65292;&#36890;&#36807;&#35774;&#35745;&#26368;&#20248;&#26435;&#37325;&#19978;&#19979;&#30028;&#36827;&#34892;&#25913;&#36827;&#65307;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;logit&#35843;&#25972;&#30340;LAB-CVaR&#65288;LAB-CVaR-logit&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#31283;&#23450;&#20248;&#21270;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.15405</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#31614;&#24863;&#30693;&#26377;&#30028;CVaR&#23454;&#29616;&#40065;&#26834;&#30340;&#38271;&#23614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Long-Tailed Learning via Label-Aware Bounded CVaR. (arXiv:2308.15405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;CVaR&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#38271;&#23614;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#22791;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#31614;&#24863;&#30693;&#26377;&#30028;CVaR&#65288;LAB-CVaR&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#21407;&#22987;CVaR&#30340;&#24754;&#35266;&#32467;&#26524;&#65292;&#36890;&#36807;&#35774;&#35745;&#26368;&#20248;&#26435;&#37325;&#19978;&#19979;&#30028;&#36827;&#34892;&#25913;&#36827;&#65307;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;logit&#35843;&#25972;&#30340;LAB-CVaR&#65288;LAB-CVaR-logit&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#31283;&#23450;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#25968;&#25454;&#24448;&#24448;&#26159;&#19981;&#24179;&#34913;&#25110;&#38271;&#23614;&#20998;&#24067;&#30340;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#31867;&#21035;&#25317;&#26377;&#22823;&#37096;&#20998;&#26679;&#26412;&#65292;&#24182;&#20027;&#23548;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26222;&#36890;&#27169;&#22411;&#24448;&#24448;&#22312;&#23569;&#25968;&#31867;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#20043;&#21069;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#20462;&#27491;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#23614;&#23398;&#20064;&#38382;&#39064;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#23545;&#21516;&#19968;&#31867;&#21035;&#30340;&#26679;&#26412;&#28448;&#19981;&#20851;&#24515;&#65292;&#35201;&#20040;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;CVaR&#65288;&#26465;&#20214;&#20215;&#20540;-at-Risk&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;&#38271;&#23614;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#31614;&#24863;&#30693;&#26377;&#30028;CVaR&#65288;LAB-CVaR&#65289;&#25439;&#22833;&#20989;&#25968;&#26469;&#20811;&#26381;&#21407;&#22987;CVaR&#24754;&#35266;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35774;&#35745;&#20102;LAB-CVaR&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#19979;&#30028;&#12290;&#22522;&#20110;LAB-CVaR&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;logit&#35843;&#25972;&#30340;LAB-CVaR&#65288;LAB-CVaR-logit&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#31283;&#23450;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data in the real-world classification problems are always imbalanced or long-tailed, wherein the majority classes have the most of the samples that dominate the model training. In such setting, the naive model tends to have poor performance on the minority classes. Previously, a variety of loss modifications have been proposed to address the long-tailed leaning problem, while these methods either treat the samples in the same class indiscriminatingly or lack a theoretical guarantee. In this paper, we propose two novel approaches based on CVaR (Conditional Value at Risk) to improve the performance of long-tailed learning with a solid theoretical ground. Specifically, we firstly introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss to overcome the pessimistic result of the original CVaR, and further design the optimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we additionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to stabilize the optimization pro
&lt;/p&gt;</description></item><item><title>CausalBench&#25361;&#25112;&#36187;&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31454;&#36187;&#65292;&#26088;&#22312;&#26500;&#24314;&#22522;&#22240;&#32593;&#32476;&#25512;&#26029;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#21442;&#19982;&#32773;&#21033;&#29992;&#22823;&#35268;&#27169;&#36951;&#20256;&#24178;&#25200;&#25968;&#25454;&#25552;&#21319;&#20102;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15395</link><description>&lt;p&gt;
CausalBench&#25361;&#25112;&#36187;&#65306;&#22522;&#20110;&#21333;&#32454;&#32990;&#24178;&#25200;&#25968;&#25454;&#30340;&#22522;&#22240;&#32593;&#32476;&#25512;&#26029;&#30340;&#26426;&#22120;&#23398;&#20064;&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
The CausalBench challenge: A machine learning contest for gene network inference from single-cell perturbation data. (arXiv:2308.15395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15395
&lt;/p&gt;
&lt;p&gt;
CausalBench&#25361;&#25112;&#36187;&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31454;&#36187;&#65292;&#26088;&#22312;&#26500;&#24314;&#22522;&#22240;&#32593;&#32476;&#25512;&#26029;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#21442;&#19982;&#32773;&#21033;&#29992;&#22823;&#35268;&#27169;&#36951;&#20256;&#24178;&#25200;&#25968;&#25454;&#25552;&#21319;&#20102;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#32472;&#21046;&#32454;&#32990;&#31995;&#32479;&#20869;&#22522;&#22240;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20851;&#31995;&#26159;&#20851;&#38190;&#30340;&#26089;&#26399;&#27493;&#39588;&#12290;&#36825;&#26377;&#21161;&#20110;&#21046;&#23450;&#20851;&#20110;&#21487;&#33021;&#34987;&#26410;&#26469;&#33647;&#29289;&#38774;&#21521;&#30340;&#20998;&#23376;&#26426;&#21046;&#30340;&#20551;&#35774;&#12290;CausalBench&#25361;&#25112;&#26159;&#19968;&#39033;&#36992;&#35831;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#26469;&#25512;&#36827;&#26500;&#24314;&#22522;&#22240;-&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#20513;&#35758;&#12290;&#36825;&#20123;&#32593;&#32476;&#26159;&#20174;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#30340;&#21333;&#32454;&#32990;&#25968;&#25454;&#38598;&#20013;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#32463;&#36807;&#19981;&#21516;&#31867;&#22411;&#30340;&#24178;&#25200;&#12290;&#36825;&#20123;&#32593;&#32476;&#23545;&#20110;&#29702;&#35299;&#30142;&#30149;&#29983;&#29289;&#23398;&#30340;&#22240;&#26524;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#21442;&#19982;&#32773;&#21033;&#29992;CausalBench&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20219;&#21153;&#26159;&#25552;&#21319;&#20808;&#36827;&#26041;&#27861;&#21033;&#29992;&#22823;&#35268;&#27169;&#36951;&#20256;&#24178;&#25200;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#26412;&#25253;&#21578;&#20998;&#26512;&#21644;&#24635;&#32467;&#20102;&#25361;&#25112;&#36187;&#26399;&#38388;&#25552;&#20132;&#30340;&#26041;&#27861;&#65292;&#20197;&#25551;&#32472;&#31454;&#36187;&#26399;&#38388;&#25216;&#26415;&#21457;&#23637;&#30340;&#37096;&#20998;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In drug discovery, mapping interactions between genes within cellular systems is a crucial early step. This helps formulate hypotheses regarding molecular mechanisms that could potentially be targeted by future medicines. The CausalBench Challenge was an initiative to invite the machine learning community to advance the state of the art in constructing gene-gene interaction networks. These networks, derived from large-scale, real-world datasets of single cells under various perturbations, are crucial for understanding the causal mechanisms underlying disease biology. Using the framework provided by the CausalBench benchmark, participants were tasked with enhancing the capacity of the state of the art methods to leverage large-scale genetic perturbation data. This report provides an analysis and summary of the methods submitted during the challenge to give a partial image of the state of the art at the time of the challenge. The winning solutions significantly improved performance compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#30005;&#33655;&#24179;&#34913;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#20013;&#30340;&#30005;&#33655;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24179;&#22343;&#19968;&#33268;&#24615;&#31639;&#27861;&#26469;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.15394</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30005;&#33655;&#24179;&#34913;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-agent Reinforcement Learning based State-of-Charge Balancing Strategy for Distributed Energy Storage System. (arXiv:2308.15394v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#30005;&#33655;&#24179;&#34913;&#31574;&#30053;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#20013;&#30340;&#30005;&#33655;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24179;&#22343;&#19968;&#33268;&#24615;&#31639;&#27861;&#26469;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;Dec-MARL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#24335;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#20013;&#30340;&#30005;&#33655;&#24179;&#34913;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#23558;&#30005;&#33655;&#24179;&#34913;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#26377;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#26681;&#25454;&#38656;&#27714;&#24179;&#34913;&#24471;&#21040;&#30340;&#21160;&#20316;&#32422;&#26463;&#65292;&#37319;&#29992;Dec-MARL&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21033;&#29992;&#19968;&#38454;&#24179;&#22343;&#19968;&#33268;&#24615;&#31639;&#27861;&#20197;&#23436;&#20840;&#20998;&#25955;&#30340;&#26041;&#24335;&#25193;&#23637;&#20998;&#24067;&#24335;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#29366;&#24577;&#30340;&#35266;&#27979;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#35266;&#27979;&#20915;&#23450;&#21021;&#22987;&#21160;&#20316;&#65288;&#21363;&#36755;&#20986;&#21151;&#29575;&#65289;&#12290;&#20026;&#20102;&#24471;&#21040;&#20801;&#35768;&#33539;&#22260;&#20869;&#30340;&#26368;&#32456;&#21160;&#20316;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#20107;&#23454;&#38656;&#27714;&#24179;&#34913;&#31639;&#27861;&#26469;&#24179;&#34913;&#24635;&#38656;&#27714;&#21644;&#21021;&#22987;&#21160;&#20316;&#12290;&#28982;&#21518;&#65292;&#26234;&#33021;&#20307;&#25191;&#34892;&#26368;&#32456;&#21160;&#20316;&#24182;&#20174;&#29615;&#22659;&#20013;&#33719;&#24471;&#23616;&#37096;&#22870;&#21169;&#65292;&#20351;&#31995;&#32479;&#36827;&#20837;&#19979;&#19968;&#20010;&#29366;&#24577;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#19968;&#38454;&#24179;&#22343;&#19968;&#33268;&#24615;&#31639;&#27861;&#65292;&#26234;&#33021;&#20307;&#33719;&#24471;&#24179;&#22343;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#31574;&#30053;&#26469;&#19981;&#26029;&#20248;&#21270;&#30005;&#33655;&#24179;&#34913;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops a Decentralized Multi-Agent Reinforcement Learning (Dec-MARL) method to solve the SoC balancing problem in the distributed energy storage system (DESS). First, the SoC balancing problem is formulated into a finite Markov decision process with action constraints derived from demand balance, which can be solved by Dec-MARL. Specifically, the first-order average consensus algorithm is utilized to expand the observations of the DESS state in a fully-decentralized way, and the initial actions (i.e., output power) are decided by the agents (i.e., energy storage units) according to these observations. In order to get the final actions in the allowable range, a counterfactual demand balance algorithm is proposed to balance the total demand and the initial actions. Next, the agents execute the final actions and get local rewards from the environment, and the DESS steps into the next state. Finally, through the first-order average consensus algorithm, the agents get the avera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24418;&#29366;&#36793;&#32536;&#30693;&#35782;&#22686;&#24378;&#32593;&#32476;&#65288;SkaNet&#65289;&#30340;&#30002;&#29366;&#33146;&#32467;&#33410;&#20998;&#21106;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#30002;&#29366;&#33146;&#32467;&#33410;&#20998;&#21106;&#21644;&#35786;&#26029;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#20219;&#21153;&#30340;&#19968;&#20307;&#21270;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#30002;&#29366;&#33146;&#24433;&#20687;&#25253;&#21578;&#21644;&#25968;&#25454;&#31995;&#32479;&#65288;TI-RADS&#65289;&#65292;&#21033;&#29992;&#24418;&#29366;&#21644;&#36793;&#32536;&#29305;&#24449;&#26469;&#21306;&#20998;&#33391;&#24615;&#21644;&#24694;&#24615;&#30002;&#29366;&#33146;&#32467;&#33410;&#12290;</title><link>http://arxiv.org/abs/2308.15386</link><description>&lt;p&gt;
&#22522;&#20110;&#24418;&#29366;&#36793;&#32536;&#30693;&#35782;&#22686;&#24378;&#32593;&#32476;&#30340;&#30002;&#29366;&#33146;&#32467;&#33410;&#20998;&#21106;&#21644;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Shape-Margin Knowledge Augmented Network for Thyroid Nodule Segmentation and Diagnosis. (arXiv:2308.15386v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24418;&#29366;&#36793;&#32536;&#30693;&#35782;&#22686;&#24378;&#32593;&#32476;&#65288;SkaNet&#65289;&#30340;&#30002;&#29366;&#33146;&#32467;&#33410;&#20998;&#21106;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#30002;&#29366;&#33146;&#32467;&#33410;&#20998;&#21106;&#21644;&#35786;&#26029;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#20219;&#21153;&#30340;&#19968;&#20307;&#21270;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#30002;&#29366;&#33146;&#24433;&#20687;&#25253;&#21578;&#21644;&#25968;&#25454;&#31995;&#32479;&#65288;TI-RADS&#65289;&#65292;&#21033;&#29992;&#24418;&#29366;&#21644;&#36793;&#32536;&#29305;&#24449;&#26469;&#21306;&#20998;&#33391;&#24615;&#21644;&#24694;&#24615;&#30002;&#29366;&#33146;&#32467;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30002;&#29366;&#33146;&#32467;&#33410;&#20998;&#21106;&#26159;&#21307;&#29983;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#31995;&#32479;&#35786;&#26029;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#23558;&#20998;&#21106;&#21644;&#35786;&#26029;&#20316;&#20026;&#29420;&#31435;&#30340;&#20219;&#21153;&#65292;&#27809;&#26377;&#32771;&#34385;&#36825;&#20123;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#31995;&#32479;&#20013;&#29420;&#31435;&#20219;&#21153;&#30340;&#39034;&#24207;&#27493;&#39588;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#24046;&#30340;&#32047;&#31215;&#12290;&#22240;&#27492;&#65292;&#20540;&#24471;&#36890;&#36807;&#25506;&#32034;&#30002;&#29366;&#33146;&#32467;&#33410;&#20998;&#21106;&#21644;&#35786;&#26029;&#20043;&#38388;&#30340;&#20851;&#31995;&#23558;&#23427;&#20204;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#36827;&#34892;&#32452;&#21512;&#12290;&#26681;&#25454;&#30002;&#29366;&#33146;&#24433;&#20687;&#25253;&#21578;&#21644;&#25968;&#25454;&#31995;&#32479;&#65288;TI-RADS&#65289;&#65292;&#24418;&#29366;&#21644;&#36793;&#32536;&#29305;&#24449;&#30340;&#35780;&#20272;&#26159;&#21306;&#20998;&#33391;&#24615;&#21644;&#24694;&#24615;&#30002;&#29366;&#33146;&#32467;&#33410;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#22312;&#30002;&#29366;&#33146;&#32467;&#33410;&#20998;&#21106;&#25513;&#27169;&#20013;&#35266;&#23519;&#21040;&#12290;&#21463;&#21040;TI-RADS&#35786;&#26029;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#30002;&#29366;&#33146;&#32467;&#33410;&#20998;&#21106;&#21644;&#35786;&#26029;&#30340;&#24418;&#29366;&#36793;&#32536;&#30693;&#35782;&#22686;&#24378;&#32593;&#32476;&#65288;SkaNet&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thyroid nodule segmentation is a crucial step in the diagnostic procedure of physicians and computer-aided diagnosis systems. Mostly, current studies treat segmentation and diagnosis as independent tasks without considering the correlation between these tasks. The sequence steps of these independent tasks in computer-aided diagnosis systems may lead to the accumulation of errors. Therefore, it is worth combining them as a whole through exploring the relationship between thyroid nodule segmentation and diagnosis. According to the thyroid imaging reporting and data system (TI-RADS), the assessment of shape and margin characteristics is the prerequisite for the discrimination of benign and malignant thyroid nodules. These characteristics can be observed in the thyroid nodule segmentation masks. Inspired by the diagnostic procedure of TI-RADS, this paper proposes a shape-margin knowledge augmented network (SkaNet) for simultaneously thyroid nodule segmentation and diagnosis. Due to the sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#21709;&#24212;&#24322;&#26041;&#24046;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#22238;&#24402;&#12289;&#20998;&#31867;&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#36817;&#20284;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#22312;&#25429;&#25417;&#20989;&#25968;&#24179;&#28369;&#24615;&#30340;&#31361;&#21464;&#21644;&#36866;&#24212;&#24322;&#26041;&#24046;&#38169;&#35823;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15370</link><description>&lt;p&gt;
&#22810;&#21709;&#24212;&#24322;&#26041;&#24046;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#21450;&#20854;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Multi-Response Heteroscedastic Gaussian Process Models and Their Inference. (arXiv:2308.15370v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#21709;&#24212;&#24322;&#26041;&#24046;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#22238;&#24402;&#12289;&#20998;&#31867;&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#36817;&#20284;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#22312;&#25429;&#25417;&#20989;&#25968;&#24179;&#28369;&#24615;&#30340;&#31361;&#21464;&#21644;&#36866;&#24212;&#24322;&#26041;&#24046;&#38169;&#35823;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#34987;&#24191;&#27867;&#29992;&#20110;&#28789;&#27963;&#30340;&#38750;&#21442;&#25968;&#24314;&#27169;&#65292;&#20294;&#23427;&#20204;&#22312;&#26377;&#25928;&#25429;&#25417;&#20989;&#25968;&#24179;&#28369;&#24615;&#30340;&#31361;&#21464;&#21644;&#36866;&#24212;&#24322;&#26041;&#24046;&#38169;&#35823;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24322;&#26041;&#24046;&#39640;&#26031;&#36807;&#31243;&#65288;HeGP&#65289;&#22238;&#24402;&#26088;&#22312;&#36890;&#36807;&#25215;&#35748;&#22238;&#24402;&#27169;&#22411;&#20013;&#21327;&#21464;&#37327;&#38388;&#27531;&#24046;&#26041;&#24046;&#30340;&#21487;&#21464;&#24615;&#26469;&#24341;&#20837;&#28789;&#27963;&#24615;&#12290;&#26412;&#25991;&#23558;HeGP&#27010;&#24565;&#25193;&#23637;&#21040;&#20998;&#31867;&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#39640;&#26031;&#36807;&#31243;&#19982;&#21327;&#21464;&#37327;&#35825;&#23548;&#30340;&#31934;&#24230;&#30697;&#38453;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#37319;&#29992;&#28151;&#21512;&#24418;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#23545;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#24322;&#26041;&#24046;&#21327;&#26041;&#24046;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#37319;&#26679;&#24102;&#26469;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#37319;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#36817;&#20284;&#21518;&#39564;&#24182;&#20415;&#21033;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the widespread utilization of Gaussian process models for versatile nonparametric modeling, they exhibit limitations in effectively capturing abrupt changes in function smoothness and accommodating relationships with heteroscedastic errors. Addressing these shortcomings, the heteroscedastic Gaussian process (HeGP) regression seeks to introduce flexibility by acknowledging the variability of residual variances across covariates in the regression model. In this work, we extend the HeGP concept, expanding its scope beyond regression tasks to encompass classification and state-space models. To achieve this, we propose a novel framework where the Gaussian process is coupled with a covariate-induced precision matrix process, adopting a mixture formulation. This approach enables the modeling of heteroscedastic covariance functions across covariates. To mitigate the computational challenges posed by sampling, we employ variational inference to approximate the posterior and facilitate p
&lt;/p&gt;</description></item><item><title>pFedPG is a novel personalized FL framework that generates client-specific prompts to adapt frozen backbones to local data distributions, enabling efficient model personalization for heterogeneous clients in FL.</title><link>http://arxiv.org/abs/2308.15367</link><description>&lt;p&gt;
&#36890;&#36807;&#23458;&#25143;&#31471;&#29305;&#23450;&#30340;&#25552;&#31034;&#29983;&#25104;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation. (arXiv:2308.15367v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15367
&lt;/p&gt;
&lt;p&gt;
pFedPG is a novel personalized FL framework that generates client-specific prompts to adapt frozen backbones to local data distributions, enabling efficient model personalization for heterogeneous clients in FL.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#20998;&#25955;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#20197;&#20445;&#25252;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#22810;&#20010;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#35757;&#32451;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;Vision Transformer&#65289;&#23637;&#31034;&#20102;&#20174;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#20013;&#33719;&#24471;&#31283;&#20581;&#34920;&#31034;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12289;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#36890;&#20449;&#24102;&#23485;&#38480;&#21046;&#20102;&#22823;&#22411;&#27169;&#22411;&#22312;FL&#26694;&#26550;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#21033;&#29992;&#22823;&#22411;&#27169;&#22411;&#30340;&#31283;&#20581;&#34920;&#31034;&#65292;&#21516;&#26102;&#23454;&#29616;&#23545;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#39640;&#25928;&#27169;&#22411;&#20010;&#24615;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;FL&#26694;&#26550;&#65292;&#21363;&#23458;&#25143;&#31471;&#29305;&#23450;&#30340;&#25552;&#31034;&#29983;&#25104;&#65288;pFedPG&#65289;&#65292;&#23427;&#23398;&#20064;&#22312;&#26381;&#21153;&#22120;&#31471;&#37096;&#32626;&#20010;&#24615;&#21270;&#25552;&#31034;&#29983;&#25104;&#22120;&#65292;&#29992;&#20197;&#20135;&#29983;&#36866;&#24212;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#29305;&#23450;&#35270;&#35273;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23558;&#20923;&#32467;&#30340;&#39592;&#24178;&#32593;&#32476;&#36866;&#24212;&#21040;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) emerges as a decentralized learning framework which trains models from multiple distributed clients without sharing their data to preserve privacy. Recently, large-scale pre-trained models (e.g., Vision Transformer) have shown a strong capability of deriving robust representations. However, the data heterogeneity among clients, the limited computation resources, and the communication bandwidth restrict the deployment of large-scale models in FL frameworks. To leverage robust representations from large-scale models while enabling efficient model personalization for heterogeneous clients, we propose a novel personalized FL framework of client-specific Prompt Generation (pFedPG), which learns to deploy a personalized prompt generator at the server for producing client-specific visual prompts that efficiently adapts frozen backbones to local data distributions. Our proposed framework jointly optimizes the stages of personalized prompt adaptation locally and personal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#24314;&#27169;&#22810;&#20010;&#24322;&#26500;&#30456;&#20851;&#20219;&#21153;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#65288;MOGP&#65289;&#20419;&#36827;&#20219;&#21153;&#20043;&#38388;&#30340;&#20449;&#24687;&#20849;&#20139;&#21644;&#38750;&#21442;&#25968;&#21442;&#25968;&#20272;&#35745;&#65292;&#24182;&#24212;&#29992;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#26469;&#35299;&#20915;&#38750;&#20849;&#36717;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15364</link><description>&lt;p&gt;
&#24322;&#26500;&#22810;&#20219;&#21153;&#39640;&#26031;Cox&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Multi-Task Gaussian Cox Processes. (arXiv:2308.15364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#24314;&#27169;&#22810;&#20010;&#24322;&#26500;&#30456;&#20851;&#20219;&#21153;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#65288;MOGP&#65289;&#20419;&#36827;&#20219;&#21153;&#20043;&#38388;&#30340;&#20449;&#24687;&#20849;&#20139;&#21644;&#38750;&#21442;&#25968;&#21442;&#25968;&#20272;&#35745;&#65292;&#24182;&#24212;&#29992;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#26469;&#35299;&#20915;&#38750;&#20849;&#36717;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;Cox&#36807;&#31243;&#30340;&#25193;&#23637;&#65292;&#29992;&#20110;&#32852;&#21512;&#24314;&#27169;&#22810;&#20010;&#24322;&#26500;&#30456;&#20851;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#31867;&#21644;&#22238;&#24402;&#65292;&#36890;&#36807;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#65288;MOGP&#65289;&#12290;&#36890;&#36807;&#23545;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#28857;&#36807;&#31243;&#20219;&#21153;&#30340;&#19987;&#29992;&#20284;&#28982;&#21442;&#25968;&#24341;&#20837;MOGP&#20808;&#39564;&#65292;&#21487;&#20197;&#20419;&#36827;&#24322;&#26500;&#20219;&#21153;&#20043;&#38388;&#30340;&#20449;&#24687;&#20849;&#20139;&#65292;&#21516;&#26102;&#20801;&#35768;&#38750;&#21442;&#25968;&#21442;&#25968;&#20272;&#35745;&#12290;&#20026;&#20102;&#20811;&#26381;MOGP&#35843;&#21046;&#30340;&#24322;&#26500;&#22810;&#20219;&#21153;&#26694;&#26550;&#20013;&#30340;&#38750;&#20849;&#36717;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#65292;&#24182;&#23548;&#20986;&#22343;&#22330;&#36817;&#20284;&#26469;&#23454;&#29616;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#38381;&#24335;&#36845;&#20195;&#26356;&#26032;&#12290;&#25105;&#20204;&#22312;1D&#21512;&#25104;&#25968;&#25454;&#21644;&#28201;&#21733;&#21326;&#30340;2D&#22478;&#24066;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#24615;&#33021;&#21644;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel extension of multi-task Gaussian Cox processes for modeling multiple heterogeneous correlated tasks jointly, e.g., classification and regression, via multi-output Gaussian processes (MOGP). A MOGP prior over the parameters of the dedicated likelihoods for classification, regression and point process tasks can facilitate sharing of information between heterogeneous tasks, while allowing for nonparametric parameter estimation. To circumvent the non-conjugate Bayesian inference in the MOGP modulated heterogeneous multi-task framework, we employ the data augmentation technique and derive a mean-field approximation to realize closed-form iterative updates for estimating model parameters. We demonstrate the performance and inference on both 1D synthetic data as well as 2D urban data of Vancouver.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.15363</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#25991;&#26412;&#21040;SQL&#30340;&#30740;&#31350;&#65306;&#19968;&#20010;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#19968;&#31181;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#22522;&#20934;&#38459;&#30861;&#20102;&#35774;&#35745;&#26377;&#25928;&#12289;&#39640;&#25928;&#21644;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#21644;&#24191;&#27867;&#30340;&#27604;&#36739;&#65292;&#21253;&#25324;&#38382;&#39064;&#34920;&#31034;&#12289;&#31034;&#20363;&#36873;&#25321;&#21644;&#31034;&#20363;&#32452;&#32455;&#65292;&#24182;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#35814;&#32454;&#38416;&#36848;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#21517;&#20026;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#65292;&#36798;&#21040;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#26438;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24378;&#35843;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#65292;&#24182;&#22312;&#27492;&#24230;&#37327;&#19979;&#27604;&#36739;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24320;&#28304;LLMs&#65292;&#24182;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#30417;&#30563;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborates their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. Towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. Additionally, we investigate open-source LLMs in in-context learning, and further enhance their performance with task-specific superv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Lie-Poisson&#31070;&#32463;&#32593;&#32476;&#65288;LPNets&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;&#25968;&#25454;&#35745;&#31639;&#20855;&#26377;&#23545;&#31216;&#24615;&#30340;Hamiltonian&#31995;&#32479;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#20934;&#30830;&#20445;&#25345;Poisson&#25324;&#21495;&#21644;&#29305;&#27530;&#20989;&#25968;&#26469;&#39044;&#27979;&#31995;&#32479;&#30340;&#38271;&#26399;&#28436;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.15349</link><description>&lt;p&gt;
Lie-Poisson&#31070;&#32463;&#32593;&#32476;&#65288;LPNets&#65289;&#65306;&#22522;&#20110;&#25968;&#25454;&#30340;&#20855;&#26377;&#23545;&#31216;&#24615;&#30340;Hamiltonian&#31995;&#32479;&#30340;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Lie-Poisson Neural Networks (LPNets): Data-Based Computing of Hamiltonian Systems with Symmetries. (arXiv:2308.15349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Lie-Poisson&#31070;&#32463;&#32593;&#32476;&#65288;LPNets&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;&#25968;&#25454;&#35745;&#31639;&#20855;&#26377;&#23545;&#31216;&#24615;&#30340;Hamiltonian&#31995;&#32479;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#20934;&#30830;&#20445;&#25345;Poisson&#25324;&#21495;&#21644;&#29305;&#27530;&#20989;&#25968;&#26469;&#39044;&#27979;&#31995;&#32479;&#30340;&#38271;&#26399;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#22522;&#20110;&#25968;&#25454;&#39044;&#27979;Hamiltonian&#31995;&#32479;&#30340;&#38271;&#26399;&#28436;&#21270;&#38656;&#35201;&#19968;&#20010;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#20445;&#25345;&#36866;&#24403;&#32467;&#26500;&#30340;&#32593;&#32476;&#12290;&#27599;&#20010;Hamiltonian&#31995;&#32479;&#37117;&#21253;&#21547;&#20004;&#20010;&#22522;&#26412;&#35201;&#32032;&#65306;Poisson&#25324;&#21495;&#21644;Hamiltonian&#12290;&#20855;&#26377;&#23545;&#31216;&#24615;&#30340;Hamiltonian&#31995;&#32479;&#65292;&#20854;&#20856;&#22411;&#20363;&#23376;&#26159;Lie-Poisson&#31995;&#32479;&#65292;&#24050;&#34987;&#35777;&#26126;&#33021;&#25551;&#36848;&#20174;&#21355;&#26143;&#36816;&#21160;&#21040;&#27700;&#19979;&#36710;&#36742;&#12289;&#27969;&#20307;&#12289;&#22320;&#29699;&#29289;&#29702;&#24212;&#29992;&#12289;&#22797;&#26434;&#27969;&#20307;&#21644;&#31561;&#31163;&#23376;&#29289;&#29702;&#31561;&#24191;&#27867;&#30340;&#29289;&#29702;&#29616;&#35937;&#12290;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#65292;Poisson&#25324;&#21495;&#26469;&#33258;&#23545;&#31216;&#24615;&#65292;&#32780;Hamiltonian&#26469;&#33258;&#24213;&#23618;&#29289;&#29702;&#12290;&#25105;&#20204;&#23558;&#31995;&#32479;&#30340;&#23545;&#31216;&#24615;&#35270;&#20026;&#39318;&#35201;&#65292;&#22240;&#27492;Lie-Poisson&#25324;&#21495;&#34987;&#20934;&#30830;&#30693;&#26195;&#65292;&#32780;Hamiltonian&#21017;&#34987;&#35270;&#20026;&#26469;&#33258;&#29289;&#29702;&#19988;&#21487;&#33021;&#19981;&#30693;&#26195;&#25110;&#36817;&#20284;&#24050;&#30693;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#25442;&#30340;&#32593;&#32476;&#65292;&#33021;&#22815;&#20934;&#30830;&#20445;&#25345;Poisson&#25324;&#21495;&#21644;&#29305;&#27530;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
An accurate data-based prediction of the long-term evolution of Hamiltonian systems requires a network that preserves the appropriate structure under each time step. Every Hamiltonian system contains two essential ingredients: the Poisson bracket and the Hamiltonian. Hamiltonian systems with symmetries, whose paradigm examples are the Lie-Poisson systems, have been shown to describe a broad category of physical phenomena, from satellite motion to underwater vehicles, fluids, geophysical applications, complex fluids, and plasma physics. The Poisson bracket in these systems comes from the symmetries, while the Hamiltonian comes from the underlying physics. We view the symmetry of the system as primary, hence the Lie-Poisson bracket is known exactly, whereas the Hamiltonian is regarded as coming from physics and is considered not known, or known approximately. Using this approach, we develop a network based on transformations that exactly preserve the Poisson bracket and the special funct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#19981;&#21487;&#23519;&#35273;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25915;&#20987;&#36755;&#20837;&#22270;&#20687;&#36793;&#30028;&#26469;&#26597;&#25214;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36793;&#30028;&#21487;&#20197;&#20027;&#23548;DNN&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2308.15344</link><description>&lt;p&gt;
&#20174;&#22270;&#20687;&#36793;&#30028;&#36827;&#34892;&#30340;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21487;&#23519;&#35273;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Imperceptible Adversarial Attack on Deep Neural Networks from Image Boundary. (arXiv:2308.15344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#19981;&#21487;&#23519;&#35273;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25915;&#20987;&#36755;&#20837;&#22270;&#20687;&#36793;&#30028;&#26469;&#26597;&#25214;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36793;&#30028;&#21487;&#20197;&#20027;&#23548;DNN&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#20363;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65288;ViTs&#65289;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#20294;&#23427;&#20204;&#34987;&#35777;&#26126;&#23481;&#26131;&#34987;&#20266;&#35013;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#65288;AEs&#65289;&#25152;&#27450;&#39575;&#12290;&#23545;AE&#30340;&#30740;&#31350;&#19968;&#30452;&#24456;&#27963;&#36291;&#65292;&#33258;&#20174;2014&#24180;&#21457;&#29616;&#20197;&#26469;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#35299;&#37322;&#12290;AE&#23384;&#22312;&#30340;&#22885;&#31192;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#65292;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;DNN&#35757;&#32451;&#31639;&#27861;&#23384;&#22312;&#30450;&#21306;&#12290;&#26174;&#33879;&#23545;&#35937;&#36890;&#24120;&#19981;&#19982;&#36793;&#30028;&#37325;&#21472;&#65292;&#22240;&#27492;&#36793;&#30028;&#19981;&#26159;DNN&#27169;&#22411;&#30340;&#20851;&#27880;&#28857;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36793;&#30028;&#21487;&#20197;&#20027;&#23548;DNN&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#35266;&#23519;AE&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#23545;&#36755;&#20837;&#22270;&#20687;&#36793;&#30028;&#36827;&#34892;&#31995;&#32479;&#24615;&#25915;&#20987;&#20197;&#23547;&#25214;AE&#30340;&#19981;&#21487;&#23519;&#35273;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Although Deep Neural Networks (DNNs), such as the convolutional neural networks (CNN) and Vision Transformers (ViTs), have been successfully applied in the field of computer vision, they are demonstrated to be vulnerable to well-sought Adversarial Examples (AEs) that can easily fool the DNNs. The research in AEs has been active, and many adversarial attacks and explanations have been proposed since they were discovered in 2014. The mystery of the AE's existence is still an open question, and many studies suggest that DNN training algorithms have blind spots. The salient objects usually do not overlap with boundaries; hence, the boundaries are not the DNN model's attention. Nevertheless, recent studies show that the boundaries can dominate the behavior of the DNN models. Hence, this study aims to look at the AEs from a different perspective and proposes an imperceptible adversarial attack that systemically attacks the input image boundary for finding the AEs. The experimental results ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#20851;&#27880;&#29305;&#24449;&#22270;&#26469;&#22686;&#24378;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36741;&#21161;&#29305;&#24449;&#20256;&#36882;&#32473;&#19979;&#28216;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#20998;&#24067;&#20043;&#22806;&#26679;&#26412;&#30340;&#31283;&#20581;&#24615;&#21644;&#23398;&#20064;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.15327</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#30340;&#20154;&#31867;&#20851;&#27880;&#29305;&#24449;&#22270;&#26469;&#22686;&#24378;&#26426;&#22120;&#20154;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Robot Learning through Learned Human-Attention Feature Maps. (arXiv:2308.15327v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#20851;&#27880;&#29305;&#24449;&#22270;&#26469;&#22686;&#24378;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36741;&#21161;&#29305;&#24449;&#20256;&#36882;&#32473;&#19979;&#28216;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#20998;&#24067;&#20043;&#22806;&#26679;&#26412;&#30340;&#31283;&#20581;&#24615;&#21644;&#23398;&#20064;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#65292;&#24378;&#22823;&#32780;&#39640;&#25928;&#30340;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22797;&#26434;&#30340;&#35270;&#35273;&#36755;&#20837;&#12290;&#21463;&#21040;&#20154;&#31867;&#27880;&#24847;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35748;&#20026;&#23558;&#20851;&#27880;&#28966;&#28857;&#30340;&#36741;&#21161;&#20449;&#24687;&#23884;&#20837;&#21040;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#65292;&#23558;&#25552;&#39640;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#31283;&#20581;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#19968;&#20010;&#36817;&#20284;&#39044;&#27979;&#27169;&#22411;&#26469;&#24314;&#27169;&#21644;&#27169;&#25311;&#20154;&#31867;&#30340;&#27880;&#24847;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36755;&#20986;&#20316;&#20026;&#32467;&#26500;&#21270;&#30340;&#36741;&#21161;&#29305;&#24449;&#22270;&#20256;&#36882;&#32473;&#19979;&#28216;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25163;&#21160;&#39550;&#39542;&#20013;&#65292;&#20174;&#20154;&#31867;&#27880;&#35270;&#35760;&#24405;&#20013;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#26469;&#39564;&#35777;&#36825;&#20010;&#24819;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23398;&#20064;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#8212;&#8212;&#30446;&#26631;&#26816;&#27979;&#21644;&#27169;&#20223;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#39044;&#27979;&#30340;&#20154;&#31867;&#20851;&#27880;&#24341;&#20837;&#21040;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#20998;&#24067;&#20043;&#22806;&#26679;&#26412;&#30340;&#31283;&#20581;&#24615;&#65292;&#21516;&#26102;&#20063;&#33021;&#23454;&#29616;&#26356;&#24555;&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust and efficient learning remains a challenging problem in robotics, in particular with complex visual inputs. Inspired by human attention mechanism, with which we quickly process complex visual scenes and react to changes in the environment, we think that embedding auxiliary information about focus point into robot learning would enhance efficiency and robustness of the learning process. In this paper, we propose a novel approach to model and emulate the human attention with an approximate prediction model. We then leverage this output and feed it as a structured auxiliary feature map into downstream learning tasks. We validate this idea by learning a prediction model from human-gaze recordings of manual driving in the real world. We test our approach on two learning tasks - object detection and imitation learning. Our experiments demonstrate that the inclusion of predicted human attention leads to improved robustness of the trained models to out-of-distribution samples and faster
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22343;&#21248;&#30340;Tanh&#21464;&#25442;&#36827;&#34892;&#38754;&#37096;&#35299;&#26512;&#30340;&#36974;&#25377;&#24863;&#30693;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#38754;&#37096;&#36974;&#25377;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#34701;&#21512;&#26356;&#22810;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#36974;&#25377;&#24863;&#30693;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#36793;&#30028;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15323</link><description>&lt;p&gt;
&#36890;&#36807;&#22343;&#21248;&#30340;Tanh&#21464;&#25442;&#30340;&#38754;&#37096;&#35299;&#26512;&#30340;&#36974;&#25377;&#24863;&#30693;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Occlusion-Aware Deep Convolutional Neural Network via Homogeneous Tanh-transforms for Face Parsing. (arXiv:2308.15323v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22343;&#21248;&#30340;Tanh&#21464;&#25442;&#36827;&#34892;&#38754;&#37096;&#35299;&#26512;&#30340;&#36974;&#25377;&#24863;&#30693;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#38754;&#37096;&#36974;&#25377;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#34701;&#21512;&#26356;&#22810;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#36974;&#25377;&#24863;&#30693;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#36793;&#30028;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#35299;&#26512;&#26159;&#20026;&#27599;&#20010;&#35821;&#20041;&#38754;&#37096;&#32452;&#20214;&#25512;&#26029;&#20687;&#32032;&#32423;&#26631;&#31614;&#22270;&#30340;&#36807;&#31243;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23545;&#26080;&#36974;&#25377;&#30340;&#38754;&#37096;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#22312;&#38754;&#37096;&#36974;&#25377;&#19979;&#24573;&#30053;&#20102;&#36974;&#25377;&#21644;&#24573;&#35270;&#20102;&#21333;&#20010;&#38754;&#37096;&#22806;&#19968;&#20123;&#19978;&#19979;&#25991;&#21306;&#22495;&#65292;&#23588;&#20854;&#26159;&#22312;COVID-19&#27969;&#34892;&#26399;&#38388;&#65292;&#38754;&#37096;&#36974;&#25377;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24120;&#35265;&#24773;&#20917;&#12290;&#21463;&#22270;&#20687;&#29031;&#26126;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#21363;&#30001;&#22235;&#20010;Tanh&#21464;&#25442;&#32452;&#25104;&#30340;&#22343;&#21248;Tanh&#21464;&#25442;&#65292;&#23558;&#20013;&#22830;&#35270;&#35273;&#21644;&#21608;&#36793;&#35270;&#35273;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36974;&#25377;&#19979;&#38754;&#37096;&#35299;&#26512;&#30340;&#22256;&#22659;&#65292;&#24182;&#21387;&#32553;&#20102;&#26356;&#22810;&#30340;&#21608;&#22260;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22522;&#20110;&#22343;&#21248;&#30340;Tanh&#21464;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36974;&#25377;&#38754;&#37096;&#35299;&#26512;&#30340;&#36974;&#25377;&#24863;&#30693;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#32467;&#21512;&#20102;Tanh&#26497;&#22352;&#26631;&#31354;&#38388;&#21644;Tanh&#31515;&#21345;&#23572;&#31354;&#38388;&#20013;&#30340;&#20449;&#24687;&#65292;&#33021;&#22815;&#22686;&#24378;&#24863;&#21463;&#37326;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36974;&#25377;&#24863;&#30693;&#25439;&#22833;&#65292;&#19987;&#27880;&#20110;&#36974;&#25377;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face parsing infers a pixel-wise label map for each semantic facial component. Previous methods generally work well for uncovered faces, however overlook the facial occlusion and ignore some contextual area outside a single face, especially when facial occlusion has become a common situation during the COVID-19 epidemic. Inspired by the illumination theory of image, we propose a novel homogeneous tanh-transforms for image preprocessing, which made up of four tanh-transforms, that fuse the central vision and the peripheral vision together. Our proposed method addresses the dilemma of face parsing under occlusion and compresses more information of surrounding context. Based on homogeneous tanh-transforms, we propose an occlusion-aware convolutional neural network for occluded face parsing. It combines the information both in Tanh-polar space and Tanh-Cartesian space, capable of enhancing receptive fields. Furthermore, we introduce an occlusion-aware loss to focus on the boundaries of occ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15321</link><description>&lt;p&gt;
&#38416;&#26126;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#8220;&#26333;&#20809;&#20559;&#24046;&#8221;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#65292;&#32570;&#20047;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#20808;&#23545;&#37319;&#26679;&#20998;&#24067;&#36827;&#34892;&#20998;&#26512;&#24314;&#27169;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#30340;&#39044;&#27979;&#35823;&#24046;&#24402;&#22240;&#20026;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#38500;&#20102;&#38416;&#26126;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;Epsilon Scaling&#65292;&#20197;&#20943;&#36731;&#26333;&#20809;&#20559;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Epsilon Scaling&#36890;&#36807;&#32553;&#23567;&#32593;&#32476;&#36755;&#20986;&#65288;Epsilon&#65289;&#26126;&#30830;&#22320;&#23558;&#37319;&#26679;&#36712;&#36857;&#31227;&#36817;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21040;&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#12290;&#22312;&#21508;&#31181;&#25193;&#25955;&#26694;&#26550;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#20351;&#29992;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35774;&#22791;&#31471;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26368;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15308</link><description>&lt;p&gt;
&#20351;&#29992;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35774;&#22791;&#19978;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On-Device Learning with Binary Neural Networks. (arXiv:2308.15308v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#20351;&#29992;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35774;&#22791;&#31471;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26368;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#22312;&#20302;&#21151;&#32791;&#23884;&#20837;&#24335;CPU&#19978;&#37096;&#32626;&#26102;&#65292;&#20165;&#37096;&#20998;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21151;&#32791;&#12289;&#20869;&#23384;&#21644;&#35745;&#31639;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;(BNN)&#30340;&#39640;&#25928;&#24615;&#65292;BNN&#20351;&#29992;1&#20301;&#29992;&#20110;&#26435;&#37325;&#21644;&#28608;&#27963;&#20197;&#39640;&#25928;&#25191;&#34892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;CWR*&#30340;&#28151;&#21512;&#37327;&#21270;&#26041;&#27861;&#65288;&#19968;&#31181;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#21069;&#21521;&#20256;&#36882;&#21644;&#21453;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#20998;&#21035;&#32771;&#34385;&#65292;&#20197;&#22312;&#26799;&#24230;&#26356;&#26032;&#27493;&#39588;&#20013;&#20445;&#25345;&#26356;&#39640;&#30340;&#31934;&#24230;&#65292;&#24182;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#24310;&#36831;&#24320;&#38144;&#12290;&#36873;&#25321;&#20108;&#36827;&#21046;&#32593;&#32476;&#20316;&#20026;&#39592;&#24178;&#32593;&#32476;&#23545;&#20110;&#28385;&#36275;&#20302;&#21151;&#32791;&#35774;&#22791;&#30340;&#38480;&#21046;&#33267;&#20851;&#37325;&#35201;&#65292;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23581;&#35797;&#35777;&#26126;&#20351;&#29992;BNN&#36827;&#34892;&#35774;&#22791;&#19978;&#30340;&#23398;&#20064;&#12290;&#36827;&#34892;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing Continual Learning (CL) solutions only partially address the constraints on power, memory and computation of the deep learning models when deployed on low-power embedded CPUs. In this paper, we propose a CL solution that embraces the recent advancements in CL field and the efficiency of the Binary Neural Networks (BNN), that use 1-bit for weights and activations to efficiently execute deep learning models. We propose a hybrid quantization of CWR* (an effective CL approach) that considers differently forward and backward pass in order to retain more precision during gradient update step and at the same time minimizing the latency overhead. The choice of a binary network as backbone is essential to meet the constraints of low power devices and, to the best of authors' knowledge, this is the first attempt to prove on-device learning with BNN. The experimental validation carried out confirms the validity and the suitability of the proposed method.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSMs)&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33258;&#21160;&#24515;&#30005;&#22270;&#20998;&#26512;&#20013;&#30340;&#23450;&#37327;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;SSMs&#21487;&#20197;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;&#35786;&#26029;&#20219;&#21153;&#20013;&#65292;&#36739;&#39640;&#30340;&#37319;&#26679;&#29575;&#21644;&#25193;&#22823;&#36755;&#20837;&#23610;&#23544;&#24182;&#27809;&#26377;&#26126;&#26174;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15291</link><description>&lt;p&gt;
&#36808;&#21521;&#24515;&#30005;&#22270;&#20998;&#26512;&#30340;&#23450;&#37327;&#31934;&#30830;&#24615;&#65306;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24739;&#32773;&#20803;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Towards quantitative precision for ECG analysis: Leveraging state space models, self-supervision and patient metadata. (arXiv:2308.15291v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15291
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSMs)&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33258;&#21160;&#24515;&#30005;&#22270;&#20998;&#26512;&#20013;&#30340;&#23450;&#37327;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;SSMs&#21487;&#20197;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;&#35786;&#26029;&#20219;&#21153;&#20013;&#65292;&#36739;&#39640;&#30340;&#37319;&#26679;&#29575;&#21644;&#25193;&#22823;&#36755;&#20837;&#23610;&#23544;&#24182;&#27809;&#26377;&#26126;&#26174;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#20026;&#33258;&#21160;&#24515;&#30005;&#22270;&#20998;&#26512;&#30340;&#39318;&#36873;&#24314;&#27169;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19977;&#20010;&#26088;&#22312;&#25552;&#39640;&#36825;&#31181;&#31995;&#32479;&#23450;&#37327;&#20934;&#30830;&#24615;&#30340;&#35201;&#32032;&#12290;&#36825;&#20123;&#32452;&#20214;&#22987;&#32456;&#36229;&#36234;&#20102;&#22522;&#20110;&#21367;&#31215;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSMs)&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#26041;&#38754;&#26174;&#31034;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#23558;SSMs&#32435;&#20837;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#19981;&#20165;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36824;&#23545;&#35813;&#39046;&#22495;&#30340;&#38271;&#26399;&#30456;&#20851;&#38382;&#39064;&#26377;&#20102;&#26356;&#28145;&#30340;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#26631;&#20934;&#35786;&#26029;&#20219;&#21153;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;100Hz&#30456;&#27604;&#65292;&#20351;&#29992;500Hz&#31561;&#26356;&#39640;&#37319;&#26679;&#29575;&#27809;&#26377;&#20248;&#21183;&#12290;&#31867;&#20284;&#22320;&#65292;&#23558;&#27169;&#22411;&#30340;&#36755;&#20837;&#23610;&#23544;&#25193;&#22823;&#21040;3&#31186;&#20197;&#19978;&#20063;&#27809;&#26377;&#24102;&#26469;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#23545;&#27604;&#39044;&#27979;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#22815;&#22686;&#24378;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#31532;&#20108;&#20010;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has emerged as the preferred modeling approach for automatic ECG analysis. In this study, we investigate three elements aimed at improving the quantitative accuracy of such systems. These components consistently enhance performance beyond the existing state-of-the-art, which is predominantly based on convolutional models. Firstly, we explore more expressive architectures by exploiting structured state space models (SSMs). These models have shown promise in capturing long-term dependencies in time series data. By incorporating SSMs into our approach, we not only achieve better performance, but also gain insights into long-standing questions in the field. Specifically, for standard diagnostic tasks, we find no advantage in using higher sampling rates such as 500Hz compared to 100Hz. Similarly, extending the input size of the model beyond 3 seconds does not lead to significant improvements. Secondly, we demonstrate that self-supervised learning using contrastive predictive c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21516;&#24577;&#35745;&#25968;&#30340;&#32467;&#26500;&#33410;&#28857;&#23884;&#20837;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36825;&#31181;&#23884;&#20837;&#22312;&#35299;&#37322;&#21644;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.15283</link><description>&lt;p&gt;
&#20351;&#29992;&#21516;&#24577;&#35745;&#25968;&#30340;&#32467;&#26500;&#33410;&#28857;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Structural Node Embeddings with Homomorphism Counts. (arXiv:2308.15283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21516;&#24577;&#35745;&#25968;&#30340;&#32467;&#26500;&#33410;&#28857;&#23884;&#20837;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36825;&#31181;&#23884;&#20837;&#22312;&#35299;&#37322;&#21644;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21516;&#24577;&#35745;&#25968;&#26159;1967&#24180;&#30001;Lov\'asz&#39318;&#27425;&#25506;&#32034;&#30340;&#65292;&#26368;&#36817;&#22312;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;Grohe (PODS 2020)&#25552;&#20986;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#22270;&#32423;&#21035;&#21644;&#33410;&#28857;&#32423;&#21035;&#20219;&#21153;&#20013;&#20351;&#29992;&#21516;&#24577;&#35745;&#25968;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#30001;&#20110;&#20854;&#26412;&#36136;&#19978;&#25429;&#25417;&#20102;&#23616;&#37096;&#32467;&#26500;&#20449;&#24687;&#65292;&#36825;&#20351;&#24471;&#21019;&#24314;&#31283;&#20581;&#30340;&#32467;&#26500;&#23884;&#20837;&#25104;&#20026;&#21487;&#33021;&#12290;&#34429;&#28982;Nguyen&#21644;Maehara (ICML 2020)&#24050;&#32463;&#25552;&#20986;&#20102;&#29992;&#20110;&#22270;&#32423;&#20219;&#21153;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22522;&#20110;&#21516;&#24577;&#35745;&#25968;&#30340;&#33410;&#28857;&#23884;&#20837;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#23884;&#20837;&#19982;&#33410;&#28857;&#26631;&#31614;&#12289;&#33410;&#28857;&#26435;&#37325;&#21644;&#36793;&#26435;&#37325;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#23545;&#22270;&#25968;&#25454;&#30340;&#21487;&#35299;&#37322;&#24615;&#34920;&#31034;&#65292;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26500;&#19981;&#21464;&#30340;&#22522;&#20110;&#21516;&#24577;&#35745;&#25968;&#30340;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;&#39640;&#25928;&#35745;&#31639;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph homomorphism counts, first explored by Lov\'asz in 1967, have recently garnered interest as a powerful tool in graph-based machine learning. Grohe (PODS 2020) proposed the theoretical foundations for using homomorphism counts in machine learning on graph level as well as node level tasks. By their very nature, these capture local structural information, which enables the creation of robust structural embeddings. While a first approach for graph level tasks has been made by Nguyen and Maehara (ICML 2020), we experimentally show the effectiveness of homomorphism count based node embeddings. Enriched with node labels, node weights, and edge weights, these offer an interpretable representation of graph data, allowing for enhanced explainability of machine learning models.  We propose a theoretical framework for isomorphism-invariant homomorphism count based embeddings which lend themselves to a wide variety of downstream tasks. Our approach capitalises on the efficient computability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#24314;&#39640;&#36136;&#37327;&#35821;&#38899;&#30340;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#23545;&#22810;&#26144;&#23556;&#38382;&#39064;&#21644;&#32454;&#33410;&#31934;&#28860;&#26469;&#26174;&#33879;&#25913;&#36827;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.15256</link><description>&lt;p&gt;
&#35753;&#22768;&#38899;&#23384;&#22312;&#65306;&#20174;&#26080;&#22768;&#35270;&#39057;&#20013;&#37325;&#24314;&#39640;&#36136;&#37327;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Let There Be Sound: Reconstructing High Quality Speech from Silent Videos. (arXiv:2308.15256v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#24314;&#39640;&#36136;&#37327;&#35821;&#38899;&#30340;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#23545;&#22810;&#26144;&#23556;&#38382;&#39064;&#21644;&#32454;&#33410;&#31934;&#28860;&#26469;&#26174;&#33879;&#25913;&#36827;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20165;&#36890;&#36807;&#21767;&#36816;&#21160;&#37325;&#24314;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#65292;&#20063;&#34987;&#31216;&#20026;&#21767;&#35821;&#36716;&#35821;&#38899;&#12290;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30001;&#20110;&#21516;&#24418;&#24322;&#38899;&#21644;&#22810;&#26679;&#21270;&#35821;&#38899;&#21464;&#21270;&#32780;&#36896;&#25104;&#30340;&#19968;&#23545;&#22810;&#26144;&#23556;&#65292;&#23548;&#33268;&#21457;&#38899;&#38169;&#35823;&#21644;&#36807;&#24230;&#24179;&#28369;&#30340;&#35821;&#38899;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21767;&#35821;&#36716;&#35821;&#38899;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#32531;&#35299;&#19968;&#23545;&#22810;&#26144;&#23556;&#38382;&#39064;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#65288;1&#65289;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#34920;&#31034;&#26469;&#28040;&#38500;&#21516;&#24418;&#24322;&#38899;&#65292;&#21644;&#65288;2&#65289;&#22768;&#23398;&#21464;&#24322;&#20449;&#24687;&#26469;&#24314;&#27169;&#22810;&#26679;&#21270;&#30340;&#35821;&#38899;&#39118;&#26684;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#27969;&#30340;&#21518;&#22788;&#29702;&#32593;&#32476;&#65292;&#25429;&#25417;&#21644;&#31934;&#28860;&#25152;&#29983;&#25104;&#35821;&#38899;&#30340;&#32454;&#33410;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#25509;&#36817;&#30495;&#23454;&#20154;&#31867;&#35821;&#38899;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this work is to reconstruct high quality speech from lip motions alone, a task also known as lip-to-speech. A key challenge of lip-to-speech systems is the one-to-many mapping caused by (1) the existence of homophenes and (2) multiple speech variations, resulting in a mispronounced and over-smoothed speech. In this paper, we propose a novel lip-to-speech system that significantly improves the generation quality by alleviating the one-to-many mapping problem from multiple perspectives. Specifically, we incorporate (1) self-supervised speech representations to disambiguate homophenes, and (2) acoustic variance information to model diverse speech styles. Additionally, to better solve the aforementioned problem, we employ a flow based post-net which captures and refines the details of the generated speech. We perform extensive experiments and demonstrate that our method achieves the generation quality close to that of real human utterance, outperforming existing methods in term
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#30456;&#23545;&#39640;&#26031;&#26426;&#21046;(RGM)&#65292;&#23427;&#21033;&#29992;&#20102;&#30456;&#23545;L2&#25935;&#24863;&#24615;&#20551;&#35774;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#30028;&#23450;&#38544;&#31169;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2308.15250</link><description>&lt;p&gt;
&#30456;&#23545;&#39640;&#26031;&#26426;&#21046;&#21450;&#20854;&#22312;&#31169;&#26377;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Relative Gaussian Mechanism and its Application to Private Gradient Descent. (arXiv:2308.15250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#30456;&#23545;&#39640;&#26031;&#26426;&#21046;(RGM)&#65292;&#23427;&#21033;&#29992;&#20102;&#30456;&#23545;L2&#25935;&#24863;&#24615;&#20551;&#35774;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#33021;&#22815;&#26356;&#31934;&#30830;&#22320;&#30028;&#23450;&#38544;&#31169;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#26426;&#21046;(GM)&#26159;&#19968;&#31181;&#22312;&#21457;&#24067;&#20043;&#21069;&#21521;&#30690;&#37327;&#26597;&#35810;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#30340;&#26631;&#20934;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#12290;&#29305;&#21035;&#22320;&#65292;&#22914;&#26524;&#26597;&#35810;&#28385;&#36275;&#26576;&#31181;L2&#25935;&#24863;&#24615;&#23646;&#24615;(&#20219;&#24847;&#20004;&#20010;&#30456;&#37051;&#36755;&#20837;&#19978;&#36755;&#20986;&#20043;&#38388;&#30340;L2&#36317;&#31163;&#26377;&#30028;)&#65292;GM&#20445;&#35777;&#20102;R&#233;nyi&#24046;&#20998;&#38544;&#31169;(RDP)&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#31934;&#30830;&#22320;&#30028;&#23450;L2&#25935;&#24863;&#24615;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#20174;&#32780;&#23548;&#33268;&#26494;&#24347;&#30340;&#38544;&#31169;&#30028;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#30456;&#23545;L2&#25935;&#24863;&#24615;&#20551;&#35774;&#65292;&#22312;&#36825;&#31181;&#20551;&#35774;&#19979;&#65292;&#20004;&#20010;&#26597;&#35810;&#36755;&#20986;&#20043;&#38388;&#30340;&#36317;&#31163;&#30028;&#38480;&#20063;&#21487;&#33021;&#21462;&#20915;&#20110;&#23427;&#20204;&#30340;&#33539;&#25968;&#12290;&#21033;&#29992;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30456;&#23545;&#39640;&#26031;&#26426;&#21046;(RGM)&#65292;&#20854;&#20013;&#22122;&#22768;&#30340;&#26041;&#24046;&#21462;&#20915;&#20110;&#36755;&#20986;&#30340;&#33539;&#25968;&#12290;&#25105;&#20204;&#22312;&#30456;&#23545;L2&#25935;&#24863;&#24615;&#19979;&#35777;&#26126;&#20102;RDP&#21442;&#25968;&#30340;&#20005;&#26684;&#30028;&#38480;&#65292;&#24182;&#25551;&#36848;&#20102;&#22240;&#20351;&#29992;&#36755;&#20986;&#30456;&#20851;&#22122;&#22768;&#32780;&#20135;&#29983;&#30340;&#38544;&#31169;&#25439;&#22833;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RGM&#21487;&#20197;&#33258;&#28982;&#22320;&#36866;&#24212;&#28508;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Gaussian Mechanism (GM), which consists in adding Gaussian noise to a vector-valued query before releasing it, is a standard privacy protection mechanism. In particular, given that the query respects some L2 sensitivity property (the L2 distance between outputs on any two neighboring inputs is bounded), GM guarantees R\'enyi Differential Privacy (RDP). Unfortunately, precisely bounding the L2 sensitivity can be hard, thus leading to loose privacy bounds. In this work, we consider a Relative L2 sensitivity assumption, in which the bound on the distance between two query outputs may also depend on their norm. Leveraging this assumption, we introduce the Relative Gaussian Mechanism (RGM), in which the variance of the noise depends on the norm of the output. We prove tight bounds on the RDP parameters under relative L2 sensitivity, and characterize the privacy loss incurred by using output-dependent noise. In particular, we show that RGM naturally adapts to a latent variable that would
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;COMPAS&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#21487;&#38752;&#24615;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#36755;&#20986;&#21487;&#38752;&#24615;&#23384;&#22312;&#31995;&#32479;&#24615;&#24046;&#24322;&#65292;&#24046;&#24322;&#30340;&#26041;&#21521;&#21462;&#20915;&#20110;&#25152;&#20351;&#29992;&#30340;&#35780;&#20272;&#32773;&#38388;&#32479;&#35745;&#37327;&#30340;&#31867;&#22411;&#21644;&#26159;&#21542;&#36827;&#34892;&#20102;&#23545;&#32676;&#20307;&#39044;&#27979;&#27010;&#29575;&#30340;&#20462;&#27491;&#12290;</title><link>http://arxiv.org/abs/2308.15243</link><description>&lt;p&gt;
COMPAS&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#21487;&#38752;&#24615;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Reliability Gaps Between Groups in COMPAS Dataset. (arXiv:2308.15243v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;COMPAS&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#21487;&#38752;&#24615;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#36755;&#20986;&#21487;&#38752;&#24615;&#23384;&#22312;&#31995;&#32479;&#24615;&#24046;&#24322;&#65292;&#24046;&#24322;&#30340;&#26041;&#21521;&#21462;&#20915;&#20110;&#25152;&#20351;&#29992;&#30340;&#35780;&#20272;&#32773;&#38388;&#32479;&#35745;&#37327;&#30340;&#31867;&#22411;&#21644;&#26159;&#21542;&#36827;&#34892;&#20102;&#23545;&#32676;&#20307;&#39044;&#27979;&#27010;&#29575;&#30340;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39118;&#38505;&#35780;&#20272;&#24037;&#20855;&#65288;RAIs&#65289;&#30340;&#35780;&#20272;&#32773;&#38388;&#21487;&#38752;&#24615;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#19981;&#21516;&#30340;&#31038;&#20250;&#26174;&#33879;&#32676;&#20307;&#26159;&#21542;&#21463;&#21040;RAIs&#35780;&#20272;&#32773;&#38388;&#21487;&#38752;&#24615;&#19981;&#36275;&#30340;&#24433;&#21709;&#65292;&#21363;&#19981;&#21516;&#32676;&#20307;&#30340;&#38169;&#35823;&#26159;&#21542;&#20250;&#23545;&#20182;&#20204;&#20135;&#29983;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;COMPAS&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#25311;&#30740;&#31350;&#26469;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#20013;&#27880;&#20837;&#20102;&#25511;&#21046;&#30340;&#22122;&#22768;&#65292;&#36825;&#31181;&#22122;&#22768;&#21487;&#20197;&#35299;&#37322;&#20026;&#21046;&#36896;&#38169;&#35823;&#30340;&#21512;&#25104;&#35780;&#20272;&#32773;&#12290;&#20027;&#35201;&#21457;&#29616;&#26159;COMPAS&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#36755;&#20986;&#21487;&#38752;&#24615;&#23384;&#22312;&#31995;&#32479;&#24046;&#24322;&#12290;&#24046;&#24322;&#30340;&#26041;&#21521;&#21462;&#20915;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#32773;&#38388;&#32479;&#35745;&#37327;&#30340;&#31867;&#22411;&#65288;Cohen's Kappa&#12289;Byrt's PABAK&#12289;ICC&#65289;&#65292;&#29305;&#21035;&#26159;&#26159;&#21542;&#20351;&#29992;&#20102;&#23545;&#32676;&#20307;&#39044;&#27979;&#27010;&#29575;&#36827;&#34892;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the inter-rater reliability of risk assessment instruments (RAIs). The main question is whether different, socially salient groups are affected differently by a lack of inter-rater reliability of RAIs, that is, whether mistakes with respect to different groups affects them differently. The question is investigated with a simulation study of the COMPAS dataset. A controlled degree of noise is injected into the input data of a predictive model; the noise can be interpreted as a synthetic rater that makes mistakes. The main finding is that there are systematic differences in output reliability between groups in the COMPAS dataset. The sign of the difference depends on the kind of inter-rater statistic that is used (Cohen's Kappa, Byrt's PABAK, ICC), and in particular whether or not a correction of predictions prevalences of the groups is used.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24490;&#29615;&#24179;&#31283;&#24615;&#26816;&#27979;&#24694;&#24847;&#36719;&#20214;&#34892;&#20026;&#65292;&#24182;&#25214;&#21040;&#22312;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#24490;&#29615;&#24179;&#31283;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.15237</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#36873;&#25321;&#21644;&#20998;&#31867;&#35780;&#20272;&#24490;&#29615;&#24179;&#31283;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Assessing Cyclostationary Malware Detection via Feature Selection and Classification. (arXiv:2308.15237v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24490;&#29615;&#24179;&#31283;&#24615;&#26816;&#27979;&#24694;&#24847;&#36719;&#20214;&#34892;&#20026;&#65292;&#24182;&#25214;&#21040;&#22312;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#24490;&#29615;&#24179;&#31283;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#24179;&#31283;&#24615;&#28041;&#21450;&#20449;&#21495;&#21644;&#36827;&#31243;&#20013;&#30340;&#21608;&#26399;&#24615;&#32479;&#35745;&#21464;&#21270;&#65292;&#36890;&#24120;&#29992;&#20110;&#20449;&#21495;&#20998;&#26512;&#21644;&#32593;&#32476;&#23433;&#20840;&#12290;&#22312;&#25915;&#20987;&#24773;&#22659;&#20013;&#65292;&#24490;&#29615;&#24179;&#31283;&#24615;&#26377;&#21161;&#20110;&#26816;&#27979;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#24694;&#24847;&#34892;&#20026;&#65292;&#22914;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#20013;&#30340;&#27969;&#37327;&#27169;&#24335;&#25110;&#24694;&#24847;&#36719;&#20214;&#20013;&#30340;&#38544;&#34255;&#36890;&#20449;&#36890;&#36947;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#24322;&#24120;&#27169;&#24335;&#24182;&#36890;&#30693;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDSs&#65289;&#35782;&#21035;&#28508;&#22312;&#25915;&#20987;&#65292;&#22686;&#24378;&#23545;&#24050;&#30693;&#21644;&#26032;&#39062;&#23041;&#32961;&#30340;&#20445;&#25252;&#12290;&#26412;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#35782;&#21035;&#24490;&#29615;&#24179;&#31283;&#30340;&#24694;&#24847;&#36719;&#20214;&#34892;&#20026;&#21450;&#20854;&#26816;&#27979;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#30830;&#23450;&#22312;NIDSs&#20013;&#20351;&#29992;&#30340;&#20851;&#38190;&#24490;&#29615;&#24179;&#31283;&#29305;&#24449;&#12290;&#36825;&#20123;&#29305;&#24449;&#20351;&#29992;Boruta&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#31561;&#31639;&#27861;&#25552;&#21462;&#65292;&#28982;&#21518;&#36827;&#34892;&#20998;&#31867;&#20197;&#25214;&#21040;&#26368;&#37325;&#35201;&#30340;&#24490;&#29615;&#24179;&#31283;&#27169;&#24335;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24490;&#29615;&#24179;&#31283;&#24615;&#25581;&#31034;&#24694;&#24847;&#36719;&#20214;&#34892;&#20026;&#30340;&#21608;&#26399;&#24615;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cyclostationarity involves periodic statistical variations in signals and processes, commonly used in signal analysis and network security. In the context of attacks, cyclostationarity helps detect malicious behaviors within network traffic, such as traffic patterns in Distributed Denial of Service (DDoS) attacks or hidden communication channels in malware. This approach enhances security by identifying abnormal patterns and informing Network Intrusion Detection Systems (NIDSs) to recognize potential attacks, enhancing protection against both known and novel threats. This research focuses on identifying cyclostationary malware behavior and its detection. The main goal is to pinpoint essential cyclostationary features used in NIDSs. These features are extracted using algorithms such as Boruta and Principal Component Analysis (PCA), and then categorized to find the most significant cyclostationary patterns. The aim of this article is to reveal periodically changing malware behaviors thro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#30340;&#20998;&#31867;&#24863;&#30693;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20914;&#31361;&#20998;&#31867;&#21644;&#20027;&#39064;&#21457;&#29616;&#12290;&#35813;&#27169;&#22411;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#20998;&#31867;&#32467;&#26524;&#21644;&#21457;&#29616;&#30340;&#20027;&#39064;&#30340;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15232</link><description>&lt;p&gt;
&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#30340;&#20998;&#31867;&#24863;&#30693;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#8212;&#8212;&#29992;&#20110;&#20914;&#31361;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification-Aware Neural Topic Model Combined With Interpretable Analysis -- For Conflict Classification. (arXiv:2308.15232v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#30340;&#20998;&#31867;&#24863;&#30693;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20914;&#31361;&#20998;&#31867;&#21644;&#20027;&#39064;&#21457;&#29616;&#12290;&#35813;&#27169;&#22411;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#20998;&#31867;&#32467;&#26524;&#21644;&#21457;&#29616;&#30340;&#20027;&#39064;&#30340;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#19978;&#26377;&#22823;&#37327;&#30340;&#20914;&#31361;&#20107;&#20214;&#19968;&#30452;&#22312;&#24433;&#21709;&#30528;&#25105;&#20204;&#12290;&#20026;&#20102;&#26377;&#25928;&#20998;&#26512;&#36825;&#20123;&#20914;&#31361;&#20107;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20914;&#31361;&#20449;&#24687;&#20998;&#31867;&#21644;&#20027;&#39064;&#21457;&#29616;&#30340;&#20998;&#31867;&#24863;&#30693;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;CANTM-IA&#65289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#26469;&#25552;&#20379;&#21487;&#38752;&#30340;&#20998;&#31867;&#32467;&#26524;&#21644;&#21457;&#29616;&#30340;&#20027;&#39064;&#30340;&#35299;&#37322;&#12290;&#21516;&#26102;&#65292;&#23558;&#35299;&#37322;&#24615;&#24341;&#20837;&#27169;&#22411;&#26550;&#26500;&#20013;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#20351;&#35299;&#37322;&#36827;&#19968;&#27493;&#20851;&#27880;&#25968;&#25454;&#30340;&#32454;&#33410;&#12290;&#26368;&#21518;&#65292;&#23545;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large number of conflict events are affecting the world all the time. In order to analyse such conflict events effectively, this paper presents a Classification-Aware Neural Topic Model (CANTM-IA) for Conflict Information Classification and Topic Discovery. The model provides a reliable interpretation of classification results and discovered topics by introducing interpretability analysis. At the same time, interpretation is introduced into the model architecture to improve the classification performance of the model and to allow interpretation to focus further on the details of the data. Finally, the model architecture is optimised to reduce the complexity of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#32534;&#30721;&#26469;&#20943;&#23569;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27495;&#35270;&#65292;&#20174;&#32780;&#20026;&#20197;&#21069;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#25552;&#20379;&#20844;&#24179;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2308.15230</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20026;&#20197;&#21069;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#25552;&#20379;&#20844;&#24179;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Providing Previously Unseen Users Fair Recommendations Using Variational Autoencoders. (arXiv:2308.15230v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#32534;&#30721;&#26469;&#20943;&#23569;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27495;&#35270;&#65292;&#20174;&#32780;&#20026;&#20197;&#21069;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#25552;&#20379;&#20844;&#24179;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20851;&#20110;&#20844;&#24179;&#24615;&#30340;&#26032;&#23450;&#20041;&#35201;&#27714;&#27169;&#22411;&#23545;&#29992;&#25143;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#19981;&#21487;&#35265;&#65292;&#20363;&#22914;&#65292;&#29992;&#25143;&#30340;&#24615;&#21035;&#25110;&#24180;&#40836;&#19981;&#24212;&#24433;&#21709;&#27169;&#22411;&#12290;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#29305;&#21035;&#23481;&#26131;&#36890;&#36807;&#20854;&#26174;&#24335;&#30340;&#29992;&#25143;&#20851;&#27880;&#21644;&#29992;&#25143;&#24314;&#27169;&#26469;&#36829;&#21453;&#36825;&#20010;&#23450;&#20041;&#12290;&#26174;&#24335;&#30340;&#29992;&#25143;&#24314;&#27169;&#20063;&#26159;&#35768;&#22810;&#25512;&#33616;&#31995;&#32479;&#26080;&#27861;&#20026;&#20197;&#21069;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#25552;&#20379;&#25512;&#33616;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38480;&#21046;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#32534;&#30721;&#30340;&#26032;&#26041;&#27861;&#26469;&#20943;&#23569;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27495;&#35270;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#35780;&#20272;&#20013;&#20026;&#26410;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#29992;&#25143;&#25552;&#20379;&#20844;&#24179;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emerging definition of fairness in machine learning requires that models are oblivious to demographic user information, e.g., a user's gender or age should not influence the model. Personalized recommender systems are particularly prone to violating this definition through their explicit user focus and user modelling. Explicit user modelling is also an aspect that makes many recommender systems incapable of providing hitherto unseen users with recommendations. We propose novel approaches for mitigating discrimination in Variational Autoencoder-based recommender systems by limiting the encoding of demographic information. The approaches are capable of, and evaluated on, providing users that are not represented in the training data with fair recommendations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#26041;&#27861;&#26469;&#25351;&#31034;&#20998;&#31867;&#20915;&#31574;&#20013;&#26368;&#30456;&#20851;&#30340;&#36890;&#36947;&#21644;&#26102;&#38388;&#24207;&#21015;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.15223</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating Explanation Methods for Multivariate Time Series Classification. (arXiv:2308.15223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#26041;&#27861;&#26469;&#25351;&#31034;&#20998;&#31867;&#20915;&#31574;&#20013;&#26368;&#30456;&#20851;&#30340;&#36890;&#36947;&#21644;&#26102;&#38388;&#24207;&#21015;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#35745;&#31639;&#20219;&#21153;&#65292;&#20986;&#29616;&#22312;&#25968;&#25454;&#38543;&#26102;&#38388;&#21644;&#22810;&#20010;&#36890;&#36947;&#35760;&#24405;&#30340;&#24212;&#29992;&#20013;&#12290;&#20363;&#22914;&#65292;&#26234;&#33021;&#25163;&#34920;&#21487;&#20197;&#35760;&#24405;&#20154;&#20307;&#36816;&#21160;&#30340;&#21152;&#36895;&#24230;&#21644;&#26041;&#21521;&#65292;&#36825;&#20123;&#20449;&#21495;&#34987;&#35760;&#24405;&#20026;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#21487;&#20197;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#20102;&#35299;&#21644;&#39044;&#27979;&#20154;&#20307;&#36816;&#21160;&#21644;&#21508;&#31181;&#23646;&#24615;&#65292;&#22914;&#20581;&#36523;&#27700;&#24179;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20165;&#38752;&#20998;&#31867;&#26159;&#19981;&#22815;&#30340;&#65292;&#25105;&#20204;&#36890;&#24120;&#38656;&#35201;&#20998;&#31867;&#65292;&#21516;&#26102;&#36824;&#35201;&#29702;&#35299;&#27169;&#22411;&#23398;&#21040;&#20102;&#20160;&#20040;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#25968;&#25454;&#20013;&#30340;&#21738;&#20123;&#20449;&#24687;&#32473;&#20986;&#20102;&#39044;&#27979;&#65289;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#20998;&#26512;&#21644;&#35780;&#20272;&#19987;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;MTSC&#65289;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#33021;&#25351;&#20986;&#20998;&#31867;&#20915;&#31574;&#20013;&#26368;&#30456;&#20851;&#36890;&#36947;&#21644;&#26102;&#38388;&#24207;&#21015;&#28857;&#30340;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#27969;&#34892;&#19988;&#20934;&#30830;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65292;ROCKET&#21644;...
&lt;/p&gt;
&lt;p&gt;
Multivariate time series classification is an important computational task arising in applications where data is recorded over time and over multiple channels. For example, a smartwatch can record the acceleration and orientation of a person's motion, and these signals are recorded as multivariate time series. We can classify this data to understand and predict human movement and various properties such as fitness levels. In many applications classification alone is not enough, we often need to classify but also understand what the model learns (e.g., why was a prediction given, based on what information in the data). The main focus of this paper is on analysing and evaluating explanation methods tailored to Multivariate Time Series Classification (MTSC). We focus on saliency-based explanation methods that can point out the most relevant channels and time series points for the classification decision. We analyse two popular and accurate multivariate time series classifiers, ROCKET and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#22863;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#25552;&#21319;&#24369;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#23545;&#21453;&#20107;&#23454;&#23454;&#20363;&#30340;&#26368;&#23567;&#21270;&#12289;&#21487;&#25805;&#20316;&#24615;&#12289;&#31283;&#23450;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#21512;&#29702;&#24615;&#21644;&#36776;&#21035;&#21147;&#30340;&#20840;&#35206;&#30422;&#12290;</title><link>http://arxiv.org/abs/2308.15194</link><description>&lt;p&gt;
&#21512;&#22863;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Ensemble of Counterfactual Explainers. (arXiv:2308.15194v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15194
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#22863;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#25552;&#21319;&#24369;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#23545;&#21453;&#20107;&#23454;&#23454;&#20363;&#30340;&#26368;&#23567;&#21270;&#12289;&#21487;&#25805;&#20316;&#24615;&#12289;&#31283;&#23450;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#21512;&#29702;&#24615;&#21644;&#36776;&#21035;&#21147;&#30340;&#20840;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;&#65292;&#27599;&#31181;&#35299;&#37322;&#22120;&#37117;&#20851;&#27880;&#21453;&#20107;&#23454;&#23454;&#20363;&#30340;&#19968;&#20123;&#21487;&#21462;&#29305;&#24615;&#65306;&#26368;&#23567;&#21270;&#12289;&#21487;&#25805;&#20316;&#24615;&#12289;&#31283;&#23450;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#21512;&#29702;&#24615;&#12289;&#36776;&#21035;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#22863;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;&#65292;&#23427;&#33021;&#22815;&#22686;&#24378;&#24369;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#24369;&#35299;&#37322;&#22120;&#20165;&#25552;&#20379;&#36825;&#20123;&#29305;&#24615;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#28085;&#30422;&#25152;&#26377;&#29305;&#24615;&#12290;&#35813;&#21512;&#22863;&#35299;&#37322;&#22120;&#22312;&#19968;&#20123;&#23454;&#20363;&#21644;&#29305;&#24449;&#30340;&#26679;&#26412;&#19978;&#36816;&#34892;&#24369;&#35299;&#37322;&#22120;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#22810;&#26679;&#24615;&#39537;&#21160;&#30340;&#36873;&#25321;&#20989;&#25968;&#26469;&#21512;&#24182;&#20854;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#23553;&#35013;&#26041;&#27861;&#65292;&#20063;&#26159;&#25968;&#25454;&#26080;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In eXplainable Artificial Intelligence (XAI), several counterfactual explainers have been proposed, each focusing on some desirable properties of counterfactual instances: minimality, actionability, stability, diversity, plausibility, discriminative power. We propose an ensemble of counterfactual explainers that boosts weak explainers, which provide only a subset of such properties, to a powerful method covering all of them. The ensemble runs weak explainers on a sample of instances and of features, and it combines their results by exploiting a diversity-driven selection function. The method is model-agnostic and, through a wrapping approach based on autoencoders, it is also data-agnostic.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#21160;&#27668;&#33016;&#35786;&#26029;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;Grad-CAM&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#19981;&#19968;&#23450;&#20250;&#26174;&#33879;&#25913;&#21892;Grad-CAM&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15172</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;Grad-CAM&#30340;&#35270;&#35273;&#35299;&#37322;&#26356;&#21487;&#38752;&#21527;&#65311;&#20197;&#33258;&#21160;&#27668;&#33016;&#35786;&#26029;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is visual explanation with Grad-CAM more reliable for deeper neural networks? a case study with automatic pneumothorax diagnosis. (arXiv:2308.15172v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#21160;&#27668;&#33016;&#35786;&#26029;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;Grad-CAM&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#19981;&#19968;&#23450;&#20250;&#26174;&#33879;&#25913;&#21892;Grad-CAM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#21508;&#31181;&#20020;&#24202;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#20851;&#20110;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#37322;&#24615;&#21487;&#20197;&#26497;&#22823;&#22686;&#24378;&#36825;&#20123;&#26041;&#27861;&#22312;&#23433;&#20840;&#21644;&#24555;&#36895;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#26377;&#39640;&#28789;&#27963;&#24615;&#30340;&#26799;&#24230;&#21152;&#26435;&#31867;&#21035;&#28608;&#27963;&#26144;&#23556;&#65288;Grad-CAM&#65289;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20197;&#25552;&#20379;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#20013;&#25512;&#29702;&#36807;&#31243;&#30340;&#30452;&#35266;&#35270;&#35273;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35813;&#25216;&#26415;&#30340;&#27969;&#34892;&#65292;&#23545;Grad-CAM&#22312;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#19978;&#30340;&#24615;&#33021;&#20173;&#32570;&#20047;&#31995;&#32479;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20854;&#22312;&#19981;&#21516;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#65292;&#37325;&#28857;&#20851;&#27880;&#32593;&#32476;&#30340;&#28145;&#24230;&#21644;&#26550;&#26500;&#31867;&#22411;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#20351;&#29992;X&#20809;&#25195;&#25551;&#20013;&#33258;&#21160;&#27668;&#33016;&#35786;&#26029;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#19968;&#23450;&#20250;&#26174;&#33879;&#25913;&#21892;Grad-CAM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning techniques have provided the state-of-the-art performance in various clinical tasks, explainability regarding their decision-making process can greatly enhance the credence of these methods for safer and quicker clinical adoption. With high flexibility, Gradient-weighted Class Activation Mapping (Grad-CAM) has been widely adopted to offer intuitive visual interpretation of various deep learning models' reasoning processes in computer-assisted diagnosis. However, despite the popularity of the technique, there is still a lack of systematic study on Grad-CAM's performance on different deep learning architectures. In this study, we investigate its robustness and effectiveness across different popular deep learning models, with a focus on the impact of the networks' depths and architecture types, by using a case study of automatic pneumothorax diagnosis in X-ray scans. Our results show that deeper neural networks do not necessarily contribute to a strong improvement of p
&lt;/p&gt;</description></item><item><title>ABS-SGD &#26159;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;GPU&#38598;&#32676;&#30340;&#24310;&#36831;&#21516;&#27493;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#36164;&#28304;&#30340;&#20805;&#20998;&#21033;&#29992;&#24182;&#32531;&#35299;&#20102;&#36807;&#26102;&#26799;&#24230;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15164</link><description>&lt;p&gt;
ABS-SGD: &#19968;&#31181;&#29992;&#20110;&#24322;&#26500;GPU&#38598;&#32676;&#30340;&#24310;&#36831;&#21516;&#27493;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#21644;&#33258;&#36866;&#24212;&#25209;&#37327;&#22823;&#23567;
&lt;/p&gt;
&lt;p&gt;
ABS-SGD: A Delayed Synchronous Stochastic Gradient Descent Algorithm with Adaptive Batch Size for Heterogeneous GPU Clusters. (arXiv:2308.15164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15164
&lt;/p&gt;
&lt;p&gt;
ABS-SGD &#26159;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;GPU&#38598;&#32676;&#30340;&#24310;&#36831;&#21516;&#27493;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#36164;&#28304;&#30340;&#20805;&#20998;&#21033;&#29992;&#24182;&#32531;&#35299;&#20102;&#36807;&#26102;&#26799;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#22686;&#38271;&#65292;&#20197;&#24182;&#34892;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#22312;&#24322;&#26500;&#38598;&#32676;&#20013;&#23384;&#22312;&#30528;&#35745;&#31639;&#36164;&#28304;&#21033;&#29992;&#19981;&#20805;&#20998;&#21644;&#25910;&#25947;&#24615;&#24046;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;GPU&#38598;&#32676;&#30340;&#24310;&#36831;&#21516;&#27493;SGD&#31639;&#27861;&#21644;&#33258;&#36866;&#24212;&#25209;&#37327;&#22823;&#23567;&#65288;ABS-SGD&#65289;&#12290;&#22312;ABS-SGD&#20013;&#65292;&#24037;&#20316;&#33410;&#28857;&#36827;&#34892;&#20840;&#23616;&#21516;&#27493;&#20197;&#32047;&#31215;&#24310;&#36831;&#26799;&#24230;&#65292;&#24182;&#20351;&#29992;&#32047;&#31215;&#30340;&#24310;&#36831;&#26799;&#24230;&#26469;&#26356;&#26032;&#21442;&#25968;&#12290;&#22312;&#24037;&#20316;&#33410;&#28857;&#36827;&#34892;&#24310;&#36831;&#26799;&#24230;&#30340;&#20840;&#23616;&#21516;&#27493;&#26102;&#65292;&#23427;&#20204;&#22312;&#25552;&#21069;&#25351;&#23450;&#25209;&#37327;&#22823;&#23567;&#20043;&#21069;&#36827;&#34892;&#19979;&#19968;&#25209;&#27425;&#30340;&#35745;&#31639;&#65292;&#36825;&#26679;&#21487;&#20197;&#23454;&#29616;&#35745;&#31639;&#36164;&#28304;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#30001;&#20110;&#26799;&#24230;&#24310;&#36831;&#20165;&#20026;&#19968;&#20010;&#36845;&#20195;&#65292;&#21487;&#20197;&#32531;&#35299;&#36807;&#26102;&#26799;&#24230;&#38382;&#39064;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ABS-SGD&#22312;&#24322;&#26500;&#38598;&#32676;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of models and datasets grows, it has become increasingly common to train models in parallel. However, existing distributed stochastic gradient descent (SGD) algorithms suffer from insufficient utilization of computational resources and poor convergence in heterogeneous clusters. In this paper, we propose a delayed synchronous SGD algorithm with adaptive batch size (ABS-SGD) for heterogeneous GPU clusters. In ABS-SGD, workers perform global synchronization to accumulate delayed gradients and use the accumulated delayed gradients to update parameters. While workers are performing global synchronization for delayed gradients, they perform the computation of the next batch without specifying batch size in advance, which lasts until the next global synchronization starts, realizing the full utilization of computational resources. Since the gradient delay is only one iteration, the stale gradient problem can be alleviated. We theoretically prove the convergence of ABS-SGD in hete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#30340;&#25913;&#36827;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#39640;&#20869;&#37096;&#39044;&#27979;&#27169;&#22411;&#30340;&#31934;&#30830;&#24615;&#26469;&#33258;&#21160;&#25913;&#21892;&#25972;&#20010;&#25511;&#21046;&#22120;&#65292;&#32467;&#26524;&#34920;&#26126;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#24635;&#20307;&#19978;&#23558;&#25913;&#21892;&#25511;&#21046;&#22120;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.15157</link><description>&lt;p&gt;
&#20851;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#30340;&#25913;&#36827;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the improvement of model-predictive controllers. (arXiv:2308.15157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#30340;&#25913;&#36827;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#39640;&#20869;&#37096;&#39044;&#27979;&#27169;&#22411;&#30340;&#31934;&#30830;&#24615;&#26469;&#33258;&#21160;&#25913;&#21892;&#25972;&#20010;&#25511;&#21046;&#22120;&#65292;&#32467;&#26524;&#34920;&#26126;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#24635;&#20307;&#19978;&#23558;&#25913;&#21892;&#25511;&#21046;&#22120;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#25104;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#38382;&#39064;&#65292;&#20197;&#35777;&#26126;&#20869;&#37096;&#39044;&#27979;&#27169;&#22411;(PM)&#30340;&#31934;&#30830;&#24615;&#25552;&#39640;&#20250;&#33258;&#21160;&#25913;&#36827;&#25972;&#20010;&#25511;&#21046;&#22120;&#12290;&#19982;&#24378;&#21270;&#23398;&#20064;(RL)&#19981;&#21516;&#65292;MPC&#20351;&#29992;PM&#26469;&#39044;&#27979;&#21463;&#25511;&#31995;&#32479;(CS)&#30340;&#21518;&#32493;&#29366;&#24577;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#25512;&#33616;&#21512;&#36866;&#30340;&#21160;&#20316;&#12290;&#20026;&#20102;&#35780;&#20272;PM&#30340;&#31934;&#30830;&#24615;&#23545;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;DNN&#30340;PM&#19982;&#19977;&#20010;&#22797;&#26434;&#31243;&#24230;&#19981;&#21516;&#30340;&#33879;&#21517;&#25511;&#21046;&#38382;&#39064;&#30340;&#26368;&#20339;&#22522;&#20934;PM&#36827;&#34892;&#27604;&#36739;&#12290;&#22522;&#20934;PM&#36890;&#36807;&#35775;&#38382;CS&#26412;&#36523;&#30340;&#27169;&#25311;&#23454;&#29616;&#20102;&#23436;&#32654;&#30340;&#20934;&#30830;&#24615;&#12290;&#26681;&#25454;&#25152;&#24471;&#32467;&#26524;&#65292;&#25105;&#20204;&#35748;&#20026;&#25552;&#39640;PM&#23558;&#24635;&#20307;&#19978;&#25913;&#21892;&#25511;&#21046;&#22120;&#65292;&#32780;&#26080;&#38656;&#32771;&#34385;&#20854;&#20182;&#32452;&#20214;&#65288;&#26412;&#25991;&#20013;&#22522;&#20110;&#36827;&#21270;&#20248;&#21270;&#30340;&#21160;&#20316;&#36873;&#25321;&#65289;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article investigates synthetic model-predictive control (MPC) problems to demonstrate that an increased precision of the internal prediction model (PM) automatially entails an improvement of the controller as a whole. In contrast to reinforcement learning (RL), MPC uses the PM to predict subsequent states of the controlled system (CS), instead of directly recommending suitable actions. To assess how the precision of the PM translates into the quality of the model-predictive controller, we compare a DNN-based PM to the optimal baseline PM for three well-known control problems of varying complexity. The baseline PM achieves perfect accuracy by accessing the simulation of the CS itself. Based on the obtained results, we argue that an improvement of the PM will always improve the controller as a whole, without considering the impact of other components such as action selection (which, in this article, relies on evolutionary optimization).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35757;&#32451;&#31574;&#30053;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#19982;&#20004;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#24515;&#33039;&#30913;&#20849;&#25391;&#22270;&#20687;&#20998;&#31867;&#30340;&#20004;&#20010;&#20020;&#24202;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#24615;&#33021;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.15141</link><description>&lt;p&gt;
&#25552;&#39640;&#24515;&#33039;&#30913;&#20849;&#25391;&#22270;&#20687;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Aware Training to Improve Deep Learning Model Calibration for Classification of Cardiac MR Images. (arXiv:2308.15141v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35757;&#32451;&#31574;&#30053;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#19982;&#20004;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#24515;&#33039;&#30913;&#20849;&#25391;&#22270;&#20687;&#20998;&#31867;&#30340;&#20004;&#20010;&#20020;&#24202;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#24615;&#33021;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#34987;&#35748;&#20026;&#26159;&#24320;&#21457;&#26356;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#19968;&#31181;&#26041;&#24335;&#65292;&#36229;&#36234;&#20256;&#32479;&#24615;&#33021;&#25351;&#26631;&#25253;&#21578;&#12290;&#22312;&#32771;&#34385;&#20854;&#22312;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#29615;&#22659;&#20013;&#30340;&#20316;&#29992;&#26102;&#65292;&#20154;&#24037;&#26234;&#33021;&#20998;&#31867;&#27169;&#22411;&#29702;&#24819;&#24773;&#20917;&#19979;&#24212;&#36991;&#20813;&#33258;&#20449;&#30340;&#38169;&#35823;&#39044;&#27979;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#27491;&#30830;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#36825;&#26679;&#20570;&#30340;&#27169;&#22411;&#34987;&#35748;&#20026;&#22312;&#32622;&#20449;&#24230;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#26102;&#65292;&#22914;&#20309;&#25913;&#21892;&#26657;&#20934;&#24615;&#65292;&#21363;&#20351;&#35757;&#32451;&#31574;&#30053;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#65292;&#21364;&#30456;&#23545;&#23569;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#19982;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20020;&#24202;&#24212;&#29992;&#30340;&#24615;&#33021;&#65306;&#24515;&#33039;&#22797;&#21516;&#27493;&#30103;&#27861;&#65288;CRT&#65289;&#21453;&#24212;&#39044;&#27979;&#21644;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#30340;&#24515;&#33039;&#30913;&#20849;&#25391;&#65288;CMR&#65289;&#22270;&#20687;&#35786;&#26029;&#12290;&#22312;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#26159;
&lt;/p&gt;
&lt;p&gt;
Quantifying uncertainty of predictions has been identified as one way to develop more trustworthy artificial intelligence (AI) models beyond conventional reporting of performance metrics. When considering their role in a clinical decision support setting, AI classification models should ideally avoid confident wrong predictions and maximise the confidence of correct predictions. Models that do this are said to be well-calibrated with regard to confidence. However, relatively little attention has been paid to how to improve calibration when training these models, i.e., to make the training strategy uncertainty-aware. In this work we evaluate three novel uncertainty-aware training strategies comparing against two state-of-the-art approaches. We analyse performance on two different clinical applications: cardiac resynchronisation therapy (CRT) response prediction and coronary artery disease (CAD) diagnosis from cardiac magnetic resonance (CMR) images. The best-performing model in terms of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#36136;&#37327;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#22788;&#29702;&#38381;&#38598;&#20998;&#24067;&#36716;&#25442;&#30340;&#31639;&#27861;&#12290;&#35813;&#26694;&#26550;&#20551;&#35774;&#22312;&#35757;&#32451;&#26102;&#26377;&#19968;&#20010;&#21487;&#20449;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#24102;&#26377;&#25968;&#25454;&#38598;&#36716;&#25442;&#21644;&#24369;&#30417;&#30563;&#30340;&#19981;&#21487;&#20449;&#25968;&#25454;&#38598;&#21487;&#29992;&#65292;&#21487;&#20197;&#24212;&#23545;&#20219;&#20309;&#20998;&#24067;&#36716;&#25442;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#21452;&#36136;&#37327;&#23398;&#20064;&#65292;&#20998;&#21035;&#28789;&#24863;&#26469;&#28304;&#20110;&#26631;&#31614;&#22122;&#22768;&#21644;&#21327;&#21464;&#37327;&#36716;&#25442;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15132</link><description>&lt;p&gt;
&#20004;&#36136;&#37327;&#23398;&#20064;&#65306;&#22788;&#29702;&#38381;&#38598;&#20998;&#24067;&#36716;&#25442;&#30340;&#31639;&#27861;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Biquality Learning: a Framework to Design Algorithms Dealing with Closed-Set Distribution Shifts. (arXiv:2308.15132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#36136;&#37327;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35774;&#35745;&#33021;&#22815;&#22788;&#29702;&#38381;&#38598;&#20998;&#24067;&#36716;&#25442;&#30340;&#31639;&#27861;&#12290;&#35813;&#26694;&#26550;&#20551;&#35774;&#22312;&#35757;&#32451;&#26102;&#26377;&#19968;&#20010;&#21487;&#20449;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#24102;&#26377;&#25968;&#25454;&#38598;&#36716;&#25442;&#21644;&#24369;&#30417;&#30563;&#30340;&#19981;&#21487;&#20449;&#25968;&#25454;&#38598;&#21487;&#29992;&#65292;&#21487;&#20197;&#24212;&#23545;&#20219;&#20309;&#20998;&#24067;&#36716;&#25442;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#21452;&#36136;&#37327;&#23398;&#20064;&#65292;&#20998;&#21035;&#28789;&#24863;&#26469;&#28304;&#20110;&#26631;&#31614;&#22122;&#22768;&#21644;&#21327;&#21464;&#37327;&#36716;&#25442;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24369;&#30417;&#30563;&#21644;&#25968;&#25454;&#38598;&#36716;&#25442;&#30340;&#25968;&#25454;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20381;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#35774;&#35745;&#31639;&#27861;&#24182;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#20063;&#19981;&#33021;&#22788;&#29702;&#26368;&#22797;&#26434;&#30340;&#20998;&#24067;&#36716;&#25442;&#12290;&#25105;&#20204;&#35748;&#20026;&#21452;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#26159;&#35774;&#35745;&#36825;&#31181;&#31639;&#27861;&#30340;&#21512;&#36866;&#26694;&#26550;&#12290;&#21452;&#36136;&#37327;&#23398;&#20064;&#20551;&#35774;&#22312;&#35757;&#32451;&#26102;&#26377;&#20004;&#20010;&#25968;&#25454;&#38598;&#21487;&#29992;&#65306;&#19968;&#20010;&#20174;&#24863;&#20852;&#36259;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#21487;&#20449;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#24102;&#26377;&#25968;&#25454;&#38598;&#36716;&#25442;&#21644;&#24369;&#30417;&#30563;&#30340;&#19981;&#21487;&#20449;&#25968;&#25454;&#38598;&#65288;&#21363;&#20998;&#24067;&#36716;&#25442;&#65289;&#12290;&#35757;&#32451;&#26102;&#21487;&#29992;&#30340;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#25968;&#25454;&#38598;&#20351;&#24471;&#35774;&#35745;&#33021;&#22815;&#22788;&#29702;&#20219;&#20309;&#20998;&#24067;&#36716;&#25442;&#30340;&#31639;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#26631;&#31614;&#22122;&#22768;&#25991;&#29486;&#65292;&#21478;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#21327;&#21464;&#37327;&#36716;&#25442;&#25991;&#29486;&#65292;&#29992;&#20110;&#21452;&#36136;&#37327;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#23581;&#35797;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#21512;&#25104;&#27010;&#24565;&#28418;&#31227;&#21644;&#31867;&#26465;&#20214;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning models from data with weak supervision and dataset shifts is still challenging. Designing algorithms when these two situations arise has not been explored much, and existing algorithms cannot always handle the most complex distributional shifts. We think the biquality data setup is a suitable framework for designing such algorithms. Biquality Learning assumes that two datasets are available at training time: a trusted dataset sampled from the distribution of interest and the untrusted dataset with dataset shifts and weaknesses of supervision (aka distribution shifts). The trusted and untrusted datasets available at training time make designing algorithms dealing with any distribution shifts possible. We propose two methods, one inspired by the label noise literature and another by the covariate shift literature for biquality learning. We experiment with two novel methods to synthetically introduce concept drift and class-conditional shifts in real-world datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.15126</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#35780;&#20272;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;LVLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#24187;&#35273;&#25351;&#30340;&#26159;LVLMs&#21709;&#24212;&#20013;&#19981;&#23384;&#22312;&#20110;&#35270;&#35273;&#36755;&#20837;&#20013;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#37325;&#22823;&#21518;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#30446;&#21069;&#23545;LVLMs&#20013;&#30340;&#24187;&#35273;&#35780;&#20272;&#30340;&#30740;&#31350;&#24037;&#20316;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#12290;HaELM&#30340;&#24615;&#33021;&#36817;&#20284;&#20110;ChatGPT&#30340;95%&#65292;&#24182;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#21487;&#22797;&#29616;&#12289;&#20445;&#25252;&#38544;&#31169;&#21644;&#26412;&#22320;&#37096;&#32626;&#31561;&#39069;&#22806;&#20248;&#21183;&#12290;&#21033;&#29992;HaELM&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;LVLMs&#20013;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23548;&#33268;LVLMs&#20013;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#26377;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation halluci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#25311;&#36830;&#32493;&#21160;&#24577;&#26465;&#20214;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15116</link><description>&lt;p&gt;
&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators. (arXiv:2308.15116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Mixup&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#27169;&#25311;&#22120;&#30340;&#39640;&#25928;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27169;&#25311;&#36830;&#32493;&#21160;&#24577;&#26465;&#20214;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#29983;&#29289;&#20998;&#23376;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#20998;&#23376;&#33021;&#22815;&#27874;&#21160;&#30340;&#21508;&#31181;&#26465;&#20214;&#19979;&#23545;&#19968;&#32452;&#31890;&#23376;&#36827;&#34892;&#27169;&#25311;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36719;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#20998;&#23376;&#21160;&#21147;&#23398;&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#36866;&#24212;&#24615;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#38750;&#24120;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#22330;&#26223;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#24037;&#20316;&#20197;&#28201;&#24230;&#20026;&#27979;&#35797;&#26696;&#20363;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#20351;&#20854;&#21487;&#20197;&#36890;&#36807;&#20219;&#20309;&#36830;&#32493;&#30340;&#21160;&#24577;&#26465;&#20214;&#65288;&#22914;&#21387;&#21147;&#21644;&#20307;&#31215;&#65289;&#36827;&#34892;&#26377;&#25928;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#20004;&#20010;&#38454;&#27573;&#65306;1&#65289;&#20351;&#29992;&#25968;&#25454;&#28151;&#21512;&#25216;&#26415;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22686;&#24378;&#20998;&#23376;&#32467;&#26500;&#25968;&#25454;&#21644;&#28201;&#24230;&#25552;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#27604;&#20363;&#30340;&#26041;&#24335;&#24212;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;2&#65289;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#24494;&#35843;&#26694;&#26550;&#25552;&#39640;&#20102;&#24494;&#35843;&#36807;&#31243;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#20026;&#36719;&#25552;&#31034;&#24494;&#35843;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular dynamics simulations have emerged as a fundamental instrument for studying biomolecules. At the same time, it is desirable to perform simulations of a collection of particles under various conditions in which the molecules can fluctuate. In this paper, we explore and adapt the soft prompt-based learning method to molecular dynamics tasks. Our model can remarkably generalize to unseen and out-of-distribution scenarios with limited training data. While our work focuses on temperature as a test case, the versatility of our approach allows for efficient simulation through any continuous dynamic conditions, such as pressure and volumes. Our framework has two stages: 1) Pre-trains with data mixing technique, augments molecular structure data and temperature prompts, then applies a curriculum learning method by increasing the ratio of them smoothly. 2) Meta-learning-based fine-tuning framework improves sample-efficiency of fine-tuning process and gives the soft prompt-tuning better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#20989;&#25968;&#31354;&#38388;&#21644;&#22270;&#21453;&#39304;&#30340;&#38543;&#26426;&#32972;&#26223;&#36172;&#21338;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#27492;&#21069;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#35813;&#31639;&#27861;&#25552;&#20379;&#20102;&#22870;&#21169;&#24046;&#36317;&#20381;&#36182;&#30340;&#19978;&#30028;&#65292;&#24182;&#22312;&#36951;&#25022;&#19978;&#30028;&#26041;&#38754;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15107</link><description>&lt;p&gt;
&#24102;&#26377;&#20391;&#38754;&#35266;&#27979;&#30340;&#38543;&#26426;&#22270;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stochastic Graph Bandit Learning with Side-Observations. (arXiv:2308.15107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#20989;&#25968;&#31354;&#38388;&#21644;&#22270;&#21453;&#39304;&#30340;&#38543;&#26426;&#32972;&#26223;&#36172;&#21338;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#27492;&#21069;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#35813;&#31639;&#27861;&#25552;&#20379;&#20102;&#22870;&#21169;&#24046;&#36317;&#20381;&#36182;&#30340;&#19978;&#30028;&#65292;&#24182;&#22312;&#36951;&#25022;&#19978;&#30028;&#26041;&#38754;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#20989;&#25968;&#31354;&#38388;&#21644;&#22270;&#21453;&#39304;&#30340;&#38543;&#26426;&#32972;&#26223;&#36172;&#21338;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#28508;&#22312;&#30340;&#22270;&#32467;&#26500;&#21644;&#22870;&#21169;&#24046;&#36317;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#22312;&#36825;&#20010;&#38543;&#26426;&#35774;&#32622;&#19979;&#25552;&#20379;&#22870;&#21169;&#24046;&#36317;&#20381;&#36182;&#30340;&#19978;&#30028;&#30340;&#31532;&#19968;&#20010;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;[35]&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#19982;[31,33,35]&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#22270;&#24418;&#25968;&#37327;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36951;&#25022;&#19978;&#30028;&#26041;&#38754;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#25105;&#20204;&#31639;&#27861;&#22312;&#25512;&#36827;&#20855;&#26377;&#22270;&#21453;&#39304;&#30340;&#38543;&#26426;&#32972;&#26223;&#36172;&#21338;&#39046;&#22495;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#21508;&#20010;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#24320;&#36767;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the stochastic contextual bandit with general function space and graph feedback. We propose an algorithm that addresses this problem by adapting to both the underlying graph structures and reward gaps. To the best of our knowledge, our algorithm is the first to provide a gap-dependent upper bound in this stochastic setting, bridging the research gap left by the work in [35]. In comparison to [31,33,35], our method offers improved regret upper bounds and does not require knowledge of graphical quantities. We conduct numerical experiments to demonstrate the computational efficiency and effectiveness of our approach in terms of regret upper bounds. These findings highlight the significance of our algorithm in advancing the field of stochastic contextual bandits with graph feedback, opening up avenues for practical applications in various domains.
&lt;/p&gt;</description></item><item><title>&#33258;&#35299;&#37322;&#30340;GNN&#30340;&#21487;&#20449;&#24230;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#20102;&#27169;&#22411;&#26412;&#36523;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#20960;&#20010;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#21150;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15096</link><description>&lt;p&gt;
&#33258;&#35299;&#37322;GNN&#65288;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#21487;&#20449;&#24230;&#26377;&#22810;&#39640;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Faithful are Self-Explainable GNNs?. (arXiv:2308.15096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15096
&lt;/p&gt;
&lt;p&gt;
&#33258;&#35299;&#37322;&#30340;GNN&#30340;&#21487;&#20449;&#24230;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#20102;&#27169;&#22411;&#26412;&#36523;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#20960;&#20010;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#21150;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35299;&#37322;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#33021;&#22815;&#36755;&#20986;&#27169;&#22411;&#25512;&#29702;&#30340;&#20808;&#39564;&#23616;&#37096;&#35299;&#37322;&#30340;&#26368;&#26032;&#27169;&#22411;&#65292;&#22240;&#27492;&#22312;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#12290;&#33258;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26088;&#22312;&#22312;&#22270;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#23454;&#29616;&#30456;&#21516;&#30340;&#21151;&#33021;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#22312;&#21487;&#20449;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#20854;&#38544;&#24335;&#20445;&#35777;&#65311;&#22312;&#36825;&#20010;&#25193;&#23637;&#25688;&#35201;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#21487;&#20449;&#24230;&#24230;&#37327;&#20998;&#26512;&#20102;&#20960;&#20010;&#33258;&#35299;&#37322;&#30340;GNN&#65292;&#24182;&#30830;&#23450;&#20102;&#20960;&#20010;&#38480;&#21046;-&#26080;&#35770;&#26159;&#22312;&#27169;&#22411;&#26412;&#36523;&#36824;&#26159;&#22312;&#35780;&#20272;&#25351;&#26631;&#19978;-&#24182;&#27010;&#36848;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#21150;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-explainable deep neural networks are a recent class of models that can output ante-hoc local explanations that are faithful to the model's reasoning, and as such represent a step forward toward filling the gap between expressiveness and interpretability. Self-explainable graph neural networks (GNNs) aim at achieving the same in the context of graph data. This begs the question: do these models fulfill their implicit guarantees in terms of faithfulness? In this extended abstract, we analyze the faithfulness of several self-explainable GNNs using different measures of faithfulness, identify several limitations -- both in the models themselves and in the evaluation metrics -- and outline possible ways forward.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#20301;&#25968;&#22238;&#24402;&#26657;&#20934;&#23454;&#29616;&#22522;&#20110;&#32676;&#20307;&#26465;&#20214;&#30340;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#23454;&#38469;&#20892;&#19994;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20316;&#29289;&#19982;&#26434;&#33609;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15094</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#20301;&#25968;&#22238;&#24402;&#26657;&#20934;&#23454;&#29616;&#22522;&#20110;&#32676;&#20307;&#26465;&#20214;&#30340;&#31526;&#21512;&#39044;&#27979;&#65292;&#29992;&#20110;&#20316;&#29289;&#19982;&#26434;&#33609;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Group-Conditional Conformal Prediction via Quantile Regression Calibration for Crop and Weed Classification. (arXiv:2308.15094v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15094
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#20301;&#25968;&#22238;&#24402;&#26657;&#20934;&#23454;&#29616;&#22522;&#20110;&#32676;&#20307;&#26465;&#20214;&#30340;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#23454;&#38469;&#20892;&#19994;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20316;&#29289;&#19982;&#26434;&#33609;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#25104;&#20026;&#31934;&#20934;&#20892;&#19994;&#31995;&#32479;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#19968;&#20010;&#38459;&#30861;&#37319;&#29992;&#36825;&#20123;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#38556;&#30861;&#26159;&#29992;&#25143;&#23545;&#36825;&#20123;&#39640;&#24230;&#22797;&#26434;&#12289;&#19981;&#36879;&#26126;&#21644;&#19981;&#30830;&#23450;&#30340;&#27169;&#22411;&#32570;&#20047;&#20449;&#20219;&#12290;&#23454;&#38469;&#19978;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27809;&#26377;&#20219;&#20309;&#26126;&#30830;&#30340;&#20445;&#35777;&#65292;&#21487;&#20197;&#29992;&#26469;&#35777;&#26126;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#22312;&#39640;&#24230;&#21464;&#21270;&#30340;&#20197;&#21450;&#26080;&#27861;&#25511;&#21046;&#30340;&#29615;&#22659;&#20013;&#65292;&#27604;&#22914;&#20856;&#22411;&#30340;&#20892;&#19994;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#25152;&#38754;&#20020;&#30340;&#29615;&#22659;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20854;&#20182;&#39046;&#22495;&#24320;&#21457;&#30340;&#26576;&#20123;&#26041;&#27861;&#23545;&#20892;&#19994;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#20219;&#20309;&#40657;&#30418;&#23376;&#39044;&#27979;&#26426;&#22120;&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#20379;&#26377;&#25928;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20219;&#20309;&#20551;&#35774;&#65292;&#24212;&#29992;&#20110;&#23454;&#38469;&#26465;&#20214;&#19979;&#30340;&#28145;&#24230;&#21487;&#35270;&#21270;&#26434;&#33609;&#21644;&#20316;&#29289;&#20998;&#31867;&#38382;&#39064;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#26694;&#26550;&#30340;&#23454;&#38469;&#26041;&#38754;&#65292;&#24182;&#29305;&#21035;&#27880;&#24847;&#20102;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning predictive models become an integral part of a large spectrum of precision agricultural systems, a barrier to the adoption of such automated solutions is the lack of user trust in these highly complex, opaque and uncertain models. Indeed, deep neural networks are not equipped with any explicit guarantees that can be used to certify the system's performance, especially in highly varying uncontrolled environments such as the ones typically faced in computer vision for agriculture.Fortunately, certain methods developed in other communities can prove to be important for agricultural applications. This article presents the conformal prediction framework that provides valid statistical guarantees on the predictive performance of any black box prediction machine, with almost no assumptions, applied to the problem of deep visual classification of weeds and crops in real-world conditions. The framework is exposed with a focus on its practical aspects and special attention accor
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#31639;&#27861;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#24341;&#21457;&#20102;&#23433;&#20840;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#35813;&#20027;&#39064;&#30340;&#27010;&#36848;&#65292;&#20851;&#27880;&#23545;&#24212;&#29992;&#21644;&#35745;&#31639;&#25968;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21487;&#33021;&#24863;&#20852;&#36259;&#30340;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2308.15092</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#20381;&#36182;&#20154;&#24037;&#26234;&#33021;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Rely on AI?. (arXiv:2308.15092v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15092
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#31639;&#27861;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#24341;&#21457;&#20102;&#23433;&#20840;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#35813;&#20027;&#39064;&#30340;&#27010;&#36848;&#65292;&#20851;&#27880;&#23545;&#24212;&#29992;&#21644;&#35745;&#31639;&#25968;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21487;&#33021;&#24863;&#20852;&#36259;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#31639;&#27861;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#36825;&#20123;&#31639;&#27861;&#24341;&#21457;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#12290;&#20174;&#23454;&#38469;&#35282;&#24230;&#26469;&#30475;&#65292;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#21457;&#23637;&#20043;&#38388;&#23384;&#22312;&#30528;&#19968;&#31181;&#21319;&#32423;&#30340;&#25112;&#20105;&#12290;&#22312;&#26356;&#21152;&#29702;&#35770;&#30340;&#23618;&#38754;&#19978;&#65292;&#30740;&#31350;&#20154;&#21592;&#20063;&#30740;&#31350;&#20102;&#25915;&#20987;&#30340;&#23384;&#22312;&#24615;&#21644;&#21487;&#35745;&#31639;&#24615;&#31561;&#26356;&#22823;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#31616;&#35201;&#27010;&#36848;&#20102;&#36825;&#20010;&#20027;&#39064;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#23545;&#24212;&#29992;&#21644;&#35745;&#31639;&#25968;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21487;&#33021;&#24863;&#20852;&#36259;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, adversarial attack algorithms have revealed instabilities in deep learning tools. These algorithms raise issues regarding safety, reliability and interpretability in artificial intelligence; especially in high risk settings. From a practical perspective, there has been a war of escalation between those developing attack and defence strategies. At a more theoretical level, researchers have also studied bigger picture questions concerning the existence and computability of attacks. Here we give a brief overview of the topic, focusing on aspects that are likely to be of interest to researchers in applied and computational mathematics.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#31867;Willis&#29615;&#34880;&#31649;&#20998;&#21449;&#30340;&#26041;&#27861;&#65292;&#24110;&#21161;&#31070;&#32463;&#25918;&#23556;&#23398;&#23478;&#21450;&#26102;&#23450;&#20301;&#39640;&#39045;&#20869;&#21160;&#33033;&#30244;&#39118;&#38505;&#30340;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.15088</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;Circle of Willis&#34880;&#31649;&#20998;&#21449;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Using deep learning for an automatic detection and classification of the vascular bifurcations along the Circle of Willis. (arXiv:2308.15088v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15088
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#31867;Willis&#29615;&#34880;&#31649;&#20998;&#21449;&#30340;&#26041;&#27861;&#65292;&#24110;&#21161;&#31070;&#32463;&#25918;&#23556;&#23398;&#23478;&#21450;&#26102;&#23450;&#20301;&#39640;&#39045;&#20869;&#21160;&#33033;&#30244;&#39118;&#38505;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#39045;&#20869;&#21160;&#33033;&#30244;&#21457;&#29983;&#22312;&#34987;&#31216;&#20026;Willis&#29615;&#30340;&#22823;&#33041;&#34880;&#31649;&#26641;&#30340;&#29305;&#23450;&#37096;&#20301;&#19978;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#20204;&#20027;&#35201;&#20986;&#29616;&#22312;&#26500;&#25104;&#36825;&#20010;&#29615;&#24418;&#32467;&#26500;&#30340;&#21313;&#20116;&#20010;&#20027;&#35201;&#21160;&#33033;&#20998;&#21449;&#19978;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#36827;&#34892;&#39640;&#25928;&#21644;&#21450;&#26102;&#30340;&#35786;&#26029;&#65292;&#24320;&#21457;&#19968;&#20123;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#27599;&#20010;&#24863;&#20852;&#36259;&#30340;&#20998;&#21449;&#30340;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#23454;&#38469;&#19978;&#65292;&#33258;&#21160;&#25552;&#21462;&#20855;&#26377;&#36739;&#39640;ICA&#21457;&#29983;&#39118;&#38505;&#30340;&#20998;&#21449;&#23558;&#20026;&#31070;&#32463;&#25918;&#23556;&#23398;&#23478;&#25552;&#20379;&#23545;&#26368;&#20196;&#20154;&#25285;&#24551;&#30340;&#21306;&#22495;&#30340;&#24555;&#36895;&#27010;&#36848;&#12290;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#36817;&#21162;&#21147;&#65292;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#35768;&#22810;&#27169;&#24335;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#21508;&#31181;&#26041;&#27861;&#34987;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#36825;&#39033;&#30740;&#31350;&#24847;&#22312;&#24110;&#21161;&#31070;&#32463;&#25918;&#23556;&#23398;&#23478;&#24555;&#36895;&#23450;&#20301;&#20219;&#20309;&#20855;&#26377;&#39640;ICA&#39118;&#38505;&#30340;&#20998;&#21449;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the intracranial aneurysms (ICA) occur on a specific portion of the cerebral vascular tree named the Circle of Willis (CoW). More particularly, they mainly arise onto fifteen of the major arterial bifurcations constituting this circular structure. Hence, for an efficient and timely diagnosis it is critical to develop some methods being able to accurately recognize each Bifurcation of Interest (BoI). Indeed, an automatic extraction of the bifurcations presenting the higher risk of developing an ICA would offer the neuroradiologists a quick glance at the most alarming areas. Due to the recent efforts on Artificial Intelligence, Deep Learning turned out to be the best performing technology for many pattern recognition tasks. Moreover, various methods have been particularly designed for medical image analysis purposes. This study intends to assist the neuroradiologists to promptly locate any bifurcation presenting a high risk of ICA occurrence. It can be seen as a Computer Aided Di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PED&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21463;&#21147;&#39537;&#21160;&#30340;&#29289;&#29702;&#27169;&#22411;&#20013;&#25429;&#25417;&#21160;&#24577;&#34920;&#31034;&#30340;&#36816;&#21160;&#26469;&#38477;&#20302;&#28508;&#22312;&#33021;&#37327;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#36873;&#25321;&#26102;&#30340;&#25361;&#25112;&#65292;&#24182;&#33719;&#24471;&#26356;&#24378;&#22823;&#21644;&#26356;&#31283;&#23450;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15074</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#33021;&#37327;&#25506;&#32034;&#27169;&#22411;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Model Transferability through the Lens of Potential Energy. (arXiv:2308.15074v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PED&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21463;&#21147;&#39537;&#21160;&#30340;&#29289;&#29702;&#27169;&#22411;&#20013;&#25429;&#25417;&#21160;&#24577;&#34920;&#31034;&#30340;&#36816;&#21160;&#26469;&#38477;&#20302;&#28508;&#22312;&#33021;&#37327;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#36873;&#25321;&#26102;&#30340;&#25361;&#25112;&#65292;&#24182;&#33719;&#24471;&#26356;&#24378;&#22823;&#21644;&#26356;&#31283;&#23450;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#37327;&#21487;&#29992;&#24615;&#65292;&#36801;&#31227;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20174;&#22810;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#34913;&#37327;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#36801;&#31227;&#24615;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#32534;&#30721;&#30340;&#38745;&#24577;&#29305;&#24449;&#21644;&#20219;&#21153;&#26631;&#31614;&#20043;&#38388;&#30340;&#32479;&#35745;&#30456;&#20851;&#24615;&#65292;&#20294;&#23427;&#20204;&#24573;&#35270;&#20102;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24213;&#23618;&#34920;&#31034;&#21160;&#24577;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#32467;&#26524;&#19981;&#21487;&#38752;&#65292;&#23588;&#20854;&#23545;&#20110;&#33258;&#30417;&#30563;&#27169;&#22411;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PED&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#28508;&#22312;&#33021;&#37327;&#30340;&#35270;&#35282;&#37325;&#26032;&#26500;&#31569;&#20102;&#27169;&#22411;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#24182;&#30452;&#25509;&#24314;&#27169;&#24433;&#21709;&#24494;&#35843;&#21160;&#24577;&#30340;&#30456;&#20114;&#20316;&#29992;&#21147;&#12290;&#36890;&#36807;&#22312;&#21463;&#21147;&#39537;&#21160;&#30340;&#29289;&#29702;&#27169;&#22411;&#20013;&#25429;&#25417;&#21160;&#24577;&#34920;&#31034;&#30340;&#36816;&#21160;&#26469;&#38477;&#20302;&#28508;&#22312;&#33021;&#37327;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#26356;&#24378;&#22823;&#21644;&#26356;&#31283;&#23450;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has become crucial in computer vision tasks due to the vast availability of pre-trained deep learning models. However, selecting the optimal pre-trained model from a diverse pool for a specific downstream task remains a challenge. Existing methods for measuring the transferability of pre-trained models rely on statistical correlations between encoded static features and task labels, but they overlook the impact of underlying representation dynamics during fine-tuning, leading to unreliable results, especially for self-supervised models. In this paper, we present an insightful physics-inspired approach named PED to address these challenges. We reframe the challenge of model selection through the lens of potential energy and directly model the interaction forces that influence fine-tuning dynamics. By capturing the motion of dynamic representations to decline the potential energy within a force-driven physical model, we can acquire an enhanced and more stable observatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;logit&#26356;&#26032;&#30340;&#26032;&#21407;&#21017;(ALU)&#65292;&#29992;&#20110;&#25512;&#26029;&#23545;&#25239;&#26679;&#26412;&#30340;&#26631;&#31614;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#30340;logit&#24046;&#24322;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15072</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;logit&#26356;&#26032;&#25512;&#36827;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Advancing Adversarial Robustness Through Adversarial Logit Update. (arXiv:2308.15072v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;logit&#26356;&#26032;&#30340;&#26032;&#21407;&#21017;(ALU)&#65292;&#29992;&#20110;&#25512;&#26029;&#23545;&#25239;&#26679;&#26412;&#30340;&#26631;&#31614;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#30340;logit&#24046;&#24322;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#23545;&#25239;&#35757;&#32451;&#21644;&#23545;&#25239;&#20928;&#21270;&#26159;&#24191;&#27867;&#35748;&#21487;&#30340;&#38450;&#24481;&#31574;&#30053;&#20043;&#19968;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26377;&#19981;&#21516;&#30340;&#22522;&#26412;&#36923;&#36753;&#65292;&#20294;&#37117;&#20381;&#36182;&#20110;&#32477;&#23545;logit&#20540;&#29983;&#25104;&#26631;&#31614;&#39044;&#27979;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20174;&#29702;&#35770;&#35282;&#24230;&#23545;&#25104;&#21151;&#30340;&#23545;&#25239;&#25915;&#20987;&#21608;&#22260;&#30340;logit&#24046;&#24322;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21407;&#21017;&#65292;&#21363;&#23545;&#25239;logit&#26356;&#26032;(ALU)&#65292;&#29992;&#20110;&#25512;&#26029;&#23545;&#25239;&#26679;&#26412;&#30340;&#26631;&#31614;&#12290;&#22522;&#20110;ALU&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#33539;&#24335;&#65292;&#21033;&#29992;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#30340;logit&#24046;&#24322;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#28165;&#27905;&#25968;&#25454;&#21512;&#25104;&#27169;&#22411;&#19981;&#38656;&#35201;&#23545;&#25239;&#25110;&#39069;&#22806;&#30340;&#25968;&#25454;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;&#21644;&#22522;&#20110;ALU&#30340;&#25968;&#25454;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks are susceptible to adversarial perturbations. Adversarial training and adversarial purification are among the most widely recognized defense strategies. Although these methods have different underlying logic, both rely on absolute logit values to generate label predictions. In this study, we theoretically analyze the logit difference around successful adversarial attacks from a theoretical point of view and propose a new principle, namely Adversarial Logit Update (ALU), to infer adversarial sample's labels. Based on ALU, we introduce a new classification paradigm that utilizes pre- and post-purification logit differences for model's adversarial robustness boost. Without requiring adversarial or additional data for model training, our clean data synthesis model can be easily applied to various pre-trained models for both adversarial sample detection and ALU-based data classification. Extensive experiments on both CIFAR-10, CIFAR-100, and tiny-ImageNet datasets show 
&lt;/p&gt;</description></item><item><title>MadSGM&#26159;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#32771;&#34385;&#20102;&#37325;&#26500;&#12289;&#23494;&#24230;&#21644;&#26799;&#24230;&#31561;&#20840;&#38754;&#30340;&#24322;&#24120;&#27979;&#37327;&#22240;&#32032;&#12290;&#23454;&#39564;&#35777;&#26126;MadSGM&#20855;&#26377;&#26368;&#24378;&#22823;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15069</link><description>&lt;p&gt;
MadSGM&#65306;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#21464;&#37327;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MadSGM: Multivariate Anomaly Detection with Score-based Generative Models. (arXiv:2308.15069v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15069
&lt;/p&gt;
&lt;p&gt;
MadSGM&#26159;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#32771;&#34385;&#20102;&#37325;&#26500;&#12289;&#23494;&#24230;&#21644;&#26799;&#24230;&#31561;&#20840;&#38754;&#30340;&#24322;&#24120;&#27979;&#37327;&#22240;&#32032;&#12290;&#23454;&#39564;&#35777;&#26126;MadSGM&#20855;&#26377;&#26368;&#24378;&#22823;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#20013;&#26368;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#19982;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21644;&#20998;&#31867;&#19981;&#21516;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#36890;&#24120;&#38656;&#35201;&#26080;&#30417;&#30563;&#65288;&#25110;&#33258;&#30417;&#30563;&#65289;&#35757;&#32451;&#65292;&#22240;&#20026;&#25910;&#38598;&#21644;&#26631;&#35760;&#24322;&#24120;&#35266;&#27979;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#26377;&#38480;&#24418;&#24335;&#30340;&#24322;&#24120;&#27979;&#37327;&#65292;&#22240;&#27492;&#19981;&#28165;&#26970;&#23427;&#20204;&#26159;&#21542;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#37117;&#26159;&#26368;&#20248;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#31216;&#20026;MadSGM&#65292;&#23427;&#32771;&#34385;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#24191;&#27867;&#30340;&#24322;&#24120;&#27979;&#37327;&#22240;&#32032;&#65306;i&#65289;&#22522;&#20110;&#37325;&#26500;&#30340;&#12289;ii&#65289;&#22522;&#20110;&#23494;&#24230;&#30340;&#21644;iii&#65289;&#22522;&#20110;&#26799;&#24230;&#30340;&#24322;&#24120;&#27979;&#37327;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26465;&#20214;&#35780;&#20998;&#32593;&#32476;&#21450;&#20854;&#21435;&#22122;&#35780;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#23545;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MadSGM&#23454;&#29616;&#20102;&#26368;&#24378;&#22823;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The time-series anomaly detection is one of the most fundamental tasks for time-series. Unlike the time-series forecasting and classification, the time-series anomaly detection typically requires unsupervised (or self-supervised) training since collecting and labeling anomalous observations are difficult. In addition, most existing methods resort to limited forms of anomaly measurements and therefore, it is not clear whether they are optimal in all circumstances. To this end, we present a multivariate time-series anomaly detector based on score-based generative models, called MadSGM, which considers the broadest ever set of anomaly measurement factors: i) reconstruction-based, ii) density-based, and iii) gradient-based anomaly measurements. We also design a conditional score network and its denoising score matching loss for the time-series anomaly detection. Experiments on five real-world benchmark datasets illustrate that MadSGM achieves the most robust and accurate predictions.
&lt;/p&gt;</description></item><item><title>OEBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20851;&#31995;&#25968;&#25454;&#27969;&#20013;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;&#30340;&#24320;&#25918;&#29615;&#22659;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#36825;&#31181;&#25361;&#25112;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2308.15059</link><description>&lt;p&gt;
OEBench: &#30740;&#31350;&#29616;&#23454;&#19990;&#30028;&#20013;&#20851;&#31995;&#25968;&#25454;&#27969;&#20013;&#30340;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
OEBench: Investigating Open Environment Challenges in Real-World Relational Data Streams. (arXiv:2308.15059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15059
&lt;/p&gt;
&lt;p&gt;
OEBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20851;&#31995;&#25968;&#25454;&#27969;&#20013;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;&#30340;&#24320;&#25918;&#29615;&#22659;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#36825;&#31181;&#25361;&#25112;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25968;&#25454;&#38598;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#24182;&#19988;&#36890;&#24120;&#20197;&#25968;&#25454;&#27969;&#30340;&#26041;&#24335;&#20256;&#36882;&#12290;&#36825;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#29305;&#27530;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#20998;&#24067;&#28418;&#31227;&#12289;&#24322;&#24120;&#20540;&#12289;&#26032;&#20852;&#31867;&#21035;&#21644;&#29305;&#24449;&#21464;&#21270;&#65292;&#26368;&#36817;&#23558;&#36825;&#20123;&#25361;&#25112;&#25551;&#36848;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#20851;&#20110;&#25968;&#25454;&#27969;&#30340;&#22686;&#37327;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#20294;&#20854;&#35780;&#20272;&#20027;&#35201;&#26159;&#22522;&#20110;&#25163;&#21160;&#20998;&#21106;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#26377;&#20960;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#27969;&#25968;&#25454;&#38598;&#21487;&#29992;&#65292;&#20294;&#36825;&#20123;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;&#26159;&#21542;&#26222;&#36941;&#23384;&#22312;&#20197;&#21450;&#29616;&#26377;&#30340;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#23578;&#19981;&#30830;&#23450;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OEBench&#30340;&#24320;&#25918;&#29615;&#22659;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#20851;&#31995;&#25968;&#25454;&#27969;&#20013;&#30340;&#24320;&#25918;&#29615;&#22659;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;55&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#27969;&#25968;&#25454;&#38598;&#65292;&#24182;&#30830;&#31435;&#20102;&#24320;&#25918;&#29615;&#22659;&#22330;&#26223;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#65292;&#36825;&#22312;&#24403;&#21069;&#30340;&#30740;&#31350;&#20013;&#36824;&#27809;&#26377;&#34987;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational datasets are widespread in real-world scenarios and are usually delivered in a streaming fashion. This type of data stream can present unique challenges, such as distribution drifts, outliers, emerging classes, and changing features, which have recently been described as open environment challenges for machine learning. While some work has been done on incremental learning for data streams, their evaluations are mostly conducted with manually partitioned datasets. Moreover, while several real-world streaming datasets are available, it is uncertain whether these open environment challenges are prevalent and how existing incremental learning algorithms perform on real datasets. To fill this gap, we develop an Open Environment Benchmark named OEBench to evaluate open environment challenges in relational data streams. Specifically, we investigate 55 real-world streaming datasets and establish that open environment scenarios are indeed widespread in real-world datasets, which pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24418;&#24577;&#20449;&#24687;&#30340;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24418;&#24577;&#35789;&#20041;&#26631;&#27880;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290; &#23613;&#31649;&#22312;&#21333;&#26631;&#31614;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#19981;&#22914;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#65292;&#20294;&#22312;&#21069;n&#20010;&#39044;&#27979;&#26631;&#31614;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290; &#36825;&#20010;&#26041;&#27861;&#22312;&#20154;&#26426;&#21327;&#20316;&#26631;&#27880;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15055</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24418;&#24577;&#35789;&#20041;&#26631;&#27880;&#20013;&#30340;&#20998;&#31867;&#20007;&#22833;
&lt;/p&gt;
&lt;p&gt;
Taxonomic Loss for Morphological Glossing of Low-Resource Languages. (arXiv:2308.15055v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24418;&#24577;&#20449;&#24687;&#30340;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24418;&#24577;&#35789;&#20041;&#26631;&#27880;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290; &#23613;&#31649;&#22312;&#21333;&#26631;&#31614;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#19981;&#22914;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#65292;&#20294;&#22312;&#21069;n&#20010;&#39044;&#27979;&#26631;&#31614;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290; &#36825;&#20010;&#26041;&#27861;&#22312;&#20154;&#26426;&#21327;&#20316;&#26631;&#27880;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#35789;&#20041;&#26631;&#27880;&#26159;&#33258;&#21160;&#35821;&#35328;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#20854;&#20182;&#19979;&#28216;&#24212;&#29992;&#30340;&#25928;&#26524;&#12290;&#23613;&#31649;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26631;&#27880;&#31995;&#32479;&#22312;&#25968;&#25454;&#20016;&#23500;&#30340;&#35821;&#35328;&#19978;&#34920;&#29616;&#38750;&#24120;&#22909;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#21019;&#24314;&#26377;&#29992;&#30340;&#27169;&#22411;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24418;&#24577;&#20449;&#24687;&#30340;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#24418;&#24577;&#35789;&#20041;&#26631;&#27880;&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#20351;&#29992;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#22312;&#21333;&#26631;&#31614;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#22914;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#65292;&#20294;&#22312;&#32771;&#34385;&#21069;n&#20010;&#39044;&#27979;&#26631;&#31614;&#26102;&#65292;&#23427;&#20135;&#29983;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#24615;&#36136;&#20351;&#24471;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#22312;&#20154;&#26426;&#21327;&#20316;&#26631;&#27880;&#30340;&#29615;&#22659;&#20013;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Morpheme glossing is a critical task in automated language documentation and can benefit other downstream applications greatly. While state-of-the-art glossing systems perform very well for languages with large amounts of existing data, it is more difficult to create useful models for low-resource languages. In this paper, we propose the use of a taxonomic loss function that exploits morphological information to make morphological glossing more performant when data is scarce. We find that while the use of this loss function does not outperform a standard loss function with regards to single-label prediction accuracy, it produces better predictions when considering the top-n predicted labels. We suggest this property makes the taxonomic loss function useful in a human-in-the-loop annotation setting.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#24179;&#34913;&#24863;&#30693;&#24335;&#25151;&#38388;&#24067;&#23616;&#20272;&#35745;&#65288;iBARLE&#65289;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#22806;&#35266;&#21464;&#21270;&#29983;&#25104;&#27169;&#22359;&#12289;&#22797;&#26434;&#32467;&#26500;&#28151;&#21512;&#27169;&#22359;&#21644;&#26799;&#24230;-based&#24067;&#23616;&#30446;&#26631;&#20989;&#25968;&#65292;&#26088;&#22312;&#35299;&#20915;&#25151;&#38388;&#24067;&#23616;&#20272;&#35745;&#20013;&#30340;&#19981;&#24179;&#34913;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15050</link><description>&lt;p&gt;
iBARLE&#65306;&#24179;&#34913;&#24863;&#30693;&#24335;&#25151;&#38388;&#24067;&#23616;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
iBARLE: imBalance-Aware Room Layout Estimation. (arXiv:2308.15050v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15050
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#24179;&#34913;&#24863;&#30693;&#24335;&#25151;&#38388;&#24067;&#23616;&#20272;&#35745;&#65288;iBARLE&#65289;&#26694;&#26550;&#65292;&#23427;&#21253;&#25324;&#22806;&#35266;&#21464;&#21270;&#29983;&#25104;&#27169;&#22359;&#12289;&#22797;&#26434;&#32467;&#26500;&#28151;&#21512;&#27169;&#22359;&#21644;&#26799;&#24230;-based&#24067;&#23616;&#30446;&#26631;&#20989;&#25968;&#65292;&#26088;&#22312;&#35299;&#20915;&#25151;&#38388;&#24067;&#23616;&#20272;&#35745;&#20013;&#30340;&#19981;&#24179;&#34913;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25151;&#38388;&#24067;&#23616;&#20272;&#35745;&#26159;&#20174;&#21333;&#20010;&#20840;&#26223;&#22270;&#39044;&#27979;&#24067;&#23616;&#12290;&#23427;&#38656;&#35201;&#20855;&#26377;&#22823;&#35268;&#27169;&#21644;&#22810;&#26679;&#21270;&#30340;&#25151;&#38388;&#24418;&#29366;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#24179;&#34913;&#65292;&#21253;&#25324;&#24067;&#23616;&#22797;&#26434;&#24230;&#30340;&#23610;&#23544;&#12289;&#30456;&#26426;&#20301;&#32622;&#21644;&#22330;&#26223;&#22806;&#35266;&#30340;&#21464;&#21270;&#12290;&#36825;&#20123;&#38382;&#39064;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24179;&#34913;&#24863;&#30693;&#24335;&#25151;&#38388;&#24067;&#23616;&#20272;&#35745;&#65288;iBARLE&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;iBARLE&#21253;&#25324;&#65288;1&#65289;&#22806;&#35266;&#21464;&#21270;&#29983;&#25104;&#65288;AVG&#65289;&#27169;&#22359;&#65292;&#20419;&#36827;&#35270;&#35273;&#22806;&#35266;&#39046;&#22495;&#30340;&#27867;&#21270;&#65292;&#65288;2&#65289;&#22797;&#26434;&#32467;&#26500;&#28151;&#21512;&#65288;CSMix&#65289;&#27169;&#22359;&#65292;&#25552;&#39640;&#23545;&#25151;&#38388;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21644;&#65288;3&#65289;&#22522;&#20110;&#26799;&#24230;&#30340;&#24067;&#23616;&#30446;&#26631;&#20989;&#25968;&#65292;&#26356;&#26377;&#25928;&#22320;&#32771;&#34385;&#22797;&#26434;&#24067;&#23616;&#20013;&#30340;&#36974;&#25377;&#12290;&#25152;&#26377;&#27169;&#22359;&#37117;&#26159;&#32852;&#21512;&#35757;&#32451;&#30340;&#65292;&#24444;&#27492;&#24110;&#21161;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#22522;&#20110;ZInD~\cite{cruz2021zillow}&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Room layout estimation predicts layouts from a single panorama. It requires datasets with large-scale and diverse room shapes to train the models. However, there are significant imbalances in real-world datasets including the dimensions of layout complexity, camera locations, and variation in scene appearance. These issues considerably influence the model training performance. In this work, we propose the imBalance-Aware Room Layout Estimation (iBARLE) framework to address these issues. iBARLE consists of (1) Appearance Variation Generation (AVG) module, which promotes visual appearance domain generalization, (2) Complex Structure Mix-up (CSMix) module, which enhances generalizability w.r.t. room structure, and (3) a gradient-based layout objective function, which allows more effective accounting for occlusions in complex layouts. All modules are jointly trained and help each other to achieve the best performance. Experiments and ablation studies based on ZInD~\cite{cruz2021zillow} dat
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#20197;&#31867;&#20284;&#20110;&#30693;&#35782;&#24211;&#30340;&#26041;&#24335;&#32452;&#32455;&#27010;&#24565;&#65292;&#36825;&#34920;&#26126;&#23427;&#20204;&#20855;&#22791;&#20154;&#31867;&#25512;&#29702;&#35821;&#20041;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15047</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#24565;&#32452;&#32455;&#19978;&#36235;&#21521;&#20154;&#31867;&#30340;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Large language models converge toward human-like concept organization. (arXiv:2308.15047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15047
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#20197;&#31867;&#20284;&#20110;&#30693;&#35782;&#24211;&#30340;&#26041;&#24335;&#32452;&#32455;&#27010;&#24565;&#65292;&#36825;&#34920;&#26126;&#23427;&#20204;&#20855;&#22791;&#20154;&#31867;&#25512;&#29702;&#35821;&#20041;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#25552;&#21462;&#12289;&#25512;&#29702;&#21644;&#23545;&#35805;&#26041;&#38754;&#23637;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#20294;&#36825;&#31181;&#34920;&#29616;&#26159;&#30001;&#20110;&#35760;&#24518;&#21644;&#27169;&#24335;&#21305;&#37197;&#65292;&#36824;&#26159;&#21453;&#26144;&#20102;&#31867;&#20284;&#20110;&#20154;&#31867;&#25512;&#29702;&#35821;&#20041;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#34920;&#29616;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#12290;&#30693;&#35782;&#24211;&#65288;&#22914;WikiData&#65289;&#25552;&#20379;&#20102;&#25512;&#29702;&#35821;&#20041;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#20197;&#19982;&#36825;&#20123;&#30693;&#35782;&#24211;&#20013;&#27010;&#24565;&#30340;&#32452;&#32455;&#26041;&#24335;&#24778;&#20154;&#30456;&#20284;&#30340;&#26041;&#24335;&#32452;&#32455;&#27010;&#24565;&#12290;&#30693;&#35782;&#24211;&#27169;&#25311;&#20102;&#38598;&#20307;&#12289;&#26426;&#26500;&#21270;&#30340;&#30693;&#35782;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20284;&#20046;&#20174;&#21407;&#22987;&#25991;&#26412;&#20013;&#20135;&#29983;&#20102;&#36825;&#26679;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#22823;&#26356;&#22909;&#30340;&#27169;&#22411;&#22312;&#27010;&#24565;&#32452;&#32455;&#19978;&#34920;&#29616;&#20986;&#26356;&#21152;&#20154;&#31867;&#21270;&#30340;&#29305;&#28857;&#65292;&#28085;&#30422;&#20102;&#22235;&#20010;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#21644;&#19977;&#20010;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models show human-like performance in knowledge extraction, reasoning and dialogue, but it remains controversial whether this performance is best explained by memorization and pattern matching, or whether it reflects human-like inferential semantics and world knowledge. Knowledge bases such as WikiData provide large-scale, high-quality representations of inferential semantics and world knowledge. We show that large language models learn to organize concepts in ways that are strikingly similar to how concepts are organized in such knowledge bases. Knowledge bases model collective, institutional knowledge, and large language models seem to induce such knowledge from raw text. We show that bigger and better models exhibit more human-like concept organization, across four families of language models and three knowledge graph embeddings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPU&#30340;&#22823;&#35268;&#27169;&#24182;&#34892;&#36830;&#32493;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#26469;&#21152;&#36895;&#28151;&#21512;SAT&#27714;&#35299;&#22120;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#26032;&#22411;&#24182;&#34892;&#31639;&#27861;&#26469;&#35745;&#31639;&#22522;&#26412;&#23545;&#31216;&#22810;&#39033;&#24335;&#65292;&#24182;&#22312;&#25628;&#32034;&#20013;&#20351;&#29992;&#37325;&#21551;&#21551;&#21457;&#24335;&#31639;&#27861;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.15020</link><description>&lt;p&gt;
&#22522;&#20110;GPU&#30340;&#28151;&#21512;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#27714;&#35299;&#22120;&#30340;&#22823;&#35268;&#27169;&#24182;&#34892;&#36830;&#32493;&#23616;&#37096;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Massively Parallel Continuous Local Search for Hybrid SAT Solving on GPUs. (arXiv:2308.15020v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15020
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPU&#30340;&#22823;&#35268;&#27169;&#24182;&#34892;&#36830;&#32493;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#26469;&#21152;&#36895;&#28151;&#21512;SAT&#27714;&#35299;&#22120;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#26032;&#22411;&#24182;&#34892;&#31639;&#27861;&#26469;&#35745;&#31639;&#22522;&#26412;&#23545;&#31216;&#22810;&#39033;&#24335;&#65292;&#24182;&#22312;&#25628;&#32034;&#20013;&#20351;&#29992;&#37325;&#21551;&#21551;&#21457;&#24335;&#31639;&#27861;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22522;&#20110;&#20914;&#31361;&#39537;&#21160;&#23376;&#21477;&#23398;&#20064;&#65288;CDCL&#65289;&#30340;&#26368;&#20808;&#36827;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65288;SAT&#65289;&#27714;&#35299;&#22120;&#22312;&#24037;&#31243;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#20854;&#39034;&#24207;&#24615;&#36136;&#38480;&#21046;&#20102;&#22312;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#65288;GPU&#65289;&#31561;&#24179;&#21488;&#19978;&#36827;&#34892;&#21152;&#36895;&#30340;&#24182;&#34892;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FastFourierSAT&#65292;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#39537;&#21160;&#36830;&#32493;&#23616;&#37096;&#25628;&#32034;&#65288;CLS&#65289;&#30340;&#39640;&#24230;&#24182;&#34892;&#28151;&#21512;SAT&#27714;&#35299;&#22120;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#21463;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#21367;&#31215;&#21551;&#21457;&#30340;&#26032;&#22411;&#24182;&#34892;&#31639;&#27861;&#26469;&#23454;&#29616;&#30340;&#65292;&#29992;&#20110;&#35745;&#31639;&#20197;&#21069;&#30340;CLS&#26041;&#27861;&#20013;&#30340;&#20027;&#35201;&#35745;&#31639;&#20219;&#21153;&#8212;&#8212;&#22522;&#26412;&#23545;&#31216;&#22810;&#39033;&#24335;&#65288;ESP&#65289;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#19982;&#26368;&#20339;&#30340;&#20197;&#21069;&#32467;&#26524;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31639;&#27861;&#22266;&#26377;&#30340;&#22823;&#35268;&#27169;&#24182;&#34892;&#24615;&#33021;&#22815;&#21033;&#29992;GPU&#36827;&#34892;&#21152;&#36895;&#65292;&#30456;&#27604;&#20197;&#21069;&#30340;CLS&#26041;&#27861;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23558;&#37325;&#21551;&#21551;&#21457;&#24335;&#31639;&#27861;&#34701;&#20837;CLS&#20197;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#24212;&#29992;&#19982;&#20854;&#20182;SAT&#27714;&#35299;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although state-of-the-art (SOTA) SAT solvers based on conflict-driven clause learning (CDCL) have achieved remarkable engineering success, their sequential nature limits the parallelism that may be extracted for acceleration on platforms such as the graphics processing unit (GPU). In this work, we propose FastFourierSAT, a highly parallel hybrid SAT solver based on gradient-driven continuous local search (CLS). This is realized by a novel parallel algorithm inspired by the Fast Fourier Transform (FFT)-based convolution for computing the elementary symmetric polynomials (ESPs), which is the major computational task in previous CLS methods. The complexity of our algorithm matches the best previous result. Furthermore, the substantial parallelism inherent in our algorithm can leverage the GPU for acceleration, demonstrating significant improvement over the previous CLS approaches. We also propose to incorporate the restart heuristics in CLS to improve search efficiency. We compare our app
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#23433;&#20840;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#20855;&#26377;&#20984;&#32422;&#26463;&#30340;&#24773;&#20917;&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#21508;&#31181;&#38543;&#26426;&#37319;&#26679;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15006</link><description>&lt;p&gt;
&#21033;&#29992;&#38382;&#39064;&#20960;&#20309;&#29305;&#24449;&#30340;&#23433;&#20840;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Problem Geometry in Safe Linear Bandits. (arXiv:2308.15006v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15006
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#23433;&#20840;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#20855;&#26377;&#20984;&#32422;&#26463;&#30340;&#24773;&#20917;&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#21508;&#31181;&#38543;&#26426;&#37319;&#26679;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#26159;&#32463;&#20856;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#19968;&#20010;&#29256;&#26412;&#65292;&#20854;&#20013;&#23398;&#20064;&#22120;&#30340;&#34892;&#21160;&#24517;&#39035;&#22312;&#25152;&#26377;&#22238;&#21512;&#28385;&#36275;&#19968;&#20010;&#19981;&#30830;&#23450;&#30340;&#32447;&#24615;&#32422;&#26463;&#12290;&#30001;&#20110;&#20854;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#36817;&#24180;&#26469;&#36825;&#20010;&#38382;&#39064;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#21033;&#29992;&#29305;&#23450;&#38382;&#39064;&#35774;&#32622;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#21487;&#20197;&#20026;&#30456;&#20114;&#20998;&#31163;&#30340;&#38382;&#39064;&#23454;&#20363;&#21644;&#26377;&#38480;&#26143;&#20984;&#38598;&#30340;&#34892;&#21160;&#38598;&#25552;&#20379;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#38382;&#39064;&#21442;&#25968;&#65292;&#24182;&#20855;&#26377;&#33267;&#23569;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#24403;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23433;&#20840;&#32447;&#24615;&#36172;&#21338;&#26426;&#35774;&#32622;&#30340;&#25512;&#24191;&#65292;&#20854;&#20013;&#32422;&#26463;&#26159;&#20984;&#30340;&#65292;&#24182;&#21033;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#20984;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#26469;&#35843;&#25972;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#12290;&#36890;&#36807;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#38543;&#26426;&#37319;&#26679;&#30340;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The safe linear bandit problem is a version of the classic linear bandit problem where the learner's actions must satisfy an uncertain linear constraint at all rounds. Due its applicability to many real-world settings, this problem has received considerable attention in recent years. We find that by exploiting the geometry of the specific problem setting, we can achieve improved regret guarantees for both well-separated problem instances and action sets that are finite star convex sets. Additionally, we propose a novel algorithm for this setting that chooses problem parameters adaptively and enjoys at least as good regret guarantees as existing algorithms. Lastly, we introduce a generalization of the safe linear bandit setting where the constraints are convex and adapt our algorithms and analyses to this setting by leveraging a novel convex-analysis based approach. Simulation results show improved performance over existing algorithms for a variety of randomly sampled settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#25277;&#26679;&#21644;&#22122;&#22768;&#28155;&#21152;&#30340;&#39118;&#26684;&#22686;&#24378;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#39118;&#26684;&#36716;&#31227;&#20013;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#22312;STL-10&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.14995</link><description>&lt;p&gt;
WSAM: &#20174;&#39118;&#26684;&#22686;&#24378;&#30340;&#21487;&#35299;&#37322;&#24615;&#20316;&#20026;&#23545;&#25239;&#25915;&#20987;&#32773;&#21450;&#20854;&#23545;&#22270;&#20687;&#20998;&#31867;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
WSAM: Visual Explanations from Style Augmentation as Adversarial Attacker and Their Influence in Image Classification. (arXiv:2308.14995v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#25277;&#26679;&#21644;&#22122;&#22768;&#28155;&#21152;&#30340;&#39118;&#26684;&#22686;&#24378;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#39118;&#26684;&#36716;&#31227;&#20013;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#22312;STL-10&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#30001;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23545;&#32441;&#29702;&#32780;&#19981;&#26159;&#24418;&#29366;&#30340;&#24378;&#28872;&#20559;&#22909;&#65292;&#39118;&#26684;&#22686;&#24378;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#39118;&#26684;&#21270;&#26041;&#27861;&#35201;&#20040;&#25191;&#34892;&#20302;&#20445;&#30495;&#24230;&#30340;&#39118;&#26684;&#36716;&#31227;&#65292;&#35201;&#20040;&#22312;&#23884;&#20837;&#21521;&#37327;&#20013;&#36827;&#34892;&#24369;&#39118;&#26684;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#25277;&#26679;&#21644;&#22122;&#22768;&#28155;&#21152;&#30340;&#39118;&#26684;&#22686;&#24378;&#31639;&#27861;&#65292;&#20197;&#25913;&#21892;&#23545;&#39118;&#26684;&#36716;&#31227;&#30340;&#19968;&#33324;&#32447;&#24615;&#21464;&#25442;&#30340;&#38543;&#26426;&#21270;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#25152;&#26377;&#27169;&#22411;&#19981;&#20165;&#20855;&#26377;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#30340;&#23545;&#22270;&#20687;&#39118;&#26684;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#22312;STL-10&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#20808;&#21069;&#30340;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#19981;&#21516;&#39118;&#26684;&#21464;&#21270;&#19979;&#30340;&#27169;&#22411;&#35299;&#37322;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22312;&#35757;&#32451;&#29615;&#22659;&#20013;&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#26102;&#30340;&#24615;&#33021;&#30340;&#20840;&#38754;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, style augmentation is capturing attention due to convolutional neural networks (CNN) being strongly biased toward recognizing textures rather than shapes. Most existing styling methods either perform a low-fidelity style transfer or a weak style representation in the embedding vector. This paper outlines a style augmentation algorithm using stochastic-based sampling with noise addition to improving randomization on a general linear transformation for style transfer. With our augmentation strategy, all models not only present incredible robustness against image stylizing but also outperform all previous methods and surpass the state-of-the-art performance for the STL-10 dataset. In addition, we present an analysis of the model interpretations under different style variations. At the same time, we compare comprehensive experiments demonstrating the performance when applied to deep neural architectures in training settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31070;&#32463;&#21551;&#21457;&#30340;&#36866;&#24212;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#27169;&#25311;&#26524;&#34631;&#23398;&#20064;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#21464;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#23398;&#20064;&#21487;&#22609;&#24615;&#65292;&#24182;&#30830;&#20445;&#35299;&#20915;&#26041;&#26696;&#30340;&#20860;&#23481;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14991</link><description>&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#24341;&#20837;&#31070;&#32463;&#21551;&#21457;&#30340;&#36866;&#24212;&#24615;&#65292;&#20197;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incorporating Neuro-Inspired Adaptability for Continual Learning in Artificial Intelligence. (arXiv:2308.14991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31070;&#32463;&#21551;&#21457;&#30340;&#36866;&#24212;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#27169;&#25311;&#26524;&#34631;&#23398;&#20064;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#21464;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#23398;&#20064;&#21487;&#22609;&#24615;&#65292;&#24182;&#30830;&#20445;&#35299;&#20915;&#26041;&#26696;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#36171;&#20104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#24378;&#22823;&#36866;&#24212;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#19968;&#20010;&#29702;&#24819;&#30340;&#35299;&#20915;&#26041;&#26696;&#24212;&#35813;&#22312;&#35760;&#24518;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21487;&#22609;&#24615;&#20043;&#38388;&#20445;&#25345;&#36866;&#24403;&#24179;&#34913;&#65292;&#24182;&#33719;&#24471;&#36275;&#22815;&#30340;&#20860;&#23481;&#24615;&#26469;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#20998;&#24067;&#12290;&#29616;&#26377;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#20445;&#25345;&#35760;&#24518;&#31283;&#23450;&#24615;&#20197;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20294;&#20173;&#38590;&#20197;&#20687;&#29983;&#29289;&#26234;&#33021;&#65288;BI&#65289;&#37027;&#26679;&#28789;&#27963;&#22320;&#36866;&#24212;&#22686;&#37327;&#21464;&#21270;&#12290;&#36890;&#36807;&#24314;&#27169;&#19968;&#20010;&#33021;&#22815;&#20027;&#21160;&#35843;&#33410;&#36951;&#24536;&#30340;&#31283;&#20581;&#26524;&#34631;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#22810;&#20010;&#23398;&#20064;&#27169;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21442;&#25968;&#20998;&#24067;&#20013;&#36866;&#24403;&#34928;&#20943;&#26087;&#35760;&#24518;&#26469;&#25913;&#21892;&#23398;&#20064;&#21487;&#22609;&#24615;&#65292;&#24182;&#30456;&#24212;&#22320;&#21327;&#35843;&#22810;&#23398;&#20064;&#32773;&#26550;&#26500;&#26469;&#30830;&#20445;&#35299;&#20915;&#26041;&#26696;&#30340;&#20860;&#23481;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#26126;&#26174;&#25552;&#39640;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#31361;&#35302;&#35843;&#33410;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning aims to empower artificial intelligence (AI) with strong adaptability to the real world. For this purpose, a desirable solution should properly balance memory stability with learning plasticity, and acquire sufficient compatibility to capture the observed distributions. Existing advances mainly focus on preserving memory stability to overcome catastrophic forgetting, but remain difficult to flexibly accommodate incremental changes as biological intelligence (BI) does. By modeling a robust Drosophila learning system that actively regulates forgetting with multiple learning modules, here we propose a generic approach that appropriately attenuates old memories in parameter distributions to improve learning plasticity, and accordingly coordinates a multi-learner architecture to ensure solution compatibility. Through extensive theoretical and empirical validation, our approach not only clearly enhances the performance of continual learning, especially over synaptic regula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#37327;&#26500;&#24314;&#23398;&#20064;&#21644;&#38598;&#25104;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28378;&#21160;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#19978;&#23454;&#26045;&#22686;&#37327;&#23398;&#20064;&#65292;&#32467;&#21512;&#20113;&#29305;&#24449;&#25552;&#21462;&#21644;&#23567;&#27874;&#21253;&#20998;&#35299;&#65292;&#25552;&#39640;&#20102;&#25925;&#38556;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14983</link><description>&lt;p&gt;
&#22522;&#20110;&#22686;&#37327;&#26500;&#24314;&#23398;&#20064;&#21644;&#38598;&#25104;&#22495;&#36866;&#24212;&#30340;&#28378;&#21160;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Constructive Incremental Learning for Fault Diagnosis of Rolling Bearings with Ensemble Domain Adaptation. (arXiv:2308.14983v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#37327;&#26500;&#24314;&#23398;&#20064;&#21644;&#38598;&#25104;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28378;&#21160;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#19978;&#23454;&#26045;&#22686;&#37327;&#23398;&#20064;&#65292;&#32467;&#21512;&#20113;&#29305;&#24449;&#25552;&#21462;&#21644;&#23567;&#27874;&#21253;&#20998;&#35299;&#65292;&#25552;&#39640;&#20102;&#25925;&#38556;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#28378;&#21160;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#22312;&#21508;&#31181;&#24037;&#20917;&#19979;&#30340;&#23454;&#38469;&#38382;&#39064;&#26222;&#36941;&#23384;&#22312;&#65292;&#26679;&#26412;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#22686;&#21152;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#22806;&#37096;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21644;&#28378;&#21160;&#36724;&#25215;&#30340;&#32467;&#26500;&#32463;&#24120;&#34920;&#29616;&#20986;&#20855;&#26377;&#38543;&#26426;&#24615;&#21644;&#27169;&#31946;&#24615;&#30340;&#25925;&#38556;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#25925;&#38556;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#65292;&#24182;&#38480;&#21046;&#20102;&#25925;&#38556;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26500;&#36896;&#22686;&#37327;&#23398;&#20064;&#30340;&#38598;&#25104;&#22495;&#36866;&#24212;&#65288;CIL-EDA&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#22312;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCN&#65289;&#19978;&#23454;&#26045;&#65292;&#20197;&#22312;&#22810;&#20010;&#22495;&#20013;&#26500;&#24314;&#20854;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#37319;&#29992;&#20113;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#32467;&#21512;&#23567;&#27874;&#21253;&#20998;&#35299;&#65288;WPD&#65289;&#26469;&#25429;&#25417;&#26469;&#33258;&#22810;&#20010;&#20998;&#36776;&#29575;&#26041;&#38754;&#30340;&#25925;&#38556;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#38543;&#21518;&#65292;&#37319;&#29992;&#22686;&#37327;&#26500;&#24314;&#23398;&#20064;&#30340;&#22495;&#36866;&#24212;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the prevalence of rolling bearing fault diagnosis as a practical issue across various working conditions, the limited availability of samples compounds the challenge. Additionally, the complexity of the external environment and the structure of rolling bearings often manifests faults characterized by randomness and fuzziness, hindering the effective extraction of fault characteristics and restricting the accuracy of fault diagnosis. To overcome these problems, this paper presents a novel approach termed constructive Incremental learning-based ensemble domain adaptation (CIL-EDA) approach. Specifically, it is implemented on stochastic configuration networks (SCN) to constructively improve its adaptive performance in multi-domains. Concretely, a cloud feature extraction method is employed in conjunction with wavelet packet decomposition (WPD) to capture the uncertainty of fault information from multiple resolution aspects. Subsequently, constructive Incremental learning-based domai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#27010;&#29575;&#30005;&#36335;&#30340;&#21464;&#20998;&#30005;&#36335;&#65292;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;Max-Cut&#38382;&#39064;&#30340;&#25968;&#20540;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#21464;&#20998;&#30005;&#36335;&#22312;&#22810;&#31181;&#22270;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#12290;&#22312;&#35780;&#20272;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#30340;&#24615;&#33021;&#26102;&#65292;&#21487;&#20197;&#23558;&#20854;&#19982;&#20855;&#26377;&#23376;&#36890;&#29992;&#38376;&#38598;&#30340;&#21464;&#20998;&#30005;&#36335;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#35782;&#21035;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#30340;&#20248;&#21183;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.14981</link><description>&lt;p&gt;
&#23376;&#36890;&#29992;&#21464;&#20998;&#30005;&#36335;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#32763;&#35793;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Sub-universal variational circuits for combinatorial optimization problems. (arXiv:2308.14981v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#27010;&#29575;&#30005;&#36335;&#30340;&#21464;&#20998;&#30005;&#36335;&#65292;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;Max-Cut&#38382;&#39064;&#30340;&#25968;&#20540;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#21464;&#20998;&#30005;&#36335;&#22312;&#22810;&#31181;&#22270;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#12290;&#22312;&#35780;&#20272;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#30340;&#24615;&#33021;&#26102;&#65292;&#21487;&#20197;&#23558;&#20854;&#19982;&#20855;&#26377;&#23376;&#36890;&#29992;&#38376;&#38598;&#30340;&#21464;&#20998;&#30005;&#36335;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#35782;&#21035;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#30340;&#20248;&#21183;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32463;&#20856;&#27010;&#29575;&#30005;&#36335;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#30001;&#20108;&#20301;&#38543;&#26426;&#30697;&#38453;&#26500;&#24314;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#36817;&#20284;&#35299;&#12290;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#21464;&#20998;&#30005;&#36335;&#22312;&#35299;&#20915;&#19981;&#26029;&#22686;&#21152;&#35268;&#27169;&#30340;&#21508;&#31181;&#22270;&#30340;Max-Cut&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32463;&#20856;&#31639;&#27861;&#22312;&#22810;&#31181;&#31867;&#22411;&#30340;&#22270;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23558;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#30340;&#24615;&#33021;&#19982;&#20855;&#26377;&#23376;&#36890;&#29992;&#38376;&#38598;&#30340;&#21464;&#20998;&#30005;&#36335;&#36827;&#34892;&#35780;&#20272;&#65292;&#26159;&#35782;&#21035;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#21487;&#31361;&#20986;&#20248;&#21183;&#39046;&#22495;&#30340;&#26377;&#20215;&#20540;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum variational circuits have gained significant attention due to their applications in the quantum approximate optimization algorithm and quantum machine learning research. This work introduces a novel class of classical probabilistic circuits designed for generating approximate solutions to combinatorial optimization problems constructed using two-bit stochastic matrices. Through a numerical study, we investigate the performance of our proposed variational circuits in solving the Max-Cut problem on various graphs of increasing sizes. Our classical algorithm demonstrates improved performance for several graph types to the quantum approximate optimization algorithm. Our findings suggest that evaluating the performance of quantum variational circuits against variational circuits with sub-universal gate sets is a valuable benchmark for identifying areas where quantum variational circuits can excel.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31895;&#30053;&#26631;&#27880;&#30340;&#22825;&#25991;&#35270;&#39057;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#26631;&#27880;&#36136;&#37327;&#21644;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#20943;&#23569;&#26631;&#27880;&#36807;&#31243;&#20013;&#30340;&#26102;&#38388;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2308.14976</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#25928;&#23545;&#22826;&#38451;&#36890;&#37327;&#28436;&#21270;&#35270;&#39057;&#36827;&#34892;&#26631;&#27880;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient labeling of solar flux evolution videos by a deep learning model. (arXiv:2308.14976v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14976
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31895;&#30053;&#26631;&#27880;&#30340;&#22825;&#25991;&#35270;&#39057;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#26631;&#27880;&#36136;&#37327;&#21644;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#20943;&#23569;&#26631;&#27880;&#36807;&#31243;&#20013;&#30340;&#26102;&#38388;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27491;&#25104;&#20026;&#23545;&#22823;&#22411;&#22797;&#26434;&#25968;&#25454;&#36827;&#34892;&#38382;&#35810;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#26631;&#27880;&#65292;&#21363;&#28155;&#21152;&#26377;&#24847;&#20041;&#30340;&#27880;&#37322;&#30340;&#36807;&#31243;&#65292;&#26159;&#30417;&#30563;&#24335;ML&#30340;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#25968;&#25454;&#38598;&#38750;&#24120;&#32791;&#26102;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21487;&#20197;&#21033;&#29992;&#31895;&#30053;&#26631;&#27880;&#30340;&#22825;&#25991;&#35270;&#39057;&#26469;&#25552;&#39640;&#25968;&#25454;&#26631;&#27880;&#36136;&#37327;&#65292;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#20351;&#29992;&#22826;&#38451;&#30913;&#22330;&#30340;&#35270;&#39057;&#65292;&#26681;&#25454;&#23427;&#20204;&#22312;&#22826;&#38451;&#30424;&#19978;&#39318;&#27425;&#26816;&#27979;&#21040;&#30340;&#21452;&#26497;&#30913;&#21306;&#65288;BMRs&#65289;&#30340;&#20986;&#29616;&#25110;&#38750;&#20986;&#29616;&#23558;&#20854;&#31895;&#30053;&#26631;&#35760;&#25104;&#20004;&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#20351;&#29992;&#31895;&#30053;&#26631;&#31614;&#35757;&#32451;CNN&#65292;&#25163;&#21160;&#39564;&#35777;&#21644;&#32416;&#27491;CNN&#19982;&#26631;&#27880;&#30340;&#19981;&#19968;&#33268;&#20043;&#22788;&#65292;&#24182;&#37325;&#22797;&#27492;&#36807;&#31243;&#30452;&#33267;&#25910;&#25947;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#37327;&#20986;&#29616;&#30340;&#26631;&#27880;&#26159;&#25163;&#21160;&#23436;&#25104;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#24471;&#21040;&#30340;&#39640;&#36136;&#37327;&#26631;&#27880;&#25968;&#25454;&#38598;&#21487;&#20197;&#23558;&#20154;&#24037;&#39564;&#35777;&#30340;&#38656;&#27714;&#38477;&#20302;50%&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#36880;&#28176;&#23631;&#34109;&#25481;&#26631;&#31614;&#65292;&#25105;&#20204;&#21487;&#20197;&#36880;&#27493;&#25552;&#39640;CNN&#22312;&#20934;&#30830;&#26631;&#27880;&#36807;&#31243;&#20013;&#30340;&#33258;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) is becoming a critical tool for interrogation of large complex data. Labeling, defined as the process of adding meaningful annotations, is a crucial step of supervised ML. However, labeling datasets is time consuming. Here we show that convolutional neural networks (CNNs), trained on crudely labeled astronomical videos, can be leveraged to improve the quality of data labeling and reduce the need for human intervention. We use videos of the solar magnetic field, crudely labeled into two classes: emergence or non-emergence of bipolar magnetic regions (BMRs), based on their first detection on the solar disk. We train CNNs using crude labels, manually verify, correct labeling vs. CNN disagreements, and repeat this process until convergence. Traditionally, flux emergence labelling is done manually. We find that a high-quality labeled dataset, derived through this iterative process, reduces the necessary manual verification by 50%. Furthermore, by gradually masking the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#22810;&#26234;&#33021;&#20307;&#30446;&#26631;&#25628;&#32034;&#21644;&#36319;&#36394;&#26041;&#27861;&#65292;&#22312;&#26410;&#30693;&#30446;&#26631;&#22330;&#26223;&#19979;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23454;&#29616;&#30446;&#26631;&#25506;&#32034;&#21644;&#20915;&#31574;&#35268;&#21010;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14971</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#30446;&#26631;&#25628;&#32034;&#21644;&#36319;&#36394;&#65306;&#39640;&#26031;&#36807;&#31243;&#19982;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributed multi-agent target search and tracking with Gaussian process and reinforcement learning. (arXiv:2308.14971v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#22810;&#26234;&#33021;&#20307;&#30446;&#26631;&#25628;&#32034;&#21644;&#36319;&#36394;&#26041;&#27861;&#65292;&#22312;&#26410;&#30693;&#30446;&#26631;&#22330;&#26223;&#19979;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23454;&#29616;&#30446;&#26631;&#25506;&#32034;&#21644;&#20915;&#31574;&#35268;&#21010;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22810;&#20010;&#26426;&#22120;&#20154;&#29992;&#20110;&#30446;&#26631;&#25628;&#32034;&#21644;&#36319;&#36394;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26377;&#24456;&#22810;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#26410;&#30693;&#25110;&#37096;&#20998;&#24050;&#30693;&#30446;&#26631;&#30340;&#24773;&#20917;&#19979;&#12290;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#20351;&#24471;&#26234;&#33021;&#25511;&#21046;&#25216;&#26415;&#22914;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#35753;&#26234;&#33021;&#20307;&#22312;&#19982;&#29615;&#22659;&#20132;&#20114;&#20013;&#20174;&#38646;&#21040;&#26377;&#19968;&#23450;&#30340;&#33258;&#20027;&#23398;&#20064;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#35299;&#20915;&#26410;&#30693;&#30446;&#26631;&#35268;&#21010;&#20013;&#30340;&#21208;&#25506;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#65292;&#28040;&#38500;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#20381;&#36182;&#65292;&#24182;&#31616;&#21270;&#20915;&#31574;&#27969;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#39640;&#26031;&#36807;&#31243;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#30446;&#26631;&#22320;&#22270;&#26500;&#24314;&#12290;&#25105;&#20204;&#21033;&#29992;&#20998;&#24067;&#24335;&#39640;&#26031;&#36807;&#31243;&#23545;&#30446;&#26631;&#20301;&#32622;&#36827;&#34892;&#32622;&#20449;&#24230;&#32534;&#30721;&#65292;&#24182;&#39640;&#25928;&#22320;&#35268;&#21010;&#26410;&#30693;&#30446;&#26631;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#35757;&#32451;&#31574;&#30053;&#30340;&#24615;&#33021;&#21644;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deploying multiple robots for target search and tracking has many practical applications, yet the challenge of planning over unknown or partially known targets remains difficult to address. With recent advances in deep learning, intelligent control techniques such as reinforcement learning have enabled agents to learn autonomously from environment interactions with little to no prior knowledge. Such methods can address the exploration-exploitation tradeoff of planning over unknown targets in a data-driven manner, eliminating the reliance on heuristics typical of traditional approaches and streamlining the decision-making pipeline with end-to-end training. In this paper, we propose a multi-agent reinforcement learning technique with target map building based on distributed Gaussian process. We leverage the distributed Gaussian process to encode belief over the target locations and efficiently plan over unknown targets. We evaluate the performance and transferability of the trained polic
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#24425;&#31080;&#24335;&#36716;&#31227;&#30340;&#39640;&#25928;&#21487;&#38752;&#24615;&#65292;&#30740;&#31350;&#20102;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#35270;&#35273;&#25552;&#31034;/&#37325;&#26032;&#32534;&#31243;&#65288;VP&#65289;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#27169;&#22411;&#31232;&#30095;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24425;&#31080;&#24335;&#36716;&#31227;&#24182;&#38750;&#36890;&#29992;&#30340;&#37325;&#26032;&#32534;&#31243;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.14969</link><description>&lt;p&gt;
&#21463;&#38480;&#21046;&#19979;&#30340;&#37325;&#26032;&#32534;&#31243;: &#37325;&#26032;&#23457;&#35270;&#24425;&#31080;&#24335;&#36716;&#31227;&#30340;&#39640;&#25928;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets. (arXiv:2308.14969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14969
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24425;&#31080;&#24335;&#36716;&#31227;&#30340;&#39640;&#25928;&#21487;&#38752;&#24615;&#65292;&#30740;&#31350;&#20102;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#35270;&#35273;&#25552;&#31034;/&#37325;&#26032;&#32534;&#31243;&#65288;VP&#65289;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#27169;&#22411;&#31232;&#30095;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#24425;&#31080;&#24335;&#36716;&#31227;&#24182;&#38750;&#36890;&#29992;&#30340;&#37325;&#26032;&#32534;&#31243;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#39044;&#31639;&#24040;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#65292;&#19979;&#28216;&#20219;&#21153;&#24050;&#32463;&#36716;&#21521;&#20102;&#39640;&#25928;&#24555;&#36895;&#36866;&#24212;&#30340;&#21465;&#36848;&#12290;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#22522;&#20110;&#20998;&#31867;&#30340;&#20219;&#21153;&#65292;&#26368;&#39640;&#25928;&#30340;&#26041;&#27861;&#26159;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#35270;&#35273;&#25552;&#31034;/&#37325;&#26032;&#32534;&#31243;&#65288;VP&#65289;; &#21069;&#32773;&#26088;&#22312;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#30340;&#29305;&#24449;&#19978;&#23398;&#20064;&#32447;&#24615;&#22836;&#37096;&#20998;&#31867;&#22120;&#65292;&#32780;&#21518;&#32773;&#23558;&#36755;&#20837;&#25968;&#25454;&#26144;&#23556;&#21040;&#26368;&#21021;&#22312;&#20854;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#28304;&#25968;&#25454;&#39046;&#22495;&#12290;&#23613;&#31649;&#24191;&#27867;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;LP&#21644;VP&#22312;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;&#24230;&#36724;&#26469;&#25506;&#32034;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#33021;&#21147;: (a) &#25968;&#25454;&#31232;&#30095;&#24615;: &#23569;&#26679;&#26412;&#33258;&#36866;&#24212;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450; (b) &#27169;&#22411;&#31232;&#30095;&#24615;: &#24425;&#31080;&#24335;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24425;&#31080;&#24335;&#36716;&#31227;&#19981;&#26159;&#36890;&#29992;&#30340;&#37325;&#26032;&#32534;&#31243;&#22120;&#65292;&#21363;&#23545;&#20110;&#26576;&#20123;&#30446;&#26631;&#25968;&#25454;&#38598;&#65292;&#37325;&#26032;&#32534;&#31243;&#24425;&#31080;&#24335;&#36716;&#31227;&#20250;&#20135;&#29983;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of foundation models with huge pre-training budgets, the downstream tasks have been shifted to the narrative of efficient and fast adaptation. For classification-based tasks in the domain of computer vision, the two most efficient approaches have been linear probing (LP) and visual prompting/reprogramming (VP); the former aims to learn a classifier in the form of a linear head on the features extracted by the pre-trained model, while the latter maps the input data to the domain of the source data on which the model was originally pre-trained on. Although extensive studies have demonstrated the differences between LP and VP in terms of downstream performance, we explore the capabilities of the two aforementioned methods via the sparsity axis: (a) Data sparsity: the impact of few-shot adaptation and (b) Model sparsity: the impact of lottery tickets (LT). We demonstrate that LT are not universal reprogrammers, i.e., for certain target datasets, reprogramming an LT yields signif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21387;&#32553;&#27969;&#24335;&#31185;&#23398;&#25968;&#25454;&#30340;&#27969;&#24335;&#24369;SINDy&#31639;&#27861;&#12290;&#19982;&#32463;&#20856;&#30340;&#31163;&#32447;&#21387;&#32553;&#31639;&#27861;&#19981;&#21516;&#65292;&#27969;&#24335;&#21387;&#32553;&#31639;&#27861;&#21487;&#20197;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#21387;&#32553;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;&#12290;&#27969;&#24335;&#24369;SINDy&#31639;&#27861;&#21033;&#29992;&#20102;&#24213;&#23618;&#25968;&#25454;&#30340;&#29305;&#24449;&#26469;&#36827;&#34892;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2308.14962</link><description>&lt;p&gt;
&#36890;&#36807;&#24369;SINDy&#31639;&#27861;&#36827;&#34892;&#27969;&#24335;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Streaming Compression of Scientific Data via weak-SINDy. (arXiv:2308.14962v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21387;&#32553;&#27969;&#24335;&#31185;&#23398;&#25968;&#25454;&#30340;&#27969;&#24335;&#24369;SINDy&#31639;&#27861;&#12290;&#19982;&#32463;&#20856;&#30340;&#31163;&#32447;&#21387;&#32553;&#31639;&#27861;&#19981;&#21516;&#65292;&#27969;&#24335;&#21387;&#32553;&#31639;&#27861;&#21487;&#20197;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#21387;&#32553;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;&#12290;&#27969;&#24335;&#24369;SINDy&#31639;&#27861;&#21033;&#29992;&#20102;&#24213;&#23618;&#25968;&#25454;&#30340;&#29305;&#24449;&#26469;&#36827;&#34892;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;&#27969;&#24335;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;&#30340;&#27969;&#24335;&#24369;SINDy&#31639;&#27861;&#12290;&#31185;&#23398;&#25968;&#25454;&#30340;&#20135;&#29983;&#26080;&#35770;&#26159;&#36890;&#36807;&#27169;&#25311;&#36824;&#26159;&#23454;&#39564;&#65292;&#37117;&#27491;&#22312;&#32463;&#21382;&#19968;&#27573;&#25351;&#25968;&#22686;&#38271;&#30340;&#38454;&#27573;&#65292;&#36825;&#20351;&#24471;&#25968;&#25454;&#21387;&#32553;&#23545;&#20110;&#23384;&#20648;&#21644;&#21033;&#29992;&#22823;&#35268;&#27169;&#31185;&#23398;&#25968;&#25454;&#38598;&#26469;&#35828;&#21464;&#24471;&#37325;&#35201;&#19988;&#24120;&#24120;&#26159;&#24517;&#35201;&#30340;&#12290;&#19982;&#32463;&#20856;&#30340;&#8220;&#31163;&#32447;&#8221;&#21387;&#32553;&#31639;&#27861;&#30456;&#21453;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#22312;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#21387;&#32553;&#65292;&#27969;&#24335;&#21387;&#32553;&#31639;&#27861;&#22312;&#27169;&#25311;&#25110;&#23454;&#39564;&#29983;&#25104;&#30340;&#25968;&#25454;&#20173;&#22312;&#31995;&#32479;&#20013;&#27969;&#21160;&#26102;&#36827;&#34892;&#25968;&#25454;&#21387;&#32553;&#12290;&#36825;&#20010;&#29305;&#24615;&#20351;&#24471;&#27969;&#24335;&#21387;&#32553;&#31639;&#27861;&#38750;&#24120;&#36866;&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;&#65292;&#22240;&#20026;&#22312;&#31163;&#32447;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#24448;&#24448;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27969;&#24335;&#21387;&#32553;&#31639;&#27861;&#65292;&#27969;&#24335;&#24369;SINDy&#65292;&#23427;&#22312;&#21387;&#32553;&#36807;&#31243;&#20013;&#21033;&#29992;&#20102;&#24213;&#23618;&#25968;&#25454;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper a streaming weak-SINDy algorithm is developed specifically for compressing streaming scientific data. The production of scientific data, either via simulation or experiments, is undergoing an stage of exponential growth, which makes data compression important and often necessary for storing and utilizing large scientific data sets. As opposed to classical ``offline" compression algorithms that perform compression on a readily available data set, streaming compression algorithms compress data ``online" while the data generated from simulation or experiments is still flowing through the system. This feature makes streaming compression algorithms well-suited for scientific data compression, where storing the full data set offline is often infeasible. This work proposes a new streaming compression algorithm, streaming weak-SINDy, which takes advantage of the underlying data characteristics during compression. The streaming weak-SINDy algorithm constructs feature matrices and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#24320;&#25918;&#38598;&#35328;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;MFCC&#21644;&#38899;&#39640;&#29305;&#24449;&#65292;TDNN&#27169;&#22411;&#25552;&#21462;&#29305;&#24449;&#23884;&#20837;&#65292;&#35774;&#32622;&#32622;&#20449;&#24230;&#38408;&#20540;&#65292;&#20197;&#21450;&#20351;&#29992;LDA&#21644;pLDA&#23398;&#20064;&#26032;&#30340;&#26410;&#30693;&#35821;&#35328;&#20998;&#31867;&#26469;&#23454;&#29616;&#12290;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#35328;&#19978;&#65292;&#31995;&#32479;&#20934;&#30830;&#29575;&#36798;&#21040;91.76%&#65292;&#24182;&#19988;&#20855;&#22791;&#23454;&#26102;&#36866;&#24212;&#26410;&#30693;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.14951</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#24320;&#25918;&#38598;&#35328;&#35821;&#35782;&#21035;&#21644;CU MultiLang&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Robust Open-Set Spoken Language Identification and the CU MultiLang Dataset. (arXiv:2308.14951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#24320;&#25918;&#38598;&#35328;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;MFCC&#21644;&#38899;&#39640;&#29305;&#24449;&#65292;TDNN&#27169;&#22411;&#25552;&#21462;&#29305;&#24449;&#23884;&#20837;&#65292;&#35774;&#32622;&#32622;&#20449;&#24230;&#38408;&#20540;&#65292;&#20197;&#21450;&#20351;&#29992;LDA&#21644;pLDA&#23398;&#20064;&#26032;&#30340;&#26410;&#30693;&#35821;&#35328;&#20998;&#31867;&#26469;&#23454;&#29616;&#12290;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#35328;&#19978;&#65292;&#31995;&#32479;&#20934;&#30830;&#29575;&#36798;&#21040;91.76%&#65292;&#24182;&#19988;&#20855;&#22791;&#23454;&#26102;&#36866;&#24212;&#26410;&#30693;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#35328;&#35821;&#35782;&#21035;&#27169;&#22411;&#26159;&#38381;&#38598;&#30340;&#65292;&#21363;&#23427;&#20204;&#21482;&#33021;&#36755;&#20986;&#23427;&#20204;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#31867;&#21035;&#38598;&#21512;&#20013;&#30340;&#35821;&#35328;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#24320;&#25918;&#38598;&#35328;&#35821;&#35782;&#21035;&#31995;&#32479;&#20855;&#22791;&#26816;&#27979;&#36755;&#20837;&#26159;&#21542;&#19981;&#23646;&#20110;&#21407;&#22987;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#38598;&#35328;&#35821;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;MFCC&#21644;&#38899;&#39640;&#29305;&#24449;&#65292; TDNN&#27169;&#22411;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#36890;&#36807;&#23545;softmax&#36755;&#20986;&#36827;&#34892;&#32622;&#20449;&#24230;&#38408;&#20540;&#22788;&#29702;&#65292;&#20197;&#21450;&#20351;&#29992;LDA&#21644;pLDA&#23398;&#20064;&#23545;&#26032;&#30340;&#26410;&#30693;&#35821;&#35328;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35328;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#20854;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#35328;&#19978;&#23454;&#29616;&#20102;91.76%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#20855;&#22791;&#23454;&#26102;&#36866;&#24212;&#26410;&#30693;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;CU MultiLang&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most state-of-the-art spoken language identification models are closed-set; in other words, they can only output a language label from the set of classes they were trained on. Open-set spoken language identification systems, however, gain the ability to detect when an input exhibits none of the original languages. In this paper, we implement a novel approach to open-set spoken language identification that uses MFCC and pitch features, a TDNN model to extract meaningful feature embeddings, confidence thresholding on softmax outputs, and LDA and pLDA for learning to classify new unknown languages. We present a spoken language identification system that achieves 91.76% accuracy on trained languages and has the capability to adapt to unknown languages on the fly. To that end, we also built the CU MultiLang Dataset, a large and diverse multilingual speech corpus which was used to train and evaluate our system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#24179;&#28369;&#24863;&#30693;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20302;&#27604;&#29305;&#37327;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#21644;&#28145;&#23618;GNN&#20013;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#12289;&#23398;&#20064;&#37327;&#21270;&#33539;&#22260;&#21644;&#25511;&#21046;&#23618;&#38388;&#30456;&#20284;&#24615;&#30340;&#21464;&#21270;&#26469;&#23454;&#29616;&#39640;&#25928;&#22788;&#29702;&#21644;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.14949</link><description>&lt;p&gt;
&#24102;&#26377;&#24179;&#28369;&#24863;&#30693;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20302;&#27604;&#29305;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Low-bit Quantization for Deep Graph Neural Networks with Smoothness-aware Message Propagation. (arXiv:2308.14949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#24179;&#28369;&#24863;&#30693;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20302;&#27604;&#29305;&#37327;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#21644;&#28145;&#23618;GNN&#20013;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#12289;&#23398;&#20064;&#37327;&#21270;&#33539;&#22260;&#21644;&#25511;&#21046;&#23618;&#38388;&#30456;&#20284;&#24615;&#30340;&#21464;&#21270;&#26469;&#23454;&#29616;&#39640;&#25928;&#22788;&#29702;&#21644;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#38754;&#20020;&#30528;&#27169;&#22411;&#35268;&#27169;&#21644;&#23618;&#25968;&#22686;&#21152;&#23548;&#33268;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#38477;&#20302;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#30340;&#22823;&#35268;&#27169;&#21644;&#28145;&#23618;GNN&#20013;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20197;&#23454;&#29616;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#39640;&#25928;&#30340;GNN&#65292;&#24182;&#36991;&#20813;&#28145;&#23618;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;GNN&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#20174;&#35757;&#32451;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#21040;&#33410;&#28857;&#20998;&#31867;&#65292;&#21387;&#32553;&#27169;&#22411;&#24182;&#23454;&#29616;&#39640;&#25928;&#22788;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;GNN&#37327;&#21270;&#22120;&#23398;&#20064;&#37327;&#21270;&#33539;&#22260;&#65292;&#21363;&#20351;&#22312;&#20302;&#27604;&#29305;&#37327;&#21270;&#19979;&#20063;&#33021;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#24182;&#33719;&#24471;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#38543;&#30528;&#23618;&#25968;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35757;&#32451;&#20013;&#30340;&#28040;&#24687;&#20256;&#25773;&#26426;&#21046;&#65292;&#25511;&#21046;&#30456;&#37051;&#33410;&#28857;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#36880;&#23618;&#21464;&#21270;&#12290;&#36890;&#36807;&#23558;&#36825;&#20010;&#30446;&#26631;&#32435;&#20837;&#24102;&#32422;&#26463;&#30340;Lagrangian&#20989;&#25968;&#21644;&#24046;&#20998;&#22810;&#20803;&#26368;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network (GNN) training and inference involve significant challenges of scalability with respect to both model sizes and number of layers, resulting in degradation of efficiency and accuracy for large and deep GNNs. We present an end-to-end solution that aims to address these challenges for efficient GNNs in resource constrained environments while avoiding the oversmoothing problem in deep GNNs. We introduce a quantization based approach for all stages of GNNs, from message passing in training to node classification, compressing the model and enabling efficient processing. The proposed GNN quantizer learns quantization ranges and reduces the model size with comparable accuracy even under low-bit quantization. To scale with the number of layers, we devise a message propagation mechanism in training that controls layer-wise changes of similarities between neighboring nodes. This objective is incorporated into a Lagrangian function with constraints and a differential multiplie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#65292;&#22810;&#26679;&#21270;&#29615;&#22659;&#21644;&#24314;&#27169;&#34892;&#20154;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.14947</link><description>&lt;p&gt;
&#25913;&#36827;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Reinforcement Learning Training Regimes for Social Robot Navigation. (arXiv:2308.14947v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#65292;&#22810;&#26679;&#21270;&#29615;&#22659;&#21644;&#24314;&#27169;&#34892;&#20154;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35753;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#22312;&#20154;&#31867;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#23427;&#20204;&#24517;&#39035;&#36981;&#23432;&#25105;&#20204;&#30340;&#31038;&#20132;&#35268;&#33539;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#35757;&#32451;&#26426;&#22120;&#20154;&#23548;&#33322;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#20351;&#20854;&#33021;&#22815;&#36981;&#23432;&#36825;&#20123;&#35268;&#33539;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#20013;&#29616;&#26377;&#24037;&#20316;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#22312;&#31616;&#21270;&#29615;&#22659;&#20013;&#36827;&#34892;RL&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20854;&#32467;&#26524;&#30340;&#23454;&#36136;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;RL&#31038;&#20132;&#23548;&#33322;&#26041;&#27861;&#27867;&#21270;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#29615;&#22659;&#31867;&#22411;&#21644;&#22810;&#20010;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#24314;&#27169;&#34892;&#20154;&#65292;&#25105;&#20204;&#33021;&#22815;&#36880;&#27493;&#22686;&#21152;&#35757;&#32451;&#30340;&#22810;&#26679;&#24615;&#21644;&#38590;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#21487;&#20197;&#23454;&#29616;&#27604;&#20197;&#21069;&#30340;&#35757;&#32451;&#26041;&#27861;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35768;&#22810;&#29616;&#26377;&#32479;&#35745;&#32467;&#26524;&#30340;&#32467;&#26524;&#20043;&#19968;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order for autonomous mobile robots to navigate in human spaces, they must abide by our social norms. Reinforcement learning (RL) has emerged as an effective method to train robot navigation policies that are able to respect these norms. However, a large portion of existing work in the field conducts both RL training and testing in simplistic environments. This limits the generalization potential of these models to unseen environments, and the meaningfulness of their reported results. We propose a method to improve the generalization performance of RL social navigation methods using curriculum learning. By employing multiple environment types and by modeling pedestrians using multiple dynamics models, we are able to progressively diversify and escalate difficulty in training. Our results show that the use of curriculum learning in training can be used to achieve better generalization performance than previous training methods. We also show that results presented in many existing stat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#22270;&#20687;&#37325;&#24314;&#20013;&#23398;&#20064;&#37319;&#26679;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24212;&#29992;&#28145;&#24230;Q&#23398;&#20064;&#21644;REINFORCE&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#21457;&#29616;&#20102;&#26368;&#20339;&#37319;&#26679;&#27169;&#24335;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2308.14946</link><description>&lt;p&gt;
&#29992;&#20110;&#26102;&#38388;&#21307;&#23398;&#22270;&#20687;&#24207;&#21015;&#37319;&#26679;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Sampling on Temporal Medical Imaging Sequences. (arXiv:2308.14946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#22270;&#20687;&#37325;&#24314;&#20013;&#23398;&#20064;&#37319;&#26679;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24212;&#29992;&#28145;&#24230;Q&#23398;&#20064;&#21644;REINFORCE&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#21457;&#29616;&#20102;&#26368;&#20339;&#37319;&#26679;&#27169;&#24335;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36895;&#30913;&#20849;&#25391;&#25104;&#20687;&#36890;&#36807;&#20613;&#37324;&#21494;&#22495;&#23376;&#37319;&#26679;&#25110;&#26356;&#22909;&#30340;&#37325;&#24314;&#31639;&#27861;&#26469;&#22788;&#29702;&#36739;&#23569;&#30340;&#27979;&#37327;&#25968;&#25454;&#65292;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#22270;&#20687;&#12290;&#22312;&#32473;&#23450;&#22266;&#23450;&#37325;&#24314;&#21327;&#35758;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#26368;&#20339;&#37319;&#26679;&#31574;&#30053;&#36890;&#24120;&#20855;&#26377;&#32452;&#21512;&#22797;&#26434;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#21452;&#37325;&#28145;&#24230;Q&#23398;&#20064;&#21644;REINFORCE&#31639;&#27861;&#26469;&#23398;&#20064;&#21160;&#24577;&#22270;&#20687;&#37325;&#24314;&#30340;&#37319;&#26679;&#31574;&#30053;&#12290;&#25105;&#20204;&#32771;&#34385;&#30340;&#25968;&#25454;&#26684;&#24335;&#26159;&#26102;&#38388;&#24207;&#21015;&#65292;&#37325;&#24314;&#26041;&#27861;&#26159;&#39044;&#35757;&#32451;&#30340;&#33258;&#32534;&#30721;&#22120;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#21363;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#21457;&#29616;&#28508;&#22312;&#20110;&#39044;&#35757;&#32451;&#37325;&#26500;&#32593;&#32476;&#20013;&#30340;&#26368;&#20339;&#37319;&#26679;&#27169;&#24335;&#65288;&#21363;&#29615;&#22659;&#20013;&#30340;&#21160;&#24577;&#65289;&#12290;&#21487;&#20197;&#22312;https://github.com/zhishenhuang/RLsamp&#25214;&#21040;&#22797;&#21046;&#23454;&#39564;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accelerated magnetic resonance imaging resorts to either Fourier-domain subsampling or better reconstruction algorithms to deal with fewer measurements while still generating medical images of high quality. Determining the optimal sampling strategy given a fixed reconstruction protocol often has combinatorial complexity. In this work, we apply double deep Q-learning and REINFORCE algorithms to learn the sampling strategy for dynamic image reconstruction. We consider the data in the format of time series, and the reconstruction method is a pre-trained autoencoder-typed neural network. We present a proof of concept that reinforcement learning algorithms are effective to discover the optimal sampling pattern which underlies the pre-trained reconstructor network (i.e., the dynamics in the environment). The code for replicating experiments can be found at https://github.com/zhishenhuang/RLsamp.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27491;&#21017;&#21270;Wasserstein Proximal&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#22122;&#22768;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#30340;&#28508;&#21183;&#20989;&#25968;&#30830;&#23450;&#24615;&#22320;&#36827;&#34892;&#31890;&#23376;&#28436;&#21270;&#65292;&#24182;&#25552;&#20379;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#32500;&#24230;&#20381;&#36182;&#24615;&#21644;&#36895;&#24230;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.14945</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;Wasserstein Proximals&#23454;&#29616;&#26080;&#22122;&#22768;&#30340;&#25277;&#26679;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Noise-Free Sampling Algorithms via Regularized Wasserstein Proximals. (arXiv:2308.14945v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27491;&#21017;&#21270;Wasserstein Proximal&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#22122;&#22768;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#30340;&#28508;&#21183;&#20989;&#25968;&#30830;&#23450;&#24615;&#22320;&#36827;&#34892;&#31890;&#23376;&#28436;&#21270;&#65292;&#24182;&#25552;&#20379;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#32500;&#24230;&#20381;&#36182;&#24615;&#21644;&#36895;&#24230;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#30001;&#28508;&#21183;&#20989;&#25968;&#25511;&#21046;&#30340;&#20998;&#24067;&#25277;&#26679;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;&#22522;&#20110;&#35780;&#20998;&#30340;&#30830;&#23450;&#24615;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#20351;&#24471;&#31890;&#23376;&#30340;&#28436;&#21270;&#21464;&#20026;&#30830;&#23450;&#24615;&#30340;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#28436;&#21270;&#12290;&#35780;&#20998;&#39033;&#30001;&#27491;&#21017;&#21270;&#30340;Wasserstein proximal&#20197;&#38381;&#21512;&#24418;&#24335;&#32473;&#20986;&#65292;&#20351;&#29992;&#37319;&#26679;&#26469;&#36817;&#20284;&#26680;&#21367;&#31215;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#24555;&#36895;&#25910;&#25947;&#65292;&#24182;&#19988;&#19982;&#26410;&#35843;&#25972;Langevin&#31639;&#27861;&#21644;Metropolis&#35843;&#25972;Langevin&#31639;&#27861;&#30456;&#27604;&#65292;&#26174;&#31034;&#20102;&#39640;&#26031;&#20998;&#24067;&#30340;&#28151;&#21512;&#26102;&#38388;&#36793;&#30028;&#30340;&#25913;&#21892;&#32500;&#24230;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20102;&#20108;&#27425;&#28508;&#21183;&#20989;&#25968;&#27599;&#27425;&#36845;&#20195;&#30340;&#20998;&#24067;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#65292;&#34920;&#24449;&#20102;&#26041;&#24046;&#38477;&#20302;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#31890;&#23376;&#30340;&#34892;&#20026;&#26159;&#26377;&#32452;&#32455;&#30340;&#65292;&#20301;&#20110;&#28508;&#21183;&#30340;&#31561;&#20540;&#32447;&#19978;&#12290;&#27492;&#22806;&#65292;&#21518;&#39564;&#22343;&#20540;&#20272;&#35745;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sampling from a distribution governed by a potential function. This work proposes an explicit score-based MCMC method that is deterministic, resulting in a deterministic evolution for particles rather than a stochastic differential equation evolution. The score term is given in closed form by a regularized Wasserstein proximal, using a kernel convolution that is approximated by sampling. We demonstrate fast convergence on various problems and show improved dimensional dependence of mixing time bounds for the case of Gaussian distributions compared to the unadjusted Langevin algorithm (ULA) and the Metropolis-adjusted Langevin algorithm (MALA). We additionally derive closed form expressions for the distributions at each iterate for quadratic potential functions, characterizing the variance reduction. Empirical results demonstrate that the particles behave in an organized manner, lying on level set contours of the potential. Moreover, the posterior mean estimat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#29109;&#30340;&#25439;&#22833;&#39033;&#65292;&#36890;&#36807;&#27979;&#37327;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#25968;&#25454;&#26102;&#30340;&#29109;&#21464;&#21270;&#65292;&#25351;&#23548;&#31070;&#32463;&#32593;&#32476;&#20197;&#26356;&#24555;&#36895;&#30340;&#25910;&#25947;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#23398;&#20064;&#20016;&#23500;&#30340;&#28508;&#22312;&#25968;&#25454;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.14938</link><description>&lt;p&gt;
&#22522;&#20110;&#29109;&#30340;&#25351;&#23548;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#25910;&#25947;&#21644;&#25913;&#21892;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Entropy-based Guidance of Deep Neural Networks for Accelerated Convergence and Improved Performance. (arXiv:2308.14938v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#29109;&#30340;&#25439;&#22833;&#39033;&#65292;&#36890;&#36807;&#27979;&#37327;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#25968;&#25454;&#26102;&#30340;&#29109;&#21464;&#21270;&#65292;&#25351;&#23548;&#31070;&#32463;&#32593;&#32476;&#20197;&#26356;&#24555;&#36895;&#30340;&#25910;&#25947;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#23398;&#20064;&#20016;&#23500;&#30340;&#28508;&#22312;&#25968;&#25454;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#25105;&#20204;&#20174;&#22823;&#35268;&#27169;&#12289;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#36328;&#36234;&#26080;&#25968;&#23398;&#31185;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20915;&#31574;&#19981;&#26131;&#35299;&#37322;&#65292;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#24314;&#31435;&#21644;&#35757;&#32451;&#23427;&#20204;&#26159;&#19981;&#30830;&#23450;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#32473;&#36825;&#20123;&#21162;&#21147;&#22686;&#21152;&#32467;&#26500;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#25968;&#23398;&#32467;&#26524;&#65292;&#20197;&#39640;&#25928;&#22320;&#27979;&#37327;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#25968;&#25454;&#26102;&#30340;&#29109;&#21464;&#21270;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#29109;&#30340;&#25439;&#22833;&#39033;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22270;&#20687;&#21387;&#32553;&#21644;&#22270;&#20687;&#20998;&#31867;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#25439;&#22833;&#39033;&#25351;&#23548;&#31070;&#32463;&#32593;&#32476;&#20197;&#26356;&#23569;&#30340;&#32500;&#24230;&#23398;&#20064;&#20016;&#23500;&#30340;&#28508;&#22312;&#25968;&#25454;&#34920;&#31034;&#65292;&#25910;&#25947;&#20110;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#27425;&#65292;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#27979;&#35797;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have dramatically increased our capacity to learn from large, high-dimensional datasets across innumerable disciplines. However, their decisions are not easily interpretable, their computational costs are high, and building and training them are uncertain processes. To add structure to these efforts, we derive new mathematical results to efficiently measure the changes in entropy as fully-connected and convolutional neural networks process data, and introduce entropy-based loss terms. Experiments in image compression and image classification on benchmark datasets demonstrate these losses guide neural networks to learn rich latent data representations in fewer dimensions, converge in fewer training epochs, and achieve better test metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#37327;&#23376;&#39044;&#22788;&#29702;&#28388;&#27874;&#22120;&#65288;QPF&#65289;&#22312;&#20108;&#20540;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#22312;MNIST&#12289;EMNIST&#21644;CIFAR-10&#19978;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;GTSRB&#19978;&#38477;&#20302;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.14930</link><description>&lt;p&gt;
&#37327;&#23376;&#39044;&#22788;&#29702;&#28388;&#27874;&#22120;&#22312;&#23567;&#26679;&#26412;&#20108;&#20540;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Quantum Pre-Processing Filter for Binary Image Classification with Small Samples. (arXiv:2308.14930v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#37327;&#23376;&#39044;&#22788;&#29702;&#28388;&#27874;&#22120;&#65288;QPF&#65289;&#22312;&#20108;&#20540;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#22312;MNIST&#12289;EMNIST&#21644;CIFAR-10&#19978;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;GTSRB&#19978;&#38477;&#20302;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#26469;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#22312;&#30740;&#31350;&#20154;&#21592;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#25913;&#21464;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#24050;&#24320;&#21457;&#20986;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#29305;&#24615;&#30340;&#20960;&#31181;&#27169;&#22411;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25105;&#20204;&#20043;&#21069;&#25552;&#20986;&#30340;&#37327;&#23376;&#39044;&#22788;&#29702;&#28388;&#27874;&#22120;&#65288;QPF&#65289;&#22312;&#20108;&#20540;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;QPF&#30340;&#35780;&#20272;&#65306;MNIST&#65288;&#25163;&#20889;&#25968;&#23383;&#65289;&#12289;EMNIST&#65288;&#25163;&#20889;&#25968;&#23383;&#21644;&#23383;&#27597;&#65289;&#12289;CIFAR-10&#65288;&#29031;&#29255;&#22270;&#20687;&#65289;&#21644;GTSRB&#65288;&#30495;&#23454;&#20132;&#36890;&#26631;&#24535;&#22270;&#20687;&#65289;&#12290;&#19982;&#25105;&#20204;&#20043;&#21069;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#32467;&#26524;&#31867;&#20284;&#65292;&#24212;&#29992;QPF&#20351;&#24471;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;MNIST&#12289;EMNIST&#21644;CIFAR-10&#30340;&#20108;&#20540;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#29575;&#20998;&#21035;&#20174;98.9%&#25552;&#39640;&#21040;99.2%&#12289;&#20174;97.8%&#25552;&#39640;&#21040;98.3%&#21644;&#20174;71.2%&#25552;&#39640;&#21040;76.1%&#65292;&#20294;&#22312;GTSRB&#19978;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#20102;&#20174;93.5%&#21040;92.0%&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;QPF&#24212;&#29992;&#20110;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, there has been significant interest in Quantum Machine Learning (QML) among researchers, as it has the potential to transform the field of machine learning. Several models that exploit the properties of quantum mechanics have been developed for practical applications. In this study, we investigated the application of our previously proposed quantum pre-processing filter (QPF) to binary image classification. We evaluated the QPF on four datasets: MNIST (handwritten digits), EMNIST (handwritten digits and alphabets), CIFAR-10 (photographic images) and GTSRB (real-life traffic sign images). Similar to our previous multi-class classification results, the application of QPF improved the binary image classification accuracy using neural network against MNIST, EMNIST, and CIFAR-10 from 98.9% to 99.2%, 97.8% to 98.3%, and 71.2% to 76.1%, respectively, but degraded it against GTSRB from 93.5% to 92.0%. We then applied QPF in cases using a smaller number of training and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35757;&#32451;&#20998;&#35299;&#25581;&#31034;&#20302;&#31209;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#22823;&#27169;&#22411;&#26102;&#30340;&#32791;&#26102;&#21644;&#36164;&#28304;&#28040;&#32791;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.14929</link><description>&lt;p&gt;
Maestro: &#36890;&#36807;&#21487;&#35757;&#32451;&#20998;&#35299;&#25581;&#31034;&#20302;&#31209;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Maestro: Uncovering Low-Rank Structures via Trainable Decomposition. (arXiv:2308.14929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35757;&#32451;&#20998;&#35299;&#25581;&#31034;&#20302;&#31209;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#22823;&#27169;&#22411;&#26102;&#30340;&#32791;&#26102;&#21644;&#36164;&#28304;&#28040;&#32791;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36817;&#24180;&#26469;&#25512;&#21160;&#21644;&#20419;&#25104;&#20102;&#20154;&#24037;&#26234;&#33021;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;&#20026;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#24212;&#23545;&#26032;&#20852;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#21253;&#25324;AR/VR&#21644;&#26234;&#33021;&#21161;&#25163;&#65292;&#36825;&#20123;&#27169;&#22411;&#36234;&#26469;&#36234;&#22823;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22823;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#32791;&#26102;&#36153;&#21147;&#65292;&#36890;&#24120;&#21482;&#33021;&#29983;&#25104;&#19968;&#20010;&#36866;&#24212;&#25152;&#26377;&#30446;&#26631;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#27169;&#22411;&#26435;&#37325;&#21644;&#26356;&#26032;&#30340;&#21098;&#26525;&#12289;&#31232;&#30095;&#21270;&#25110;&#37327;&#21270;&#12290;&#34429;&#28982;&#21487;&#20197;&#23454;&#29616;&#39640;&#21387;&#32553;&#29575;&#65292;&#20294;&#24448;&#24448;&#20250;&#36896;&#25104;&#35745;&#31639;&#24320;&#38144;&#25110;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#21478;&#22806;&#65292;&#36824;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#26041;&#27861;&#23558;&#20302;&#31209;&#21387;&#32553;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;(&#20363;&#22914;SVD)&#24120;&#24120;&#20381;&#36182;&#20110;&#35745;&#31639;&#26114;&#36149;&#30340;&#23618;&#27425;&#20998;&#35299;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#22914;DNNs&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#20998;&#35299;&#25581;&#31034;&#20302;&#31209;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have been a large driver and enabler for AI breakthroughs in recent years. These models have been getting larger in their attempt to become more accurate and tackle new upcoming use-cases, including AR/VR and intelligent assistants. However, the training process of such large models is a costly and time-consuming process, which typically yields a single model to fit all targets. To mitigate this, various techniques have been proposed in the literature, including pruning, sparsification or quantization of the model weights and updates. While able to achieve high compression rates, they often incur computational overheads or accuracy penalties. Alternatively, factorization methods have been leveraged to incorporate low-rank compression in the training process. Similarly, such techniques (e.g.,~SVD) frequently rely on the computationally expensive decomposition of layers and are potentially sub-optimal for non-linear models, such as DNNs. In this work, we take 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#35199;&#38376;&#23376;&#33021;&#28304;&#25552;&#20379;&#30340;&#28909;&#21147;&#23398;&#36719;&#20214;&#32435;&#20837;&#29615;&#22659;&#27169;&#22411;&#65292;&#24182;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#65292;&#25581;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#32463;&#27982;&#29123;&#27668;&#36718;&#26426;&#35843;&#24230;&#20248;&#21270;&#30340;&#22909;&#22788;&#65292;&#24182;&#21457;&#29616;Deep Q-Networks (DQN) &#22312;&#31639;&#27861;&#21644;&#22522;&#20934;&#26041;&#27861;&#20013;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2308.14924</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#32463;&#27982;&#29123;&#27668;&#36718;&#26426;&#35843;&#24230;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimal Economic Gas Turbine Dispatch with Deep Reinforcement Learning. (arXiv:2308.14924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#35199;&#38376;&#23376;&#33021;&#28304;&#25552;&#20379;&#30340;&#28909;&#21147;&#23398;&#36719;&#20214;&#32435;&#20837;&#29615;&#22659;&#27169;&#22411;&#65292;&#24182;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#65292;&#25581;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#32463;&#27982;&#29123;&#27668;&#36718;&#26426;&#35843;&#24230;&#20248;&#21270;&#30340;&#22909;&#22788;&#65292;&#24182;&#21457;&#29616;Deep Q-Networks (DQN) &#22312;&#31639;&#27861;&#21644;&#22522;&#20934;&#26041;&#27861;&#20013;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#30005;&#21147;&#32593;&#32476;&#20013;&#65292;&#29123;&#27668;&#36718;&#26426;&#30340;&#35843;&#24230;&#31574;&#30053;&#27491;&#22312;&#21457;&#29983;&#21464;&#21270;&#12290;&#19982;&#38388;&#27463;&#24615;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#26085;&#30410;&#34701;&#20837;&#30456;&#27604;&#65292;&#29123;&#27668;&#36718;&#26426;&#38656;&#35201;&#26356;&#39057;&#32321;&#22320;&#20197;&#26356;&#30701;&#21608;&#26399;&#21644;&#37096;&#20998;&#36127;&#36733;&#36816;&#34892;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#21487;&#20197;&#24212;&#23545;&#36825;&#31181;&#21457;&#23637;&#24182;&#22312;&#32463;&#27982;&#19978;&#35843;&#24230;&#29123;&#27668;&#36718;&#26426;&#30340;&#24037;&#20855;&#12290;DRL&#30340;&#20027;&#35201;&#20248;&#21183;&#26159;&#26080;&#27169;&#22411;&#20248;&#21270;&#21644;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#65292;&#27604;&#22914;&#30001;&#19981;&#21516;&#36127;&#36733;&#25110;&#21487;&#20877;&#29983;&#33021;&#28304;&#20135;&#29983;&#30340;&#21464;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;DRL&#31639;&#27861;&#26469;&#35299;&#20915;&#21152;&#25343;&#22823;&#38463;&#23572;&#20271;&#22612;&#30465;&#30340;&#32463;&#27982;&#29123;&#27668;&#36718;&#26426;&#35843;&#24230;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#35199;&#38376;&#23376;&#33021;&#28304;&#25552;&#20379;&#30340;&#29616;&#26377;&#28909;&#21147;&#23398;&#36719;&#20214;&#32435;&#20837;&#29615;&#22659;&#27169;&#22411;&#24182;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#65288;&#22914;&#30005;&#20215;&#12289;&#36127;&#36733;&#21644;&#29615;&#22659;&#26465;&#20214;&#30340;&#21464;&#21270;&#65289;&#26469;&#20984;&#26174;DRL&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dispatching strategies for gas turbines (GTs) are changing in modern electricity grids. A growing incorporation of intermittent renewable energy requires GTs to operate more but shorter cycles and more frequently on partial loads. Deep reinforcement learning (DRL) has recently emerged as a tool that can cope with this development and dispatch GTs economically. The key advantages of DRL are a model-free optimization and the ability to handle uncertainties, such as those introduced by varying loads or renewable energy production. In this study, three popular DRL algorithms are implemented for an economic GT dispatch problem on a case study in Alberta, Canada. We highlight the benefits of DRL by incorporating an existing thermodynamic software provided by Siemens Energy into the environment model and by simulating uncertainty via varying electricity prices, loads, and ambient conditions. Among the tested algorithms and baseline methods, Deep Q-Networks (DQN) obtained the highest rewards w
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#65292;&#23427;&#20204;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#19982;&#20010;&#20154;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#19968;&#33268;&#30340;&#32844;&#19994;&#65292;&#24182;&#19988;&#25918;&#22823;&#20102;&#20559;&#35265;&#65292;&#36229;&#36807;&#20102;&#29616;&#23454;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.14921</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Gender bias and stereotypes in Large Language Models. (arXiv:2308.14921v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14921
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#65292;&#23427;&#20204;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#19982;&#20010;&#20154;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#19968;&#33268;&#30340;&#32844;&#19994;&#65292;&#24182;&#19988;&#25918;&#22823;&#20102;&#20559;&#35265;&#65292;&#36229;&#36807;&#20102;&#29616;&#23454;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#20010;&#26376;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25171;&#30772;&#20102;&#26368;&#20808;&#36827;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#26412;&#25991;&#30740;&#31350;LLMs&#22312;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#30340;&#34892;&#20026;&#65292;&#36825;&#26159;&#20808;&#21069;&#27169;&#22411;&#20013;&#24050;&#30693;&#30340;&#19968;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#33539;&#20363;&#26469;&#27979;&#35797;&#24615;&#21035;&#20559;&#35265;&#30340;&#23384;&#22312;&#65292;&#36825;&#19968;&#33539;&#20363;&#24314;&#31435;&#22312;&#20294;&#19982;WinoBias&#19981;&#21516;&#65292;&#21518;&#32773;&#26159;&#19968;&#20010;&#24120;&#29992;&#30340;&#24615;&#21035;&#20559;&#35265;&#25968;&#25454;&#38598;&#65292;&#24456;&#21487;&#33021;&#21253;&#21547;&#22312;&#30446;&#21069;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#22235;&#20010;&#26368;&#36817;&#21457;&#24067;&#30340;LLMs&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#22312;&#30007;&#24615;&#21644;&#22899;&#24615;&#32844;&#19994;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#20559;&#35265;&#30340;&#20551;&#35774;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22914;&#19979;&#65306;&#65288;a&#65289;LLMs&#22312;&#36873;&#25321;&#19982;&#20154;&#30340;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#19968;&#33268;&#30340;&#32844;&#19994;&#26102;&#30340;&#27010;&#29575;&#26159;3-6&#20493;&#65307;&#65288;b&#65289;&#36825;&#20123;&#36873;&#25321;&#19982;&#20154;&#20204;&#30340;&#24863;&#30693;&#26356;&#21152;&#19968;&#33268;&#65292;&#32780;&#19981;&#26159;&#19982;&#23448;&#26041;&#32844;&#19994;&#32479;&#35745;&#25968;&#25454;&#30340;&#30495;&#23454;&#24773;&#20917;&#19968;&#33268;&#65307;&#65288;c&#65289;&#20107;&#23454;&#19978;&#65292;LLMs&#25918;&#22823;&#20102;&#20559;&#35265;&#65292;&#36229;&#36807;&#20102;&#20154;&#20204;&#30340;&#24863;&#30693;&#25110;&#30495;&#23454;&#24773;&#20917;&#65307;&#65288;d&#65289;LLMs&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#27495;&#20041;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs' behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women's occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person's gender; (b) these choices align with people's perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities 
&lt;/p&gt;</description></item><item><title>Matbench Discovery&#26159;&#19968;&#20010;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#26230;&#20307;&#31283;&#23450;&#24615;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#22312;&#28909;&#21147;&#23398;&#31283;&#23450;&#24615;&#39044;&#27979;&#26041;&#38754;&#30340;&#27979;&#35797;&#20013;&#65292;CHGNet&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2308.14920</link><description>&lt;p&gt;
Matbench Discovery - &#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#26230;&#20307;&#31283;&#23450;&#24615;&#39044;&#27979;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Matbench Discovery -- An evaluation framework for machine learning crystal stability prediction. (arXiv:2308.14920v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14920
&lt;/p&gt;
&lt;p&gt;
Matbench Discovery&#26159;&#19968;&#20010;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#26230;&#20307;&#31283;&#23450;&#24615;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#22312;&#28909;&#21147;&#23398;&#31283;&#23450;&#24615;&#39044;&#27979;&#26041;&#38754;&#30340;&#27979;&#35797;&#20013;&#65292;CHGNet&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Matbench Discovery&#36890;&#36807;&#27169;&#25311;&#26426;&#22120;&#23398;&#20064;&#33021;&#28304;&#27169;&#22411;&#22312;&#39640;&#36890;&#37327;&#25628;&#32034;&#31283;&#23450;&#26080;&#26426;&#26230;&#20307;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#28909;&#21147;&#23398;&#31283;&#23450;&#24615;&#21644;&#24418;&#25104;&#33021;&#20043;&#38388;&#30340;&#24046;&#24322;&#20197;&#21450;&#22495;&#20869;&#19982;&#22495;&#22806;&#24615;&#33021;&#20043;&#38388;&#30340;&#33073;&#33410;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;Python&#21253;&#65292;&#20197;&#20415;&#20110;&#26410;&#26469;&#27169;&#22411;&#30340;&#25552;&#20132;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#32447;&#25490;&#34892;&#27036;&#65292;&#36827;&#19968;&#27493;&#27934;&#23519;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#23545;&#28909;&#21147;&#23398;&#31283;&#23450;&#24615;&#39044;&#27979;&#30340;&#27979;&#35797;&#38598;F1&#24471;&#20998;&#36827;&#34892;&#25490;&#21517;&#65292;&#25105;&#20204;&#21457;&#29616;CHGNet &gt; M3GNet &gt; MACE &gt; ALIGNN &gt; MEGNet &gt; CGCNN &gt; CGCNN+P &gt; Wrenformer &gt; BOWSR &gt; Voronoi tessellation fingerprints with random forest&#12290;
&lt;/p&gt;
&lt;p&gt;
Matbench Discovery simulates the deployment of machine learning (ML) energy models in a high-throughput search for stable inorganic crystals. We address the disconnect between (i) thermodynamic stability and formation energy and (ii) in-domain vs out-of-distribution performance. Alongside this paper, we publish a Python package to aid with future model submissions and a growing online leaderboard with further insights into trade-offs between various performance metrics. To answer the question which ML methodology performs best at materials discovery, our initial release explores a variety of models including random forests, graph neural networks (GNN), one-shot predictors, iterative Bayesian optimizers and universal interatomic potentials (UIP). Ranked best-to-worst by their test set F1 score on thermodynamic stability prediction, we find CHGNet &gt; M3GNet &gt; MACE &gt; ALIGNN &gt; MEGNet &gt; CGCNN &gt; CGCNN+P &gt; Wrenformer &gt; BOWSR &gt; Voronoi tessellation fingerprints with random forest. The top 3 mod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22120;&#29992;&#20110;&#20272;&#35745;&#21333;&#20010;&#29366;&#24577;&#20540;&#65292;&#24182;&#36890;&#36807;&#26681;&#25454;&#22870;&#21169;&#20195;&#26367;&#24120;&#29992;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#24120;&#25968;&#65292;&#25552;&#20379;&#20102;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#25216;&#24039;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.14919</link><description>&lt;p&gt;
&#35770;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22870;&#21169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
On Reward Structures of Markov Decision Processes. (arXiv:2308.14919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22120;&#29992;&#20110;&#20272;&#35745;&#21333;&#20010;&#29366;&#24577;&#20540;&#65292;&#24182;&#36890;&#36807;&#26681;&#25454;&#22870;&#21169;&#20195;&#26367;&#24120;&#29992;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#24120;&#25968;&#65292;&#25552;&#20379;&#20102;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#25216;&#24039;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#36716;&#31227;&#26680;&#19982;&#22870;&#21169;&#20989;&#25968;&#21442;&#25968;&#21270;&#12290;&#36825;&#20004;&#20010;&#22240;&#32032;&#22312;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#27491;&#22914;&#23427;&#20204;&#22312;&#36125;&#23572;&#26364;&#26041;&#31243;&#20013;&#30340;&#23384;&#22312;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#38024;&#23545;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#20851;&#30340;&#21508;&#31181;"&#25104;&#26412;"&#65292;&#22870;&#21169;&#26159;&#29702;&#35299;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#32467;&#26500;&#30340;&#26680;&#24515;&#65292;&#22870;&#21169;&#20013;&#24515;&#27010;&#24565;&#21487;&#20197;&#38416;&#26126;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#27010;&#24565;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31574;&#30053;&#35780;&#20272;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#20854;&#23454;&#20363;&#29305;&#23450;&#30340;&#35823;&#24046;&#30028;&#20026;$\tilde{O}(\sqrt{\frac{\tau_s}{n}})$&#65292;&#29992;&#20110;&#20272;&#35745;&#21333;&#20010;&#29366;&#24577;&#20540;&#12290;&#22312;&#22312;&#32447;&#36951;&#25022;&#26368;&#23567;&#21270;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#36716;&#31227;&#30340;MDP&#24120;&#25968;&#65292;&#30452;&#24452;&#65292;&#25913;&#36827;&#20026;&#22522;&#20110;&#22870;&#21169;&#30340;&#24120;&#25968;&#65292;&#26368;&#22823;&#39044;&#26399;&#21040;&#36798;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#35813;&#24120;&#25968;&#20026;&#19968;&#31181;&#24191;&#20026;&#20154;&#30693;&#30340;&#25216;&#26415;&#65292;&#22522;&#20110;&#28508;&#21147;&#30340;&#22870;&#21169;&#24418;&#29366;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Markov decision process can be parameterized by a transition kernel and a reward function. Both play essential roles in the study of reinforcement learning as evidenced by their presence in the Bellman equations. In our inquiry of various kinds of ``costs'' associated with reinforcement learning inspired by the demands in robotic applications, rewards are central to understanding the structure of a Markov decision process and reward-centric notions can elucidate important concepts in reinforcement learning. Specifically, we studied the sample complexity of policy evaluation and developed a novel estimator with an instance-specific error bound of $\tilde{O}(\sqrt{\frac{\tau_s}{n}})$ for estimating a single state value. Under the online regret minimization setting, we refined the transition-based MDP constant, diameter, into a reward-based constant, maximum expected hitting cost, and with it, provided a theoretical explanation for how a well-known technique, potential-based reward shap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#24615;&#34917;&#25937;&#26694;&#26550;&#65292;&#29992;&#20110;&#24110;&#21161;&#29702;&#35299;&#25512;&#33616;&#31995;&#32479;&#30340;&#27169;&#22411;&#24182;&#20462;&#25913;&#25512;&#33616;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.14916</link><description>&lt;p&gt;
RecRec: &#25512;&#33616;&#31995;&#32479;&#30340;&#31639;&#27861;&#24615;&#34917;&#25937;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
RecRec: Algorithmic Recourse for Recommender Systems. (arXiv:2308.14916v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#24615;&#34917;&#25937;&#26694;&#26550;&#65292;&#29992;&#20110;&#24110;&#21161;&#29702;&#35299;&#25512;&#33616;&#31995;&#32479;&#30340;&#27169;&#22411;&#24182;&#20462;&#25913;&#25512;&#33616;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#23089;&#20048;&#12289;&#36141;&#29289;&#12289;&#39135;&#29289;&#12289;&#26032;&#38395;&#12289;&#23601;&#19994;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#30340;&#20915;&#31574;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36825;&#20123;&#25512;&#33616;&#31995;&#32479;&#32972;&#21518;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20110;&#29992;&#25143;&#12289;&#20869;&#23481;&#25552;&#20379;&#32773;&#21644;&#31995;&#32479;&#24320;&#21457;&#32773;&#26469;&#35828;&#36890;&#24120;&#37117;&#26159;&#24040;&#22823;&#19988;&#19981;&#36879;&#26126;&#30340;&#12290;&#23545;&#20110;&#25152;&#26377;&#21033;&#30410;&#30456;&#20851;&#32773;&#26469;&#35828;&#65292;&#29702;&#35299;&#27169;&#22411;&#22312;&#36827;&#34892;&#26576;&#20123;&#39044;&#27979;&#21644;&#25512;&#33616;&#26102;&#30340;&#21407;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#37027;&#20123;&#29983;&#35745;&#20381;&#36182;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#20869;&#23481;&#25552;&#20379;&#32773;&#26469;&#35828;&#23588;&#20854;&#22914;&#27492;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#20174;&#23454;&#38469;&#38656;&#27714;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#20869;&#23481;&#25552;&#20379;&#32773;&#30340;&#25512;&#33616;&#31995;&#32479;&#34917;&#25937;&#26694;&#26550;&#12290;&#25512;&#33616;&#35774;&#32622;&#20013;&#30340;&#31639;&#27861;&#24615;&#34917;&#25937;&#26159;&#19968;&#32452;&#25805;&#20316;&#65292;&#22914;&#26524;&#25191;&#34892;&#65292;&#23558;&#20197;&#26399;&#26395;&#30340;&#26041;&#24335;&#20462;&#25913;&#39033;&#30446;&#30340;&#25512;&#33616;&#65288;&#25110;&#25490;&#24207;&#65289;&#12290;&#34917;&#25937;&#25514;&#26045;&#25552;&#20379;&#30340;&#25805;&#20316;&#24418;&#24335;&#20026;&#65306;&#8220;&#22914;&#26524;&#19968;&#20010;&#29305;&#24449;&#20174;X&#21464;&#20026;Y&#65292;&#37027;&#20040;&#35813;&#39033;&#30446;&#30340;&#25490;&#21517;&#20063;&#20250;&#30456;&#24212;&#21464;&#21270;&#12290;&#8221;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play an essential role in the choices people make in domains such as entertainment, shopping, food, news, employment, and education. The machine learning models underlying these recommender systems are often enormously large and black-box in nature for users, content providers, and system developers alike. It is often crucial for all stakeholders to understand the model's rationale behind making certain predictions and recommendations. This is especially true for the content providers whose livelihoods depend on the recommender system. Drawing motivation from the practitioners' need, in this work, we propose a recourse framework for recommender systems, targeted towards the content providers. Algorithmic recourse in the recommendation setting is a set of actions that, if executed, would modify the recommendations (or ranking) of an item in the desired manner. A recourse suggests actions of the form: "if a feature changes X to Y, then the ranking of that item for a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21098;&#26525;&#26041;&#27861;&#26469;&#25552;&#39640;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#21098;&#26525;&#33258;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#22810;&#20313;&#36830;&#25509;&#65292;&#24182;&#37319;&#29992;&#21487;&#24494;&#30340;&#21098;&#26525;&#26041;&#27861;&#20197;&#33258;&#21160;&#23398;&#20064;&#21098;&#26525;&#38408;&#20540;&#12290;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;&#38646;&#21796;&#37266;&#22810;&#35828;&#35805;&#32773;TTS&#20013;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14909</link><description>&lt;p&gt;
&#21098;&#26525;&#33258;&#27880;&#24847;&#21147;&#23454;&#29616;&#38646;&#21796;&#37266;&#22810;&#35828;&#35805;&#32773;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech. (arXiv:2308.14909v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21098;&#26525;&#26041;&#27861;&#26469;&#25552;&#39640;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#21098;&#26525;&#33258;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#22810;&#20313;&#36830;&#25509;&#65292;&#24182;&#37319;&#29992;&#21487;&#24494;&#30340;&#21098;&#26525;&#26041;&#27861;&#20197;&#33258;&#21160;&#23398;&#20064;&#21098;&#26525;&#38408;&#20540;&#12290;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;&#38646;&#21796;&#37266;&#22810;&#35828;&#35805;&#32773;TTS&#20013;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#38899;&#29983;&#25104;&#65292;&#24517;&#39035;&#22312;&#38480;&#23450;&#30340;&#30446;&#26631;&#35828;&#35805;&#32773;&#25968;&#25454;&#20013;&#25104;&#21151;&#23454;&#29616;&#31070;&#32463;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#22522;&#32447;TTS&#27169;&#22411;&#38656;&#35201;&#24191;&#27867;&#36866;&#29992;&#20110;&#39046;&#22495;&#22806;&#30340;&#25968;&#25454;&#65288;&#21363;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#35821;&#38899;&#65289;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;TTS&#20013;&#36825;&#20010;&#39046;&#22495;&#22806;&#27867;&#21270;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#23578;&#26410;&#34987;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#27880;&#24847;&#21147;&#65292;&#20197;&#25552;&#39640;TTS&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21098;&#26525;&#33258;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#22810;&#20313;&#36830;&#25509;&#65292;&#20854;&#27880;&#24847;&#21147;&#26435;&#37325;&#20302;&#20110;&#38408;&#20540;&#12290;&#20026;&#20102;&#28789;&#27963;&#22320;&#30830;&#23450;&#21098;&#26525;&#24378;&#24230;&#20197;&#25628;&#32034;&#26368;&#20339;&#30340;&#27867;&#21270;&#31243;&#24230;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#38408;&#20540;&#12290;&#23545;&#38646;&#21796;&#37266;&#22810;&#35828;&#35805;&#32773;TTS&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22768;&#38899;&#36136;&#37327;&#21644;&#35828;&#35805;&#32773;&#30456;&#20284;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For personalized speech generation, a neural text-to-speech (TTS) model must be successfully implemented with limited data from a target speaker. To this end, the baseline TTS model needs to be amply generalized to out-of-domain data (i.e., target speaker's speech). However, approaches to address this out-of-domain generalization problem in TTS have yet to be thoroughly studied. In this work, we propose an effective pruning method for a transformer known as sparse attention, to improve the TTS model's generalization abilities. In particular, we prune off redundant connections from self-attention layers whose attention weights are below the threshold. To flexibly determine the pruning strength for searching optimal degree of generalization, we also propose a new differentiable pruning method that allows the model to automatically learn the thresholds. Evaluations on zero-shot multi-speaker TTS verify the effectiveness of our method in terms of voice quality and speaker similarity.
&lt;/p&gt;</description></item><item><title>BayOTIDE&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#20302;&#31209;&#26102;&#24207;&#22240;&#23376;&#32452;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#36827;&#34892;&#25554;&#34917;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20840;&#23616;&#36235;&#21183;&#21644;&#21608;&#26399;&#24615;&#27169;&#24335;&#30340;&#24573;&#30053;&#20197;&#21450;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#22788;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.14906</link><description>&lt;p&gt;
BayOTIDE: &#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
BayOTIDE: Bayesian Online Multivariate Time series Imputation with functional decomposition. (arXiv:2308.14906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14906
&lt;/p&gt;
&lt;p&gt;
BayOTIDE&#26159;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#20302;&#31209;&#26102;&#24207;&#22240;&#23376;&#32452;&#30340;&#21152;&#26435;&#32452;&#21512;&#26469;&#36827;&#34892;&#25554;&#34917;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20840;&#23616;&#36235;&#21183;&#21644;&#21608;&#26399;&#24615;&#27169;&#24335;&#30340;&#24573;&#30053;&#20197;&#21450;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#22788;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#22914;&#20132;&#36890;&#21644;&#33021;&#28304;&#65292;&#32463;&#24120;&#35266;&#23519;&#21040;&#20855;&#26377;&#32570;&#22833;&#20540;&#21644;&#22122;&#22768;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#29978;&#33267;&#26159;&#19981;&#35268;&#21017;&#37319;&#26679;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25554;&#34917;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#25968;&#21482;&#36866;&#29992;&#20110;&#23616;&#37096;&#35270;&#35282;&#65292;&#21363;&#23558;&#38271;&#24207;&#21015;&#25286;&#20998;&#20026;&#36866;&#24403;&#22823;&#23567;&#30340;&#25209;&#27425;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#23616;&#37096;&#35270;&#35282;&#21487;&#33021;&#20351;&#27169;&#22411;&#24573;&#30053;&#20840;&#23616;&#36235;&#21183;&#25110;&#21608;&#26399;&#24615;&#27169;&#24335;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20960;&#20046;&#25152;&#26377;&#26041;&#27861;&#37117;&#20551;&#35774;&#35266;&#27979;&#20540;&#22312;&#35268;&#21017;&#30340;&#26102;&#38388;&#38388;&#38548;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#19988;&#26080;&#27861;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#24212;&#29992;&#30340;&#22797;&#26434;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22312;&#31163;&#32447;&#29366;&#24577;&#19979;&#36827;&#34892;&#23398;&#20064;&#30340;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#37027;&#20123;&#26377;&#24555;&#36895;&#21040;&#36798;&#30340;&#27969;&#25968;&#25454;&#30340;&#24212;&#29992;&#26469;&#35828;&#65292;&#23427;&#20204;&#24182;&#19981;&#21512;&#36866;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BayOTIDE&#65306;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#19982;&#20989;&#25968;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios like traffic and energy, massive time-series data with missing values and noises are widely observed, even sampled irregularly. While many imputation methods have been proposed, most of them work with a local horizon, which means models are trained by splitting the long sequence into batches of fit-sized patches. This local horizon can make models ignore global trends or periodic patterns. More importantly, almost all methods assume the observations are sampled at regular time stamps, and fail to handle complex irregular sampled time series arising from different applications. Thirdly, most existing methods are learned in an offline manner. Thus, it is not suitable for many applications with fast-arriving streaming data. To overcome these limitations, we propose \ours: Bayesian Online Multivariate Time series Imputation with functional decomposition. We treat the multivariate time series as the weighted combination of groups of low-rank temporal factors with dif
&lt;/p&gt;</description></item><item><title>MADBAL&#26159;&#19968;&#31181;&#25104;&#29087;&#24230;&#24863;&#30693;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#30340;&#26041;&#24335;&#32508;&#21512;&#32771;&#34385;&#20102;&#19981;&#21516;&#26679;&#26412;&#23450;&#20041;&#65292;&#33021;&#22815;&#36873;&#25321;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#20998;&#21106;&#20687;&#32032;&#12290;&#23427;&#36824;&#37319;&#29992;&#20102;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#36798;&#26041;&#24335;&#65292;&#24182;&#22312;&#26089;&#26399;&#23398;&#20064;&#38454;&#27573;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#20943;&#36731;&#20102;&#35757;&#32451;&#36127;&#25285;&#12290;&#22312;Cityscapes&#21644;PASCAL VOC&#25968;&#25454;&#38598;&#19978;&#65292;MADBAL&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.14904</link><description>&lt;p&gt;
&#20855;&#26377;&#20998;&#23618;&#33258;&#36866;&#24212;&#26679;&#26412;&#35780;&#20272;&#30340;&#35821;&#20041;&#20998;&#21106;&#30340;&#25104;&#29087;&#24230;&#24863;&#30693;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maturity-Aware Active Learning for Semantic Segmentation with Hierarchically-Adaptive Sample Assessment. (arXiv:2308.14904v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14904
&lt;/p&gt;
&lt;p&gt;
MADBAL&#26159;&#19968;&#31181;&#25104;&#29087;&#24230;&#24863;&#30693;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#30340;&#26041;&#24335;&#32508;&#21512;&#32771;&#34385;&#20102;&#19981;&#21516;&#26679;&#26412;&#23450;&#20041;&#65292;&#33021;&#22815;&#36873;&#25321;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#20998;&#21106;&#20687;&#32032;&#12290;&#23427;&#36824;&#37319;&#29992;&#20102;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#36798;&#26041;&#24335;&#65292;&#24182;&#22312;&#26089;&#26399;&#23398;&#20064;&#38454;&#27573;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#65292;&#20943;&#36731;&#20102;&#35757;&#32451;&#36127;&#25285;&#12290;&#22312;Cityscapes&#21644;PASCAL VOC&#25968;&#25454;&#38598;&#19978;&#65292;MADBAL&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#30340;&#20027;&#21160;&#23398;&#20064;(AL)&#30001;&#20110;&#20005;&#37325;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#19981;&#21516;&#26041;&#24335;&#23450;&#20041;&#30340;&#8220;&#26679;&#26412;&#8221;(&#20687;&#32032;&#12289;&#21306;&#22495;&#31561;)&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20351;&#24471;&#25968;&#25454;&#20998;&#24067;&#30340;&#35299;&#37322;&#27169;&#31946;&#19981;&#28165;&#12290;&#25105;&#20204;&#25552;&#20986;&#8220;&#25104;&#29087;&#24230;&#24863;&#30693;&#20998;&#24067;&#25286;&#20998;&#30340;&#20027;&#21160;&#23398;&#20064;&#8221;(MADBAL)&#65292;&#19968;&#31181;&#20174;&#20998;&#23618;&#30340;&#35282;&#24230;&#23450;&#20041;&#22810;&#35270;&#22270;&#25968;&#25454;&#20998;&#24067;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#32508;&#21512;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#8220;&#26679;&#26412;&#8221;&#23450;&#20041;&#65292;&#33021;&#22815;&#36873;&#25321;&#20855;&#26377;&#32508;&#21512;&#29702;&#35299;&#30340;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#20998;&#21106;&#20687;&#32032;&#12290;MADBAL&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#36798;&#26041;&#24335;&#65292;&#20854;&#20013;&#21253;&#25324;AL&#25903;&#25345;&#27169;&#22359;&#26469;&#24863;&#30693;&#29305;&#24615;&#30340;&#25104;&#29087;&#24230;&#65292;&#20854;&#21152;&#26435;&#24433;&#21709;&#25345;&#32493;&#36129;&#29486;&#20110;&#19981;&#30830;&#23450;&#24615;&#26816;&#27979;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;MADBAL&#21363;&#20351;&#22312;&#26089;&#26399;AL&#38454;&#27573;&#20063;&#33021;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#36731;&#20102;&#35757;&#32451;&#36127;&#25285;&#12290;&#25105;&#20204;&#22312;Cityscapes&#21644;PASCAL VOC&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;MADBAL&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Learning (AL) for semantic segmentation is challenging due to heavy class imbalance and different ways of defining "sample" (pixels, areas, etc.), leaving the interpretation of the data distribution ambiguous. We propose "Maturity-Aware Distribution Breakdown-based Active Learning'' (MADBAL), an AL method that benefits from a hierarchical approach to define a multiview data distribution, which takes into account the different "sample" definitions jointly, hence able to select the most impactful segmentation pixels with comprehensive understanding. MADBAL also features a novel uncertainty formulation, where AL supporting modules are included to sense the features' maturity whose weighted influence continuously contributes to the uncertainty detection. In this way, MADBAL makes significant performance leaps even in the early AL stage, hence reducing the training burden significantly. It outperforms state-of-the-art methods on Cityscapes and PASCAL VOC datasets as verified in our e
&lt;/p&gt;</description></item><item><title>Ad-Rec&#26159;&#19968;&#20010;&#21033;&#29992;&#39640;&#32423;&#29305;&#24449;&#20132;&#20114;&#25216;&#26415;&#35299;&#20915;&#25512;&#33616;&#32593;&#32476;&#20013;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25513;&#30721;&#36716;&#25442;&#22120;&#23454;&#29616;&#39640;&#38454;&#20132;&#21449;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#36136;&#37327;&#12289;&#21152;&#36895;&#20102;&#25910;&#25947;&#36895;&#24230;&#24182;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.14902</link><description>&lt;p&gt;
Ad-Rec: &#39640;&#32423;&#29305;&#24449;&#20132;&#20114;&#26469;&#35299;&#20915;&#25512;&#33616;&#32593;&#32476;&#20013;&#30340;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ad-Rec: Advanced Feature Interactions to Address Covariate-Shifts in Recommendation Networks. (arXiv:2308.14902v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14902
&lt;/p&gt;
&lt;p&gt;
Ad-Rec&#26159;&#19968;&#20010;&#21033;&#29992;&#39640;&#32423;&#29305;&#24449;&#20132;&#20114;&#25216;&#26415;&#35299;&#20915;&#25512;&#33616;&#32593;&#32476;&#20013;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25513;&#30721;&#36716;&#25442;&#22120;&#23454;&#29616;&#39640;&#38454;&#20132;&#21449;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#36136;&#37327;&#12289;&#21152;&#36895;&#20102;&#25910;&#25947;&#36895;&#24230;&#24182;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#25552;&#20379;&#20010;&#24615;&#21270;&#29992;&#25143;&#20307;&#39564;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#27169;&#22411;&#32463;&#24120;&#38754;&#20020;&#29992;&#25143;&#34892;&#20026;&#21644;&#29289;&#21697;&#29305;&#24449;&#19981;&#26029;&#21464;&#21270;&#23548;&#33268;&#30340;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#12290;&#22312;&#22788;&#29702;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#21644;&#36866;&#24212;&#29992;&#25143;&#34892;&#20026;&#21464;&#21270;&#26041;&#38754;&#65292;&#26377;&#25928;&#30340;&#29305;&#24449;&#20132;&#20114;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#29305;&#24449;&#20132;&#20114;&#25216;&#26415;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Ad-Rec&#65292;&#19968;&#20010;&#21033;&#29992;&#29305;&#24449;&#20132;&#20114;&#25216;&#26415;&#26469;&#35299;&#20915;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#30340;&#39640;&#32423;&#32593;&#32476;&#12290;&#36825;&#26377;&#21161;&#20110;&#28040;&#38500;&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#26080;&#20851;&#20132;&#20114;&#12290;Ad-Rec&#21033;&#29992;&#25513;&#30721;&#36716;&#25442;&#22120;&#26469;&#23454;&#29616;&#39640;&#38454;&#20132;&#21449;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#20943;&#36731;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;Area Under Curve&#65288;AUC&#65289;&#25351;&#26631;&#34913;&#37327;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#36136;&#37327;&#65292;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation models are vital in delivering personalized user experiences by leveraging the correlation between multiple input features. However, deep learning-based recommendation models often face challenges due to evolving user behaviour and item features, leading to covariate shifts. Effective cross-feature learning is crucial to handle data distribution drift and adapting to changing user behaviour. Traditional feature interaction techniques have limitations in achieving optimal performance in this context.  This work introduces Ad-Rec, an advanced network that leverages feature interaction techniques to address covariate shifts. This helps eliminate irrelevant interactions in recommendation tasks. Ad-Rec leverages masked transformers to enable the learning of higher-order cross-features while mitigating the impact of data distribution drift. Our approach improves model quality, accelerates convergence, and reduces training time, as measured by the Area Under Curve (AUC) metric.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#31574;&#30053;&#35780;&#20272;&#30340;&#31163;&#32447;&#24207;&#21015;&#24314;&#27169;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#35777;&#26126;&#20855;&#26377;&#26041;&#24046;&#32553;&#20943;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14897</link><description>&lt;p&gt;
&#39640;&#25928;&#32479;&#35745;&#26041;&#24046;&#32553;&#20943;&#30340;&#21452;&#31574;&#30053;&#35780;&#20272;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Statistically Efficient Variance Reduction with Double Policy Estimation for Off-Policy Evaluation in Sequence-Modeled Reinforcement Learning. (arXiv:2308.14897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#31574;&#30053;&#35780;&#20272;&#30340;&#31163;&#32447;&#24207;&#21015;&#24314;&#27169;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#35777;&#26126;&#20855;&#26377;&#26041;&#24046;&#32553;&#20943;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#20808;&#21069;&#25910;&#38598;&#30340;&#29615;&#22659;-&#21160;&#20316;&#20132;&#20114;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#26469;&#23398;&#20064;&#19968;&#20010;&#26080;&#38656;&#35775;&#38382;&#30495;&#23454;&#29615;&#22659;&#30340;&#31574;&#30053;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20915;&#31574;&#36716;&#25442;&#22120;&#31561;&#26041;&#27861;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#12290;&#23613;&#31649;&#36825;&#20123;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;&#22312;&#22238;&#25253;&#29575;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#22312;&#38656;&#35201;&#36739;&#38271;&#30340;&#22238;&#21512;&#25110;&#22238;&#25253;&#31232;&#32570;&#30340;&#20219;&#21153;&#19978;&#65292;&#20294;&#22312;&#22788;&#29702;&#31163;&#32447;&#25968;&#25454;&#26102;&#24182;&#26410;&#32771;&#34385;&#37325;&#35201;&#24615;&#37319;&#26679;&#26469;&#26657;&#27491;&#31574;&#30053;&#20559;&#24046;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#32570;&#20047;&#34892;&#20026;&#31574;&#30053;&#21644;&#20351;&#29992;&#30830;&#23450;&#24615;&#35780;&#20272;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DPE&#65306;&#19968;&#31181;&#23558;&#31163;&#32447;&#24207;&#21015;&#24314;&#27169;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#32479;&#35745;&#19978;&#35777;&#26126;&#20855;&#26377;&#26041;&#24046;&#32553;&#20943;&#24615;&#36136;&#30340;&#21452;&#31574;&#30053;&#35780;&#20272;&#65288;DPE&#65289;&#34701;&#21512;&#22312;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#30340;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning aims to utilize datasets of previously gathered environment-action interaction records to learn a policy without access to the real environment. Recent work has shown that offline reinforcement learning can be formulated as a sequence modeling problem and solved via supervised learning with approaches such as decision transformer. While these sequence-based methods achieve competitive results over return-to-go methods, especially on tasks that require longer episodes or with scarce rewards, importance sampling is not considered to correct the policy bias when dealing with off-policy data, mainly due to the absence of behavior policy and the use of deterministic evaluation policies. To this end, we propose DPE: an RL algorithm that blends offline sequence modeling and offline reinforcement learning with Double Policy Estimation (DPE) in a unified framework with statistically proven properties on variance reduction. We validate our method in multiple tasks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#20803;&#23398;&#20064;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#20803;&#23398;&#20064;&#22120;&#19978;&#24212;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31243;&#24207;&#65292;&#29983;&#25104;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14895</link><description>&lt;p&gt;
&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#19968;&#33268;&#24615;&#20803;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Conformal Meta-learners for Predictive Inference of Individual Treatment Effects. (arXiv:2308.14895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#20803;&#23398;&#20064;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#20803;&#23398;&#20064;&#22120;&#19978;&#24212;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31243;&#24207;&#65292;&#29983;&#25104;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25512;&#29702;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#33021;&#22815;&#25552;&#20379;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATE&#65289;&#30340;&#28857;&#20272;&#35745;&#30340;&#20803;&#23398;&#20064;&#22120;&#65307;&#36825;&#20123;&#26041;&#27861;&#26159;&#25226;&#20013;&#38388;&#30340;&#26080;&#20851;&#35201;&#32032;&#20272;&#35745;&#38598;&#21512;&#36215;&#26469;&#20135;&#29983;CATE&#20272;&#35745;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#20803;&#23398;&#20064;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;CATE&#20803;&#23398;&#20064;&#22120;&#20043;&#19978;&#24212;&#29992;&#26631;&#20934;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#31243;&#24207;&#26469;&#29983;&#25104;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#20851;&#27880;&#22522;&#20110;&#20004;&#38454;&#27573;&#20266;&#32467;&#26524;&#22238;&#24402;&#30340;&#24191;&#27867;&#31867;&#21035;&#30340;&#20803;&#23398;&#20064;&#22120;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#38543;&#26426;&#25490;&#24207;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#33268;&#24615;&#20803;&#23398;&#20064;&#22120;&#30340;&#25512;&#29702;&#22312;&#20854;&#65288;&#20266;&#32467;&#26524;&#65289;&#19968;&#33268;&#24615;&#24471;&#20998;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#19978;&#38543;&#26426;&#20248;&#20110;oracle&#19968;&#33268;&#24615;&#24471;&#20998;&#26102;&#26159;&#36793;&#38469;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24120;&#29992;&#30340;CATE&#20803;&#23398;&#20064;&#22120;...
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of machine learning-based (ML) predictive inference on individual treatment effects (ITEs). Previous work has focused primarily on developing ML-based meta-learners that can provide point estimates of the conditional average treatment effect (CATE); these are model-agnostic approaches for combining intermediate nuisance estimates to produce estimates of CATE. In this paper, we develop conformal meta-learners, a general framework for issuing predictive intervals for ITEs by applying the standard conformal prediction (CP) procedure on top of CATE meta-learners. We focus on a broad class of meta-learners based on two-stage pseudo-outcome regression and develop a stochastic ordering framework to study their validity. We show that inference with conformal meta-learners is marginally valid if their (pseudo outcome) conformity scores stochastically dominate oracle conformity scores evaluated on the unobserved ITEs. Additionally, we prove that commonly used CATE meta
&lt;/p&gt;</description></item><item><title>&#24403;&#21069;&#22270;&#20687;&#27169;&#22411;&#22312;&#35757;&#32451;&#26102;&#24120;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#20294;&#20854;&#22312;&#27867;&#21270;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;SCHaNe&#65292;&#36890;&#36807;&#21152;&#26435;&#22256;&#38590;&#30340;&#36127;&#26679;&#26412;&#25366;&#25496;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SCHaNe&#22312;&#21508;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;Top-1&#20934;&#30830;&#29575;&#20248;&#20110;BEiT-3&#12290;</title><link>http://arxiv.org/abs/2308.14893</link><description>&lt;p&gt;
&#24403;&#22256;&#38590;&#30340;&#36127;&#26679;&#26412;&#37319;&#26679;&#19982;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
When hard negative sampling meets supervised contrastive learning. (arXiv:2308.14893v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14893
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22270;&#20687;&#27169;&#22411;&#22312;&#35757;&#32451;&#26102;&#24120;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#20294;&#20854;&#22312;&#27867;&#21270;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;SCHaNe&#65292;&#36890;&#36807;&#21152;&#26435;&#22256;&#38590;&#30340;&#36127;&#26679;&#26412;&#25366;&#25496;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SCHaNe&#22312;&#21508;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;Top-1&#20934;&#30830;&#29575;&#20248;&#20110;BEiT-3&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#22270;&#20687;&#27169;&#22411;&#36890;&#24120;&#36981;&#24490;&#20004;&#38454;&#27573;&#31574;&#30053;&#65306;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#36827;&#34892;&#24494;&#35843;&#12290;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20132;&#21449;&#29109;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#27867;&#21270;&#21644;&#31283;&#23450;&#24615;&#12290;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#36890;&#36807;&#20851;&#27880;&#31867;&#20869;&#30456;&#20284;&#24615;&#21644;&#31867;&#38388;&#24046;&#24322;&#26469;&#35299;&#20915;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#19968;&#20123;&#38480;&#21046;&#65292;&#20294;&#23427;&#24573;&#35270;&#20102;&#22256;&#38590;&#36127;&#26679;&#26412;&#25366;&#25496;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#36890;&#36807;&#26681;&#25454;&#36127;&#26679;&#26412;&#19982;&#27491;&#26679;&#26412;&#30340;&#19981;&#30456;&#20284;&#31243;&#24230;&#36827;&#34892;&#21152;&#26435;&#65292;&#27169;&#22411;&#23558;&#20174;&#24615;&#33021;&#25913;&#36827;&#20013;&#21463;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;SCHaNe&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#24341;&#20837;&#20102;&#22256;&#38590;&#36127;&#26679;&#26412;&#37319;&#26679;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SCHaNe&#22312;&#21508;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;Top-1&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;BEiT-3&#65292;&#32780;&#26080;&#38656;&#19987;&#38376;&#30340;&#26550;&#26500;&#12289;&#39069;&#22806;&#30340;&#25968;&#25454;&#25110;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art image models predominantly follow a two-stage strategy: pre-training on large datasets and fine-tuning with cross-entropy loss. Many studies have shown that using cross-entropy can result in sub-optimal generalisation and stability. While the supervised contrastive loss addresses some limitations of cross-entropy loss by focusing on intra-class similarities and inter-class differences, it neglects the importance of hard negative mining. We propose that models will benefit from performance improvement by weighting negative samples based on their dissimilarity to positive counterparts. In this paper, we introduce a new supervised contrastive learning objective, SCHaNe, which incorporates hard negative sampling during the fine-tuning phase. Without requiring specialized architectures, additional data, or extra computational resources, experimental results indicate that SCHaNe outperforms the strong baseline BEiT-3 in Top-1 accuracy across various benchmarks, with signific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#32553;&#25918;&#26041;&#27861;CommunityFish&#65292;&#23427;&#21033;&#29992;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#22312;&#35789;&#31354;&#38388;&#19978;&#32858;&#31867;&#65292;&#20174;&#32780;&#25581;&#31034;&#25991;&#26412;&#25968;&#25454;&#20013;&#29420;&#31435;&#35789;&#32452;&#65288;&#31038;&#21306;&#65289;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.14873</link><description>&lt;p&gt;
CommunityFish: &#19968;&#31181;&#22522;&#20110;&#27850;&#26494;&#20998;&#24067;&#30340;&#23618;&#27425;&#32858;&#31867;&#25991;&#26412;&#32553;&#25918;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CommunityFish: A Poisson-based Document Scaling With Hierarchical Clustering. (arXiv:2308.14873v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#32553;&#25918;&#26041;&#27861;CommunityFish&#65292;&#23427;&#21033;&#29992;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#22312;&#35789;&#31354;&#38388;&#19978;&#32858;&#31867;&#65292;&#20174;&#32780;&#25581;&#31034;&#25991;&#26412;&#25968;&#25454;&#20013;&#29420;&#31435;&#35789;&#32452;&#65288;&#31038;&#21306;&#65289;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32553;&#25918;&#26159;&#31038;&#20250;&#31185;&#23398;&#23478;&#21644;&#25919;&#27835;&#30740;&#31350;&#20154;&#21592;&#22312;&#25991;&#26412;&#25968;&#25454;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20063;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#32553;&#25918;&#26041;&#27861;CommunityFish&#65292;&#23427;&#22522;&#20110;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;Louvain&#65292;&#22312;&#35789;&#31354;&#38388;&#19978;&#36827;&#34892;&#32858;&#31867;&#65292;&#36890;&#36807;&#35782;&#21035;&#20849;&#29616;&#22312;&#25991;&#26723;&#20013;&#30340;&#29420;&#31435;&#35789;&#32452;&#65288;&#31216;&#20026;&#31038;&#21306;&#65289;&#65292;&#20174;&#32780;&#25581;&#31034;&#28436;&#35762;&#32773;&#25110;&#25919;&#20826;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document scaling has been a key component in text-as-data applications for social scientists and a major field of interest for political researchers, who aim at uncovering differences between speakers or parties with the help of different probabilistic and non-probabilistic approaches. Yet, most of these techniques are either built upon the agnostically bag-of-word hypothesis or use prior information borrowed from external sources that might embed the results with a significant bias. If the corpus has long been considered as a collection of documents, it can also be seen as a dense network of connected words whose structure could be clustered to differentiate independent groups of words, based on their co-occurrences in documents, known as communities. This paper introduces CommunityFish as an augmented version of Wordfish based on a hierarchical clustering, namely the Louvain algorithm, on the word space to yield communities as semantic and independent n-grams emerging from the corpus
&lt;/p&gt;</description></item><item><title>NAS-X&#26159;&#19968;&#31181;&#22522;&#20110;&#25197;&#26354;&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;&#21796;&#37266;-&#30561;&#30496;&#31639;&#27861;&#26469;&#23398;&#20064;&#21644;&#25512;&#26029;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#25512;&#26029;&#21644;&#21442;&#25968;&#24674;&#22797;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.14864</link><description>&lt;p&gt;
NAS-X: &#22522;&#20110;&#25197;&#26354;&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NAS-X: Neural Adaptive Smoothing via Twisting. (arXiv:2308.14864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14864
&lt;/p&gt;
&lt;p&gt;
NAS-X&#26159;&#19968;&#31181;&#22522;&#20110;&#25197;&#26354;&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;&#21796;&#37266;-&#30561;&#30496;&#31639;&#27861;&#26469;&#23398;&#20064;&#21644;&#25512;&#26029;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#25512;&#26029;&#21644;&#21442;&#25968;&#24674;&#22797;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NAS-X&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#37325;&#26032;&#21152;&#26435;&#30340;&#21796;&#37266;-&#30561;&#30496;&#31639;&#27861;&#36827;&#34892;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#23398;&#20064;&#21644;&#25512;&#26029;&#12290;NAS-X&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#28508;&#21464;&#37327;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;SMC&#26041;&#27861;&#26469;&#25311;&#21512;&#27604;&#20256;&#32479;&#30340;&#37325;&#26032;&#21152;&#26435;&#21796;&#37266;-&#30561;&#30496;&#26041;&#27861;&#26356;&#24191;&#27867;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;NAS-X&#65292;&#24182;&#21457;&#29616;&#22312;&#25512;&#26029;&#21644;&#21442;&#25968;&#24674;&#22797;&#26041;&#38754;&#65292;&#23427;&#26126;&#26174;&#20248;&#20110;&#20808;&#21069;&#30340;&#21464;&#20998;&#21644;&#22522;&#20110;&#37325;&#26032;&#21152;&#26435;&#21796;&#37266;-&#30561;&#30496;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Neural Adaptive Smoothing via Twisting (NAS-X), a method for learning and inference in sequential latent variable models based on reweighted wake-sleep (RWS). NAS-X works with both discrete and continuous latent variables, and leverages smoothing SMC to fit a broader range of models than traditional RWS methods. We test NAS-X on discrete and continuous tasks and find that it substantially outperforms previous variational and RWS-based methods in inference and parameter recovery.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#29076;&#27744;&#22270;&#20687;&#27969;&#36827;&#34892;&#21360;&#21047;&#36712;&#36857;&#24322;&#24120;&#20998;&#31867;&#30340;&#20851;&#38190;&#26102;&#31354;&#23398;&#20064;&#22120;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#39046;&#20808;&#30340;&#28145;&#24230;&#26102;&#31354;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#36341;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.14861</link><description>&lt;p&gt;
&#35780;&#20272;&#20351;&#29992;&#29076;&#27744;&#22270;&#20687;&#27969;&#36827;&#34892;&#21360;&#21047;&#36712;&#36857;&#24322;&#24120;&#20998;&#31867;&#30340;&#20851;&#38190;&#26102;&#31354;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Key Spatiotemporal Learners for Print Track Anomaly Classification Using Melt Pool Image Streams. (arXiv:2308.14861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#29076;&#27744;&#22270;&#20687;&#27969;&#36827;&#34892;&#21360;&#21047;&#36712;&#36857;&#24322;&#24120;&#20998;&#31867;&#30340;&#20851;&#38190;&#26102;&#31354;&#23398;&#20064;&#22120;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#39046;&#20808;&#30340;&#28145;&#24230;&#26102;&#31354;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#36341;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#65288;MAM&#65289;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#35299;&#20915;&#26222;&#36941;&#37319;&#29992;MAM&#25216;&#26415;&#30340;&#20851;&#38190;&#38556;&#30861;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#24378;&#35843;&#20102;&#21033;&#29992;&#29076;&#27744;&#29305;&#24449;&#36827;&#34892;&#23454;&#26102;&#32570;&#38519;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#39640;&#36136;&#37327;&#30340;&#29076;&#27744;&#22270;&#20687;&#25968;&#25454;&#20855;&#26377;&#23454;&#29616;&#31934;&#30830;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#20294;&#23545;&#20110;&#33021;&#22815;&#21033;&#29992;&#22686;&#26448;&#21046;&#36896;&#36807;&#31243;&#30340;&#30636;&#24577;&#21644;&#26102;&#24207;&#29305;&#24449;&#30340;&#23574;&#31471;&#26102;&#31354;&#27169;&#22411;&#30340;&#21033;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#24182;&#23454;&#36341;&#20102;&#19968;&#20123;&#39046;&#20808;&#30340;&#28145;&#24230;&#26102;&#31354;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20998;&#31867;&#26469;&#33258;&#19981;&#21516;&#26448;&#26009;&#12289;&#31995;&#32479;&#21644;&#24212;&#29992;&#30340;&#29076;&#27744;&#22270;&#20687;&#27969;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#30740;&#31350;&#20102;&#30001;&#31354;&#38388;&#21644;&#26102;&#38388;&#27969;&#32452;&#25104;&#30340;&#20004;&#27969;&#32593;&#32476;&#12289;&#24490;&#29615;&#31354;&#38388;&#32593;&#32476;&#21644;&#22240;&#24335;&#20998;&#35299;3D&#21367;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent applications of machine learning in metal additive manufacturing (MAM) have demonstrated significant potential in addressing critical barriers to the widespread adoption of MAM technology. Recent research in this field emphasizes the importance of utilizing melt pool signatures for real-time defect prediction. While high-quality melt pool image data holds the promise of enabling precise predictions, there has been limited exploration into the utilization of cutting-edge spatiotemporal models that can harness the inherent transient and sequential characteristics of the additive manufacturing process. This research introduces and puts into practice some of the leading deep spatiotemporal learning models that can be adapted for the classification of melt pool image streams originating from various materials, systems, and applications. Specifically, it investigates two-stream networks comprising spatial and temporal streams, a recurrent spatial network, and a factorized 3D convoluti
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#27969;&#32858;&#31867;&#30340;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#26041;&#27861;SMOClust&#21487;&#20197;&#22312;&#28436;&#21270;&#25968;&#25454;&#27969;&#20013;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#27010;&#24565;&#28418;&#31227;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#22256;&#38590;&#22240;&#32032;&#26469;&#36866;&#24212;&#28418;&#31227;&#24182;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.14845</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#32858;&#31867;&#30340;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#26041;&#27861;SMOClust&#29992;&#20110;&#28436;&#21270;&#25968;&#25454;&#27969;
&lt;/p&gt;
&lt;p&gt;
SMOClust: Synthetic Minority Oversampling based on Stream Clustering for Evolving Data Streams. (arXiv:2308.14845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14845
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27969;&#32858;&#31867;&#30340;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#26041;&#27861;SMOClust&#21487;&#20197;&#22312;&#28436;&#21270;&#25968;&#25454;&#27969;&#20013;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#27010;&#24565;&#28418;&#31227;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#22256;&#38590;&#22240;&#32032;&#26469;&#36866;&#24212;&#28418;&#31227;&#24182;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#27969;&#24212;&#29992;&#19981;&#20165;&#38754;&#20020;&#27010;&#24565;&#28418;&#31227;&#30340;&#25361;&#25112;&#65292;&#36824;&#38754;&#20020;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#21516;&#26102;&#25506;&#32034;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#23398;&#20064;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#27969;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#25968;&#25454;&#22256;&#38590;&#22240;&#32032;&#65292;&#32780;&#36825;&#20123;&#22240;&#32032;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#27969;&#20013;&#26159;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#28418;&#31227;&#30340;&#36807;&#37319;&#26679;&#31574;&#30053;&#65292;&#26681;&#25454;&#27969;&#32858;&#31867;&#26469;&#21512;&#25104;&#23569;&#25968;&#31867;&#26679;&#26412;&#12290;&#21160;&#26426;&#26159;&#27969;&#32858;&#31867;&#26041;&#27861;&#19981;&#26029;&#26356;&#26032;&#33258;&#36523;&#20197;&#21453;&#26144;&#24403;&#21069;&#28508;&#22312;&#27010;&#24565;&#30340;&#29305;&#24615;&#65292;&#21253;&#25324;&#25968;&#25454;&#22256;&#38590;&#22240;&#32032;&#12290;&#36825;&#20010;&#29305;&#24615;&#21487;&#20197;&#20351;&#29992;&#21387;&#32553;&#36807;&#21435;&#20449;&#24687;&#26469;&#29983;&#25104;&#26368;&#26032;&#23569;&#25968;&#31867;&#26679;&#26412;&#21306;&#22495;&#20869;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#32780;&#19981;&#38656;&#35201;&#26174;&#24335;&#22320;&#32531;&#23384;&#25968;&#25454;&#12290;&#36890;&#36807;&#20154;&#24037;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#27969;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world data stream applications not only suffer from concept drift but also class imbalance. Yet, very few existing studies investigated this joint challenge. Data difficulty factors, which have been shown to be key challenges in class imbalanced data streams, are not taken into account by existing approaches when learning class imbalanced data streams. In this work, we propose a drift adaptable oversampling strategy to synthesise minority class examples based on stream clustering. The motivation is that stream clustering methods continuously update themselves to reflect the characteristics of the current underlying concept, including data difficulty factors. This nature can potentially be used to compress past information without caching data in the memory explicitly. Based on the compressed information, synthetic examples can be created within the region that recently generated new minority class examples. Experiments with artificial and real-world data streams show that the
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#32908;&#30005;&#22270;&#35774;&#22791;&#27979;&#37327;&#12289;&#24314;&#27169;&#21644;&#39044;&#27979;&#20102;VR&#29992;&#25143;&#22312;&#19982;&#34394;&#25311;&#29615;&#22659;&#20132;&#20114;&#26102;&#30340;&#39048;&#37096;&#32908;&#32905;&#25910;&#32553;&#27700;&#24179;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#29289;&#29289;&#29702;&#21551;&#21457;&#24335;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#22836;&#37096;&#36816;&#21160;&#29366;&#24577;&#19979;&#30340;&#39048;&#37096;&#25910;&#32553;&#27700;&#24179;&#65292;&#24182;&#19988;&#21487;&#20197;&#39044;&#27979;&#20165;&#36890;&#36807;&#30446;&#26631;&#22836;&#37096;&#23039;&#21183;&#30340;&#28508;&#22312;&#25910;&#32553;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.14841</link><description>&lt;p&gt;
&#20248;&#21270;&#34394;&#25311;&#29616;&#23454;/&#22686;&#24378;&#29616;&#23454;&#20154;&#26426;&#24037;&#31243;&#23398;&#65306;&#24314;&#27169;&#21644;&#39044;&#27979;&#29992;&#25143;&#39048;&#37096;&#32908;&#32905;&#25910;&#32553;
&lt;/p&gt;
&lt;p&gt;
Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck Muscle Contraction. (arXiv:2308.14841v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14841
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#32908;&#30005;&#22270;&#35774;&#22791;&#27979;&#37327;&#12289;&#24314;&#27169;&#21644;&#39044;&#27979;&#20102;VR&#29992;&#25143;&#22312;&#19982;&#34394;&#25311;&#29615;&#22659;&#20132;&#20114;&#26102;&#30340;&#39048;&#37096;&#32908;&#32905;&#25910;&#32553;&#27700;&#24179;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#29289;&#29289;&#29702;&#21551;&#21457;&#24335;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#19981;&#21516;&#22836;&#37096;&#36816;&#21160;&#29366;&#24577;&#19979;&#30340;&#39048;&#37096;&#25910;&#32553;&#27700;&#24179;&#65292;&#24182;&#19988;&#21487;&#20197;&#39044;&#27979;&#20165;&#36890;&#36807;&#30446;&#26631;&#22836;&#37096;&#23039;&#21183;&#30340;&#28508;&#22312;&#25910;&#32553;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#35268;&#27169;&#21644;&#38271;&#26399;&#37319;&#29992;VR/AR&#20307;&#39564;&#26469;&#35828;&#65292;&#20154;&#20307;&#24037;&#31243;&#23398;&#25928;&#29575;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#34429;&#28982;VR/AR&#22836;&#25140;&#26174;&#31034;&#22120;&#22312;&#35266;&#30475;&#26102;&#35299;&#38145;&#20102;&#29992;&#25143;&#33258;&#28982;&#30340;&#24191;&#27867;&#22836;&#37096;&#36816;&#21160;&#65292;&#20294;&#30001;&#20110;&#22686;&#21152;&#30340;&#30828;&#20214;&#37325;&#37327;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#24433;&#21709;&#20102;&#20182;&#20204;&#30340;&#39048;&#37096;&#32908;&#32905;&#33298;&#36866;&#24230;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#22810;&#23569;&#37327;&#21270;&#30340;&#30693;&#35782;&#26469;&#29702;&#35299;&#21644;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21033;&#29992;&#32908;&#30005;&#22270;&#35774;&#22791;&#65292;&#25105;&#20204;&#27979;&#37327;&#12289;&#24314;&#27169;&#21644;&#39044;&#27979;VR&#29992;&#25143;&#22312;&#19982;&#34394;&#25311;&#29615;&#22659;&#20132;&#20114;&#26102;&#22836;&#37096;&#36816;&#21160;&#26102;&#30340;&#39048;&#37096;&#32908;&#32905;&#25910;&#32553;&#27700;&#24179;&#65288;MCL&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#23398;&#20064;&#25910;&#38598;&#30340;&#29983;&#29702;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#29289;&#29289;&#29702;&#21551;&#21457;&#24335;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#22312;&#19981;&#21516;&#22836;&#37096;&#36816;&#21160;&#29366;&#24577;&#19979;&#30340;&#39048;&#37096;MCL&#12290;&#38500;&#20102;&#37327;&#21270;&#23436;&#25104;&#30340;&#22836;&#37096;&#36816;&#21160;&#30340;&#32047;&#31215;MCL&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#39044;&#27979;&#20165;&#36890;&#36807;&#30446;&#26631;&#22836;&#37096;&#23039;&#21183;&#30340;&#28508;&#22312;MCL&#38656;&#27714;&#12290;&#19968;&#31995;&#21015;&#23458;&#35266;&#35780;&#20272;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ergonomic efficiency is essential to the mass and prolonged adoption of VR/AR experiences. While VR/AR head-mounted displays unlock users' natural wide-range head movements during viewing, their neck muscle comfort is inevitably compromised by the added hardware weight. Unfortunately, little quantitative knowledge for understanding and addressing such an issue is available so far.  Leveraging electromyography devices, we measure, model, and predict VR users' neck muscle contraction levels (MCL) while they move their heads to interact with the virtual environment. Specifically, by learning from collected physiological data, we develop a bio-physically inspired computational model to predict neck MCL under diverse head kinematic states. Beyond quantifying the cumulative MCL of completed head movements, our model can also predict potential MCL requirements with target head poses only. A series of objective evaluations and user studies demonstrate its prediction accuracy and generality, as
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#19981;&#24179;&#34913;&#20998;&#31867;&#20013;&#22810;&#26679;&#30340;&#23569;&#25968;&#32676;&#20307;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#23569;&#25968;&#21644;&#22810;&#25968;&#32676;&#20307;&#30340;&#25968;&#25454;&#26679;&#26412;&#26469;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#20998;&#25955;&#20998;&#24067;&#30340;&#23569;&#25968;&#32676;&#20307;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.14838</link><description>&lt;p&gt;
&#24212;&#23545;&#19981;&#24179;&#34913;&#20998;&#31867;&#20013;&#22810;&#26679;&#30340;&#23569;&#25968;&#32676;&#20307;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling Diverse Minorities in Imbalanced Classification. (arXiv:2308.14838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14838
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#19981;&#24179;&#34913;&#20998;&#31867;&#20013;&#22810;&#26679;&#30340;&#23569;&#25968;&#32676;&#20307;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#23569;&#25968;&#21644;&#22810;&#25968;&#32676;&#20307;&#30340;&#25968;&#25454;&#26679;&#26412;&#26469;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#20998;&#25955;&#20998;&#24067;&#30340;&#23569;&#25968;&#32676;&#20307;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#22312;&#35757;&#32451;&#20998;&#31867;&#22120;&#26102;&#20250;&#24102;&#26469;&#37325;&#22823;&#25361;&#25112;&#12290;&#24403;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#65292;&#19981;&#24179;&#34913;&#38382;&#39064;&#21487;&#33021;&#36827;&#19968;&#27493;&#24694;&#21270;&#65292;&#20351;&#24471;&#26377;&#25928;&#35757;&#32451;&#20998;&#31867;&#22120;&#21464;&#24471;&#24322;&#24120;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#36807;&#37319;&#26679;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#23569;&#25968;&#32676;&#20307;&#21644;&#20854;&#37051;&#23621;&#20043;&#38388;&#32447;&#24615;&#25554;&#20540;&#25968;&#25454;&#23454;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20363;&#22914;&#24322;&#24120;&#26816;&#27979;&#65292;&#23569;&#25968;&#32676;&#20307;&#23454;&#20363;&#36890;&#24120;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#20998;&#25955;&#20998;&#24067;&#32780;&#19981;&#26159;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;&#21463;&#39046;&#22495;&#26080;&#20851;&#25968;&#25454;&#28151;&#21512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#28151;&#21512;&#23569;&#25968;&#32676;&#20307;&#21644;&#22810;&#25968;&#32676;&#20307;&#30340;&#25968;&#25454;&#26679;&#26412;&#26469;&#36845;&#20195;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#12290;&#24320;&#21457;&#36825;&#26679;&#19968;&#20010;&#26694;&#26550;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#65292;&#25361;&#25112;&#21253;&#25324;&#28304;&#26679;&#26412;&#36873;&#25321;&#12289;&#28151;&#21512;&#31574;&#30053;&#36873;&#25321;&#20197;&#21450;&#24213;&#23618;&#27169;&#22411;&#21644;&#28151;&#21512;&#31574;&#30053;&#20043;&#38388;&#30340;&#21327;&#35843;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imbalanced datasets are commonly observed in various real-world applications, presenting significant challenges in training classifiers. When working with large datasets, the imbalanced issue can be further exacerbated, making it exceptionally difficult to train classifiers effectively. To address the problem, over-sampling techniques have been developed to linearly interpolating data instances between minorities and their neighbors. However, in many real-world scenarios such as anomaly detection, minority instances are often dispersed diversely in the feature space rather than clustered together. Inspired by domain-agnostic data mix-up, we propose generating synthetic samples iteratively by mixing data samples from both minority and majority classes. It is non-trivial to develop such a framework, the challenges include source sample selection, mix-up strategy selection, and the coordination between the underlying model and mix-up strategies. To tackle these challenges, we formulate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#19981;&#21516;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#65288;DST&#65289;&#32452;&#20214;&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#22635;&#34917;&#20851;&#38190;&#30740;&#31350;&#31354;&#30333;&#24182;&#20026;&#25345;&#32493;&#23398;&#20064;&#19979;&#30340;DST&#25552;&#20379;&#26368;&#20339;&#37197;&#32622;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.14831</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#30340;&#25345;&#32493;&#23398;&#20064;&#65306;&#25506;&#32034;&#26377;&#25928;&#27169;&#22411;&#26356;&#26032;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates. (arXiv:2308.14831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#19981;&#21516;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#65288;DST&#65289;&#32452;&#20214;&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#22635;&#34917;&#20851;&#38190;&#30740;&#31350;&#31354;&#30333;&#24182;&#20026;&#25345;&#32493;&#23398;&#20064;&#19979;&#30340;DST&#25552;&#20379;&#26368;&#20339;&#37197;&#32622;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#25351;&#26234;&#33021;&#31995;&#32479;&#20174;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#20013;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#35745;&#31639;&#24320;&#38144;&#39034;&#24207;&#33719;&#21462;&#21644;&#20445;&#30041;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24050;&#22312;&#25991;&#29486;&#20013;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#12289;&#37325;&#25918;&#12289;&#26550;&#26500;&#21644;&#21442;&#25968;&#38548;&#31163;&#31561;&#26041;&#27861;&#12290;&#20351;&#29992;&#31232;&#30095;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#38548;&#31163;&#65292;&#21487;&#20197;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21516;&#37096;&#20998;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#22312;&#20219;&#21153;&#30456;&#20284;&#26102;&#20849;&#20139;&#21442;&#25968;&#12290;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;(DST)&#26159;&#21457;&#29616;&#36825;&#20123;&#31232;&#30095;&#32593;&#32476;&#24182;&#20026;&#27599;&#20010;&#20219;&#21153;&#36827;&#34892;&#38548;&#31163;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;&#26412;&#25991;&#26159;&#39318;&#20010;&#23545;CL&#33539;&#24335;&#19979;&#19981;&#21516;DST&#32452;&#20214;&#25928;&#26524;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#22635;&#34917;&#20851;&#38190;&#30740;&#31350;&#31354;&#30333;&#24182;&#20026;CL&#19979;&#30340;DST&#30340;&#26368;&#20339;&#37197;&#32622;&#25552;&#20379;&#25351;&#23548;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#33879;&#21517;&#30340;CIFAR100&#21644;miniImage&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#21508;&#31181;DST&#32452;&#20214;&#20197;&#25214;&#21040;&#27599;&#20010;&#20219;&#21153;&#30340;&#26368;&#20339;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) refers to the ability of an intelligent system to sequentially acquire and retain knowledge from a stream of data with as little computational overhead as possible. To this end; regularization, replay, architecture, and parameter isolation approaches were introduced to the literature. Parameter isolation using a sparse network which enables to allocate distinct parts of the neural network to different tasks and also allows to share of parameters between tasks if they are similar. Dynamic Sparse Training (DST) is a prominent way to find these sparse networks and isolate them for each task. This paper is the first empirical study investigating the effect of different DST components under the CL paradigm to fill a critical research gap and shed light on the optimal configuration of DST for CL if it exists. Therefore, we perform a comprehensive study in which we investigate various DST components to find the best topology per task on well-known CIFAR100 and miniImag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#22823;&#37327;&#30340;&#20998;&#24067;&#19978;&#25552;&#20379;&#23545;&#40657;&#30418;&#31995;&#32479;&#34892;&#20026;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.14815</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Statistical Verification with Imprecise Neural Networks. (arXiv:2308.14815v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#22823;&#37327;&#30340;&#20998;&#24067;&#19978;&#25552;&#20379;&#23545;&#40657;&#30418;&#31995;&#32479;&#34892;&#20026;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#23433;&#20840;&#39046;&#22495;&#65292;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26159;&#22312;&#39640;&#32500;&#33258;&#20027;&#31995;&#32479;&#30340;&#34892;&#20026;&#19978;&#25552;&#20379;&#20445;&#35777;&#12290;&#20197;&#21487;&#36798;&#24615;&#20998;&#26512;&#20026;&#20013;&#24515;&#30340;&#39564;&#35777;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#65292;&#32780;&#32431;&#31929;&#30340;&#32479;&#35745;&#26041;&#27861;&#21463;&#21040;&#23545;&#37319;&#26679;&#36807;&#31243;&#30340;&#20998;&#24067;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#40657;&#30418;&#31995;&#32479;&#30340;&#20998;&#24067;&#40065;&#26834;&#29256;&#26412;&#30340;&#32479;&#35745;&#39564;&#35777;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#24615;&#33021;&#20445;&#35777;&#36866;&#29992;&#20110;&#22823;&#37327;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26680;&#24515;&#37096;&#20998;&#26159;&#19968;&#31181;&#31216;&#20026;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#23427;&#25552;&#20379;&#20102;&#19981;&#30830;&#23450;&#24615;&#20197;&#25351;&#23548;&#20027;&#21160;&#23398;&#20064;&#12290;&#20027;&#21160;&#23398;&#20064;&#20351;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026;Sherlock&#30340;&#20840;&#38754;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#24037;&#20855;&#26469;&#25910;&#38598;&#26679;&#26412;&#12290;&#22312;openAI gym Mujoco&#29615;&#22659;&#20013;&#20351;&#29992;&#22810;&#20010;&#29289;&#29702;&#27169;&#25311;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A particularly challenging problem in AI safety is providing guarantees on the behavior of high-dimensional autonomous systems. Verification approaches centered around reachability analysis fail to scale, and purely statistical approaches are constrained by the distributional assumptions about the sampling process. Instead, we pose a distributionally robust version of the statistical verification problem for black-box systems, where our performance guarantees hold over a large family of distributions. This paper proposes a novel approach based on a combination of active learning, uncertainty quantification, and neural network verification. A central piece of our approach is an ensemble technique called Imprecise Neural Networks, which provides the uncertainty to guide active learning. The active learning uses an exhaustive neural-network verification tool Sherlock to collect samples. An evaluation on multiple physical simulators in the openAI gym Mujoco environments with reinforcement-
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#36827;&#34892;&#25955;&#23556;&#39044;&#27979;&#65292;&#35777;&#26126;&#20102;&#22312;&#37327;&#23376;&#21147;&#23398;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#31070;&#32463;&#31639;&#23376;&#22312;&#20004;&#20010;&#20855;&#20307;&#38382;&#39064;&#20013;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#25928;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.14789</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#30340;&#25955;&#23556;
&lt;/p&gt;
&lt;p&gt;
Scattering with Neural Operators. (arXiv:2308.14789v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14789
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#36827;&#34892;&#25955;&#23556;&#39044;&#27979;&#65292;&#35777;&#26126;&#20102;&#22312;&#37327;&#23376;&#21147;&#23398;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#31070;&#32463;&#31639;&#23376;&#22312;&#20004;&#20010;&#20855;&#20307;&#38382;&#39064;&#20013;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#25928;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#30830;&#31435;&#20102;&#19968;&#31867;&#31216;&#20026;&#31070;&#32463;&#31639;&#23376;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#33021;&#22815;&#36817;&#20284;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#21463;&#21040;&#23558;&#20854;&#24212;&#29992;&#20110;&#22522;&#30784;&#29289;&#29702;&#23398;&#30340;&#21069;&#26223;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20854;&#22312;&#37327;&#23376;&#21147;&#23398;&#20013;&#25955;&#23556;&#36807;&#31243;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#36845;&#20195;&#21464;&#20307;&#26469;&#23398;&#20064;&#34203;&#23450;&#35860;&#31639;&#23376;&#30340;&#29289;&#29702;&#23398;&#65292;&#35813;&#31639;&#23376;&#23558;&#21021;&#22987;&#27874;&#20989;&#25968;&#21644;&#21183;&#22330;&#26144;&#23556;&#21040;&#26368;&#32456;&#27874;&#20989;&#25968;&#12290;&#36825;&#20123;&#28145;&#24230;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#24605;&#24819;&#22312;&#20004;&#20010;&#20855;&#20307;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;&#19968;&#20010;&#31070;&#32463;&#31639;&#23376;&#39044;&#27979;&#19968;&#20010;&#22312;$1+1$&#32500;&#24230;&#20013;&#19982;&#20013;&#24515;&#21183;&#22330;&#21457;&#29983;&#25955;&#23556;&#30340;&#27874;&#21253;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#20197;&#21450;$2+1$&#32500;&#24230;&#20013;&#30340;&#21452;&#32541;&#23454;&#39564;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#26377;&#38480;&#24046;&#20998;&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;&#31070;&#32463;&#31639;&#23376;&#21487;&#20197;&#25552;&#39640;&#25968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning establish the ability of certain neural-network architectures called neural operators to approximate maps between function spaces. Motivated by a prospect of employing them in fundamental physics, we examine applications to scattering processes in quantum mechanics. We use an iterated variant of Fourier neural operators to learn the physics of Schr\"odinger operators, which map from the space of initial wave functions and potentials to the final wave functions. These deep operator learning ideas are put to test in two concrete problems: a neural operator predicting the time evolution of a wave packet scattering off a central potential in $1+1$ dimensions, and the double-slit experiment in $2+1$ dimensions. At inference, neural operators can become orders of magnitude more efficient compared to traditional finite-difference solvers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27169;&#31946;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#65292;&#35813;&#25351;&#26631;&#32771;&#34385;&#20102;&#22312;&#32858;&#31867;&#25968;&#37327;&#36873;&#25321;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#22810;&#20010;&#36873;&#39033;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.14785</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27169;&#31946;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#19982;&#27425;&#35201;&#36873;&#39033;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
A correlation-based fuzzy cluster validity index with secondary options detector. (arXiv:2308.14785v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27169;&#31946;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#65292;&#35813;&#25351;&#26631;&#32771;&#34385;&#20102;&#22312;&#32858;&#31867;&#25968;&#37327;&#36873;&#25321;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#22810;&#20010;&#36873;&#39033;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#32858;&#31867;&#20998;&#26512;&#26102;&#65292;&#26368;&#20339;&#32858;&#31867;&#25968;&#37327;&#26159;&#20027;&#35201;&#20851;&#27880;&#28857;&#20043;&#19968;&#12290;&#24050;&#32463;&#24341;&#20837;&#20102;&#22810;&#20010;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#26377;&#22810;&#20010;&#36873;&#39033;&#21487;&#20197;&#20316;&#20026;&#26368;&#32456;&#30340;&#32858;&#31867;&#25968;&#37327;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#22312;&#36825;&#20010;&#39046;&#22495;&#24573;&#35270;&#20102;&#36825;&#19968;&#26041;&#38754;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27169;&#31946;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#65292;&#31216;&#20026;Wiroonsri-Preedasawakul&#65288;WP&#65289;&#25351;&#26631;&#12290;&#35813;&#25351;&#26631;&#26681;&#25454;&#19968;&#23545;&#25968;&#25454;&#28857;&#30340;&#23454;&#38469;&#36317;&#31163;&#19982;&#30456;&#24212;&#23545;&#30340;&#35843;&#25972;&#36136;&#24515;&#20043;&#38388;&#30340;&#36317;&#31163;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#23450;&#20041;&#12290;&#25105;&#20204;&#35780;&#20272;&#24182;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#25351;&#26631;&#19982;Xie-Beni&#65292;Pakhira-Bandyopadhyay-Maulik&#65292;Tang&#65292;Wu-Li&#65292;&#24191;&#20041;C&#21644;Kwon2&#31561;&#20960;&#20010;&#29616;&#26377;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65306;&#20154;&#24037;&#25968;&#25454;&#38598;&#65292;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#24102;&#26377;&#31561;&#32423;&#30340;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#27169;&#31946;c-mea&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal number of clusters is one of the main concerns when applying cluster analysis. Several cluster validity indexes have been introduced to address this problem. However, in some situations, there is more than one option that can be chosen as the final number of clusters. This aspect has been overlooked by most of the existing works in this area. In this study, we introduce a correlation-based fuzzy cluster validity index known as the Wiroonsri-Preedasawakul (WP) index. This index is defined based on the correlation between the actual distance between a pair of data points and the distance between adjusted centroids with respect to that pair. We evaluate and compare the performance of our index with several existing indexes, including Xie-Beni, Pakhira-Bandyopadhyay-Maulik, Tang, Wu-Li, generalized C, and Kwon2. We conduct this evaluation on four types of datasets: artificial datasets, real-world datasets, simulated datasets with ranks, and image datasets, using the fuzzy c-mea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#32422;&#26463;&#19979;&#29983;&#25104;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#37325;&#22797;&#21644;&#38544;&#31169;&#27844;&#38706;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.14784</link><description>&lt;p&gt;
&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#29983;&#25104;&#34920;&#26684;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Generating tabular datasets under differential privacy. (arXiv:2308.14784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14784
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#32422;&#26463;&#19979;&#29983;&#25104;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#37325;&#22797;&#21644;&#38544;&#31169;&#27844;&#38706;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#34892;&#19994;&#20013;&#25512;&#21160;&#20102;&#36827;&#23637;&#65292;&#20294;&#20854;&#20381;&#36182;&#20110;&#21487;&#35775;&#38382;&#21644;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#19968;&#20123;&#26368;&#37325;&#35201;&#30340;&#25968;&#25454;&#38598;&#20197;&#34920;&#26684;&#21644;&#20851;&#31995;&#25968;&#25454;&#24211;&#30340;&#24418;&#24335;&#20986;&#29616;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#37329;&#34701;&#39046;&#22495;&#12290;&#20294;&#36825;&#20123;&#34920;&#26684;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#25935;&#24863;&#24615;&#36136;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25581;&#31034;&#25935;&#24863;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#20294;&#29983;&#25104;&#27169;&#22411;&#24448;&#24448;&#20250;&#35760;&#24518;&#21644;&#37325;&#22797;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#30772;&#22351;&#38544;&#31169;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#25968;&#23398;&#26694;&#26550;&#34701;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#20294;&#36825;&#20250;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#38544;&#31169;&#20043;&#38388;&#20135;&#29983;&#26435;&#34913;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#30340;&#20027;&#35201;&#33539;&#24335;&#65292;&#20294;&#21463;&#21040;&#19981;&#31283;&#23450;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#27169;&#24335;&#22349;&#22604;&#30340;&#22256;&#25200;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#38544;&#31169;&#32422;&#26463;&#21644;&#22797;&#26434;&#30340;&#34920;&#26684;&#25968;&#25454;&#27169;&#24577;&#19979;&#26356;&#21152;&#20005;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) is accelerating progress across fields and industries, but relies on accessible and high-quality training data. Some of the most important datasets are found in biomedical and financial domains in the form of spreadsheets and relational databases. But this tabular data is often sensitive in nature. Synthetic data generation offers the potential to unlock sensitive data, but generative models tend to memorise and regurgitate training data, which undermines the privacy goal. To remedy this, researchers have incorporated the mathematical framework of Differential Privacy (DP) into the training process of deep neural networks. But this creates a trade-off between the quality and privacy of the resulting data. Generative Adversarial Networks (GANs) are the dominant paradigm for synthesising tabular data under DP, but suffer from unstable adversarial training and mode collapse, which are exacerbated by the privacy constraints and challenging tabular data modality. This 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26641;&#32593;&#32476;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#23545;&#20998;&#24067;&#24335;&#21452;&#22352;&#26631;&#19978;&#21319;&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#24310;&#36831;&#24191;&#20041;&#20998;&#24067;&#24335;&#21452;&#22352;&#26631;&#19978;&#21319;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.14783</link><description>&lt;p&gt;
&#22312;&#19968;&#33324;&#26641;&#32593;&#32476;&#19978;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#21452;&#22352;&#26631;&#19978;&#21319;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distributed Dual Coordinate Ascent with Imbalanced Data on a General Tree Network. (arXiv:2308.14783v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26641;&#32593;&#32476;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#23545;&#20998;&#24067;&#24335;&#21452;&#22352;&#26631;&#19978;&#21319;&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#24310;&#36831;&#24191;&#20041;&#20998;&#24067;&#24335;&#21452;&#22352;&#26631;&#19978;&#21319;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19981;&#24179;&#34913;&#25968;&#25454;&#23545;&#20110;&#35299;&#20915;&#22522;&#20110;&#32463;&#39564;&#25439;&#22833;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#20998;&#24067;&#24335;&#21452;&#22352;&#26631;&#19978;&#21319;&#31639;&#27861;&#22312;&#26641;&#32593;&#32476;&#20013;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24310;&#36831;&#24191;&#20041;&#20998;&#24067;&#24335;&#21452;&#22352;&#26631;&#19978;&#21319;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#20998;&#26512;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25913;&#21892;&#26641;&#32593;&#32476;&#20013;&#20998;&#24067;&#24335;&#21452;&#22352;&#26631;&#19978;&#21319;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the impact of imbalanced data on the convergence of distributed dual coordinate ascent in a tree network for solving an empirical loss minimization problem in distributed machine learning. To address this issue, we propose a method called delayed generalized distributed dual coordinate ascent that takes into account the information of the imbalanced data, and provide the analysis of the proposed algorithm. Numerical experiments confirm the effectiveness of our proposed method in improving the convergence speed of distributed dual coordinate ascent in a tree network.
&lt;/p&gt;</description></item><item><title>C3AL&#26159;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65292;&#36890;&#36807;&#23558;&#35266;&#27979;&#26641;&#20316;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#24182;&#26368;&#23567;&#21270;&#27979;&#35797;&#27425;&#25968;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.14781</link><description>&lt;p&gt;
&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conflict-Aware Active Automata Learning. (arXiv:2308.14781v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14781
&lt;/p&gt;
&lt;p&gt;
C3AL&#26159;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65292;&#36890;&#36807;&#23558;&#35266;&#27979;&#26641;&#20316;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#24182;&#26368;&#23567;&#21270;&#27979;&#35797;&#27425;&#25968;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#31639;&#27861;&#22312;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20914;&#31361;&#65288;&#21516;&#19968;&#36755;&#20837;&#23545;&#24212;&#19981;&#21516;&#36755;&#20986;&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#31181;&#22266;&#26377;&#30340;&#20914;&#31361;&#24674;&#22797;&#33021;&#21147;&#19981;&#36275;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#23384;&#22312;&#22122;&#22768;&#25110;&#23398;&#20064;&#20013;&#30340;&#31995;&#32479;&#21464;&#21270;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20914;&#31361;&#24863;&#30693;&#30340;&#20027;&#21160;&#26377;&#38480;&#29366;&#24577;&#26426;&#23398;&#20064;&#65288;C3AL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#22788;&#29702;&#20914;&#31361;&#20449;&#24687;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#25152;&#35859;&#30340;&#35266;&#27979;&#26641;&#35270;&#20026;&#23398;&#20064;&#36807;&#31243;&#30340;&#19968;&#31561;&#20844;&#27665;&#12290;&#23613;&#31649;&#36825;&#20010;&#24819;&#27861;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#25506;&#32034;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#65292;&#24182;&#22312;&#38754;&#23545;&#20914;&#31361;&#26102;&#26368;&#23567;&#21270;&#23545;&#27491;&#22312;&#23398;&#20064;&#30340;&#31995;&#32479;&#25191;&#34892;&#30340;&#27979;&#35797;&#27425;&#25968;&#65292;&#20805;&#20998;&#21457;&#25381;&#20102;&#23427;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;C3AL&#65292;&#28085;&#30422;&#20102;30&#22810;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#30446;&#26631;&#21644;18,000&#22810;&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;C3AL&#26159;&#19968;&#20010;&#21512;&#36866;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active automata learning algorithms cannot easily handle \emph{conflict} in the observation data (different outputs observed for the same inputs). This inherent inability to recover after a conflict impairs their effective applicability in scenarios where noise is present or the system under learning is mutating.  We propose the Conflict-Aware Active Automata Learning (C3AL) framework to enable handling conflicting information during the learning process. The core idea is to consider the so-called observation tree as a first-class citizen in the learning process. Though this idea is explored in recent work, we take it to its full effect by enabling its use with any existing learner and minimizing the number of tests performed on the system under learning, specially in the face of conflicts. We evaluate C3AL in a large set of benchmarks, covering over 30 different realistic targets, and over 18,000 different scenarios. The results of the evaluation show that C3AL is a suitable alternati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#24615;&#22320;&#38477;&#32500;&#39640;&#20809;&#35889;&#22270;&#20687;&#65292;&#23454;&#29616;&#23545;&#39640;&#36890;&#37327;&#22609;&#26009;&#29305;&#24615;&#30340;&#35780;&#20272;&#12290;&#21516;&#26102;&#24212;&#29992;&#21270;&#23398;&#32479;&#35745;&#23398;&#20013;&#30340;&#39640;&#31185;&#25216;&#21457;&#23637;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#24490;&#35777;&#25968;&#25454;&#32553;&#20943;&#65292;&#25552;&#39640;&#20102;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#30340;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.14776</link><description>&lt;p&gt;
&#39640;&#36890;&#37327;&#22609;&#26009;&#29305;&#24615;&#30340;&#31995;&#32479;&#21270;&#20302;&#32500;&#24230;&#39640;&#20809;&#35889;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Systematic reduction of Hyperspectral Images for high-throughput Plastic Characterization. (arXiv:2308.14776v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#24615;&#22320;&#38477;&#32500;&#39640;&#20809;&#35889;&#22270;&#20687;&#65292;&#23454;&#29616;&#23545;&#39640;&#36890;&#37327;&#22609;&#26009;&#29305;&#24615;&#30340;&#35780;&#20272;&#12290;&#21516;&#26102;&#24212;&#29992;&#21270;&#23398;&#32479;&#35745;&#23398;&#20013;&#30340;&#39640;&#31185;&#25216;&#21457;&#23637;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#24490;&#35777;&#25968;&#25454;&#32553;&#20943;&#65292;&#25552;&#39640;&#20102;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#30340;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20809;&#35889;&#25104;&#20687;&#65288;HSI&#65289;&#32467;&#21512;&#26174;&#24494;&#38236;&#21644;&#20809;&#35889;&#23398;&#65292;&#29992;&#20110;&#35780;&#20272;&#29289;&#20307;&#20013;&#20809;&#35889;&#27963;&#24615;&#21270;&#21512;&#29289;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#24182;&#22312;&#39135;&#21697;&#36136;&#37327;&#25511;&#21046;&#12289;&#21046;&#33647;&#36807;&#31243;&#21644;&#24223;&#29289;&#20998;&#31867;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;HSI&#25968;&#25454;&#38598;&#30340;&#20307;&#31215;&#24222;&#22823;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#30721;&#22522;&#30784;&#35774;&#26045;&#20013;&#36827;&#34892;&#20998;&#26512;&#21644;&#23384;&#20648;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#24223;&#29289;&#20998;&#31867;&#20013;&#65292;&#36895;&#24230;&#21644;&#25968;&#25454;&#23384;&#20648;&#36164;&#28304;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#20687;&#32032;&#21644;&#21464;&#37327;&#30340;&#36873;&#25321;&#23545;&#20110;&#20445;&#30041;&#21270;&#23398;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#20809;&#35889;&#25968;&#25454;&#23384;&#22312;&#26174;&#33879;&#30340;&#20887;&#20313;&#12290;&#36817;&#24180;&#26469;&#65292;&#21270;&#23398;&#32479;&#35745;&#23398;&#20013;&#30340;&#39640;&#31185;&#25216;&#21457;&#23637;&#20351;&#24471;&#33258;&#21160;&#21270;&#21644;&#24490;&#35777;&#25968;&#25454;&#32553;&#20943;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#30340;&#36895;&#24230;&#21644;&#24615;&#33021;&#65292;NMF&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;HSI&#25968;&#25454;&#21270;&#23398;&#20998;&#26512;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#24674;&#22797;&#20998;&#24067;&#21270;&#21512;&#29289;&#30340;&#32431;&#36129;&#29486;&#22270;&#21644;&#20809;&#35889;&#29305;&#24449;&#65292;NMF&#21487;&#20197;&#25552;&#20379;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral Imaging (HSI) combines microscopy and spectroscopy to assess the spatial distribution of spectroscopically active compounds in objects, and has diverse applications in food quality control, pharmaceutical processes, and waste sorting. However, due to the large size of HSI datasets, it can be challenging to analyze and store them within a reasonable digital infrastructure, especially in waste sorting where speed and data storage resources are limited. Additionally, as with most spectroscopic data, there is significant redundancy, making pixel and variable selection crucial for retaining chemical information. Recent high-tech developments in chemometrics enable automated and evidence-based data reduction, which can substantially enhance the speed and performance of Non-Negative Matrix Factorization (NMF), a widely used algorithm for chemical resolution of HSI data. By recovering the pure contribution maps and spectral profiles of distributed compounds, NMF can provide evide
&lt;/p&gt;</description></item><item><title>XVir&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#21487;&#38752;&#35782;&#21035;&#20154;&#31867;&#32959;&#30244;&#20013;&#30340;&#30149;&#27602;DNA&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23545;&#26469;&#33258;&#30149;&#27602;&#21644;&#20154;&#31867;&#22522;&#22240;&#32452;&#30340;&#22522;&#22240;&#32452;&#27979;&#24207;&#35835;&#21462;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#19982;&#32959;&#30244;&#24207;&#21015;&#20449;&#24687;&#19968;&#36215;&#20351;&#29992;&#20197;&#25214;&#21040;&#30149;&#27602;DNA&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.14769</link><description>&lt;p&gt;
XVir&#65306;&#19968;&#31181;&#29992;&#20110;&#20174;&#30284;&#30151;&#26679;&#26412;&#20013;&#35782;&#21035;&#30149;&#27602;&#35835;&#21462;&#24207;&#21015;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
XVir: A Transformer-Based Architecture for Identifying Viral Reads from Cancer Samples. (arXiv:2308.14769v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14769
&lt;/p&gt;
&lt;p&gt;
XVir&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#21487;&#38752;&#35782;&#21035;&#20154;&#31867;&#32959;&#30244;&#20013;&#30340;&#30149;&#27602;DNA&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23545;&#26469;&#33258;&#30149;&#27602;&#21644;&#20154;&#31867;&#22522;&#22240;&#32452;&#30340;&#22522;&#22240;&#32452;&#27979;&#24207;&#35835;&#21462;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#19982;&#32959;&#30244;&#24207;&#21015;&#20449;&#24687;&#19968;&#36215;&#20351;&#29992;&#20197;&#25214;&#21040;&#30149;&#27602;DNA&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#20840;&#29699;&#32422;15%&#30340;&#30284;&#30151;&#19982;&#30149;&#27602;&#24863;&#26579;&#26377;&#20851;&#12290;&#21487;&#20197;&#24341;&#36215;&#25110;&#22686;&#21152;&#30284;&#30151;&#39118;&#38505;&#30340;&#30149;&#27602;&#21253;&#25324;&#20154;&#20083;&#22836;&#30244;&#30149;&#27602;&#12289;&#20057;&#32925;&#21644;&#19993;&#32925;&#30149;&#27602;&#12289;&#22467;&#26222;&#26031;&#22374;-&#24052;&#23572;&#30149;&#27602;&#21644;&#20154;&#31867;&#20813;&#30123;&#32570;&#38519;&#30149;&#27602;&#31561;&#12290;&#38543;&#30528;&#27979;&#24207;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23545;&#22823;&#37327;&#32959;&#30244;DNA&#25968;&#25454;&#30340;&#35745;&#31639;&#20998;&#26512;&#20351;&#24471;&#30740;&#31350;&#30284;&#30151;&#21644;&#30149;&#27602;&#30149;&#21407;&#20307;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#32852;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#32959;&#30244;&#30149;&#27602;&#23478;&#26063;&#30340;&#39640;&#22810;&#26679;&#24615;&#20351;&#24471;&#21487;&#38752;&#26816;&#27979;&#30149;&#27602;DNA&#21464;&#24471;&#22256;&#38590;&#65292;&#20174;&#32780;&#20351;&#24471;&#27492;&#31867;&#20998;&#26512;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;XVir&#30340;&#25968;&#25454;&#31649;&#36947;&#65292;&#23427;&#20381;&#36182;&#20110;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#20154;&#31867;&#32959;&#30244;&#20013;&#30340;&#30149;&#27602;DNA&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;XVir&#26159;&#36890;&#36807;&#23545;&#26469;&#33258;&#30149;&#27602;&#21644;&#20154;&#31867;&#22522;&#22240;&#32452;&#30340;&#22522;&#22240;&#32452;&#27979;&#24207;&#35835;&#21462;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#21487;&#20197;&#19982;&#32959;&#30244;&#24207;&#21015;&#20449;&#24687;&#19968;&#36215;&#20351;&#29992;&#20197;&#25214;&#21040;&#30149;&#27602;DNA&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is estimated that approximately 15% of cancers worldwide can be linked to viral infections. The viruses that can cause or increase the risk of cancer include human papillomavirus, hepatitis B and C viruses, Epstein-Barr virus, and human immunodeficiency virus, to name a few. The computational analysis of the massive amounts of tumor DNA data, whose collection is enabled by the recent advancements in sequencing technologies, have allowed studies of the potential association between cancers and viral pathogens. However, the high diversity of oncoviral families makes reliable detection of viral DNA difficult and thus, renders such analysis challenging. In this paper, we introduce XVir, a data pipeline that relies on a transformer-based deep learning architecture to reliably identify viral DNA present in human tumors. In particular, XVir is trained on genomic sequencing reads from viral and human genomes and may be used with tumor sequence information to find evidence of viral DNA in hu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#27010;&#24565;&#32534;&#36753;&#65288;UCE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#38381;&#21512;&#35299;&#20915;&#26041;&#26696;&#23545;&#27169;&#22411;&#36827;&#34892;&#32534;&#36753;&#65292;&#21516;&#26102;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12289;&#29256;&#26435;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14761</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#32479;&#19968;&#27010;&#24565;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Unified Concept Editing in Diffusion Models. (arXiv:2308.14761v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#27010;&#24565;&#32534;&#36753;&#65288;UCE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#38381;&#21512;&#35299;&#20915;&#26041;&#26696;&#23545;&#27169;&#22411;&#36827;&#34892;&#32534;&#36753;&#65292;&#21516;&#26102;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12289;&#29256;&#26435;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23384;&#22312;&#21508;&#31181;&#23433;&#20840;&#38382;&#39064;&#65292;&#21487;&#33021;&#38480;&#21046;&#20854;&#36866;&#29992;&#24615;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12289;&#29256;&#26435;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#31561;&#21508;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#65292;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#37117;&#21516;&#26102;&#20986;&#29616;&#22312;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#26041;&#27861;&#35299;&#20915;&#25152;&#26377;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#32479;&#19968;&#27010;&#24565;&#32534;&#36753;&#65288;UCE&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#32463;&#36807;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#38381;&#21512;&#35299;&#20915;&#26041;&#26696;&#23545;&#27169;&#22411;&#36827;&#34892;&#32534;&#36753;&#65292;&#24182;&#21487;&#26080;&#32541;&#22320;&#25193;&#23637;&#21040;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#19978;&#36827;&#34892;&#24182;&#34892;&#32534;&#36753;&#12290;&#25105;&#20204;&#36890;&#36807;&#32534;&#36753;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25237;&#24433;&#26469;&#23637;&#31034;&#21487;&#25193;&#23637;&#30340;&#21516;&#26102;&#21435;&#20559;&#35265;&#12289;&#28040;&#38500;&#39118;&#26684;&#21644;&#20869;&#23481;&#35843;&#33410;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://unified.baulab.info&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models suffer from various safety issues that may limit their suitability for deployment. Previous methods have separately addressed individual issues of bias, copyright, and offensive content in text-to-image models. However, in the real world, all of these issues appear simultaneously in the same model. We present a method that tackles all issues with a single approach. Our method, Unified Concept Editing (UCE), edits the model without training using a closed-form solution, and scales seamlessly to concurrent edits on text-conditional diffusion models. We demonstrate scalable simultaneous debiasing, style erasure, and content moderation by editing text-to-image projections, and we present extensive experiments demonstrating improved efficacy and scalability over prior work. Our code is available at https://unified.baulab.info
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#21147;&#23548;&#21521;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;3D&#20998;&#23376;&#26500;&#22411;&#65292;&#28085;&#30422;&#20102;&#24179;&#34913;&#21644;&#38750;&#24179;&#34913;&#25968;&#25454;&#12290;&#36890;&#36807;&#30452;&#25509;&#20174;&#21407;&#23376;&#21147;&#20013;&#23398;&#20064;&#38750;&#24179;&#34913;&#25968;&#25454;&#21644;&#20351;&#29992;&#38646;&#21147;&#27491;&#21017;&#21270;&#21644;&#22522;&#20110;&#21147;&#30340;&#21435;&#22122;&#25216;&#26415;&#36817;&#20284;&#36817;&#24179;&#34913;&#21147;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;1500&#19975;&#20010;&#22810;&#26679;&#26500;&#22411;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#26410;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#21147;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#22823;&#32422;3&#20493;&#12290;</title><link>http://arxiv.org/abs/2308.14759</link><description>&lt;p&gt;
&#24895;&#21407;&#21147;&#19982;&#20320;&#21516;&#22312;&#65306;&#32479;&#19968;&#30340;&#21147;&#23548;&#21521;&#39044;&#35757;&#32451;&#29992;&#20110;3D&#20998;&#23376;&#26500;&#22411;
&lt;/p&gt;
&lt;p&gt;
May the Force be with You: Unified Force-Centric Pre-Training for 3D Molecular Conformations. (arXiv:2308.14759v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#21147;&#23548;&#21521;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;3D&#20998;&#23376;&#26500;&#22411;&#65292;&#28085;&#30422;&#20102;&#24179;&#34913;&#21644;&#38750;&#24179;&#34913;&#25968;&#25454;&#12290;&#36890;&#36807;&#30452;&#25509;&#20174;&#21407;&#23376;&#21147;&#20013;&#23398;&#20064;&#38750;&#24179;&#34913;&#25968;&#25454;&#21644;&#20351;&#29992;&#38646;&#21147;&#27491;&#21017;&#21270;&#21644;&#22522;&#20110;&#21147;&#30340;&#21435;&#22122;&#25216;&#26415;&#36817;&#20284;&#36817;&#24179;&#34913;&#21147;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;1500&#19975;&#20010;&#22810;&#26679;&#26500;&#22411;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#26410;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#21147;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#22823;&#32422;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#23398;&#20064;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;3D&#20998;&#23376;&#34920;&#31034;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#24179;&#34913;&#25968;&#25454;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#38750;&#24179;&#34913;&#26500;&#22411;&#12290;&#23558;&#36825;&#20123;&#26041;&#27861;&#25193;&#23637;&#21040;&#38750;&#24179;&#34913;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#35757;&#32451;&#30446;&#26631;&#20381;&#36182;&#20110;&#26500;&#22411;&#26159;&#23616;&#37096;&#33021;&#37327;&#26497;&#23567;&#20540;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21147;&#23548;&#21521;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#29992;&#20110;3D&#20998;&#23376;&#26500;&#22411;&#30340;&#24179;&#34913;&#21644;&#38750;&#24179;&#34913;&#25968;&#25454;&#12290;&#23545;&#20110;&#38750;&#24179;&#34913;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30452;&#25509;&#20174;&#21407;&#23376;&#21147;&#20013;&#23398;&#20064;&#12290;&#23545;&#20110;&#24179;&#34913;&#25968;&#25454;&#65292;&#25105;&#20204;&#24341;&#20837;&#38646;&#21147;&#27491;&#21017;&#21270;&#21644;&#22522;&#20110;&#21147;&#30340;&#21435;&#22122;&#25216;&#26415;&#26469;&#36817;&#20284;&#36817;&#24179;&#34913;&#21147;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#36229;&#36807;1500&#19975;&#20010;&#22810;&#26679;&#30340;&#26500;&#22411;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#26410;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#19979;&#65292;&#25105;&#20204;&#23558;&#21147;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#22823;&#32422;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown the promise of learning pre-trained models for 3D molecular representation. However, existing pre-training models focus predominantly on equilibrium data and largely overlook off-equilibrium conformations. It is challenging to extend these methods to off-equilibrium data because their training objective relies on assumptions of conformations being the local energy minima. We address this gap by proposing a force-centric pretraining model for 3D molecular conformations covering both equilibrium and off-equilibrium data. For off-equilibrium data, our model learns directly from their atomic forces. For equilibrium data, we introduce zero-force regularization and forced-based denoising techniques to approximate near-equilibrium forces. We obtain a unified pre-trained model for 3D molecular representation with over 15 million diverse conformations. Experiments show that, with our pre-training objective, we increase forces accuracy by around 3 times compared to the un
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#24378;&#21270;&#23398;&#20064;&#23637;&#31034;&#20102;&#20854;&#20174;&#22810;&#20010;&#35282;&#24230;&#24341;&#20837;&#20154;&#31867;&#24402;&#32435;&#20559;&#22909;&#30340;&#24378;&#22823;&#21644;&#28789;&#27963;&#24615;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#24615;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.14328</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Generative AI: A Survey. (arXiv:2308.14328v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14328
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#24378;&#21270;&#23398;&#20064;&#23637;&#31034;&#20102;&#20854;&#20174;&#22810;&#20010;&#35282;&#24230;&#24341;&#20837;&#20154;&#31867;&#24402;&#32435;&#20559;&#22909;&#30340;&#24378;&#22823;&#21644;&#28789;&#27963;&#24615;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#24615;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#26159;&#26426;&#22120;&#23398;&#20064;&#30028;&#20013;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#37325;&#35201;&#20027;&#39064;&#65292;&#21487;&#20197;&#24433;&#21709;&#21040;&#35832;&#22810;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#25991;&#26412;&#29983;&#25104;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#12290;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#33539;&#24335;&#26159;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#36890;&#36807;&#20943;&#23567;&#27169;&#22411;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25512;&#21160;&#23398;&#20064;&#22120;&#25429;&#25417;&#24182;&#36924;&#36817;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#12290;&#36825;&#31181;&#20844;&#24335;&#25104;&#21151;&#22320;&#24314;&#31435;&#20102;&#29983;&#25104;&#20219;&#21153;&#30340;&#30446;&#26631;&#65292;&#28982;&#32780;&#21364;&#26080;&#27861;&#28385;&#36275;&#20351;&#29992;&#32773;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25152;&#26377;&#38656;&#27714;&#12290;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#31454;&#20105;&#24615;&#36873;&#25321;&#65292;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#30446;&#26631;&#26469;&#27880;&#20837;&#26032;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#24378;&#22823;&#21644;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#24341;&#20837;&#20154;&#31867;&#24402;&#32435;&#20559;&#22909;&#65292;&#22914;&#23545;&#25239;&#23398;&#20064;&#12289;&#25163;&#21160;&#35774;&#35745;&#35268;&#21017;&#21644;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#24615;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Generative AI has been a long-standing essential topic in the machine learning community, which can impact a number of application areas like text generation and computer vision. The major paradigm to train a generative model is maximum likelihood estimation, which pushes the learner to capture and approximate the target data distribution by decreasing the divergence between the model distribution and the target distribution. This formulation successfully establishes the objective of generative tasks, while it is incapable of satisfying all the requirements that a user might expect from a generative model. Reinforcement learning, serving as a competitive option to inject new training signals by creating new objectives that exploit novel signals, has demonstrated its power and flexibility to incorporate human inductive bias from multiple angles, such as adversarial learning, hand-designed rules and learned reward model to build a performant model. Thereby, reinforcement learning ha
&lt;/p&gt;</description></item><item><title>chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.14120</link><description>&lt;p&gt;
&#25480;&#26435;&#20020;&#24202;&#21307;&#29983;&#24182;&#27665;&#20027;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#20020;&#24202;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290; (arXiv:2308.14120v2 [cs.LG] &#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14120
&lt;/p&gt;
&lt;p&gt;
chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24320;&#21457;&#32773;&#65288;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#65289;&#21644;&#20174;&#19994;&#32773;&#65288;&#22914;&#20020;&#24202;&#21307;&#29983;&#65289;&#20043;&#38388;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#65292;&#38459;&#30861;&#20102;ML&#22312;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;chatGPT Advanced Data Analysis&#65288;ADA&#65289;&#65292;&#21363;GPT-4&#30340;&#25193;&#23637;&#65292;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#24182;&#39640;&#25928;&#25191;&#34892;ML&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21521;chatGPT ADA&#25552;&#20379;&#20102;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#30340;&#22823;&#22411;&#35797;&#39564;&#30340;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21644;&#30740;&#31350;&#35814;&#32454;&#20449;&#24687;&#65292;&#27809;&#26377;&#32473;&#20986;&#20855;&#20307;&#25351;&#23548;&#12290;ChatGPT ADA&#22522;&#20110;&#21407;&#22987;&#30740;&#31350;&#30340;&#35757;&#32451;&#25968;&#25454;&#33258;&#20027;&#24320;&#21457;&#20102;&#26368;&#20808;&#36827;&#30340;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#65292;&#22914;&#30284;&#30151;&#21457;&#23637;&#12289;&#30284;&#30151;&#36827;&#23637;&#12289;&#30142;&#30149;&#24182;&#21457;&#30151;&#25110;&#33268;&#30149;&#22522;&#22240;&#24207;&#21015;&#31561;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;ML&#27169;&#22411;&#19982;&#20854;&#24050;&#21457;&#34920;&#30340;&#23545;&#24212;&#29289;&#30456;&#21305;&#37197;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;chatGPT ADA&#20026;&#27665;&#20027;&#21270;&#21307;&#23398;&#20013;&#30340;ML&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#20351;&#38750;ML&#19987;&#23478;&#33021;&#22815;&#33719;&#24471;&#20808;&#36827;&#30340;&#20998;&#26512;&#24037;&#20855;&#24182;&#25512;&#21160;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23457;&#35270;&#65292;&#26088;&#22312;&#32416;&#27491;&#25968;&#25454;&#20998;&#24067;&#20559;&#24046;&#65292;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#21644;&#20195;&#34920;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13821</link><description>&lt;p&gt;
&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#32508;&#36848;&#65306;&#38382;&#39064;&#12289;&#25216;&#26415;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey of Imbalanced Learning on Graphs: Problems, Techniques, and Future Directions. (arXiv:2308.13821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23457;&#35270;&#65292;&#26088;&#22312;&#32416;&#27491;&#25968;&#25454;&#20998;&#24067;&#20559;&#24046;&#65292;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#21644;&#20195;&#34920;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#19982;&#19990;&#30028;&#21508;&#31181;&#22330;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#30456;&#20114;&#36830;&#25509;&#30340;&#32467;&#26500;&#12290;&#26377;&#25928;&#30340;&#22270;&#20998;&#26512;&#25216;&#26415;&#65292;&#22914;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#20174;&#22270;&#25968;&#25454;&#20013;&#33719;&#24471;&#28145;&#21051;&#30340;&#27934;&#23519;&#21147;&#65292;&#20026;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#36335;&#39044;&#27979;&#31561;&#21508;&#31181;&#20219;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#22270;&#25968;&#25454;&#20013;&#26576;&#20123;&#29255;&#27573;&#25317;&#26377;&#22823;&#37327;&#25968;&#25454;&#32780;&#20854;&#20182;&#25968;&#25454;&#31232;&#32570;&#65292;&#20174;&#32780;&#23548;&#33268;&#20559;&#20506;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#36825;&#23601;&#38656;&#35201;&#20986;&#29616;&#20102;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#26088;&#22312;&#32416;&#27491;&#36825;&#20123;&#25968;&#25454;&#20998;&#24067;&#20559;&#24046;&#65292;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#21644;&#20195;&#34920;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23457;&#35270;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#23545;&#35813;&#27010;&#24565;&#21644;&#30456;&#20851;&#26415;&#35821;&#30340;&#26126;&#30830;&#29702;&#35299;&#65292;&#20026;&#35835;&#32773;&#24314;&#31435;&#20102;&#25166;&#23454;&#30340;&#22522;&#30784;&#30693;&#35782;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65306;&#65288;1&#65289;&#38382;&#39064;&#20998;&#31867;&#27861;&#65288;Problem Taxonomy&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs represent interconnected structures prevalent in a myriad of real-world scenarios. Effective graph analytics, such as graph learning methods, enables users to gain profound insights from graph data, underpinning various tasks including node classification and link prediction. However, these methods often suffer from data imbalance, a common issue in graph data where certain segments possess abundant data while others are scarce, thereby leading to biased learning outcomes. This necessitates the emerging field of imbalanced learning on graphs, which aims to correct these data distribution skews for more accurate and representative learning outcomes. In this survey, we embark on a comprehensive review of the literature on imbalanced learning on graphs. We begin by providing a definitive understanding of the concept and related terminologies, establishing a strong foundational understanding for readers. Following this, we propose two comprehensive taxonomies: (1) the problem taxono
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;RL-EA&#65289;&#65292;&#35813;&#31639;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#23545;&#21508;&#31181;RL-EA&#30340;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#21644;&#25628;&#32034;&#27169;&#24335;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27010;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.13420</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65306;&#35843;&#26597;&#21644;&#30740;&#31350;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities. (arXiv:2308.13420v2 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;RL-EA&#65289;&#65292;&#35813;&#31639;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#23545;&#21508;&#31181;RL-EA&#30340;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#21644;&#25628;&#32034;&#27169;&#24335;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#26159;&#19968;&#31867;&#22522;&#20110;&#33258;&#28982;&#36827;&#21270;&#21407;&#29702;&#30340;&#38543;&#26426;&#25628;&#32034;&#26041;&#27861;&#65292;&#22240;&#20854;&#22312;&#21508;&#31181;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#32780;&#24191;&#21463;&#36190;&#35465;&#12290;&#23613;&#31649;&#20840;&#29699;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#36827;&#21270;&#31639;&#27861;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#23398;&#32773;&#31215;&#26497;&#25506;&#32034;&#25913;&#36827;&#31639;&#27861;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#12289;&#25628;&#32034;&#27169;&#24335;&#31561;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#20248;&#21270;&#24615;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#36827;&#21270;&#31639;&#27861;&#26694;&#26550;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#36229;&#36234;&#24615;&#33021;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#21040;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#34987;&#31216;&#20026;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#36827;&#21270;&#31639;&#27861;&#65288;RL-EA&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;RL-EA&#20013;&#19981;&#21516;&#32467;&#26500;&#12289;&#25805;&#20316;&#31526;&#21644;&#25628;&#32034;&#27169;&#24335;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary algorithms (EA), a class of stochastic search methods based on the principles of natural evolution, have received widespread acclaim for their exceptional performance in various real-world optimization problems. While researchers worldwide have proposed a wide variety of EAs, certain limitations remain, such as slow convergence speed and poor generalization capabilities. Consequently, numerous scholars actively explore improvements to algorithmic structures, operators, search patterns, etc., to enhance their optimization performance. Reinforcement learning (RL) integrated as a component in the EA framework has demonstrated superior performance in recent years. This paper presents a comprehensive survey on integrating reinforcement learning into the evolutionary algorithm, referred to as reinforcement learning-assisted evolutionary algorithm (RL-EA). We begin with the conceptual outlines of reinforcement learning and the evolutionary algorithm. We then provide a taxonomy of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#25552;&#20986;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#23436;&#25972;&#25991;&#26723;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12896</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#39029;&#20998;&#31867;&#65306;&#35774;&#35745;&#12289;&#25968;&#25454;&#38598;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#25552;&#20986;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#23436;&#25972;&#25991;&#26723;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#21363;&#22312;&#27979;&#35797;&#25968;&#25454;&#30340;&#24615;&#36136;&#19978;&#65288;$X$&#65306;&#22810;&#36890;&#36947;&#12289;&#22810;&#39029;&#12289;&#22810;&#34892;&#19994;&#65307;$Y$&#65306;&#31867;&#21035;&#20998;&#24067;&#21644;&#26631;&#31614;&#38598;&#30340;&#22810;&#26679;&#24615;&#65289;&#21644;&#32771;&#34385;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#65288;$f$&#65306;&#22810;&#39029;&#25991;&#26723;&#12289;&#39029;&#38754;&#27969;&#21644;&#25991;&#26723;&#25414;&#32465;&#20998;&#31867;&#65292;...&#65289;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20844;&#20849;&#30340;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#24182;&#35268;&#33539;&#20102;&#24212;&#29992;&#22330;&#26223;&#20013;&#20135;&#29983;&#30340;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#28608;&#21457;&#20102;&#20197;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#20026;&#30446;&#26631;&#30340;&#20215;&#20540;&#12290;&#23545;&#25552;&#20986;&#30340;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#21464;&#24471;&#26080;&#20851;&#32039;&#35201;&#65292;&#24182;&#38656;&#35201;&#26356;&#26032;&#20197;&#35780;&#20272;&#23454;&#38469;&#20013;&#33258;&#28982;&#21457;&#29983;&#30340;&#23436;&#25972;&#25991;&#26723;&#12290;&#36825;&#20010;&#29616;&#23454;&#24773;&#20917;&#26816;&#26597;&#20063;&#21628;&#21505;&#26356;&#25104;&#29087;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#28085;&#30422;&#26657;&#20934;&#35780;&#20272;&#12289;&#25512;&#29702;&#22797;&#26434;&#24615;&#65288;&#26102;&#38388;-&#20869;&#23384;&#65289;&#21644;&#19968;&#31995;&#21015;&#29616;&#23454;&#20998;&#25955;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested ($X$: multi-channel, multi-paged, multi-industry; $Y$: class distributions and label set variety) and in classification tasks considered ($f$: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distr
&lt;/p&gt;</description></item><item><title>EquiDiff&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#36712;&#36857;&#39044;&#27979;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#21382;&#21490;&#20449;&#24687;&#21644;&#38543;&#26426;&#22122;&#22768;&#26469;&#29983;&#25104;&#26410;&#26469;&#36710;&#36742;&#36712;&#36857;&#65292;&#24182;&#21033;&#29992;&#20960;&#20309;&#29305;&#24615;&#21644;&#31038;&#20132;&#20132;&#20114;&#25552;&#21462;&#25216;&#26415;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06564</link><description>&lt;p&gt;
EquiDiff:&#19968;&#31181;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;&#30340;&#26465;&#20214;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction. (arXiv:2308.06564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06564
&lt;/p&gt;
&lt;p&gt;
EquiDiff&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#36712;&#36857;&#39044;&#27979;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#21382;&#21490;&#20449;&#24687;&#21644;&#38543;&#26426;&#22122;&#22768;&#26469;&#29983;&#25104;&#26410;&#26469;&#36710;&#36742;&#36712;&#36857;&#65292;&#24182;&#21033;&#29992;&#20960;&#20309;&#29305;&#24615;&#21644;&#31038;&#20132;&#20132;&#20114;&#25552;&#21462;&#25216;&#26415;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#36712;&#36857;&#39044;&#27979;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#21644;&#39640;&#25928;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#30427;&#34892;&#20351;&#24471;&#20986;&#29616;&#20102;&#35768;&#22810;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#12290;&#23613;&#31649;&#30830;&#23450;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22240;&#20854;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#24182;&#32771;&#34385;&#36712;&#36857;&#19981;&#30830;&#23450;&#24615;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EquiDiff&#65292;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#36710;&#36742;&#36712;&#36857;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;EquiDiff&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#21382;&#21490;&#20449;&#24687;&#21644;&#38543;&#26426;&#39640;&#26031;&#22122;&#22768;&#26469;&#29983;&#25104;&#26410;&#26469;&#36712;&#36857;&#12290;EquiDiff&#30340;&#39592;&#24178;&#27169;&#22411;&#26159;&#19968;&#20010;SO(2)&#31561;&#21464;&#25442;&#22120;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#20301;&#32622;&#22352;&#26631;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#25552;&#21462;&#21382;&#21490;&#36712;&#36857;&#20013;&#30340;&#31038;&#20132;&#20132;&#20114;&#12290;&#20026;&#20102;&#35780;&#20272;EquiDiff&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate trajectory prediction is crucial for the safe and efficient operation of autonomous vehicles. The growing popularity of deep learning has led to the development of numerous methods for trajectory prediction. While deterministic deep learning models have been widely used, deep generative models have gained popularity as they learn data distributions from training data and account for trajectory uncertainties. In this study, we propose EquiDiff, a deep generative model for predicting future vehicle trajectories. EquiDiff is based on the conditional diffusion model, which generates future trajectories by incorporating historical information and random Gaussian noise. The backbone model of EquiDiff is an SO(2)-equivariant transformer that fully utilizes the geometric properties of location coordinates. In addition, we employ Recurrent Neural Networks and Graph Attention Networks to extract social interactions from historical trajectories. To evaluate the performance of EquiDiff, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#23618;&#27425;&#32858;&#31867;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20892;&#23398;&#30740;&#31350;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#36890;&#36807;&#25972;&#21512;&#38543;&#26426;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#23398;&#20064;&#33021;&#21147;&#65292;&#23454;&#29616;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.06399</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#23618;&#27425;&#32858;&#31867;&#23398;&#20064;&#20855;&#26377;&#24322;&#26500;&#20892;&#19994;&#25968;&#25454;&#38598;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering. (arXiv:2308.06399v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#23618;&#27425;&#32858;&#31867;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20892;&#23398;&#30740;&#31350;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#36890;&#36807;&#25972;&#21512;&#38543;&#26426;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#23398;&#20064;&#33021;&#21147;&#65292;&#23454;&#29616;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#22810;&#26679;&#20294;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#20013;&#65292;&#20854;&#20013;&#21327;&#21464;&#37327;&#19982;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#32852;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#65292;&#22312;&#21253;&#25324;&#20892;&#23398;&#30740;&#31350;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#37117;&#24456;&#26222;&#36941;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#20351;&#29992;&#23618;&#27425;&#27169;&#22411;&#65292;&#20063;&#34987;&#31216;&#20026;&#22810;&#23618;&#27169;&#22411;&#65292;&#26469;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#65292;&#24182;&#36866;&#24212;&#23427;&#20204;&#30340;&#19981;&#21516;&#29305;&#28857;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32467;&#26500;&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#24322;&#36136;&#24615;&#65292;&#22240;&#20026;&#21464;&#37327;&#36890;&#24120;&#24418;&#25104;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#12290;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BNs&#65289;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#26469;&#27169;&#25311;&#36825;&#31181;&#20851;&#31995;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#38543;&#26426;&#25928;&#24212;&#25972;&#21512;&#21040;BN&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#25968;&#25454;&#12290;&#26469;&#33258;&#30495;&#23454;&#20892;&#23398;&#35797;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.  Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20195;&#30721;&#20013;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#31243;&#24207;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24490;&#29615;&#23618;&#26469;&#25552;&#39640;&#22312;&#31243;&#24207;&#20998;&#26512;&#21644;&#24314;&#27169;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.03312</link><description>&lt;p&gt;
&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#31243;&#24207;&#34920;&#31034;&#27861;&#29992;&#20110;&#23398;&#20064;&#20195;&#30721;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Symmetry-Preserving Program Representations for Learning Code Semantics. (arXiv:2308.03312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20195;&#30721;&#20013;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#31243;&#24207;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24490;&#29615;&#23618;&#26469;&#25552;&#39640;&#22312;&#31243;&#24207;&#20998;&#26512;&#21644;&#24314;&#27169;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#21160;&#31243;&#24207;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36825;&#26159;&#35768;&#22810;&#23433;&#20840;&#20219;&#21153;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#20195;&#30721;&#30340;LLM&#26550;&#26500;&#36890;&#24120;&#20174;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#20511;&#29992;&#65292;&#24341;&#21457;&#23545;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#26410;&#30693;&#20195;&#30721;&#30340;&#20581;&#22766;&#24615;&#30340;&#25285;&#24551;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#27867;&#21270;&#25361;&#25112;&#26159;&#23558;&#20195;&#30721;&#35821;&#20041;&#30340;&#30693;&#35782;&#65292;&#21253;&#25324;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#65292;&#32435;&#20837;LLM&#26550;&#26500;&#20013;&#12290;&#21463;&#21040;&#21033;&#29992;&#24179;&#31227;&#23545;&#31216;&#24615;&#30340;&#21367;&#31215;&#23618;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20195;&#30721;&#23545;&#31216;&#24615;&#22914;&#20309;&#22686;&#24378;&#31243;&#24207;&#20998;&#26512;&#21644;&#24314;&#27169;&#30340;LLM&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#32676;&#35770;&#26694;&#26550;&#65292;&#24418;&#24335;&#21270;&#22320;&#23450;&#20041;&#20102;&#20195;&#30721;&#23545;&#31216;&#24615;&#20316;&#20026;&#20445;&#25345;&#35821;&#20041;&#30340;&#21464;&#25442;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;LLM&#26550;&#26500;&#20013;&#31934;&#30830;&#25512;&#29702;&#23545;&#31216;&#24615;&#20445;&#25345;&#30340;&#25216;&#26415;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20445;&#25345;&#31243;&#24207;&#23545;&#31216;&#24615;&#30340;&#26032;&#22411;&#33258;&#27880;&#24847;&#21147;&#21464;&#20307;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures.  Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31283;&#20581;&#28857;&#20113;&#20998;&#31867;&#30340;&#39118;&#38505;&#20248;&#21270;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#21033;&#29992;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#28040;&#38500;&#39069;&#22806;&#30340;&#24322;&#24120;&#20540;&#24182;&#24674;&#22797;&#25968;&#25454;&#12290;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#20998;&#26512;&#30830;&#23450;&#27599;&#20010;&#28857;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#20248;&#21270;&#39640;&#39118;&#38505;&#28857;&#30340;&#36807;&#28388;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.10875</link><description>&lt;p&gt;
&#38754;&#21521;&#31283;&#20581;&#28857;&#20113;&#20998;&#31867;&#30340;&#39118;&#38505;&#20248;&#21270;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Risk-optimized Outlier Removal for Robust Point Cloud Classification. (arXiv:2307.10875v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31283;&#20581;&#28857;&#20113;&#20998;&#31867;&#30340;&#39118;&#38505;&#20248;&#21270;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#21033;&#29992;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#28040;&#38500;&#39069;&#22806;&#30340;&#24322;&#24120;&#20540;&#24182;&#24674;&#22797;&#25968;&#25454;&#12290;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#20998;&#26512;&#30830;&#23450;&#27599;&#20010;&#28857;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#20248;&#21270;&#39640;&#39118;&#38505;&#28857;&#30340;&#36807;&#28388;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#28145;&#24230;&#27169;&#22411;&#22312;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#26159;&#28857;&#20113;&#22122;&#22768;&#21487;&#33021;&#20250;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;(PointCVaR)&#65292;&#23427;&#21487;&#20197;&#20351;&#26631;&#20934;&#35757;&#32451;&#30340;&#27169;&#22411;&#28040;&#38500;&#39069;&#22806;&#30340;&#24322;&#24120;&#20540;&#24182;&#24674;&#22797;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#36827;&#34892;&#24402;&#22240;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#28857;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#28857;&#30340;&#39118;&#38505;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540; (CVaR) &#20316;&#20026;&#30446;&#26631;&#65292;&#20248;&#21270;&#39640;&#39118;&#38505;&#28857;&#30340;&#36807;&#28388;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#35266;&#23519;&#21040;&#28857;&#20113;&#22122;&#22768;&#28857;&#24448;&#24448;&#32858;&#38598;&#22312;&#39118;&#38505;&#20998;&#24067;&#30340;&#23614;&#37096;&#65292;&#39057;&#29575;&#20302;&#20294;&#39118;&#38505;&#27700;&#24179;&#39640;&#65292;&#20174;&#32780;&#23545;&#20998;&#31867;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24178;&#25200;&#12290;&#23613;&#31649;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21364;&#33021;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of point cloud deep models for safety-critical purposes has increased, but the reliability and security of these models can be compromised by intentional or naturally occurring point cloud noise. To combat this issue, we present a novel point cloud outlier removal method called PointCVaR, which empowers standard-trained models to eliminate additional outliers and restore the data. Our approach begins by conducting attribution analysis to determine the influence of each point on the model output, which we refer to as point risk. We then optimize the process of filtering high-risk points using Conditional Value at Risk (CVaR) as the objective. The rationale for this approach is based on the observation that noise points in point clouds tend to cluster in the tail of the risk distribution, with a low frequency but a high level of risk, resulting in significant interference with classification results. Despite requiring no additional training effort, our method produces exce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#36523;&#22270;&#20687;&#30740;&#31350;&#20102;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#37197;&#20934;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#26368;&#33021;&#39044;&#27979;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#65292;&#24182;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#26159;&#26368;&#37325;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.07439</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Atlas-Based Interpretable Age Prediction. (arXiv:2307.07439v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#36523;&#22270;&#20687;&#30740;&#31350;&#20102;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#37197;&#20934;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#26368;&#33021;&#39044;&#27979;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#65292;&#24182;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#26159;&#26368;&#37325;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24180;&#40836;&#39044;&#27979;&#26159;&#21307;&#23398;&#35780;&#20272;&#21644;&#30740;&#31350;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#21487;&#20197;&#36890;&#36807;&#31361;&#20986;&#23454;&#38469;&#24180;&#40836;&#21644;&#29983;&#29289;&#24180;&#40836;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#24110;&#21161;&#26816;&#27979;&#30142;&#30149;&#21644;&#24322;&#24120;&#34928;&#32769;&#12290;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20840;&#36523;&#22270;&#20687;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#21033;&#29992;Grad-CAM&#35299;&#37322;&#24615;&#26041;&#27861;&#30830;&#23450;&#26368;&#33021;&#39044;&#27979;&#19968;&#20010;&#20154;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#12290;&#36890;&#36807;&#20351;&#29992;&#37197;&#20934;&#25216;&#26415;&#29983;&#25104;&#25972;&#20010;&#20154;&#32676;&#30340;&#35299;&#37322;&#24615;&#22270;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#20010;&#20307;&#20043;&#22806;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20197;&#19968;&#20010;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;2.76&#24180;&#30340;&#27169;&#22411;&#65292;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19977;&#20010;&#20027;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#65306;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#65292;&#20854;&#20013;&#24515;&#33039;&#21306;&#22495;&#20855;&#26377;&#26368;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Age prediction is an important part of medical assessments and research. It can aid in detecting diseases as well as abnormal ageing by highlighting the discrepancy between chronological and biological age. To gain a comprehensive understanding of age-related changes observed in various body parts, we investigate them on a larger scale by using whole-body images. We utilise the Grad-CAM interpretability method to determine the body areas most predictive of a person's age. We expand our analysis beyond individual subjects by employing registration techniques to generate population-wide interpretability maps. Furthermore, we set state-of-the-art whole-body age prediction with a model that achieves a mean absolute error of 2.76 years. Our findings reveal three primary areas of interest: the spine, the autochthonous back muscles, and the cardiac region, which exhibits the highest importance.
&lt;/p&gt;</description></item><item><title>inTformer&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#20851;&#27880;&#26426;&#21046;Transformer&#65292;&#29992;&#20110;&#20351;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#39044;&#27979;&#36335;&#21475;&#20107;&#25925;&#21487;&#33021;&#24615;&#12290;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;inTformer&#20855;&#26377;&#22788;&#29702;&#38271;&#26399;&#20381;&#36182;&#24615;&#12289;&#24182;&#34892;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.03854</link><description>&lt;p&gt;
inTformer: &#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#20851;&#27880;&#26426;&#21046;Transformer&#29992;&#20110;&#20351;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#30340;&#36335;&#21475;&#20107;&#25925;&#21487;&#33021;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data. (arXiv:2307.03854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03854
&lt;/p&gt;
&lt;p&gt;
inTformer&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#20851;&#27880;&#26426;&#21046;Transformer&#65292;&#29992;&#20110;&#20351;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#39044;&#27979;&#36335;&#21475;&#20107;&#25925;&#21487;&#33021;&#24615;&#12290;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;inTformer&#20855;&#26377;&#22788;&#29702;&#38271;&#26399;&#20381;&#36182;&#24615;&#12289;&#24182;&#34892;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#20107;&#25925;&#21487;&#33021;&#24615;&#39044;&#27979;&#27169;&#22411;&#26159;&#20027;&#21160;&#20132;&#36890;&#23433;&#20840;&#31649;&#29702;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22810;&#24180;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#26500;&#24314;&#20107;&#25925;&#21487;&#33021;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#12290;&#22312;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#37319;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#26469;&#35782;&#21035;&#20107;&#25925;&#28508;&#22312;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;Transformer&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#28508;&#22312;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#22522;&#26412;&#21407;&#29702;&#26159;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#36827;&#34892;&#25805;&#20316;&#12290;Transformer&#22312;&#21151;&#33021;&#19978;&#27604;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;LSTM&#65292;CNN&#31561;&#65289;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;Transformer&#21487;&#20197;&#36731;&#26494;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#12290;&#20854;&#27425;&#65292;Transformer&#21487;&#20197;&#22312;&#35757;&#32451;&#26399;&#38388;&#24182;&#34892;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20013;&#30340;&#25152;&#26377;&#20803;&#32032;&#12290;&#26368;&#21518;&#65292;Transformer&#19981;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#12290;&#35748;&#35782;&#21040;Transformer&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
The real-time crash likelihood prediction model is an essential component of the proactive traffic safety management system. Over the years, numerous studies have attempted to construct a crash likelihood prediction model in order to enhance traffic safety, but mostly on freeways. In the majority of the existing studies, researchers have primarily employed a deep learning-based framework to identify crash potential. Lately, Transformer has emerged as a potential deep neural network that fundamentally operates through attention-based mechanisms. Transformer has several functional benefits over extant deep learning models such as Long Short-Term Memory (LSTM), Convolution Neural Network (CNN), etc. Firstly, Transformer can readily handle long-term dependencies in a data sequence. Secondly, Transformer can parallelly process all elements in a data sequence during training. Finally, Transformer does not have the vanishing gradient issue. Realizing the immense possibility of Transformer, th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20840;&#33021;SAM&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#22312;&#25972;&#20010;AI&#24320;&#21457;&#24037;&#20316;&#27969;&#31243;&#20013;&#20351;&#29992;SAM&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#38454;&#27573;&#26080;&#38656;&#25163;&#21160;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#20174;&#24369;&#27880;&#37322;&#21040;&#22522;&#20110;&#25552;&#31034;&#30340;&#20687;&#32032;&#32423;&#32454;&#32990;&#26680;&#20998;&#21106;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.00290</link><description>&lt;p&gt;
&#20840;&#33021;SAM&#65306;&#20174;&#24369;&#27880;&#37322;&#21040;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35266;&#32454;&#32990;&#26680;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
All-in-SAM: from Weak Annotation to Pixel-wise Nuclei Segmentation with Prompt-based Finetuning. (arXiv:2307.00290v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20840;&#33021;SAM&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#22312;&#25972;&#20010;AI&#24320;&#21457;&#24037;&#20316;&#27969;&#31243;&#20013;&#20351;&#29992;SAM&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#38454;&#27573;&#26080;&#38656;&#25163;&#21160;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#20174;&#24369;&#27880;&#37322;&#21040;&#22522;&#20110;&#25552;&#31034;&#30340;&#20687;&#32032;&#32423;&#32454;&#32990;&#26680;&#20998;&#21106;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;Segment Anything Model (SAM)&#26159;&#19968;&#31181;&#20351;&#29992;&#25552;&#31034;&#30340;&#36890;&#29992;&#38646;&#26679;&#26412;&#20998;&#21106;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#35813;&#27969;&#31243;&#22312;&#25512;&#29702;&#38454;&#27573;&#20173;&#28982;&#38656;&#35201;&#25163;&#21160;&#25552;&#31034;&#65292;&#23545;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20173;&#28982;&#36164;&#28304;&#23494;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20840;&#33021;SAM&#30340;&#27969;&#31243;&#65292;&#23427;&#22312;&#25972;&#20010;AI&#24320;&#21457;&#24037;&#20316;&#27969;&#31243;&#20013;&#20351;&#29992;&#20102;SAM&#65292;&#24182;&#19988;&#22312;&#25512;&#29702;&#38454;&#27573;&#26080;&#38656;&#25163;&#21160;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SAM&#39318;&#20808;&#21033;&#29992;&#24369;&#25552;&#31034;&#65288;&#20363;&#22914;&#28857;&#12289;&#36793;&#30028;&#26694;&#65289;&#29983;&#25104;&#20687;&#32032;&#32423;&#27880;&#37322;&#65292;&#28982;&#21518;&#20351;&#29992;&#20687;&#32032;&#32423;&#27880;&#37322;&#23545;SAM&#20998;&#21106;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#20004;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;1&#65289;&#25152;&#25552;&#20986;&#30340;pi
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is a recently proposed prompt-based segmentation model in a generic zero-shot segmentation approach. With the zero-shot segmentation capacity, SAM achieved impressive flexibility and precision on various segmentation tasks. However, the current pipeline requires manual prompts during the inference stage, which is still resource intensive for biomedical image segmentation. In this paper, instead of using prompts during the inference stage, we introduce a pipeline that utilizes the SAM, called all-in-SAM, through the entire AI development workflow (from annotation generation to model finetuning) without requiring manual prompts during the inference stage. Specifically, SAM is first employed to generate pixel-level annotations from weak prompts (e.g., points, bounding box). Then, the pixel-level annotations are used to finetune the SAM segmentation model rather than training from scratch. Our experimental results reveal two key findings: 1) the proposed pi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.16740</link><description>&lt;p&gt;
&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Principles and Guidelines for Evaluating Social Robot Navigation Algorithms. (arXiv:2306.16740v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#26159;&#37096;&#32626;&#26426;&#22120;&#20154;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#34429;&#28982;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#35780;&#20272;&#35299;&#20915;&#31038;&#20132;&#23548;&#33322;&#30340;&#31639;&#27861;&#20173;&#28982;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#28041;&#21450;&#26426;&#22120;&#20154;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#31227;&#21160;&#65292;&#36824;&#28041;&#21450;&#21040;&#21160;&#24577;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#21450;&#20854;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24863;&#30693;&#36866;&#24212;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#28165;&#26224;&#12289;&#21487;&#37325;&#22797;&#12289;&#26131;&#20110;&#33719;&#24471;&#30340;&#22522;&#20934;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20256;&#32479;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#39046;&#22495;&#21152;&#36895;&#20102;&#36827;&#23637;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20844;&#24179;&#27604;&#36739;&#31639;&#27861;&#65292;&#25581;&#31034;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21576;&#29616;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#21521;&#12290;&#25105;&#20204;&#30456;&#20449;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#21161;&#20110;&#31038;&#20132;&#23548;&#33322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#24314;&#31435;&#20102;&#20849;&#21516;&#12289;&#24191;&#27867;&#21487;&#29992;&#19988;&#21487;&#37325;&#22797;&#30340;&#22522;&#20934;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#24049;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20986;&#29616;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.11167</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35823;&#23548;&#65306;&#20351;&#29992;Only Connect Wall&#25968;&#25454;&#38598;&#25506;&#32034;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#21644;Einstellung&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. (arXiv:2306.11167v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20986;&#29616;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#20154;&#24037;&#26234;&#33021;&#35806;&#29983;&#20197;&#26469;&#65292;&#23545;&#20154;&#31867;&#20223;&#30495;&#26234;&#33021;&#30340;&#36861;&#27714;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#25345;&#20037;&#35805;&#39064;&#12290;&#26368;&#26032;&#19968;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25216;&#26415;&#28436;&#36827;&#21644;&#26032;&#20852;&#33021;&#21147;&#23558;&#36825;&#20010;&#20027;&#39064;&#20174;&#23398;&#26415;&#30028;&#24102;&#21040;&#20102;&#25991;&#21270;&#26102;&#20195;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;NLP&#35780;&#20272;&#22522;&#20934;&#20219;&#21153;&#27979;&#35797;&#20102;&#20154;&#31867;&#20223;&#30495;&#34892;&#20026;&#30340;&#19968;&#20123;&#26041;&#38754;&#65288;&#20363;&#22914;BIG-bench&#30340;&#8220;&#31867;&#20154;&#34892;&#20026;&#8221;&#20219;&#21153;&#65289;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#19968;&#20010;&#20219;&#21153;&#32771;&#23519;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20154;&#31867;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#26159;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#20013;&#30740;&#31350;&#36739;&#20026;&#28145;&#20837;&#30340;&#20027;&#39064;&#65292;&#26631;&#20934;&#21270;&#27979;&#35797;&#20027;&#35201;&#20351;&#29992;&#23558;&#32447;&#32034;&#35789;&#20043;&#38388;&#30340;&#65288;&#24322;&#26500;&#65289;&#36830;&#25509;&#33021;&#21147;&#20316;&#20026;&#21019;&#36896;&#24615;&#30340;&#24230;&#37327;&#12290;&#22312;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#65292;&#26263;&#31034;&#24615;&#30340;&#35823;&#23548;&#24615;&#21050;&#28608;-&#34987;&#31216;&#20026;&#8220;&#35825;&#23548;&#35823;&#35299;&#8221;&#30340;&#24178;&#25200;&#22240;&#32032;-&#36890;&#36807;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#38459;&#30861;&#20102;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#22312;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20107;&#20808;&#35753;&#21442;&#19982;&#32773;&#25509;&#35302;&#21040;&#26377;&#30456;&#20284;&#25340;&#20889;&#30340;&#38169;&#35823;&#22240;&#32032;&#26469;&#23454;&#39564;&#24615;&#22320;&#35825;&#23548;&#36825;&#26679;&#30340;&#22266;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quest for human imitative AI has been an enduring topic in AI research since its inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to the cultural zeitgeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behaviour (e.g., BIG-bench's 'human-like behavior' tasks), few, if not none, examine creative problem solving abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use the ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#36328;&#36234;&#35821;&#38899;&#21644;&#35821;&#35328;&#22788;&#29702;&#23398;&#31185;&#30340;&#19971;&#20010;&#20250;&#35758;&#20013;&#65292;&#28304;&#20195;&#30721;&#21487;&#29992;&#24615;&#23545;&#20110;Interspeech&#20250;&#35758;&#35201;&#23569;&#20110;40%&#65292;&#24182;&#25552;&#20986;&#20102;&#24314;&#35758;&#20197;&#25552;&#39640;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#37325;&#22797;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10033</link><description>&lt;p&gt;
&#22312;Interspeech&#20250;&#35758;&#19978;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#65306;&#19968;&#31181;&#38271;&#26399;&#21644;&#27604;&#36739;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investigating Reproducibility at Interspeech Conferences: A Longitudinal and Comparative Perspective. (arXiv:2306.10033v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10033
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#36328;&#36234;&#35821;&#38899;&#21644;&#35821;&#35328;&#22788;&#29702;&#23398;&#31185;&#30340;&#19971;&#20010;&#20250;&#35758;&#20013;&#65292;&#28304;&#20195;&#30721;&#21487;&#29992;&#24615;&#23545;&#20110;Interspeech&#20250;&#35758;&#35201;&#23569;&#20110;40%&#65292;&#24182;&#25552;&#20986;&#20102;&#24314;&#35758;&#20197;&#25552;&#39640;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#37325;&#22797;&#24615;&#26159;&#27178;&#36328;&#23398;&#31185;&#30340;&#31185;&#23398;&#36827;&#23637;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#38477;&#20302;&#24320;&#25918;&#31185;&#23398;&#30340;&#38556;&#30861;&#26159;Interspeech 2023&#20027;&#39064;&#30340;&#28966;&#28857;&#39046;&#22495;&#12290;&#28304;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#26159;&#20419;&#36827;&#21487;&#37325;&#22797;&#24615;&#30340;&#25351;&#26631;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#19982;&#39046;&#22495;&#20869;&#20854;&#20182;&#20250;&#35758;&#30456;&#27604;&#65292;Interspeech&#20250;&#35758;&#30340;&#20877;&#29616;&#29575;&#36739;&#20302;&#26159;&#25105;&#20204;&#40092;&#26377;&#20102;&#35299;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#36328;&#36234;&#35821;&#38899;&#21644;&#35821;&#35328;&#22788;&#29702;&#23398;&#31185;&#30340;&#19971;&#20010;&#20250;&#35758;&#30340;27,717&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#25509;&#21463;&#30340;&#35770;&#25991;&#25968;&#37327;&#19982;&#20854;&#20182;&#20250;&#35758;&#30456;&#36817;&#65292;&#20294;Interspeech&#30340;&#21487;&#29992;&#28304;&#20195;&#30721;&#36739;&#23569;&#65292;&#36798;&#21040;40%&#12290;&#38500;&#20102;&#25253;&#21578;&#25105;&#20204;&#22312;&#30740;&#31350;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#24314;&#35758;&#21644;&#21487;&#33021;&#30340;&#26041;&#21521;&#65292;&#20197;&#22686;&#21152;&#21487;&#37325;&#22797;&#24615;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducibility is a key aspect for scientific advancement across disciplines, and reducing barriers for open science is a focus area for the theme of Interspeech 2023. Availability of source code is one of the indicators that facilitates reproducibility. However, less is known about the rates of reproducibility at Interspeech conferences in comparison to other conferences in the field. In order to fill this gap, we have surveyed 27,717 papers at seven conferences across speech and language processing disciplines. We find that despite having a close number of accepted papers to the other conferences, Interspeech has up to 40% less source code availability. In addition to reporting the difficulties we have encountered during our research, we also provide recommendations and possible directions to increase reproducibility for further studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09539</link><description>&lt;p&gt;
&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#22312;&#38656;&#35201;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#24615;&#24182;&#19988;&#38656;&#35201;&#39640;&#25928;&#25193;&#23637;&#21040;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24778;&#20154;&#30340;&#25928;&#26524;&#12290;&#23613;&#31649;&#26368;&#21021;&#26159;&#20026;&#36830;&#32493;&#20449;&#21495;&#35774;&#35745;&#30340;&#65292;&#20294;SSM&#22312;&#35270;&#35273;&#21644;&#38899;&#39057;&#31561;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65307;&#28982;&#32780;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;SSM&#20173;&#28982;&#33853;&#21518;&#20110;Transformers&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#23618;&#65292;&#23427;&#22312;&#20869;&#37096;&#32452;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#21270;&#30340;SSM&#23376;&#23618;&#21644;&#19968;&#20010;&#29992;&#20110;&#30701;&#26399;&#24207;&#21015;&#34920;&#31034;&#30340;&#22359;&#21464;&#25442;&#22120;&#23376;&#23618;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#12289;&#23436;&#20840;&#21487;&#24182;&#34892;&#30340;&#38598;&#25104;SSM&#21644;&#22359;&#27880;&#24847;&#21147;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#30340;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;AutoML&#25216;&#26415;&#26469;&#20943;&#23569;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#36827;&#20248;&#21270;&#20989;&#25968;&#21644;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#32467;&#21512;&#20844;&#24179;&#30446;&#26631;&#65292;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#22522;&#20110;ML&#30340;&#36719;&#20214;&#20013;&#30340;&#20559;&#35265;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;AutoML&#30340;&#20844;&#24179;&#24863;&#30693;&#25628;&#32034;&#31354;&#38388;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#20462;&#22797;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.09297</link><description>&lt;p&gt;
&#20462;&#22797;&#20844;&#24179;&#24615;&#65292;&#32780;&#19981;&#26159;&#30772;&#22351;&#20934;&#30830;&#24615;&#65306;&#20351;&#29992;AutoML&#30340;&#24615;&#33021;&#24863;&#30693;&#20844;&#24179;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Fix Fairness, Don't Ruin Accuracy: Performance Aware Fairness Repair using AutoML. (arXiv:2306.09297v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;AutoML&#25216;&#26415;&#26469;&#20943;&#23569;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#36827;&#20248;&#21270;&#20989;&#25968;&#21644;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#32467;&#21512;&#20844;&#24179;&#30446;&#26631;&#65292;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#22522;&#20110;ML&#30340;&#36719;&#20214;&#20013;&#30340;&#20559;&#35265;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;AutoML&#30340;&#20844;&#24179;&#24863;&#30693;&#25628;&#32034;&#31354;&#38388;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#20462;&#22797;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#20851;&#38190;&#20915;&#31574;&#36719;&#20214;&#20013;&#65292;&#20294;&#20107;&#25925;&#24341;&#21457;&#20102;&#20851;&#20110;ML&#39044;&#27979;&#20844;&#24179;&#24615;&#30340;&#36136;&#30097;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38656;&#35201;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#26469;&#20943;&#23569;&#22522;&#20110;ML&#30340;&#36719;&#20214;&#20013;&#30340;&#20559;&#35265;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20559;&#35265;&#32531;&#35299;&#31639;&#27861;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#21482;&#33021;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#24182;&#19988;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#25216;&#26415;&#26469;&#20943;&#23569;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#20989;&#25968;&#21644;&#19968;&#20010;&#20844;&#24179;&#24863;&#30693;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#36890;&#36807;&#25913;&#36827;AutoML&#30340;&#40664;&#35748;&#20248;&#21270;&#20989;&#25968;&#24182;&#32467;&#21512;&#20844;&#24179;&#30446;&#26631;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;AutoML&#30340;&#20844;&#24179;&#24863;&#30693;&#25628;&#32034;&#31354;&#38388;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#20462;&#22797;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#26368;&#20808;&#36827;&#30340;Auto-Sklearn&#24037;&#20855;&#19978;&#65292;&#26088;&#22312;&#20943;&#23569;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) is increasingly being used in critical decision-making software, but incidents have raised questions about the fairness of ML predictions. To address this issue, new tools and methods are needed to mitigate bias in ML-based software. Previous studies have proposed bias mitigation algorithms that only work in specific situations and often result in a loss of accuracy. Our proposed solution is a novel approach that utilizes automated machine learning (AutoML) techniques to mitigate bias. Our approach includes two key innovations: a novel optimization function and a fairness-aware search space. By improving the default optimization function of AutoML and incorporating fairness objectives, we are able to mitigate bias with little to no loss of accuracy. Additionally, we propose a fairness-aware search space pruning method for AutoML to reduce computational cost and repair time. Our approach, built on the state-of-the-art Auto-Sklearn tool, is designed to reduce bias i
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#26680;&#26426;&#22120;&#20998;&#31867;&#22120;&#20013;&#32467;&#21512;&#21407;&#22987;&#21644;&#23545;&#20598;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21487;&#35265;&#21333;&#20803;&#21644;&#38544;&#34255;&#21333;&#20803;&#30340;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#32423;&#21035;&#30340;&#28145;&#24230;&#26550;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20855;&#26377;&#20248;&#21183;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#36755;&#20837;&#21644;&#22823;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.07015</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#21463;&#38480;&#26680;&#26426;&#22120;&#20998;&#31867;&#22120;&#20013;&#32467;&#21512;&#21407;&#22987;&#21644;&#23545;&#20598;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Combining Primal and Dual Representations in Deep Restricted Kernel Machines Classifiers. (arXiv:2306.07015v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#26680;&#26426;&#22120;&#20998;&#31867;&#22120;&#20013;&#32467;&#21512;&#21407;&#22987;&#21644;&#23545;&#20598;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21487;&#35265;&#21333;&#20803;&#21644;&#38544;&#34255;&#21333;&#20803;&#30340;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#32423;&#21035;&#30340;&#28145;&#24230;&#26550;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20855;&#26377;&#20248;&#21183;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#36755;&#20837;&#21644;&#22823;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#19982;&#26680;&#26426;&#22120;&#30340;&#32972;&#26223;&#19979;&#65292;&#28145;&#24230;&#21463;&#38480;&#26680;&#26426;&#22120;&#65288;DRKM&#65289;&#26694;&#26550;&#20801;&#35768;&#23558;&#22810;&#20010;&#32423;&#21035;&#30340;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;KPCA&#65289;&#21644;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;LSSVM&#65289;&#32467;&#21512;&#20026;&#19968;&#20010;&#20351;&#29992;&#21487;&#35265;&#21333;&#20803;&#21644;&#38544;&#34255;&#21333;&#20803;&#30340;&#28145;&#24230;&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DRKM&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;KPCA&#21644;&#20998;&#31867;&#32423;&#21035;&#30340;&#30446;&#26631;&#30456;&#32467;&#21512;&#65292;&#38544;&#34255;&#29305;&#24449;&#30697;&#38453;&#20301;&#20110;Stiefel&#27969;&#24418;&#19978;&#12290;&#20998;&#31867;&#32423;&#21035;&#21487;&#20197;&#34920;&#31034;&#20026;LSSVM&#25110;MLP&#29305;&#24449;&#22270;&#65292;&#32467;&#21512;&#32423;&#21035;&#21644;&#23618;&#25968;&#30340;&#28145;&#24230;&#12290;&#20998;&#31867;&#32423;&#21035;&#22312;&#20854;&#21407;&#22987;&#24418;&#24335;&#20013;&#34920;&#36798;&#65292;&#32780;KPCA&#30340;&#28145;&#24230;&#32423;&#21035;&#22312;&#20854;&#23545;&#20598;&#24418;&#24335;&#20013;&#21487;&#20197;&#23558;&#25968;&#25454;&#30340;&#26368;&#20449;&#24687;&#21270;&#32452;&#20998;&#23884;&#20837;&#21040;&#19968;&#20010;&#26356;&#20302;&#32500;&#30340;&#31354;&#38388;&#20013;&#12290;&#23545;&#20598;&#35774;&#32622;&#29420;&#31435;&#20110;&#36755;&#20837;&#30340;&#32500;&#24230;&#65292;&#21407;&#22987;&#35774;&#32622;&#26159;&#21442;&#25968;&#21270;&#30340;&#65292;&#36825;&#20351;&#24471;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#39640;&#32500;&#36755;&#20837;&#21644;&#22823;&#25968;&#25454;&#38598;&#19978;&#37117;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of deep learning with kernel machines, the deep Restricted Kernel Machine (DRKM) framework allows multiple levels of kernel PCA (KPCA) and Least-Squares Support Vector Machines (LSSVM) to be combined into a deep architecture using visible and hidden units. We propose a new method for DRKM classification coupling the objectives of KPCA and classification levels, with the hidden feature matrix lying on the Stiefel manifold. The classification level can be formulated as an LSSVM or as an MLP feature map, combining depth in terms of levels and layers. The classification level is expressed in its primal formulation, as the deep KPCA levels, in their dual formulation, can embed the most informative components of the data in a much lower dimensional space. The dual setting is independent of the dimension of the inputs and the primal setting is parametric, which makes the proposed method computationally efficient for both high-dimensional inputs and large datasets. In the experi
&lt;/p&gt;</description></item><item><title>&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#23545;&#25968;&#25454;&#26631;&#27880;&#30340;&#24433;&#21709;&#24456;&#37325;&#35201;&#12290;&#36890;&#36807;POPQUORN&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#22312;&#20182;&#20204;&#30340;&#21028;&#26029;&#20013;&#36215;&#21040;&#20102;&#26174;&#33879;&#20316;&#29992;&#65292;&#24182;&#19988;&#24212;&#35813;&#32771;&#34385;&#20197;&#21069;&#26410;&#32771;&#34385;&#30340;&#32972;&#26223;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24314;&#35758;&#29702;&#35299;&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#65292;&#20174;&#20855;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#34913;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#20013;&#25910;&#38598;&#26631;&#31614;&#65292;&#20197;&#20943;&#23569;&#25968;&#25454;&#38598;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.06826</link><description>&lt;p&gt;
Annotator Demographics Matter - Measuring the Influence of Annotator Demographics with the POPQUORN Dataset
&lt;/p&gt;
&lt;p&gt;
When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the POPQUORN Dataset. (arXiv:2306.06826v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06826
&lt;/p&gt;
&lt;p&gt;
&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#23545;&#25968;&#25454;&#26631;&#27880;&#30340;&#24433;&#21709;&#24456;&#37325;&#35201;&#12290;&#36890;&#36807;POPQUORN&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#22312;&#20182;&#20204;&#30340;&#21028;&#26029;&#20013;&#36215;&#21040;&#20102;&#26174;&#33879;&#20316;&#29992;&#65292;&#24182;&#19988;&#24212;&#35813;&#32771;&#34385;&#20197;&#21069;&#26410;&#32771;&#34385;&#30340;&#32972;&#26223;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24314;&#35758;&#29702;&#35299;&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#65292;&#20174;&#20855;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#34913;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#20013;&#25910;&#38598;&#26631;&#31614;&#65292;&#20197;&#20943;&#23569;&#25968;&#25454;&#38598;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#21644;&#32463;&#21382;&#20250;&#24433;&#21709;&#20182;&#20204;&#23545;&#25968;&#25454;&#30340;&#26631;&#27880;&#65292;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21482;&#36817;&#26399;&#24320;&#22987;&#32771;&#34385;&#26631;&#27880;&#32773;&#36523;&#20221;&#23545;&#20182;&#20204;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;POPQUORN&#65288;POtato-Prolific &#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#12289;&#20882;&#29359;&#24615;&#12289;&#25991;&#26412;&#25913;&#20889;&#21644;&#31036;&#35980;&#35780;&#20998;&#65292;&#21253;&#21547;&#20154;&#21475;&#32479;&#35745;&#23398;&#32454;&#24494;&#24046;&#24322;&#65289;&#12290;POPQUORN&#21253;&#21547;1,484&#20010;&#26631;&#27880;&#32773;&#30340;45,000&#20010;&#26631;&#27880;&#65292;&#37319;&#29992;&#20102;&#32654;&#22269;&#20154;&#21475;&#20013;&#24615;&#21035;&#12289;&#24180;&#40836;&#21644;&#31181;&#26063;&#30340;&#20195;&#34920;&#26679;&#26412;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#27880;&#32773;&#32972;&#26223;&#22312;&#20182;&#20204;&#30340;&#21028;&#26029;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#34920;&#26126;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20197;&#21069;&#26410;&#32771;&#34385;&#30340;&#32972;&#26223;&#22240;&#32032;&#65288;&#20363;&#22914;&#25945;&#32946;&#65289;&#26159;&#26377;&#24847;&#20041;&#19988;&#24212;&#35813;&#34987;&#32771;&#34385;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29702;&#35299;&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#65292;&#24182;&#20174;&#20855;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#34913;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#20013;&#25910;&#38598;&#26631;&#31614;&#65292;&#23545;&#20943;&#23569;&#25968;&#25454;&#38598;&#20559;&#24046;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotators are not fungible. Their demographics, life experiences, and backgrounds all contribute to how they label data. However, NLP has only recently considered how annotator identity might influence their decisions. Here, we present POPQUORN (the POtato-Prolific dataset for QUestion-Answering, Offensiveness, text Rewriting, and politeness rating with demographic Nuance). POPQUORN contains 45,000 annotations from 1,484 annotators, drawn from a representative sample regarding sex, age, and race as the US population. Through a series of analyses, we show that annotators' background plays a significant role in their judgments. Further, our work shows that backgrounds not previously considered in NLP (e.g., education), are meaningful and should be considered. Our study suggests that understanding the background of annotators and collecting labels from a demographically balanced pool of crowd workers is important to reduce the bias of datasets. The dataset, annotator background, and anno
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20808;&#21069;&#25968;&#25454;&#30340;&#24178;&#39044;&#25514;&#26045;&#25552;&#39640;&#22522;&#20110;AI&#30340;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;CDHF&#26694;&#26550;&#26469;&#25972;&#21512;&#20154;&#31867;&#21453;&#39304;&#65292;&#39044;&#27979;&#24314;&#35758;&#25509;&#21463;&#31243;&#24230;&#24182;&#20915;&#23450;&#20309;&#26102;&#23637;&#31034;&#21738;&#20123;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04930</link><description>&lt;p&gt;
&#20309;&#26102;&#23637;&#31034;&#24314;&#35758;&#65311;&#22312;AI&#36741;&#21161;&#32534;&#31243;&#20013;&#25972;&#21512;&#20154;&#31867;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
When to Show a Suggestion? Integrating Human Feedback in AI-Assisted Programming. (arXiv:2306.04930v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20808;&#21069;&#25968;&#25454;&#30340;&#24178;&#39044;&#25514;&#26045;&#25552;&#39640;&#22522;&#20110;AI&#30340;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;CDHF&#26694;&#26550;&#26469;&#25972;&#21512;&#20154;&#31867;&#21453;&#39304;&#65292;&#39044;&#27979;&#24314;&#35758;&#25509;&#21463;&#31243;&#24230;&#24182;&#20915;&#23450;&#20309;&#26102;&#23637;&#31034;&#21738;&#20123;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#30340;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#65292;&#22914;Copilot&#21644;CodeWhisperer&#65292;&#25552;&#20379;&#31243;&#24207;&#21592;&#29615;&#22659;&#65288;&#20363;&#22914;IDE&#65289;&#20869;&#30340;&#20195;&#30721;&#24314;&#35758;&#65292;&#26088;&#22312;&#25552;&#39640;&#20182;&#20204;&#30340;&#29983;&#20135;&#21147;&#12290;&#30001;&#20110;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#65292;&#31243;&#24207;&#21592;&#25509;&#21463;&#21644;&#25298;&#32477;&#24314;&#35758;&#65292;&#22240;&#27492;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#35813;&#31995;&#32479;&#24212;&#20351;&#29992;&#27492;&#21453;&#39304;&#20197;&#20419;&#36827;&#36825;&#19968;&#30446;&#26631;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#31243;&#24207;&#21592;&#19982;Copilot&#20132;&#20114;&#30340;&#20808;&#21069;&#25968;&#25454;&#65292;&#24320;&#21457;&#21487;&#20197;&#33410;&#30465;&#31243;&#24207;&#21592;&#26102;&#38388;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#19982;&#31243;&#24207;&#21592;&#30340;&#20132;&#20114;&#65292;&#24182;&#20915;&#23450;&#20309;&#26102;&#23637;&#31034;&#21738;&#20123;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#8220;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#26465;&#20214;&#24314;&#35758;&#23637;&#31034;&#8221;&#65288;CDHF&#65289;&#22522;&#20110;&#23545;&#31243;&#24207;&#21592;&#25805;&#20316;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#20351;&#29992;535&#21517;&#31243;&#24207;&#21592;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21487;&#20197;&#39044;&#27979;&#24314;&#35758;&#25509;&#21463;&#31243;&#24230;&#30340;&#27169;&#22411;&#12290;&#22312;&#23545;&#36890;&#36807;AI&#36741;&#21161;&#32534;&#31243;&#35299;&#20915;&#30340;&#30495;&#23454;&#19990;&#30028;&#32534;&#31243;&#20219;&#21153;&#30340;&#22238;&#39038;&#24615;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;CDHF&#33021;&#22815;&#23454;&#29616;&#26377;&#21033;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25972;&#21512;&#20154;&#31867;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;AI&#30340;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI powered code-recommendation systems, such as Copilot and CodeWhisperer, provide code suggestions inside a programmer's environment (e.g., an IDE) with the aim to improve their productivity. Since, in these scenarios, programmers accept and reject suggestions, ideally, such a system should use this feedback in furtherance of this goal. In this work we leverage prior data of programmers interacting with Copilot to develop interventions that can save programmer time. We propose a utility theory framework, which models this interaction with programmers and decides when and which suggestions to display. Our framework Conditional suggestion Display from Human Feedback (CDHF) is based on predictive models of programmer actions. Using data from 535 programmers we build models that predict the likelihood of suggestion acceptance. In a retrospective evaluation on real-world programming tasks solved with AI-assisted programming, we find that CDHF can achieve favorable tradeoffs. Our findings s
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;AI&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;AI&#30340;MSF&#24863;&#30693;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#12290;</title><link>http://arxiv.org/abs/2306.03454</link><description>&lt;p&gt;
AI&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#31995;&#32479;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#65306;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Robustness of AI-enabled Multi-sensor Fusion Systems: Challenges and Opportunities. (arXiv:2306.03454v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;AI&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;AI&#30340;MSF&#24863;&#30693;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#65288;MSF&#65289;&#30340;&#24863;&#30693;&#31995;&#32479;&#26159;&#25903;&#25745;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#21644;&#39046;&#22495;&#30340;&#22522;&#30784;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12289;&#26426;&#22120;&#20154;&#33218;&#21644;&#26080;&#20154;&#26426;&#12290;&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26041;&#38754;&#30340;&#24555;&#36895;&#36827;&#27493;&#20026;MSF&#31995;&#32479;&#30340;&#24615;&#33021;&#25552;&#39640;&#25552;&#20379;&#20102;&#24555;&#36895;&#22686;&#38271;&#30340;&#36235;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#26234;&#33021;&#31995;&#32479;&#21450;&#20854;&#24863;&#30693;&#31995;&#32479;&#26041;&#38754;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;AI&#30340;MSF&#24863;&#30693;&#31995;&#32479;&#21644;&#25216;&#26415;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20844;&#24320;&#30340;&#19987;&#27880;&#20110;MSF&#24863;&#30693;&#30340;&#22522;&#20934;&#27979;&#35797;&#26377;&#38480;&#12290;&#37492;&#20110;&#35768;&#22810;&#26234;&#33021;&#31995;&#32479;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#22312;&#24863;&#30693;&#31995;&#32479;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#36825;&#23601;&#24613;&#38656;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#36825;&#20123;MSF&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#36808;&#20986;&#20102;&#26089;&#26399;&#30340;&#19968;&#27493;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;AI&#30340;MSF&#24863;&#30693;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#19987;&#27880;&#20110;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21644;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Sensor Fusion (MSF) based perception systems have been the foundation in supporting many industrial applications and domains, such as self-driving cars, robotic arms, and unmanned aerial vehicles. Over the past few years, the fast progress in data-driven artificial intelligence (AI) has brought a fast-increasing trend to empower MSF systems by deep learning techniques to further improve performance, especially on intelligent systems and their perception systems. Although quite a few AI-enabled MSF perception systems and techniques have been proposed, up to the present, limited benchmarks that focus on MSF perception are publicly available. Given that many intelligent systems such as self-driving cars are operated in safety-critical contexts where perception systems play an important role, there comes an urgent need for a more in-depth understanding of the performance and reliability of these MSF systems. To bridge this gap, we initiate an early step in this direction and construc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#20248;&#32531;&#23384;&#19982;&#27169;&#22411;&#22797;&#29992;&#20004;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#22823;&#22411;&#27169;&#22411;&#25512;&#29702;&#20013;&#36164;&#28304;&#28040;&#32791;&#21644;&#24310;&#36831;&#25361;&#25112;&#65292;&#32463;&#36807;&#23454;&#35777;&#27169;&#25311;&#21457;&#29616;&#36825;&#31181;&#32452;&#21512;&#22823;&#22823;&#25552;&#39640;&#20102;&#20256;&#32479;&#27169;&#22411;&#25512;&#29702;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02003</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#26368;&#20248;&#32531;&#23384;&#19982;&#27169;&#22411;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Optimal Caching and Model Multiplexing for Large Model Inference. (arXiv:2306.02003v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#20248;&#32531;&#23384;&#19982;&#27169;&#22411;&#22797;&#29992;&#20004;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#22823;&#22411;&#27169;&#22411;&#25512;&#29702;&#20013;&#36164;&#28304;&#28040;&#32791;&#21644;&#24310;&#36831;&#25361;&#25112;&#65292;&#32463;&#36807;&#23454;&#35777;&#27169;&#25311;&#21457;&#29616;&#36825;&#31181;&#32452;&#21512;&#22823;&#22823;&#25552;&#39640;&#20102;&#20256;&#32479;&#27169;&#22411;&#25512;&#29702;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20854;&#20182;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#23610;&#23544;&#21152;&#21095;&#20102;&#29616;&#26377;&#30340;&#36164;&#28304;&#28040;&#32791;&#21644;&#24310;&#36831;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65306;&#21033;&#29992;&#32531;&#23384;&#23384;&#20648;&#20808;&#21069;&#30340;&#26597;&#35810;&#21644;&#23398;&#20064;&#27169;&#22411;&#22797;&#29992;&#22120;&#26469;&#36873;&#25321;&#29992;&#20110;&#26597;&#35810;&#22788;&#29702;&#30340;&#27169;&#22411;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26368;&#20248;&#31639;&#27861;&#26469;&#32852;&#21512;&#20248;&#21270;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#31163;&#32447;&#21644;&#22312;&#32447;&#21046;&#34920;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#36890;&#36807;&#23558;&#32531;&#23384;&#31639;&#27861;&#21644;&#27169;&#22411;&#22797;&#29992;&#22120;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#37117;&#23454;&#29616;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;&#23454;&#35777;&#27169;&#25311;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32531;&#23384;&#21644;&#27169;&#22411;&#22797;&#29992;&#31639;&#27861;&#30340;&#32452;&#21512;&#22823;&#22823;&#25552;&#39640;&#20102;&#20256;&#32479;&#27169;&#22411;&#25512;&#29702;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) and other large foundation models have achieved noteworthy success, but their size exacerbates existing resource consumption and latency challenges. In particular, the large-scale deployment of these models is hindered by the significant resource requirements during inference. In this paper, we study two approaches for mitigating these challenges: employing a cache to store previous queries and learning a model multiplexer to choose from an ensemble of models for query processing.  Theoretically, we provide an optimal algorithm for jointly optimizing both approaches to reduce the inference cost in both offline and online tabular settings. By combining a caching algorithm, namely Greedy Dual Size with Frequency (GDSF) or Least Expected Cost (LEC), with a model multiplexer, we achieve optimal rates in both offline and online settings. Empirically, simulations show that the combination of our caching and model multiplexing algorithms greatly improves over the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22359;&#32423;&#24182;&#34892;Transformer&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#26356;&#38271;32&#20493;&#30340;&#35757;&#32451;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.19370</link><description>&lt;p&gt;
&#22823;&#22411;&#38271;&#24207;&#21015;&#27169;&#22411;&#30340;&#22359;&#32423;&#24182;&#34892;Transformer
&lt;/p&gt;
&lt;p&gt;
Blockwise Parallel Transformer for Long Context Large Models. (arXiv:2305.19370v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22359;&#32423;&#24182;&#34892;Transformer&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#26356;&#38271;32&#20493;&#30340;&#35757;&#32451;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#22522;&#30707;&#65292;&#22312;&#21508;&#31181;AI&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#22823;&#22411;&#21069;&#39304;&#32593;&#32476;&#25152;&#38656;&#30340;&#20869;&#23384;&#23481;&#37327;&#38480;&#21046;&#20102;&#23427;&#20204;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20026;&#28041;&#21450;&#22810;&#20010;&#38271;&#24207;&#21015;&#25110;&#38271;&#26399;&#20381;&#36182;&#30340;&#20219;&#21153;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#22359;&#32423;&#24182;&#34892;Transformer&#65288;BPT&#65289;&#65292;&#23427;&#21033;&#29992;&#22359;&#32423;&#35745;&#31639;&#33258;&#25105;&#27880;&#24847;&#21644;&#21069;&#39304;&#32593;&#32476;&#34701;&#21512;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#12290;&#36890;&#36807;&#22312;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#30340;&#21516;&#26102;&#22788;&#29702;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#65292;BPT&#20351;&#35757;&#32451;&#24207;&#21015;&#30340;&#38271;&#24230;&#27604;&#21407;&#22987;&#30340;Transformer&#38271;32&#20493;&#65292;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#38271;2&#21040;4&#20493;&#12290;&#23545;&#35821;&#35328;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;BPT&#22312;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#25552;&#39640;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving perfo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#20219;&#21153;&#20013;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22238;&#25253;&#20989;&#25968;&#30340;&#38543;&#26426;&#21338;&#24328;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#26426;&#21046;&#30340;&#31639;&#27861;&#65292;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#30340;Q&#20989;&#25968;&#23558;&#20250;&#25910;&#25947;&#20110;&#19968;&#20010;&#32435;&#20160;&#22343;&#34913;&#30340;Q&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#38454;&#27573;&#21338;&#24328;&#20855;&#26377;&#20840;&#23616;&#26368;&#20248;&#28857;&#25110;&#32773;&#38797;&#28857;&#65292;&#24182;&#19988;&#26234;&#33021;&#20307;&#22522;&#20110;&#36825;&#19968;&#28857;&#36827;&#34892;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2305.17372</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#20351;&#29992;&#22870;&#21169;&#26426;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning With Reward Machines in Stochastic Games. (arXiv:2305.17372v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17372
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#20219;&#21153;&#20013;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22238;&#25253;&#20989;&#25968;&#30340;&#38543;&#26426;&#21338;&#24328;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#26426;&#21046;&#30340;&#31639;&#27861;&#65292;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#30340;Q&#20989;&#25968;&#23558;&#20250;&#25910;&#25947;&#20110;&#19968;&#20010;&#32435;&#20160;&#22343;&#34913;&#30340;Q&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#38454;&#27573;&#21338;&#24328;&#20855;&#26377;&#20840;&#23616;&#26368;&#20248;&#28857;&#25110;&#32773;&#38797;&#28857;&#65292;&#24182;&#19988;&#26234;&#33021;&#20307;&#22522;&#20110;&#36825;&#19968;&#28857;&#36827;&#34892;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22797;&#26434;&#20219;&#21153;&#20013;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#22238;&#25253;&#20989;&#25968;&#30340;&#38543;&#26426;&#21338;&#24328;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;&#22870;&#21169;&#26426;&#21046;&#26469;&#25972;&#21512;&#39640;&#23618;&#27425;&#30340;&#22797;&#26434;&#20219;&#21153;&#30693;&#35782;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31216;&#20026;QRM-SG&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#12290;&#22312;QRM-SG&#20013;&#65292;&#25105;&#20204;&#22312;&#22686;&#24191;&#29366;&#24577;&#31354;&#38388;&#20013;&#23450;&#20041;&#20102;&#32435;&#20160;&#22343;&#34913;&#19979;&#30340;Q&#20989;&#25968;&#12290;&#22686;&#24191;&#29366;&#24577;&#31354;&#38388;&#25972;&#21512;&#20102;&#38543;&#26426;&#21338;&#24328;&#30340;&#29366;&#24577;&#21644;&#22870;&#21169;&#26426;&#21046;&#30340;&#29366;&#24577;&#12290;&#27599;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#20102;&#31995;&#32479;&#20013;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;Q&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;QRM-SG&#20013;&#23398;&#20064;&#30340;Q&#20989;&#25968;&#23558;&#20250;&#25910;&#25947;&#20110;&#19968;&#20010;&#32435;&#20160;&#22343;&#34913;&#30340;Q&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#38454;&#27573;&#21338;&#24328;&#20855;&#26377;&#20840;&#23616;&#26368;&#20248;&#28857;&#25110;&#32773;&#38797;&#28857;&#65292;&#24182;&#19988;&#26234;&#33021;&#20307;&#22522;&#20110;&#36825;&#19968;&#28857;&#36827;&#34892;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#30340;Q&#20989;&#25968;&#26356;&#26032;&#12290;&#25105;&#20204;&#20351;&#29992;Lemke-Howson&#26041;&#27861;&#26469;&#24471;&#20986;&#32473;&#23450;&#24403;&#21069;Q&#20989;&#25968;&#26102;&#30340;&#26368;&#20339;&#24212;&#31572;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate multi-agent reinforcement learning for stochastic games with complex tasks, where the reward functions are non-Markovian. We utilize reward machines to incorporate high-level knowledge of complex tasks. We develop an algorithm called Q-learning with reward machines for stochastic games (QRM-SG), to learn the best-response strategy at Nash equilibrium for each agent. In QRM-SG, we define the Q-function at a Nash equilibrium in augmented state space. The augmented state space integrates the state of the stochastic game and the state of reward machines. Each agent learns the Q-functions of all agents in the system. We prove that Q-functions learned in QRM-SG converge to the Q-functions at a Nash equilibrium if the stage game at each time step during learning has a global optimum point or a saddle point, and the agents update Q-functions based on the best-response strategy at this point. We use the Lemke-Howson method to derive the best-response strategy given current Q-func
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#39640;&#21361;&#23381;&#20135;&#22919;&#35745;&#21010;&#65292;&#25552;&#20986;&#20102;&#26089;&#26399;&#22922;&#23072;&#26816;&#27979;&#12289;&#20934;&#30830;&#35782;&#21035;&#39640;&#39118;&#38505;&#20250;&#21592;&#21644;&#25552;&#20379;&#21487;&#35299;&#37322;&#25351;&#26631;&#31561;&#19977;&#20010;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#23381;&#26399;&#39118;&#38505;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.17261</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20851;&#38381;&#39640;&#21361;&#23381;&#20135;&#22919;&#25252;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Closing the Gap in High-Risk Pregnancy Care Using Machine Learning and Human-AI Collaboration. (arXiv:2305.17261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#39640;&#21361;&#23381;&#20135;&#22919;&#35745;&#21010;&#65292;&#25552;&#20986;&#20102;&#26089;&#26399;&#22922;&#23072;&#26816;&#27979;&#12289;&#20934;&#30830;&#35782;&#21035;&#39640;&#39118;&#38505;&#20250;&#21592;&#21644;&#25552;&#20379;&#21487;&#35299;&#37322;&#25351;&#26631;&#31561;&#19977;&#20010;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#23381;&#26399;&#39118;&#38505;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#20445;&#38505;&#20844;&#21496;&#36890;&#24120;&#20351;&#29992;&#31639;&#27861;&#26469;&#35782;&#21035;&#20250;&#21463;&#30410;&#20110;&#25252;&#29702;&#21644;&#29366;&#20917;&#31649;&#29702;&#35745;&#21010;&#30340;&#20250;&#21592;&#65292;&#35813;&#35745;&#21010;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#39640;&#31471;&#20020;&#24202;&#25903;&#25345;&#12290;&#31639;&#27861;&#35782;&#21035;&#19982;&#20020;&#24202;&#24178;&#39044;&#20043;&#38388;&#30340;&#21450;&#26102;&#12289;&#20934;&#30830;&#21644;&#26080;&#32541;&#38598;&#25104;&#21462;&#20915;&#20110;&#31995;&#32479;&#35774;&#35745;&#24072;&#21644;&#25252;&#29702;&#31649;&#29702;&#21592;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;&#12290;&#25105;&#20204;&#20851;&#27880;&#20102;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#23381;&#20135;&#22919;&#19981;&#33391;&#20135;&#21069;&#12289;&#20135;&#26399;&#21644;&#20135;&#21518;&#20107;&#20214;&#30340;&#39640;&#21361;&#23381;&#20135;&#22919;&#35745;&#21010;&#65292;&#24182;&#25551;&#36848;&#20102;&#25105;&#20204;&#22914;&#20309;&#20811;&#26381;&#25252;&#29702;&#31649;&#29702;&#21592;&#25152;&#36848;&#30340;&#19977;&#20010;HRP&#35745;&#21010;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#26089;&#26399;&#26816;&#27979;&#22922;&#23072;&#65292;&#65288;2&#65289;&#20934;&#30830;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#39640;&#39118;&#38505;&#20250;&#21592;&#65292;&#20197;&#21450;&#65288;3&#65289;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#25351;&#26631;&#26469;&#34917;&#20805;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23381;&#26399;&#35782;&#21035;&#31639;&#27861;&#65292;&#22312;&#22238;&#39038;&#24615;&#30740;&#31350;&#20013;&#27604;&#20043;&#21069;&#22522;&#20110;&#20195;&#30721;&#30340;&#27169;&#22411;&#25552;&#21069;&#20102;57&#22825;&#35782;&#21035;&#22922;&#23072;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#20250;&#24433;&#21709;&#23381;&#26399;&#30340;&#24182;&#21457;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Health insurers often use algorithms to identify members who would benefit from care and condition management programs, which provide personalized, high-touch clinical support. Timely, accurate, and seamless integration between algorithmic identification and clinical intervention depends on effective collaboration between the system designers and nurse care managers. We focus on a high-risk pregnancy (HRP) program designed to reduce the likelihood of adverse prenatal, perinatal, and postnatal events and describe how we overcome three challenges of HRP programs as articulated by nurse care managers; (1) early detection of pregnancy, (2) accurate identification of impactable high-risk members, and (3) provision of explainable indicators to supplement predictions. We propose a novel algorithm for pregnancy identification that identifies pregnancies 57 days earlier than previous code-based models in a retrospective study. We then build a model to predict impactable pregnancy complications 
&lt;/p&gt;</description></item><item><title>Scissorhands&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21033;&#29992;&#37325;&#35201;&#24615;&#25345;&#20037;&#24615;&#20551;&#35774;&#23558;LLM KV&#32531;&#23384;&#30340;&#20869;&#23384;&#20351;&#29992;&#32500;&#25345;&#22312;&#22266;&#23450;&#39044;&#31639;&#20869;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.17118</link><description>&lt;p&gt;
&#21098;&#20992;&#25163;&#65306;&#21033;&#29992;&#37325;&#35201;&#24615;&#25345;&#20037;&#24615;&#20551;&#35774;&#22312;&#27979;&#35797;&#26102;&#23545;LLM KV&#32531;&#23384;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time. (arXiv:2305.17118v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17118
&lt;/p&gt;
&lt;p&gt;
Scissorhands&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21033;&#29992;&#37325;&#35201;&#24615;&#25345;&#20037;&#24615;&#20551;&#35774;&#23558;LLM KV&#32531;&#23384;&#30340;&#20869;&#23384;&#20351;&#29992;&#32500;&#25345;&#22312;&#22266;&#23450;&#39044;&#31639;&#20869;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#21457;&#20102;&#19968;&#27874;&#26032;&#30340;&#20196;&#20154;&#20852;&#22859;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;&#22823;&#35268;&#27169;&#25176;&#31649;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#36164;&#28304;&#12290;&#37096;&#32626;&#36807;&#31243;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#20869;&#23384;&#29942;&#39048;&#26469;&#33258;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#27169;&#22411;&#26435;&#37325;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#65307;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23384;&#20648;&#30340;&#38190;&#20540;&#23884;&#20837;&#22823;&#23567;&#65288;KV&#32531;&#23384;&#65289;&#24448;&#24448;&#36229;&#36807;&#20102;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#24040;&#22823;&#30340;KV&#32531;&#23384;&#22823;&#23567;&#23545;&#20110;&#20851;&#38190;&#23383;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#25512;&#29702;&#20135;&#29983;&#32422;&#26463;&#65292;&#36825;&#23545;&#20110;&#39640;&#21534;&#21520;&#37327;&#30340;&#25512;&#29702;&#24037;&#20316;&#36127;&#36733;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;&#21040;&#27880;&#24847;&#21147;&#20998;&#25968;&#30340;&#26377;&#36259;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25345;&#20037;&#24615;&#37325;&#35201;&#24615;&#30340;&#20551;&#35774;&#65306;&#21482;&#26377;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#20851;&#38190;&#26631;&#35760;&#65292;&#22312;&#19968;&#27493;&#20013;&#26377;&#23454;&#36136;&#24615;&#24433;&#21709;&#65292;&#25165;&#20250;&#22312;&#26410;&#26469;&#30340;&#29983;&#25104;&#20013;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22522;&#20110;&#23545;&#36825;&#19968;&#20551;&#35774;&#30340;&#32463;&#39564;&#39564;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21098;&#20992;&#25163;&#65292;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#24494;&#35843;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23558;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#20351;&#29992;&#32500;&#25345;&#22312;&#22266;&#23450;&#39044;&#31639;&#20869;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose Scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model. In 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#38598;&#21512;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#25110;&#26377;&#38480;&#32500;&#21472;&#21152;&#27867;&#21270;&#27169;&#22411;&#20013;&#36873;&#25321;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#24615;&#33021;&#30340;&#26368;&#20248;&#21472;&#21152;&#27867;&#21270;&#19982;&#26368;&#20248;&#35299;&#24615;&#33021;&#30456;&#36817;&#12290;</title><link>http://arxiv.org/abs/2305.15786</link><description>&lt;p&gt;
&#23398;&#20064;&#38598;&#21512;&#31574;&#30053;&#30340;&#29702;&#35770;&#20445;&#35777;&#21450;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Theoretical Guarantees of Learning Ensembling Strategies with Applications to Time Series Forecasting. (arXiv:2305.15786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#38598;&#21512;&#31574;&#30053;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#25110;&#26377;&#38480;&#32500;&#21472;&#21152;&#27867;&#21270;&#27169;&#22411;&#20013;&#36873;&#25321;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#24615;&#33021;&#30340;&#26368;&#20248;&#21472;&#21152;&#27867;&#21270;&#19982;&#26368;&#20248;&#35299;&#24615;&#33021;&#30456;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#21512;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#20943;&#23569;&#26041;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#38024;&#23545;&#40657;&#30418;&#22522;&#23398;&#20064;&#22120;&#30340;&#22823;&#22810;&#25968;&#38598;&#21512;&#26041;&#27861;&#37117;&#23646;&#20110;&#8220;&#21472;&#21152;&#27867;&#21270;&#8221;&#33539;&#30068;&#65292;&#21363;&#35757;&#32451;&#19968;&#20010;&#25509;&#21463;&#22522;&#23398;&#20064;&#22120;&#25512;&#29702;&#20316;&#20026;&#36755;&#20837;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#34429;&#28982;&#21472;&#21152;&#27867;&#21270;&#22312;&#23454;&#36341;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#29702;&#35770;&#24615;&#36136;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#19968;&#20010;&#26032;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#36873;&#25321;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#24615;&#33021;&#30340;&#8220;&#26377;&#38480;&#25110;&#26377;&#38480;&#32500;&#8221;&#21472;&#21152;&#27867;&#21270;&#20013;&#30340;&#26368;&#20339;&#21472;&#21152;&#27867;&#21270;&#24182;&#19981;&#27604;&#26368;&#20248;&#35299;&#34920;&#29616;&#8220;&#24046;&#24471;&#22810;&#8221;&#12290;&#36825;&#19968;&#32467;&#26524;&#21152;&#24378;&#21644;&#22823;&#22823;&#25193;&#23637;&#20102;Van der Laan&#31561;&#20154;&#65288;2007&#24180;&#65289;&#30340;&#32467;&#26524;&#12290;&#21463;&#21040;&#29702;&#35770;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#27010;&#29575;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#25935;&#24863;&#24615;&#30340;&#21472;&#21152;&#27867;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensembling is among the most popular tools in machine learning (ML) due to its effectiveness in minimizing variance and thus improving generalization. Most ensembling methods for black-box base learners fall under the umbrella of "stacked generalization," namely training an ML algorithm that takes the inferences from the base learners as input. While stacking has been widely applied in practice, its theoretical properties are poorly understood. In this paper, we prove a novel result, showing that choosing the best stacked generalization from a (finite or finite-dimensional) family of stacked generalizations based on cross-validated performance does not perform "much worse" than the oracle best. Our result strengthens and significantly extends the results in Van der Laan et al. (2007). Inspired by the theoretical analysis, we further propose a particular family of stacked generalizations in the context of probabilistic forecasting, each one with a different sensitivity for how much the 
&lt;/p&gt;</description></item><item><title>torchgfn&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#26500;&#24314;&#30340;GFlowNet&#24211;&#65292;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;API&#21644;&#26377;&#29992;&#30340;&#25277;&#35937;&#65292;&#35299;&#20915;&#20102;&#19981;&#21516;&#20195;&#30721;&#24211;&#30340;&#32422;&#23450;&#38382;&#39064;&#65292;&#24182;&#19988;&#37325;&#29616;&#20102;&#24050;&#21457;&#34920;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14594</link><description>&lt;p&gt;
torchgfn&#65306;&#19968;&#20010;PyTorch GFlowNet&#24211;
&lt;/p&gt;
&lt;p&gt;
torchgfn: A PyTorch GFlowNet library. (arXiv:2305.14594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14594
&lt;/p&gt;
&lt;p&gt;
torchgfn&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#26500;&#24314;&#30340;GFlowNet&#24211;&#65292;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;API&#21644;&#26377;&#29992;&#30340;&#25277;&#35937;&#65292;&#35299;&#20915;&#20102;&#19981;&#21516;&#20195;&#30721;&#24211;&#30340;&#32422;&#23450;&#38382;&#39064;&#65292;&#24182;&#19988;&#37325;&#29616;&#20102;&#24050;&#21457;&#34920;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#25110;GFNs&#65289;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#20195;&#30721;&#26469;&#28304;&#20063;&#21464;&#24471;&#36234;&#26469;&#36234;&#22810;&#12290;&#36825;&#20250;&#22952;&#30861;&#23454;&#29616;&#26032;&#21151;&#33021;&#65288;&#20363;&#22914;&#35757;&#32451;&#25439;&#22833;&#65289;&#65292;&#36825;&#20123;&#21151;&#33021;&#21487;&#20197;&#36731;&#26494;&#22320;&#19982;&#29616;&#26377;&#21151;&#33021;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#22312;&#19968;&#32452;&#24120;&#35265;&#29615;&#22659;&#20013;&#36827;&#34892;&#20351;&#29992;&#12290;&#38500;&#20102;&#20943;&#32531;GFlowNets&#39046;&#22495;&#30340;&#30740;&#31350;&#22806;&#65292;&#19981;&#21516;&#30340;&#20195;&#30721;&#24211;&#36824;&#20351;&#29992;&#19981;&#21516;&#30340;&#32422;&#23450;&#65292;&#21487;&#33021;&#20250;&#35753;&#26032;&#25163;&#24863;&#21040;&#22256;&#24785;&#12290;torchgfn&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#26500;&#24314;&#30340;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#23427;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;API&#21644;&#26377;&#29992;&#30340;&#25277;&#35937;&#65292;&#20197;&#23454;&#29616;&#37319;&#26679;&#22120;&#21644;&#25439;&#22833;&#20989;&#25968;&#12290;&#25552;&#20379;&#20102;&#22810;&#20010;&#31034;&#20363;&#65292;&#37325;&#29616;&#20102;&#24050;&#21457;&#34920;&#30340;&#32467;&#26524;&#12290;&#35813;&#20195;&#30721;&#21487;&#22312;https://github.com/saleml/torchgfn&#20013;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing popularity of generative flow networks (GFlowNets or GFNs) is accompanied with a proliferation of code sources. This hinders the implementation of new features, such as training losses, that can readily be compared to existing ones, on a set of common environments. In addition to slowing down research in the field of GFlowNets, different code bases use different conventions, that might be confusing for newcomers. `torchgfn` is a library built on top of PyTorch, that aims at addressing both problems. It provides user with a simple API for environments, and useful abstractions for samplers and losses. Multiple examples are provided, replicating published results. The code is available in https://github.com/saleml/torchgfn.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VIC-DDPM&#30340;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#21487;&#35265;&#24230;&#25968;&#25454;&#21644;&#33039;&#22270;&#20687;&#30340;&#24110;&#21161;&#19979;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#32454;&#33410;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#28040;&#38500;&#22122;&#22768;&#21644;&#20266;&#24433;&#12290;&#30456;&#20851;&#23454;&#39564;&#35777;&#23454;&#65292;&#35813;&#31639;&#27861;&#22312;&#24674;&#22797;&#24494;&#24369;&#20449;&#21495;&#12289;&#20445;&#30041;&#32454;&#33410;&#32467;&#26500;&#21644;&#28040;&#38500;&#20266;&#24433;&#31561;&#26041;&#38754;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09121</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23556;&#30005;&#24178;&#28041;&#25104;&#20687;&#37325;&#24314;&#30340;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Conditional Denoising Diffusion Probabilistic Model for Radio Interferometric Image Reconstruction. (arXiv:2305.09121v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VIC-DDPM&#30340;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#22312;&#21487;&#35265;&#24230;&#25968;&#25454;&#21644;&#33039;&#22270;&#20687;&#30340;&#24110;&#21161;&#19979;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#32454;&#33410;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#28040;&#38500;&#22122;&#22768;&#21644;&#20266;&#24433;&#12290;&#30456;&#20851;&#23454;&#39564;&#35777;&#23454;&#65292;&#35813;&#31639;&#27861;&#22312;&#24674;&#22797;&#24494;&#24369;&#20449;&#21495;&#12289;&#20445;&#30041;&#32454;&#33410;&#32467;&#26500;&#21644;&#28040;&#38500;&#20266;&#24433;&#31561;&#26041;&#38754;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#65292;&#23556;&#30005;&#26395;&#36828;&#38236;&#25509;&#25910;&#21040;&#30340;&#20449;&#21495;&#20250;&#36716;&#25442;&#20026;&#22825;&#20307;&#23545;&#35937;&#30340;&#22270;&#20687;&#12290;&#20294;&#36825;&#20123;&#22270;&#20687;&#36890;&#24120;&#20250;&#21253;&#21547;&#20266;&#28304;&#21644;&#20854;&#20182;&#22240;&#32032;&#23548;&#33268;&#30340;&#20266;&#24433;&#65292;&#31216;&#20026;&#8220;&#33039;&#22270;&#20687;&#8221;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#23545;&#33039;&#22270;&#20687;&#36827;&#34892;&#37325;&#24314;&#20197;&#33719;&#21462;&#26356;&#24178;&#20928;&#30340;&#22270;&#20687;&#24182;&#28040;&#38500;&#20266;&#24433;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#24674;&#22797;&#24494;&#24369;&#20449;&#21495;&#65292;&#20445;&#30041;&#32454;&#33410;&#32467;&#26500;&#21644;&#28040;&#38500;&#20266;&#24433;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VIC-DDPM&#30340;&#21487;&#35265;&#24230;&#21644;&#22270;&#20687;&#26465;&#20214;&#19979;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21516;&#26102;&#21033;&#29992;&#26102;&#22495;&#21644;&#31354;&#22495;&#30340;&#20449;&#24687;&#26469;&#29983;&#25104;&#22270;&#20687;&#65292;&#20197;&#36798;&#21040;&#28040;&#38500;&#22122;&#22768;&#21644;&#29983;&#25104;&#26356;&#32454;&#33410;&#30340;&#22270;&#20687;&#30340;&#30446;&#30340;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#23792;&#20540;&#20449;&#22122;&#27604;&#12289;&#35270;&#35273;&#36136;&#37327;&#21644;&#24674;&#22797;&#24494;&#24369;&#20449;&#21495;&#30340;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In radio astronomy, signals from radio telescopes are transformed into images of observed celestial objects, or sources. However, these images, called dirty images, contain real sources as well as artifacts due to signal sparsity and other factors. Therefore, radio interferometric image reconstruction is performed on dirty images, aiming to produce clean images in which artifacts are reduced and real sources are recovered. So far, existing methods have limited success on recovering faint sources, preserving detailed structures, and eliminating artifacts. In this paper, we present VIC-DDPM, a Visibility and Image Conditioned Denoising Diffusion Probabilistic Model. Our main idea is to use both the original visibility data in the spectral domain and dirty images in the spatial domain to guide the image generation process with DDPM. This way, we can leverage DDPM to generate fine details and eliminate noise, while utilizing visibility data to separate signals from noise and retaining spat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;RWSADMM&#65292;&#20197;&#35299;&#20915;&#22312;&#21160;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#19981;&#19968;&#33268;&#21644;&#36890;&#20449;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12534</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#34892;&#36208;&#38543;&#26426;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#31639;&#27861;&#25512;&#21160;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mobilizing Personalized Federated Learning via Random Walk Stochastic ADMM. (arXiv:2304.12534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;RWSADMM&#65292;&#20197;&#35299;&#20915;&#22312;&#21160;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#19981;&#19968;&#33268;&#21644;&#36890;&#20449;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26102;&#23384;&#22312;&#30340;&#38556;&#30861;&#65292;&#20854;&#20013;&#19981;&#33021;&#32500;&#25252;&#20013;&#22830;&#26381;&#21153;&#22120;&#19982;&#25152;&#26377;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#19968;&#33268;&#36830;&#25509;&#65292;&#24182;&#19988;&#25968;&#25454;&#20998;&#24067;&#26159;&#24322;&#26500;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21160;&#24577;&#32852;&#37030;&#23398;&#20064;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#22312;&#30456;&#37051;&#23458;&#25143;&#31471;&#32452;&#20043;&#38388;&#31227;&#21160;&#20197;&#23398;&#20064;&#26412;&#22320;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21363;&#38543;&#26426;&#34892;&#36208;&#38543;&#26426;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#31639;&#27861;&#65288;RWSADMM&#65289;&#65292;&#21482;&#35201;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#36830;&#25509;&#23458;&#25143;&#31471;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#65292;&#23601;&#33021;&#36866;&#24212;&#21160;&#24577;&#21644;&#21363;&#24109;&#32593;&#32476;&#26465;&#20214;&#12290;&#22312;RWSADMM&#20013;&#65292;&#26381;&#21153;&#22120;&#38543;&#26426;&#21521;&#19968;&#32452;&#23458;&#25143;&#31471;&#34892;&#36208;&#12290;&#23427;&#22522;&#20110;&#30828;&#19981;&#31561;&#24335;&#32422;&#26463;&#24418;&#25104;&#30456;&#37051;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#23616;&#37096;&#36817;&#20284;&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#19968;&#33268;&#26356;&#26032;&#26469;&#35299;&#20915;&#25968;&#25454;&#24322;&#26500;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#25910;&#25947;&#30340;&#65292;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#65292;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we investigate the barriers associated with implementing Federated Learning (FL) in real-world scenarios, where a consistent connection between the central server and all clients cannot be maintained, and data distribution is heterogeneous. To address these challenges, we focus on mobilizing the federated setting, where the server moves between groups of adjacent clients to learn local models. Specifically, we propose a new algorithm, Random Walk Stochastic Alternating Direction Method of Multipliers (RWSADMM), capable of adapting to dynamic and ad-hoc network conditions as long as a sufficient number of connected clients are available for model training. In RWSADMM, the server walks randomly toward a group of clients. It formulates local proximity among adjacent clients based on hard inequality constraints instead of consensus updates to address data heterogeneity. Our proposed method is convergent, reduces communication costs, and enhances scalability by reducing th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31526;&#21495;&#35745;&#31639;&#20013;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26696;&#20363;&#65292;&#36890;&#36807;&#20351;&#29992;SHAP&#24037;&#20855;&#20026;&#22278;&#26609;&#20195;&#25968;&#20998;&#35299;&#30340;&#21464;&#37327;&#25490;&#24207;&#25552;&#20379;&#26032;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20174;&#32780;&#20026;&#31526;&#21495;&#35745;&#31639;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2304.12154</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#27934;&#23519;&#22312;&#31526;&#21495;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;&#22278;&#26609;&#20195;&#25968;&#20998;&#35299;&#21464;&#37327;&#25490;&#24207;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explainable AI Insights for Symbolic Computation: A case study on selecting the variable ordering for cylindrical algebraic decomposition. (arXiv:2304.12154v2 [cs.SC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31526;&#21495;&#35745;&#31639;&#20013;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26696;&#20363;&#65292;&#36890;&#36807;&#20351;&#29992;SHAP&#24037;&#20855;&#20026;&#22278;&#26609;&#20195;&#25968;&#20998;&#35299;&#30340;&#21464;&#37327;&#25490;&#24207;&#25552;&#20379;&#26032;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20174;&#32780;&#20026;&#31526;&#21495;&#35745;&#31639;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22312;&#25968;&#23398;&#39046;&#22495;&#20013;&#23545;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#30340;&#20351;&#29992;&#26377;&#25152;&#22686;&#21152;&#65292;&#21253;&#25324;&#22312;&#31526;&#21495;&#35745;&#31639;&#20013;&#65292;&#21487;&#20197;&#23433;&#20840;&#22320;&#24212;&#29992;ML&#26469;&#20248;&#21270;&#25110;&#36873;&#25321;&#31639;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#36825;&#20123;ML&#27169;&#22411;&#19978;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#26159;&#21542;&#21487;&#20197;&#20026;&#31526;&#21495;&#35745;&#31639;&#25552;&#20379;&#26032;&#30340;&#27934;&#23519;&#65292;&#20197;&#28608;&#21457;&#22312;&#35745;&#31639;&#26426;&#20195;&#25968;&#31995;&#32479;&#20013;&#23454;&#29616;&#26032;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#30452;&#25509;&#35843;&#29992;AI&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;ML&#20026;&#22522;&#30784;&#26469;&#36873;&#25321;&#22278;&#26609;&#20195;&#25968;&#20998;&#35299;&#21464;&#37327;&#25490;&#24207;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#24050;&#32463;&#35777;&#26126;ML&#21487;&#20197;&#36827;&#34892;&#24456;&#22909;&#30340;&#36873;&#25321;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;SHAP&#26469;&#20026;&#31526;&#21495;&#35745;&#31639;&#20013;&#24120;&#29992;&#30340;&#20154;&#24037;&#35774;&#35745;&#21551;&#21457;&#24335;&#26041;&#27861;&#25552;&#20379;&#26032;&#30340;&#21551;&#31034;&#65292;&#20854;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years there has been increased use of machine learning (ML) techniques within mathematics, including symbolic computation where it may be applied safely to optimise or select algorithms. This paper explores whether using explainable AI (XAI) techniques on such ML models can offer new insight for symbolic computation, inspiring new implementations within computer algebra systems that do not directly call upon AI tools. We present a case study on the use of ML to select the variable ordering for cylindrical algebraic decomposition. It has already been demonstrated that ML can make the choice well, but here we show how the SHAP tool for explainability can be used to inform new heuristics of a size and complexity similar to those human-designed heuristics currently commonly used in symbolic computation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#31532;&#19968;&#20010;&#23454;&#39564;&#35774;&#32622;&#65292;&#25506;&#35752;&#20102;LiDAR&#35821;&#20041;&#20998;&#21106;&#22312;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#36328;&#39046;&#22495;&#35774;&#32622;&#20013;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31232;&#30095;-&#23494;&#38598;&#21367;&#31215;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#30340;&#26174;&#33879;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.11705</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;LiDAR&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#26041;&#27861;&#30740;&#31350;&#21644;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation. (arXiv:2304.11705v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#31532;&#19968;&#20010;&#23454;&#39564;&#35774;&#32622;&#65292;&#25506;&#35752;&#20102;LiDAR&#35821;&#20041;&#20998;&#21106;&#22312;&#19981;&#21516;&#39046;&#22495;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#36328;&#39046;&#22495;&#35774;&#32622;&#20013;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#31232;&#30095;-&#23494;&#38598;&#21367;&#31215;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#30340;&#26174;&#33879;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#36807;&#31243;&#20013;&#65292;&#33021;&#22815;&#23433;&#20840;&#22320;&#22312;&#22810;&#26679;&#21270;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#22312;&#21516;&#19968;&#39046;&#22495;&#30340;LiDAR&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#27867;&#21270;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#39046;&#22495;&#27867;&#21270;&#65288;DG-LSS&#65289;&#30340;&#23454;&#39564;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36328;&#39046;&#22495;&#30340;&#35774;&#32622;&#20013;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65306;&#20363;&#22914;&#65292;&#22312;&#28304;&#25968;&#25454;&#38598;&#65288;SemanticKITTI&#65289;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30446;&#26631;&#25968;&#25454;&#19978;&#33719;&#24471;&#20102;26.53&#30340;mIoU&#65292;&#32780;&#22312;&#30446;&#26631;&#22495;&#65288;nuScenes&#65289;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21017;&#33719;&#24471;&#20102;48.49&#30340;mIoU&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;DG-LSS&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#33719;&#24471;&#20102;34.88&#30340;mIoU&#65292;&#36229;&#36234;&#20102;&#25152;&#26377;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#31232;&#30095;&#21367;&#31215;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;3D&#20998;&#21106;&#32593;&#32476;&#19982;&#39069;&#22806;&#30340;&#23494;&#38598;2D&#21367;&#31215;&#30456;&#32467;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;
The ability to deploy robots that can operate safely in diverse environments is crucial for developing embodied intelligent agents. As a community, we have made tremendous progress in within-domain LiDAR semantic segmentation. However, do these methods generalize across domains? To answer this question, we design the first experimental setup for studying domain generalization (DG) for LiDAR semantic segmentation (DG-LSS). Our results confirm a significant gap between methods, evaluated in a cross-domain setting: for example, a model trained on the source dataset (SemanticKITTI) obtains $26.53$ mIoU on the target data, compared to $48.49$ mIoU obtained by the model trained on the target domain (nuScenes). To tackle this gap, we propose the first method specifically designed for DG-LSS, which obtains $34.88$ mIoU on the target domain, outperforming all baselines. Our method augments a sparse-convolutional encoder-decoder 3D segmentation network with an additional, dense 2D convolutional 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; MLGCN&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#22330;&#26223;&#20013;&#21516;&#31867;&#20559;&#22909;&#30340;&#35821;&#20041;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MLGCN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10398</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-label Node Classification On Graph-Structured Data. (arXiv:2304.10398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; MLGCN&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#22330;&#26223;&#20013;&#21516;&#31867;&#20559;&#22909;&#30340;&#35821;&#20041;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MLGCN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#20013;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#25913;&#36827;&#12290;&#34429;&#28982;&#36825;&#20123;&#36827;&#23637;&#22312;&#22810;&#31867;&#20998;&#31867;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#23637;&#31034;&#65292;&#20294;&#19968;&#20010;&#26356;&#26222;&#36941;&#21644;&#29616;&#23454;&#30340;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#33410;&#28857;&#21487;&#33021;&#26377;&#22810;&#20010;&#26631;&#31614;&#65292;&#19968;&#30452;&#20197;&#26469;&#21463;&#21040;&#20102;&#24456;&#23569;&#20851;&#27880;&#12290;&#22312;&#36827;&#34892;&#20851;&#20110;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#30340;&#37325;&#28857;&#30740;&#31350;&#30340;&#39318;&#35201;&#25361;&#25112;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#26631;&#31614;&#22270;&#25968;&#25454;&#38598;&#25968;&#37327;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19977;&#20010;&#30495;&#23454;&#30340;&#29983;&#29289;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#26631;&#31614;&#22270;&#29983;&#25104;&#22120;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#21487;&#35843;&#23646;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;&#39640;&#26631;&#31614;&#30456;&#20284;&#24615;&#65288;&#39640;&#21516;&#31867;&#20559;&#22909;&#65289;&#36890;&#24120;&#34987;&#24402;&#22240;&#20110;GNN&#30340;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#22810;&#26631;&#31614;&#22330;&#26223;&#24182;&#19981;&#36981;&#24490;&#30446;&#21069;&#20026;&#22810;&#31867;&#22330;&#26223;&#23450;&#20041;&#30340;&#21516;&#31867;&#20559;&#22909;&#21644;&#24322;&#31867;&#20559;&#22909;&#30340;&#24120;&#35268;&#35821;&#20041;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#38500;&#20102;&#20026;&#22810;&#26631;&#31614;&#22330;&#26223;&#23450;&#20041;&#21516;&#31867;&#20559;&#22909;&#22806;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;MLGCN&#65288;&#22810;&#26631;&#31614;&#22270;&#21367;&#31215;&#32593;&#32476;&#65289;&#65292;&#26469;&#22788;&#29702;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MLGCN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have shown state-of-the-art improvements in node classification tasks on graphs. While these improvements have been largely demonstrated in a multi-class classification scenario, a more general and realistic scenario in which each node could have multiple labels has so far received little attention. The first challenge in conducting focused studies on multi-label node classification is the limited number of publicly available multi-label graph datasets. Therefore, as our first contribution, we collect and release three real-world biological datasets and develop a multi-label graph generator to generate datasets with tunable properties. While high label similarity (high homophily) is usually attributed to the success of GNNs, we argue that a multi-label scenario does not follow the usual semantics of homophily and heterophily so far defined for a multi-class scenario. As our second contribution, besides defining homophily for the multi-label scenario, we dev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#30456;&#20851;&#38745;&#24577;&#20998;&#26512;&#20135;&#21697;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#20013;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#28155;&#21152;&#26174;&#31034;&#20449;&#24687;&#26469;&#25552;&#21462;&#20195;&#30721;&#20013;&#30340;&#35821;&#20041;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2304.06815</link><description>&lt;p&gt;
&#29992;&#30456;&#20851;&#38745;&#24577;&#20998;&#26512;&#20135;&#21697;&#25913;&#21892;&#23569;&#26679;&#26412;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improving Few-Shot Prompts with Relevant Static Analysis Products. (arXiv:2304.06815v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#30456;&#20851;&#38745;&#24577;&#20998;&#26512;&#20135;&#21697;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#20013;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#28155;&#21152;&#26174;&#31034;&#20449;&#24687;&#26469;&#25552;&#21462;&#20195;&#30721;&#20013;&#30340;&#35821;&#20041;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#31867;&#26032;&#22411;&#35745;&#31639;&#24341;&#25806;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23454;&#29616;"&#32534;&#31243;"&#12290;&#25105;&#20204;&#20173;&#22312;&#23398;&#20064;&#22914;&#20309;&#26368;&#22909;&#22320;"&#32534;&#31243;"&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#36825;&#26679;&#19968;&#31181;&#30452;&#35273;&#20986;&#21457;&#65292;&#21363;&#24320;&#21457;&#20154;&#21592;&#22312;&#22788;&#29702;&#32534;&#30721;&#20219;&#21153;&#26102;&#20250;&#26377;&#19968;&#31995;&#21015;&#24847;&#35782;&#21644;&#26080;&#24847;&#35782;&#30340;&#35821;&#20041;&#20107;&#23454;&#12290;&#23545;&#20110;&#19968;&#20010;&#20989;&#25968;&#32780;&#35328;&#65292;&#36825;&#20123;&#35821;&#20041;&#20107;&#23454;&#21487;&#33021;&#21253;&#25324;&#21442;&#25968;&#21644;&#23616;&#37096;&#21464;&#37327;&#21517;&#31216;&#12289;&#36820;&#22238;&#34920;&#36798;&#24335;&#12289;&#31616;&#21333;&#30340;&#21069;&#32622;&#21644;&#21518;&#32622;&#26465;&#20214;&#20197;&#21450;&#22522;&#26412;&#30340;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#65292;&#31561;&#31561;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#20195;&#30721;&#25688;&#35201;&#20219;&#21153;&#24182;&#35780;&#20272;&#26159;&#21542;&#20351;&#29992;&#26174;&#24335;&#28155;&#21152;&#20449;&#24687;&#33021;&#22815;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#36825;&#20123;&#35821;&#20041;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) are a new class of computation engines, "programmed" via prompt engineering. We are still learning how to best "program" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously have a collection of semantics facts in mind when working on coding tasks. Mostly these are shallow, simple facts arising from a quick read. For a function, examples of facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.  One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them inherently capable of doing this simple level of "code analysis" and extracting such information, implicitly, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether auto
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#24320;&#25918;&#38598;(UIOS)&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#26679;&#26412;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#35745;&#31639;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#26469;&#34920;&#36798;&#20854;&#32622;&#20449;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20026;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#32593;&#33180;&#24322;&#24120;&#31579;&#26597;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.03981</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#24320;&#25918;&#38598;&#23398;&#20064;&#29992;&#20110;&#35270;&#32593;&#33180;&#24322;&#24120;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-inspired Open Set Learning for Retinal Anomaly Identification. (arXiv:2304.03981v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03981
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#24320;&#25918;&#38598;(UIOS)&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#26679;&#26412;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#35745;&#31639;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#26469;&#34920;&#36798;&#20854;&#32622;&#20449;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20026;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#32593;&#33180;&#24322;&#24120;&#31579;&#26597;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#23454;&#38469;&#35270;&#32593;&#33180;&#24322;&#24120;&#20998;&#31867;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#26080;&#27861;&#35782;&#21035;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#26679;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#24320;&#25918;&#38598;(UIOS)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;9&#20010;&#24120;&#35265;&#35270;&#32593;&#33180;&#30149;&#21464;&#30340;&#30524;&#24213;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#12290;&#38500;&#20102;&#27599;&#20010;&#31867;&#21035;&#30340;&#27010;&#29575;&#65292;UIOS&#36824;&#35745;&#31639;&#20102;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#26469;&#34920;&#36798;&#20854;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;UIOS&#27169;&#22411;&#36890;&#36807;&#35774;&#32622;&#38408;&#20540;&#31574;&#30053;&#65292;&#22312;&#20869;&#37096;&#27979;&#35797;&#25968;&#25454;&#38598;&#12289;&#22806;&#37096;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#38750;&#20856;&#22411;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#30340;F1&#20998;&#21035;&#36798;&#21040;&#20102;99.55&#65285;&#12289;97.01&#65285;&#21644;91.91&#65285;&#65292;&#30456;&#27604;&#26631;&#20934;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;F1&#20998;&#21035;&#20026;92.20&#65285;&#12289;80.69&#65285;&#21644;64.74&#65285;&#12290;&#27492;&#22806;&#65292;UIOS&#27491;&#30830;&#39044;&#27979;&#20102;&#32597;&#35265;&#35270;&#32593;&#33180;&#30142;&#30149;&#12289;&#20302;&#36136;&#37327;&#30524;&#24213;&#22270;&#20687;&#21644;&#38750;&#30524;&#24213;&#22270;&#20687;&#31561;&#25968;&#25454;&#38598;&#20013;&#30340;&#39640;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#65292;&#25552;&#31034;&#38656;&#35201;&#25163;&#21160;&#26816;&#26597;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#32593;&#33180;&#24322;&#24120;&#31579;&#26597;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Failure to recognize samples from the classes unseen during training is a major limit of artificial intelligence (AI) in real-world implementation of retinal anomaly classification. To resolve this obstacle, we propose an uncertainty-inspired open-set (UIOS) model which was trained with fundus images of 9 common retinal conditions. Besides the probability of each category, UIOS also calculates an uncertainty score to express its confidence. Our UIOS model with thresholding strategy achieved an F1 score of 99.55%, 97.01% and 91.91% for the internal testing set, external testing set and non-typical testing set, respectively, compared to the F1 score of 92.20%, 80.69% and 64.74% by the standard AI model. Furthermore, UIOS correctly predicted high uncertainty scores, which prompted the need for a manual check, in the datasets of rare retinal diseases, low-quality fundus images, and non-fundus images. This work provides a robust method for real-world screening of retinal anomalies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Wyner&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#22810;&#23618;&#25351;&#32441;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#29305;&#24449;&#20849;&#20139;&#30340;&#35774;&#22791;&#20449;&#24687;&#65292;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15860</link><description>&lt;p&gt;
Wyner&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#26080;&#30417;&#30563;&#22810;&#23618;&#26080;&#32447;&#25351;&#32441;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
The Wyner Variational Autoencoder for Unsupervised Multi-Layer Wireless Fingerprinting. (arXiv:2303.15860v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Wyner&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#22810;&#23618;&#25351;&#32441;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#29305;&#24449;&#20849;&#20139;&#30340;&#35774;&#22791;&#20449;&#24687;&#65292;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#25351;&#32441;&#35782;&#21035;&#26159;&#19968;&#31181;&#21033;&#29992;&#30828;&#20214;&#19981;&#23436;&#21892;&#21644;&#26080;&#32447;&#20449;&#36947;&#21464;&#21270;&#20316;&#20026;&#26631;&#35782;&#30340;&#35774;&#22791;&#35782;&#21035;&#26041;&#27861;&#12290;&#38500;&#20102;&#29289;&#29702;&#23618;&#29305;&#24449;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#32593;&#32476;&#27969;&#37327;&#65288;&#20363;&#22914;&#21253;&#38271;&#24230;&#65289;&#35782;&#21035;&#29992;&#25143;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#35299;&#23494;&#26377;&#25928;&#36733;&#33655;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#25351;&#32441;&#35782;&#21035;&#26694;&#26550;&#65292;&#32852;&#21512;&#32771;&#34385;&#22810;&#23618;&#29305;&#24449;&#20197;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#22810;&#35270;&#22270;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#21363;&#22810;&#24418;&#24335;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#32858;&#21512;&#22810;&#23618;&#29305;&#24449;&#20849;&#20139;&#30340;&#35774;&#22791;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20449;&#24687;&#35770;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#26377;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#35774;&#32622;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#25512;&#23548;&#33719;&#24471;&#31616;&#21333;&#30340;&#23548;&#20986;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#25152;&#21046;&#23450;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#33719;&#24471;&#32039;&#23494;&#30340;&#20195;&#29702;&#30028;&#38480;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#20248;&#21270;&#12290;&#22312;&#25552;&#21462;&#29305;&#24449;&#26041;&#38754;&#65292;&#25105;&#20204;&#20351;&#29992;Wyner&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#20811;&#26381;&#22810;&#35270;&#22270;&#25968;&#25454;&#20998;&#24067;&#38388;&#30340;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wireless fingerprinting refers to a device identification method leveraging hardware imperfections and wireless channel variations as signatures. Beyond physical layer characteristics, recent studies demonstrated that user behaviours could be identified through network traffic, e.g., packet length, without decryption of the payload. Inspired by these results, we propose a multi-layer fingerprinting framework that jointly considers the multi-layer signatures for improved identification performance. In contrast to previous works, by leveraging the recent multi-view machine learning paradigm, i.e., data with multiple forms, our method can cluster the device information shared among the multi-layer features without supervision. Our information-theoretic approach can be extended to supervised and semi-supervised settings with straightforward derivations. In solving the formulated problem, we obtain a tight surrogate bound using variational inference for efficient optimization. In extracting
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;DIAMOND&#31639;&#27861;&#29992;&#20110;&#26080;&#32447;&#24178;&#25200;&#32593;&#32476;&#20013;&#30340;&#22810;&#27969;&#20256;&#36755;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#25910;&#25947;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26082;&#21487;&#20197;&#38598;&#20013;&#24335;&#35745;&#31639;&#22810;&#27969;&#20256;&#36755;&#31574;&#30053;&#65292;&#20063;&#21487;&#20197;&#20998;&#24067;&#24335;&#23454;&#29616;&#25968;&#25454;&#21253;&#30340;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;15-20&#65285;&#30340;&#32593;&#32476;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15544</link><description>&lt;p&gt;
&#26080;&#32447;&#24178;&#25200;&#32593;&#32476;&#20013;&#30340;&#22810;&#27969;&#20256;&#36755;&#65306;&#22522;&#20110;&#25910;&#25947;&#22270;&#23398;&#20064;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Flow Transmission in Wireless Interference Networks: A Convergent Graph Learning Approach. (arXiv:2303.15544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;DIAMOND&#31639;&#27861;&#29992;&#20110;&#26080;&#32447;&#24178;&#25200;&#32593;&#32476;&#20013;&#30340;&#22810;&#27969;&#20256;&#36755;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#25910;&#25947;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26082;&#21487;&#20197;&#38598;&#20013;&#24335;&#35745;&#31639;&#22810;&#27969;&#20256;&#36755;&#31574;&#30053;&#65292;&#20063;&#21487;&#20197;&#20998;&#24067;&#24335;&#23454;&#29616;&#25968;&#25454;&#21253;&#30340;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;15-20&#65285;&#30340;&#32593;&#32476;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26080;&#32447;&#32593;&#32476;&#20013;&#22810;&#27969;&#20256;&#36755;&#30340;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#20854;&#20013;&#19981;&#21516;&#27969;&#38388;&#30340;&#25968;&#25454;&#20449;&#21495;&#21487;&#33021;&#30001;&#20110;&#23427;&#20204;&#36335;&#24452;&#19978;&#30340;&#38142;&#36335;&#38388;&#30456;&#20114;&#24178;&#25200;&#32780;&#20351;&#38142;&#36335;&#23481;&#37327;&#20943;&#23567;&#12290;&#20854;&#30446;&#30340;&#26159;&#24320;&#21457;&#19968;&#31181;&#22810;&#27969;&#20256;&#36755;&#31574;&#30053;&#65292;&#36890;&#36807;&#36335;&#30001;&#22312;&#26080;&#32447;&#24178;&#25200;&#32593;&#32476;&#20013;&#20256;&#36755;&#27969;&#20197;&#26368;&#22823;&#21270;&#32593;&#32476;&#25928;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28041;&#21450;&#22823;&#37327;&#29366;&#24577;&#21644;&#25805;&#20316;&#31354;&#38388;&#65292;&#33719;&#24471;&#26368;&#20248;&#35299;&#30340;&#35745;&#31639;&#20195;&#20215;&#26159;&#26114;&#36149;&#30340;&#12290;&#20026;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;Dual-stage Interference-Aware Multi-flow Optimization of Network Data-signals (DIAMOND)&#12290;DIAMOND&#31639;&#27861;&#30340;&#35774;&#35745;&#20801;&#35768;&#37319;&#29992;&#28151;&#21512;&#24335;&#30340;&#38598;&#20013;&#24335;-&#20998;&#24067;&#24335;&#23454;&#29616;&#65292;&#36825;&#26159;5G&#21450;&#26356;&#39640;&#25216;&#26415;&#20013;&#20855;&#26377;&#20013;&#22830;&#21333;&#20803;&#37096;&#32626;&#29305;&#24449;&#30340;&#19968;&#31181;&#12290;&#38598;&#20013;&#24335;&#38454;&#27573;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#24378;&#21270;&#23398;&#20064; (RL) &#36335;&#30001;&#20195;&#29702;&#35745;&#31639;&#22810;&#27969;&#20256;&#36755;&#31574;&#30053;&#12290;GNN-RL&#20195;&#29702;&#24050;&#32463;&#35757;&#32451;&#22909;&#20102;&#20197;&#23398;&#20064;&#32593;&#32476;&#29366;&#24577;&#24182;&#35774;&#35745;&#19968;&#20010;&#32771;&#34385;&#24178;&#25200;&#30340;&#36335;&#30001;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#36825;&#20010;&#31574;&#30053;&#34987;&#29992;&#20110;&#25351;&#23548;&#27839;&#26080;&#24178;&#25200;&#36335;&#24452;&#20256;&#36755;&#27969;&#65292;&#38543;&#21518;&#20998;&#24067;&#24335;&#38454;&#27573;&#21017;&#21033;&#29992;&#36825;&#20010;&#31574;&#30053;&#26469;&#20256;&#36755;&#25968;&#25454;&#21253;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;DIAMOND&#31639;&#27861;&#22312;&#32593;&#32476;&#24615;&#33021;&#26041;&#38754;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#25552;&#39640;&#20102;15-20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of of multi-flow transmission in wireless networks, where data signals from different flows can interfere with each other due to mutual interference between links along their routes, resulting in reduced link capacities. The objective is to develop a multi-flow transmission strategy that routes flows across the wireless interference network to maximize the network utility. However, obtaining an optimal solution is computationally expensive due to the large state and action spaces involved. To tackle this challenge, we introduce a novel algorithm called Dual-stage Interference-Aware Multi-flow Optimization of Network Data-signals (DIAMOND). The design of DIAMOND allows for a hybrid centralized-distributed implementation, which is a characteristic of 5G and beyond technologies with centralized unit deployments. A centralized stage computes the multi-flow transmission strategy using a novel design of graph neural network (GNN) reinforcement learning (RL) routing ag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#20957;&#32858;&#27010;&#24565;&#65292;&#26500;&#24314;&#22312;&#20998;&#21306;&#23616;&#37096;&#28145;&#24230;&#30340;&#25216;&#26415;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#20102;&#26089;&#26399;&#32467;&#26524;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#30340;&#31038;&#21306;&#21457;&#29616;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.10167</link><description>&lt;p&gt;
&#24191;&#20041;&#21010;&#20998;&#23616;&#37096;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
Generalized partitioned local depth. (arXiv:2303.10167v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#20957;&#32858;&#27010;&#24565;&#65292;&#26500;&#24314;&#22312;&#20998;&#21306;&#23616;&#37096;&#28145;&#24230;&#30340;&#25216;&#26415;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#20102;&#26089;&#26399;&#32467;&#26524;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#30340;&#31038;&#21306;&#21457;&#29616;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#36817;&#30001;Berenhaut&#12289;Moore&#21644;Melvin [Proccedings of the National Academy of Sciences, 119 (4) (2022)]&#25552;&#20986;&#30340;&#20957;&#32858;&#27010;&#24565;&#30340;&#27010;&#25324;&#12290;&#25152;&#25552;&#20986;&#30340;&#34920;&#36848;&#22522;&#20110;&#20998;&#21306;&#23616;&#37096;&#28145;&#24230;&#30340;&#25216;&#26415;&#24182;&#25552;&#28860;&#20102;&#20004;&#20010;&#20851;&#38190;&#27010;&#29575;&#27010;&#24565;&#65306;&#23616;&#37096;&#30456;&#20851;&#24615;&#21644;&#25903;&#25345;&#20998;&#21106;&#12290;&#26089;&#26399;&#32467;&#26524;&#22312;&#26032;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#25193;&#23637;&#65292;&#24182;&#21253;&#25324;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#20013;&#25581;&#31034;&#31038;&#21306;&#30340;&#24212;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we provide a generalization of the concept of cohesion as introduced recently by Berenhaut, Moore and Melvin [Proceedings of the National Academy of Sciences, 119 (4) (2022)]. The formulation presented builds on the technique of partitioned local depth by distilling two key probabilistic concepts: local relevance and support division. Earlier results are extended within the new context, and examples of applications to revealing communities in data with uncertainty are included.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07122</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#37327;&#21270;&#21271;&#26497;&#25918;&#22823;&#30340;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference. (arXiv:2303.07122v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21271;&#26497;&#21464;&#26262;&#65292;&#20063;&#31216;&#21271;&#26497;&#25918;&#22823;&#65292;&#30001;&#22810;&#31181;&#22823;&#27668;&#21644;&#28023;&#27915;&#22240;&#32032;&#23548;&#33268;&#65292;&#20294;&#20854;&#22522;&#30784;&#28909;&#21147;&#22240;&#32032;&#30340;&#35814;&#32454;&#24773;&#20917;&#20173;&#19981;&#28165;&#26970;&#12290;&#20351;&#29992;&#22266;&#23450;&#27835;&#30103;&#25928;&#24212;&#31574;&#30053;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#20250;&#23548;&#33268;&#19981;&#29616;&#23454;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#26102;&#38388;&#21464;&#21270;&#30340;&#28151;&#28102;&#30340;&#24433;&#21709;&#32780;&#24341;&#36215;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TCINet - &#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#65292;&#20197;&#36830;&#32493;&#27835;&#30103;&#26041;&#24335;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#35266;&#27979;&#25968;&#25454;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#22914;&#20309;&#22823;&#22823;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The warming of the Arctic, also known as Arctic amplification, is led by several atmospheric and oceanic drivers, however, the details of its underlying thermodynamic causes are still unknown. Inferring the causal effects of atmospheric processes on sea ice melt using fixed treatment effect strategies leads to unrealistic counterfactual estimations. Such models are also prone to bias due to time-varying confoundedness. In order to tackle these challenges, we propose TCINet - time-series causal inference model to infer causation under continuous treatment using recurrent neural networks. Through experiments on synthetic and observational data, we show how our research can substantially improve the ability to quantify the leading causes of Arctic sea ice melt.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#22797;&#26434;&#26753;&#31995;&#32479;&#20013;&#27491;&#21521;&#21644;&#36870;&#21521;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#27714;&#35299;&#27431;&#25289;-&#20271;&#21162;&#21033;&#21644;Timoshenko&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#39640;&#25928;&#22320;&#35745;&#31639;&#27178;&#21521;&#20301;&#31227;&#21644;&#27178;&#25130;&#38754;&#26059;&#36716;&#65292;&#24182;&#31283;&#20581;&#22320;&#30830;&#23450;&#26410;&#30693;&#27169;&#22411;&#21442;&#25968;&#21644;&#26045;&#21152;&#21147;&#65292;&#20026;&#24037;&#31243;&#32467;&#26500;&#21644;&#26426;&#26800;&#38382;&#39064;&#30340;&#35299;&#20915;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.01055</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#26753;&#31995;&#32479;&#20013;&#35299;&#20915;&#27491;&#21521;&#21644;&#36870;&#21521;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks for solving forward and inverse problems in complex beam systems. (arXiv:2303.01055v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35299;&#20915;&#22797;&#26434;&#26753;&#31995;&#32479;&#20013;&#27491;&#21521;&#21644;&#36870;&#21521;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#27714;&#35299;&#27431;&#25289;-&#20271;&#21162;&#21033;&#21644;Timoshenko&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#39640;&#25928;&#22320;&#35745;&#31639;&#27178;&#21521;&#20301;&#31227;&#21644;&#27178;&#25130;&#38754;&#26059;&#36716;&#65292;&#24182;&#31283;&#20581;&#22320;&#30830;&#23450;&#26410;&#30693;&#27169;&#22411;&#21442;&#25968;&#21644;&#26045;&#21152;&#21147;&#65292;&#20026;&#24037;&#31243;&#32467;&#26500;&#21644;&#26426;&#26800;&#38382;&#39064;&#30340;&#35299;&#20915;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26469;&#27169;&#25311;&#30001;&#27431;&#25289;-&#20271;&#21162;&#21033;&#21644;Timoshenko&#29702;&#35770;&#26500;&#25104;&#30340;&#22797;&#26434;&#32467;&#26500;&#31995;&#32479;&#65292;&#20854;&#20013;&#21452;&#26753;&#19982;Winkler&#22522;&#30784;&#30456;&#36830;&#12290;&#29305;&#21035;&#22320;&#65292;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#25439;&#22833;&#20989;&#25968;&#35299;&#20915;&#20102;&#27431;&#25289;-&#20271;&#21162;&#21033;&#21644;Timoshenko&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#27491;&#21521;&#21644;&#36870;&#21521;&#38382;&#39064;&#12290;&#39640;&#38454;&#22797;&#26434;&#26753;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#27491;&#21521;&#38382;&#39064;&#20013;&#34987;&#39640;&#25928;&#22320;&#35299;&#20915;&#65292;&#20197;&#35745;&#31639;&#27178;&#21521;&#20301;&#31227;&#21644;&#27178;&#25130;&#38754;&#26059;&#36716;&#65292;&#35823;&#24046;&#23567;&#20110;1e-3&#65285;&#12290;&#27492;&#22806;&#65292;&#36870;&#21521;&#38382;&#39064;&#31283;&#20581;&#22320;&#35299;&#20915;&#20102;&#22312;&#25972;&#20010;&#26102;&#31354;&#22495;&#20869;&#30830;&#23450;&#26410;&#30693;&#26080;&#37327;&#32434;&#27169;&#22411;&#21442;&#25968;&#21644;&#26045;&#21152;&#21147;&#30340;&#38382;&#39064;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;PINNs&#26159;&#35299;&#20915;&#28041;&#21450;&#26753;&#31995;&#32479;&#30340;&#24037;&#31243;&#32467;&#26500;&#21644;&#26426;&#26800;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new framework using physics-informed neural networks (PINNs) to simulate complex structural systems that consist of single and double beams based on Euler-Bernoulli and Timoshenko theory, where the double beams are connected with a Winkler foundation. In particular, forward and inverse problems for the Euler-Bernoulli and Timoshenko partial differential equations (PDEs) are solved using nondimensional equations with the physics-informed loss function. Higher-order complex beam PDEs are efficiently solved for forward problems to compute the transverse displacements and cross-sectional rotations with less than 1e-3 percent error. Furthermore, inverse problems are robustly solved to determine the unknown dimensionless model parameters and applied force in the entire space-time domain, even in the case of noisy data. The results suggest that PINNs are a promising strategy for solving problems in engineering structures and machines involving beam systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#26377;&#19968;&#33268;&#30340;&#20248;&#21183;&#65292;&#20294;&#32477;&#23545;&#34920;&#29616;&#20173;&#26377;&#25552;&#39640;&#31354;&#38388;&#65292;&#40065;&#26834;&#24615;&#20173;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.12095</link><description>&lt;p&gt;
&#35770;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65306;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (arXiv:2302.12095v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#26377;&#19968;&#33268;&#30340;&#20248;&#21183;&#65292;&#20294;&#32477;&#23545;&#34920;&#29616;&#20173;&#26377;&#25552;&#39640;&#31354;&#38388;&#65292;&#40065;&#26834;&#24615;&#20173;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#26368;&#36817;&#21457;&#24067;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26381;&#21153;&#65292;&#24182;&#22312;&#36807;&#21435;&#20960;&#20010;&#26376;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#23545;ChatGPT&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20294;&#20854;&#40065;&#26834;&#24615;&#65292;&#21363;&#23545;&#20110;&#26410;&#39044;&#26399;&#36755;&#20837;&#30340;&#34920;&#29616;&#65292;&#20173;&#19981;&#28165;&#26970;&#12290;&#40065;&#26834;&#24615;&#22312;&#36127;&#36131;&#20219;&#30340;AI&#20013;&#29305;&#21035;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#35282;&#24230;&#23545;ChatGPT&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;AdvGLUE&#21644;ANLI&#22522;&#20934;&#26469;&#35780;&#20272;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#37319;&#29992;Flipkart&#35780;&#35770;&#21644;DDXPlus&#21307;&#23398;&#35786;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;OOD&#35780;&#20272;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20960;&#20010;&#27969;&#34892;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#22522;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#23545;&#25239;&#24615;&#21644;OOD&#20998;&#31867;&#21644;&#32763;&#35793;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#20248;&#21183;&#12290;&#20294;&#26159;&#65292;&#32477;&#23545;&#30340;&#34920;&#29616;&#36828;&#38750;&#23436;&#32654;&#65292;&#36825;&#34920;&#26126;&#23545;&#25239;&#24615;&#21644;OOD&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#38543;&#26426;&#27493;&#38271;&#21644;&#24490;&#29615;&#27493;&#38271;&#30456;&#23545;&#20110;&#24120;&#25968;&#27493;&#38271;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#32771;&#34385;&#23614;&#37096;&#25351;&#25968;&#22914;&#20309;&#38543;&#27493;&#38271;&#35843;&#24230;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#26469;&#25512;&#21160;&#29702;&#35770;&#30740;&#31350;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#23427;&#20204;&#23545;&#20110;&#27867;&#21270;&#24615;&#33021;&#30340;&#25913;&#21892;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.05516</link><description>&lt;p&gt;
&#38543;&#26426;&#27493;&#38271;&#21644;&#24490;&#29615;&#27493;&#38271;&#24341;&#21457;&#20102;&#27604;&#24120;&#25968;&#27493;&#38271;&#26356;&#37325;&#30340;SGD&#23614;&#37096;
&lt;/p&gt;
&lt;p&gt;
Cyclic and Randomized Stepsizes Invoke Heavier Tails in SGD than Constant Stepsize. (arXiv:2302.05516v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#38543;&#26426;&#27493;&#38271;&#21644;&#24490;&#29615;&#27493;&#38271;&#30456;&#23545;&#20110;&#24120;&#25968;&#27493;&#38271;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#32771;&#34385;&#23614;&#37096;&#25351;&#25968;&#22914;&#20309;&#38543;&#27493;&#38271;&#35843;&#24230;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#26469;&#25512;&#21160;&#29702;&#35770;&#30740;&#31350;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#23427;&#20204;&#23545;&#20110;&#27867;&#21270;&#24615;&#33021;&#30340;&#25913;&#21892;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#27493;&#38271;&#21644;&#24490;&#29615;&#27493;&#38271;&#22312;&#28145;&#24230;&#23398;&#20064;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#24182;&#19988;&#36890;&#24120;&#21487;&#20197;&#32988;&#36807;&#24120;&#25968;&#27493;&#38271;&#36873;&#25321;&#65292;&#22914;SGD&#20013;&#30340;&#24120;&#25968;&#27493;&#38271;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#32463;&#39564;&#19978;&#33719;&#24471;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#23545;&#20110;&#23427;&#20204;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#33021;&#22312;&#29702;&#35770;&#19978;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#30340;&#20102;&#35299;&#36824;&#19981;&#22815;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#20851;&#20110;&#23398;&#20064;&#30340;&#39532;&#23572;&#21487;&#22827;&#27493;&#38271;&#65292;&#20854;&#20013;&#21253;&#21547;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#38543;&#26426;&#27493;&#38271;&#12289;&#24490;&#29615;&#27493;&#38271;&#20197;&#21450;&#24120;&#25968;&#27493;&#38271;&#31561;&#29305;&#27530;&#24773;&#20917;&#12290;&#21463;&#21040;&#25991;&#29486;&#20013;&#26174;&#31034;&#30340;SGD&#36845;&#20195;&#30340;&#23614;&#37096;&#65288;&#36890;&#36807;&#25152;&#35859;&#30340;&#8220;&#23614;&#37096;&#25351;&#25968;&#8221;&#26469;&#34913;&#37327;&#65289;&#30340;&#37325;&#23614;&#29616;&#35937;&#19982;&#27867;&#21270;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23614;&#37096;&#25351;&#25968;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#23614;&#37096;&#25351;&#25968;&#22312;&#27493;&#38271;&#35843;&#24230;&#19978;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20174;&#23614;&#37096;&#34892;&#20026;&#30340;&#35282;&#24230;&#19978;&#35752;&#35770;&#24490;&#29615;&#21644;&#38543;&#26426;&#27493;&#38271;&#30456;&#23545;&#20110;&#24120;&#25968;&#27493;&#38271;&#30340;&#22909;&#22788;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#32447;&#24615;&#22238;&#24402;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#23637;&#31034;&#20102;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cyclic and randomized stepsizes are widely used in the deep learning practice and can often outperform standard stepsize choices such as constant stepsize in SGD. Despite their empirical success, not much is currently known about when and why they can theoretically improve the generalization performance. We consider a general class of Markovian stepsizes for learning, which contain i.i.d. random stepsize, cyclic stepsize as well as the constant stepsize as special cases, and motivated by the literature which shows that heaviness of the tails (measured by the so-called "tail-index") in the SGD iterates is correlated with generalization, we study tail-index and provide a number of theoretical results that demonstrate how the tail-index varies on the stepsize scheduling. Our results bring a new understanding of the benefits of cyclic and randomized stepsizes compared to constant stepsize in terms of the tail behavior. We illustrate our theory on linear regression experiments and show thro
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#20559;&#33258;&#27880;&#24847;&#21147;&#30340;&#20844;&#24179;&#24863;&#30693;&#35270;&#35273;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#23450;&#20301;&#21644;&#23631;&#34109;&#36825;&#20123;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2301.13803</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20559;&#33258;&#27880;&#24847;&#21147;&#30340;&#20844;&#24179;&#24863;&#30693;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fairness-aware Vision Transformer via Debiased Self-Attention. (arXiv:2301.13803v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13803
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#20559;&#33258;&#27880;&#24847;&#21147;&#30340;&#20844;&#24179;&#24863;&#30693;&#35270;&#35273;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#23450;&#20301;&#21644;&#23631;&#34109;&#36825;&#20123;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#25552;&#21462;&#20449;&#24687;&#29305;&#24449;&#21644;&#36890;&#36807;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#24314;&#27169;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#22312;&#35299;&#20915;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#38382;&#39064;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;ViT&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;ViT&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21253;&#25324;&#20854;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#21478;&#19968;&#20010;&#38656;&#27714;&#65292;&#20844;&#24179;&#24615;&#65292;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#20844;&#24179;&#24863;&#30693;&#31639;&#27861;&#65288;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;CNN&#65289;&#22312;ViT&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#23601;&#38656;&#35201;&#25105;&#20204;&#36890;&#36807;&#21435;&#20559;&#33258;&#27880;&#24847;&#65288;DSA&#65289;&#24320;&#21457;&#25105;&#20204;&#30340;&#26032;&#26694;&#26550;&#12290;DSA&#26159;&#19968;&#31181;&#36890;&#36807;&#30450;&#30446;&#26041;&#27861;&#26469;&#24378;&#21046;ViT&#28040;&#38500;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#20197;&#20943;&#36731;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#34987;&#29992;&#26469;&#23450;&#20301;&#21644;&#23631;&#34109;&#36755;&#20837;&#22270;&#20687;&#22359;&#20013;&#30340;&#34394;&#20551;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has recently gained significant interest in solving computer vision (CV) problems due to its capability of extracting informative features and modeling long-range dependencies through the self-attention mechanism. To fully realize the advantages of ViT in real-world applications, recent works have explored the trustworthiness of ViT, including its robustness and explainability. However, another desiderata, fairness has not yet been adequately addressed in the literature. We establish that the existing fairness-aware algorithms (primarily designed for CNNs) do not perform well on ViT. This necessitates the need for developing our novel framework via Debiased Self-Attention (DSA). DSA is a fairness-through-blindness approach that enforces ViT to eliminate spurious features correlated with the sensitive attributes for bias mitigation. Notably, adversarial examples are leveraged to locate and mask the spurious features in the input image patches. In addition, DSA u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2301.03364</link><description>&lt;p&gt;
&#36208;&#21521;AI-enabled&#36830;&#25509;&#20135;&#19994;: AGV&#36890;&#20449;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Towards an AI-enabled Connected Industry: AGV Communication and Sensor Measurement Datasets. (arXiv:2301.03364v3 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#24037;&#19994;&#27979;&#35797;&#24179;&#21488;&#19978;&#30340;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;: &#24037;&#19994;&#36710;&#36742;&#38388;&#36890;&#20449;(iV2V)&#21644;&#24037;&#19994;&#36710;&#36742;&#21040;&#22522;&#30784;&#35774;&#26045;&#21152;&#20256;&#24863;&#22120;(iV2I+)&#12290;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20004;&#20010;&#25429;&#33719;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;iV2V&#28085;&#30422;&#20102;&#33258;&#21160;&#24341;&#23548;&#36710;(AGVs)&#20043;&#38388;&#30340;&#20391;&#21521;&#38142;&#36335;&#36890;&#20449;&#22330;&#26223;&#65292;&#32780;iV2I+&#21017;&#26159;&#22312;&#24037;&#19994;&#35774;&#32622;&#20013;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#33258;&#20027;&#28165;&#27905;&#26426;&#22120;&#20154;&#36830;&#25509;&#21040;&#31169;&#26377;&#34562;&#31389;&#32593;&#32476;&#12290;&#19981;&#21516;&#36890;&#20449;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#36830;&#21516;&#20849;&#21516;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;(ML)&#21487;&#20197;&#21033;&#29992;&#30340;&#27934;&#23519;&#21147;&#65292;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#24050;&#26631;&#35760;&#21644;&#39044;&#36807;&#28388;&#65292;&#20197;&#20415;&#24555;&#36895;&#21551;&#21160;&#21644;&#24212;&#29992;&#12290;&#23545;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#36824;&#35814;&#32454;&#20171;&#32461;&#20102;&#30456;&#24212;&#30340;&#27979;&#35797;&#24179;&#21488;&#21644;&#27979;&#37327;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two wireless measurement campaigns in industrial testbeds: industrial Vehicle-to-vehicle (iV2V) and industrial Vehicle-to-infrastructure plus Sensor (iV2I+). Detailed information about the two captured datasets is provided as well. iV2V covers sidelink communication scenarios between Automated Guided Vehicles (AGVs), while iV2I+ is conducted at an industrial setting where an autonomous cleaning robot is connected to a private cellular network. The combination of different communication technologies, together with a common measurement methodology, provides insights that can be exploited by Machine Learning (ML) for tasks such as fingerprinting, line-of-sight detection, prediction of quality of service or link selection. Moreover, the datasets are labelled and pre-filtered for fast on-boarding and applicability. The corresponding testbeds and measurements are also presented in detail for both datasets.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;NetEffect&#65292;&#19968;&#31181;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#29702;&#35299;&#20855;&#26377;&#23569;&#37327;&#33410;&#28857;&#26631;&#31614;&#30340;&#22823;&#22411;&#22270;&#20013;&#30340;&#24191;&#20041;&#32593;&#32476;&#25928;&#24212;&#65288;&#22914;&#21516;&#36136;&#24615;&#12289;&#24322;&#36136;&#24615;&#25110;&#20108;&#32773;&#30340;&#32452;&#21512;&#65289;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#25928;&#24212;&#26469;&#25913;&#36827;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.00270</link><description>&lt;p&gt;
&#21457;&#29616;&#21644;&#21033;&#29992;&#24191;&#20041;&#32593;&#32476;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Discovery and Exploitation of Generalized Network Effects. (arXiv:2301.00270v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00270
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;NetEffect&#65292;&#19968;&#31181;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#29702;&#35299;&#20855;&#26377;&#23569;&#37327;&#33410;&#28857;&#26631;&#31614;&#30340;&#22823;&#22411;&#22270;&#20013;&#30340;&#24191;&#20041;&#32593;&#32476;&#25928;&#24212;&#65288;&#22914;&#21516;&#36136;&#24615;&#12289;&#24322;&#36136;&#24615;&#25110;&#20108;&#32773;&#30340;&#32452;&#21512;&#65289;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#25928;&#24212;&#26469;&#25913;&#36827;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20855;&#26377;&#23569;&#37327;&#33410;&#28857;&#26631;&#31614;&#30340;&#22823;&#22411;&#22270;&#65292;&#25105;&#20204;&#22914;&#20309;&#65288;a&#65289;&#30830;&#23450;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#24191;&#20041;&#32593;&#32476;&#25928;&#24212;&#65288;GNE&#65289;&#65292;&#65288;b&#65289;&#20272;&#35745;GNE&#20197;&#35299;&#37322;&#33410;&#28857;&#31867;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#20197;&#21450;&#65288;c&#65289;&#21033;&#29992;GNE&#25913;&#36827;&#35832;&#22914;&#20934;&#30830;&#39640;&#25928;&#22320;&#39044;&#27979;&#26410;&#30693;&#26631;&#31614;&#31561;&#19979;&#28216;&#20219;&#21153;&#65311;&#23545;&#20110;&#33410;&#28857;&#20998;&#31867;&#21644;&#23450;&#21521;&#24191;&#21578;&#31561;&#21508;&#31181;&#20219;&#21153;&#65292;&#20102;&#35299;GNE&#26159;&#24456;&#26377;&#20215;&#20540;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33410;&#28857;&#26631;&#31614;&#26377;&#38480;&#19988;&#36793;&#32536;&#22122;&#38899;&#65292;&#35782;&#21035;&#21644;&#29702;&#35299;GNE&#65288;&#22914;&#21516;&#36136;&#24615;&#12289;&#24322;&#36136;&#24615;&#25110;&#20108;&#32773;&#30340;&#32452;&#21512;&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NetEffect&#65292;&#19968;&#31181;&#22270;&#25366;&#25496;&#26041;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;i&#65289;&#22522;&#20110;&#21407;&#21017;&#65306;&#29992;&#32479;&#35745;&#27979;&#35797;&#30830;&#23450;&#24102;&#26377;&#23569;&#37327;&#33410;&#28857;&#26631;&#31614;&#30340;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;GNE&#65307;&#65288;ii&#65289;&#26222;&#36941;&#19988;&#21487;&#35299;&#37322;&#65306;&#20272;&#35745;&#35266;&#23519;&#21040;&#30340;&#29305;&#23450;&#31867;&#22411;&#30340;GNE&#30340;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#65307;&#65288;iii&#65289;&#20934;&#30830;&#19988;&#21487;&#25193;&#23637;&#65306;&#38598;&#25104;GNE&#20197;&#23454;&#29616;&#20934;&#30830;&#19988;&#24555;&#36895;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a large graph with few node labels, how can we (a) identify whether there is generalized network-effects (GNE) of the graph or not, (b) estimate GNE to explain the interrelations among node classes, and (c) exploit GNE to improve downstream tasks such as predicting the unknown labels accurately and efficiently? The knowledge of GNE is valuable for various tasks like node classification and targeted advertising. However, identifying and understanding GNE such as homophily, heterophily or their combination is challenging in real-world graphs due to limited availability of node labels and noisy edges. We propose NetEffect, a graph mining approach to address the above issues, enjoying the following properties: (i) Principled: a statistical test to determine the presence of GNE in a graph with few node labels; (ii) General and Explainable: a closed-form solution to estimate the specific type of GNE observed; and (iii) Accurate and Scalable: the integration of GNE for accurate and fast
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#26080;&#32447;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#25511;&#21046;&#12289;&#30417;&#27979;&#21644;&#25968;&#25454;&#37319;&#38598;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#37327;&#21270;&#21644;&#22788;&#29702;&#30001;&#20110;&#25968;&#25454;&#37327;&#21644;&#36136;&#37327;&#30340;&#38480;&#21046;&#23548;&#33268;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.01351</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#26080;&#32447;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#25511;&#21046;&#12289;&#30417;&#27979;&#21644;&#25968;&#25454;&#37319;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Framework for Digital Twin-Based Control, Monitoring, and Data Collection in Wireless Systems. (arXiv:2212.01351v3 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#26080;&#32447;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#25511;&#21046;&#12289;&#30417;&#27979;&#21644;&#25968;&#25454;&#37319;&#38598;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#37327;&#21270;&#21644;&#22788;&#29702;&#30001;&#20110;&#25968;&#25454;&#37327;&#21644;&#36136;&#37327;&#30340;&#38480;&#21046;&#23548;&#33268;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#24179;&#21488;&#22312;&#21046;&#36896;&#19994;&#21644;&#33322;&#31354;&#33322;&#22825;&#19994;&#24191;&#27867;&#24212;&#29992;&#65292;&#34987;&#35270;&#20026;&#25511;&#21046;&#12289;&#30417;&#27979;&#21644;&#20998;&#26512;&#22522;&#20110;&#36719;&#20214;&#30340;&#8220;&#24320;&#25918;&#8221;&#36890;&#20449;&#31995;&#32479;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#12290;&#29305;&#21035;&#26159;&#65292;DT&#24179;&#21488;&#20026;&#27979;&#35797;&#36890;&#20449;&#31995;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#27801;&#30418;&#65292;&#28508;&#22312;&#22320;&#20943;&#23569;&#20102;&#22312;&#29289;&#29702;&#23402;&#29983;&#65288;PT&#65289;&#19978;&#25910;&#38598;&#25968;&#25454;&#21644;&#27979;&#35797;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;DT&#31995;&#32479;&#37096;&#32626;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30830;&#20445;DT&#19978;&#30340;&#34394;&#25311;&#25511;&#21046;&#20248;&#21270;&#12289;&#30417;&#27979;&#21644;&#20998;&#26512;&#26159;&#23433;&#20840;&#21487;&#38752;&#30340;&#65292;&#36991;&#20813;&#8220;&#27169;&#22411;&#28389;&#29992;&#8221;&#23548;&#33268;&#30340;&#38169;&#35823;&#20915;&#31574;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#26088;&#22312;&#37327;&#21270;&#21644;&#32771;&#34385;&#22312;DT&#19978;&#30001;&#20110;&#22312;PT&#19978;&#21487;&#29992;&#30340;&#25968;&#25454;&#37327;&#21644;&#36136;&#37327;&#30340;&#38480;&#21046;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#35813;&#25552;&#35758;&#30340;&#26694;&#26550;&#20013;&#65292;DT&#24314;&#31435;&#20102;&#36890;&#20449;&#31995;&#32479;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonly adopted in the manufacturing and aerospace sectors, digital twin (DT) platforms are increasingly seen as a promising paradigm to control, monitor, and analyze software-based, "open", communication systems. Notably, DT platforms provide a sandbox in which to test artificial intelligence (AI) solutions for communication systems, potentially reducing the need to collect data and test algorithms in the field, i.e., on the physical twin (PT). A key challenge in the deployment of DT systems is to ensure that virtual control optimization, monitoring, and analysis at the DT are safe and reliable, avoiding incorrect decisions caused by "model exploitation". To address this challenge, this paper presents a general Bayesian framework with the aim of quantifying and accounting for model uncertainty at the DT that is caused by limitations in the amount and quality of data available at the DT from the PT. In the proposed framework, the DT builds a Bayesian model of the communication system,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25200;&#21160;&#35266;&#27979;&#22120;&#21644;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#36827;&#34892;&#23433;&#20840;&#39640;&#25928;&#22686;&#24378;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#27169;&#22411;&#23398;&#20064;&#65292;&#21033;&#29992;&#25200;&#21160;&#35266;&#27979;&#22120;&#20934;&#30830;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#28857;&#20540;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#40065;&#26834;&#30340;CBF&#26465;&#20214;&#20013;&#29983;&#25104;&#23433;&#20840;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2211.17250</link><description>&lt;p&gt;
&#20351;&#29992;&#25200;&#21160;&#35266;&#27979;&#22120;&#21644;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#39640;&#25928;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe and Efficient Reinforcement Learning Using Disturbance-Observer-Based Control Barrier Functions. (arXiv:2211.17250v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25200;&#21160;&#35266;&#27979;&#22120;&#21644;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#36827;&#34892;&#23433;&#20840;&#39640;&#25928;&#22686;&#24378;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#27169;&#22411;&#23398;&#20064;&#65292;&#21033;&#29992;&#25200;&#21160;&#35266;&#27979;&#22120;&#20934;&#30830;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#28857;&#20540;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#40065;&#26834;&#30340;CBF&#26465;&#20214;&#20013;&#29983;&#25104;&#23433;&#20840;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20851;&#27880;&#36234;&#26469;&#36234;&#22810;&#30340;&#26159;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#35777;&#30828;&#29366;&#24577;&#32422;&#26463;&#28385;&#36275;&#30340;&#23433;&#20840;&#22686;&#24378;&#23398;&#20064;&#12290;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#20363;&#22914;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;(CBFs)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21363;&#26102;&#20462;&#25913;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#30340;&#19981;&#23433;&#20840;&#21160;&#20316;&#26469;&#23454;&#29616;&#23433;&#20840;&#30340;&#22686;&#24378;&#23398;&#20064;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#23398;&#20064;&#19981;&#30830;&#23450;&#21160;&#21147;&#23398;&#21644;&#37327;&#21270;&#23398;&#20064;&#27169;&#22411;&#35823;&#24046;&#65292;&#36825;&#23548;&#33268;&#22312;&#25910;&#38598;&#36275;&#22815;&#30340;&#25968;&#25454;&#23398;&#20064;&#22909;&#27169;&#22411;&#20043;&#21069;&#65292;&#36807;&#28388;&#22120;&#36807;&#20110;&#20445;&#23432;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25200;&#21160;&#35266;&#27979;&#22120;&#21644;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#36827;&#34892;&#23433;&#20840;&#39640;&#25928;&#22686;&#24378;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22788;&#29702;&#30828;&#29366;&#24577;&#32422;&#26463;&#30340;&#23433;&#20840;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#28041;&#21450;&#27169;&#22411;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#25200;&#21160;&#35266;&#27979;&#22120;&#20934;&#30830;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#28857;&#20540;&#65292;&#28982;&#21518;&#23558;&#20854;&#32435;&#20837;&#40065;&#26834;&#30340;CBF&#26465;&#20214;&#29983;&#25104;&#23433;&#20840;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning (RL) with assured satisfaction of hard state constraints during training has recently received a lot of attention. Safety filters, e.g., based on control barrier functions (CBFs), provide a promising way for safe RL via modifying the unsafe actions of an RL agent on the fly. Existing safety filter-based approaches typically involve learning of uncertain dynamics and quantifying the learned model error, which leads to conservative filters before a large amount of data is collected to learn a good model, thereby preventing efficient exploration. This paper presents a method for safe and efficient RL using disturbance observers (DOBs) and control barrier functions (CBFs). Unlike most existing safe RL methods that deal with hard state constraints, our method does not involve model learning, and leverages DOBs to accurately estimate the pointwise value of the uncertainty, which is then incorporated into a robust CBF condition to generate safe actions. The DOB-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeCurvEd&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21487;&#20132;&#25442;&#30340;&#26041;&#24335;&#30830;&#23450;&#28508;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#20132;&#38169;&#30690;&#37327;&#22330;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2211.14573</link><description>&lt;p&gt;
&#28145;&#24230;&#26354;&#32447;&#32534;&#36753;&#65306;&#38024;&#23545;&#39044;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20132;&#25442;&#21644;&#38750;&#32447;&#24615;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation for Pretrained Deep Generative Model. (arXiv:2211.14573v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeCurvEd&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21487;&#20132;&#25442;&#30340;&#26041;&#24335;&#30830;&#23450;&#28508;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#20132;&#38169;&#30690;&#37327;&#22330;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#30340;&#35821;&#20041;&#32534;&#36753;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#22522;&#26412;&#30446;&#26631;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#20294;&#36890;&#24120;&#23427;&#20204;&#19981;&#20855;&#22791;&#23545;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#32534;&#36753;&#30340;&#20869;&#22312;&#26041;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#25805;&#32437;&#28508;&#21464;&#37327;&#26469;&#30830;&#23450;&#35201;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20551;&#35774;&#32447;&#24615;&#35821;&#20041;&#31639;&#26415;&#30340;&#26041;&#27861;&#22312;&#22270;&#20687;&#32534;&#36753;&#26041;&#38754;&#20855;&#26377;&#26576;&#20123;&#23616;&#38480;&#24615;&#65292;&#32780;&#21457;&#29616;&#38750;&#32447;&#24615;&#30340;&#35821;&#20041;&#36335;&#24452;&#25552;&#20379;&#20102;&#19981;&#21487;&#20132;&#25442;&#30340;&#32534;&#36753;&#65292;&#36825;&#22312;&#20197;&#19981;&#21516;&#30340;&#39034;&#24207;&#24212;&#29992;&#26102;&#19981;&#19968;&#33268;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#26354;&#32447;&#32534;&#36753;&#65288;DeCurvEd&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#28508;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#20132;&#38169;&#30690;&#37327;&#22330;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#30001;&#20110;&#21487;&#20132;&#25442;&#24615;&#65292;&#22810;&#20010;&#23646;&#24615;&#30340;&#32534;&#36753;&#20165;&#21462;&#20915;&#20110;&#25968;&#37327;&#32780;&#19981;&#26159;&#39034;&#24207;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic editing of images is the fundamental goal of computer vision. Although deep learning methods, such as generative adversarial networks (GANs), are capable of producing high-quality images, they often do not have an inherent way of editing generated images semantically. Recent studies have investigated a way of manipulating the latent variable to determine the images to be generated. However, methods that assume linear semantic arithmetic have certain limitations in terms of the quality of image editing, whereas methods that discover nonlinear semantic pathways provide non-commutative editing, which is inconsistent when applied in different orders. This study proposes a novel method called deep curvilinear editing (DeCurvEd) to determine semantic commuting vector fields on the latent space. We theoretically demonstrate that owing to commutativity, the editing of multiple attributes depends only on the quantities and not on the order. Furthermore, we experimentally demonstrate th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25277;&#35937;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#38750;&#32447;&#24615;&#31181;&#32676;&#30721;&#23454;&#29616;&#33258;&#28982;&#22270;&#20687;&#22359;&#30340;&#39640;&#25928;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#26089;&#26399;&#35270;&#35273;&#31995;&#32479;&#30340;&#20449;&#24687;&#20256;&#36755;&#21644;&#20256;&#24863;&#22120;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2210.13004</link><description>&lt;p&gt;
&#33258;&#28982;&#22270;&#20687;&#22359;&#30340;&#39640;&#25928;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Efficient Representation of Natural Image Patches. (arXiv:2210.13004v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13004
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25277;&#35937;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#38750;&#32447;&#24615;&#31181;&#32676;&#30721;&#23454;&#29616;&#33258;&#28982;&#22270;&#20687;&#22359;&#30340;&#39640;&#25928;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#26089;&#26399;&#35270;&#35273;&#31995;&#32479;&#30340;&#20449;&#24687;&#20256;&#36755;&#21644;&#20256;&#24863;&#22120;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#20449;&#24687;&#22788;&#29702;&#30340;&#22797;&#26434;&#39046;&#22495;&#20013;&#65292;&#20174;&#27425;&#35201;&#32454;&#33410;&#20013;&#36776;&#21035;&#20986;&#22522;&#26412;&#21407;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#25105;&#20204;&#24050;&#32463;&#23545;&#26089;&#26399;&#35270;&#35273;&#31995;&#32479;&#30340;&#35299;&#21078;&#23398;&#21644;&#29983;&#29702;&#23398;&#26377;&#20102;&#24191;&#27867;&#30340;&#20102;&#35299;&#65292;&#20294;&#19968;&#20010;&#20840;&#38754;&#30340;&#35745;&#31639;&#29702;&#35770;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#25105;&#20204;&#33021;&#21542;&#36890;&#36807;&#25277;&#35937;&#20986;&#31995;&#32479;&#30340;&#35814;&#32454;&#23454;&#29616;&#24182;&#19987;&#27880;&#20110;&#31995;&#32479;&#26088;&#22312;&#35299;&#20915;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#26469;&#27934;&#23519;&#29983;&#29289;&#31995;&#32479;&#30340;&#22522;&#26412;&#21407;&#29702;&#21602;&#65311;&#21033;&#29992;&#22522;&#20110;&#26368;&#31616;&#21270;&#32780;&#21448;&#29616;&#23454;&#30340;&#20551;&#35774;&#30340;&#19968;&#20010;&#25277;&#35937;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23454;&#29616;&#26089;&#26399;&#35270;&#35273;&#31995;&#32479;&#30340;&#20004;&#20010;&#26368;&#32456;&#30446;&#26631;&#65306;&#39640;&#25928;&#30340;&#20449;&#24687;&#20256;&#36755;&#21644;&#20256;&#24863;&#22120;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20026;&#20102;&#20248;&#21270;&#20449;&#24687;&#20256;&#36755;&#65292;&#24182;&#19981;&#24847;&#21619;&#30528;&#33719;&#24471;&#20102;&#26368;&#20248;&#30340;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#20108;&#32500;&#31995;&#32479;&#21644;&#22270;&#20687;&#22359;&#26469;&#35828;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#30001;&#20004;&#20010;&#31867;&#22411;&#39537;&#21160;&#30340;&#38750;&#32447;&#24615;&#31181;&#32676;&#30721;&#23454;&#29616;&#19968;&#20010;&#39640;&#25928;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the complex domain of neural information processing, discerning fundamental principles from ancillary details remains a significant challenge. While there is extensive knowledge about the anatomy and physiology of the early visual system, a comprehensive computational theory remains elusive. Can we gain insights into the underlying principles of a biological system by abstracting away from its detailed implementation and focusing on the fundamental problems that the system is designed to solve? Utilizing an abstract model based on minimal yet realistic assumptions, we show how to achieve the early visual system's two ultimate objectives: efficient information transmission and sensor probability distribution modeling. We show that optimizing for information transmission does not yield optimal probability distribution modeling. We illustrate, using a two-pixel (2D) system and image patches, that an efficient representation can be realized via nonlinear population code driven by two ty
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37319;&#29992;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#26469;&#24314;&#27169;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#26102;&#38388;&#30340;&#36924;&#36817;&#65292;&#23454;&#29616;&#23545;&#26410;&#35266;&#27979;&#21306;&#22495;&#30340;&#31934;&#30830;&#22806;&#25512;&#65292;&#24182;&#24212;&#29992;&#24352;&#37327;&#23436;&#25104;&#31639;&#27861;&#20248;&#21270;&#20302;&#31209;&#27491;&#20132;-&#22810;&#39033;&#24335;&#65288;CP&#65289;&#20998;&#35299;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23384;&#20648;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.10184</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#23436;&#25104;&#30340;&#22810;&#21442;&#25968;&#24615;&#33021;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-Parameter Performance Modeling via Tensor Completion. (arXiv:2210.10184v2 [cs.PF] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37319;&#29992;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#26469;&#24314;&#27169;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#26102;&#38388;&#30340;&#36924;&#36817;&#65292;&#23454;&#29616;&#23545;&#26410;&#35266;&#27979;&#21306;&#22495;&#30340;&#31934;&#30830;&#22806;&#25512;&#65292;&#24182;&#24212;&#29992;&#24352;&#37327;&#23436;&#25104;&#31639;&#27861;&#20248;&#21270;&#20302;&#31209;&#27491;&#20132;-&#22810;&#39033;&#24335;&#65288;CP&#65289;&#20998;&#35299;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23384;&#20648;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#26159;&#24615;&#33021;&#35843;&#25972;&#12289;&#36719;&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#21644;&#20316;&#19994;&#35843;&#24230;&#31561;&#35768;&#22810;&#20219;&#21153;&#25152;&#20381;&#36182;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#26469;&#24314;&#27169;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#35268;&#21017;&#32593;&#26684;&#23545;&#24212;&#29992;&#31243;&#24207;&#30340;&#36755;&#20837;&#21644;&#37197;&#32622;&#22495;&#36827;&#34892;&#31163;&#25955;&#21270;&#12290;&#22312;&#32593;&#26684;&#21333;&#20803;&#20013;&#26144;&#23556;&#30340;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#26102;&#38388;&#34987;&#24179;&#22343;&#24182;&#34920;&#31034;&#20026;&#24352;&#37327;&#20803;&#32032;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20302;&#31209;&#30340;&#27491;&#20132;-&#22810;&#39033;&#24335;&#65288;CP&#65289;&#24352;&#37327;&#20998;&#35299;&#36890;&#36807;&#23545;&#36825;&#20123;&#24352;&#37327;&#36827;&#34892;&#36924;&#36817;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#36825;&#31181;&#20998;&#35299;&#20351;&#24471;&#23545;&#19968;&#20010;&#24212;&#29992;&#31243;&#24207;&#21442;&#25968;&#31354;&#38388;&#20013;&#26410;&#35266;&#27979;&#21306;&#22495;&#30340;&#31934;&#30830;&#22806;&#25512;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#24352;&#37327;&#23436;&#25104;&#26469;&#20248;&#21270;&#32473;&#23450;&#19968;&#32452;&#31232;&#30095;&#30340;&#35266;&#23519;&#30340;&#36816;&#34892;&#26102;&#38388;&#30340;CP&#20998;&#35299;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20845;&#20010;&#24212;&#29992;&#30340;&#26367;&#20195;&#20998;&#27573;/&#32593;&#26684;&#27169;&#22411;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#24352;&#37327;&#23436;&#25104;&#21151;&#33021;&#20248;&#21270;&#30340;CP&#20998;&#35299;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23384;&#20648;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance tuning, software/hardware co-design, and job scheduling are among the many tasks that rely on models to predict application performance. We propose and evaluate low rank tensor decomposition for modeling application performance. We discretize the input and configuration domain of an application using regular grids. Application execution times mapped within grid-cells are averaged and represented by tensor elements. We show that low-rank canonical-polyadic (CP) tensor decomposition is effective in approximating these tensors. We further show that this decomposition enables accurate extrapolation of unobserved regions of an application's parameter space. We then employ tensor completion to optimize a CP decomposition given a sparse set of observed runtimes. We consider alternative piecewise/grid-based models and supervised learning models for six applications and demonstrate that CP decomposition optimized using tensor completion offers higher prediction accuracy and memory-e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#31574;&#30053;&#26799;&#24230;&#12290;&#20256;&#32479;&#30340;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#25928;&#29992;&#30340;&#38382;&#39064;&#65292;&#32780;&#26412;&#25991;&#25512;&#23548;&#20986;&#30340;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.00991</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#31574;&#30053;&#26799;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient for Reinforcement Learning with General Utilities. (arXiv:2210.00991v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#31574;&#30053;&#26799;&#24230;&#12290;&#20256;&#32479;&#30340;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#25928;&#29992;&#30340;&#38382;&#39064;&#65292;&#32780;&#26412;&#25991;&#25512;&#23548;&#20986;&#30340;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#30340;&#30446;&#26631;&#26159;&#21457;&#29616;&#26368;&#22823;&#21270;&#26399;&#26395;&#32047;&#31215;&#22870;&#21169;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#36825;&#20010;&#30446;&#26631;&#20063;&#21487;&#20197;&#30475;&#20316;&#26159;&#25214;&#21040;&#19968;&#20010;&#20248;&#21270;&#29366;&#24577;-&#21160;&#20316;&#21344;&#26377;&#24230;&#37327;&#30340;&#32447;&#24615;&#20989;&#25968;&#30340;&#31574;&#30053;&#65292;&#21363;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#24182;&#19981;&#36866;&#29992;&#20110;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#27604;&#22914;&#23398;&#24466;&#23398;&#20064;&#12289;&#32431;&#25506;&#32034;&#21644;&#21464;&#20998;&#20869;&#22312;&#25511;&#21046;&#65292;&#20854;&#20013;&#30340;&#30446;&#26631;&#26159;&#21344;&#26377;&#24230;&#37327;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#38750;&#32447;&#24615;&#25928;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#30475;&#36215;&#26469;&#19981;&#37027;&#20040;&#26041;&#20415;&#65292;&#22240;&#20026;&#22312;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#30340;&#36125;&#23572;&#26364;&#26041;&#31243;&#12289;&#20540;&#36845;&#20195;&#12289;&#31574;&#30053;&#26799;&#24230;&#12289;&#21160;&#24577;&#35268;&#21010;&#31561;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#27867;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#22788;&#29702;&#20855;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#12290;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#22240;&#20854;&#31616;&#27905;&#26131;&#23454;&#29616;&#30340;&#29305;&#24615;&#32780;&#25104;&#20026;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Reinforcement Learning (RL), the goal of agents is to discover an optimal policy that maximizes the expected cumulative rewards. This objective may also be viewed as finding a policy that optimizes a linear function of its state-action occupancy measure, hereafter referred as Linear RL. However, many supervised and unsupervised RL problems are not covered in the Linear RL framework, such as apprenticeship learning, pure exploration and variational intrinsic control, where the objectives are non-linear functions of the occupancy measures. RL with non-linear utilities looks unwieldy, as methods like Bellman equation, value iteration, policy gradient, dynamic programming that had tremendous success in Linear RL, fail to trivially generalize. In this paper, we derive the policy gradient theorem for RL with general utilities. The policy gradient theorem proves to be a cornerstone in Linear RL due to its elegance and ease of implementability. Our policy gradient theorem for RL with genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#20351;&#29992;&#38750;&#32447;&#24615;&#22240;&#23376;&#27169;&#22411;&#23545;&#22823;&#22411;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#22238;&#25253;&#30340;&#31934;&#30830;&#30697;&#38453;&#36827;&#34892;&#19968;&#33268;&#20272;&#35745;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#36866;&#29992;&#20110;&#37329;&#34701;&#24066;&#22330;&#20856;&#22411;&#30340;&#20302;&#20449;&#22122;&#27604;&#29615;&#22659;&#65292;&#36824;&#19982;&#24369;&#22240;&#23376;&#20860;&#23481;&#65292;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#30028;&#38480;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#19968;&#33268;&#35823;&#24046;&#21327;&#26041;&#24046;&#20272;&#35745;&#26041;&#27861;&#12290;&#27169;&#25311;&#21644;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.04512</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38750;&#32447;&#24615;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#27531;&#24046;&#65306;&#20302;&#20449;&#22122;&#27604;&#19979;&#36164;&#20135;&#22238;&#25253;&#30340;&#31934;&#30830;&#30697;&#38453;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Residuals in Non-linear Factor Models: Precision Matrix Estimation of Returns with Low Signal-to-Noise Ratio. (arXiv:2209.04512v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#20351;&#29992;&#38750;&#32447;&#24615;&#22240;&#23376;&#27169;&#22411;&#23545;&#22823;&#22411;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#22238;&#25253;&#30340;&#31934;&#30830;&#30697;&#38453;&#36827;&#34892;&#19968;&#33268;&#20272;&#35745;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#36866;&#29992;&#20110;&#37329;&#34701;&#24066;&#22330;&#20856;&#22411;&#30340;&#20302;&#20449;&#22122;&#27604;&#29615;&#22659;&#65292;&#36824;&#19982;&#24369;&#22240;&#23376;&#20860;&#23481;&#65292;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#30028;&#38480;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#19968;&#33268;&#35823;&#24046;&#21327;&#26041;&#24046;&#20272;&#35745;&#26041;&#27861;&#12290;&#27169;&#25311;&#21644;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#20351;&#29992;&#38750;&#32447;&#24615;&#22240;&#23376;&#27169;&#22411;&#23545;&#22823;&#22411;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#22238;&#25253;&#30340;&#31934;&#30830;&#30697;&#38453;&#36827;&#34892;&#19968;&#33268;&#20272;&#35745;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#26041;&#27861;&#21363;&#20351;&#22312;&#37329;&#34701;&#24066;&#22330;&#20856;&#22411;&#30340;&#20449;&#22122;&#27604;&#20302;&#30340;&#29615;&#22659;&#20013;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#19982;&#24369;&#22240;&#23376;&#20860;&#23481;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#23545;&#20110;&#19981;&#26029;&#22686;&#21152;&#30340;&#36164;&#20135;&#25968;&#37327;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#26399;&#20272;&#35745;&#39118;&#38505;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#22522;&#20110;&#25968;&#25454;&#30340;&#19968;&#33268;&#35823;&#24046;&#21327;&#26041;&#24046;&#20272;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#27169;&#25311;&#21644;&#23454;&#35777;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a consistent estimator and rate of convergence for the precision matrix of asset returns in large portfolios using a non-linear factor model within the deep learning framework. Our estimator remains valid even in low signal-to-noise ratio environments typical for financial markets and is compatible with weak factors. Our theoretical analysis establishes uniform bounds on expected estimation risk based on deep neural networks for an expanding number of assets. Additionally, we provide a new consistent data-dependent estimator of error covariance in deep neural networks. Our models demonstrate superior accuracy in extensive simulations and the empirics.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#25277;&#35937;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#25277;&#35937;&#29366;&#24577;&#20250;&#24341;&#20837;&#26679;&#26412;&#30456;&#20851;&#24615;&#65292;&#32780;&#20351;&#29992;&#38789;&#19981;&#31561;&#24335;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#23558;&#29616;&#26377;MBRL&#31639;&#27861;&#30340;&#20445;&#35777;&#25193;&#23637;&#21040;&#24102;&#26377;&#25277;&#35937;&#30340;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2208.14407</link><description>&lt;p&gt;
&#25277;&#35937;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Abstracted Model-Based Reinforcement Learning. (arXiv:2208.14407v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#25277;&#35937;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#25277;&#35937;&#29366;&#24577;&#20250;&#24341;&#20837;&#26679;&#26412;&#30456;&#20851;&#24615;&#65292;&#32780;&#20351;&#29992;&#38789;&#19981;&#31561;&#24335;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#23558;&#29616;&#26377;MBRL&#31639;&#27861;&#30340;&#20445;&#35777;&#25193;&#23637;&#21040;&#24102;&#26377;&#25277;&#35937;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#26041;&#27861;&#37117;&#33021;&#22815;&#25552;&#20379;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#25928;&#29575;&#30340;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;&#29366;&#24577;&#25277;&#35937;&#25216;&#26415;&#21487;&#20197;&#22312;&#20445;&#25345;&#19982;&#21407;&#38382;&#39064;&#26377;&#30028;&#25439;&#22833;&#30340;&#21516;&#26102;&#20943;&#23569;MDP&#30340;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#20004;&#31181;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#26102;&#65292;&#21363;MBRL&#20165;&#20165;&#35266;&#23519;&#25277;&#35937;&#29366;&#24577;&#26102;&#65292;&#21364;&#27809;&#26377;&#30456;&#24212;&#30340;&#20445;&#35777;&#21487;&#29992;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25277;&#35937;&#21487;&#20197;&#24341;&#20837;&#22312;&#32447;&#37319;&#38598;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65288;&#20363;&#22914;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#37319;&#38598;&#30340;&#26679;&#26412;&#65289;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#22914;&#26524;&#19981;&#32771;&#34385;&#36825;&#31181;&#30456;&#20851;&#24615;&#65292;MBRL&#30340;&#32467;&#26524;&#19981;&#33021;&#30452;&#25509;&#25512;&#24191;&#21040;&#36825;&#20010;&#35774;&#32622;&#20013;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#38789;&#19981;&#31561;&#24335;&#26469;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20010;&#32467;&#26524;&#20351;&#24471;&#23558;&#29616;&#26377;MBRL&#31639;&#27861;&#30340;&#20445;&#35777;&#25193;&#23637;&#21040;&#24102;&#26377;&#25277;&#35937;&#30340;&#35774;&#32622;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many methods for Model-based Reinforcement learning (MBRL) in Markov decision processes (MDPs) provide guarantees for both the accuracy of the model they can deliver and the learning efficiency. At the same time, state abstraction techniques allow for a reduction of the size of an MDP while maintaining a bounded loss with respect to the original problem. Therefore, it may come as a surprise that no such guarantees are available when combining both techniques, i.e., where MBRL merely observes abstract states. Our theoretical analysis shows that abstraction can introduce a dependence between samples collected online (e.g., in the real world). That means that, without taking this dependence into account, results for MBRL do not directly extend to this setting. Our result shows that we can use concentration inequalities for martingales to overcome this problem. This result makes it possible to extend the guarantees of existing MBRL algorithms to the setting with abstraction. We illustrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#33258;&#36866;&#24212;&#21644;&#33258;&#36866;&#24212;&#24773;&#20917;&#19979;&#65292;&#21463;&#32676;&#20307;&#24179;&#31561;&#32422;&#26463;&#30340;&#32463;&#20856;&#23376;&#27169;&#22359;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#31639;&#27861;&#27809;&#26377;&#32771;&#34385;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#23548;&#33268;&#19968;&#20123;&#29305;&#23450;&#32676;&#20307;&#30340;&#27424;&#20195;&#34920;&#25110;&#36807;&#20195;&#34920;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32676;&#20307;&#24179;&#31561;&#32422;&#26463;&#19979;&#36873;&#25321;&#19968;&#32452;&#39033;&#20197;&#26368;&#22823;&#21270;&#23376;&#27169;&#22359;&#25928;&#29992;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2207.03364</link><description>&lt;p&gt;
&#22312;&#33258;&#36866;&#24212;&#23376;&#27169;&#22359;&#26368;&#22823;&#21270;&#20013;&#30340;&#32676;&#20307;&#24179;&#31561;
&lt;/p&gt;
&lt;p&gt;
Group Equality in Adaptive Submodular Maximization. (arXiv:2207.03364v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#33258;&#36866;&#24212;&#21644;&#33258;&#36866;&#24212;&#24773;&#20917;&#19979;&#65292;&#21463;&#32676;&#20307;&#24179;&#31561;&#32422;&#26463;&#30340;&#32463;&#20856;&#23376;&#27169;&#22359;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#31639;&#27861;&#27809;&#26377;&#32771;&#34385;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#23548;&#33268;&#19968;&#20123;&#29305;&#23450;&#32676;&#20307;&#30340;&#27424;&#20195;&#34920;&#25110;&#36807;&#20195;&#34920;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32676;&#20307;&#24179;&#31561;&#32422;&#26463;&#19979;&#36873;&#25321;&#19968;&#32452;&#39033;&#20197;&#26368;&#22823;&#21270;&#23376;&#27169;&#22359;&#25928;&#29992;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#33258;&#36866;&#24212;&#21644;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#65292;&#21463;&#32676;&#20307;&#24179;&#31561;&#32422;&#26463;&#30340;&#32463;&#20856;&#23376;&#27169;&#22359;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#21253;&#25324;&#25968;&#25454;&#27719;&#24635;&#12289;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#26368;&#22823;&#21270;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#28385;&#36275;&#23376;&#27169;&#22359;&#24615;&#36136;&#12290;&#22240;&#27492;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#23376;&#27169;&#22359;&#26368;&#22823;&#21270;&#30340;&#26680;&#24515;&#26159;&#22312;&#21508;&#31181;&#32422;&#26463;&#19979;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#39033;&#65288;&#20363;&#22914;&#65292;&#25968;&#25454;&#28857;&#65289;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#30340;&#35774;&#35745;&#27809;&#26377;&#32771;&#34385;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#23548;&#33268;&#26576;&#20123;&#29305;&#23450;&#32676;&#20307;&#30340;&#27424;&#20195;&#34920;&#25110;&#36807;&#20195;&#34920;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#30740;&#31350;&#20855;&#26377;&#32676;&#20307;&#24179;&#31561;&#30340;&#23376;&#27169;&#22359;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#21363;&#22312;&#32676;&#20307;&#24179;&#31561;&#32422;&#26463;&#19979;&#36873;&#25321;&#19968;&#32452;&#39033;&#20197;&#26368;&#22823;&#21270;&#19968;&#20010;&#65288;&#21487;&#33021;&#38750;&#21333;&#35843;&#65289;&#30340;&#23376;&#27169;&#22359;&#25928;&#29992;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the classic submodular maximization problem subject to a group equality constraint under both non-adaptive and adaptive settings. It has been shown that the utility function of many machine learning applications, including data summarization, influence maximization in social networks, and personalized recommendation, satisfies the property of submodularity. Hence, maximizing a submodular function subject to various constraints can be found at the heart of many of those applications. On a high level, submodular maximization aims to select a group of most representative items (e.g., data points). However, the design of most existing algorithms does not incorporate the fairness constraint, leading to under- or over-representation of some particular groups. This motivates us to study the submodular maximization problem with group equality, where we aim to select a group of items to maximize a (possibly non-monotone) submodular utility function subject to a group equ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#26512;Hessian&#30697;&#38453;&#30340;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#19968;&#31867;Polyak-\L{}ojasiewicz&#20989;&#25968;&#22312;&#38750;&#20984;&#24615;&#34987;&#24179;&#22343;&#22788;&#29702;&#26102;&#30340;&#21487;&#35777;&#26126;&#21152;&#36895;&#65292;&#23545;&#20110;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2206.11872</link><description>&lt;p&gt;
&#19968;&#31867;Polyak-\L{}ojasiewicz&#20989;&#25968;&#22312;&#38750;&#20984;&#24615;&#34987;&#24179;&#22343;&#22788;&#29702;&#26102;&#65292;Heavy Ball&#36229;&#36234;&#20108;&#27425;&#21152;&#36895;&#30340;&#21487;&#35777;&#26126;&#24615;
&lt;/p&gt;
&lt;p&gt;
Provable Acceleration of Heavy Ball beyond Quadratics for a Class of Polyak-\L{}ojasiewicz Functions when the Non-Convexity is Averaged-Out. (arXiv:2206.11872v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#26512;Hessian&#30697;&#38453;&#30340;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#19968;&#31867;Polyak-\L{}ojasiewicz&#20989;&#25968;&#22312;&#38750;&#20984;&#24615;&#34987;&#24179;&#22343;&#22788;&#29702;&#26102;&#30340;&#21487;&#35777;&#26126;&#21152;&#36895;&#65292;&#23545;&#20110;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Heavy Ball (HB)&#26159;&#24403;&#20170;&#38750;&#20984;&#20248;&#21270;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#21160;&#37327;&#26041;&#27861;&#20043;&#19968;&#12290;&#24191;&#27867;&#35266;&#23519;&#21040;&#65292;&#22312;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20013;&#24341;&#20837;Heavy Ball&#21160;&#21147;&#23398;&#21487;&#20197;&#21152;&#36895;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#21152;&#36895;&#30340;&#29702;&#35770;&#22522;&#30784;&#22312;&#36827;&#23637;&#19978;&#26174;&#28982;&#36828;&#36828;&#33853;&#21518;&#20110;&#23454;&#35777;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#30340;&#21487;&#35777;&#26126;&#21152;&#36895;&#32467;&#26524;&#20165;&#38480;&#20110;&#20108;&#27425;&#25110;&#25509;&#36817;&#20108;&#27425;&#20989;&#25968;&#65292;&#22240;&#20026;&#30446;&#21069;HB&#21152;&#36895;&#30340;&#25216;&#26415;&#23616;&#38480;&#20110;Hessian&#30697;&#38453;&#22266;&#23450;&#30340;&#24773;&#20917;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20123;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#26512;&#20004;&#20010;&#36830;&#32493;&#26102;&#38388;&#28857;&#19978;Hessian&#30697;&#38453;&#30340;&#21464;&#21270;&#23545;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#20108;&#27425;&#21152;&#36895;&#30340;&#35777;&#26126;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#25216;&#26415;&#32467;&#26524;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31867;Polyak-\L{}ojasiewicz&#65288;PL&#65289;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;HB&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#21152;&#36895;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#35777;&#26126;&#20102;&#23545;&#20110;&#36825;&#20010;&#31867;&#21035;&#30340;&#38382;&#39064;&#65292;HB&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heavy Ball (HB) nowadays is one of the most popular momentum methods in non-convex optimization. It has been widely observed that incorporating the Heavy Ball dynamic in gradient-based methods accelerates the training process of modern machine learning models. However, the progress on establishing its theoretical foundation of acceleration is apparently far behind its empirical success. Existing provable acceleration results are of the quadratic or close-to-quadratic functions, as the current techniques of showing HB's acceleration are limited to the case when the Hessian is fixed. In this work, we develop some new techniques that help show acceleration beyond quadratics, which is achieved by analyzing how the change of the Hessian at two consecutive time points affects the convergence speed. Based on our technical results, a class of Polyak-\L{}ojasiewicz (PL) optimization problems for which provable acceleration can be achieved via HB is identified. Moreover, our analysis demonstrate
&lt;/p&gt;</description></item><item><title>PRANC&#26159;&#19968;&#31181;&#29992;&#20110;&#21387;&#32553;&#28145;&#24230;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#27169;&#22411;&#37325;&#26032;&#21442;&#25968;&#21270;&#20026;&#22810;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#22522;&#30784;&#32593;&#32476;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#23454;&#29616;&#12290; PRANC&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#28145;&#24230;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#24182;&#35299;&#20915;&#20102;&#23384;&#20648;&#21644;&#20256;&#36755;&#28145;&#24230;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;PRANC&#34920;&#29616;&#20986;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.08464</link><description>&lt;p&gt;
PRANC&#65306;&#29992;&#20110;&#21387;&#32553;&#28145;&#24230;&#27169;&#22411;&#30340;&#20266;&#38543;&#26426;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PRANC: Pseudo RAndom Networks for Compacting deep models. (arXiv:2206.08464v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08464
&lt;/p&gt;
&lt;p&gt;
PRANC&#26159;&#19968;&#31181;&#29992;&#20110;&#21387;&#32553;&#28145;&#24230;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#27169;&#22411;&#37325;&#26032;&#21442;&#25968;&#21270;&#20026;&#22810;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#22522;&#30784;&#32593;&#32476;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#23454;&#29616;&#12290; PRANC&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#28145;&#24230;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#24182;&#35299;&#20915;&#20102;&#23384;&#20648;&#21644;&#20256;&#36755;&#28145;&#24230;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;PRANC&#34920;&#29616;&#20986;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#27169;&#22411;&#21487;&#20197;&#34987;&#37325;&#26032;&#21442;&#25968;&#21270;&#20026;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#20960;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#24182;&#20923;&#32467;&#30340;&#28145;&#24230;&#27169;&#22411;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23547;&#25214;&#23384;&#22312;&#20110;&#36825;&#20123;&#38543;&#26426;&#27169;&#22411;&#65288;&#21363;&#8220;&#22522;&#30784;&#8221;&#32593;&#32476;&#65289;&#25152;&#24352;&#25104;&#30340;&#23376;&#31354;&#38388;&#20013;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;PRANC&#33021;&#22815;&#26174;&#33879;&#21387;&#32553;&#28145;&#24230;&#27169;&#22411;&#12290;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#30340;&#26631;&#37327;&#8220;&#31181;&#23376;&#8221;&#26469;&#29983;&#25104;&#20266;&#38543;&#26426;&#30340;&#8220;&#22522;&#30784;&#8221;&#32593;&#32476;&#65292;&#20877;&#32467;&#21512;&#23398;&#20064;&#21040;&#30340;&#32447;&#24615;&#28151;&#21512;&#31995;&#25968;&#65292;&#21487;&#20197;&#37325;&#26500;&#27169;&#22411;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;PRANC&#35299;&#20915;&#20102;&#39640;&#25928;&#23384;&#20648;&#21644;&#20256;&#36755;&#28145;&#24230;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#36825;&#22312;&#21253;&#25324;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#32852;&#37030;&#31995;&#32479;&#21644;&#36793;&#32536;&#35774;&#22791;&#31561;&#22810;&#31181;&#24773;&#20917;&#19979;&#37117;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#29942;&#39048;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;PRANC&#26469;&#21387;&#32553;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#24182;&#36890;&#36807;&#21387;&#32553;&#20854;&#30456;&#20851;&#30340;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#26469;&#21387;&#32553;&#22270;&#20687;&#12290;PRANC&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate that a deep model can be reparametrized as a linear combination of several randomly initialized and frozen deep models in the weight space. During training, we seek local minima that reside within the subspace spanned by these random models (i.e., `basis' networks). Our framework, PRANC, enables significant compaction of a deep model. The model can be reconstructed using a single scalar `seed,' employed to generate the pseudo-random `basis' networks, together with the learned linear mixture coefficients.  In practical applications, PRANC addresses the challenge of efficiently storing and communicating deep models, a common bottleneck in several scenarios, including multi-agent learning, continual learners, federated systems, and edge devices, among others. In this study, we employ PRANC to condense image classification models and compress images by compacting their associated implicit neural networks. PRANC outperforms baselines with a large margin on image classificatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26234;&#33021;&#29289;&#32852;&#32593;&#25968;&#25454;&#24066;&#22330;&#30340;&#25112;&#30053;&#32852;&#30431;&#65292;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#20998;&#26512;&#65292;&#22522;&#20110;&#25968;&#25454;&#20215;&#20540;&#25511;&#21046;&#25968;&#25454;&#20215;&#26684;&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#35299;&#20915;&#20102;&#21442;&#19982;&#24230;&#30340;&#25361;&#25112;&#65292;&#24314;&#31435;&#20102;&#25968;&#25454;&#24066;&#22330;&#65292;&#24182;&#26368;&#22823;&#21270;&#20102;&#31038;&#20132;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2206.07785</link><description>&lt;p&gt;
&#26234;&#33021;&#29289;&#32852;&#32593;&#25968;&#25454;&#24066;&#22330;&#30340;&#25112;&#30053;&#32852;&#30431;
&lt;/p&gt;
&lt;p&gt;
Strategic Coalition for Data Pricing in IoT Data Markets. (arXiv:2206.07785v4 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26234;&#33021;&#29289;&#32852;&#32593;&#25968;&#25454;&#24066;&#22330;&#30340;&#25112;&#30053;&#32852;&#30431;&#65292;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#20998;&#26512;&#65292;&#22522;&#20110;&#25968;&#25454;&#20215;&#20540;&#25511;&#21046;&#25968;&#25454;&#20215;&#26684;&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#35299;&#20915;&#20102;&#21442;&#19982;&#24230;&#30340;&#25361;&#25112;&#65292;&#24314;&#31435;&#20102;&#25968;&#25454;&#24066;&#22330;&#65292;&#24182;&#26368;&#22823;&#21270;&#20102;&#31038;&#20132;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#29992;&#20110;&#20132;&#26131;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29289;&#32852;&#32593;&#25968;&#25454;&#30340;&#24066;&#22330;&#12290;&#25968;&#25454;&#65292;&#26080;&#35770;&#26159;&#21407;&#22987;&#36824;&#26159;&#32463;&#36807;&#22788;&#29702;&#65292;&#36890;&#36807;&#32593;&#32476;&#25552;&#20379;&#32473;&#24066;&#22330;&#24179;&#21488;&#65292;&#24182;&#26681;&#25454;&#20854;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20215;&#20540;&#26469;&#25511;&#21046;&#25968;&#25454;&#30340;&#20215;&#26684;&#12290;&#25105;&#20204;&#22312;&#21338;&#24328;&#35770;&#30340;&#26694;&#26550;&#19979;&#25506;&#35752;&#20102;&#25968;&#25454;&#30340;&#30456;&#20851;&#23646;&#24615;&#65292;&#26368;&#32456;&#24471;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#20998;&#24067;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#24378;&#35843;&#20102;&#35774;&#22791;&#21644;&#24066;&#22330;&#30340;&#30456;&#20114;&#21033;&#30410;&#12290;&#20851;&#38190;&#25552;&#35758;&#26159;&#19968;&#31181;&#29992;&#20110;&#24066;&#22330;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20849;&#21516;&#24212;&#23545;&#21442;&#19982;&#24230;&#30340;&#21487;&#29992;&#24615;&#21644;&#24322;&#36136;&#24615;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#22312;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#30340;&#20449;&#20219;&#20256;&#36882;&#21644;&#25968;&#25454;&#20132;&#25442;&#30340;&#32463;&#27982;&#20215;&#20540;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#21152;&#24378;&#20855;&#26377;&#30456;&#20851;&#25968;&#25454;&#30340;&#35774;&#22791;&#20043;&#38388;&#30340;&#21327;&#20316;&#26426;&#20250;&#65292;&#24314;&#31435;&#20102;&#25968;&#25454;&#24066;&#22330;&#65292;&#20197;&#36991;&#20813;&#20449;&#24687;&#27844;&#38706;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32593;&#32476;&#20248;&#21270;&#38382;&#39064;&#65292;&#26368;&#22823;&#21270;&#20102;&#31038;&#20132;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a market for trading Internet of Things (IoT) data that is used to train machine learning models. The data, either raw or processed, is supplied to the market platform through a network and the price of such data is controlled based on the value it brings to the machine learning model. We explore the correlation property of data in a game-theoretical setting to eventually derive a simplified distributed solution for a data trading mechanism that emphasizes the mutual benefit of devices and the market. The key proposal is an efficient algorithm for markets that jointly addresses the challenges of availability and heterogeneity in participation, as well as the transfer of trust and the economic value of data exchange in IoT networks. The proposed approach establishes the data market by reinforcing collaboration opportunities between device with correlated data to avoid information leakage. Therein, we develop a network-wide optimization problem that maximizes the soc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#26102;&#38388;&#32447;&#20998;&#21106;&#25104;&#22810;&#20010;&#27573;&#24182;&#29992;&#27850;&#26494;&#36807;&#31243;&#26469;&#24314;&#27169;&#36793;&#30340;&#21442;&#25968;&#65292;&#23545;&#26102;&#38388;&#32593;&#32476;&#20013;&#30340;&#24490;&#29615;&#27963;&#21160;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2205.09862</link><description>&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#20998;&#21106;&#21644;&#22359;&#27169;&#22411;&#30340;&#26102;&#38388;&#32593;&#32476;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Recurrent segmentation meets block models in temporal networks. (arXiv:2205.09862v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#26102;&#38388;&#32447;&#20998;&#21106;&#25104;&#22810;&#20010;&#27573;&#24182;&#29992;&#27850;&#26494;&#36807;&#31243;&#26469;&#24314;&#27169;&#36793;&#30340;&#21442;&#25968;&#65292;&#23545;&#26102;&#38388;&#32593;&#32476;&#20013;&#30340;&#24490;&#29615;&#27963;&#21160;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32593;&#32476;&#20132;&#20114;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#23558;&#20854;&#34920;&#31034;&#20026;&#19968;&#20010;&#33410;&#28857;&#26159;&#20195;&#29702;&#32773;&#32780;&#36793;&#26159;&#20132;&#20114;&#30340;&#32593;&#32476;&#12290;&#36825;&#20123;&#20132;&#20114;&#36890;&#24120;&#26377;&#26102;&#38388;&#25139;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#24102;&#26377;&#26102;&#38388;&#25139;&#30340;&#36793;&#12290;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#32593;&#32476;&#20855;&#26377;&#24490;&#29615;&#25110;&#21487;&#33021;&#24490;&#29615;&#30340;&#34892;&#20026;&#12290;&#20363;&#22914;&#65292;&#31038;&#20132;&#32593;&#32476;&#27963;&#21160;&#21487;&#33021;&#22312;&#19968;&#22825;&#20013;&#30340;&#29305;&#23450;&#26102;&#38388;&#22686;&#21152;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#23545;&#36825;&#31181;&#26102;&#38388;&#32593;&#32476;&#20013;&#30340;&#24490;&#29615;&#27963;&#21160;&#36827;&#34892;&#24314;&#27169;&#12290;&#20316;&#20026;&#36215;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#38543;&#26426;&#22359;&#27169;&#22411;&#65292;&#36825;&#26159;&#27169;&#25311;&#38745;&#24577;&#32593;&#32476;&#30340;&#19968;&#31181;&#27969;&#34892;&#36873;&#25321;&#65292;&#20854;&#20013;&#33410;&#28857;&#34987;&#20998;&#25104; R &#32452;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#27850;&#26494;&#36807;&#31243;&#26469;&#23558;&#35813;&#27169;&#22411;&#25193;&#23637;&#21040;&#26102;&#38388;&#32593;&#32476;&#20013;&#20197;&#24314;&#27169;&#36793;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#26102;&#38388;&#32447;&#20998;&#21106;&#25104; K &#20010;&#27573;&#26469;&#20351;&#36807;&#31243;&#30340;&#21442;&#25968;&#20381;&#36182;&#20110;&#26102;&#38388;&#12290;&#20026;&#20102;&#23454;&#29616;&#24490;&#29615;&#27963;&#21160;&#65292;&#25105;&#20204;&#35201;&#27714;&#21482;&#33021;&#20351;&#29992; H &lt; K &#20010;&#19981;&#21516;&#30340;&#21442;&#25968;&#38598;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#20960;&#20010;&#19981;&#19968;&#23450;&#36830;&#32493;&#30340;&#27573;&#24517;&#39035;&#20849;&#20139;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
A popular approach to model interactions is to represent them as a network with nodes being the agents and the interactions being the edges. Interactions are often timestamped, which leads to having timestamped edges. Many real-world temporal networks have a recurrent or possibly cyclic behaviour. For example, social network activity may be heightened during certain hours of day. In this paper, our main interest is to model recurrent activity in such temporal networks. As a starting point we use stochastic block model, a popular choice for modelling static networks, where nodes are split into $R$ groups. We extend this model to temporal networks by modelling the edges with a Poisson process. We make the parameters of the process dependent on time by segmenting the time line into $K$ segments. To enforce the recurring activity we require that only $H &lt; K$ different set of parameters can be used, that is, several, not necessarily consecutive, segments must share their parameters. We prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Langevin&#25193;&#25955;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#25277;&#26679;&#21644;&#32039;&#23494;&#22343;&#21248;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#20174;&#32780;&#23545;&#20984;&#20248;&#21270;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#26368;&#20248;&#30340;&#36807;&#24230;&#32463;&#39564;&#21644;&#24635;&#20307;&#39118;&#38505;&#20445;&#35777;&#12290;&#35813;&#26694;&#26550;&#36824;&#20801;&#35768;&#35774;&#35745;&#24046;&#20998;&#38544;&#31169;&#22343;&#21248;&#25277;&#26679;&#22120;&#65292;&#24212;&#29992;&#20110;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#31561;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2204.01585</link><description>&lt;p&gt;
&#26469;&#33258;&#25289;&#20160;&#33707;&#24681;&#38598;&#21512;&#30340;&#24046;&#20998;&#38544;&#31169;&#25277;&#26679;&#20197;&#21450;Langevin&#25193;&#25955;&#22312;&#20984;&#20248;&#21270;&#20013;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Sampling from Rashomon Sets, and the Universality of Langevin Diffusion for Convex Optimization. (arXiv:2204.01585v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Langevin&#25193;&#25955;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#25277;&#26679;&#21644;&#32039;&#23494;&#22343;&#21248;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#20174;&#32780;&#23545;&#20984;&#20248;&#21270;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#26368;&#20248;&#30340;&#36807;&#24230;&#32463;&#39564;&#21644;&#24635;&#20307;&#39118;&#38505;&#20445;&#35777;&#12290;&#35813;&#26694;&#26550;&#36824;&#20801;&#35768;&#35774;&#35745;&#24046;&#20998;&#38544;&#31169;&#22343;&#21248;&#25277;&#26679;&#22120;&#65292;&#24212;&#29992;&#20110;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#31561;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Langevin&#25193;&#25955;&#65288;LD&#65289;&#21450;&#20854;&#30456;&#24212;&#31163;&#25955;&#21270;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#65306;i&#65289;&#19968;&#31181;&#20174;&#25351;&#25968;&#26426;&#21046;&#20013;&#36827;&#34892;&#25277;&#26679;&#30340;&#31639;&#27861;&#65292;&#20854;&#38544;&#31169;&#20998;&#26512;&#19981;&#20381;&#36182;&#20110;&#20984;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20219;&#20309;&#26102;&#20505;&#20572;&#27490;&#32780;&#19981;&#25439;&#23475;&#38544;&#31169;&#65307;ii&#65289;&#25351;&#25968;&#26426;&#21046;&#30340;&#32039;&#23494;&#22343;&#21248;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;&#20316;&#20026;&#30452;&#25509;&#32467;&#26524;&#65292;&#22312;&#32431;&#31929;&#21644;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#19979;&#65292;&#33719;&#24471;&#20102;&#65288;&#24378;&#65289;&#20984;&#25439;&#22833;&#30340;&#26368;&#20248;&#36807;&#24230;&#32463;&#39564;&#21644;&#24635;&#20307;&#39118;&#38505;&#20445;&#35777;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#25105;&#20204;&#35774;&#35745;&#26469;&#33258;&#25289;&#20160;&#33707;&#24681;&#38598;&#21512;&#30340;&#24046;&#20998;&#38544;&#31169;&#22343;&#21248;&#25277;&#26679;&#22120;&#12290;&#25289;&#20160;&#33707;&#24681;&#38598;&#21512;&#22312;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#12289;&#29702;&#35299;&#21464;&#37327;&#37325;&#35201;&#24615;&#21644;&#34920;&#24449;&#20844;&#24179;&#24615;&#26041;&#38754;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we provide an algorithmic framework based on Langevin diffusion (LD) and its corresponding discretizations that allow us to simultaneously obtain: i) An algorithm for sampling from the exponential mechanism, whose privacy analysis does not depend on convexity and which can be stopped at anytime without compromising privacy, and ii) tight uniform stability guarantees for the exponential mechanism. As a direct consequence, we obtain optimal excess empirical and population risk guarantees for (strongly) convex losses under both pure and approximate differential privacy (DP). The framework allows us to design a DP uniform sampler from the Rashomon set. Rashomon sets are widely used in interpretable and robust machine learning, understanding variable importance, and characterizing fairness.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#32852;&#21512;&#23398;&#20064;&#20013;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#35777;&#21644;&#38450;&#24481;&#25915;&#20987;&#65292;&#24182;&#37319;&#29992;&#20102;&#23433;&#20840;&#30340;&#32858;&#21512;&#21327;&#35758;&#21644;&#38646;&#30693;&#35782;&#35777;&#26126;&#21327;&#35758;&#26469;&#35299;&#20915;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.03402</link><description>&lt;p&gt;
&#22312;&#32852;&#21512;&#23398;&#20064;&#20013;&#20445;&#25252;&#38544;&#31169;&#21644;&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
Preserving Privacy and Security in Federated Learning. (arXiv:2202.03402v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03402
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#32852;&#21512;&#23398;&#20064;&#20013;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#35777;&#21644;&#38450;&#24481;&#25915;&#20987;&#65292;&#24182;&#37319;&#29992;&#20102;&#23433;&#20840;&#30340;&#32858;&#21512;&#21327;&#35758;&#21644;&#38646;&#30693;&#35782;&#35777;&#26126;&#21327;&#35758;&#26469;&#35299;&#20915;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#23454;&#23384;&#22312;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#33268;&#21147;&#20110;&#38450;&#27490;&#26469;&#33258;&#29992;&#25143;&#30340;&#25915;&#20987;&#65292;&#35201;&#20040;&#33268;&#21147;&#20110;&#23558;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#23545;&#26381;&#21153;&#22120;&#36827;&#34892;&#38544;&#34255;&#65292;&#20294;&#24182;&#38750;&#20004;&#32773;&#20860;&#24471;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20004;&#20010;&#30740;&#31350;&#39046;&#22495;&#25972;&#21512;&#36215;&#26469;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#23041;&#32961;&#27169;&#22411;&#26041;&#38754;&#24120;&#24120;&#30456;&#20114;&#20914;&#31361;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#25915;&#20987;&#36827;&#34892;&#26816;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#32858;&#21512;&#21327;&#35758;&#65292;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#20351;&#26381;&#21153;&#22120;&#21487;&#20197;&#20197;&#38544;&#31169;&#26041;&#24335;&#21512;&#24182;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#21327;&#35758;&#65292;&#23558;&#26816;&#27979;&#25915;&#20987;&#30340;&#20219;&#21153;&#20174;&#26381;&#21153;&#22120;&#36716;&#31227;&#21040;&#29992;&#25143;&#31471;&#12290;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#26381;&#21153;&#22120;&#19981;&#20877;&#38656;&#35201;&#35775;&#38382;&#26412;&#22320;&#27169;&#22411;&#20197;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is known to be vulnerable to both security and privacy issues. Existing research has focused either on preventing poisoning attacks from users or on concealing the local model updates from the server, but not both. However, integrating these two lines of research remains a crucial challenge since they often conflict with one another with respect to the threat model. In this work, we develop a principle framework that offers both privacy guarantees for users and detection against poisoning attacks from them. With a new threat model that includes both an honest-but-curious server and malicious users, we first propose a secure aggregation protocol using homomorphic encryption for the server to combine local model updates in a private manner. Then, a zero-knowledge proof protocol is leveraged to shift the task of detecting attacks in the local models from the server to the users. The key observation here is that the server no longer needs access to the local models for a
&lt;/p&gt;</description></item><item><title>AdaTerm&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;T&#20998;&#24067;&#20272;&#35745;&#31283;&#20581;&#30697;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#20248;&#21270;&#31639;&#27861;&#30340;&#32479;&#19968;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2201.06714</link><description>&lt;p&gt;
AdaTerm: &#33258;&#36866;&#24212;T&#20998;&#24067;&#20272;&#35745;&#31283;&#20581;&#30697;&#29992;&#20110;&#22122;&#22768;&#20581;&#22766;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
AdaTerm: Adaptive T-Distribution Estimated Robust Moments for Noise-Robust Stochastic Gradient Optimization. (arXiv:2201.06714v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06714
&lt;/p&gt;
&lt;p&gt;
AdaTerm&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;T&#20998;&#24067;&#20272;&#35745;&#31283;&#20581;&#30697;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#20248;&#21270;&#31639;&#27861;&#30340;&#32479;&#19968;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#22686;&#21152;&#65292;&#20174;&#21508;&#31181;&#26469;&#28304;&#22914;&#27979;&#37327;&#35823;&#24046;&#12289;&#38169;&#35823;&#26631;&#35760;&#21644;&#20272;&#35745;&#20195;&#29702;&#36755;&#20837;/&#36755;&#20986;&#20013;&#21463;&#25439;&#30340;&#25968;&#25454;&#38598;&#19981;&#21487;&#36991;&#20813;&#22320;&#24433;&#21709;&#20102;&#20248;&#21270;&#32467;&#26524;&#65292;&#25552;&#39640;&#20248;&#21270;&#31639;&#27861;&#23545;&#22122;&#22768;&#30340;&#31283;&#20581;&#24615;&#24050;&#25104;&#20026;&#24120;&#35265;&#20570;&#27861;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;Adam-like&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#20013;&#20351;&#29992;&#30340;&#19968;&#38454;&#30697;&#21487;&#20197;&#22522;&#20110;&#23398;&#29983;t&#20998;&#24067;&#36827;&#34892;&#20462;&#25913;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20462;&#25913;&#21482;&#24433;&#21709;&#20102;&#19968;&#38454;&#30697;&#65292;&#20854;&#20182;&#20851;&#32852;&#30340;&#32479;&#35745;&#37327;&#20445;&#25345;&#19981;&#21464;&#65292;&#23548;&#33268;&#20102;&#25152;&#20551;&#35774;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AdaTerm&#65292;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#23558;&#23398;&#29983;t&#20998;&#24067;&#29992;&#20110;&#25512;&#23548;&#19968;&#38454;&#30697;&#21644;&#25152;&#26377;&#20851;&#32852;&#30340;&#32479;&#35745;&#37327;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#23545;&#20248;&#21270;&#31639;&#27861;&#30340;&#32479;&#19968;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing practicality of deep learning applications, practitioners are inevitably faced with datasets corrupted by noise from various sources such as measurement errors, mislabeling, and estimated surrogate inputs/outputs that can adversely impact the optimization results. It is a common practice to improve the optimization algorithm's robustness to noise, since this algorithm is ultimately in charge of updating the network parameters. Previous studies revealed that the first-order moment used in Adam-like stochastic gradient descent optimizers can be modified based on the Student's t-distribution. While this modification led to noise-resistant updates, the other associated statistics remained unchanged, resulting in inconsistencies in the assumed models. In this paper, we propose AdaTerm, a novel approach that incorporates the Student's t-distribution to derive not only the first-order moment but also all the associated statistics. This provides a unified treatment of the o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#19981;&#23436;&#20840;&#29305;&#24449;&#21644;&#26631;&#31614;&#12289;&#22122;&#22768;&#35270;&#22270;&#21644;&#26631;&#31614;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#19981;&#23436;&#20840;&#30340;&#35270;&#22270;&#21644;&#24369;&#26631;&#31614;&#23884;&#20837;&#21040;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#26435;&#37325;&#21644;&#23884;&#20837;&#30697;&#38453;&#24046;&#24322;&#26469;&#20943;&#23569;&#20887;&#20313;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.01079</link><description>&lt;p&gt;
&#19981;&#23436;&#20840;&#30340;&#22810;&#35270;&#35282;&#24369;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incomplete Multi-View Weak-Label Learning. (arXiv:2201.01079v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#19981;&#23436;&#20840;&#29305;&#24449;&#21644;&#26631;&#31614;&#12289;&#22122;&#22768;&#35270;&#22270;&#21644;&#26631;&#31614;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#19981;&#23436;&#20840;&#30340;&#35270;&#22270;&#21644;&#24369;&#26631;&#31614;&#23884;&#20837;&#21040;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#26435;&#37325;&#21644;&#23884;&#20837;&#30697;&#38453;&#24046;&#24322;&#26469;&#20943;&#23569;&#20887;&#20313;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24212;&#29992;&#20013;&#23384;&#22312;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#65292;&#27599;&#20010;&#26679;&#26412;&#20855;&#26377;&#22810;&#20010;&#35270;&#22270;&#29305;&#24449;&#65292;&#22810;&#20010;&#26631;&#31614;&#36890;&#36807;&#20849;&#21516;&#30340;&#35270;&#22270;&#30456;&#20851;&#32852;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#19981;&#33021;&#30452;&#25509;&#22788;&#29702;&#27599;&#20010;&#26679;&#26412;&#21482;&#35266;&#23519;&#21040;&#37096;&#20998;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;&#24182;&#24573;&#30053;&#20102;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#35270;&#22270;&#21644;&#19981;&#22343;&#34913;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#23427;&#23558;&#19981;&#23436;&#20840;&#30340;&#35270;&#22270;&#21644;&#24369;&#26631;&#31614;&#19968;&#36215;&#23884;&#20837;&#21040;&#33258;&#36866;&#24212;&#26435;&#37325;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#65288;HSIC&#65289;&#30340;&#23884;&#20837;&#26435;&#37325;&#30697;&#38453;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#20943;&#23569;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#23427;&#36890;&#36807;&#32858;&#28966;&#25439;&#22833;&#33258;&#36866;&#24212;&#23398;&#20064;&#35270;&#22270;&#30340;&#37325;&#35201;&#24615;&#20197;&#26816;&#27979;&#22122;&#22768;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#32858;&#28966;&#25439;&#22833;&#20943;&#36731;&#26631;&#31614;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23545;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A variety of modern applications exhibit multi-view multi-label learning, where each sample has multi-view features, and multiple labels are correlated via common views. Current methods usually fail to directly deal with the setting where only a subset of features and labels are observed for each sample, and ignore the presence of noisy views and imbalanced labels in real-world problems. In this paper, we propose a novel method to overcome the limitations. It jointly embeds incomplete views and weak labels into a low-dimensional subspace with adaptive weights, and facilitates the difference between embedding weight matrices via auto-weighted Hilbert-Schmidt Independence Criterion (HSIC) to reduce the redundancy. Moreover, it adaptively learns view-wise importance for embedding to detect noisy views, and mitigates the label imbalance problem by focal loss. Experimental results on four real-world multi-view multi-label datasets demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#21457;&#29616;&#36890;&#29992;&#30340;&#21069;&#26399;&#35757;&#32451;&#21487;&#20197;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2112.09153</link><description>&lt;p&gt;
&#21069;&#26399;&#35757;&#32451;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Investigation of the Role of Pre-training in Lifelong Learning. (arXiv:2112.09153v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09153
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#21457;&#29616;&#36890;&#29992;&#30340;&#21069;&#26399;&#35757;&#32451;&#21487;&#20197;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#32456;&#36523;&#23398;&#20064;&#33539;&#24335;&#19981;&#20165;&#22240;&#20854;&#31867;&#20284;&#29983;&#29289;&#23398;&#20064;&#30340;&#29305;&#24615;&#32780;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#32780;&#19988;&#22240;&#20854;&#36890;&#36807;&#36991;&#20813;&#36807;&#22810;&#30340;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#32780;&#20943;&#23569;&#33021;&#28304;&#28010;&#36153;&#30340;&#28508;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#19968;&#33539;&#24335;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26085;&#30410;&#27969;&#34892;&#21644;&#25104;&#21151;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#65292;&#21069;&#26399;&#35757;&#32451;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#26041;&#38754;&#25198;&#28436;&#20309;&#31181;&#35282;&#33394;&#65311;&#25105;&#20204;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#30740;&#31350;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#25991;&#26412;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20351;&#29992;&#19968;&#20010;&#26032;&#39062;&#30340;&#21253;&#21547;15&#20010;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#30456;&#27604;&#65292;&#36890;&#29992;&#30340;&#21069;&#26399;&#35757;&#32451;&#22312;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#26102;&#38544;&#21547;&#22320;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning but also its potential to reduce energy waste by obviating excessive model re-training. A key challenge to this paradigm is the phenomenon of catastrophic forgetting. With the increasing popularity and success of pre-trained models in machine learning, we pose the question: What role does pre-training play in lifelong learning, specifically with respect to catastrophic forgetting? We investigate existing methods in the context of large, pre-trained models and evaluate their performance on a variety of text and image classification tasks, including a large-scale study using a novel data set of 15 diverse NLP tasks. Across all settings, we observe that generic pre-training implicitly alleviates the effects of catastrophic forgetting when learning multiple tasks sequentially compared to randomly initialized mo
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#20581;&#22766;&#24615;&#19979;Bayes&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#23384;&#22312;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#33324;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#21487;&#20197;&#20026;&#30740;&#31350;&#23545;&#25239;&#24615;&#20195;&#29702;&#25439;&#22833;&#21644;&#20854;&#19968;&#33268;&#24615;&#23646;&#24615;&#25552;&#20379;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2112.01694</link><description>&lt;p&gt;
&#20851;&#20110;&#23545;&#25239;&#24615;Bayes&#20998;&#31867;&#22120;&#23384;&#22312;&#24615;&#30340;&#30740;&#31350;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
On the Existence of the Adversarial Bayes Classifier (Extended Version). (arXiv:2112.01694v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#20581;&#22766;&#24615;&#19979;Bayes&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#23384;&#22312;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#33324;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#21487;&#20197;&#20026;&#30740;&#31350;&#23545;&#25239;&#24615;&#20195;&#29702;&#25439;&#22833;&#21644;&#20854;&#19968;&#33268;&#24615;&#23646;&#24615;&#25552;&#20379;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#20581;&#22766;&#24615;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#26368;&#36817;&#24050;&#32463;&#26377;&#22810;&#39033;&#29702;&#35770;&#30740;&#31350;&#65292;&#20294;&#19982;&#23545;&#25239;&#35757;&#32451;&#20581;&#22766;&#24615;&#30456;&#20851;&#30340;&#35768;&#22810;&#37325;&#35201;&#38382;&#39064;&#20173;&#28982;&#26410;&#34987;&#35299;&#20915;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20851;&#20110;&#23545;&#25239;&#35757;&#32451;&#20581;&#22766;&#24615;&#19979;Bayes&#26368;&#20248;&#20998;&#31867;&#22120;&#23384;&#22312;&#24615;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#33324;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#20445;&#35777;&#23384;&#22312;&#23545;&#25239;&#35757;&#32451;&#20581;&#22766;&#24615;&#19979;&#30340;Bayes&#26368;&#20248;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#20026;&#23545;&#21518;&#32493;&#23545;&#25239;&#35757;&#32451;&#20581;&#22766;&#24615;&#19979;&#20195;&#29702;&#25439;&#22833;&#21644;&#23427;&#20204;&#30340;&#19968;&#33268;&#24615;&#23646;&#24615;&#30340;&#30740;&#31350;&#25552;&#20379;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#26159;&#8220;&#20851;&#20110;&#23545;&#25239;&#24615;Bayes&#20998;&#31867;&#22120;&#23384;&#22312;&#24615;&#8221;&#30340;&#30699;&#27491;&#21644;&#25193;&#23637;&#29256;&#26412;&#65292;&#35813;&#31295;&#20214;&#24050;&#21457;&#34920;&#22312;NeurIPS 2021&#19978;&#12290;&#21407;&#22987;&#35770;&#25991;&#20013;&#26377;&#20004;&#22788;&#23450;&#29702;&#38169;&#35823;&#65292;&#19968;&#22788;&#26159;&#23545;&#20266;&#21487;&#35777;&#20581;&#22766;&#24615;&#30340;&#23450;&#20041;&#65292;&#21478;&#19968;&#22788;&#26159;&#38024;&#23545;&#20219;&#24847;&#24230;&#37327;&#31354;&#38388;&#30340;$A^\e$&#21487;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness is a critical property in a variety of modern machine learning applications. While it has been the subject of several recent theoretical studies, many important questions related to adversarial robustness are still open. In this work, we study a fundamental question regarding Bayes optimality for adversarial robustness. We provide general sufficient conditions under which the existence of a Bayes optimal classifier can be guaranteed for adversarial robustness. Our results can provide a useful tool for a subsequent study of surrogate losses in adversarial robustness and their consistency properties. This manuscript is the extended and corrected version of the paper \emph{On the Existence of the Adversarial Bayes Classifier} published in NeurIPS 2021. There were two errors in theorem statements in the original paper -- one in the definition of pseudo-certifiable robustness and the other in the measurability of $A^\e$ for arbitrary metric spaces. In this version we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#35757;&#32451;&#26234;&#33021;&#20307;&#37319;&#21462;&#20154;&#31867;&#33324;&#30340;&#30896;&#25758;&#36991;&#20813;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#36229;&#36234;&#19987;&#23478;&#21644;&#26410;&#32463;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#65292;&#22312;&#30896;&#25758;&#36991;&#20813;&#21644;&#30446;&#26631;&#23548;&#21521;&#36716;&#21521;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2103.10000</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#20154;&#31867;&#21551;&#21457;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Human-Inspired Multi-Agent Navigation using Knowledge Distillation. (arXiv:2103.10000v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.10000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#35757;&#32451;&#26234;&#33021;&#20307;&#37319;&#21462;&#20154;&#31867;&#33324;&#30340;&#30896;&#25758;&#36991;&#20813;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#36229;&#36234;&#19987;&#23478;&#21644;&#26410;&#32463;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#65292;&#22312;&#30896;&#25758;&#36991;&#20813;&#21644;&#30446;&#26631;&#23548;&#21521;&#36716;&#21521;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26234;&#33021;&#20307;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20173;&#32570;&#20047;&#20154;&#31867;&#25152;&#23637;&#31034;&#30340;&#22797;&#26434;&#24615;&#21644;&#26234;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20154;&#31867;&#33324;&#26222;&#36941;&#30340;&#30896;&#25758;&#36991;&#20813;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#23436;&#20840;&#20998;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;-&#26234;&#33021;&#20307;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#26681;&#25454;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#20174;&#20154;&#31867;&#36712;&#36857;&#28436;&#31034;&#20013;&#25552;&#21462;&#30340;&#19987;&#23478;&#31574;&#30053;&#26469;&#30830;&#23450;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#22312;&#30896;&#25758;&#36991;&#20813;&#21644;&#30446;&#26631;&#23548;&#21521;&#36716;&#21521;&#20219;&#21153;&#20013;&#37319;&#21462;&#20154;&#31867;&#33324;&#30340;&#36712;&#36857;&#65292;&#36229;&#36807;&#20102;&#19987;&#23478;&#21644;&#26410;&#32463;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant advancements in the field of multi-agent navigation, agents still lack the sophistication and intelligence that humans exhibit in multi-agent settings. In this paper, we propose a framework for learning a human-like general collision avoidance policy for agent-agent interactions in fully decentralized, multi-agent environments. Our approach uses knowledge distillation with reinforcement learning to shape the reward function based on expert policies extracted from human trajectory demonstrations through behavior cloning. We show that agents trained with our approach can take human-like trajectories in collision avoidance and goal-directed steering tasks not provided by the demonstrations, outperforming the experts as well as learning-based agents trained without knowledge distillation.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StarNet&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25317;&#25380;&#26143;&#22330;&#20013;&#20998;&#31163;&#20809;&#28304;&#12290;StarNet&#21033;&#29992;&#20102;&#21464;&#20998;&#25512;&#26029;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#27604;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2102.02409</link><description>&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#29992;&#20110;&#20998;&#31163;&#25317;&#25380;&#26143;&#22330;
&lt;/p&gt;
&lt;p&gt;
Variational Inference for Deblending Crowded Starfields. (arXiv:2102.02409v3 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.02409
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StarNet&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25317;&#25380;&#26143;&#22330;&#20013;&#20998;&#31163;&#20809;&#28304;&#12290;StarNet&#21033;&#29992;&#20102;&#21464;&#20998;&#25512;&#26029;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#27604;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22825;&#25991;&#35843;&#26597;&#20013;&#25910;&#38598;&#30340;&#22270;&#20687;&#20013;&#65292;&#26143;&#26143;&#21644;&#26143;&#31995;&#24120;&#24120;&#22312;&#35270;&#35273;&#19978;&#37325;&#21472;&#12290;&#20998;&#31163;&#26159;&#22312;&#35843;&#26597;&#22270;&#20687;&#20013;&#21306;&#20998;&#21644;&#34920;&#24449;&#21333;&#20010;&#20809;&#28304;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;StarNet&#65292;&#19968;&#31181;&#29992;&#20110;&#20998;&#31163;&#25317;&#25380;&#26143;&#26143;&#21306;&#22495;&#22825;&#25991;&#22270;&#20687;&#20013;&#20809;&#28304;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;StarNet&#21033;&#29992;&#20102;&#21464;&#20998;&#25512;&#26029;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#25674;&#38144;&#30340;&#21464;&#20998;&#20998;&#24067;&#21644;&#38024;&#23545;&#21069;&#21521;KL&#25955;&#24230;&#26399;&#26395;&#30340;&#20248;&#21270;&#30446;&#26631;&#12290;&#22312;&#25105;&#20204;&#23545;M2&#29699;&#29366;&#26143;&#22242;&#30340;SDSS&#22270;&#20687;&#30340;&#23454;&#39564;&#20013;&#65292;StarNet&#27604;&#20004;&#31181;&#31454;&#20105;&#26041;&#27861;Probabilistic Cataloging (PCAT)&#65288;&#19968;&#31181;&#20351;&#29992;MCMC&#36827;&#34892;&#25512;&#26029;&#30340;&#26041;&#27861;&#65289;&#21644;SDSS&#29992;&#20110;&#20998;&#31163;&#30340;&#36719;&#20214;&#31649;&#32447;DAOPHOT&#35201;&#20934;&#30830;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#25674;&#38144;&#25512;&#26029;&#26041;&#27861;&#36171;&#20104;&#20102;StarNet&#22312;&#29616;&#20195;&#22825;&#25991;&#35843;&#26597;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#25152;&#38656;&#30340;&#21487;&#25193;&#23637;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In images collected by astronomical surveys, stars and galaxies often overlap visually. Deblending is the task of distinguishing and characterizing individual light sources in survey images. We propose StarNet, a Bayesian method to deblend sources in astronomical images of crowded star fields. StarNet leverages recent advances in variational inference, including amortized variational distributions and an optimization objective targeting an expectation of the forward KL divergence. In our experiments with SDSS images of the M2 globular cluster, StarNet is substantially more accurate than two competing methods: Probabilistic Cataloging (PCAT), a method that uses MCMC for inference, and DAOPHOT, a software pipeline employed by SDSS for deblending. In addition, the amortized approach to inference gives StarNet the scaling characteristics necessary to perform Bayesian inference on modern astronomical surveys.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#24403;&#36755;&#20837;&#21442;&#25968;&#19981;&#30830;&#23450;&#25110;&#26368;&#21021;&#26410;&#30693;&#26102;&#65292;&#36890;&#36807;&#20840;&#24102;&#22238;&#39304;&#30340;&#32452;&#21512;&#32431;&#25506;&#32034;&#65288;CPE&#65289;&#26469;&#35299;&#20915;&#35813;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21322;&#36172;&#21338;&#26426;&#21453;&#39304;&#25110;&#20551;&#35774;&#27599;&#20010;&#36793;&#30340;&#32467;&#26524;&#22987;&#32456;&#21487;&#20197;&#35775;&#38382;&#65292;&#32780;&#36825;&#31687;&#35770;&#25991;&#32771;&#34385;&#20102;&#24378;&#21453;&#39304;&#20449;&#24687;&#19981;&#19968;&#23450;&#21487;&#29992;&#30340;&#23454;&#38469;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2012.15584</link><description>&lt;p&gt;
&#36890;&#36807;&#20840;&#24102;&#22238;&#39304;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65306;&#32452;&#21512;&#32431;&#25506;&#32034;&#21644;&#26356;&#22810;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Combinatorial Pure Exploration with Full-bandit Feedback and Beyond: Solving Combinatorial Optimization under Uncertainty with Limited Observation. (arXiv:2012.15584v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.15584
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#24403;&#36755;&#20837;&#21442;&#25968;&#19981;&#30830;&#23450;&#25110;&#26368;&#21021;&#26410;&#30693;&#26102;&#65292;&#36890;&#36807;&#20840;&#24102;&#22238;&#39304;&#30340;&#32452;&#21512;&#32431;&#25506;&#32034;&#65288;CPE&#65289;&#26469;&#35299;&#20915;&#35813;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21322;&#36172;&#21338;&#26426;&#21453;&#39304;&#25110;&#20551;&#35774;&#27599;&#20010;&#36793;&#30340;&#32467;&#26524;&#22987;&#32456;&#21487;&#20197;&#35775;&#38382;&#65292;&#32780;&#36825;&#31687;&#35770;&#25991;&#32771;&#34385;&#20102;&#24378;&#21453;&#39304;&#20449;&#24687;&#19981;&#19968;&#23450;&#21487;&#29992;&#30340;&#23454;&#38469;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#26159;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#36816;&#31609;&#23398;&#20013;&#24191;&#27867;&#30740;&#31350;&#30340;&#22522;&#30784;&#30740;&#31350;&#39046;&#22495;&#20043;&#19968;&#12290;&#22312;&#24320;&#21457;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#26102;&#65292;&#36890;&#24120;&#20551;&#35774;&#36755;&#20837;&#30340;&#21442;&#25968;&#65288;&#20363;&#22914;&#36793;&#26435;&#37325;&#65289;&#26159;&#20934;&#30830;&#24050;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#24456;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#20363;&#22914;&#25512;&#33616;&#31995;&#32479;&#12289;&#20247;&#21253;&#12289;&#36890;&#20449;&#32593;&#32476;&#21644;&#22312;&#32447;&#24191;&#21578;&#65292;&#36755;&#20837;&#21442;&#25968;&#24448;&#24448;&#26159;&#19981;&#30830;&#23450;&#30340;&#25110;&#32773;&#26368;&#21021;&#26410;&#30693;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#26679;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#65288;CPE&#65289;&#21450;&#20854;&#21464;&#31181;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26089;&#26399;&#20851;&#20110;CPE&#30340;&#30740;&#31350;&#20027;&#35201;&#26159;&#30740;&#31350;&#21322;&#36172;&#21338;&#26426;&#21453;&#39304;&#25110;&#32773;&#20551;&#35774;&#27599;&#20010;&#36793;&#30340;&#32467;&#26524;&#22312;&#27599;&#36718;&#20013;&#37117;&#26159;&#21487;&#20197;&#35775;&#38382;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#32422;&#26463;&#65288;&#20363;&#22914;&#39044;&#31639;&#38480;&#21046;&#25110;&#38544;&#31169;&#38382;&#39064;&#65289;&#65292;&#36817;&#26399;&#30340;&#24212;&#29992;&#20013;&#24182;&#19981;&#24635;&#26159;&#26377;&#36825;&#20040;&#24378;&#30340;&#21453;&#39304;&#20449;&#24687;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combinatorial optimization is one of the fundamental research fields that has been extensively studied in theoretical computer science and operations research. When developing an algorithm for combinatorial optimization, it is commonly assumed that parameters such as edge weights are exactly known as inputs. However, this assumption may not be fulfilled since input parameters are often uncertain or initially unknown in many applications such as recommender systems, crowdsourcing, communication networks, and online advertisement. To resolve such uncertainty, the problem of combinatorial pure exploration of multi-armed bandits (CPE) and its variants have recieved increasing attention. Earlier work on CPE has studied the semi-bandit feedback or assumed that the outcome from each individual edge is always accessible at all rounds. However, due to practical constraints such as a budget ceiling or privacy concern, such strong feedback is not always available in recent applications. In this a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#21516;&#26102;&#36827;&#34892;&#30340;EEG-fMRI&#20013;&#21435;&#38500;&#24515;&#21160;&#25581;&#31034;&#29616;&#35937;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#32593;&#32476;&#27169;&#22411;&#30340;&#23616;&#37096;&#34920;&#31034;&#33021;&#21147;&#65292;&#25913;&#21892;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#19968;&#20010;&#21487;&#38752;&#30340;&#29983;&#25104;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#21442;&#32771;&#20449;&#21495;&#25110;&#22797;&#26434;&#30340;&#30828;&#20214;&#35774;&#22791;&#12290;</title><link>http://arxiv.org/abs/2011.01710</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#21516;&#26102;&#36827;&#34892;&#30340;EEG-fMRI&#20013;&#21435;&#38500;&#24515;&#21160;&#25581;&#31034;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Ballistocardiogram artifact removal in simultaneous EEG-fMRI using generative adversarial network. (arXiv:2011.01710v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.01710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#21516;&#26102;&#36827;&#34892;&#30340;EEG-fMRI&#20013;&#21435;&#38500;&#24515;&#21160;&#25581;&#31034;&#29616;&#35937;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#32593;&#32476;&#27169;&#22411;&#30340;&#23616;&#37096;&#34920;&#31034;&#33021;&#21147;&#65292;&#25913;&#21892;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#19968;&#20010;&#21487;&#38752;&#30340;&#29983;&#25104;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#21442;&#32771;&#20449;&#21495;&#25110;&#22797;&#26434;&#30340;&#30828;&#20214;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21516;&#26102;&#36827;&#34892;&#30340;&#33041;&#30005;&#22270;-&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;EEG-fMRI&#65289;&#30340;&#39640;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#20248;&#21183;&#65292;&#35813;&#25216;&#26415;&#21560;&#24341;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#24182;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33041;&#31185;&#23398;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#33041;&#37096;&#30340;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#36807;&#31243;&#20013;&#65292;&#24515;&#21160;&#25581;&#31034;&#29616;&#35937;&#65288;BCG&#65289;&#20250;&#20005;&#37325;&#27745;&#26579;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#12290;&#20316;&#20026;&#19968;&#20010;&#38750;&#37197;&#23545;&#38382;&#39064;&#65292;BCG&#25581;&#31034;&#29616;&#35937;&#30340;&#21435;&#38500;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21450;&#30456;&#24212;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#20248;&#21270;&#27599;&#20010;&#27169;&#22359;&#30340;&#21442;&#25968;&#26469;&#25913;&#36827;&#32593;&#32476;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#24076;&#26395;&#25552;&#39640;&#32593;&#32476;&#27169;&#22411;&#30340;&#23616;&#37096;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#20854;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#19968;&#20010;&#21487;&#38752;&#30340;&#29992;&#20110;&#21435;&#38500;BCG&#25581;&#31034;&#29616;&#35937;&#30340;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#21442;&#32771;&#20449;&#21495;&#25110;&#22797;&#26434;&#30340;&#30828;&#20214;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to its advantages of high temporal and spatial resolution, the technology of simultaneous electroencephalogram-functional magnetic resonance imaging (EEG-fMRI) acquisition and analysis has attracted much attention, and has been widely used in various research fields of brain science. However, during the fMRI of the brain, ballistocardiogram (BCG) artifacts can seriously contaminate the EEG. As an unpaired problem, BCG artifact removal now remains a considerable challenge. Aiming to provide a solution, this paper proposed a novel modular generative adversarial network (GAN) and corresponding training strategy to improve the network performance by optimizing the parameters of each module. In this manner, we hope to improve the local representation ability of the network model, thereby improving its overall performance and obtaining a reliable generator for BCG artifact removal. Moreover, the proposed method does not rely on additional reference signal or complex hardware equipment. E
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#32852;&#21512;&#20998;&#20301;&#25968;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#27169;&#22411;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#38656;&#35201;&#36827;&#34892;&#29305;&#24449;&#30340;&#36873;&#25321;&#65292;&#24182;&#19988;&#20801;&#35768;&#36827;&#34892;&#21363;&#26102;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2010.01654</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#29305;&#24449;&#36873;&#25321;&#22312;&#32852;&#21512;&#20998;&#20301;&#25968;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Feature Selection in Joint Quantile Time Series Analysis. (arXiv:2010.01654v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.01654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#32852;&#21512;&#20998;&#20301;&#25968;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#27169;&#22411;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#38656;&#35201;&#36827;&#34892;&#29305;&#24449;&#30340;&#36873;&#25321;&#65292;&#24182;&#19988;&#20801;&#35768;&#36827;&#34892;&#21363;&#26102;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30456;&#20851;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#20301;&#25968;&#29305;&#24449;&#36873;&#25321;&#19968;&#30452;&#26159;&#19968;&#31181;&#26041;&#27861;&#19978;&#30340;&#25361;&#25112;&#21644;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#36125;&#21494;&#26031;&#38477;&#32500;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#32852;&#21512;&#20998;&#20301;&#25968;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#65292;&#35813;&#26041;&#27861;&#34987;&#31216;&#20026;&#20998;&#20301;&#25968;&#29305;&#24449;&#36873;&#25321;&#26102;&#38388;&#24207;&#21015;&#65288;QFSTS&#65289;&#27169;&#22411;&#12290;QFSTS&#27169;&#22411;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32467;&#26500;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#23545;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20135;&#29983;&#20102;&#21487;&#30452;&#25509;&#35299;&#37322;&#30340;&#21152;&#24615;&#36129;&#29486;&#12290;&#20854;&#28789;&#27963;&#24615;&#20307;&#29616;&#22312;&#29992;&#25143;&#21487;&#20197;&#20026;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#28155;&#21152;/&#20943;&#23569;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#19988;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#21487;&#20197;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#30340;&#29305;&#23450;&#20540;&#32452;&#25104;&#37096;&#20998;&#12290;&#29305;&#24449;&#36873;&#25321;&#26159;&#22312;&#20998;&#20301;&#25968;&#22238;&#24402;&#32452;&#20214;&#20013;&#36827;&#34892;&#30340;&#65292;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#37117;&#26377;&#33258;&#24049;&#30340;&#21516;&#26102;&#22806;&#37096;&#39044;&#27979;&#21464;&#37327;&#27744;&#65292;&#20801;&#35768;&#36827;&#34892;&#21363;&#26102;&#39044;&#27979;&#12290;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#23558;&#29305;&#24449;&#36873;&#25321;&#25193;&#23637;&#21040;&#20998;&#20301;&#25968;&#26102;&#38388;&#24207;&#21015;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantile feature selection over correlated multivariate time series data has always been a methodological challenge and is an open problem. In this paper, we propose a general Bayesian dimension reduction methodology for feature selection in high-dimensional joint quantile time series analysis, under the name of the quantile feature selection time series (QFSTS) model. The QFSTS model is a general structural time series model, where each component yields an additive contribution to the time series modeling with direct interpretations. Its flexibility is compound in the sense that users can add/deduct components for each time series and each time series can have its own specific valued components of different sizes. Feature selection is conducted in the quantile regression component, where each time series has its own pool of contemporaneous external predictors allowing nowcasting. Bayesian methodology in extending feature selection to the quantile time series research area is developed
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#38454;&#26465;&#20214;&#26799;&#24230;&#28369;&#21160;&#65288;SOCGS&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#35299;&#20915;&#32422;&#26463;&#20108;&#27425;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#26377;&#38480;&#27425;&#32447;&#24615;&#25910;&#25947;&#36845;&#20195;&#21518;&#20108;&#27425;&#25910;&#25947;&#20110;&#21407;&#22987;&#38388;&#38553;&#12290;</title><link>http://arxiv.org/abs/2002.08907</link><description>&lt;p&gt;
&#20108;&#38454;&#26465;&#20214;&#26799;&#24230;&#28369;&#21160;
&lt;/p&gt;
&lt;p&gt;
Second-order Conditional Gradient Sliding. (arXiv:2002.08907v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.08907
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#38454;&#26465;&#20214;&#26799;&#24230;&#28369;&#21160;&#65288;SOCGS&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#35299;&#20915;&#32422;&#26463;&#20108;&#27425;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#26377;&#38480;&#27425;&#32447;&#24615;&#25910;&#25947;&#36845;&#20195;&#21518;&#20108;&#27425;&#25910;&#25947;&#20110;&#21407;&#22987;&#38388;&#38553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38656;&#35201;&#39640;&#31934;&#24230;&#35299;&#20915;&#38382;&#39064;&#26102;&#65292;&#32422;&#26463;&#20108;&#38454;&#20984;&#20248;&#21270;&#31639;&#27861;&#26159;&#39318;&#36873;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#24615;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#26102;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#32422;&#26463;&#20108;&#27425;&#23376;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;\emph{&#20108;&#38454;&#26465;&#20214;&#26799;&#24230;&#28369;&#21160;}&#65288;SOCGS&#65289;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#26080;&#25237;&#24433;&#31639;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#32422;&#26463;&#20108;&#27425;&#23376;&#38382;&#39064;&#12290;&#24403;&#21487;&#34892;&#22495;&#26159;&#19968;&#20010;&#22810;&#38754;&#20307;&#26102;&#65292;&#35813;&#31639;&#27861;&#22312;&#26377;&#38480;&#27425;&#32447;&#24615;&#25910;&#25947;&#36845;&#20195;&#21518;&#20108;&#27425;&#25910;&#25947;&#20110;&#21407;&#22987;&#38388;&#38553;&#12290;&#36827;&#20837;&#20108;&#27425;&#25910;&#25947;&#38454;&#27573;&#21518;&#65292;SOCGS&#31639;&#27861;&#38656;&#36890;&#36807;$\mathcal{O}(\log(\log 1/\varepsilon))$&#27425;&#19968;&#38454;&#21644;Hessian&#27491;&#20132;&#35843;&#29992;&#20197;&#21450;$\mathcal{O}(\log (1/\varepsilon) \log(\log1/\varepsilon))$&#27425;&#32447;&#24615;&#26368;&#23567;&#21270;&#27491;&#20132;&#35843;&#29992;&#26469;&#23454;&#29616;$\varepsilon$-&#26368;&#20248;&#35299;&#12290;&#24403;&#21487;&#34892;&#22495;&#21482;&#33021;&#36890;&#36807;&#32447;&#24615;&#20248;&#21270;&#27491;&#20132;&#35843;&#29992;&#39640;&#25928;&#35775;&#38382;&#26102;&#65292;&#27492;&#31639;&#27861;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constrained second-order convex optimization algorithms are the method of choice when a high accuracy solution to a problem is needed, due to their local quadratic convergence. These algorithms require the solution of a constrained quadratic subproblem at every iteration. We present the \emph{Second-Order Conditional Gradient Sliding} (SOCGS) algorithm, which uses a projection-free algorithm to solve the constrained quadratic subproblems inexactly. When the feasible region is a polytope the algorithm converges quadratically in primal gap after a finite number of linearly convergent iterations. Once in the quadratic regime the SOCGS algorithm requires $\mathcal{O}(\log(\log 1/\varepsilon))$ first-order and Hessian oracle calls and $\mathcal{O}(\log (1/\varepsilon) \log(\log1/\varepsilon))$ linear minimization oracle calls to achieve an $\varepsilon$-optimal solution. This algorithm is useful when the feasible region can only be accessed efficiently through a linear optimization oracle, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;&#39038;&#23458;&#30340;&#21040;&#36798;&#26159;&#36830;&#32493;&#30340;&#65292;&#26377;&#38480;&#22343;&#20540;&#36830;&#32493;&#20998;&#24067;&#30340;&#25928;&#29992;&#20540;&#21644;&#26377;&#30028;&#31163;&#25955;&#25110;&#36830;&#32493;&#20998;&#24067;&#30340;&#24211;&#23384;&#37327;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#21021;&#22987;&#24211;&#23384;&#37327;&#19982;&#39038;&#23458;&#25968;&#25104;&#32447;&#24615;&#27604;&#20363;&#65292;&#38543;&#30528;&#39038;&#23458;&#25968;&#30340;&#22686;&#21152;&#65292;&#39044;&#26399;&#30340;&#36951;&#25022;&#23558;&#20197;&#23545;&#25968;&#30340;&#36895;&#24230;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/1912.08917</link><description>&lt;p&gt;
&#22312;&#22810;&#31192;&#20070;&#38382;&#39064;&#21644;&#36830;&#32493;&#20272;&#20540;&#30340;&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#23545;&#25968;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Logarithmic Regret in Multisecretary and Online Linear Programming Problems with Continuous Valuations. (arXiv:1912.08917v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.08917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;&#39038;&#23458;&#30340;&#21040;&#36798;&#26159;&#36830;&#32493;&#30340;&#65292;&#26377;&#38480;&#22343;&#20540;&#36830;&#32493;&#20998;&#24067;&#30340;&#25928;&#29992;&#20540;&#21644;&#26377;&#30028;&#31163;&#25955;&#25110;&#36830;&#32493;&#20998;&#24067;&#30340;&#24211;&#23384;&#37327;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#21021;&#22987;&#24211;&#23384;&#37327;&#19982;&#39038;&#23458;&#25968;&#25104;&#32447;&#24615;&#27604;&#20363;&#65292;&#38543;&#30528;&#39038;&#23458;&#25968;&#30340;&#22686;&#21152;&#65292;&#39044;&#26399;&#30340;&#36951;&#25022;&#23558;&#20197;&#23545;&#25968;&#30340;&#36895;&#24230;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#30740;&#31350;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#25910;&#30410;&#31649;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;$n$&#20010;&#39038;&#23458;&#22312;$n$&#20010;&#26102;&#26399;&#20869;&#39034;&#24207;&#21040;&#36798;&#65292;&#24744;&#24517;&#39035;&#21160;&#24577;&#20915;&#23450;&#35201;&#28385;&#36275;&#21738;&#20010;&#39038;&#23458;&#12290;&#28385;&#36275;&#31532;$t$&#20010;&#26102;&#26399;&#30340;&#39038;&#23458;&#21487;&#20197;&#33719;&#24471;&#25928;&#29992;$u_{t}\in \mathbb{R}_{+}$&#65292;&#24182;&#20943;&#23569;&#24744;&#30340;&#24211;&#23384;&#37327;$A_{t}\in \mathbb{R}_{+}^{M}$&#12290;&#39038;&#23458;&#21521;&#37327;$(u_{t}, A_{t}')'$&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#20854;&#20013;$u_{t}$&#26159;&#20174;&#26377;&#38480;&#22343;&#20540;&#36830;&#32493;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#65292;$A_{t}$&#26159;&#20174;&#26377;&#30028;&#31163;&#25955;&#25110;&#36830;&#32493;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#12290;&#25105;&#30740;&#31350;&#20102;&#35813;&#31995;&#32479;&#30340;&#36951;&#25022;&#65292;&#21363;&#22914;&#26524;&#24744;&#19981;&#38656;&#35201;&#21363;&#26102;&#20915;&#31574;&#65292;&#24744;&#21487;&#20197;&#33719;&#24471;&#30340;&#39069;&#22806;&#25928;&#29992;&#12290;&#25105;&#23637;&#31034;&#20102;&#22914;&#26524;&#24744;&#30340;&#21021;&#22987;&#24211;&#23384;&#36164;&#20135;&#19982;$n$&#25104;&#32447;&#24615;&#27604;&#20363;&#65292;&#37027;&#20040;&#24403;$n \rightarrow \infty$&#26102;&#65292;&#24744;&#30340;&#39044;&#26399;&#36951;&#25022;&#26159;$ \Theta(\log(n)) $&#12290;&#25105;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;$ \Theta(\log(n)) $&#30340;&#36951;&#25022;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#23558;&#36825;&#20010;&#32467;&#26524;&#25193;&#23637;&#21040;Arlotto&#21644;Gurich&#65288;2019&#65289;&#30340;&#22810;&#31192;&#20070;&#38382;&#39064;&#65292;&#20854;&#20013;&#31192;&#20070;&#20272;&#20540;&#26159;&#22343;&#21248;&#20998;&#24067;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
I study a general revenue management problem in which $ n $ customers arrive sequentially over $ n $ periods, and you must dynamically decide which to satisfy. Satisfying the period-$ t $ customer yields utility $ u_{t} \in \mathbb{R}_{+} $ and decreases your inventory holdings by $ A_{t} \in \mathbb{R}_{+}^{M} $. The customer vectors, $ (u_{t}, A_{t}')' $, are i.i.d., with $ u_{t} $ drawn from a finite-mean continuous distribution and $ A_{t} $ drawn from a bounded discrete or continuous distribution. I study this system's regret, which is the additional utility you could get if you didn't have to make decisions on the fly. I show that if your initial inventory endowment scales linearly with $ n $ then your expected regret is $ \Theta(\log(n)) $ as $ n \rightarrow \infty $. I provide a simple policy that achieves this $ \Theta(\log(n)) $ regret rate. Finally, I extend this result to Arlotto and Gurich's (2019) multisecretary problem with uniformly distributed secretary valuations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21322;&#30417;&#30563;&#30690;&#37327;&#20540;&#23398;&#20064;&#30340;&#25913;&#36827;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;Rademacher&#22797;&#26434;&#24230;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#24471;&#20986;&#20102;&#26356;&#31934;&#30830;&#30340;&#36229;&#20986;&#39118;&#38505;&#30028;&#38480;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/1909.04883</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#30690;&#37327;&#20540;&#23398;&#20064;&#65306;&#25913;&#36827;&#30340;&#30028;&#38480;&#19982;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Vector-valued Learning: Improved Bounds and Algorithms. (arXiv:1909.04883v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1909.04883
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21322;&#30417;&#30563;&#30690;&#37327;&#20540;&#23398;&#20064;&#30340;&#25913;&#36827;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;Rademacher&#22797;&#26434;&#24230;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#24471;&#20986;&#20102;&#26356;&#31934;&#30830;&#30340;&#36229;&#20986;&#39118;&#38505;&#30028;&#38480;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30690;&#37327;&#20540;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#20854;&#36755;&#20986;&#31354;&#38388;&#20855;&#26377;&#30690;&#37327;&#20540;&#32467;&#26500;&#65292;&#28085;&#30422;&#20102;&#35768;&#22810;&#37325;&#35201;&#39046;&#22495;&#65292;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;&#23616;&#37096;Rademacher&#22797;&#26434;&#24230;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#20174;&#26680;&#20989;&#25968;&#21644;&#32447;&#24615;&#26041;&#27861;&#30340;&#35282;&#24230;&#20026;&#19968;&#33324;&#30690;&#37327;&#20540;&#23398;&#20064;&#23548;&#20986;&#20102;&#26032;&#30340;&#21322;&#30417;&#30563;&#36229;&#20986;&#39118;&#38505;&#30028;&#38480;&#12290;&#36825;&#20123;&#30028;&#38480;&#27604;&#29616;&#26377;&#30340;&#30028;&#38480;&#26356;&#31934;&#30830;&#65292;&#25910;&#25947;&#36895;&#24230;&#20174;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#30340;&#24179;&#26041;&#26681;&#25913;&#36827;&#20026;&#24635;&#26679;&#26412;&#22823;&#23567;&#30340;&#24179;&#26041;&#26681;&#25110;&#30452;&#25509;&#20381;&#36182;&#20110;&#26631;&#35760;&#26679;&#26412;&#22823;&#23567;&#12290;&#22312;&#29702;&#35770;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21322;&#30417;&#30563;&#31639;&#27861;&#26469;&#39640;&#25928;&#23398;&#20064;&#30690;&#37327;&#20540;&#20989;&#25968;&#65292;&#32467;&#21512;&#20102;&#23616;&#37096;Rademacher&#22797;&#26434;&#24230;&#21644;&#25289;&#26222;&#25289;&#26031;&#27491;&#21017;&#21270;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19982;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#30456;&#21563;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector-valued learning, where the output space admits a vector-valued structure, is an important problem that covers a broad family of important domains, e.g. multi-task learning and transfer learning. Using local Rademacher complexity and unlabeled data, we derive novel semi-supervised excess risk bounds for general vector-valued learning from both kernel perspective and linear perspective. The derived bounds are much sharper than existing ones and the convergence rates are improved from the square root of labeled sample size to the square root of total sample size or directly dependent on labeled sample size. Motivated by our theoretical analysis, we propose a general semi-supervised algorithm for efficiently learning vector-valued functions, incorporating both local Rademacher complexity and Laplacian regularization. Extensive experimental results illustrate the proposed algorithm significantly outperforms the compared methods, which coincides with our theoretical findings.
&lt;/p&gt;</description></item></channel></rss>