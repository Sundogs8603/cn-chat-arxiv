<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#20135;&#29983;&#30340;&#25991;&#26412;&#23384;&#22312;&#20005;&#37325;&#38480;&#21046;&#12290;</title><link>https://rss.arxiv.org/abs/2401.09407</link><description>&lt;p&gt;
&#35299;&#35835;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;: &#36890;&#36807;&#22823;&#35268;&#27169;&#35821;&#35328;&#35821;&#20041;&#30340;&#24191;&#20041;&#31574;&#30053;&#26469;&#26816;&#27979;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.09407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#20135;&#29983;&#30340;&#25991;&#26412;&#23384;&#22312;&#20005;&#37325;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#23545;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#24037;&#20855;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#12290;&#26377;&#25928;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;: &#39318;&#20808;&#65292;&#20182;&#20204;&#22312;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#26102;&#38754;&#20020;&#30528;&#26497;&#22823;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#22330;&#26223;&#20013;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26159;&#30001;&#21508;&#31181;&#29983;&#25104;&#22120;&#20135;&#29983;&#30340;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;GPT-4&#21644;Dolly&#65292;&#24182;&#28085;&#30422;&#21508;&#31181;&#39046;&#22495;&#65292;&#20174;&#23398;&#26415;&#25163;&#31295;&#21040;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#35270;&#20026;&#20005;&#26684;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#22810;&#26679;&#24615;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#19981;&#21516;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#20135;&#29983;&#30340;&#25991;&#26412;&#26102;&#21463;&#21040;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent proliferation of Large Language Models (LLMs), there has been an increasing demand for tools to detect machine-generated text. The effective detection of machine-generated text face two pertinent problems: First, they are severely limited in generalizing against real-world scenarios, where machine-generated text is produced by a variety of generators, including but not limited to GPT-4 and Dolly, and spans diverse domains, ranging from academic manuscripts to social media posts. Second, existing detection methodologies treat texts produced by LLMs through a restrictive binary classification lens, neglecting the nuanced diversity of artifacts generated by different LLMs. In this work, we undertake a systematic study on the detection of machine-generated text in real-world scenarios. We first study the effectiveness of state-of-the-art approaches and find that they are severely limited against text produced by diverse generators and domains in the real world. Furthermore,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#27979;&#37327;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26631;&#20934;ALOHa&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#27979;&#37327;&#24187;&#35273;&#23545;&#35937;&#65292;&#24182;&#25104;&#21151;&#35782;&#21035;&#27604;&#29616;&#26377;&#25351;&#26631;CHAIR&#26356;&#22810;&#30340;&#24187;&#35273;&#23545;&#35937;&#12290;</title><link>https://arxiv.org/abs/2404.02904</link><description>&lt;p&gt;
ALOHa&#65306;&#27979;&#37327;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26032;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
ALOHa: A New Measure for Hallucination in Captioning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02904
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#27979;&#37327;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26631;&#20934;ALOHa&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#27979;&#37327;&#24187;&#35273;&#23545;&#35937;&#65292;&#24182;&#25104;&#21151;&#35782;&#21035;&#27604;&#29616;&#26377;&#25351;&#26631;CHAIR&#26356;&#22810;&#30340;&#24187;&#35273;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35270;&#35273;&#25551;&#36848;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#20250;&#20135;&#29983;&#21253;&#21547;&#38169;&#35823;&#30340;&#23383;&#24149;&#65292;&#27604;&#22914;&#22312;&#22330;&#26223;&#20013;&#23384;&#22312;&#24187;&#35273;&#23545;&#35937;&#12290;&#29616;&#26377;&#30340;&#20027;&#35201;&#24187;&#35273;&#23545;&#35937;&#24230;&#37327;&#26631;&#20934;CHAIR&#65292;&#20165;&#38480;&#20110;&#19968;&#32452;&#22266;&#23450;&#30340;MS COCO&#23545;&#35937;&#21644;&#21516;&#20041;&#35789;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29616;&#20195;&#21270;&#30340;&#24320;&#25918;&#35789;&#27719;&#24230;&#37327;&#26631;&#20934;ALOHa&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#34913;&#37327;&#23545;&#35937;&#24187;&#35273;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#20174;&#20505;&#36873;&#23383;&#24149;&#20013;&#25552;&#21462;&#21487;&#36830;&#25509;&#30340;&#23545;&#35937;&#65292;&#34913;&#37327;&#23427;&#20204;&#19982;&#23383;&#24149;&#21644;&#23545;&#35937;&#26816;&#27979;&#20013;&#21442;&#32771;&#23545;&#35937;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#24182;&#20351;&#29992;&#21256;&#29273;&#21033;&#21305;&#37197;&#29983;&#25104;&#26368;&#32456;&#30340;&#24187;&#35273;&#24471;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ALOHa&#22312;HAT&#19978;&#27604;CHAIR&#22312;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#24187;&#35273;&#26631;&#35760;&#30340;MS COCO&#23383;&#24149;&#30340;&#37329;&#26631;&#20934;&#23376;&#38598;&#19978;&#27491;&#30830;&#35782;&#21035;&#20102;&#26356;&#22810;&#30340;&#24187;&#35273;&#23545;&#35937;&#65288;&#22810;&#20986;13.6%&#65289;&#65292;&#22312;nocaps&#19978;&#65288;&#20854;&#20013;&#23545;&#35937;&#36229;&#20986;&#20102;MS COCO&#31867;&#21035;&#65289;&#35782;&#21035;&#20102;&#26356;&#22810;&#30340;&#24187;&#35273;&#23545;&#35937;&#65288;&#22810;&#33267;30.8%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02904v1 Announce Type: cross  Abstract: Despite recent advances in multimodal pre-training for visual description, state-of-the-art models still produce captions containing errors, such as hallucinating objects not present in a scene. The existing prominent metric for object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations. Specifically, we use an LLM to extract groundable objects from a candidate caption, measure their semantic similarity to reference objects from captions and object detections, and use Hungarian matching to produce a final hallucination score. We show that ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where objects extend beyond MS COCO categories.
&lt;/p&gt;</description></item><item><title>DeiT-LT&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26377;&#25928;&#30340;&#33976;&#39311;&#26041;&#24335;&#65292;&#23558;CNN&#33976;&#39311;&#21040;ViT&#20013;&#65292;&#20197;&#24212;&#23545;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;ViT&#26102;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2404.02900</link><description>&lt;p&gt;
DeiT-LT&#33976;&#39311;&#37325;&#36820;&#65292;&#29992;&#20110;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#30340;Vision Transformer&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02900
&lt;/p&gt;
&lt;p&gt;
DeiT-LT&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26377;&#25928;&#30340;&#33976;&#39311;&#26041;&#24335;&#65292;&#23558;CNN&#33976;&#39311;&#21040;ViT&#20013;&#65292;&#20197;&#24212;&#23545;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;ViT&#26102;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformer&#65288;ViT&#65289;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#31361;&#20986;&#30340;&#26550;&#26500;&#12290;&#22312; ViT &#20013;&#65292;&#25105;&#20204;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#25104;&#34917;&#19969;&#20196;&#29260;&#65292;&#24182;&#36890;&#36807;&#19968;&#22534;&#33258;&#25105;&#27880;&#24847;&#22359;&#36827;&#34892;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#19981;&#21516;&#65292;ViT &#30340;&#31616;&#21333;&#26550;&#26500;&#27809;&#26377;&#20449;&#24687;&#24615;&#24402;&#32435;&#20559;&#24046;&#65288;&#20363;&#22914;&#23616;&#37096;&#24615;&#31561;&#65289;&#12290;&#30001;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;ViT &#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#26041;&#27861;&#65288;DeiT&#65289;&#26469;&#26377;&#25928;&#22320;&#35757;&#32451;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;ViT&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#24456;&#23569;&#35752;&#35770;&#20351;&#29992;ViT&#26469;&#22788;&#29702;&#38271;&#23614;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;DeiT-LT&#26469;&#35299;&#20915;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#30340;ViT&#30340;&#38382;&#39064;&#12290;&#22312; DeiT-LT &#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36229;&#20986;&#20998;&#24067;&#22270;&#20687;&#21644;&#37325;&#26032;&#21152;&#26435;&#33976;&#39311;&#25439;&#22833;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33976;&#39311;&#26041;&#24335;&#65292;&#36890;&#36807;&#33976;&#39311; DIST &#20196;&#29260;&#20174;CNN&#36827;&#34892;&#33976;&#39311;&#65292;&#20197;&#22686;&#24378;&#23545;&#23614;&#37096;&#31867;&#21035;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02900v1 Announce Type: cross  Abstract: Vision Transformer (ViT) has emerged as a prominent architecture for various computer vision tasks. In ViT, we divide the input image into patch tokens and process them through a stack of self attention blocks. However, unlike Convolutional Neural Networks (CNN), ViTs simple architecture has no informative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a large amount of data for pre-training. Various data efficient approaches (DeiT) have been proposed to train ViT on balanced datasets effectively. However, limited literature discusses the use of ViT for datasets with long-tailed imbalances. In this work, we introduce DeiT-LT to tackle the problem of training ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an efficient and effective way of distillation from CNN via distillation DIST token by using out-of-distribution images and re-weighting the distillation loss to enhance focus on tail classes. Thi
&lt;/p&gt;</description></item><item><title>&#35780;&#35770;&#20102;&#21478;&#19968;&#31687;&#20851;&#20110;&#20174;&#24494;&#20998;&#26041;&#31243;&#20013;&#23398;&#20064;&#23432;&#24658;&#23450;&#24459;&#30340;&#25991;&#31456;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#25512;&#23548;&#38169;&#35823;</title><link>https://arxiv.org/abs/2404.02896</link><description>&lt;p&gt;
&#23545;&#8220;&#20174;&#24494;&#20998;&#26041;&#31243;&#20013;&#23398;&#20064;&#23432;&#24658;&#23450;&#24459;&#8221;&#19968;&#25991;&#30340;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Comment on "Machine learning conservation laws from differential equations"
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02896
&lt;/p&gt;
&lt;p&gt;
&#35780;&#35770;&#20102;&#21478;&#19968;&#31687;&#20851;&#20110;&#20174;&#24494;&#20998;&#26041;&#31243;&#20013;&#23398;&#20064;&#23432;&#24658;&#23450;&#24459;&#30340;&#25991;&#31456;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#25512;&#23548;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27492;&#35780;&#35770;&#20013;&#65292;&#20316;&#32773;&#22238;&#39038;&#20102;&#21016;, &#39532;&#24503;&#21704;&#19975;&#21644;&#27888;&#26684;&#39532;&#20811;&#25552;&#20986;&#30340;&#19982;&#20316;&#32773;&#25552;&#20986;&#30340;&#19968;&#32500;&#38459;&#23612;&#35856;&#25391;&#23376;&#30340;&#23432;&#24658;&#37327;&#30456;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#25351;&#20986;&#20182;&#20204;&#25512;&#23548;&#20013;&#23384;&#22312;&#20845;&#20010;&#20005;&#37325;&#38169;&#35823;&#65292;&#23548;&#33268;&#20182;&#20204;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#22343;&#19981;&#27491;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02896v1 Announce Type: new  Abstract: In lieu of abstract, first paragraph reads: Six months after the author derived a constant of motion for a 1D damped harmonic oscillator [1], a similar result appeared by Liu, Madhavan, and Tegmark [2, 3], without citing the author. However, their derivation contained six serious errors, causing both their method and result to be incorrect. In this Comment, those errors are reviewed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#31639;&#23376;&#23398;&#20064;&#25361;&#25112;&#30340;&#26032;&#22411;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;&#21333;&#20010;&#31070;&#32463;&#31639;&#23376;&#22788;&#29702;&#22810;&#31639;&#23376;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#24179;&#22343;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.02892</link><description>&lt;p&gt;
MODNO: &#20855;&#26377;&#20998;&#24067;&#24335;&#31070;&#32463;&#31639;&#23376;&#30340;&#22810;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MODNO: Multi Operator Learning With Distributed Neural Operators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#31639;&#23376;&#23398;&#20064;&#25361;&#25112;&#30340;&#26032;&#22411;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;&#21333;&#20010;&#31070;&#32463;&#31639;&#23376;&#22788;&#29702;&#22810;&#31639;&#23376;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#24179;&#22343;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#30740;&#31350;&#28041;&#21450;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#31639;&#23376;&#12290;&#20256;&#32479;&#19978;&#65292;&#37325;&#28857;&#25918;&#22312;&#21333;&#31639;&#23376;&#23398;&#20064;&#65288;SOL&#65289;&#19978;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#36805;&#36895;&#23558;&#20854;&#25193;&#23637;&#21040;&#21253;&#21547;&#20351;&#29992;&#20855;&#26377;&#25968;&#30334;&#19975;&#25110;&#25968;&#21313;&#20159;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#36924;&#36817;&#22810;&#31639;&#23376;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#22810;&#31639;&#23376;&#23398;&#20064;&#65288;MOL&#65289;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#21333;&#20010;&#31070;&#32463;&#31639;&#23376;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#31639;&#23376;&#23398;&#20064;&#25361;&#25112;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#24179;&#22343;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#20284;Chen-Chen&#22411;&#31070;&#32463;&#31639;&#23376;&#65292;&#22914;&#28145;&#31639;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;DON&#65289;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#29420;&#31435;&#23398;&#20064;&#27599;&#20010;&#31639;&#23376;&#30340;&#36755;&#20986;&#22522;&#20989;&#25968;&#65292;&#20351;&#29992;&#20854;&#19987;&#29992;&#25968;&#25454;&#65292;&#21516;&#26102;&#38598;&#20013;&#23398;&#20064;&#36755;&#20837;fu&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02892v1 Announce Type: new  Abstract: The study of operator learning involves the utilization of neural networks to approximate operators. Traditionally, the focus has been on single-operator learning (SOL). However, recent advances have rapidly expanded this to include the approximation of multiple operators using foundation models equipped with millions or billions of trainable parameters, leading to the research of multi-operator learning (MOL). In this paper, we present a novel distributed training approach aimed at enabling a single neural operator with significantly fewer parameters to effectively tackle multi-operator learning challenges, all without incurring additional average costs. Our method is applicable to various Chen-Chen-type neural operators, such as Deep Operator Neural Networks (DON). The core idea is to independently learn the output basis functions for each operator using its dedicated data, while simultaneously centralizing the learning of the input fu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38024;&#23545;&#25193;&#25955;&#22411;T2I&#27169;&#22411;&#36827;&#34892;&#28040;&#34701;&#23454;&#39564;&#65292;&#21457;&#29616;&#22686;&#21152;transformer&#22359;&#23545;&#20110;&#25913;&#21892;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#27604;&#22686;&#21152;&#36890;&#36947;&#25968;&#26356;&#20855;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.02883</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Scalability of Diffusion-based Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38024;&#23545;&#25193;&#25955;&#22411;T2I&#27169;&#22411;&#36827;&#34892;&#28040;&#34701;&#23454;&#39564;&#65292;&#21457;&#29616;&#22686;&#21152;transformer&#22359;&#23545;&#20110;&#25913;&#21892;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#27604;&#22686;&#21152;&#36890;&#36947;&#25968;&#26356;&#20855;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#23545;&#20110;LLMs&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#25193;&#23637;&#27861;&#21017;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#25193;&#23637;&#27169;&#22411;&#20197;&#22312;&#38477;&#20302;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#24615;&#33021;&#20063;&#19981;&#22826;&#28165;&#26970;&#12290;&#19981;&#21516;&#30340;&#35757;&#32451;&#35774;&#32622;&#21644;&#26114;&#36149;&#30340;&#35757;&#32451;&#25104;&#26412;&#20351;&#24471;&#36827;&#34892;&#20844;&#24179;&#30340;&#27169;&#22411;&#27604;&#36739;&#21464;&#24471;&#26497;&#20026;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#21435;&#22122;&#39592;&#24178;&#21644;&#35757;&#32451;&#38598;&#30340;&#22823;&#37327;&#32780;&#20005;&#26684;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#23545;&#25193;&#25955;&#22411;T2I&#27169;&#22411;&#30340;&#25193;&#23637;&#29305;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21253;&#25324;&#22312;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;600M&#22270;&#20687;&#30340;&#33539;&#22260;&#20869;&#35757;&#32451;&#21442;&#25968;&#20174;0.4B&#21040;4B&#30340;&#32553;&#25918;UNet&#21644;Transformer&#21464;&#20307;&#12290;&#23545;&#20110;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#25105;&#20204;&#21457;&#29616;&#36328;&#20851;&#27880;&#30340;&#20301;&#32622;&#21644;&#25968;&#37327;&#21306;&#20998;&#20102;&#29616;&#26377;UNet&#35774;&#35745;&#30340;&#24615;&#33021;&#12290;&#22686;&#21152;transformer&#22359;&#23545;&#20110;&#25552;&#39640;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#27604;&#22686;&#21152;&#36890;&#36947;&#25968;&#26356;&#20855;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02883v1 Announce Type: cross  Abstract: Scaling up model and data size has been quite successful for the evolution of LLMs. However, the scaling law for the diffusion based text-to-image (T2I) models is not fully explored. It is also unclear how to efficiently scale the model for better performance at reduced cost. The different training settings and expensive training cost make a fair model comparison extremely difficult. In this work, we empirically study the scaling properties of diffusion based T2I models by performing extensive and rigours ablations on scaling both denoising backbones and training set, including training scaled UNet and Transformer variants ranging from 0.4B to 4B parameters on datasets upto 600M images. For model scaling, we find the location and amount of cross attention distinguishes the performance of existing UNet designs. And increasing the transformer blocks is more parameter-efficient for improving text-image alignment than increasing channel nu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#65288;LASP&#65289;&#30340;&#39640;&#25928;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#38024;&#23545;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#26426;&#21046;&#21644;&#25191;&#34892;&#20869;&#26680;&#34701;&#21512;&#26469;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#23454;&#29616;&#30828;&#20214;&#21451;&#22909;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02882</link><description>&lt;p&gt;
&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#21270;
&lt;/p&gt;
&lt;p&gt;
Linear Attention Sequence Parallelism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#65288;LASP&#65289;&#30340;&#39640;&#25928;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#38024;&#23545;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#26426;&#21046;&#21644;&#25191;&#34892;&#20869;&#26680;&#34701;&#21512;&#26469;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#23454;&#29616;&#30828;&#20214;&#21451;&#22909;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#24182;&#34892;&#65288;SP&#65289;&#20316;&#20026;&#19968;&#31181;&#22788;&#29702;&#36229;&#20986;&#21333;&#20010;GPU&#20869;&#23384;&#38480;&#21046;&#30340;&#38271;&#24207;&#21015;&#30340;&#27969;&#34892;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SP&#26041;&#27861;&#24182;&#26410;&#21033;&#29992;&#32447;&#24615;&#27880;&#24847;&#21147;&#29305;&#24615;&#65292;&#23548;&#33268;&#22312;&#22522;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#24182;&#34892;&#25928;&#29575;&#21644;&#21487;&#29992;&#24615;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#65288;LASP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#22522;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#39640;&#25928;SP&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#26426;&#21046;&#65292;&#20197;&#21033;&#29992;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#21491;&#20056;&#20869;&#26680;&#25216;&#24039;&#65292;&#20174;&#32780;&#26174;&#30528;&#38477;&#20302;SP&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25191;&#34892;&#20869;&#26680;&#34701;&#21512;&#21644;&#20013;&#38388;&#29366;&#24577;&#32531;&#23384;&#26469;&#22686;&#24378;LASP&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#20351;LASP&#22312;GPU&#38598;&#32676;&#19978;&#30340;&#30828;&#20214;&#21451;&#22909;&#24615;&#24471;&#21040;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31934;&#24515;&#30830;&#20445;&#24207;&#21015;&#32423;LASP&#19982;&#25152;&#26377;&#31867;&#22411;&#30340;&#25209;&#32423;&#25968;&#25454;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02882v1 Announce Type: cross  Abstract: Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data par
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#31867;&#37327;&#23376; Hamilton Monte Carlo &#26041;&#27861;&#21040;&#19981;&#31561;&#24335;&#21644;&#21333;&#35843;&#24615;&#32422;&#26463;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#65292;&#22312;&#27010;&#29575;&#24847;&#20041;&#19978;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#20102;&#26041;&#24046;</title><link>https://arxiv.org/abs/2404.02873</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#19982;&#36719;&#19981;&#31561;&#24335;&#21644;&#21333;&#35843;&#24615;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Regression with Soft Inequality and Monotonicity Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02873
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#31867;&#37327;&#23376; Hamilton Monte Carlo &#26041;&#27861;&#21040;&#19981;&#31561;&#24335;&#21644;&#21333;&#35843;&#24615;&#32422;&#26463;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#65292;&#22312;&#27010;&#29575;&#24847;&#20041;&#19978;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#20102;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gaussian process&#65288;GP&#65289;&#22238;&#24402;&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#12289;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#29992;&#20110;&#36924;&#36817;&#22797;&#26434;&#27169;&#22411;&#12290;&#26631;&#20934;&#30340;GP&#22238;&#24402;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#26080;&#30028;&#65292;&#23548;&#33268;&#26576;&#20123;&#28857;&#37319;&#29992;&#19981;&#21487;&#34892;&#30340;&#20540;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;GP&#26041;&#27861;&#65292;&#20197;&#27010;&#29575;&#26041;&#24335;&#24378;&#21046;&#25191;&#34892;&#29289;&#29702;&#32422;&#26463;&#12290;&#35813;GP&#27169;&#22411;&#36890;&#36807;&#31867;&#37327;&#23376;&#21551;&#21457;&#30340; Hamilton Monte Carlo&#65288;QHMC&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;QHMC&#26159;&#20174;&#21508;&#31181;&#20998;&#24067;&#20013;&#39640;&#25928;&#25277;&#26679;&#30340;&#26041;&#27861;&#12290;&#19982;&#26631;&#20934;&#30340; Hamilton Monte Carlo &#31639;&#27861;&#19981;&#21516;&#65292;&#20854;&#20013;&#31890;&#23376;&#20855;&#26377;&#22266;&#23450;&#36136;&#37327;&#65292;QHMC&#20801;&#35768;&#31890;&#23376;&#20855;&#26377;&#38543;&#26426;&#36136;&#37327;&#30697;&#38453;&#24182;&#24102;&#26377;&#27010;&#29575;&#20998;&#24067;&#12290;&#23558; QHMC &#26041;&#27861;&#24341;&#20837;&#27010;&#29575;&#24847;&#20041;&#19978;&#30340;&#19981;&#31561;&#24335;&#21644;&#21333;&#35843;&#24615;&#32422;&#26463;&#30340; GP &#22238;&#24402;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#32467;&#26524; GP &#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#20102;&#26041;&#24046;&#12290;&#26681;&#25454;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02873v1 Announce Type: cross  Abstract: Gaussian process (GP) regression is a non-parametric, Bayesian framework to approximate complex models. Standard GP regression can lead to an unbounded model in which some points can take infeasible values. We introduce a new GP method that enforces the physical constraints in a probabilistic manner. This GP model is trained by the quantum-inspired Hamiltonian Monte Carlo (QHMC). QHMC is an efficient way to sample from a broad class of distributions. Unlike the standard Hamiltonian Monte Carlo algorithm in which a particle has a fixed mass, QHMC allows a particle to have a random mass matrix with a probability distribution. Introducing the QHMC method to the inequality and monotonicity constrained GP regression in the probabilistic sense, our approach improves the accuracy and reduces the variance in the resulting GP model. According to our experiments on several datasets, the proposed approach serves as an efficient method as it accel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#30340;&#21152;&#36895;&#24230;&#35745;&#25429;&#33719;&#19981;&#21516;&#26085;&#24120;&#27963;&#21160;&#30340;&#25968;&#25454;&#65292;&#25552;&#21462;&#29305;&#24449;&#24182;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#23454;&#26102;&#27963;&#21160;&#35782;&#21035;&#21644;&#21345;&#36335;&#37324;&#28040;&#32791;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2404.02869</link><description>&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#36827;&#34892;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition using Smartphones
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#30340;&#21152;&#36895;&#24230;&#35745;&#25429;&#33719;&#19981;&#21516;&#26085;&#24120;&#27963;&#21160;&#30340;&#25968;&#25454;&#65292;&#25552;&#21462;&#29305;&#24449;&#24182;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#23454;&#26102;&#27963;&#21160;&#35782;&#21035;&#21644;&#21345;&#36335;&#37324;&#28040;&#32791;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#26159;&#24403;&#21069;&#30340;&#30740;&#31350;&#28909;&#28857;&#20043;&#19968;&#65292;&#22312;&#36828;&#31243;&#21307;&#30103;&#12289;&#32769;&#24180;&#20154;&#25110;&#27531;&#38556;&#20154;&#22763;&#27963;&#21160;&#36319;&#36394;&#12289;&#28040;&#32791;&#21345;&#36335;&#37324;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#25105;&#20204;&#30340;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;Android&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#23454;&#26102;&#35782;&#21035;&#26085;&#24120;&#20154;&#31867;&#27963;&#21160;&#24182;&#35745;&#31639;&#28040;&#32791;&#30340;&#21345;&#36335;&#37324;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#30340;&#20869;&#32622;&#21152;&#36895;&#24230;&#35745;&#25429;&#33719;&#20102;&#19981;&#21516;&#26085;&#24120;&#20154;&#31867;&#27963;&#21160;&#30340;&#26631;&#35760;&#19977;&#36724;&#21152;&#36895;&#24230;&#35835;&#25968;&#12290;&#28982;&#21518;&#20351;&#29992;&#20013;&#20540;&#28388;&#27874;&#23545;&#36825;&#20123;&#35835;&#25968;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#26041;&#27861;&#25552;&#21462;&#20102;42&#20010;&#29305;&#24449;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20197;&#21450;&#38477;&#32500;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#22312;&#25105;&#20204;&#30340;Android&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#25552;&#20379;&#20102;&#26368;&#39640;&#20934;&#30830;&#24615;&#21644;&#26368;&#30701;&#27169;&#22411;&#26500;&#24314;&#26102;&#38388;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#36825;&#29992;&#20110;&#23454;&#26102;&#27963;&#21160;&#35782;&#21035;&#21644;&#21033;&#29992;&#22522;&#20110;&#20195;&#35874;&#30340;&#20844;&#24335;&#35745;&#31639;&#28040;&#32791;&#30340;&#21345;&#36335;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02869v1 Announce Type: cross  Abstract: Human Activity Recognition is a subject of great research today and has its applications in remote healthcare, activity tracking of the elderly or the disables, calories burnt tracking etc. In our project, we have created an Android application that recognizes the daily human activities and calculate the calories burnt in real time. We first captured labeled triaxial acceleration readings for different daily human activities from the smartphone's embedded accelerometer. These readings were preprocessed using a median filter. 42 features were extracted using various methods. We then tested various machine learning algorithms along with dimensionality reduction. Finally, in our Android application, we used the machine learning algorithm and a subset of features that provided maximum accuracy and minimum model building time. This is used for real-time activity recognition and calculation of calories burnt using a formula based on Metaboli
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#21040;&#26368;&#21518;&#23618;&#30340;&#28608;&#27963;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#20351;&#29992;HCR&#30028;&#38480;&#21487;&#37327;&#21270;&#20445;&#25252;&#26426;&#23494;&#24615;&#30340;&#21487;&#20449;&#24230;</title><link>https://arxiv.org/abs/2404.02866</link><description>&lt;p&gt;
&#36890;&#36807;Hammersley-Chapman-Robbins&#30028;&#38480;&#20445;&#35777;&#26426;&#23494;&#24615;
&lt;/p&gt;
&lt;p&gt;
Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02866
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#21040;&#26368;&#21518;&#23618;&#30340;&#28608;&#27963;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#20351;&#29992;HCR&#30028;&#38480;&#21487;&#37327;&#21270;&#20445;&#25252;&#26426;&#23494;&#24615;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#36807;&#31243;&#20013;&#36890;&#36807;&#21521;&#26368;&#21518;&#20960;&#23618;&#30340;&#28608;&#27963;&#28155;&#21152;&#22122;&#22768;&#26469;&#20445;&#25252;&#38544;&#31169;&#26159;&#21487;&#33021;&#30340;&#12290;&#36825;&#20123;&#23618;&#20013;&#30340;&#28608;&#27963;&#34987;&#31216;&#20026;&#8220;&#29305;&#24449;&#8221;&#65288;&#23569;&#35265;&#30340;&#31216;&#20026;&#8220;&#23884;&#20837;&#8221;&#25110;&#8220;&#29305;&#24449;&#23884;&#20837;&#8221;&#65289;&#12290;&#28155;&#21152;&#30340;&#22122;&#22768;&#26377;&#21161;&#20110;&#38450;&#27490;&#20174;&#22024;&#26434;&#30340;&#29305;&#24449;&#20013;&#37325;&#24314;&#36755;&#20837;&#12290;&#36890;&#36807;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#26080;&#20559;&#20272;&#35745;&#37327;&#30340;&#26041;&#24046;&#36827;&#34892;&#19979;&#38480;&#20272;&#35745;&#65292;&#37327;&#21270;&#20102;&#30001;&#27492;&#28155;&#21152;&#30340;&#22122;&#22768;&#20135;&#29983;&#30340;&#26426;&#23494;&#24615;&#12290;&#32463;&#20856;&#19981;&#31561;&#24335;Hammersley&#21644;Chapman&#20197;&#21450;Robbins&#25552;&#20379;&#20415;&#21033;&#30340;&#12289;&#21487;&#35745;&#31639;&#30340;&#30028;&#38480;-- HCR&#30028;&#38480;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#20110;&#21253;&#21547;10&#20010;&#31867;&#21035;&#30340;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#8220;MNIST&#8221;&#21644;&#8220;CIFAR-10&#8221;&#65292;HCR&#30028;&#38480;&#22312;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;HCR&#30028;&#38480;&#20284;&#20046;&#21333;&#29420;&#26080;&#27861;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02866v1 Announce Type: new  Abstract: Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as "features" (or, less commonly, as "embeddings" or "feature embeddings"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, "MNIST" and "CIFAR-10," which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;TSAP&#26041;&#27861;&#26469;&#33258;&#21160;&#35843;&#25972;&#25968;&#25454;&#22686;&#24378;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24102;&#26469;&#20102;&#31471;&#21040;&#31471;&#30340;&#33258;&#35843;&#33410;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02865</link><description>&lt;p&gt;
&#31471;&#21040;&#31471;&#33258;&#35843;&#33410;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
End-To-End Self-tuning Self-supervised Time Series Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02865
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;TSAP&#26041;&#27861;&#26469;&#33258;&#21160;&#35843;&#25972;&#25968;&#25454;&#22686;&#24378;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24102;&#26469;&#20102;&#31471;&#21040;&#31471;&#30340;&#33258;&#35843;&#33410;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;TSAD&#65289;&#22312;&#30417;&#25511;&#29615;&#22659;&#20256;&#24863;&#22120;&#12289;&#34892;&#19994;KPI&#12289;&#24739;&#32773;&#29983;&#29289;&#26631;&#24535;&#29289;&#31561;&#26041;&#38754;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;TSAD&#30340;&#19968;&#20010;&#21452;&#37325;&#25361;&#25112;&#26159;&#38656;&#35201;&#19968;&#31181;&#22810;&#21151;&#33021;&#19988;&#26080;&#30417;&#30563;&#27169;&#22411;&#65292;&#33021;&#22815;&#26816;&#27979;&#21508;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#65288;&#23574;&#23792;&#12289;&#19981;&#36830;&#32493;&#12289;&#36235;&#21183;&#21464;&#21270;&#31561;&#65289;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TSAP&#26469;&#25191;&#34892;TSA&#8220;&#33258;&#21160;&#39550;&#39542;&#8221;&#65292;&#21487;&#20197;&#31471;&#21040;&#31471;&#33258;&#21160;&#35843;&#25972;&#25968;&#25454;&#22686;&#24378;&#30340;&#36229;&#21442;&#25968;&#65292;&#33258;&#36866;&#24212;&#36873;&#25321;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02865v1 Announce Type: new  Abstract: Time series anomaly detection (TSAD) finds many applications such as monitoring environmental sensors, industry KPIs, patient biomarkers, etc. A two-fold challenge for TSAD is a versatile and unsupervised model that can detect various different types of time series anomalies (spikes, discontinuities, trend shifts, etc.) without any labeled data. Modern neural networks have outstanding ability in modeling complex time series. Self-supervised models in particular tackle unsupervised TSAD by transforming the input via various augmentations to create pseudo anomalies for training. However, their performance is sensitive to the choice of augmentation, which is hard to choose in practice, while there exists no effort in the literature on data augmentation tuning for TSAD without labels. Our work aims to fill this gap. We introduce TSAP for TSA "on autoPilot", which can (self-)tune augmentation hyperparameters end-to-end. It stands on two key c
&lt;/p&gt;</description></item><item><title>MoE-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#25512;&#29702;&#26102;&#38388;&#19982;&#19987;&#23478;&#25968;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#25512;&#29702;&#25928;&#29575;&#20316;&#20026;&#32553;&#25918;&#23450;&#24459;&#30340;&#35843;&#25972;&#26041;&#26696;</title><link>https://arxiv.org/abs/2404.02852</link><description>&lt;p&gt;
&#26397;&#21521;&#25512;&#29702;&#26368;&#20339;&#30340;&#28151;&#21512;&#19987;&#23478;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Toward Inference-optimal Mixture-of-Expert Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02852
&lt;/p&gt;
&lt;p&gt;
MoE-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#25512;&#29702;&#26102;&#38388;&#19982;&#19987;&#23478;&#25968;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#25512;&#29702;&#25928;&#29575;&#20316;&#20026;&#32553;&#25918;&#23450;&#24459;&#30340;&#35843;&#25972;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixture-of-Expert&#65288;MoE&#65289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;&#26368;&#36817;&#30340;Mixtral&#21644;DeepSeek-MoE&#65292;&#23637;&#31034;&#20102;&#22312;&#32553;&#25918;&#27169;&#22411;&#22823;&#23567;&#26102;&#19981;&#20250;&#36973;&#21463;&#23494;&#38598;&#21464;&#21387;&#22120;&#35757;&#32451;&#25104;&#26412;&#30340;&#20108;&#27425;&#22686;&#38271;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#19982;&#23494;&#38598;&#27169;&#22411;&#19968;&#26679;&#65292;&#35757;&#32451;MoEs&#38656;&#35201;&#22238;&#31572;&#21516;&#26679;&#30340;&#38382;&#39064;&#65306;&#22312;&#32473;&#23450;&#30340;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#27169;&#22411;&#22823;&#23567;&#21644;&#26631;&#35760;&#25968;&#30340;&#26368;&#20339;&#20998;&#37197;&#26159;&#22810;&#23569;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;&#20851;&#20110;&#27169;&#22411;&#24615;&#33021;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#19987;&#23478;&#31243;&#24230;&#20043;&#38388;&#20851;&#31995;&#30340;MoE-based LLMs&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;&#22238;&#24212;&#20808;&#21069;&#30740;&#31350;MoE&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22686;&#21152;&#19987;&#23478;&#25968;&#37327;&#30340;&#36882;&#20943;&#22238;&#25253;&#65292;&#20294;&#36825;&#20284;&#20046;&#34920;&#26126;&#25105;&#20204;&#24212;&#35813;&#25193;&#23637;&#19987;&#23478;&#25968;&#37327;&#30452;&#33267;&#39281;&#21644;&#65292;&#22240;&#20026;&#35757;&#32451;&#25104;&#26412;&#20250;&#20445;&#25345;&#24658;&#23450;&#65292;&#36825;&#22312;&#25512;&#29702;&#26102;&#38388;&#20013;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#24341;&#20837;&#25512;&#29702;&#25928;&#29575;&#20316;&#20026;&#21478;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#20462;&#25913;MoE&#30340;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02852v1 Announce Type: new  Abstract: Mixture-of-Expert (MoE) based large language models (LLMs), such as the recent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size without suffering from the quadratic growth of training cost of dense transformers. Like dense models, training MoEs requires answering the same question: given a training budget, what is the optimal allocation on the model size and number of tokens? We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric b
&lt;/p&gt;</description></item><item><title>BAdam&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#25910;&#25947;&#34892;&#20026;&#20197;&#21450;&#22312;&#24615;&#33021;&#35780;&#20272;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2404.02827</link><description>&lt;p&gt;
BAdam&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#39640;&#25928;&#20840;&#21442;&#25968;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02827
&lt;/p&gt;
&lt;p&gt;
BAdam&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#25910;&#25947;&#34892;&#20026;&#20197;&#21450;&#22312;&#24615;&#33021;&#35780;&#20272;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;BAdam&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;Adam&#20316;&#20026;&#20869;&#37096;&#27714;&#35299;&#22120;&#30340;&#22359;&#22352;&#26631;&#20248;&#21270;&#26694;&#26550;&#30340;&#20248;&#21270;&#22120;&#12290;BAdam&#25552;&#20379;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#21442;&#25968;&#24494;&#35843;&#65292;&#24182;&#19988;&#30001;&#20110;&#38142;&#24335;&#35268;&#21017;&#23646;&#24615;&#20943;&#23569;&#20102;&#21453;&#21521;&#36807;&#31243;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;BAdam&#24212;&#29992;&#20110;&#22312;Alpaca-GPT4&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#21333;&#20010;RTX3090-24GB GPU&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#30340;Llama 2-7B&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;LoRA&#21644;LOMO&#30456;&#27604;&#65292;BAdam&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;MT-bench&#23545;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;&#19979;&#28216;&#24615;&#33021;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;BAdam&#22312;&#36866;&#24230;&#36229;&#36234;LoRA&#30340;&#22522;&#30784;&#19978;&#26356;&#26174;&#33879;&#22320;&#20248;&#20110;LOMO&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;BAdam&#19982;Adam&#22312;&#20013;&#31561;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21363;&#22312;SuperGLUE&#22522;&#20934;&#19978;&#23545;RoBERTa-large&#36827;&#34892;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;BAdam&#33021;&#22815;&#32553;&#23567;&#19982;Adam&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02827v1 Announce Type: new  Abstract: This work presents BAdam, an optimizer that leverages the block coordinate optimization framework with Adam as the inner solver. BAdam offers a memory efficient approach to the full parameter finetuning of large language models and reduces running time of the backward process thanks to the chain rule property. Experimentally, we apply BAdam to instruction-tune the Llama 2-7B model on the Alpaca-GPT4 dataset using a single RTX3090-24GB GPU. The results indicate that BAdam exhibits superior convergence behavior in comparison to LoRA and LOMO. Furthermore, our downstream performance evaluation of the instruction-tuned models using the MT-bench shows that BAdam modestly surpasses LoRA and more substantially outperforms LOMO. Finally, we compare BAdam with Adam on a medium-sized task, i.e., finetuning RoBERTa-large on the SuperGLUE benchmark. The results demonstrate that BAdam is capable of narrowing the performance gap with Adam. Our code is
&lt;/p&gt;</description></item><item><title>Conifer&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;LLMs&#39537;&#21160;&#30340;&#32454;&#21270;&#36807;&#31243;&#65292;&#20197;&#21450;&#28176;&#36827;&#23398;&#20064;&#26041;&#26696;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20855;&#26377;&#22797;&#26434;&#32422;&#26463;&#30340;&#22810;&#23618;&#25351;&#20196;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2404.02823</link><description>&lt;p&gt;
Conifer: &#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22797;&#26434;&#32422;&#26463;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02823
&lt;/p&gt;
&lt;p&gt;
Conifer&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;LLMs&#39537;&#21160;&#30340;&#32454;&#21270;&#36807;&#31243;&#65292;&#20197;&#21450;&#28176;&#36827;&#23398;&#20064;&#26041;&#26696;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20855;&#26377;&#22797;&#26434;&#32422;&#26463;&#30340;&#22810;&#23618;&#25351;&#20196;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#23545;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#19968;&#20123;&#30740;&#31350;&#25351;&#20986;&#65292;LLMs&#22312;&#38754;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#25351;&#20196;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#22797;&#26434;&#32422;&#26463;&#30340;&#25351;&#20196;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Conifer&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25351;&#20196;&#35843;&#33410;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#36981;&#24490;&#20855;&#26377;&#22797;&#26434;&#32422;&#26463;&#30340;&#22810;&#23618;&#25351;&#20196;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;LLM&#39537;&#21160;&#30340;&#32454;&#21270;&#36807;&#31243;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#31574;&#21010;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#35843;&#26131;&#20110;&#38590;&#30340;&#28176;&#36827;&#23398;&#20064;&#26041;&#26696;&#65292;&#24182;&#20174;&#36807;&#31243;&#21453;&#39304;&#20013;&#23398;&#20064;&#12290;&#20351;&#29992;Conifer&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#21892;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24102;&#26377;&#22797;&#26434;&#32422;&#26463;&#30340;&#25351;&#20196;&#12290;&#22312;&#20960;&#20010;&#36981;&#24490;&#25351;&#20196;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;7B&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02823v1 Announce Type: cross  Abstract: The ability of large language models (LLMs) to follow instructions is crucial to real-world applications. Despite recent advances, several studies have highlighted that LLMs struggle when faced with challenging instructions, especially those that include complex constraints, hindering their effectiveness in various tasks. To address this challenge, we introduce Conifer, a novel instruction tuning dataset, designed to enhance LLMs to follow multi-level instructions with complex constraints. Utilizing GPT-4, we curate the dataset by a series of LLM-driven refinement processes to ensure high quality. We also propose a progressive learning scheme that emphasizes an easy-to-hard progression, and learning from process feedback. Models trained with Conifer exhibit remarkable improvements in instruction-following abilities, especially for instructions with complex constraints. On several instruction-following benchmarks, our 7B model outperfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#19977;&#31867;&#30446;&#26631;&#65288;&#8220;&#20928;&#38646;&#8221;&#65292;&#8220;&#20943;&#23569;&#8221;&#21644;&#8220;&#20854;&#20182;&#8221;&#65289;&#65292;&#24182;&#35843;&#26597;&#20102;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02822</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#35782;&#21035;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#30340;&#27668;&#20505;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Identifying Climate Targets in National Laws and Policies using Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#19977;&#31867;&#30446;&#26631;&#65288;&#8220;&#20928;&#38646;&#8221;&#65292;&#8220;&#20943;&#23569;&#8221;&#21644;&#8220;&#20854;&#20182;&#8221;&#65289;&#65292;&#24182;&#35843;&#26597;&#20102;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#37327;&#25919;&#31574;&#30446;&#26631;&#26159;&#27668;&#20505;&#25919;&#31574;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#36890;&#24120;&#20197;&#39046;&#22495;&#29305;&#23450;&#21644;&#25216;&#26415;&#24615;&#35821;&#35328;&#20026;&#29305;&#24449;&#12290;&#30446;&#21069;&#65292;&#31579;&#36873;&#20840;&#29699;&#27668;&#20505;&#25919;&#31574;&#30446;&#26631;&#30340;&#26041;&#27861;&#28041;&#21450;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#12290;&#30446;&#21069;&#24456;&#23569;&#26377;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#20174;&#22269;&#23478;&#27861;&#24459;&#25110;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#65292;&#36825;&#38480;&#21046;&#20102;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#35780;&#20272;&#31169;&#33829;&#21644;&#20844;&#20849;&#37096;&#38376;&#19982;&#20840;&#29699;&#30446;&#26631;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#20026;&#25919;&#31574;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#25552;&#21450;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#35782;&#21035;&#20102;&#19977;&#31867;&#30446;&#26631;&#65288;&#8220;&#20928;&#38646;&#8221;&#65292;&#8220;&#20943;&#23569;&#8221;&#21644;&#8220;&#20854;&#20182;&#8221;&#65288;&#20363;&#22914;&#21487;&#20877;&#29983;&#33021;&#28304;&#30446;&#26631;&#65289;&#65289;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#21487;&#38752;&#22320;&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#23427;&#20204;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#19982;&#25105;&#20204;&#27169;&#22411;&#30456;&#20851;&#30340;&#20559;&#24046;&#21644;&#20844;&#24179;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#20102;&#29305;&#23450;&#24180;&#20221;&#21644;&#22269;&#23478;&#21517;&#31216;&#20316;&#20026;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02822v1 Announce Type: cross  Abstract: Quantified policy targets are a fundamental element of climate policy, typically characterised by domain-specific and technical language. Current methods for curating comprehensive views of global climate policy targets entail significant manual effort. At present there are few scalable methods for extracting climate targets from national laws or policies, which limits policymakers' and researchers' ability to (1) assess private and public sector alignment with global goals and (2) inform policy decisions. In this paper we present an approach for extracting mentions of climate targets from national laws and policies. We create an expert-annotated dataset identifying three categories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable energy targets)) and train a classifier to reliably identify them in text. We investigate bias and equity impacts related to our model and identify specific years and country names as problemati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;&#27604;&#35270;&#22270;&#22686;&#24378;&#31574;&#30053;&#12289;&#20301;&#32622;&#24863;&#30693;&#21644;&#35821;&#20041;&#24863;&#30693;&#27491;&#26679;&#26412;&#37319;&#26679;&#31574;&#30053;&#20197;&#21450;&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#20811;&#26381;&#22270;&#25968;&#25454;&#22686;&#24378;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.02810</link><description>&lt;p&gt;
&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative-Contrastive Heterogeneous Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;&#27604;&#35270;&#22270;&#22686;&#24378;&#31574;&#30053;&#12289;&#20301;&#32622;&#24863;&#30693;&#21644;&#35821;&#20041;&#24863;&#30693;&#27491;&#26679;&#26412;&#37319;&#26679;&#31574;&#30053;&#20197;&#21450;&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#20811;&#26381;&#22270;&#25968;&#25454;&#22686;&#24378;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#34920;&#36798;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22797;&#26434;&#20851;&#31995;&#65292;&#21253;&#25324;&#22810;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#21463;&#33258;&#30417;&#30563;&#23398;&#20064;&#21551;&#21457;&#65292;&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNNs)&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#36776;&#21035;&#22120;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22270;&#30340;&#31163;&#25955;&#21644;&#25277;&#35937;&#29305;&#24615;&#65292;&#25968;&#25454;&#22686;&#24378;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;\textit{&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;(GC-HGNN)}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02810v1 Announce Type: new  Abstract: Heterogeneous Graphs (HGs) can effectively model complex relationships in the real world by multi-type nodes and edges. In recent years, inspired by self-supervised learning, contrastive Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential by utilizing data augmentation and discriminators for downstream tasks. However, data augmentation is still limited due to the discrete and abstract nature of graphs. To tackle the above limitations, we propose a novel \textit{Generative-Contrastive Heterogeneous Graph Neural Network (GC-HGNN)}. Specifically, we first propose a heterogeneous graph generative learning enhanced contrastive paradigm. This paradigm includes: 1) A contrastive view augmentation strategy by using masked autoencoder. 2) Position-aware and semantics-aware positive sample sampling strategy for generate hard negative samples. 3) A hierarchical contrastive learning strategy for capturing local and global informa
&lt;/p&gt;</description></item><item><title>&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#23454;&#29616;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#24555;&#36895;&#36866;&#24212;&#65292;&#20026;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26102;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2404.02785</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#39046;&#22495;&#27867;&#21270;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization through Meta-Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02785
&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#23454;&#29616;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#24555;&#36895;&#36866;&#24212;&#65292;&#20026;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26102;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#20294;&#26159;&#24403;&#38754;&#23545;&#20998;&#24067;&#20043;&#22806;(out-of-distribution, OOD)&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#22240;&#20026;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#19981;&#21487;&#36991;&#20813;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#34987;&#20551;&#23450;&#20026;&#20849;&#20139;&#30456;&#21516;&#20998;&#24067;&#30340;&#24120;&#35265;&#24773;&#20917;&#12290;&#23613;&#31649;DNNs&#22312;&#22823;&#37327;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#24212;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#12290;&#20803;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#30340;&#31639;&#27861;&#36827;&#34892;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#38656;&#35201;&#20174;&#22836;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#20803;&#23398;&#20064;&#39046;&#22495;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02785v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution (OOD) data, a common scenario due to the inevitable domain shifts in real-world applications. This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice. Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains. Meta-learning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch. This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization. We first clarify the 
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#24314;&#31435;&#20998;&#24067;&#24335;&#32593;&#32476;&#65292;&#27599;&#20010;&#35774;&#22791;&#20445;&#30041;&#23545;&#33258;&#36523;&#25968;&#25454;&#30340;&#25511;&#21046;&#24182;&#21442;&#19982;&#38598;&#20307;&#35745;&#31639;&#65292;&#20174;&#32780;&#23454;&#29616;&#21327;&#20316;&#22788;&#29702;&#65292;&#20854;&#23562;&#37325;&#29992;&#25143;&#38544;&#31169;&#21644;&#25968;&#25454;&#20027;&#26435;&#30340;&#21407;&#21017;&#19982;&#24403;&#19979;&#23545;&#36127;&#36131;&#20219;AI&#21644;&#36947;&#24503;&#25968;&#25454;&#23454;&#36341;&#30340;&#38656;&#27714;&#30456;&#22865;&#21512;&#12290;</title><link>https://arxiv.org/abs/2404.02779</link><description>&lt;p&gt;
&#32852;&#37030;&#35745;&#31639;&#8212;&#8212;&#26500;&#24314;&#22359;&#12289;&#25193;&#23637;&#21644;&#31995;&#32479;&#30340;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Federated Computing -- Survey on Building Blocks, Extensions and Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02779
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#24314;&#31435;&#20998;&#24067;&#24335;&#32593;&#32476;&#65292;&#27599;&#20010;&#35774;&#22791;&#20445;&#30041;&#23545;&#33258;&#36523;&#25968;&#25454;&#30340;&#25511;&#21046;&#24182;&#21442;&#19982;&#38598;&#20307;&#35745;&#31639;&#65292;&#20174;&#32780;&#23454;&#29616;&#21327;&#20316;&#22788;&#29702;&#65292;&#20854;&#23562;&#37325;&#29992;&#25143;&#38544;&#31169;&#21644;&#25968;&#25454;&#20027;&#26435;&#30340;&#21407;&#21017;&#19982;&#24403;&#19979;&#23545;&#36127;&#36131;&#20219;AI&#21644;&#36947;&#24503;&#25968;&#25454;&#23454;&#36341;&#30340;&#38656;&#27714;&#30456;&#22865;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#25968;&#25454;&#37327;&#22686;&#22823;&#21644;&#25935;&#24863;&#24615;&#22686;&#24378;&#65292;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#35745;&#31639;&#27169;&#22411;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#23433;&#20840;&#28431;&#27934;&#21644;&#30417;&#31649;&#38556;&#30861;&#12290;&#32852;&#37030;&#35745;&#31639;&#65288;FC&#65289;&#36890;&#36807;&#23454;&#29616;&#21327;&#20316;&#22788;&#29702;&#32780;&#19981;&#25439;&#23475;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;&#26469;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#19968;&#30446;&#26631;&#36890;&#36807;&#20998;&#24067;&#24335;&#35774;&#22791;&#32593;&#32476;&#23454;&#29616;&#65292;&#27599;&#20010;&#35774;&#22791;&#22312;&#21442;&#19982;&#38598;&#20307;&#35745;&#31639;&#30340;&#21516;&#26102;&#20445;&#30041;&#23545;&#20854;&#25968;&#25454;&#30340;&#25511;&#21046;&#12290;FC&#30340;&#21160;&#26426;&#19981;&#20165;&#38480;&#20110;&#25216;&#26415;&#32771;&#34385;&#65292;&#36824;&#21253;&#25324;&#31038;&#20250;&#24433;&#21709;&#12290;&#38543;&#30528;&#23545;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#36947;&#24503;&#25968;&#25454;&#23454;&#36341;&#30340;&#38656;&#27714;&#21152;&#21095;&#65292;FC&#31526;&#21512;&#29992;&#25143;&#36171;&#26435;&#21644;&#25968;&#25454;&#20027;&#26435;&#21407;&#21017;&#12290;FC&#21253;&#25324;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#32852;&#37030;&#20998;&#26512;&#65288;FA&#65289;&#12290;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#65292;FC&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#30446;&#21069;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#21644;&#25551;&#36848;&#20854;&#32452;&#25104;&#37096;&#20998;&#30340;&#20998;&#31867;&#12290;&#24403;&#21069;&#30340;&#35843;&#30740;&#21482;&#28085;&#30422;&#29305;&#23450;&#39046;&#22495;&#30340;FL&#24212;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02779v1 Announce Type: new  Abstract: In response to the increasing volume and sensitivity of data, traditional centralized computing models face challenges, such as data security breaches and regulatory hurdles. Federated Computing (FC) addresses these concerns by enabling collaborative processing without compromising individual data privacy. This is achieved through a decentralized network of devices, each retaining control over its data, while participating in collective computations. The motivation behind FC extends beyond technical considerations to encompass societal implications. As the need for responsible AI and ethical data practices intensifies, FC aligns with the principles of user empowerment and data sovereignty. FC comprises of Federated Learning (FL) and Federated Analytics (FA). FC systems became more complex over time and they currently lack a clear definition and taxonomy describing its moving pieces. Current surveys capture domain-specific FL use cases, d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02761</link><description>&lt;p&gt;
AQuA --&#32467;&#21512;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#35266;&#28857;&#65292;&#21033;&#29992;LLMs&#35780;&#20272;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#30923;&#21830;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02761
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25919;&#27835;&#22312;&#32447;&#35752;&#35770;&#20013;&#34913;&#37327;&#36129;&#29486;&#36136;&#37327;&#23545;&#20110;&#30740;&#31350;&#30923;&#21830;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#33258;&#21160;&#34913;&#37327;&#36825;&#20123;&#25351;&#26631;&#21464;&#24471;&#21487;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AQuA&#65292;&#23427;&#26159;&#19968;&#20010;&#28155;&#21152;&#20998;&#25968;&#65292;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#35745;&#31639;&#27599;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#12290;&#19982;&#20854;&#20182;&#29305;&#23450;&#20998;&#25968;&#19981;&#21516;&#65292;AQuA&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#23384;&#22312;&#30340;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02761v1 Announce Type: cross  Abstract: Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices int
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31232;&#30095;&#36755;&#20837;&#23398;&#20064;&#21344;&#29992;&#22330;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#36793;&#30028;&#19981;&#30830;&#23450;&#24615;&#30340;&#37319;&#26679;&#21644;&#26368;&#23567;&#29109;&#22330;&#20248;&#21270;&#26469;&#35299;&#20915;&#20174;3D&#28857;&#20113;&#23398;&#20064;SDF&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2404.02759</link><description>&lt;p&gt;
&#26469;&#33258;&#31232;&#30095;&#28857;&#20113;&#30340;&#26080;&#30417;&#30563;&#21344;&#29992;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Occupancy Learning from Sparse Point Cloud
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02759
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31232;&#30095;&#36755;&#20837;&#23398;&#20064;&#21344;&#29992;&#22330;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#36793;&#30028;&#19981;&#30830;&#23450;&#24615;&#30340;&#37319;&#26679;&#21644;&#26368;&#23567;&#29109;&#22330;&#20248;&#21270;&#26469;&#35299;&#20915;&#20174;3D&#28857;&#20113;&#23398;&#20064;SDF&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#25429;&#33719;&#22797;&#26434;&#30340;&#25968;&#25454;&#27169;&#24577;&#65292;&#28085;&#30422;&#20174;3D&#24418;&#29366;&#21040;&#22270;&#20687;&#21644;&#38899;&#39057;&#31561;&#24191;&#27867;&#33539;&#22260;&#12290;&#22312;3D&#24418;&#29366;&#34920;&#31034;&#39046;&#22495;&#65292;&#31070;&#32463;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#23637;&#29616;&#20986;&#20174;&#26681;&#26412;&#19978;&#32534;&#30721;&#22797;&#26434;&#24418;&#29366;&#20960;&#20309;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#27809;&#26377;&#22320;&#38754;&#30495;&#23454;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;3D&#28857;&#20113;&#20013;&#23398;&#20064;SDF&#20173;&#28982;&#26159;&#19968;&#39033;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25512;&#26029;&#21344;&#29992;&#22330;&#32780;&#19981;&#26159;SDF&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#23481;&#26131;&#20174;&#31232;&#30095;&#36755;&#20837;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#36793;&#30028;&#19981;&#30830;&#23450;&#24615;&#30340;&#36793;&#38469;&#27979;&#37327;&#65292;&#19981;&#21516;&#22320;&#20174;&#21344;&#29992;&#20989;&#25968;&#30340;&#20915;&#31574;&#36793;&#30028;&#20013;&#37319;&#26679;&#65292;&#24182;&#20351;&#29992;&#36755;&#20837;&#28857;&#20113;&#30417;&#30563;&#37319;&#26679;&#30340;&#36793;&#30028;&#28857;&#12290;&#25105;&#20204;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#36890;&#36807;&#23558;&#21344;&#29992;&#20989;&#25968;&#20559;&#21521;&#26368;&#23567;&#29109;&#22330;&#26469;&#36827;&#19968;&#27493;&#31283;&#23450;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02759v1 Announce Type: cross  Abstract: Implicit Neural Representations have gained prominence as a powerful framework for capturing complex data modalities, encompassing a wide range from 3D shapes to images and audio. Within the realm of 3D shape representation, Neural Signed Distance Functions (SDF) have demonstrated remarkable potential in faithfully encoding intricate shape geometry. However, learning SDFs from 3D point clouds in the absence of ground truth supervision remains a very challenging task. In this paper, we propose a method to infer occupancy fields instead of SDFs as they are easier to learn from sparse inputs. We leverage a margin-based uncertainty measure to differentially sample from the decision boundary of the occupancy function and supervise the sampled boundary points using the input point cloud. We further stabilize the optimization process at the early stages of the training by biasing the occupancy function towards minimal entropy fields while max
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38271;&#23614;&#20998;&#24067;&#19979;&#22823;&#37327;&#20219;&#21153;&#30340;&#19981;&#26029;&#23398;&#20064;&#31639;&#27861;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#20248;&#21270;&#22120;&#29366;&#24577;&#25913;&#36827;&#19981;&#26029;&#23398;&#20064;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02754</link><description>&lt;p&gt;
&#20174;&#38271;&#23614;&#20998;&#24067;&#20013;&#19981;&#26029;&#23398;&#20064;&#22823;&#37327;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Continual Learning of Numerous Tasks from Long-tail Distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38271;&#23614;&#20998;&#24067;&#19979;&#22823;&#37327;&#20219;&#21153;&#30340;&#19981;&#26029;&#23398;&#20064;&#31639;&#27861;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#20248;&#21270;&#22120;&#29366;&#24577;&#25913;&#36827;&#19981;&#26029;&#23398;&#20064;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Continual learning&#65292;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#33268;&#21147;&#20110;&#24320;&#21457;&#33021;&#22815;&#23398;&#20064;&#24182;&#36866;&#24212;&#26032;&#20219;&#21153;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#33719;&#24471;&#30693;&#35782;&#30340;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#19981;&#26029;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#28041;&#21450;&#23569;&#37327;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#35268;&#27169;&#30456;&#21516;&#65292;&#24182;&#19988;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#22320;&#20195;&#34920;&#29616;&#23454;&#19990;&#30028;&#30340;&#23398;&#20064;&#22330;&#26223;&#12290;&#26412;&#25991;&#30740;&#31350;&#20855;&#26377;&#22823;&#37327;&#20219;&#21153;&#30340;&#19981;&#26029;&#23398;&#20064;&#31639;&#27861;&#22312;&#38271;&#23614;&#20219;&#21153;&#22823;&#23567;&#20998;&#24067;&#19979;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#19981;&#26029;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#29616;&#26377;&#31639;&#27861;&#22312;&#36825;&#31181;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#22312;&#19981;&#26029;&#23398;&#20064;&#20013;&#34987;&#24573;&#35270;&#30340;&#22240;&#32032;&#65292;&#21363;&#20248;&#21270;&#22120;&#29366;&#24577;&#65292;&#22914;Adam&#20248;&#21270;&#22120;&#20013;&#30340;&#31532;&#19968;&#21644;&#31532;&#20108;&#26102;&#21051;&#65292;&#24182;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#23427;&#26469;&#25552;&#39640;&#19981;&#26029;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#22797;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02754v1 Announce Type: new  Abstract: Continual learning, an important aspect of artificial intelligence and machine learning research, focuses on developing models that learn and adapt to new tasks while retaining previously acquired knowledge. Existing continual learning algorithms usually involve a small number of tasks with uniform sizes and may not accurately represent real-world learning scenarios. In this paper, we investigate the performance of continual learning algorithms with a large number of tasks drawn from a task distribution that is long-tail in terms of task sizes. We design one synthetic dataset and two real-world continual learning datasets to evaluate the performance of existing algorithms in such a setting. Moreover, we study an overlooked factor in continual learning, the optimizer states, e.g. first and second moments in the Adam optimizer, and investigate how it can be used to improve continual learning performance. We propose a method that reuses the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#36882;&#24402;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#24207;&#21015;&#21560;&#24341;&#23376;&#65292;&#20197;&#31283;&#20581;&#22320;&#23384;&#20648;&#21644;&#26816;&#32034;&#39044;&#23450;&#20041;&#30340;&#27169;&#24335;&#24207;&#21015;&#65292;&#32467;&#26524;&#34920;&#26126;&#32593;&#32476;&#38656;&#35201;&#21253;&#21547;&#38544;&#34255;&#31070;&#32463;&#20803;&#26469;&#23384;&#20648;&#20219;&#24847;&#27169;&#24335;&#24207;&#21015;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#23616;&#37096;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2404.02729</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#36882;&#24402;&#32593;&#32476;&#20013;&#23398;&#20064;&#24207;&#21015;&#21560;&#24341;&#23376;
&lt;/p&gt;
&lt;p&gt;
Learning Sequence Attractors in Recurrent Networks with Hidden Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#36882;&#24402;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#24207;&#21015;&#21560;&#24341;&#23376;&#65292;&#20197;&#31283;&#20581;&#22320;&#23384;&#20648;&#21644;&#26816;&#32034;&#39044;&#23450;&#20041;&#30340;&#27169;&#24335;&#24207;&#21015;&#65292;&#32467;&#26524;&#34920;&#26126;&#32593;&#32476;&#38656;&#35201;&#21253;&#21547;&#38544;&#34255;&#31070;&#32463;&#20803;&#26469;&#23384;&#20648;&#20219;&#24847;&#27169;&#24335;&#24207;&#21015;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#23616;&#37096;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#34987;&#35774;&#35745;&#29992;&#26469;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#12290;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22823;&#33041;&#26159;&#22914;&#20309;&#23398;&#20064;&#23384;&#20648;&#21644;&#26816;&#32034;&#24207;&#21015;&#35760;&#24518;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20108;&#36827;&#21046;&#31070;&#32463;&#20803;&#30340;&#36882;&#24402;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#24207;&#21015;&#21560;&#24341;&#23376;&#65292;&#20197;&#23384;&#20648;&#39044;&#23450;&#20041;&#30340;&#27169;&#24335;&#24207;&#21015;&#24182;&#31283;&#20581;&#22320;&#26816;&#32034;&#23427;&#20204;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20026;&#20102;&#23384;&#20648;&#20219;&#24847;&#27169;&#24335;&#24207;&#21015;&#65292;&#32593;&#32476;&#38656;&#35201;&#21253;&#21547;&#38544;&#34255;&#31070;&#32463;&#20803;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#26174;&#31034;&#24207;&#21015;&#35760;&#24518;&#26041;&#38754;&#30340;&#20316;&#29992;&#26159;&#38388;&#25509;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23616;&#37096;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#32593;&#32476;&#20013;&#30340;&#24207;&#21015;&#21560;&#24341;&#23376;&#12290;&#35813;&#31639;&#27861;&#34987;&#35777;&#26126;&#20250;&#25910;&#25947;&#24182;&#23548;&#33268;&#24207;&#21015;&#21560;&#24341;&#23376;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#31283;&#20581;&#22320;&#23384;&#20648;&#21644;&#26816;&#32034;&#24207;&#21015;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#30740;&#31350;&#33021;&#22815;&#20026;&#29702;&#35299;&#22823;&#33041;&#20013;&#30340;&#24207;&#21015;&#35760;&#24518;&#21644;&#26102;&#38388;&#20449;&#24687;&#22788;&#29702;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02729v1 Announce Type: cross  Abstract: The brain is targeted for processing temporal sequence information. It remains largely unclear how the brain learns to store and retrieve sequence memories. Here, we study how recurrent networks of binary neurons learn sequence attractors to store predefined pattern sequences and retrieve them robustly. We show that to store arbitrary pattern sequences, it is necessary for the network to include hidden neurons even though their role in displaying sequence memories is indirect. We develop a local learning algorithm to learn sequence attractors in the networks with hidden neurons. The algorithm is proven to converge and lead to sequence attractors. We demonstrate that the network model can store and retrieve sequences robustly on synthetic and real-world datasets. We hope that this study provides new insights in understanding sequence memory and temporal information processing in the brain.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#25506;&#32034;&#38454;&#27573;&#23558;&#36830;&#32493;&#36816;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;, &#33258;&#21160;&#29983;&#25104;&#8220;&#21160;&#20316;&#21407;&#22411;&#8221;, &#20174;&#32780;&#23454;&#29616;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#25928;&#26524;&#39537;&#21160;&#23398;&#20064;</title><link>https://arxiv.org/abs/2404.02728</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20013;&#26377;&#25928;&#21160;&#20316;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning of Effective Actions in Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02728
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#25506;&#32034;&#38454;&#27573;&#23558;&#36830;&#32493;&#36816;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;, &#33258;&#21160;&#29983;&#25104;&#8220;&#21160;&#20316;&#21407;&#22411;&#8221;, &#20174;&#32780;&#23454;&#29616;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#25928;&#26524;&#39537;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19982;&#20915;&#31574;&#30456;&#20851;&#19988;&#21487;&#20197;&#26377;&#25928;&#25191;&#34892;&#30340;&#21160;&#20316;&#26159;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#24403;&#21069;&#26426;&#22120;&#20154;&#23398;&#20013;&#26368;&#20808;&#36827;&#30340;&#21160;&#20316;&#34920;&#31034;&#32570;&#20047;&#23545;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#36866;&#24403;&#25928;&#26524;&#39537;&#21160;&#23398;&#20064;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#25805;&#32437;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#32570;&#20047;&#36825;&#31181;&#33021;&#21147;&#65292;&#32780;&#19988;&#22312;&#20869;&#23384;&#25110;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#25104;&#26412;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#36830;&#32493;&#36816;&#21160;&#31354;&#38388;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#29983;&#25104;&#8220;&#21160;&#20316;&#21407;&#22411;&#8221;&#65292;&#27599;&#20010;&#21407;&#22411;&#22312;&#29615;&#22659;&#20013;&#20135;&#29983;&#19981;&#21516;&#30340;&#25928;&#26524;&#12290;&#22312;&#25506;&#32034;&#38454;&#27573;&#20043;&#21518;&#65292;&#35813;&#31639;&#27861;&#20250;&#33258;&#21160;&#26500;&#24314;&#23545;&#25928;&#26524;&#30340;&#34920;&#31034;&#65292;&#24182;&#23558;&#21160;&#20316;&#20998;&#32452;&#20026;&#21160;&#20316;&#21407;&#22411;&#65292;&#20854;&#20013;&#26356;&#26377;&#21487;&#33021;&#20135;&#29983;&#25928;&#26524;&#30340;&#21160;&#20316;&#27604;&#23548;&#33268;&#21487;&#24573;&#30053;&#21464;&#21270;&#30340;&#21160;&#20316;&#26356;&#22810;&#22320;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#27004;&#26799;&#25856;&#30331;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02728v1 Announce Type: cross  Abstract: Learning actions that are relevant to decision-making and can be executed effectively is a key problem in autonomous robotics. Current state-of-the-art action representations in robotics lack proper effect-driven learning of the robot's actions. Although successful in solving manipulation tasks, deep learning methods also lack this ability, in addition to their high cost in terms of memory or training data. In this paper, we propose an unsupervised algorithm to discretize a continuous motion space and generate "action prototypes", each producing different effects in the environment. After an exploration phase, the algorithm automatically builds a representation of the effects and groups motions into action prototypes, where motions more likely to produce an effect are represented more than those that lead to negligible changes. We evaluate our method on a simulated stair-climbing reinforcement learning task, and the preliminary results
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#21033;&#29992;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#22270;&#20687;&#35782;&#21035;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#25552;&#39640;&#21512;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#21306;&#20998;&#30495;&#23454;&#22270;&#20687;&#19982;&#21512;&#25104;&#22270;&#20687;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2404.02726</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#22270;&#20687;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Large Vision Language Models for Synthetic Image Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#21033;&#29992;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#22270;&#20687;&#35782;&#21035;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#25552;&#39640;&#21512;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#21306;&#20998;&#30495;&#23454;&#22270;&#20687;&#19982;&#21512;&#25104;&#22270;&#20687;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33021;&#22815;&#20174;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#30340;&#27169;&#22411;&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#36825;&#20026;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21019;&#24314;&#36924;&#30495;&#22270;&#20687;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#23637;&#20063;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#22270;&#20687;&#28508;&#22312;&#28389;&#29992;&#30340;&#25285;&#24551;&#65292;&#21253;&#25324;&#21046;&#36896;&#34394;&#20551;&#26032;&#38395;&#21644;&#23459;&#20256;&#31561;&#35823;&#23548;&#24615;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#21033;&#29992;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#36827;&#34892;&#21512;&#25104;&#22270;&#20687;&#35782;&#21035;&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#37325;&#28857;&#26159;&#35843;&#25972;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#20197;&#36827;&#34892;&#21512;&#25104;&#22270;&#20687;&#26816;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;VLMs&#30340;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#65292;&#26088;&#22312;&#21306;&#20998;&#30001;&#25193;&#25955;&#22411;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#35832;&#22914;BLIP-2&#21644;ViTGPT2&#31561;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;&#21512;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02726v1 Announce Type: cross  Abstract: In recent years, the emergence of models capable of generating images from text has attracted considerable interest, offering the possibility of creating realistic images from text descriptions. Yet these advances have also raised concerns about the potential misuse of these images, including the creation of misleading content such as fake news and propaganda. This study investigates the effectiveness of using advanced vision-language models (VLMs) for synthetic image identification. Specifically, the focus is on tuning state-of-the-art image captioning models for synthetic image detection. By harnessing the robust understanding capabilities of large VLMs, the aim is to distinguish authentic images from synthetic images produced by diffusion-based models. This study contributes to the advancement of synthetic image detection by exploiting the capabilities of visual language models such as BLIP-2 and ViTGPT2. By tailoring image captioni
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22522;&#20110;&#31526;&#21512;&#25512;&#29702;&#25216;&#26415;&#30340;&#22312;&#32447;&#37325;&#26032;&#26657;&#20934;&#31243;&#24207;&#25193;&#23637;&#26368;&#26032;&#25216;&#26415;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#26085;&#21069;&#30005;&#21147;&#20215;&#26684;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2404.02722</link><description>&lt;p&gt;
&#22312;&#32447;&#31526;&#21512;&#21270;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#29992;&#20110;&#26085;&#21069;&#30005;&#21147;&#20215;&#26684;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
On-line conformalized neural networks ensembles for probabilistic forecasting of day-ahead electricity prices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02722
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22522;&#20110;&#31526;&#21512;&#25512;&#29702;&#25216;&#26415;&#30340;&#22312;&#32447;&#37325;&#26032;&#26657;&#20934;&#31243;&#24207;&#25193;&#23637;&#26368;&#26032;&#25216;&#26415;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#26085;&#21069;&#30005;&#21147;&#20215;&#26684;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#24615;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#65288;PEPF&#65289;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#20154;&#20204;&#38656;&#35201;&#27491;&#30830;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25903;&#25345;&#22312;&#19981;&#26029;&#22686;&#21152;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#22797;&#26434;&#30005;&#21147;&#24066;&#22330;&#20013;&#30340;&#36816;&#33829;&#12290;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26368;&#36817;&#34987;&#35777;&#26126;&#33021;&#22815;&#32988;&#36807;PEPF&#22522;&#20934;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#20851;&#38190;&#30340;&#21487;&#38752;&#24615;&#22686;&#24378;&#65292;&#22240;&#20026;&#22312;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#30340;&#21508;&#20010;&#27493;&#39588;&#19978;&#26410;&#33021;&#36890;&#36807;&#35206;&#30422;&#29575;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;PEPF&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#31526;&#21512;&#25512;&#29702;&#25216;&#26415;&#30340;&#22312;&#32447;&#37325;&#26032;&#26657;&#20934;&#31243;&#24207;&#65292;&#25193;&#23637;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#24066;&#22330;&#22320;&#21306;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#25913;&#36827;&#30340;&#23567;&#26102;&#35206;&#30422;&#29575;&#21644;&#31283;&#23450;&#27010;&#29575;&#24471;&#20998;&#30340;&#26085;&#21069;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02722v1 Announce Type: new  Abstract: Probabilistic electricity price forecasting (PEPF) is subject of increasing interest, following the demand for proper quantification of prediction uncertainty, to support the operation in complex power markets with increasing share of renewable generation. Distributional neural networks ensembles have been recently shown to outperform state of the art PEPF benchmarks. Still, they require critical reliability enhancements, as fail to pass the coverage tests at various steps on the prediction horizon. In this work, we propose a novel approach to PEPF, extending the state of the art neural networks ensembles based methods through conformal inference based techniques, deployed within an on-line recalibration procedure. Experiments have been conducted on multiple market regions, achieving day-ahead forecasts with improved hourly coverage and stable probabilistic scores.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#21487;&#22609;&#24615;&#25439;&#22833;&#21644;&#31070;&#32463;&#22349;&#22604;&#30340;&#20851;&#32852;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#32531;&#35299;&#31070;&#32463;&#22349;&#22604;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#21487;&#22609;&#24615;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2404.02719</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#36890;&#36807;&#31070;&#32463;&#22349;&#22604;&#26469;&#29702;&#35299;&#21487;&#22609;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Understand Plasticity Through Neural Collapse?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02719
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#21487;&#22609;&#24615;&#25439;&#22833;&#21644;&#31070;&#32463;&#22349;&#22604;&#30340;&#20851;&#32852;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#32531;&#35299;&#31070;&#32463;&#22349;&#22604;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#21487;&#22609;&#24615;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#20004;&#20010;&#26368;&#36817;&#30830;&#23450;&#30340;&#29616;&#35937;&#20043;&#38388;&#30340;&#32852;&#31995;&#65306;&#21487;&#22609;&#24615;&#25439;&#22833;&#21644;&#31070;&#32463;&#22349;&#22604;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#31532;&#19968;&#20010;&#20219;&#21153;&#30340;&#21021;&#22987;&#35757;&#32451;&#38454;&#27573;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#32531;&#35299;&#31070;&#32463;&#22349;&#22604;&#65292;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#29305;&#23450;&#24773;&#20917;&#19979;&#20943;&#36731;&#21487;&#22609;&#24615;&#25439;&#22833;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02719v1 Announce Type: cross  Abstract: This paper explores the connection between two recently identified phenomena in deep learning: plasticity loss and neural collapse. We analyze their correlation in different scenarios, revealing a significant association during the initial training phase on the first task. Additionally, we introduce a regularization approach to mitigate neural collapse, demonstrating its effectiveness in alleviating plasticity loss in this specific setting.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#32858;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#25552;&#31034;&#29983;&#25104;&#22120;&#20026;&#27599;&#20010;&#31751;&#29983;&#25104;&#20505;&#36873;&#25552;&#31034;&#65292;&#32508;&#21512;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#20197;&#35780;&#20272;&#25552;&#31034;&#30340;&#30456;&#20851;&#24615;&#65292;&#26368;&#32456;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#35780;&#20272;&#22120;&#20026;&#26032;&#36755;&#20837;&#36873;&#25321;&#26368;&#20339;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25552;&#31034;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2404.02717</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25552;&#31034;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Automatic Prompt Selection for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02717
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#32858;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#25552;&#31034;&#29983;&#25104;&#22120;&#20026;&#27599;&#20010;&#31751;&#29983;&#25104;&#20505;&#36873;&#25552;&#31034;&#65292;&#32508;&#21512;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#20197;&#35780;&#20272;&#25552;&#31034;&#30340;&#30456;&#20851;&#24615;&#65292;&#26368;&#32456;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#35780;&#20272;&#22120;&#20026;&#26032;&#36755;&#20837;&#36873;&#25321;&#26368;&#20339;&#25552;&#31034;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25552;&#31034;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22312;&#36866;&#24403;&#30340;&#25552;&#31034;&#25351;&#23548;&#19979;&#25191;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35774;&#35745;&#26377;&#25928;&#30340;&#25552;&#31034;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#35201;&#20040;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#35201;&#20040;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#20174;&#19968;&#32452;&#26377;&#38480;&#30340;&#21512;&#25104;&#20505;&#36873;&#25552;&#31034;&#20013;&#20026;&#32473;&#23450;&#36755;&#20837;&#36873;&#25321;&#26368;&#20339;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02717v1 Announce Type: new  Abstract: Large Language Models (LLMs) can perform various natural language processing tasks with suitable instruction prompts. However, designing effective prompts manually is challenging and time-consuming. Existing methods for automatic prompt optimization either lack flexibility or efficiency. In this paper, we propose an effective approach to automatically select the optimal prompt for a given input from a finite set of synthetic candidate prompts. Our approach consists of three steps: (1) clustering the training data and generating candidate prompts for each cluster using an LLM-based prompt generator; (2) synthesizing a dataset of input-prompt-output tuples for training a prompt evaluator to rank the prompts based on their relevance to the input; (3) using the prompt evaluator to select the best prompt for a new input at test time. Our approach balances prompt generality-specificity and eliminates the need for resource-intensive training an
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23558;&#20449;&#24687;&#35770;&#38544;&#31169;&#21407;&#21017;&#19982;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#65292;&#24182;&#24341;&#20837;&#20102;&#29983;&#25104;&#24335;&#38544;&#31169;&#28431;&#26007;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.02696</link><description>&lt;p&gt;
&#28145;&#24230;&#38544;&#31169;&#28431;&#26007;&#27169;&#22411;&#65306;&#20174;&#21028;&#21035;&#24335;&#26041;&#27861;&#21040;&#29983;&#25104;&#24335;&#26041;&#27861;&#30340;&#36716;&#21464;&#65292;&#24182;&#20854;&#22312;&#20154;&#33080;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Privacy Funnel Model: From a Discriminative to a Generative Approach with an Application to Face Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02696
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23558;&#20449;&#24687;&#35770;&#38544;&#31169;&#21407;&#21017;&#19982;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#65292;&#24182;&#24341;&#20837;&#20102;&#29983;&#25104;&#24335;&#38544;&#31169;&#28431;&#26007;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#20449;&#24687;&#35770;&#38544;&#31169;&#28431;&#26007;&#65288;PF&#65289;&#27169;&#22411;&#24212;&#29992;&#20110;&#20154;&#33080;&#35782;&#21035;&#39046;&#22495;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#25968;&#25454;&#20445;&#25252;&#20013;&#27169;&#31946;&#21270;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36890;&#36807;&#23545;&#25968;&#25439;&#22833;&#65288;&#20063;&#31216;&#20026;&#33258;&#20449;&#24687;&#25439;&#22833;&#65289;&#26469;&#37327;&#21270;&#12290;&#36825;&#39033;&#30740;&#31350;&#35814;&#32454;&#25506;&#35752;&#20102;&#20449;&#24687;&#35770;&#38544;&#31169;&#21407;&#21017;&#19982;&#34920;&#31034;&#23398;&#20064;&#30340;&#25972;&#21512;&#65292;&#22312;&#29305;&#23450;&#20851;&#27880;&#20110;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#12290;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#20154;&#33080;&#35782;&#21035;&#32593;&#32476;&#30340;&#26368;&#26032;&#36827;&#23637;&#65288;&#22914;AdaFace&#21644;ArcFace&#65289;&#20043;&#38388;&#30340;&#36866;&#24212;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#29983;&#25104;&#24335;&#38544;&#31169;&#28431;&#26007;&#65288;GenPF&#65289;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#36229;&#20986;&#20256;&#32479;PF&#27169;&#22411;&#33539;&#22260;&#30340;&#33539;&#20363;&#65292;&#34987;&#31216;&#20026;&#21028;&#21035;&#24335;&#38544;&#31169;&#28431;&#26007;&#65288;DisPF&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02696v1 Announce Type: new  Abstract: In this study, we apply the information-theoretic Privacy Funnel (PF) model to the domain of face recognition, developing a novel method for privacy-preserving representation learning within an end-to-end training framework. Our approach addresses the trade-off between obfuscation and utility in data protection, quantified through logarithmic loss, also known as self-information loss. This research provides a foundational exploration into the integration of information-theoretic privacy principles with representation learning, focusing specifically on the face recognition systems. We particularly highlight the adaptability of our framework with recent advancements in face recognition networks, such as AdaFace and ArcFace. In addition, we introduce the Generative Privacy Funnel ($\mathsf{GenPF}$) model, a paradigm that extends beyond the traditional scope of the PF model, referred to as the Discriminative Privacy Funnel ($\mathsf{DisPF}$)
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#36716;&#25442;&#27169;&#22411;&#26500;&#24314;&#26041;&#27861;&#65292;&#32467;&#21512;&#29983;&#25104;&#21644;&#21160;&#24577;&#35266;&#28857;&#65292;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#25512;&#29702;&#65292;&#36890;&#36807;&#21387;&#32553;&#19968;&#32452;&#36716;&#25442;&#25104;&#19968;&#32452;&#35268;&#21017;&#65292;&#20801;&#35768;&#27169;&#22411;&#23637;&#31034;&#36229;&#20986;&#36755;&#20837;&#33539;&#22260;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2404.02692</link><description>&lt;p&gt;
&#22270;&#24418;&#36716;&#25442;&#35268;&#21017;&#30340;&#33258;&#21160;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Automated Inference of Graph Transformation Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02692
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#36716;&#25442;&#27169;&#22411;&#26500;&#24314;&#26041;&#27861;&#65292;&#32467;&#21512;&#29983;&#25104;&#21644;&#21160;&#24577;&#35266;&#28857;&#65292;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#25512;&#29702;&#65292;&#36890;&#36807;&#21387;&#32553;&#19968;&#32452;&#36716;&#25442;&#25104;&#19968;&#32452;&#35268;&#21017;&#65292;&#20801;&#35768;&#27169;&#22411;&#23637;&#31034;&#36229;&#20986;&#36755;&#20837;&#33539;&#22260;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#21487;&#29992;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#25512;&#21160;&#20102;&#23545;&#23500;&#26377;&#34920;&#29616;&#21147;&#27169;&#22411;&#21644;&#35745;&#31639;&#26041;&#27861;&#26085;&#30410;&#22686;&#38271;&#30340;&#38656;&#27714;&#12290;&#22270;&#24418;&#36716;&#25442;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#36716;&#25442;&#27169;&#22411;&#26500;&#24314;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#21644;&#21160;&#24577;&#35266;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#20379;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#25512;&#29702;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#25509;&#21463;&#20316;&#20026;&#21160;&#24577;&#23646;&#24615;&#30340;&#36755;&#20837;&#65292;&#32473;&#23450;&#20026;&#30001;&#26174;&#24335;&#36716;&#25442;&#32534;&#30721;&#30340;&#21160;&#24577;&#30340;&#8220;&#24555;&#29031;&#8221;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#20860;&#23481;&#30340;&#27169;&#22411;&#12290;&#33719;&#24471;&#30340;&#27169;&#22411;&#34987;&#20445;&#35777;&#26159;&#26368;&#23567;&#30340;&#65292;&#22240;&#27492;&#23558;&#35813;&#26041;&#27861;&#35268;&#33539;&#20026;&#27169;&#22411;&#21387;&#32553;&#65288;&#23558;&#19968;&#32452;&#36716;&#25442;&#21387;&#32553;&#20026;&#19968;&#32452;&#35268;&#21017;&#65289;&#30340;&#26041;&#27861;&#12290;&#21387;&#32553;&#23545;&#26377;&#25439;&#24773;&#20917;&#24456;&#23485;&#23481;&#65292;&#21363;&#20801;&#35768;&#26500;&#24314;&#30340;&#27169;&#22411;&#23637;&#31034;&#36229;&#20986;&#36755;&#20837;&#36716;&#25442;&#33539;&#22260;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#24314;&#35758;&#23436;&#25104;&#36755;&#20837;&#21160;&#24577;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02692v1 Announce Type: cross  Abstract: The explosion of data available in life sciences is fueling an increasing demand for expressive models and computational methods. Graph transformation is a model for dynamic systems with a large variety of applications. We introduce a novel method of the graph transformation model construction, combining generative and dynamical viewpoints to give a fully automated data-driven model inference method.   The method takes the input dynamical properties, given as a "snapshot" of the dynamics encoded by explicit transitions, and constructs a compatible model. The obtained model is guaranteed to be minimal, thus framing the approach as model compression (from a set of transitions into a set of rules). The compression is permissive to a lossy case, where the constructed model is allowed to exhibit behavior outside of the input transitions, thus suggesting a completion of the input dynamics.   The task of graph transformation model inference i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#39640;&#26031;&#36755;&#20837;&#19979;&#27880;&#24847;&#21147;&#24471;&#20998;&#31232;&#30095;&#24615;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#31232;&#30095;&#24615;&#30340;&#29305;&#24449;&#21450;&#20854;&#23545;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02690</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#39640;&#26031;&#20998;&#24067;&#36755;&#20837;&#19979;&#33258;&#28982;&#31232;&#30095;
&lt;/p&gt;
&lt;p&gt;
Attention is Naturally Sparse with Gaussian Distributed Input
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02690
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#39640;&#26031;&#36755;&#20837;&#19979;&#27880;&#24847;&#21147;&#24471;&#20998;&#31232;&#30095;&#24615;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#31232;&#30095;&#24615;&#30340;&#29305;&#24449;&#21450;&#20854;&#23545;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35745;&#31639;&#24378;&#24230;&#26159;&#20851;&#38190;&#29942;&#39048;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;transformer&#26550;&#26500;&#20013;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;$O(n^2)$&#22797;&#26434;&#24230;&#12290;&#31232;&#30095;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#24212;&#36816;&#32780;&#29983;&#65292;&#26088;&#22312;&#20943;&#23569;&#35745;&#31639;&#36127;&#33655;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#23545;LLMs&#20869;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#31232;&#30095;&#24615;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#26031;&#36755;&#20837;&#26694;&#26550;&#19979;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#32452;&#22522;&#30784;&#20551;&#35774;&#24182;&#37319;&#29992;&#19968;&#31181;&#31995;&#32479;&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#27880;&#24847;&#21147;&#20998;&#25968;&#31232;&#30095;&#24615;&#30340;&#20869;&#22312;&#29305;&#24449;&#21450;&#20854;&#23545;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25552;&#20379;&#20102;&#23545;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#31232;&#30095;&#24615;&#34920;&#29616;&#24418;&#24335;&#30340;&#35814;&#32454;&#29702;&#35770;&#26816;&#26597;&#65292;&#25581;&#31034;&#20102;&#22312;&#35745;&#31639;&#33410;&#32422;&#21644;&#27169;&#22411;&#26377;&#25928;&#24615;&#20043;&#38388;&#28508;&#22312;&#26435;&#34913;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02690v1 Announce Type: cross  Abstract: The computational intensity of Large Language Models (LLMs) is a critical bottleneck, primarily due to the $O(n^2)$ complexity of the attention mechanism in transformer architectures. Addressing this, sparse attention emerges as a key innovation, aiming to reduce computational load while maintaining model performance. This study presents a rigorous theoretical analysis of the sparsity in attention scores within LLMs, particularly under the framework of Gaussian inputs. By establishing a set of foundational assumptions and employing a methodical theoretical approach, we unravel the intrinsic characteristics of attention score sparsity and its implications on computational efficiency. Our main contribution lies in providing a detailed theoretical examination of how sparsity manifests in attention mechanisms, offering insights into the potential trade-offs between computational savings and model effectiveness. This work not only advances 
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#34987;&#24402;&#32435;&#21040;&#20998;&#31867;&#25511;&#21046;&#21407;&#29702;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#30340;&#20809;&#23398;&#30456;&#20114;&#20316;&#29992;&#65292;&#23637;&#31034;&#20102;&#26032;&#30340;&#26500;&#36896;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02688</link><description>&lt;p&gt;
&#22312;&#20998;&#31867;&#25511;&#21046;&#21407;&#29702;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning in Categorical Cybernetics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02688
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#34987;&#24402;&#32435;&#21040;&#20998;&#31867;&#25511;&#21046;&#21407;&#29702;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#30340;&#20809;&#23398;&#30456;&#20114;&#20316;&#29992;&#65292;&#23637;&#31034;&#20102;&#26032;&#30340;&#26500;&#36896;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#31181;&#20027;&#35201;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#36866;&#29992;&#20110;&#20998;&#31867;&#25511;&#21046;&#21407;&#29702;&#26694;&#26550;&#65292;&#21363;&#21442;&#25968;&#21270;&#30340;&#21452;&#21521;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#27492;&#21069;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#23637;&#24320;&#65292;&#20854;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#20215;&#20540;&#36845;&#20195;&#21487;&#20197;&#36890;&#36807;&#39044;&#21512;&#25104;&#29305;&#23450;&#30340;&#20809;&#23398;&#34920;&#31034;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#26500;&#36896;&#27010;&#36848;&#22914;&#19979;&#65306;&#65288;1&#65289;&#25105;&#20204;&#23558;Bellman&#31639;&#23376;&#25193;&#23637;&#21040;&#36866;&#29992;&#20110;&#21160;&#20316;&#20540;&#20989;&#25968;&#24182;&#20381;&#36182;&#20110;&#26679;&#26412;&#30340;&#21442;&#25968;&#21270;&#20809;&#23398;&#12290; &#65288;2&#65289;&#25105;&#20204;&#24212;&#29992;&#19968;&#20010;&#21487;&#34920;&#31034;&#30340;&#36870;&#21464;&#23376;&#20989;&#23376;&#65292;&#24471;&#21040;&#19968;&#20010;&#24212;&#29992;Bellman&#36845;&#20195;&#30340;&#21442;&#25968;&#21270;&#20989;&#25968;&#12290;&#65288;3&#65289;&#35813;&#21442;&#25968;&#21270;&#20989;&#25968;&#25104;&#20026;&#21478;&#19968;&#20010;&#20195;&#34920;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#20809;&#23398;&#30340;&#21453;&#21521;&#20256;&#36882;&#65292;&#36890;&#36807;&#20195;&#29702;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#22312;&#25105;&#20204;&#30340;&#26500;&#36896;&#20013;&#65292;&#21442;&#25968;&#21270;&#20809;&#23398;&#20197;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#20986;&#29616;&#65292;&#20854;&#20013;&#19968;&#31181;&#25104;&#20026;&#21478;&#19968;&#31181;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02688v1 Announce Type: new  Abstract: We show that several major algorithms of reinforcement learning (RL) fit into the framework of categorical cybernetics, that is to say, parametrised bidirectional processes. We build on our previous work in which we show that value iteration can be represented by precomposition with a certain optic. The outline of the main construction in this paper is: (1) We extend the Bellman operators to parametrised optics that apply to action-value functions and depend on a sample. (2) We apply a representable contravariant functor, obtaining a parametrised function that applies the Bellman iteration. (3) This parametrised function becomes the backward pass of another parametrised optic that represents the model, which interacts with an environment via an agent. Thus, parametrised optics appear in two different ways in our construction, with one becoming part of the other. As we show, many of the major classes of algorithms in RL can be seen as dif
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#26550;&#26500;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#24615;&#25104;&#26412;&#25512;&#26029;&#21644;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#20043;&#38388;&#20849;&#20139;&#32452;&#20214;&#30340;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.02684</link><description>&lt;p&gt;
&#36328;&#26550;&#26500;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#32447;&#24615;&#25104;&#26412;&#25512;&#26029;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02684
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#26550;&#26500;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#24615;&#25104;&#26412;&#25512;&#26029;&#21644;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#20043;&#38388;&#20849;&#20139;&#32452;&#20214;&#30340;&#26435;&#37325;&#65292;&#20197;&#25552;&#39640;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22810;&#31181;&#26550;&#26500;&#26469;&#36890;&#36807;&#25913;&#21464;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#35774;&#35745;&#23454;&#29616;&#32447;&#24615;&#25104;&#26412;&#25512;&#26029;(LCI)&#20197;&#25552;&#39640;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#26041;&#27861;&#26159;&#29366;&#24577;&#31354;&#38388;&#26426;&#22120;&#65288;SSMs&#65289;&#26550;&#26500;&#65292;&#23427;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#19982;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26550;&#26500;&#26356;&#25913;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#23436;&#20840;&#39044;&#35757;&#32451;&#26435;&#37325;&#65292;&#36825;&#32473;&#24076;&#26395;&#20351;&#29992;&#26032;&#26550;&#26500;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#24102;&#26469;&#20102;&#24040;&#22823;&#25104;&#26412;&#12290;&#21463;&#20256;&#32479;&#32447;&#24615;&#27880;&#24847;&#21147;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#26550;&#26500;&#36801;&#31227;&#23398;&#20064;(XATL)&#65292;&#20854;&#20013;LCI&#21644;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#22120;&#20043;&#38388;&#30340;&#20849;&#20139;&#32452;&#20214;&#30340;&#26435;&#37325;&#65292;&#22914;&#23618;&#35268;&#33539;&#12289;MLP&#12289;&#36755;&#20837;/&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02684v1 Announce Type: cross  Abstract: Recently, multiple architectures has been proposed to improve the efficiency of the Transformer Language Models through changing the design of the self-attention block to have a linear-cost inference (LCI). A notable approach in this realm is the State-Space Machines (SSMs) architecture, which showed on-par performance on language modeling tasks with the self-attention transformers. However, such an architectural change requires a full pretraining of the weights from scratch, which incurs a huge cost to researchers and practitioners who want to use the new architectures. In the more traditional linear attention works, it has been proposed to approximate full attention with linear attention by swap-and-finetune framework. Motivated by this approach, we propose Cross-Architecture Transfer Learning (XATL), in which the weights of the shared components between LCI and self-attention-based transformers, such as layernorms, MLPs, input/outpu
&lt;/p&gt;</description></item><item><title>&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30740;&#31350;&#23545;&#25239;&#25915;&#20987;&#65292;&#21457;&#29616;&#23545;&#25239;&#26679;&#26412;&#30340;&#23884;&#20837;&#32500;&#24230;&#19982;&#20854;&#24433;&#21709;&#20043;&#38388;&#23384;&#22312;&#38750;&#24120;&#24378;&#28872;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02660</link><description>&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#19982;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#30340;&#32500;&#24230;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Dimensionality in Text Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02660
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30740;&#31350;&#23545;&#25239;&#25915;&#20987;&#65292;&#21457;&#29616;&#23545;&#25239;&#26679;&#26412;&#30340;&#23884;&#20837;&#32500;&#24230;&#19982;&#20854;&#24433;&#21709;&#20043;&#38388;&#23384;&#22312;&#38750;&#24120;&#24378;&#28872;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36973;&#21463;&#23545;&#25239;&#25915;&#20987;&#24050;&#25104;&#20026;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#29992;&#20363;&#20013;&#20154;&#24037;&#26234;&#33021;&#37319;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#36825;&#20123;&#25915;&#20987;&#36890;&#36807;&#22312;&#27979;&#35797;&#26679;&#26412;&#20013;&#24341;&#20837;&#24494;&#23567;&#19988;&#32467;&#26500;&#21270;&#30340;&#25200;&#21160;&#25110;&#25913;&#21464;&#65292;&#20174;&#26681;&#26412;&#19978;&#21066;&#24369;&#20102;&#39640;&#24615;&#33021;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#36843;&#20351;&#20854;&#36827;&#34892;&#35823;&#20998;&#31867;&#12290;&#26412;&#25991;&#30740;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#25506;&#35752;&#20102;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#21407;&#22240;&#65292;&#29305;&#21035;&#26159;&#19982;&#27169;&#22411;&#22266;&#26377;&#32500;&#24230;&#24615;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#26159;&#23545;&#25239;&#26679;&#26412;&#30340;&#23884;&#20837;&#32500;&#24230;&#19982;&#20854;&#24433;&#21709;&#20043;&#38388;&#23384;&#22312;&#38750;&#24120;&#24378;&#28872;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02660v1 Announce Type: new  Abstract: Adversarial attacks on machine learning algorithms have been a key deterrent to the adoption of AI in many real-world use cases. They significantly undermine the ability of high-performance neural networks by forcing misclassifications. These attacks introduce minute and structured perturbations or alterations in the test samples, imperceptible to human annotators in general, but trained neural networks and other models are sensitive to it. Historically, adversarial attacks have been first identified and studied in the domain of image processing. In this paper, we study adversarial examples in the field of natural language processing, specifically text classification tasks. We investigate the reasons for adversarial vulnerability, particularly in relation to the inherent dimensionality of the model. Our key finding is that there is a very strong correlation between the embedding dimensionality of the adversarial samples and their effecti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#26410;&#39044;&#26009;&#21040;&#30340;&#20559;&#35265;&#30340;&#26032;&#36884;&#24452;&#65292;&#30528;&#37325;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02650</link><description>&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26410;&#39044;&#26009;&#21040;&#30340;&#20559;&#35265;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards detecting unanticipated bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#26410;&#39044;&#26009;&#21040;&#30340;&#20559;&#35265;&#30340;&#26032;&#36884;&#24452;&#65292;&#30528;&#37325;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#19968;&#24180;&#20013;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#24182;&#23637;&#29616;&#20986;&#19982;&#20197;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#31867;&#20284;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#20998;&#26512;&#21644;&#37327;&#21270;&#36825;&#20123;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#21450;&#20854;&#23545;&#36825;&#20123;&#27169;&#22411;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#21046;&#23450;&#20943;&#36731;&#31574;&#30053;&#12290;&#36825;&#39033;&#30740;&#31350;&#20027;&#35201;&#38024;&#23545;&#19982;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#26063;&#35028;&#21644;&#35821;&#35328;&#30456;&#20851;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#24456;&#26126;&#26174;&#65292;LLMs&#20063;&#21463;&#21040;&#20854;&#20182;&#19981;&#22826;&#26126;&#26174;&#30340;&#20869;&#38544;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#36890;&#24120;&#30340;&#19981;&#36879;&#26126;&#24615;&#20351;&#24471;&#26816;&#27979;&#36825;&#20123;&#20559;&#35265;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#28508;&#22312;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;LLMs&#20013;&#26816;&#27979;&#36825;&#20123;&#26410;&#39044;&#26009;&#21040;&#30340;&#20559;&#35265;&#30340;&#26032;&#36884;&#24452;&#65292;&#20855;&#20307;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#35780;&#20272;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02650v1 Announce Type: cross  Abstract: Over the last year, Large Language Models (LLMs) like ChatGPT have become widely available and have exhibited fairness issues similar to those in previous machine learning systems. Current research is primarily focused on analyzing and quantifying these biases in training data and their impact on the decisions of these models, alongside developing mitigation strategies. This research largely targets well-known biases related to gender, race, ethnicity, and language. However, it is clear that LLMs are also affected by other, less obvious implicit biases. The complex and often opaque nature of these models makes detecting such biases challenging, yet this is crucial due to their potential negative impact in various applications. In this paper, we explore new avenues for detecting these unanticipated biases in LLMs, focusing specifically on Uncertainty Quantification and Explainable AI methods. These approaches aim to assess the certainty
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#20915;&#31574;&#38382;&#39064;&#26102;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38598;&#25104;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21040;&#27748;&#26222;&#26862;&#25277;&#26679;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02649</link><description>&lt;p&gt;
&#35770;&#20915;&#31574;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24615;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On the Importance of Uncertainty in Decision-Making with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#20915;&#31574;&#38382;&#39064;&#26102;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38598;&#25104;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21040;&#27748;&#26222;&#26862;&#25277;&#26679;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#20915;&#31574;&#38382;&#39064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#29992;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#24050;&#32463;&#25104;&#20026;&#24120;&#24577;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20013;&#27809;&#26377;&#20219;&#20309;&#19968;&#20010;&#39069;&#22806;&#30340;&#38454;&#27573;&#29992;&#20110;&#20272;&#35745;&#20195;&#29702;&#22312;&#20915;&#31574;&#20219;&#21153;&#20013;&#23545;&#19990;&#30028;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#20851;&#27880;&#20197;&#33258;&#28982;&#35821;&#35328;&#20026;&#36755;&#20837;&#30340;&#22522;&#26412;&#20915;&#31574;&#26694;&#26550;&#20043;&#19968;&#65292;&#21363;&#19978;&#19979;&#25991;&#33218;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#20449;&#24687;&#21253;&#25324;&#25991;&#26412;&#12290;&#20316;&#20026;&#27809;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;&#20195;&#34920;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#24102;&#26377;&#36138;&#23146;&#31574;&#30053;&#30340;LLM&#33218;&#65292;&#35813;&#31574;&#30053;&#36873;&#25321;&#23545;&#24212;&#20110;&#26368;&#22823;&#39044;&#27979;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#25105;&#20204;&#23558;&#27492;&#22522;&#20934;&#19982;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#24615;&#38598;&#25104;&#21040;&#27748;&#26222;&#26862;&#25277;&#26679;&#31574;&#30053;&#20013;&#31215;&#26497;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;LLM&#33218;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#37319;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20363;&#22914;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#12289;&#36749;&#23398;&#21644;Epin&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02649v1 Announce Type: new  Abstract: We investigate the role of uncertainty in decision-making problems with natural language as input. For such tasks, using Large Language Models as agents has become the norm. However, none of the recent approaches employ any additional phase for estimating the uncertainty the agent has about the world during the decision-making task. We focus on a fundamental decision-making framework with natural language as input, which is the one of contextual bandits, where the context information consists of text. As a representative of the approaches with no uncertainty estimation, we consider an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward. We compare this baseline to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy. We employ different techniques for uncertainty estimation, such as Laplace Approximation, Dropout, and Epin
&lt;/p&gt;</description></item><item><title>Effector&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#21306;&#22495;&#29305;&#24449;&#25928;&#26524;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25928;&#26524;&#26469;&#38477;&#20302;&#20840;&#23616;&#29305;&#24449;&#25928;&#26524;&#26041;&#27861;&#20013;&#21487;&#33021;&#30340;&#24322;&#36136;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02629</link><description>&lt;p&gt;
Effector: &#19968;&#20010;&#29992;&#20110;&#21306;&#22495;&#35299;&#37322;&#30340;Python&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
Effector: A Python package for regional explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02629
&lt;/p&gt;
&lt;p&gt;
Effector&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#21306;&#22495;&#29305;&#24449;&#25928;&#26524;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25928;&#26524;&#26469;&#38477;&#20302;&#20840;&#23616;&#29305;&#24449;&#25928;&#26524;&#26041;&#27861;&#20013;&#21487;&#33021;&#30340;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#23616;&#29305;&#24449;&#25928;&#26524;&#26041;&#27861;&#35299;&#37322;&#19968;&#20010;&#36755;&#20986;&#27169;&#22411;&#65292;&#27599;&#20010;&#29305;&#24449;&#23545;&#24212;&#19968;&#20010;&#22270;&#12290;&#35813;&#22270;&#26174;&#31034;&#29305;&#24449;&#23545;&#36755;&#20986;&#30340;&#24179;&#22343;&#25928;&#26524;&#65292;&#20363;&#22914;&#24180;&#40836;&#23545;&#24180;&#25910;&#20837;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#30001;&#24322;&#36136;&#23616;&#37096;&#25928;&#26524;&#25512;&#23548;&#20986;&#24179;&#22343;&#25928;&#26524;&#26102;&#65292;&#24179;&#22343;&#25928;&#26524;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#65292;&#21363;&#26126;&#26174;&#20559;&#31163;&#24179;&#22343;&#20540;&#12290;&#20026;&#20102;&#20943;&#23569;&#24322;&#36136;&#24615;&#65292;&#21306;&#22495;&#25928;&#26524;&#20026;&#27599;&#20010;&#29305;&#24449;&#25552;&#20379;&#22810;&#20010;&#22270;&#65292;&#27599;&#20010;&#22270;&#20195;&#34920;&#29305;&#23450;&#23376;&#31354;&#38388;&#20869;&#30340;&#24179;&#22343;&#25928;&#26524;&#12290;&#20026;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#23376;&#31354;&#38388;&#34987;&#23450;&#20041;&#20026;&#30001;&#36923;&#36753;&#35268;&#21017;&#38142;&#23450;&#20041;&#30340;&#36229;&#30697;&#24418;&#65292;&#20363;&#22914;&#24180;&#40836;&#23545;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;&#24180;&#25910;&#20837;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#19981;&#21516;&#19987;&#19994;&#32463;&#39564;&#27700;&#24179;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Effector&#65292;&#19968;&#20010;&#33268;&#21147;&#20110;&#21306;&#22495;&#29305;&#24449;&#25928;&#26524;&#30340;Python&#24211;&#12290;Effector&#23454;&#29616;&#20102;&#19968;&#20123;&#25104;&#29087;&#30340;&#20840;&#23616;&#25928;&#26524;&#26041;&#27861;&#65292;&#35780;&#20272;&#27599;&#31181;&#26041;&#27861;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20379;&#21306;&#22495;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02629v1 Announce Type: new  Abstract: Global feature effect methods explain a model outputting one plot per feature. The plot shows the average effect of the feature on the output, like the effect of age on the annual income. However, average effects may be misleading when derived from local effects that are heterogeneous, i.e., they significantly deviate from the average. To decrease the heterogeneity, regional effects provide multiple plots per feature, each representing the average effect within a specific subspace. For interpretability, subspaces are defined as hyperrectangles defined by a chain of logical rules, like age's effect on annual income separately for males and females and different levels of professional experience. We introduce Effector, a Python library dedicated to regional feature effects. Effector implements well-established global effect methods, assesses the heterogeneity of each method and, based on that, provides regional effects. Effector automatica
&lt;/p&gt;</description></item><item><title>Diff-Comb Explainer&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#40657;&#30418;&#32452;&#21512;&#27714;&#35299;&#22120;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#19981;&#38656;&#35201;&#23545;&#35821;&#20041;&#32422;&#26463;&#36827;&#34892;&#36830;&#32493;&#25918;&#26494;&#65292;&#30456;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2404.02625</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#21487;&#24494;&#20998;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02625
&lt;/p&gt;
&lt;p&gt;
Diff-Comb Explainer&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#40657;&#30418;&#32452;&#21512;&#27714;&#35299;&#22120;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#19981;&#38656;&#35201;&#23545;&#35821;&#20041;&#32422;&#26463;&#36827;&#34892;&#36830;&#32493;&#25918;&#26494;&#65292;&#30456;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Integer Linear Programming&#65288;ILP&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#36827;&#34892;&#31934;&#30830;&#32467;&#26500;&#21644;&#35821;&#20041;&#32422;&#26463;&#32534;&#30721;&#30340;&#27491;&#24335;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ILP&#26694;&#26550;&#26159;&#19981;&#21487;&#24494;&#20998;&#30340;&#65292;&#36825;&#32473;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36830;&#32493;&#35821;&#35328;&#34920;&#31034;&#30340;&#25972;&#21512;&#24102;&#26469;&#20102;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21517;&#20026;Diff-Comb Explainer&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#40657;&#30418;&#32452;&#21512;&#27714;&#35299;&#22120;&#65288;DBCS&#65289;&#30340;&#35299;&#37322;&#22411;NLI&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#12290;&#19982;&#29616;&#26377;&#30340;&#31070;&#32463;&#31526;&#21495;&#27714;&#35299;&#22120;&#19981;&#21516;&#65292;Diff-Comb Explainer&#19981;&#38656;&#35201;&#23545;&#35821;&#20041;&#32422;&#26463;&#36827;&#34892;&#36830;&#32493;&#25918;&#26494;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#12289;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#22320;&#23558;&#31070;&#32463;&#34920;&#31034;&#34701;&#20837;&#21040;ILP&#20844;&#24335;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;ILP&#27714;&#35299;&#22120;&#12289;&#31070;&#32463;&#31526;&#21495;&#40657;&#30418;&#27714;&#35299;&#22120;&#21644;Trans&#30456;&#27604;&#65292;Diff-Comb Explainer&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02625v1 Announce Type: cross  Abstract: Integer Linear Programming (ILP) has been proposed as a formalism for encoding precise structural and semantic constraints for Natural Language Inference (NLI). However, traditional ILP frameworks are non-differentiable, posing critical challenges for the integration of continuous language representations based on deep learning. In this paper, we introduce a novel approach, named Diff-Comb Explainer, a neuro-symbolic architecture for explanation-based NLI based on Differentiable BlackBox Combinatorial Solvers (DBCS). Differently from existing neuro-symbolic solvers, Diff-Comb Explainer does not necessitate a continuous relaxation of the semantic constraints, enabling a direct, more precise, and efficient incorporation of neural representations into the ILP formulation. Our experiments demonstrate that Diff-Comb Explainer achieves superior performance when compared to conventional ILP solvers, neuro-symbolic black-box solvers, and Trans
&lt;/p&gt;</description></item><item><title>&#20174;&#39640;&#26031;&#22270;&#24418;&#24179;&#31283;&#20449;&#21495;&#20013;&#23398;&#20064;&#36793;&#32536;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22810;&#39033;&#24335;&#22270;&#24418;Lasso&#65288;PGL&#65289;&#65292;&#36890;&#36807;&#23558;&#22270;&#24418;Lasso&#30340;&#20248;&#21183;&#19982;&#26356;&#20840;&#38754;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#24314;&#27169;&#33410;&#28857;&#20851;&#31995;&#26102;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#36890;&#36807;&#20302;&#22797;&#26434;&#24230;&#31639;&#27861;&#35299;&#20915;&#20102;&#22797;&#26434;&#21644;&#38750;&#20984;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#35780;&#20272;&#34920;&#26126;&#20854;&#20248;&#20110;&#20960;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02621</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#22270;&#24418;Lasso&#65306;&#20174;&#39640;&#26031;&#22270;&#24418;&#24179;&#31283;&#20449;&#21495;&#20013;&#23398;&#20064;&#36793;&#32536;
&lt;/p&gt;
&lt;p&gt;
Polynomial Graphical Lasso: Learning Edges from Gaussian Graph-Stationary Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02621
&lt;/p&gt;
&lt;p&gt;
&#20174;&#39640;&#26031;&#22270;&#24418;&#24179;&#31283;&#20449;&#21495;&#20013;&#23398;&#20064;&#36793;&#32536;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22810;&#39033;&#24335;&#22270;&#24418;Lasso&#65288;PGL&#65289;&#65292;&#36890;&#36807;&#23558;&#22270;&#24418;Lasso&#30340;&#20248;&#21183;&#19982;&#26356;&#20840;&#38754;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#24314;&#27169;&#33410;&#28857;&#20851;&#31995;&#26102;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#36890;&#36807;&#20302;&#22797;&#26434;&#24230;&#31639;&#27861;&#35299;&#20915;&#20102;&#22797;&#26434;&#21644;&#38750;&#20984;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#35780;&#20272;&#34920;&#26126;&#20854;&#20248;&#20110;&#20960;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#22810;&#39033;&#24335;&#22270;&#24418;Lasso&#65288;PGL&#65289;&#65292;&#29992;&#20110;&#20174;&#33410;&#28857;&#20449;&#21495;&#20013;&#23398;&#20064;&#22270;&#24418;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#23558;&#20449;&#21495;&#24314;&#27169;&#20026;&#39640;&#26031;&#21644;&#22312;&#22270;&#20013;&#24179;&#31283;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19968;&#31181;&#23558;&#22270;&#24418;Lasso&#30340;&#20248;&#21183;&#19982;&#26356;&#20840;&#38754;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#22270;&#24418;&#23398;&#20064;&#20844;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#31934;&#24230;&#30697;&#38453;&#21487;&#20197;&#37319;&#29992;&#25152;&#38656;&#22270;&#24418;&#30340;&#20219;&#20309;&#22810;&#39033;&#24335;&#24418;&#24335;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#23545;&#33410;&#28857;&#20851;&#31995;&#24314;&#27169;&#30340;&#28789;&#27963;&#24615;&#12290;&#30001;&#20110;&#25152;&#24471;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21644;&#38750;&#20984;&#24615;&#65292;&#25105;&#20204;&#65288;i&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#31639;&#27861;&#65292;&#20132;&#26367;&#20272;&#35745;&#22270;&#24418;&#21644;&#31934;&#24230;&#30697;&#38453;&#65292;&#24182;&#65288;ii&#65289;&#34920;&#24449;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#30340;&#25968;&#20540;&#27169;&#25311;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#35780;&#20272;&#20102;PGL&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20960;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02621v1 Announce Type: cross  Abstract: This paper introduces Polynomial Graphical Lasso (PGL), a new approach to learning graph structures from nodal signals. Our key contribution lies in modeling the signals as Gaussian and stationary on the graph, enabling the development of a graph-learning formulation that combines the strengths of graphical lasso with a more encompassing model. Specifically, we assume that the precision matrix can take any polynomial form of the sought graph, allowing for increased flexibility in modeling nodal relationships. Given the resulting complexity and nonconvexity of the resulting optimization problem, we (i) propose a low-complexity algorithm that alternates between estimating the graph and precision matrices, and (ii) characterize its convergence. We evaluate the performance of PGL through comprehensive numerical simulations using both synthetic and real data, demonstrating its superiority over several alternatives. Overall, this approach pr
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#27450;&#35784;&#20132;&#26131;&#35782;&#21035;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#27450;&#35784;&#26816;&#27979;&#24182;&#30830;&#20445;&#20102;&#25968;&#25454;&#26426;&#23494;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02595</link><description>&lt;p&gt;
QFNN-FFD&#65306;&#29992;&#20110;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#30340;&#37327;&#23376;&#32852;&#37030;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02595
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#27450;&#35784;&#20132;&#26131;&#35782;&#21035;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#27450;&#35784;&#26816;&#27979;&#24182;&#30830;&#20445;&#20102;&#25968;&#25454;&#26426;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)&#65292;&#36825;&#26159;&#19968;&#20010;&#34701;&#21512;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21069;&#27839;&#26694;&#26550;&#65292;&#29992;&#20110;&#21019;&#26032;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#12290;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;FL&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;QFNN-FFD&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#35782;&#21035;&#27450;&#35784;&#20132;&#26131;&#30340;&#26041;&#27861;&#12290;&#22312;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#23454;&#26045;&#21452;&#38454;&#27573;&#35757;&#32451;&#27169;&#22411;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24615;&#33021;&#26041;&#27861;&#12290;QFNN-FFD&#26174;&#33879;&#25913;&#36827;&#20102;&#27450;&#35784;&#26816;&#27979;&#24182;&#30830;&#20445;&#20102;&#25968;&#25454;&#26426;&#23494;&#24615;&#65292;&#26631;&#24535;&#30528;&#37329;&#34701;&#31185;&#25216;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#22823;&#36827;&#27493;&#65292;&#24182;&#20026;&#20197;&#38544;&#31169;&#20026;&#37325;&#28857;&#30340;&#27450;&#35784;&#26816;&#27979;&#24314;&#31435;&#20102;&#26032;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02595v1 Announce Type: cross  Abstract: This study introduces the Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine Learning (QML) and quantum computing with Federated Learning (FL) to innovate financial fraud detection. Using quantum technologies' computational power and FL's data privacy, QFNN-FFD presents a secure, efficient method for identifying fraudulent transactions. Implementing a dual-phase training model across distributed clients surpasses existing methods in performance. QFNN-FFD significantly improves fraud detection and ensures data confidentiality, marking a significant advancement in fintech solutions and establishing a new standard for privacy-focused fraud detection.
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#32773;&#20542;&#21521;&#20110;&#20272;&#35745;&#20540;&#20026;&#27491;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36991;&#24320;&#20272;&#35745;&#20540;&#20026;&#36127;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23548;&#33268;&#39640;&#20272;&#35823;&#24046;&#32416;&#27491;&#20294;&#20302;&#20272;&#35823;&#24046;&#26080;&#27861;&#32416;&#27491;&#65292;&#21516;&#26102;&#30740;&#31350;&#21457;&#29616;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#36127;&#20272;&#35745;&#20250;&#23548;&#33268;&#36873;&#25321;&#36739;&#23569;&#30340;&#26679;&#26412;&#37327;&#65292;&#36825;&#31181;&#28040;&#26497;&#20559;&#35265;&#21516;&#26679;&#23384;&#22312;&#20110;&#36125;&#21494;&#26031;&#23398;&#20064;&#32773;&#20013;&#12290;</title><link>https://arxiv.org/abs/2404.02591</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#37319;&#26679;&#25919;&#31574;&#24847;&#21619;&#30528;&#23384;&#22312;&#20559;&#35265;&#20449;&#24565;: &#19968;&#33324;&#21270;&#30340;&#28909;&#28809;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Adaptive Sampling Policies Imply Biased Beliefs: A Generalization of the Hot Stove Effect
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02591
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#32773;&#20542;&#21521;&#20110;&#20272;&#35745;&#20540;&#20026;&#27491;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36991;&#24320;&#20272;&#35745;&#20540;&#20026;&#36127;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23548;&#33268;&#39640;&#20272;&#35823;&#24046;&#32416;&#27491;&#20294;&#20302;&#20272;&#35823;&#24046;&#26080;&#27861;&#32416;&#27491;&#65292;&#21516;&#26102;&#30740;&#31350;&#21457;&#29616;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#36127;&#20272;&#35745;&#20250;&#23548;&#33268;&#36873;&#25321;&#36739;&#23569;&#30340;&#26679;&#26412;&#37327;&#65292;&#36825;&#31181;&#28040;&#26497;&#20559;&#35265;&#21516;&#26679;&#23384;&#22312;&#20110;&#36125;&#21494;&#26031;&#23398;&#20064;&#32773;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#28809;&#25928;&#24212;&#26159;&#30001;&#20110;&#23398;&#20064;&#30340;&#36866;&#24212;&#24615;&#29305;&#24615;&#25152;&#23548;&#33268;&#30340;&#19968;&#31181;&#28040;&#26497;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36861;&#27714;&#20272;&#35745;&#20540;&#20026;&#27491;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36991;&#24320;&#20272;&#35745;&#20540;&#20026;&#36127;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#23398;&#20064;&#31639;&#27861;&#23558;&#32416;&#27491;&#39640;&#20272;&#35823;&#24046;&#65292;&#20294;&#26080;&#27861;&#32416;&#27491;&#20302;&#20272;&#35823;&#24046;&#12290;&#26412;&#25991;&#23558;&#28909;&#28809;&#25928;&#24212;&#30340;&#29702;&#35770;&#25512;&#24191;&#21040;&#36127;&#20272;&#35745;&#19981;&#19968;&#23450;&#23548;&#33268;&#36991;&#20813;&#32780;&#26159;&#23548;&#33268;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#65288;&#21363;&#65292;&#22914;&#26524;B&#34987;&#35748;&#20026;&#26159;&#27425;&#20248;&#30340;&#65292;&#23398;&#20064;&#32773;&#20250;&#36873;&#25321;&#26356;&#23569;&#30340;&#26367;&#20195;&#26041;&#26696;B&#32780;&#38750;&#23436;&#20840;&#36991;&#20813;B&#65289;&#12290;&#25105;&#20204;&#24418;&#24335;&#19978;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#28040;&#26497;&#20559;&#35265;&#20173;&#28982;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#36125;&#21494;&#26031;&#23398;&#20064;&#32773;&#23384;&#22312;&#28040;&#26497;&#20559;&#35265;&#65292;&#21363;&#22823;&#22810;&#25968;&#36825;&#26679;&#30340;&#23398;&#20064;&#32773;&#20302;&#20272;&#26367;&#20195;&#26041;&#26696;&#30340;&#39044;&#26399;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02591v1 Announce Type: new  Abstract: The Hot Stove Effect is a negativity bias resulting from the adaptive character of learning. The mechanism is that learning algorithms that pursue alternatives with positive estimated values, but avoid alternatives with negative estimated values, will correct errors of overestimation but fail to correct errors of underestimation. Here, we generalize the theory behind the Hot Stove Effect to settings in which negative estimates do not necessarily lead to avoidance but to a smaller sample size (i.e., a learner selects fewer of alternative B if B is believed to be inferior but does not entirely avoid B). We formally demonstrate that the negativity bias remains in this set-up. We also show there is a negativity bias for Bayesian learners in the sense that most such learners underestimate the expected value of an alternative.
&lt;/p&gt;</description></item><item><title>TranSDDP&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21019;&#26032;&#20998;&#38454;&#20998;&#35299;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#27169;&#22411;&#30340;&#32467;&#26500;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#24207;&#36143;&#26041;&#27861;&#26469;&#36924;&#36817;&#20540;&#20989;&#25968;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#22810;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02583</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#20998;&#38454;&#20998;&#35299;&#29992;&#20110;&#22823;&#35268;&#27169;&#22810;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Stagewise Decomposition for Large-Scale Multistage Stochastic Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02583
&lt;/p&gt;
&lt;p&gt;
TranSDDP&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21019;&#26032;&#20998;&#38454;&#20998;&#35299;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#27169;&#22411;&#30340;&#32467;&#26500;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#24207;&#36143;&#26041;&#27861;&#26469;&#36924;&#36817;&#20540;&#20989;&#25968;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#22810;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#65288;MSP&#65289;&#38382;&#39064;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#24120;&#29992;&#30340;&#20998;&#38454;&#20998;&#35299;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#23545;&#20598;&#21160;&#24577;&#35268;&#21010;&#65288;SDDP&#65289;&#65292;&#38543;&#30528;&#23376;&#38382;&#39064;&#35268;&#27169;&#21644;&#38382;&#39064;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#36807;&#36880;&#27493;&#32047;&#31215;&#26469;&#33258;&#20998;&#38454;&#23376;&#38382;&#39064;&#21407;&#22987;&#21644;&#23545;&#20598;&#35299;&#30340;&#27425;&#26799;&#24230;&#20999;&#21106;&#24179;&#38754;&#65292;&#23558;&#20540;&#20989;&#25968;&#36817;&#20284;&#20026;&#20998;&#27573;&#32447;&#24615;&#20984;&#20989;&#25968;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#20998;&#38454;&#20998;&#35299;&#31639;&#27861;TranSDDP&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#21033;&#29992;Transformer&#27169;&#22411;&#30340;&#32467;&#26500;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#24207;&#36143;&#26041;&#27861;&#65292;&#29992;&#20110;&#25972;&#21512;&#27425;&#26799;&#24230;&#20999;&#21106;&#24179;&#38754;&#20197;&#36924;&#36817;&#20540;&#20989;&#25968;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;TranSDDP&#22312;&#22788;&#29702;MSP&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23427;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#20998;&#27573;&#32447;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02583v1 Announce Type: new  Abstract: Solving large-scale multistage stochastic programming (MSP) problems poses a significant challenge as commonly used stagewise decomposition algorithms, including stochastic dual dynamic programming (SDDP), face growing time complexity as the subproblem size and problem count increase. Traditional approaches approximate the value functions as piecewise linear convex functions by incrementally accumulating subgradient cutting planes from the primal and dual solutions of stagewise subproblems. Recognizing these limitations, we introduce TranSDDP, a novel Transformer-based stagewise decomposition algorithm. This innovative approach leverages the structural advantages of the Transformer model, implementing a sequential method for integrating subgradient cutting planes to approximate the value function. Through our numerical experiments, we affirm TranSDDP's effectiveness in addressing MSP problems. It efficiently generates a piecewise linear 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#22870;&#21169;&#24037;&#31243;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#26469;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02577</link><description>&lt;p&gt;
&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#22870;&#21169;&#24037;&#31243;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#35299;&#20915;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02577
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#22870;&#21169;&#24037;&#31243;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#26469;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#21407;&#21017;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#22870;&#21169;&#24037;&#31243;&#35757;&#32451;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#20195;&#29702;&#65292;&#26469;&#20248;&#21270;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#39640;&#21534;&#21520;&#37327;&#24223;&#29289;&#20998;&#31867;&#35774;&#26045;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#26377;&#25928;&#24179;&#34913;&#25805;&#20316;&#23433;&#20840;&#24615;&#12289;&#20248;&#21270;&#23481;&#37327;&#21644;&#26368;&#23567;&#21270;&#36164;&#28304;&#20351;&#29992;&#31561;&#31454;&#20105;&#24615;&#30446;&#26631;&#30340;&#25361;&#25112;&#12290;&#19968;&#20010;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#22522;&#26412;&#20195;&#29702;&#22312;&#36825;&#20123;&#22810;&#37325;&#26631;&#20934;&#19978;&#22833;&#36133;&#35299;&#20915;&#38382;&#39064;&#65292;&#22240;&#20026;&#20854;&#22266;&#26377;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02577v1 Announce Type: new  Abstract: We present a proximal policy optimization (PPO) agent trained through curriculum learning (CL) principles and meticulous reward engineering to optimize a real-world high-throughput waste sorting facility. Our work addresses the challenge of effectively balancing the competing objectives of operational safety, volume optimization, and minimizing resource usage. A vanilla agent trained from scratch on these multiple criteria fails to solve the problem due to its inherent complexities. This problem is particularly difficult due to the environment's extremely delayed rewards with long time horizons and class (or action) imbalance, with important actions being infrequent in the optimal policy. This forces the agent to anticipate long-term action consequences and prioritize rare but rewarding behaviours, creating a non-trivial reinforcement learning task. Our five-stage CL approach tackles these challenges by gradually increasing the complexit
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27969;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#37327;&#23398;&#20064;&#23454;&#29616;&#25345;&#32493;&#27169;&#22411;&#36866;&#24212;&#65292;&#36873;&#25321;&#27599;&#20010;&#31867;&#21035;&#30340;&#20195;&#34920;&#24615;&#22270;&#65292;&#24182;&#21019;&#24314;&#22270;&#23884;&#20837;&#65292;&#20197;&#35299;&#20915;&#22270;&#27969;&#20998;&#31867;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02572</link><description>&lt;p&gt;
&#20855;&#26377;&#27010;&#24565;&#28418;&#31227;&#26816;&#27979;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#23884;&#20837;&#30340;&#22270;&#27969;&#20998;&#31867;&#30340;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incremental Learning with Concept Drift Detection and Prototype-based Embeddings for Graph Stream Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02572
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27969;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#37327;&#23398;&#20064;&#23454;&#29616;&#25345;&#32493;&#27169;&#22411;&#36866;&#24212;&#65292;&#36873;&#25321;&#27599;&#20010;&#31867;&#21035;&#30340;&#20195;&#34920;&#24615;&#22270;&#65292;&#24182;&#21019;&#24314;&#22270;&#23884;&#20837;&#65292;&#20197;&#35299;&#20915;&#22270;&#27969;&#20998;&#31867;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27969;&#25366;&#25496;&#26088;&#22312;&#20174;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#27969;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#38750;&#38745;&#24577;&#29615;&#22659;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#25351;&#38543;&#26102;&#38388;&#25913;&#21464;&#30340;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#27010;&#24565;&#28418;&#31227;&#12290;&#22270;&#32467;&#26500;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24314;&#27169;&#24037;&#20855;&#65292;&#29992;&#20110;&#34920;&#31034;&#22797;&#26434;&#31995;&#32479;&#65292;&#27604;&#22914;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#31995;&#32479;&#21644;&#31038;&#20132;&#32593;&#32476;&#12290;&#20174;&#22270;&#27969;&#20013;&#23398;&#20064;&#21464;&#24471;&#24517;&#19981;&#21487;&#23569;&#65292;&#20197;&#20102;&#35299;&#22270;&#32467;&#26500;&#30340;&#21160;&#24577;&#24182;&#20419;&#36827;&#26126;&#26234;&#20915;&#31574;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22270;&#27969;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20135;&#29983;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#22270;&#30340;&#19968;&#33324;&#35774;&#32622;&#19979;&#36816;&#34892;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22686;&#37327;&#23398;&#20064;&#36827;&#34892;&#25345;&#32493;&#27169;&#22411;&#36866;&#24212;&#65292;&#20026;&#27599;&#20010;&#31867;&#21035;&#36873;&#25321;&#20195;&#34920;&#24615;&#22270;&#65288;&#21407;&#22411;&#65289;&#24182;&#21019;&#24314;&#22270;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21253;&#21547;&#22522;&#20110;&#25439;&#22833;&#30340;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02572v1 Announce Type: new  Abstract: Data stream mining aims at extracting meaningful knowledge from continually evolving data streams, addressing the challenges posed by nonstationary environments, particularly, concept drift which refers to a change in the underlying data distribution over time. Graph structures offer a powerful modelling tool to represent complex systems, such as, critical infrastructure systems and social networks. Learning from graph streams becomes a necessity to understand the dynamics of graph structures and to facilitate informed decision-making. This work introduces a novel method for graph stream classification which operates under the general setting where a data generating process produces graphs with varying nodes and edges over time. The method uses incremental learning for continual model adaptation, selecting representative graphs (prototypes) for each class, and creating graph embeddings. Additionally, it incorporates a loss-based concept 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#24341;&#23548;&#38750;&#32447;&#24615;&#22238;&#24402;&#26641;&#26469;&#36817;&#20284;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#12289;&#26088;&#22312;&#35299;&#20915;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#30340;&#30005;&#21147;&#31995;&#32479;&#26242;&#24577;&#31283;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02555</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#19987;&#23478;&#24341;&#23548;&#31070;&#32463;&#22238;&#24402;&#26641;&#30340;&#21487;&#35299;&#37322;&#30005;&#21147;&#31995;&#32479;&#26242;&#24577;&#31283;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Power System Transient Stability Assessment Method with Expert Guiding Neural-Regression-Tree
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02555
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#24341;&#23548;&#38750;&#32447;&#24615;&#22238;&#24402;&#26641;&#26469;&#36817;&#20284;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#12289;&#26088;&#22312;&#35299;&#20915;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#30340;&#30005;&#21147;&#31995;&#32479;&#26242;&#24577;&#31283;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26242;&#24577;&#31283;&#23450;&#24615;&#35780;&#20272;(TSA)&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#38459;&#30861;&#20102;&#20854;&#24037;&#19994;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19987;&#23478;&#24341;&#23548;&#31070;&#32463;&#22238;&#24402;&#26641;&#30340;&#21487;&#35299;&#37322;&#30005;&#21147;&#31995;&#32479;&#26242;&#24577;&#31283;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;(TSA-ENRT)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02555v1 Announce Type: cross  Abstract: Deep learning based transient stability assessment (TSA) has achieved great success, yet the lack of interpretability hinders its industrial application. Although a great number of studies have tried to explore the interpretability of network solutions, many problems still remain unsolved: (1) the difference between the widely accepted power system knowledge and the generated interpretive rules is large, (2) the probability characteristics of the neural network have not been fully considered during generating the interpretive rules, (3) the cost of the trade-off between accuracy and interpretability is too heavy to take. To address these issues, an interpretable power system Transient Stability Assessment method with Expert guiding Neural-Regression-Tree (TSA-ENRT) is proposed. TSA-ENRT utilizes an expert guiding nonlinear regression tree to approximate the neural network prediction and the neural network can be explained by the interp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#39046;&#22495;&#30340;&#26032;&#30340;&#35745;&#25968;&#26041;&#27861;&#65292;&#31216;&#20026;&#26684;&#28857;&#26144;&#23556;&#20266;&#35745;&#25968;&#26041;&#27861;&#65288;GPC&#65289;&#65292;&#20197;&#36866;&#24212;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#24809;&#32602;Q&#20540;&#30340;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.02545</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26684;&#28857;&#26144;&#23556;&#20266;&#35745;&#25968;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02545
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#39046;&#22495;&#30340;&#26032;&#30340;&#35745;&#25968;&#26041;&#27861;&#65292;&#31216;&#20026;&#26684;&#28857;&#26144;&#23556;&#20266;&#35745;&#25968;&#26041;&#27861;&#65288;GPC&#65289;&#65292;&#20197;&#36866;&#24212;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#24809;&#32602;Q&#20540;&#30340;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#32780;&#19981;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36825;&#30830;&#20445;&#20102;&#23433;&#20840;&#24615;&#24182;&#22240;&#27492;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;&#26420;&#32032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#22833;&#36133;&#65292;&#22240;&#20026;&#30001;&#20110;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#34892;&#20026;&#24341;&#36215;&#30340;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#31639;&#27861;&#20027;&#35201;&#24809;&#32602;OOD&#34892;&#20026;&#30340;Q&#20540;&#65292;&#20854;&#32422;&#26463;&#30340;&#36136;&#37327;&#20063;&#24456;&#37325;&#35201;&#12290;&#19981;&#31934;&#30830;&#30340;&#32422;&#26463;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#35299;&#65292;&#32780;&#31934;&#30830;&#30340;&#32422;&#26463;&#21017;&#38656;&#35201;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36830;&#32493;&#39046;&#22495;&#35745;&#25968;&#26041;&#27861;&#65292;&#31216;&#20026;&#26684;&#28857;&#26144;&#23556;&#20266;&#35745;&#25968;&#26041;&#27861;&#65288;GPC&#65289;&#65292;&#20197;&#36866;&#24403;&#22320;&#24809;&#32602;Q&#20540;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#26144;&#23556;&#21040;&#31163;&#25955;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#20266;&#35745;&#25968;&#32422;&#26463;&#23427;&#20204;&#30340;Q&#20540;&#12290;&#36825;&#26159;&#19968;&#20010;&#29702;&#35770;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02545v1 Announce Type: cross  Abstract: Offline reinforcement learning learns from a static dataset without interacting with the environment, which ensures security and thus owns a good prospect of application. However, directly applying naive reinforcement learning methods usually fails in an offline environment due to function approximation errors caused by out-of-distribution(OOD) actions. To solve this problem, existing algorithms mainly penalize the Q-value of OOD actions, the quality of whose constraints also matter. Imprecise constraints may lead to suboptimal solutions, while precise constraints require significant computational costs. In this paper, we propose a novel count-based method for continuous domains, called Grid-Mapping Pseudo-Count method(GPC), to penalize the Q-value appropriately and reduce the computational cost. The proposed method maps the state and action space to discrete space and constrains their Q-values through the pseudo-count. It is theoretic
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;Transformer&#23454;&#29616;&#27969;&#21305;&#37197;&#22312;&#28508;&#31354;&#38388;&#20013;&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20272;&#35745;ODE&#27969;&#29983;&#25104;&#26679;&#26412;&#20998;&#24067;&#26102;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#36824;&#35777;&#26126;&#20102;&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#24615;&#30340;Transformer&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#36924;&#36817;&#20219;&#24847;&#20809;&#28369;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2404.02538</link><description>&lt;p&gt;
&#27969;&#21305;&#37197;&#22312;&#28508;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#19982;Transformer
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Flow Matching in Latent Space with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02538
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;Transformer&#23454;&#29616;&#27969;&#21305;&#37197;&#22312;&#28508;&#31354;&#38388;&#20013;&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20272;&#35745;ODE&#27969;&#29983;&#25104;&#26679;&#26412;&#20998;&#24067;&#26102;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#36824;&#35777;&#26126;&#20102;&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#24615;&#30340;Transformer&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#36924;&#36817;&#20219;&#24847;&#20809;&#28369;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ODE-based&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#27969;&#21305;&#37197;&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#32534;&#30721;&#22120;&#32593;&#32476;&#23558;&#39640;&#32500;&#21407;&#22987;&#36755;&#20837;&#26144;&#23556;&#21040;&#20302;&#32500;&#28508;&#31354;&#38388;&#65292;&#20854;&#20013;&#19968;&#20010;Transformer&#32593;&#32476;&#34987;&#35757;&#32451;&#26469;&#39044;&#27979;&#20174;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#21040;&#30446;&#26631;&#28508;&#31354;&#38388;&#20998;&#24067;&#30340;&#21464;&#25442;&#36895;&#24230;&#22330;&#12290;&#25105;&#20204;&#30340;&#35823;&#24046;&#20998;&#26512;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#36890;&#36807;&#20272;&#35745;&#30340;ODE&#27969;&#29983;&#25104;&#26679;&#26412;&#30340;&#20998;&#24067;&#22312;&#28201;&#26031;&#22374;-2&#36317;&#31163;&#19979;&#25910;&#25947;&#21040;&#30446;&#26631;&#20998;&#24067;&#65292;&#36825;&#22312;&#28201;&#21644;&#19988;&#23454;&#38469;&#30340;&#20551;&#35774;&#19979;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#24615;&#30340;Transformer&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#22320;&#36924;&#36817;&#20219;&#24847;&#20809;&#28369;&#20989;&#25968;&#65292;&#36825;&#21487;&#33021;&#26159;&#29420;&#31435;&#24863;&#20852;&#36259;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02538v1 Announce Type: cross  Abstract: We present theoretical convergence guarantees for ODE-based generative models, specifically flow matching. We use a pre-trained autoencoder network to map high-dimensional original inputs to a low-dimensional latent space, where a transformer network is trained to predict the velocity field of the transformation from a standard normal distribution to the target latent distribution. Our error analysis demonstrates the effectiveness of this approach, showing that the distribution of samples generated via estimated ODE flow converges to the target distribution in the Wasserstein-2 distance under mild and practical assumptions. Furthermore, we show that arbitrary smooth functions can be effectively approximated by transformer networks with Lipschitz continuity, which may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22987;-&#23545;&#20598;&#28369;&#21160;&#19982;&#26465;&#20214;&#26799;&#24230;&#28369;&#21160;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#36890;&#20449;&#39640;&#25928;&#21644;&#26368;&#23567;&#21270;&#26799;&#24230;&#35745;&#31639;&#27425;&#25968;&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#20248;&#21270;&#30340;&#26799;&#24230;&#21644;&#32447;&#24615;&#20248;&#21270;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02511</link><description>&lt;p&gt;
&#20855;&#26377;&#36739;&#23569;&#25968;&#25454;&#31070;&#35861;&#30340;&#26426;&#22120;&#23398;&#20064;&#38543;&#26426;&#32422;&#26463;&#20998;&#25955;&#20248;&#21270;&#65306;&#19968;&#31181;&#26799;&#24230;&#28369;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Constrained Decentralized Optimization for Machine Learning with Fewer Data Oracles: a Gradient Sliding Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02511
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22987;-&#23545;&#20598;&#28369;&#21160;&#19982;&#26465;&#20214;&#26799;&#24230;&#28369;&#21160;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#36890;&#20449;&#39640;&#25928;&#21644;&#26368;&#23567;&#21270;&#26799;&#24230;&#35745;&#31639;&#27425;&#25968;&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#20248;&#21270;&#30340;&#26799;&#24230;&#21644;&#32447;&#24615;&#20248;&#21270;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#20998;&#25955;&#24335;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#30830;&#20445;&#29992;&#25143;&#30340;&#36890;&#20449;&#25928;&#29575;&#21644;&#38544;&#31169;&#26159;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#20102;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#31639;&#27861;&#24517;&#39035;&#19982;&#25968;&#25454;&#20013;&#24515;&#36890;&#20449;&#65292;&#24182;&#37319;&#26679;&#25968;&#25454;&#36827;&#34892;&#26799;&#24230;&#35745;&#31639;&#65292;&#20174;&#32780;&#26292;&#38706;&#25968;&#25454;&#24182;&#22686;&#21152;&#36890;&#20449;&#25104;&#26412;&#12290;&#36825;&#23548;&#33268;&#20102;&#38656;&#35201;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#19988;&#26368;&#23567;&#21270;&#26799;&#24230;&#35745;&#31639;&#27425;&#25968;&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#31639;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#22987;-&#23545;&#20598;&#28369;&#21160;&#19982;&#26465;&#20214;&#26799;&#24230;&#28369;&#21160;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#20449;&#39640;&#25928;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#26799;&#24230;&#22797;&#26434;&#24230;&#30340; $\varepsilon$-&#36817;&#20284;&#35299;$O(1/\sqrt{\varepsilon}+\sigma^2/{\varepsilon^2})$ &#21644; $O(\log(1/\varepsilon)+\sigma^2/\varepsilon)$&#20998;&#21035;&#36866;&#29992;&#20110;&#20984;&#21644;&#24378;&#20984;&#35774;&#32622;&#65292;&#20197;&#21450;&#32473;&#23450; &#38543;&#26426; &#30340;LO (&#32447;&#24615;&#20248;&#21270;)&#22797;&#26434;&#24230;&#65292;&#37117;&#26159;$O(1/\varepsilon^2)$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02511v1 Announce Type: cross  Abstract: In modern decentralized applications, ensuring communication efficiency and privacy for the users are the key challenges. In order to train machine-learning models, the algorithm has to communicate to the data center and sample data for its gradient computation, thus exposing the data and increasing the communication cost. This gives rise to the need for a decentralized optimization algorithm that is communication-efficient and minimizes the number of gradient computations. To this end, we propose the primal-dual sliding with conditional gradient sliding framework, which is communication-efficient and achieves an $\varepsilon$-approximate solution with the optimal gradient complexity of $O(1/\sqrt{\varepsilon}+\sigma^2/{\varepsilon^2})$ and $O(\log(1/\varepsilon)+\sigma^2/\varepsilon)$ for the convex and strongly convex setting respectively and an LO (Linear Optimization) complexity of $O(1/\varepsilon^2)$ for both settings given a sto
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#23458;&#25143;&#31471;&#20915;&#31574;&#26641;&#32858;&#21512;&#36807;&#31243;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#27880;&#20837;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.02510</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#23458;&#25143;&#31471;&#20915;&#31574;&#26641;&#32858;&#21512;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Client Decision Tree Aggregation process for Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#23458;&#25143;&#31471;&#20915;&#31574;&#26641;&#32858;&#21512;&#36807;&#31243;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#27880;&#20837;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#22312;&#24403;&#20170;&#30340;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20248;&#20808;&#32771;&#34385;&#35832;&#22914;&#40065;&#26834;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#36879;&#26126;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#38544;&#31169;&#24615;&#31561;&#21407;&#21017;&#12290;&#36825;&#23548;&#33268;&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#38544;&#31169;&#21644;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20986;&#29616;&#12290;&#20915;&#31574;&#26641;&#20316;&#20026;&#33258;&#35299;&#37322;&#27169;&#22411;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#22914;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#26159;&#29702;&#24819;&#30340;&#65292;&#20197;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#27880;&#20837;&#21487;&#35299;&#37322;&#24615;&#12290;&#20915;&#31574;&#26641;&#32467;&#26500;&#20351;&#24471;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#32858;&#21512;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20107;&#24773;&#12290;&#23427;&#20204;&#38656;&#35201;&#33021;&#22815;&#21512;&#24182;&#23427;&#20204;&#30340;&#20915;&#31574;&#36335;&#24452;&#32780;&#19981;&#24341;&#20837;&#20559;&#24046;&#25110;&#36807;&#25311;&#21512;&#30340;&#25216;&#26415;&#65292;&#21516;&#26102;&#20445;&#25345;&#32858;&#21512;&#20915;&#31574;&#26641;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#30340;&#21487;&#35299;&#37322;&#23458;&#25143;&#31471;&#20915;&#31574;&#26641;&#32858;&#21512;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02510v1 Announce Type: cross  Abstract: Trustworthy Artificial Intelligence solutions are essential in today's data-driven applications, prioritizing principles such as robustness, safety, transparency, explainability, and privacy among others. This has led to the emergence of Federated Learning as a solution for privacy and distributed machine learning. While decision trees, as self-explanatory models, are ideal for collaborative model training across multiple devices in resource-constrained environments such as federated learning environments for injecting interpretability in these models. Decision tree structure makes the aggregation in a federated learning environment not trivial. They require techniques that can merge their decision paths without introducing bias or overfitting while keeping the aggregated decision trees robust and generalizable. In this paper, we propose an Interpretable Client Decision Tree Aggregation process for Federated Learning scenarios that kee
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20026;&#35270;&#38556;&#20154;&#22763;&#25552;&#20379;&#35270;&#35273;&#38382;&#39064;&#31572;&#26696;&#65292;&#25552;&#20986;&#20102; VIAssist &#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19981;&#21463;&#27426;&#36814;&#30340;&#22270;&#20687;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#25805;&#20316;&#65292;&#26368;&#32456;&#20381;&#25454;&#29992;&#25143;&#26597;&#35810;&#25552;&#20379;&#21487;&#38752;&#30340;&#31572;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.02508</link><description>&lt;p&gt;
VIAssist&#65306;&#20026;&#35270;&#35273;&#38556;&#30861;&#29992;&#25143;&#35843;&#25972;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VIAssist: Adapting Multi-modal Large Language Models for Users with Visual Impairments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20026;&#35270;&#38556;&#20154;&#22763;&#25552;&#20379;&#35270;&#35273;&#38382;&#39064;&#31572;&#26696;&#65292;&#25552;&#20986;&#20102; VIAssist &#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19981;&#21463;&#27426;&#36814;&#30340;&#22270;&#20687;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#25805;&#20316;&#65292;&#26368;&#32456;&#20381;&#25454;&#29992;&#25143;&#26597;&#35810;&#25552;&#20379;&#21487;&#38752;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#35270;&#35273;&#38556;&#30861;&#30340;&#20010;&#20307;&#65292;&#21253;&#25324;&#35270;&#35273;&#24863;&#30693;&#26041;&#38754;&#30340;&#37096;&#20998;&#25110;&#23436;&#20840;&#22256;&#38590;&#65292;&#34987;&#31216;&#20026;&#35270;&#38556;&#20154;&#22763;&#12290;&#20840;&#29699;&#20272;&#35745;&#26377;22&#20159;&#20154;&#21463;&#35270;&#21147;&#38556;&#30861;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#38750;&#20961;&#33021;&#21147;&#12290;&#24076;&#26395;&#36890;&#36807;MLLMs&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#26469;&#24110;&#21161;&#35270;&#38556;&#20154;&#22763;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#25429;&#25417;&#29702;&#24819;&#22270;&#20687;&#20197;&#28385;&#36275;&#20182;&#20204;&#30340;&#26085;&#24120;&#38656;&#27714;&#65292;&#35270;&#38556;&#20154;&#22763;&#20351;&#29992;MLLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20363;&#22914;&#65292;&#30446;&#26631;&#23545;&#35937;&#26410;&#23436;&#20840;&#25110;&#37096;&#20998;&#25918;&#32622;&#22312;&#22270;&#20687;&#20013;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;MLLMs&#20026;&#35270;&#38556;&#20154;&#22763;&#25552;&#20379;&#35270;&#35273;&#38382;&#39064;&#31572;&#26696;&#12290;VIAssist&#33021;&#22815;&#35782;&#21035;&#19981;&#21463;&#27426;&#36814;&#30340;&#22270;&#20687;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#25805;&#20316;&#12290;&#26368;&#21518;&#65292;VIAssist&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#26597;&#35810;&#25552;&#20379;&#21487;&#38752;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02508v1 Announce Type: cross  Abstract: Individuals with visual impairments, encompassing both partial and total difficulties in visual perception, are referred to as visually impaired (VI) people. An estimated 2.2 billion individuals worldwide are affected by visual impairments. Recent advancements in multi-modal large language models (MLLMs) have showcased their extraordinary capabilities across various domains. It is desirable to help VI individuals with MLLMs' great capabilities of visual understanding and reasoning. However, it is challenging for VI people to use MLLMs due to the difficulties in capturing the desirable images to fulfill their daily requests. For example, the target object is not fully or partially placed in the image. This paper explores how to leverage MLLMs for VI individuals to provide visual-question answers. VIAssist can identify undesired images and provide detailed actions. Finally, VIAssist can provide reliable answers to users' queries based on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#23398;&#20064;&#23436;&#20840;&#21487;&#35266;&#23519;&#12289;&#38750;&#30830;&#23450;&#24615;&#35745;&#21010;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#22312;&#19968;&#20123; FOND &#35745;&#21010;&#22522;&#20934;&#39046;&#22495;&#20013;&#20135;&#29983;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02499</link><description>&lt;p&gt;
&#23398;&#20064;&#38754;&#21521;&#23436;&#20840;&#21487;&#35266;&#23519;&#38750;&#30830;&#23450;&#24615;&#35745;&#21010;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Generalized Policies for Fully Observable Non-Deterministic Planning Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#23398;&#20064;&#23436;&#20840;&#21487;&#35266;&#23519;&#12289;&#38750;&#30830;&#23450;&#24615;&#35745;&#21010;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#22312;&#19968;&#20123; FOND &#35745;&#21010;&#22522;&#20934;&#39046;&#22495;&#20013;&#20135;&#29983;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#31574;&#30053;&#20195;&#34920;&#35299;&#20915;&#22823;&#37327;&#35745;&#21010;&#38382;&#39064;&#30340;&#21453;&#24212;&#24615;&#31574;&#30053;&#65292;&#20363;&#22914;&#20174;&#32473;&#23450;&#39046;&#22495;&#20013;&#26080;&#38480;&#21487;&#35299;&#23454;&#20363;&#30340;&#38598;&#21512;&#12290; &#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19968;&#31995;&#21015;&#23567;&#35757;&#32451;&#23454;&#20363;&#20013;&#23398;&#20064;&#36825;&#31181;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#32463;&#20856;&#39046;&#22495;&#12290; &#26412;&#25991;&#25193;&#23637;&#20102;&#23398;&#20064;&#38754;&#21521;&#23436;&#20840;&#21487;&#35266;&#23519;&#12289;&#38750;&#30830;&#23450;&#24615;&#65288;FOND&#65289;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#20844;&#24335;&#21644;&#23548;&#33268;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015; FOND &#35745;&#21010;&#22522;&#20934;&#39046;&#22495;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#19968;&#20123;&#39046;&#22495;&#20013;&#20135;&#29983;&#30340;&#27867;&#21270;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#27491;&#30830;&#24615;&#12290; &#23398;&#20064; FOND &#35745;&#21010;&#30340;&#27867;&#21270;&#31574;&#30053;&#26041;&#27861;&#23454;&#38469;&#19978;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#31181;&#25628;&#32034;&#32467;&#26524;&#30340;&#21478;&#19968;&#31181; FOND &#35745;&#21010;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#26159;&#22312;&#32473;&#23450;&#29366;&#24577;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26159;&#22312;&#30001;&#24517;&#39035;&#23398;&#20064;&#30340;&#29305;&#24449;&#23450;&#20041;&#30340;&#25277;&#35937;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02499v1 Announce Type: new  Abstract: General policies represent reactive strategies for solving large families of planning problems like the infinite collection of solvable instances from a given domain. Methods for learning such policies from a collection of small training instances have been developed successfully for classical domains. In this work, we extend the formulations and the resulting combinatorial methods for learning general policies over fully observable, non-deterministic (FOND) domains. We also evaluate the resulting approach experimentally over a number of benchmark domains in FOND planning, present the general policies that result in some of these domains, and prove their correctness. The method for learning general policies for FOND planning can actually be seen as an alternative FOND planning method that searches for solutions, not in the given state space but in an abstract space defined by features that must be learned as well.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02491</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Measuring Social Norms of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02491
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#20197;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35201;&#27714;&#20855;&#26377;&#35299;&#20915;&#31038;&#20250;&#35268;&#33539;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#26368;&#22823;&#30340;&#31038;&#20250;&#35268;&#33539;&#25216;&#33021;&#38598;&#65292;&#21253;&#25324;402&#39033;&#25216;&#33021;&#21644;12,383&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#35266;&#28857;&#21644;&#35770;&#28857;&#21040;&#25991;&#21270;&#21644;&#27861;&#24459;&#31561;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#29702;&#35299;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#19982;&#23567;&#23398;&#29983;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20135;&#29983;&#20960;&#20046;&#38543;&#26426;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5-Turbo&#21644;LLaMA2-Chat&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20165;&#30053;&#20302;&#20110;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
&lt;/p&gt;</description></item><item><title>&#26368;&#20339;&#26041;&#27861;&#20934;&#30830;&#35299;&#20915;&#20102;&#28041;&#21450;&#24050;&#30693;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#24773;&#26223;&#65292;&#20294;&#20173;&#26410;&#36798;&#21040;&#20934;&#30830;&#39044;&#27979;&#26032;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2404.02484</link><description>&lt;p&gt;
&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
New methods for drug synergy prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02484
&lt;/p&gt;
&lt;p&gt;
&#26368;&#20339;&#26041;&#27861;&#20934;&#30830;&#35299;&#20915;&#20102;&#28041;&#21450;&#24050;&#30693;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#24773;&#26223;&#65292;&#20294;&#20173;&#26410;&#36798;&#21040;&#20934;&#30830;&#39044;&#27979;&#26032;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#23567;&#22411;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20381;&#36182;&#20110;&#39640;&#36890;&#37327;&#32452;&#21512;&#31579;&#36873;&#30340;&#33647;&#29289;&#32452;&#21512;&#21327;&#21516;&#20316;&#29992;&#30340;&#26032;&#39044;&#27979;&#26041;&#27861;&#12290;&#33258;2021&#24180;&#20197;&#26469;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#36805;&#36895;&#36827;&#23637;&#65292;&#24050;&#21457;&#34920;&#20102;&#36229;&#36807;30&#31181;&#21407;&#21019;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#32477;&#22823;&#22810;&#25968;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31361;&#26174;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#26680;&#24515;&#25216;&#26415;&#12289;&#25968;&#25454;&#26469;&#28304;&#12289;&#36755;&#20837;&#25968;&#25454;&#31867;&#22411;&#21644;&#21327;&#21516;&#24471;&#20998;&#65292;&#20197;&#21450;&#35770;&#25991;&#25152;&#28041;&#21450;&#30340;&#39044;&#27979;&#24773;&#26223;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#23558;&#36825;&#20123;&#35770;&#25991;&#25918;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#19979;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;&#65292;&#26368;&#20339;&#26041;&#27861;&#20934;&#30830;&#22320;&#35299;&#20915;&#20102;&#28041;&#21450;&#24050;&#30693;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#24773;&#26223;&#65292;&#32780;&#28041;&#21450;&#26032;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#24773;&#26223;&#20173;&#26410;&#36798;&#21040;&#20934;&#30830;&#39044;&#27979;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02484v1 Announce Type: cross  Abstract: In this mini-review, we explore the new prediction methods for drug combination synergy relying on high-throughput combinatorial screens. The fast progress of the field is witnessed in the more than thirty original machine learning methods published since 2021, a clear majority of them based on deep learning techniques. We aim to put these papers under a unifying lens by highlighting the core technologies, the data sources, the input data types and synergy scores used in the methods, as well as the prediction scenarios and evaluation protocols that the papers deal with. Our finding is that the best methods accurately solve the synergy prediction scenarios involving known drugs or cell lines while the scenarios involving new drugs or cell lines still fall short of an accurate prediction level.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;&#21551;&#21457;&#30340;&#26032;&#22411;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;FedSelect&#65292;&#33021;&#22815;&#22312;&#24494;&#35843;&#20013;&#23450;&#21046;&#36873;&#25321;&#32593;&#32476;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#20840;&#23616;&#30693;&#35782;&#23384;&#20648;&#19981;&#22815;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02478</link><description>&lt;p&gt;
FedSelect&#65306;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#36890;&#36807;&#23450;&#21046;&#21270;&#21442;&#25968;&#36873;&#25321;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
FedSelect: Personalized Federated Learning with Customized Selection of Parameters for Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02478
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;&#21551;&#21457;&#30340;&#26032;&#22411;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;FedSelect&#65292;&#33021;&#22815;&#22312;&#24494;&#35843;&#20013;&#23450;&#21046;&#36873;&#25321;&#32593;&#32476;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#20840;&#23616;&#30693;&#35782;&#23384;&#20648;&#19981;&#22815;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#23458;&#25143;&#25968;&#25454;&#20998;&#24067;&#20855;&#26377;&#20805;&#20998;&#24322;&#36136;&#24615;&#26102;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#35299;&#20915;&#20102;&#23458;&#25143;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064; - &#19968;&#31867;&#26088;&#22312;&#20010;&#24615;&#21270;&#23398;&#20064;&#30340;&#20840;&#23616;&#30693;&#35782;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#23458;&#25143;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;FL&#31639;&#27861;&#12290;&#29616;&#26377;&#30340;PFL&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#22312;&#29305;&#23450;&#23618;&#65288;&#21363;&#20998;&#31867;&#22120;&#22836;&#37096;&#65289;&#19978;&#25191;&#34892;&#20010;&#24615;&#21270;&#21644;&#23545;&#20854;&#20313;&#32593;&#32476;&#36827;&#34892;&#20840;&#23616;&#32858;&#21512;&#26469;&#35299;&#32806;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#23616;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#36873;&#25321;&#32593;&#32476;&#23618;&#36827;&#34892;&#20010;&#24615;&#21270;&#21487;&#33021;&#23548;&#33268;&#20840;&#23616;&#30693;&#35782;&#30340;&#23384;&#20648;&#19981;&#22815;&#20248;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedSelect&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;&#20013;&#20351;&#29992;&#30340;&#36845;&#20195;&#23376;&#32593;&#32476;&#21457;&#29616;&#36807;&#31243;&#21551;&#21457;&#30340;&#26032;&#22411;PFL&#31639;&#27861;&#12290;FedSelect&#36880;&#27493;&#25193;&#23637;&#23376;&#32593;&#32476;&#20197;&#20010;&#24615;&#21270;&#23458;&#25143;&#21442;&#25968;&#65292;&#24182;&#21516;&#26102;&#23545;&#21097;&#20313;&#37096;&#20998;&#36827;&#34892;&#20840;&#23616;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02478v1 Announce Type: cross  Abstract: Standard federated learning approaches suffer when client data distributions have sufficient heterogeneity. Recent methods addressed the client data heterogeneity issue via personalized federated learning (PFL) - a class of FL algorithms aiming to personalize learned global knowledge to better suit the clients' local data distributions. Existing PFL methods usually decouple global updates in deep neural networks by performing personalization on particular layers (i.e. classifier heads) and global aggregation for the rest of the network. However, preselecting network layers for personalization may result in suboptimal storage of global knowledge. In this work, we propose FedSelect, a novel PFL algorithm inspired by the iterative subnetwork discovery procedure used for the Lottery Ticket Hypothesis. FedSelect incrementally expands subnetworks to personalize client parameters, concurrently conducting global aggregations on the remaining p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.02476</link><description>&lt;p&gt;
&#29992;&#20110;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Traveling Purchaser Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02476
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#65288;TPP&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#29992;&#20110;&#25429;&#25417;&#24066;&#22330;-&#20135;&#21697;&#20851;&#31995;&#30340;TPP&#30340;&#20108;&#37096;&#22270;&#34920;&#31034;&#65292;&#20197;&#21450;&#20174;&#20108;&#37096;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#23558;&#20854;&#29992;&#20110;&#39034;&#24207;&#26500;&#24314;&#36335;&#30001;&#30340;&#31574;&#30053;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02476v1 Announce Type: cross  Abstract: The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications. Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance. In sharp contrast, we propose a novel approach based on deep reinforcement learning (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective. The key components of our approach include a bipartite graph representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite graph and uses it to sequentially construct the route. One significant benefit of our framework is that we can efficiently const
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22914;&#20309;&#22686;&#24378;LLMs&#22312;&#27178;&#21521;&#24605;&#32771;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#20854;&#22266;&#26377;&#30340;&#36229;&#36234;&#24605;&#32500;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#21387;&#32553;&#30340;&#20449;&#24687;&#24615;&#25552;&#31034;&#21644;&#21160;&#24577;&#30340;&#24773;&#22659;&#23398;&#20064;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02474</link><description>&lt;p&gt;
uTeBC-NLP&#22312;SemEval-2024&#20219;&#21153;9&#20013;&#65306;LLMs&#33021;&#25104;&#20026;&#27178;&#21521;&#24605;&#32771;&#32773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02474
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22914;&#20309;&#22686;&#24378;LLMs&#22312;&#27178;&#21521;&#24605;&#32771;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#20854;&#22266;&#26377;&#30340;&#36229;&#36234;&#24605;&#32500;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#21387;&#32553;&#30340;&#20449;&#24687;&#24615;&#25552;&#31034;&#21644;&#21160;&#24577;&#30340;&#24773;&#22659;&#23398;&#20064;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;Jiang&#31561;&#20154;&#65288;2023c&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#27178;&#21521;&#24605;&#32500;&#65288;&#36229;&#36234;&#24605;&#32500;&#23450;&#21183;&#65289;&#30340;&#22522;&#20934;&#12290;&#22312;&#36825;&#19968;&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#22914;&#20309;&#22686;&#24378;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20197;&#25581;&#31034;&#20854;&#22266;&#26377;&#30340;&#36229;&#36234;&#24605;&#32500;&#33021;&#21147;&#12290;&#36890;&#36807;&#21442;&#21152;SemEval-2024&#30340;&#31532;9&#39033;&#20219;&#21153;&#65292;&#21363;&#21477;&#23376;&#25340;&#22270;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65306;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#21644;&#30452;&#25509;&#25552;&#31034;&#65292;&#20351;&#29992;&#20449;&#24687;&#24615;&#25551;&#36848;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31649;&#36947;&#36827;&#34892;&#24773;&#22659;&#21270;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#19977;&#31181;LLMs&#65292;&#21253;&#25324;GPT-3.5&#12289;GPT-4&#21644;Zephyr-7B-beta&#12290;&#25105;&#20204;&#20351;&#29992;GPT-4&#29983;&#25104;&#20102;&#35868;&#39064;&#21644;&#36873;&#39033;&#20043;&#38388;&#30340;&#24605;&#32500;&#36335;&#24452;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#36827;&#34892;&#20102;&#36136;&#37327;&#39564;&#35777;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21387;&#32553;&#30340;&#20449;&#24687;&#24615;&#25552;&#31034;&#33021;&#22815;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#21160;&#24577;&#30340;&#24773;&#22659;&#23398;&#20064;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02474v1 Announce Type: cross  Abstract: Inspired by human cognition, Jiang et al.(2023c) create a benchmark for assessing LLMs' lateral thinking-thinking outside the box. Building upon this benchmark, we investigate how different prompting methods enhance LLMs' performance on this task to reveal their inherent power for outside-the-box thinking ability. Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT) and direct prompting, enhancing with informative descriptions, and employing contextualizing prompts using a retrieval augmented generation (RAG) pipeline. Our experiments involve three LLMs including GPT-3.5, GPT-4, and Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and options using GPT-4, validated by humans for quality. Findings indicate that compressed informative prompts enhance performance. Dynamic in-context learning enhances model performance significantly. F
&lt;/p&gt;</description></item><item><title>&#25391;&#21160;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#22312;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#36816;&#34892;&#26102;&#25512;&#26029;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#37096;&#32626;&#20013;&#32454;&#21270;&#35843;&#25972;&#65292;&#21152;&#24555;&#24212;&#29992;&#37096;&#32626;&#26102;&#38388;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02461</link><description>&lt;p&gt;
&#25391;&#21160;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#32852;&#32593;&#20256;&#24863;&#20013;&#30340;&#25928;&#29575;&#21644;&#31283;&#20581;&#24615;&#65306;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Efficiency and Robustness of Vibration-based Foundation Models for IoT Sensing: A Case Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02461
&lt;/p&gt;
&lt;p&gt;
&#25391;&#21160;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#22312;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#36816;&#34892;&#26102;&#25512;&#26029;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#37096;&#32626;&#20013;&#32454;&#21270;&#35843;&#25972;&#65292;&#21152;&#24555;&#24212;&#29992;&#37096;&#32626;&#26102;&#38388;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#25391;&#21160;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#21033;&#29992;&#26410;&#26631;&#35760;&#20256;&#24863;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#28508;&#21147;&#65292;&#20197;&#25552;&#39640;&#65288;&#26576;&#31867;&#65289;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#36816;&#34892;&#26102;&#25512;&#26029;&#30340;&#31283;&#20581;&#24615;&#12290;&#25991;&#31456;&#20197;&#19968;&#20010;&#36710;&#36742;&#20998;&#31867;&#24212;&#29992;&#20026;&#20363;&#65292;&#20351;&#29992;&#22768;&#23398;&#21644;&#22320;&#38663;&#20256;&#24863;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#12290;&#35813;&#24037;&#20316;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#23558;FM&#27010;&#24565;&#25512;&#24191;&#21040;&#20854;&#20182;&#39046;&#22495;&#65292;&#20854;&#20013;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#20110;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#29289;&#32852;&#32593;&#24212;&#29992;&#26159;&#36825;&#26679;&#30340;&#19968;&#20010;&#39046;&#22495;&#12290;&#29289;&#32852;&#32593;&#39046;&#22495;&#20013;&#36873;&#23450;&#20256;&#24863;&#27169;&#24335;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#20197;&#19982;&#29615;&#22659;&#26080;&#20851;&#30340;&#26041;&#24335;&#21033;&#29992;&#29616;&#26377;&#30340;&#26410;&#26631;&#35760;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#23545;&#37096;&#32626;&#36827;&#34892;&#24494;&#35843;&#12290;&#25991;&#31456;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;/&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#32553;&#30701;&#24212;&#29992;&#37096;&#32626;&#26102;&#38388;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#28508;&#22312;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02461v1 Announce Type: new  Abstract: This paper demonstrates the potential of vibration-based Foundation Models (FMs), pre-trained with unlabeled sensing data, to improve the robustness of run-time inference in (a class of) IoT applications. A case study is presented featuring a vehicle classification application using acoustic and seismic sensing. The work is motivated by the success of foundation models in the areas of natural language processing and computer vision, leading to generalizations of the FM concept to other domains as well, where significant amounts of unlabeled data exist that can be used for self-supervised pre-training. One such domain is IoT applications. Foundation models for selected sensing modalities in the IoT domain can be pre-trained in an environment-agnostic fashion using available unlabeled sensor data and then fine-tuned to the deployment at hand using a small amount of labeled data. The paper shows that the pre-training/fine-tuning approach im
&lt;/p&gt;</description></item><item><title>PhonologyBench&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#26126;&#30830;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#20013;&#30340;&#38899;&#38901;&#25216;&#33021;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#27809;&#26377;&#35821;&#38899;&#25968;&#25454;&#24773;&#20917;&#19979;&#22312;PhonologyBench&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02456</link><description>&lt;p&gt;
PhonologyBench&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38899;&#38901;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
PhonologyBench: Evaluating Phonological Skills of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02456
&lt;/p&gt;
&lt;p&gt;
PhonologyBench&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#26126;&#30830;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#20013;&#30340;&#38899;&#38901;&#25216;&#33021;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#27809;&#26377;&#35821;&#38899;&#25968;&#25454;&#24773;&#20917;&#19979;&#22312;PhonologyBench&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#38901;&#23398;&#26159;&#30740;&#31350;&#35821;&#38899;&#32467;&#26500;&#21644;&#21457;&#38899;&#35268;&#21017;&#30340;&#23398;&#31185;&#65292;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30740;&#31350;&#20013;&#19968;&#20010;&#20851;&#38190;&#20294;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;LLMs&#22312;&#21508;&#31181;&#21033;&#29992;&#38899;&#38901;&#23398;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#22914;&#25945;&#32946;&#24037;&#20855;&#21644;&#35799;&#27468;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#33021;&#20250;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#19981;&#23436;&#32654;&#30340;&#27491;&#23383;&#21644;&#38899;&#26631;&#24418;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#22240;&#27492;&#65292;&#23545;LLMs&#30340;&#38899;&#38901;&#25216;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PhonologyBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#19977;&#20010;&#35786;&#26029;&#20219;&#21153;&#65292;&#26088;&#22312;&#26126;&#30830;&#27979;&#35797;LLMs&#22312;&#33521;&#35821;&#20013;&#30340;&#38899;&#38901;&#25216;&#33021;&#65306;&#24418;&#38899;&#36716;&#25442;&#12289;&#38899;&#33410;&#35745;&#25968;&#21644;&#25276;&#38901;&#35789;&#29983;&#25104;&#12290;&#23613;&#31649;&#27809;&#26377;&#35775;&#38382;&#35821;&#38899;&#25968;&#25454;&#65292;LLMs&#22312;PhonologyBench&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#25276;&#38901;&#35789;&#29983;&#25104;&#21644;&#38899;&#33410;&#35745;&#25968;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;17%&#21644;45%&#30340;&#24046;&#36317;&#65292; respectively, when...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02456v1 Announce Type: cross  Abstract: Phonology, the study of speech's structure and pronunciation rules, is a critical yet often overlooked component in Large Language Model (LLM) research. LLMs are widely used in various downstream applications that leverage phonology such as educational tools and poetry generation. Moreover, LLMs can potentially learn imperfect associations between orthographic and phonological forms from the training data. Thus, it is imperative to benchmark the phonological skills of LLMs. To this end, we present PhonologyBench, a novel benchmark consisting of three diagnostic tasks designed to explicitly test the phonological skills of LLMs in English: grapheme-to-phoneme conversion, syllable counting, and rhyme word generation. Despite having no access to speech data, LLMs showcased notable performance on the PhonologyBench tasks. However, we observe a significant gap of 17% and 45% on Rhyme Word Generation and Syllable counting, respectively, when 
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#24320;&#21457;&#19968;&#31181;&#32479;&#19968;&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#36328;&#22810;&#31181;&#27169;&#24335;&#30340;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2404.02450</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#32452;&#21512;&#36827;&#34892;&#31639;&#27861;&#35825;&#23548;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Task Agnostic Architecture for Algorithm Induction via Implicit Composition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02450
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#24320;&#21457;&#19968;&#31181;&#32479;&#19968;&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#36328;&#22810;&#31181;&#27169;&#24335;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#21516;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#25110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#19968;&#30452;&#22312;&#26500;&#24314;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30446;&#21069;&#65292;&#25105;&#20204;&#27491;&#22312;&#30446;&#30585;&#19968;&#20010;&#30456;&#21453;&#30340;&#36235;&#21183;&#65292;&#21363;&#21521;&#24320;&#21457;&#26356;&#24191;&#20041;&#30340;&#26550;&#26500;&#21457;&#23637;&#65292;&#36825;&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#27169;&#24335;&#22522;&#30784;&#27169;&#22411;&#25512;&#21160;&#30340;&#12290;&#36825;&#20123;&#26550;&#26500;&#26088;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#36328;&#22810;&#31181;&#27169;&#24335;&#30340;&#36755;&#20837;&#12290;&#23558;&#36825;&#31181;&#27867;&#21270;&#36235;&#21183;&#25512;&#21521;&#26497;&#31471;&#24847;&#21619;&#30528;&#21487;&#33021;&#23384;&#22312;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#25152;&#26377;&#20219;&#21153;&#30340;&#21333;&#19968;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#24320;&#21457;&#36825;&#26679;&#19968;&#20010;&#32479;&#19968;&#26550;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35828;&#26126;&#22914;&#20309;&#26500;&#24314;&#36825;&#26679;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#22522;&#20110;&#20197;&#19979;&#20551;&#35774;&#12290;&#39318;&#20808;&#65292;&#20219;&#21153;&#26159;&#36890;&#36807;&#25353;&#29031;&#19968;&#31995;&#21015;&#25351;&#20196;&#26469;&#35299;&#20915;&#30340;&#65292;&#36890;&#24120;&#22312;&#24120;&#35268;&#35745;&#31639;&#30828;&#20214;&#30340;&#20195;&#30721;&#20013;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02450v1 Announce Type: cross  Abstract: Different fields in applied machine learning such as computer vision, speech or natural language processing have been building domain-specialised solutions. Currently, we are witnessing an opposing trend towards developing more generalist architectures, driven by Large Language Models and multi-modal foundational models. These architectures are designed to tackle a variety of tasks, including those previously unseen and using inputs across multiple modalities. Taking this trend of generalization to the extreme suggests the possibility of a single deep network architecture capable of solving all tasks. This position paper aims to explore developing such a unified architecture and proposes a theoretical framework of how it could be constructed. Our proposal is based on the following assumptions. Firstly, tasks are solved by following a sequence of instructions, typically implemented in code for conventional computing hardware, which inhe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#30005;&#21160;&#36710;&#36742;&#30340;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#32452;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#36710;&#36742;&#36873;&#25321;&#22120;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#36873;&#25321;&#22120;&#35299;&#20915;&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#34892;&#39542;&#36317;&#31163;&#21644;&#25925;&#38556;&#22522;&#31449;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.02448</link><description>&lt;p&gt;
&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#29992;&#20110;&#24212;&#24613;&#20379;&#30005;&#65306;&#38754;&#21521;&#30005;&#20449;&#22522;&#31449;&#25937;&#21161;
&lt;/p&gt;
&lt;p&gt;
Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#30005;&#21160;&#36710;&#36742;&#30340;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#32452;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#36710;&#36742;&#36873;&#25321;&#22120;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#36873;&#25321;&#22120;&#35299;&#20915;&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#34892;&#39542;&#36317;&#31163;&#21644;&#25925;&#38556;&#22522;&#31449;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#23478;&#30005;&#20449;&#25552;&#20379;&#21830;&#65292;&#25105;&#20204;&#20844;&#21496;&#26377;&#19968;&#20010;&#20851;&#38190;&#20351;&#21629;&#65292;&#21363;&#22312;&#20572;&#30005;&#26399;&#38388;&#20445;&#25345;&#30005;&#20449;&#26381;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#20351;&#21629;&#65292;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#32500;&#25345;&#30005;&#20449;&#22522;&#31449;&#30340;&#30005;&#21147;&#12290;&#26412;&#25991;&#32771;&#34385;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#30005;&#21160;&#36710;&#36742; (EVs) &#30452;&#25509;&#21069;&#24448;&#20854;&#20301;&#32622;&#20026;&#22522;&#31449;&#25552;&#20379;&#30005;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#26368;&#23567;&#21270;&#25152;&#26377;&#30005;&#21160;&#36710;&#36742;&#30340;&#24635;&#34892;&#39542;&#36317;&#31163;&#21644;&#25925;&#38556;&#22522;&#31449;&#25968;&#37327;&#30340;EV&#36335;&#32447;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#36335;&#24452;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#26032;&#22411;&#30340;&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064; (EVRP) &#21464;&#20307;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#36710;&#36742;&#36873;&#25321;&#22120;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#33410;&#28857;&#36873;&#25321;&#22120;&#30456;&#32467;&#21512;&#30340;&#27714;&#35299;&#22120;&#12290;&#36710;&#36742;&#36873;&#25321;&#22120;&#30340;&#35268;&#21017;&#30830;&#20445;&#20102;&#25152;&#36873;EV&#24320;&#22987;&#31227;&#21160;&#26102;&#30340;&#30830;&#20999;&#29615;&#22659;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;RL&#27169;&#22411;&#30340;&#33410;&#28857;&#36873;&#25321;&#23454;&#29616;&#20102;&#24555;&#36895;&#36335;&#24452;&#29983;&#25104;&#65292;&#22312;&#32039;&#24613;&#24773;&#20917;&#19979;&#23588;&#20026;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#19978;&#23545;&#25105;&#20204;&#30340;&#27714;&#35299;&#22120;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02448v1 Announce Type: cross  Abstract: As a telecom provider, our company has a critical mission to maintain telecom services even during power outages. To accomplish the mission, it is essential to maintain the power of the telecom base stations. Here we consider a solution where electric vehicles (EVs) directly supply power to base stations by traveling to their locations. The goal is to find EV routes that minimize both the total travel distance of all EVs and the number of downed base stations. In this paper, we formulate this routing problem as a new variant of the Electric Vehicle Routing Problem (EVRP) and propose a solver that combines a rule-based vehicle selector and a reinforcement learning (RL)-based node selector. The rule of the vehicle selector ensures the exact environmental states when the selected EV starts to move. In addition, the node selection by the RL model enables fast route generation, which is critical in emergencies. We evaluate our solver on bot
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#30333;&#30418;&#35774;&#35745;&#33539;&#20363;&#12290;</title><link>https://arxiv.org/abs/2404.02446</link><description>&lt;p&gt;
&#36890;&#36807;&#24102;&#26377;&#30333;&#30418;&#21464;&#25442;&#22120;&#30340;&#32467;&#26500;&#25193;&#25955;&#36827;&#34892;&#33945;&#38754;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Masked Completion via Structured Diffusion with White-Box Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#30333;&#30418;&#35774;&#35745;&#33539;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#23398;&#20064;&#26694;&#26550;&#36890;&#24120;&#20351;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#35299;&#20915;&#31616;&#21333;&#30340;&#21069;&#32622;&#20219;&#21153;&#23398;&#20064;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#34920;&#31034;&#29992;&#20316;&#19979;&#28216;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#30333;&#30418;&#35774;&#35745;&#33539;&#20363;&#30340;&#23454;&#20363;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02446v1 Announce Type: new  Abstract: Modern learning frameworks often train deep neural networks with massive amounts of unlabeled data to learn representations by solving simple pretext tasks, then use the representations as foundations for downstream tasks. These networks are empirically designed; as such, they are usually not interpretable, their representations are not structured, and their designs are potentially redundant. White-box deep networks, in which each layer explicitly identifies and transforms structures in the data, present a promising alternative. However, existing white-box architectures have only been shown to work at scale in supervised settings with labeled data, such as classification. In this work, we provide the first instantiation of the white-box design paradigm that can be applied to large-scale unsupervised representation learning. We do this by exploiting a fundamental connection between diffusion, compression, and (masked) completion, deriving
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;NLP&#25216;&#26415;&#20174;&#21475;&#36848;&#39564;&#23608;&#25991;&#26412;&#20013;&#39044;&#27979;&#27515;&#22240;&#24182;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02438</link><description>&lt;p&gt;
&#20174;&#21465;&#36848;&#21040;&#25968;&#23383;&#65306;&#21033;&#29992;&#21475;&#36848;&#39564;&#23608;&#21465;&#36848;&#30340;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsy Narratives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02438
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;NLP&#25216;&#26415;&#20174;&#21475;&#36848;&#39564;&#23608;&#25991;&#26412;&#20013;&#39044;&#27979;&#27515;&#22240;&#24182;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37096;&#20998;&#27515;&#20129;&#20107;&#20214;&#21457;&#29983;&#22312;&#21307;&#30103;&#31995;&#32479;&#22806;&#30340;&#22330;&#26223;&#20013;&#65292;&#21475;&#36848;&#39564;&#23608;&#65288;VAs&#65289;&#26159;&#30417;&#27979;&#27515;&#22240;&#36235;&#21183;&#30340;&#24120;&#29992;&#24037;&#20855;&#12290;VAs&#26159;&#19982;&#24184;&#23384;&#30340;&#29031;&#26009;&#32773;&#25110;&#20146;&#23646;&#36827;&#34892;&#30340;&#35775;&#35848;&#65292;&#29992;&#20110;&#39044;&#27979;&#36893;&#32773;&#30340;&#27515;&#22240;&#12290;&#23558;VAs&#36716;&#21270;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#21487;&#34892;&#30340;&#35265;&#35299;&#38656;&#35201;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;i&#65289;&#20351;&#29992;VA&#35775;&#35848;&#39044;&#27979;&#21487;&#33021;&#30340;&#27515;&#22240;&#65292;&#65288;ii&#65289;&#20351;&#29992;&#39044;&#27979;&#30340;&#27515;&#22240;&#36827;&#34892;&#25512;&#26029;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;&#27515;&#20129;&#26679;&#26412;&#23545;&#27515;&#22240;&#25353;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#20998;&#35299;&#30340;&#24314;&#27169;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;NLP&#25216;&#26415;&#20174;&#33258;&#30001;&#25991;&#26412;&#39044;&#27979;&#32467;&#26524;&#65288;&#22312;&#25105;&#20204;&#30340;&#26696;&#20363;&#20013;&#20026;&#27515;&#22240;&#65289;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;multiPPI++&#65292;&#23558;&#26368;&#36817;&#30340;&#8220;&#39044;&#27979;&#39537;&#21160;&#25512;&#26029;&#8221;&#24037;&#20316;&#25193;&#23637;&#21040;&#22810;&#39033;&#20998;&#31867;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31995;&#21015;NLP&#25216;&#26415;&#36827;&#34892;COD&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23545;VA&#25968;&#25454;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02438v1 Announce Type: new  Abstract: In settings where most deaths occur outside the healthcare system, verbal autopsies (VAs) are a common tool to monitor trends in causes of death (COD). VAs are interviews with a surviving caregiver or relative that are used to predict the decedent's COD. Turning VAs into actionable insights for researchers and policymakers requires two steps (i) predicting likely COD using the VA interview and (ii) performing inference with predicted CODs (e.g. modeling the breakdown of causes by demographic factors using a sample of deaths). In this paper, we develop a method for valid inference using outcomes (in our case COD) predicted from free-form text using state-of-the-art NLP techniques. This method, which we call multiPPI++, extends recent work in "prediction-powered inference" to multinomial classification. We leverage a suite of NLP techniques for COD prediction and, through empirical analysis of VA data, demonstrate the effectiveness of our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19971;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#19977;&#31181;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#20915;&#31574;&#27169;&#22411;&#20316;&#20026;&#31639;&#27861;&#35774;&#35745;&#30340;&#21442;&#32771;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.02429</link><description>&lt;p&gt;
AD4RL&#65306;&#20855;&#26377;&#22522;&#20110;&#20215;&#20540;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AD4RL: Autonomous Driving Benchmarks for Offline Reinforcement Learning with Value-based Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19971;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#19977;&#31181;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#20915;&#31574;&#27169;&#22411;&#20316;&#20026;&#31639;&#27861;&#35774;&#35745;&#30340;&#21442;&#32771;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#25910;&#38598;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#24050;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#28508;&#21147;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#22909;&#22788;&#24050;&#34987;&#22686;&#24378;&#65292;&#20294;&#22823;&#22810;&#25968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#24320;&#21457;&#30740;&#31350;&#20173;&#20381;&#36182;&#20110;&#20855;&#26377;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#28216;&#25103;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;19&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19977;&#31181;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#19971;&#31181;&#27969;&#34892;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20915;&#31574;&#36807;&#31243;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#26377;&#25928;&#36816;&#34892;&#65292;&#20316;&#20026;&#31639;&#27861;&#35774;&#35745;&#30340;&#21442;&#32771;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#31038;&#21306;&#36827;&#19968;&#27493;&#25506;&#32034;&#29616;&#26377;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#23454;&#38469;&#26041;&#38754;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02429v1 Announce Type: cross  Abstract: Offline reinforcement learning has emerged as a promising technology by enhancing its practicality through the use of pre-collected large datasets. Despite its practical benefits, most algorithm development research in offline reinforcement learning still relies on game tasks with synthetic datasets. To address such limitations, this paper provides autonomous driving datasets and benchmarks for offline reinforcement learning research. We provide 19 datasets, including real-world human driver's datasets, and seven popular offline reinforcement learning algorithms in three realistic driving scenarios. We also provide a unified decision-making process model that can operate effectively across different scenarios, serving as a reference framework in algorithm design. Our research lays the groundwork for further collaborations in the community to explore practical aspects of existing reinforcement learning methods. Dataset and codes can be 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31232;&#30095;&#36328;&#27169;&#24577;&#36866;&#24212;&#20462;&#22797;&#31232;&#30095;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;VLM&#20462;&#21098;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#25552;&#20986;&#31232;&#30095;&#27604;&#29575;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#20462;&#22797;&#31232;&#30095;VLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#19987;&#38376;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2404.02424</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#36328;&#27169;&#24577;&#36866;&#24212;&#20462;&#22797;&#31232;&#30095;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RESSA: Repair Sparse Vision-Language Models via Sparse Cross-Modality Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02424
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#36328;&#27169;&#24577;&#36866;&#24212;&#20462;&#22797;&#31232;&#30095;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;VLM&#20462;&#21098;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#25552;&#20986;&#31232;&#30095;&#27604;&#29575;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#20462;&#22797;&#31232;&#30095;VLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#19987;&#38376;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#25972;&#21512;&#20102;&#26469;&#33258;&#22810;&#20010;&#27169;&#24577;&#30340;&#19981;&#21516;&#20449;&#24687;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#22330;&#26223;&#20013;&#37096;&#32626;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#30340;VLMs&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#23613;&#31649;&#20462;&#21098;&#21518;&#24494;&#35843;&#25552;&#20379;&#20102;&#19968;&#31181;&#20445;&#25345;&#26356;&#23567;&#27169;&#22411;&#22823;&#23567;&#24615;&#33021;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20854;&#22312;VLMs&#20013;&#30340;&#24212;&#29992;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#65292;&#36825;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#22914;&#20309;&#22312;&#19981;&#21516;&#27169;&#24577;&#29305;&#23450;&#27169;&#22411;&#20043;&#38388;&#20998;&#37197;&#31232;&#30095;&#24615;&#65292;&#20197;&#21450;&#22914;&#20309;&#20462;&#22797;&#34987;&#20462;&#21098;&#31232;&#30095;&#30340;VLMs&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20851;&#20110;VLM&#20462;&#21098;&#30340;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#20351;&#29992;&#30456;&#21516;&#31232;&#30095;&#27604;&#29575;&#20462;&#21098;&#35270;&#35273;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#23454;&#29616;&#25509;&#36817;&#26368;&#20339;&#24615;&#33021;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#19982;&#24494;&#35843;&#21333;&#27169;&#31232;&#30095;&#27169;&#22411;&#19981;&#21516;&#65292;&#31232;&#30095;VLMs&#28041;&#21450;&#36328;&#27169;&#24577;&#20132;&#20114;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02424v1 Announce Type: new  Abstract: Vision-Language Models (VLMs), integrating diverse information from multiple modalities, have shown remarkable success across various tasks. However, deploying VLMs, comprising large-scale vision and language models poses challenges in resource-constrained scenarios. While pruning followed by finetuning offers a potential solution to maintain performance with smaller model sizes, its application to VLMs remains relatively unexplored, presenting two main questions: how to distribute sparsity across different modality-specific models, and how to repair the performance of pruned sparse VLMs. To answer the first question, we conducted preliminary studies on VLM pruning and found that pruning vision models and language models with the same sparsity ratios contribute to nearly optimal performance. For the second question, unlike finetuning unimodal sparse models, sparse VLMs involve cross-modality interactions, requiring specialized techniques
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;PEFT&#21644;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#20302;&#36164;&#28304;LLMs&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#19982;0-shot&#25991;&#26412;&#20998;&#31867;&#22120;&#30456;&#23218;&#32654;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02422</link><description>&lt;p&gt;
&#20351;&#29992;PEFT&#21644;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#20302;&#36164;&#28304;LLMs&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02422
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;PEFT&#21644;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#20302;&#36164;&#28304;LLMs&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#19982;0-shot&#25991;&#26412;&#20998;&#31867;&#22120;&#30456;&#23218;&#32654;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;0-shot&#25110;few-shot&#35774;&#32622;&#19979;&#65292;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#25104;&#26524;&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#20013;&#65292;&#36890;&#24120;&#27604;0-shot&#35774;&#32622;&#33719;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#36825;&#26159;&#20197;&#25928;&#29575;&#20026;&#20195;&#20215;&#30340;&#65292;&#22240;&#20026;&#38656;&#35201;&#26356;&#38271;&#30340;&#36755;&#20837;&#25552;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#21487;&#20197;&#20351;LLMs&#20687;0-shot&#25991;&#26412;&#20998;&#31867;&#22120;&#19968;&#26679;&#39640;&#25928;&#65292;&#21516;&#26102;&#33719;&#24471;&#19982;ICL&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#38024;&#23545;&#36164;&#28304;&#31232;&#32570;&#30340;&#24773;&#20917;&#65292;&#21363;&#27599;&#31867;&#21482;&#26377;4&#20010;&#31034;&#20363;&#21487;&#29992;&#12290;&#20351;&#29992;&#21333;&#20010;LLM&#21644;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#25191;&#34892;&#19968;&#31995;&#21015;&#29983;&#25104;&#12289;&#36807;&#28388;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#27493;&#39588;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#24378;&#22823;&#32780;&#39640;&#25928;&#30340;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02422v1 Announce Type: new  Abstract: Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt. In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL. Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple text classification datasets.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#25511;&#21046;&#20219;&#21153;&#20316;&#20026;&#22522;&#20110;&#36807;&#21435;&#35266;&#23519;&#12289;&#21160;&#20316;&#21644;&#22870;&#21169;&#30340;&#24403;&#21069;&#26368;&#20248;&#21160;&#20316;&#39044;&#27979;&#26469;&#28040;&#38500;&#20272;&#35745;&#22120;&#35774;&#35745;&#38656;&#27714;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#31995;&#21015;&#21021;&#22987;&#21270;&#20915;&#31574;Transformer&#65292;&#28982;&#21518;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#23545;&#20854;&#36827;&#34892;&#25511;&#21046;&#20219;&#21153;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2404.02407</link><description>&lt;p&gt;
&#20915;&#31574;Transformer&#20316;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#36830;&#32493;&#25511;&#21046;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Decision Transformer as a Foundation Model for Partially Observable Continuous Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#25511;&#21046;&#20219;&#21153;&#20316;&#20026;&#22522;&#20110;&#36807;&#21435;&#35266;&#23519;&#12289;&#21160;&#20316;&#21644;&#22870;&#21169;&#30340;&#24403;&#21069;&#26368;&#20248;&#21160;&#20316;&#39044;&#27979;&#26469;&#28040;&#38500;&#20272;&#35745;&#22120;&#35774;&#35745;&#38656;&#27714;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#31995;&#21015;&#21021;&#22987;&#21270;&#20915;&#31574;Transformer&#65292;&#28982;&#21518;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#23545;&#20854;&#36827;&#34892;&#25511;&#21046;&#20219;&#21153;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#20855;&#26377;&#37096;&#20998;&#29366;&#24577;&#21487;&#35266;&#27979;&#24615;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#38381;&#29615;&#25511;&#21046;&#38656;&#35201;&#19987;&#23478;&#23545;&#19968;&#32452;&#19981;&#22826;&#26631;&#20934;&#21270;&#30340;&#29702;&#35770;&#24037;&#20855;&#26377;&#28145;&#20837;&#20102;&#35299;&#65292;&#27492;&#22806;&#65292;&#23427;&#36824;&#38656;&#35201;&#25511;&#21046;&#22120;&#21644;&#20272;&#35745;&#22120;&#35774;&#35745;&#30340;&#31934;&#24515;&#34701;&#21512;&#20197;&#23454;&#29616;&#26399;&#26395;&#30340;&#31995;&#32479;&#34892;&#20026;&#12290;&#20026;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#30340;&#25511;&#21046;&#22120;&#21512;&#25104;&#26694;&#26550;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20915;&#31574;Transformer&#65288;DT&#65289;&#26550;&#26500;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#25511;&#21046;&#20219;&#21153;&#26694;&#26550;&#21270;&#20026;&#22522;&#20110;&#36807;&#21435;&#30340;&#35266;&#23519;&#12289;&#21160;&#20316;&#21644;&#22870;&#21169;&#39044;&#27979;&#24403;&#21069;&#26368;&#20248;&#21160;&#20316;&#65292;&#28040;&#38500;&#20102;&#23545;&#21333;&#29420;&#20272;&#35745;&#22120;&#35774;&#35745;&#30340;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#65288;GPT&#65289;&#31995;&#21015;&#65292;&#26469;&#21021;&#22987;&#21270;DT&#65292;&#24182;&#38543;&#21518;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#23545;&#20854;&#36827;&#34892;&#25511;&#21046;&#20219;&#21153;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#28085;&#30422;&#33322;&#31354;&#33322;&#22825;&#31995;&#32479;&#30340;&#25805;&#32437;&#21040;&#25511;&#21046;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02407v1 Announce Type: cross  Abstract: Closed-loop control of nonlinear dynamical systems with partial-state observability demands expert knowledge of a diverse, less standardized set of theoretical tools. Moreover, it requires a delicate integration of controller and estimator designs to achieve the desired system behavior. To establish a general controller synthesis framework, we explore the Decision Transformer (DT) architecture. Specifically, we first frame the control task as predicting the current optimal action based on past observations, actions, and rewards, eliminating the need for a separate estimator design. Then, we leverage the pre-trained language models, i.e., the Generative Pre-trained Transformer (GPT) series, to initialize DT and subsequently train it for control tasks using low-rank adaptation (LoRA). Our comprehensive experiments across five distinct control tasks, ranging from maneuvering aerospace systems to controlling partial differential equations 
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#27874;&#26031;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24341;&#20837;&#20102;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;LLMs&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.02403</link><description>&lt;p&gt;
&#35780;&#20272;&#27874;&#26031;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20197; ChatGPT &#20026;&#20013;&#24515;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02403
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#27874;&#26031;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24341;&#20837;&#20102;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;LLMs&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27874;&#26031;&#35821;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#34429;&#28982;ChatGPT&#21644;&#38543;&#21518;&#30340;LLMs&#22312;&#33521;&#35821;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#26356;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#19968;&#31995;&#21015;&#27874;&#26031;&#35821;&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#22312;GPT-3.5-turbo&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#25105;&#20204;&#20063;&#21253;&#25324;&#20102;GPT-4&#21644;OpenChat-3.5&#20197;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#21253;&#25324;&#32463;&#20856;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#36827;&#34892;&#28145;&#20837;&#27604;&#36739;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#19982;&#29616;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#25512;&#29702;&#20219;&#21153;&#27874;&#26031;&#35821;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65306;&#19968;&#20010;&#22522;&#20110;&#23567;&#23398;&#25968;&#23398;&#38382;&#39064;&#65292;&#21478;&#19968;&#20010;&#28304;&#33258;&#31532;7&#21644;&#31532;10&#24180;&#32423;&#20837;&#23398;&#32771;&#35797;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#65292;&#23588;&#20854;&#26159;GPT-4&#65292;&#22312;&#25512;&#29702;&#20219;&#21153;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02403v1 Announce Type: new  Abstract: This paper explores the efficacy of large language models (LLMs) for Persian. While ChatGPT and consequent LLMs have shown remarkable performance in English, their efficiency for more low-resource languages remains an open question. We present the first comprehensive benchmarking study of LLMs across diverse Persian language tasks. Our primary focus is on GPT-3.5-turbo, but we also include GPT-4 and OpenChat-3.5 to provide a more holistic evaluation. Our assessment encompasses a diverse set of tasks categorized into classic, reasoning, and knowledge-based domains. To enable a thorough comparison, we evaluate LLMs against existing task-specific fine-tuned models. Given the limited availability of Persian datasets for reasoning tasks, we introduce two new benchmarks: one based on elementary school math questions and another derived from the entrance exams for 7th and 10th grades. Our findings reveal that while LLMs, especially GPT-4, excel
&lt;/p&gt;</description></item><item><title>Token Trails&#26159;&#19968;&#31181;&#21033;&#29992;token-type&#23884;&#20837;&#23548;&#33322;&#23545;&#35805;&#20013;&#22797;&#26434;&#19978;&#19979;&#25991;&#32454;&#24494;&#24046;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#39640;&#23545;&#35805;&#29702;&#35299;&#21644;&#22238;&#22797;&#29983;&#25104;&#25928;&#26524;&#65292;&#22312;&#20419;&#36827;&#19978;&#19979;&#25991;&#24847;&#35782;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#26041;&#38754;&#20855;&#26377;&#21069;&#27839;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02402</link><description>&lt;p&gt;
&#20351;&#29992;ChatLLM&#22312;&#23545;&#35805;AI&#20013;&#23548;&#33322;&#35821;&#22659;&#28145;&#24230;&#30340;Token Trails
&lt;/p&gt;
&lt;p&gt;
Token Trails: Navigating Contextual Depths in Conversational AI with ChatLLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02402
&lt;/p&gt;
&lt;p&gt;
Token Trails&#26159;&#19968;&#31181;&#21033;&#29992;token-type&#23884;&#20837;&#23548;&#33322;&#23545;&#35805;&#20013;&#22797;&#26434;&#19978;&#19979;&#25991;&#32454;&#24494;&#24046;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#39640;&#23545;&#35805;&#29702;&#35299;&#21644;&#22238;&#22797;&#29983;&#25104;&#25928;&#26524;&#65292;&#22312;&#20419;&#36827;&#19978;&#19979;&#25991;&#24847;&#35782;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#26041;&#38754;&#20855;&#26377;&#21069;&#27839;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#23545;&#35805;&#24314;&#27169;&#38656;&#35201;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#32454;&#33268;&#29702;&#35299;&#65292;&#20197;&#29983;&#25104;&#36830;&#36143;&#19988;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#22797;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Token Trails&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;token-type&#23884;&#20837;&#26469;&#23548;&#33322;&#23545;&#35805;&#20013;&#22797;&#26434;&#19978;&#19979;&#25991;&#32454;&#24494;&#24046;&#21035;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;token-type&#23884;&#20837;&#26469;&#21306;&#20998;&#29992;&#25143;&#35805;&#35821;&#21644;&#26426;&#22120;&#20154;&#22238;&#22797;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#22238;&#22797;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Token Trails&#22312;&#25552;&#39640;&#23545;&#35805;&#29702;&#35299;&#21644;&#22238;&#22797;&#29983;&#25104;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#23545;&#35805;AI&#20013;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;Token Trails&#22312;&#25512;&#21160;&#35813;&#39046;&#22495;&#21457;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20026;&#26356;&#22797;&#26434;&#21644;&#20855;&#26377;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20132;&#20114;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02402v1 Announce Type: cross  Abstract: Conversational modeling using Large Language Models (LLMs) requires a nuanced understanding of context to generate coherent and contextually relevant responses. In this paper, we present Token Trails, a novel approach that leverages token-type embeddings to navigate the intricate contextual nuances within conversations. Our framework utilizes token-type embeddings to distinguish between user utterances and bot responses, facilitating the generation of context-aware replies. Through comprehensive experimentation and evaluation, we demonstrate the effectiveness of Token Trails in improving conversational understanding and response generation, achieving state-of-the-art performance. Our results highlight the significance of contextual modeling in conversational AI and underscore the promising potential of Token Trails to advance the field, paving the way for more sophisticated and contextually aware chatbot interactions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#21033;&#29992;&#23616;&#37096;&#24179;&#28369;&#32422;&#26463;&#22312;&#25193;&#25955;&#26694;&#26550;&#20013;&#22686;&#24378;&#28857;&#20113;&#29983;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#19988;&#26356;&#21152;&#24179;&#28369;&#30340;&#28857;&#20113;&#65292;&#20248;&#20110;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02396</link><description>&lt;p&gt;
&#21033;&#29992;&#24179;&#28369;&#32422;&#26463;&#22686;&#24378;&#22522;&#20110;&#25193;&#25955;&#30340;&#28857;&#20113;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Diffusion-based Point Cloud Generation with Smoothness Constraint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02396
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#21033;&#29992;&#23616;&#37096;&#24179;&#28369;&#32422;&#26463;&#22312;&#25193;&#25955;&#26694;&#26550;&#20013;&#22686;&#24378;&#28857;&#20113;&#29983;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#19988;&#26356;&#21152;&#24179;&#28369;&#30340;&#28857;&#20113;&#65292;&#20248;&#20110;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#28857;&#20113;&#29983;&#25104;&#20219;&#21153;&#20013;&#24456;&#21463;&#27426;&#36814;&#12290;&#29616;&#26377;&#20316;&#21697;&#21033;&#29992;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#23558;&#21407;&#22987;&#28857;&#20998;&#24067;&#36716;&#25442;&#20026;&#22122;&#22768;&#20998;&#24067;&#65292;&#28982;&#21518;&#23398;&#20064;&#36870;&#25193;&#25955;&#36807;&#31243;&#20174;&#22122;&#22768;&#20998;&#24067;&#20013;&#24674;&#22797;&#28857;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24573;&#35270;&#28857;&#20113;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#36870;&#25193;&#25955;&#36807;&#31243;&#21487;&#33021;&#20250;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20855;&#26377;&#38750;&#24179;&#28369;&#28857;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23558;&#23616;&#37096;&#24179;&#28369;&#32422;&#26463;&#32435;&#20837;&#25193;&#25955;&#26694;&#26550;&#20013;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#29992;&#20110;&#28857;&#20113;&#29983;&#25104;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#24418;&#29366;&#21644;&#26356;&#21152;&#24179;&#28369;&#30340;&#28857;&#20113;&#65292;&#20248;&#20110;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02396v1 Announce Type: cross  Abstract: Diffusion models have been popular for point cloud generation tasks. Existing works utilize the forward diffusion process to convert the original point distribution into a noise distribution and then learn the reverse diffusion process to recover the point distribution from the noise distribution. However, the reverse diffusion process can produce samples with non-smooth points on the surface because of the ignorance of the point cloud geometric properties. We propose alleviating the problem by incorporating the local smoothness constraint into the diffusion framework for point cloud generation. Experiments demonstrate the proposed model can generate realistic shapes and smoother point clouds, outperforming multiple state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22312;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#29305;&#23450;&#20248;&#36234;&#24615;&#24046;&#36317;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22522;&#20110;TDMA&#30340;&#31995;&#32479;&#30340;&#26368;&#20339;&#25209;&#37327;&#20998;&#37197;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#25552;&#20986;&#30340;&#20998;&#27493;&#25209;&#37327;&#20998;&#37197;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#22522;&#20110;RA&#30340;&#23398;&#20064;&#31995;&#32479;&#30340;&#23436;&#25104;&#26102;&#38388;</title><link>https://arxiv.org/abs/2404.02395</link><description>&lt;p&gt;
&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#30340;&#26368;&#20339;&#25209;&#37327;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Optimal Batch Allocation for Wireless Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02395
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22312;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#29305;&#23450;&#20248;&#36234;&#24615;&#24046;&#36317;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22522;&#20110;TDMA&#30340;&#31995;&#32479;&#30340;&#26368;&#20339;&#25209;&#37327;&#20998;&#37197;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#25552;&#20986;&#30340;&#20998;&#27493;&#25209;&#37327;&#20998;&#37197;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#22522;&#20110;RA&#30340;&#23398;&#20064;&#31995;&#32479;&#30340;&#23436;&#25104;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#36866;&#21512;&#20998;&#24067;&#22312;&#26412;&#22320;&#35774;&#22791;&#19978;&#30340;&#25968;&#25454;&#38598;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#30452;&#25509;&#35775;&#38382;&#31169;&#26377;&#25968;&#25454;&#65292;&#21033;&#29992;&#26381;&#21153;&#22120;&#19982;&#26412;&#22320;&#35774;&#22791;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#22312;&#23454;&#38469;&#36890;&#20449;&#26041;&#26696;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23454;&#29616;&#30446;&#26631;&#24615;&#33021;&#25152;&#38656;&#30340;&#23436;&#25104;&#26102;&#38388;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#38656;&#35201;&#22810;&#23569;&#27425;&#36845;&#20195;&#25165;&#33021;&#20351;&#32852;&#37030;&#23398;&#20064;&#36798;&#21040;&#20174;&#26368;&#23567;&#20840;&#23616;&#25439;&#22833;&#21040;&#29305;&#23450;&#26368;&#20248;&#24615;&#24046;&#36317;&#30340;&#27700;&#24179;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#29305;&#24449;&#21270;&#20102;&#20004;&#31181;&#22522;&#26412;&#30340;&#22810;&#29992;&#25143;&#25509;&#20837;&#26041;&#26696;&#19979;&#27599;&#27425;&#36845;&#20195;&#25152;&#38656;&#30340;&#26102;&#38388;&#65306;&#26102;&#20998;&#22810;&#22336;&#25509;&#20837;&#65288;TDMA&#65289;&#21644;&#38543;&#26426;&#25509;&#20837;&#65288;RA&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27493;&#25209;&#37327;&#20998;&#37197;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#22522;&#20110;TDMA&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#32780;&#35328;&#26159;&#26368;&#20339;&#30340;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#25552;&#20986;&#30340;&#20998;&#27493;&#25209;&#37327;&#20998;&#37197;&#26041;&#27861;&#25552;&#20379;&#30340;&#35774;&#22791;&#20043;&#38388;&#30340;&#38750;&#38646;&#25209;&#27425;&#24046;&#36317;&#26174;&#33879;&#38477;&#20302;&#20102;&#22522;&#20110;RA&#30340;&#23398;&#20064;&#31995;&#32479;&#30340;&#23436;&#25104;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02395v1 Announce Type: new  Abstract: Federated learning aims to construct a global model that fits the dataset distributed across local devices without direct access to private data, leveraging communication between a server and the local devices. In the context of a practical communication scheme, we study the completion time required to achieve a target performance. Specifically, we analyze the number of iterations required for federated learning to reach a specific optimality gap from a minimum global loss. Subsequently, we characterize the time required for each iteration under two fundamental multiple access schemes: time-division multiple access (TDMA) and random access (RA). We propose a step-wise batch allocation, demonstrated to be optimal for TDMA-based federated learning systems. Additionally, we show that the non-zero batch gap between devices provided by the proposed step-wise batch allocation significantly reduces the completion time for RA-based learning syst
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#36882;&#24402;&#25512;&#29702;&#26426;(rRIM)&#65292;&#19968;&#31181;&#36890;&#36807;&#29289;&#29702;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23454;&#39564;&#20809;&#35889;&#20013;&#25512;&#23548;&#37197;&#23545;&#31896;&#21512;&#20989;&#25968;&#65292;&#20855;&#26377;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21487;&#35299;&#20915;&#31532;&#19968;&#31867;Fredholm&#31215;&#20998;&#26041;&#31243;&#30340;&#31867;&#20284;&#21453;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02387</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#20809;&#35889;&#25968;&#25454;&#30340;&#21453;&#28436;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
An inversion problem for optical spectrum data via physics-guided machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02387
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#36882;&#24402;&#25512;&#29702;&#26426;(rRIM)&#65292;&#19968;&#31181;&#36890;&#36807;&#29289;&#29702;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23454;&#39564;&#20809;&#35889;&#20013;&#25512;&#23548;&#37197;&#23545;&#31896;&#21512;&#20989;&#25968;&#65292;&#20855;&#26377;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21487;&#35299;&#20915;&#31532;&#19968;&#31867;Fredholm&#31215;&#20998;&#26041;&#31243;&#30340;&#31867;&#20284;&#21453;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#27491;&#21017;&#21270;&#36882;&#24402;&#25512;&#29702;&#26426;(rRIM)&#65292;&#29992;&#20110;&#35299;&#20915;&#20174;&#27979;&#37327;&#21040;&#30340;&#20809;&#35889;&#20013;&#25512;&#23548;&#37197;&#23545;&#31896;&#21512;&#20989;&#25968;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;rRIM&#23558;&#29289;&#29702;&#21407;&#29702;&#34701;&#20837;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#65292;&#20855;&#26377;&#22122;&#22768;&#40065;&#26834;&#24615;&#12289;&#21487;&#20197;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#21644;&#20943;&#23569;&#25968;&#25454;&#38656;&#27714;&#30340;&#28789;&#27963;&#24615;&#12290;&#23427;&#26377;&#25928;&#22320;&#20174;&#23454;&#39564;&#20809;&#35889;&#20013;&#33719;&#21462;&#21487;&#38752;&#30340;&#37197;&#23545;&#31896;&#21512;&#20989;&#25968;&#65292;&#24182;&#20026;&#31532;&#19968;&#31867;Fredholm&#31215;&#20998;&#26041;&#31243;&#31867;&#20284;&#30340;&#21453;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02387v1 Announce Type: cross  Abstract: We propose the regularized recurrent inference machine (rRIM), a novel machine-learning approach to solve the challenging problem of deriving the pairing glue function from measured optical spectra. The rRIM incorporates physical principles into both training and inference and affords noise robustness, flexibility with out-of-distribution data, and reduced data requirements. It effectively obtains reliable pairing glue functions from experimental optical spectra and yields promising solutions for similar inverse problems of the Fredholm integral equation of the first kind.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#25554;&#20540;&#26465;&#20214;&#19979;&#23545;&#38543;&#26426;&#21152;&#36895;&#30340;&#19968;&#33324;&#21270;&#29256;&#26412;&#30340;&#26032;&#25910;&#25947;&#36895;&#24230;&#65292;&#22312;&#24378;&#22686;&#38271;&#26465;&#20214;&#19979;&#30340;&#21152;&#36895;SGD&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2404.02378</link><description>&lt;p&gt;
&#38024;&#23545;&#25554;&#20540;&#26465;&#20214;&#19979;&#38543;&#26426;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#30340;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Faster Convergence of Stochastic Accelerated Gradient Descent under Interpolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02378
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#25554;&#20540;&#26465;&#20214;&#19979;&#23545;&#38543;&#26426;&#21152;&#36895;&#30340;&#19968;&#33324;&#21270;&#29256;&#26412;&#30340;&#26032;&#25910;&#25947;&#36895;&#24230;&#65292;&#22312;&#24378;&#22686;&#38271;&#26465;&#20214;&#19979;&#30340;&#21152;&#36895;SGD&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25554;&#20540;&#26465;&#20214;&#19979;&#23545;&#38543;&#26426;Nesterov&#21152;&#36895;&#30340;&#19968;&#33324;&#21270;&#29256;&#26412;&#30340;&#26032;&#25910;&#25947;&#36895;&#24230;&#12290;&#19982;&#20808;&#21069;&#30340;&#20998;&#26512;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21152;&#36895;&#20102;&#20219;&#20309;&#22312;&#26399;&#26395;&#20013;&#21462;&#24471;&#36275;&#22815;&#36827;&#23637;&#30340;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#12290;&#35777;&#26126;&#20351;&#29992;&#20272;&#35745;&#24207;&#21015;&#26694;&#26550;&#36827;&#34892;&#65292;&#36866;&#29992;&#20110;&#20984;&#20989;&#25968;&#21644;&#24378;&#20984;&#20989;&#25968;&#65292;&#24182;&#21487;&#36731;&#26494;&#19987;&#38376;&#29992;&#20110;&#24378;&#22686;&#38271;&#26465;&#20214;&#19979;&#30340;&#21152;&#36895;SGD&#12290;&#22312;&#36825;&#31181;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#23558;&#24378;&#22686;&#38271;&#24120;&#25968;&#30340;&#20381;&#36182;&#24615;&#20174;$\rho$&#20943;&#23569;&#21040;$\sqrt{\rho}$&#12290;&#36825;&#31181;&#25913;&#36827;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30456;&#24403;&#20110;&#26465;&#20214;&#25968;&#30340;&#24179;&#26041;&#26681;&#65292;&#24182;&#35299;&#20915;&#20102;&#20851;&#20110;&#38543;&#26426;&#21152;&#36895;&#30340;&#20445;&#35777;&#21487;&#33021;&#27604;SGD&#26356;&#24046;&#30340;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02378v1 Announce Type: cross  Abstract: We prove new convergence rates for a generalized version of stochastic Nesterov acceleration under interpolation conditions. Unlike previous analyses, our approach accelerates any stochastic gradient method which makes sufficient progress in expectation. The proof, which proceeds using the estimating sequences framework, applies to both convex and strongly convex functions and is easily specialized to accelerated SGD under the strong growth condition. In this special case, our analysis reduces the dependence on the strong growth constant from $\rho$ to $\sqrt{\rho}$ as compared to prior work. This improvement is comparable to a square-root of the condition number in the worst case and address criticism that guarantees for stochastic acceleration could be worse than those for SGD.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20869;&#23384;&#20998;&#26512;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#25104;&#26412;&#25928;&#30410;&#30340;&#27169;&#31946;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#65292;&#37325;&#28857;&#35780;&#20272;&#20854;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.02372</link><description>&lt;p&gt;
&#27169;&#31946;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65306;&#36890;&#36807;&#20869;&#23384;&#20998;&#26512;&#35843;&#26597;&#30495;&#23454;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Obfuscated Malware Detection: Investigating Real-world Scenarios through Memory Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02372
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#23384;&#20998;&#26512;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#25104;&#26412;&#25928;&#30410;&#30340;&#27169;&#31946;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#65292;&#37325;&#28857;&#35780;&#20272;&#20854;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#32852;&#32593;&#21644;&#26234;&#33021;&#35774;&#22791;&#26102;&#20195;&#65292;&#24694;&#24847;&#36719;&#20214;&#30340;&#26816;&#27979;&#23545;&#31995;&#32479;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#24694;&#24847;&#36719;&#20214;&#20316;&#32773;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#27169;&#31946;&#25216;&#26415;&#26469;&#35268;&#36991;&#20808;&#36827;&#30340;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#26816;&#27979;&#21644;&#28040;&#38500;&#23041;&#32961;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#27169;&#31946;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20869;&#23384;&#36716;&#20648;&#20998;&#26512;&#65292;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;CIC-MalMem-2022&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#27169;&#25311;&#30495;&#23454;&#22330;&#26223;&#24182;&#35780;&#20272;&#22522;&#20110;&#20869;&#23384;&#30340;&#27169;&#31946;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22914;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02372v1 Announce Type: cross  Abstract: In the era of the internet and smart devices, the detection of malware has become crucial for system security. Malware authors increasingly employ obfuscation techniques to evade advanced security solutions, making it challenging to detect and eliminate threats. Obfuscated malware, adept at hiding itself, poses a significant risk to various platforms, including computers, mobile devices, and IoT devices. Conventional methods like heuristic-based or signature-based systems struggle against this type of malware, as it leaves no discernible traces on the system. In this research, we propose a simple and cost-effective obfuscated malware detection system through memory dump analysis, utilizing diverse machine-learning algorithms. The study focuses on the CIC-MalMem-2022 dataset, designed to simulate real-world scenarios and assess memory-based obfuscated malware detection. We evaluate the effectiveness of machine learning algorithms, such 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23398;&#20064;&#19982;&#20998;&#24067;&#20559;&#31227;&#30340;&#20132;&#38598;&#38382;&#39064;&#65292;&#22312;&#22522;&#20110;&#39640;&#26031;&#35757;&#32451;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#35777;&#26126;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#19978;&#30028;&#65292;&#21253;&#25324;&#19968;&#31181;TDS&#23398;&#20064;$k$&#20010;&#40784;&#27425;&#21322;&#31354;&#38388;&#20132;&#38598;&#36798;&#21040;&#31934;&#24230;$\epsilon$&#30340;$2^{(k/\epsilon)^{O(1)}} \mathsf{poly}(d)$&#26102;&#38388;&#31639;&#27861;&#65288;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65289;&#12290;</title><link>https://arxiv.org/abs/2404.02364</link><description>&lt;p&gt;
&#23398;&#20064;&#19982;&#20998;&#24067;&#20559;&#31227;&#30340;&#21322;&#31354;&#38388;&#20132;&#38598;&#65306;&#25913;&#36827;&#31639;&#27861;&#21644;SQ&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Learning Intersections of Halfspaces with Distribution Shift: Improved Algorithms and SQ Lower Bounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02364
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23398;&#20064;&#19982;&#20998;&#24067;&#20559;&#31227;&#30340;&#20132;&#38598;&#38382;&#39064;&#65292;&#22312;&#22522;&#20110;&#39640;&#26031;&#35757;&#32451;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#35777;&#26126;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#19978;&#30028;&#65292;&#21253;&#25324;&#19968;&#31181;TDS&#23398;&#20064;$k$&#20010;&#40784;&#27425;&#21322;&#31354;&#38388;&#20132;&#38598;&#36798;&#21040;&#31934;&#24230;$\epsilon$&#30340;$2^{(k/\epsilon)^{O(1)}} \mathsf{poly}(d)$&#26102;&#38388;&#31639;&#27861;&#65288;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Klivans&#12289;Stavropoulos&#21644;Vasilyan&#26368;&#36817;&#30340;&#24037;&#20316;&#24341;&#21457;&#20102;&#23545;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#21487;&#27979;&#35797;&#23398;&#20064;&#65288;TDS&#23398;&#20064;&#65289;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#20174;&#35757;&#32451;&#20998;&#24067;$\mathcal{D}$&#33719;&#24471;&#26631;&#35760;&#26679;&#26412;&#65292;&#20174;&#27979;&#35797;&#20998;&#24067;$\mathcal{D}'$&#33719;&#24471;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#30446;&#26631;&#26159;&#22312;&#35757;&#32451;&#26679;&#26412;&#36890;&#36807;&#30456;&#24212;&#30340;&#27979;&#35797;&#26102;&#36755;&#20986;&#22312;$\mathcal{D}'$&#19978;&#20855;&#26377;&#20302;&#35823;&#24046;&#30340;&#20998;&#31867;&#22120;&#12290;&#20182;&#20204;&#30340;&#27169;&#22411;&#19981;&#21516;&#20110;&#20808;&#21069;&#30340;&#25152;&#26377;&#24037;&#20316;&#65292;&#22240;&#20026;$\mathcal{D}'$&#19978;&#27809;&#26377;&#20551;&#35774;&#12290;&#30456;&#21453;&#65292;&#24403;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#36793;&#38469;&#30456;&#31561;&#26102;&#65292;&#27979;&#35797;&#24517;&#39035;&#25509;&#21463;&#65288;&#20197;&#39640;&#27010;&#29575;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02364v1 Announce Type: cross  Abstract: Recent work of Klivans, Stavropoulos, and Vasilyan initiated the study of testable learning with distribution shift (TDS learning), where a learner is given labeled samples from training distribution $\mathcal{D}$, unlabeled samples from test distribution $\mathcal{D}'$, and the goal is to output a classifier with low error on $\mathcal{D}'$ whenever the training samples pass a corresponding test. Their model deviates from all prior work in that no assumptions are made on $\mathcal{D}'$. Instead, the test must accept (with high probability) when the marginals of the training and test distributions are equal.   Here we focus on the fundamental case of intersections of halfspaces with respect to Gaussian training distributions and prove a variety of new upper bounds including a $2^{(k/\epsilon)^{O(1)}} \mathsf{poly}(d)$-time algorithm for TDS learning intersections of $k$ homogeneous halfspaces to accuracy $\epsilon$ (prior work achieved
&lt;/p&gt;</description></item><item><title>FraGNNet&#26159;&#19968;&#31181;&#29992;&#20110;&#21270;&#21512;&#29289;&#21040;&#36136;&#35889;&#39044;&#27979;&#30340;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#39044;&#27979;&#39640;&#20998;&#36776;&#29575;&#35889;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27169;&#22411;</title><link>https://arxiv.org/abs/2404.02360</link><description>&lt;p&gt;
FraGNNet&#65306;&#19968;&#31181;&#29992;&#20110;&#36136;&#35889;&#39044;&#27979;&#30340;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FraGNNet: A Deep Probabilistic Model for Mass Spectrum Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02360
&lt;/p&gt;
&lt;p&gt;
FraGNNet&#26159;&#19968;&#31181;&#29992;&#20110;&#21270;&#21512;&#29289;&#21040;&#36136;&#35889;&#39044;&#27979;&#30340;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#39044;&#27979;&#39640;&#20998;&#36776;&#29575;&#35889;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#22797;&#26434;&#28151;&#21512;&#29289;&#20013;&#21270;&#21512;&#29289;&#30340;&#26041;&#27861;&#20013;&#65292;&#36136;&#35889;&#30340;&#35782;&#21035;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#20256;&#32479;&#30340;&#36136;&#35889;&#21040;&#21270;&#21512;&#29289;&#65288;MS2C&#65289;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#23558;&#26410;&#30693;&#30340;&#36136;&#35889;&#19982;&#24050;&#30693;&#30340;&#36136;&#35889;-&#20998;&#23376;&#24211;&#36827;&#34892;&#21305;&#37197;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#21463;&#38480;&#20110;&#24211;&#35206;&#30422;&#19981;&#23436;&#25972;&#12290;&#21270;&#21512;&#29289;&#21040;&#36136;&#35889;&#65288;C2MS&#65289;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23558;&#30495;&#23454;&#24211;&#19982;&#39044;&#27979;&#35889;&#36827;&#34892;&#22686;&#24378;&#26469;&#25552;&#39640;&#26816;&#32034;&#29575;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;C2MS&#27169;&#22411;&#22312;&#39044;&#27979;&#20998;&#36776;&#29575;&#12289;&#21487;&#25193;&#23637;&#24615;&#25110;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;C2MS&#39044;&#27979;&#27010;&#29575;&#26041;&#27861;&#8212;&#8212;FraGNNet&#65292;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#39044;&#27979;&#39640;&#20998;&#36776;&#29575;&#35889;&#12290;FraGNNet&#20351;&#29992;&#32467;&#26500;&#21270;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#25581;&#31034;&#23450;&#20041;&#35889;&#30340;&#22522;&#26412;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;&#35823;&#24046;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;C2MS&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02360v1 Announce Type: new  Abstract: The process of identifying a compound from its mass spectrum is a critical step in the analysis of complex mixtures. Typical solutions for the mass spectrum to compound (MS2C) problem involve matching the unknown spectrum against a library of known spectrum-molecule pairs, an approach that is limited by incomplete library coverage. Compound to mass spectrum (C2MS) models can improve retrieval rates by augmenting real libraries with predicted spectra. Unfortunately, many existing C2MS models suffer from problems with prediction resolution, scalability, or interpretability. We develop a new probabilistic method for C2MS prediction, FraGNNet, that can efficiently and accurately predict high-resolution spectra. FraGNNet uses a structured latent space to provide insight into the underlying processes that define the spectrum. Our model achieves state-of-the-art performance in terms of prediction error, and surpasses existing C2MS models as a t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#40723;&#21169;&#22810;&#27169;&#24577;&#27169;&#22411;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#21333;&#27169;&#24577;&#27169;&#22411;&#20248;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02359</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#33539;&#24335;&#30340;&#24402;&#22240;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Attribution Regularization for Multimodal Paradigms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#40723;&#21169;&#22810;&#27169;&#24577;&#27169;&#22411;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#21333;&#27169;&#24577;&#27169;&#22411;&#20248;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#36817;&#24180;&#26469;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#25972;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#20197;&#22686;&#24378;&#23398;&#20064;&#21644;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#35266;&#23519;&#21040;&#21333;&#27169;&#24577;&#27169;&#22411;&#20248;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#23613;&#31649;&#21518;&#32773;&#21487;&#20197;&#35775;&#38382;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#21333;&#20010;&#27169;&#24577;&#30340;&#24433;&#21709;&#24120;&#24120;&#20027;&#23548;&#20915;&#31574;&#36807;&#31243;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#36825;&#20010;&#30740;&#31350;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#35813;&#39033;&#40723;&#21169;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#20570;&#20986;&#20915;&#31574;&#26102;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;&#35813;&#39033;&#30446;&#30340;&#37325;&#28857;&#22312;&#20110;&#35270;&#39057;-&#38899;&#39057;&#39046;&#22495;&#65292;&#23613;&#31649;&#25152;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#22312;&#28041;&#21450;&#22810;&#20010;&#27169;&#24577;&#30340;&#20307;&#29616;AI&#30740;&#31350;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#21069;&#26223;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#27491;&#21017;&#21270;&#39033;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02359v1 Announce Type: new  Abstract: Multimodal machine learning has gained significant attention in recent years due to its potential for integrating information from multiple modalities to enhance learning and decision-making processes. However, it is commonly observed that unimodal models outperform multimodal models, despite the latter having access to richer information. Additionally, the influence of a single modality often dominates the decision-making process, resulting in suboptimal performance. This research project aims to address these challenges by proposing a novel regularization term that encourages multimodal models to effectively utilize information from all modalities when making decisions. The focus of this project lies in the video-audio domain, although the proposed regularization technique holds promise for broader applications in embodied AI research, where multiple modalities are involved. By leveraging this regularization term, the proposed approach
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02353</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#22312;&#22270;&#20687;&#20013;&#36827;&#34892;&#35821;&#20041;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Semantic Augmentation in Images using Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02353
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#38750;&#24120;&#24222;&#22823;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#32570;&#20047;&#36825;&#20123;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#24182;&#38480;&#21046;&#20854;&#27867;&#21270;&#21040;&#29616;&#23454;&#19990;&#30028;&#31034;&#20363;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#21033;&#29992;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#25193;&#25955;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#22686;&#24378;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#26377;&#25928;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02353v1 Announce Type: cross  Abstract: Deep Learning models are incredibly data-hungry and require very large labeled datasets for supervised learning. As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples. Recent advancements in diffusion models have enabled the generation of photorealistic images based on textual inputs. Leveraging the substantial datasets used to train these diffusion models, we propose a technique to utilize generated images to augment existing datasets. This paper explores various strategies for effective data augmentation to improve the out-of-domain generalization capabilities of deep learning models.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#26399;&#26435;&#38544;&#21547;&#20449;&#24687;&#25913;&#36827;&#22810;&#36164;&#20135;&#26399;&#26435;&#30340;&#26080;&#27169;&#22411;&#19978;&#30028;&#35745;&#31639;&#26041;&#27861;</title><link>https://arxiv.org/abs/2404.02343</link><description>&lt;p&gt;
&#21033;&#29992;&#26399;&#26435;&#38544;&#21547;&#20449;&#24687;&#21644;&#28145;&#24230;&#23398;&#20064;&#25913;&#36827;&#22810;&#36164;&#20135;&#26399;&#26435;&#30340;&#26080;&#27169;&#22411;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Improved model-free bounds for multi-asset options using option-implied information and deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02343
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#26399;&#26435;&#38544;&#21547;&#20449;&#24687;&#25913;&#36827;&#22810;&#36164;&#20135;&#26399;&#26435;&#30340;&#26080;&#27169;&#22411;&#19978;&#30028;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#23558;&#20381;&#36182;&#24615;&#19981;&#30830;&#23450;&#24615;&#19982;&#26377;&#20851;&#20381;&#36182;&#32467;&#26500;&#30340;&#39069;&#22806;&#20449;&#24687;&#30456;&#32467;&#21512;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#22810;&#36164;&#20135;&#26399;&#26435;&#30340;&#26080;&#27169;&#22411;&#30028;&#38480;&#12290; &#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36793;&#38469;&#20998;&#24067;&#24050;&#30693;&#19988;&#24066;&#22330;&#19978;&#20063;&#26377;&#22810;&#36164;&#20135;&#26399;&#26435;&#20215;&#26684;&#30340;&#37096;&#20998;&#20449;&#24687;&#30340;&#24773;&#24418;&#12290; &#25105;&#20204;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#32473;&#20986;&#20102;&#36164;&#20135;&#23450;&#20215;&#30340;&#22522;&#26412;&#23450;&#29702;&#65292;&#20197;&#21450;&#19968;&#31181;&#36229;&#23545;&#20914;&#23545;&#20598;&#65292;&#33021;&#22815;&#23558;&#22312;&#27010;&#29575;&#24230;&#37327;&#19978;&#30340;&#26368;&#22823;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#20132;&#26131;&#31574;&#30053;&#19978;&#30340;&#26356;&#26131;&#22788;&#29702;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290; &#21518;&#32773;&#26159;&#36890;&#36807;&#32467;&#21512;&#32602;&#27454;&#26041;&#27861;&#21644;&#20511;&#21161;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#26469;&#35299;&#20915;&#30340;&#12290; &#25968;&#20540;&#26041;&#27861;&#24555;&#36895;&#65292;&#24182;&#19988;&#35745;&#31639;&#26102;&#38388;&#19982;&#20132;&#26131;&#36164;&#20135;&#25968;&#37327;&#25104;&#32447;&#24615;&#27604;&#20363;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#26816;&#39564;&#20102;&#21508;&#31181;&#39069;&#22806;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02343v1 Announce Type: cross  Abstract: We consider the computation of model-free bounds for multi-asset options in a setting that combines dependence uncertainty with additional information on the dependence structure. More specifically, we consider the setting where the marginal distributions are known and partial information, in the form of known prices for multi-asset options, is also available in the market. We provide a fundamental theorem of asset pricing in this setting, as well as a superhedging duality that allows to transform the maximization problem over probability measures in a more tractable minimization problem over trading strategies. The latter is solved using a penalization approach combined with a deep learning approximation using artificial neural networks. The numerical method is fast and the computational time scales linearly with respect to the number of traded assets. We finally examine the significance of various pieces of additional information. Em
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#27169;&#22411;&#30340;&#38381;&#29615;&#35757;&#32451;&#36807;&#31243;&#23481;&#26131;&#20135;&#29983;&#36864;&#21270;&#29616;&#35937;&#65292;&#27169;&#22411;&#21487;&#33021;&#24320;&#22987;&#29983;&#25104;&#26080;&#24847;&#20041;&#30340;&#25968;&#25454;&#25110;&#20165;&#20174;&#25152;&#38656;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#23567;&#37096;&#20998;&#20013;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2404.02325</link><description>&lt;p&gt;
&#38381;&#29615;&#23398;&#20064;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;&#28909;&#21147;&#23398;&#27515;&#20129;
&lt;/p&gt;
&lt;p&gt;
Heat Death of Generative Models in Closed-Loop Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02325
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#38381;&#29615;&#35757;&#32451;&#36807;&#31243;&#23481;&#26131;&#20135;&#29983;&#36864;&#21270;&#29616;&#35937;&#65292;&#27169;&#22411;&#21487;&#33021;&#24320;&#22987;&#29983;&#25104;&#26080;&#24847;&#20041;&#30340;&#25968;&#25454;&#25110;&#20165;&#20174;&#25152;&#38656;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#23567;&#37096;&#20998;&#20013;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25913;&#36827;&#21644;&#37319;&#32435;&#27491;&#22312;&#36805;&#36895;&#21152;&#36895;&#65292;&#20363;&#22914;&#25991;&#26412;&#20013;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#27969;&#34892;&#20197;&#21450;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#26222;&#21450;&#65292;&#23427;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#25972;&#21512;&#21040;&#20844;&#20849;&#32593;&#32476;&#20013;&#30340;&#20849;&#20139;&#20869;&#23481;&#20013;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#36865;&#22238;&#21040;&#27169;&#22411;&#36827;&#34892;&#21518;&#32493;&#35757;&#32451;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#12290;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#35757;&#32451;&#36807;&#31243;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#21363;&#20844;&#20849;&#21487;&#35775;&#38382;&#20869;&#23481;&#30340;&#20998;&#24067;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#30693;&#35782;&#8221;&#65289;&#26159;&#21542;&#20445;&#25345;&#31283;&#23450;&#36824;&#26159;&#23849;&#28291;&#12290;&#25991;&#29486;&#20013;&#25253;&#36947;&#30340;&#23567;&#35268;&#27169;&#23454;&#35777;&#23454;&#39564;&#26174;&#31034;&#65292;&#36825;&#31181;&#38381;&#29615;&#35757;&#32451;&#36807;&#31243;&#23481;&#26131;&#36864;&#21270;&#12290;&#27169;&#22411;&#21487;&#33021;&#24320;&#22987;&#29983;&#25104;&#26080;&#24847;&#20041;&#30340;&#25968;&#25454;&#65292;&#25110;&#32773;&#20165;&#20174;&#25152;&#38656;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#23567;&#37096;&#20998;&#20013;&#37319;&#26679;&#65288;&#31216;&#20026;&#27169;&#24335;&#23849;&#28291;&#29616;&#35937;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02325v1 Announce Type: new  Abstract: Improvement and adoption of generative machine learning models is rapidly accelerating, as exemplified by the popularity of LLMs (Large Language Models) for text, and diffusion models for image generation.As generative models become widespread, data they generate is incorporated into shared content through the public web. This opens the question of what happens when data generated by a model is fed back to the model in subsequent training campaigns. This is a question about the stability of the training process, whether the distribution of publicly accessible content, which we refer to as "knowledge", remains stable or collapses.   Small scale empirical experiments reported in the literature show that this closed-loop training process is prone to degenerating. Models may start producing gibberish data, or sample from only a small subset of the desired data distribution (a phenomenon referred to as mode collapse). So far there has been on
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SAMMO&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32534;&#35793;&#26102;&#20248;&#21270;&#20803;&#25552;&#31034;&#31243;&#24207;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#25552;&#31034;&#22312;&#22810;&#31181;&#19981;&#21516;LLM&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02319</link><description>&lt;p&gt;
Prompt&#20316;&#20026;&#31243;&#24207;&#65306;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39640;&#25928;&#32534;&#35793;&#26102;Prompt&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02319
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SAMMO&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32534;&#35793;&#26102;&#20248;&#21270;&#20803;&#25552;&#31034;&#31243;&#24207;&#65292;&#25552;&#39640;&#20102;&#22797;&#26434;&#25552;&#31034;&#22312;&#22810;&#31181;&#19981;&#21516;LLM&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29616;&#22312;&#33021;&#22788;&#29702;&#26356;&#38271;&#26356;&#22797;&#26434;&#30340;&#36755;&#20837;&#65292;&#36825;&#20419;&#36827;&#20102;&#26356;&#22797;&#26434;&#25552;&#31034;&#30340;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25552;&#31034;&#36890;&#24120;&#38656;&#35201;&#19968;&#20123;&#35843;&#25972;&#20197;&#25552;&#39640;&#37096;&#32626;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#20294;&#38543;&#30528;&#25552;&#31034;&#22797;&#26434;&#24230;&#21644;LLM&#24378;&#24230;&#30340;&#22686;&#21152;&#65292;&#35768;&#22810;&#25552;&#31034;&#20248;&#21270;&#25216;&#26415;&#24050;&#19981;&#20877;&#36275;&#22815;&#65292;&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#20803;&#25552;&#31034;&#31243;&#24207;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SAMMO&#65292;&#19968;&#20010;&#29992;&#20110;&#20803;&#25552;&#31034;&#31243;&#24207;&#30340;{\em &#32534;&#35793;&#26102;}&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#25552;&#31034;&#34920;&#31034;&#20026;&#32467;&#26500;&#21270;&#23545;&#35937;&#65292;&#20801;&#35768;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#25628;&#32034;&#19968;&#32452;&#20016;&#23500;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;SAMMO&#25512;&#24191;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#25351;&#20196;&#35843;&#25972;&#12289;RAG&#31649;&#32447;&#35843;&#25972;&#21644;&#25552;&#31034;&#21387;&#32553;&#26041;&#38754;&#25552;&#39640;&#20102;&#22797;&#26434;&#25552;&#31034;&#22312;&#22810;&#31181;&#19981;&#21516;LLM&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#25918;&#25152;&#26377;&#20195;&#30721;&#20379;&#22823;&#23478;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02319v1 Announce Type: cross  Abstract: Large language models (LLMs) can now handle longer and more complex inputs, which facilitate the use of more elaborate prompts. However, prompts often require some tuning to improve performance for deployment. Recent work has proposed automatic prompt optimization methods, but as prompt complexity and LLM strength increase, many prompt optimization techniques are no longer sufficient and a new approach is needed to optimize {\em meta prompt programs}. To address this, we introduce SAMMO, a framework for {\em compile-time} optimizations of metaprompt programs, which represent prompts as structured objects that allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs.   We make all code available open-sou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20998;&#23376;&#25968;&#25454;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#30340;&#27491;&#21017;&#21270;&#20108;&#27425;&#25506;&#38024;&#25439;&#22833;&#65292;&#24182;&#35774;&#35745;&#20102;&#22359;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#20351;&#24471;&#22312;&#40657;&#21283;&#23376;&#35774;&#32622;&#19979;&#65292;&#31616;&#21333;&#24494;&#35843;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#29305;&#23450;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#38656;&#35201;&#12290;</title><link>https://arxiv.org/abs/2404.02314</link><description>&lt;p&gt;
&#20998;&#23376;&#23569;&#26679;&#26412;&#23398;&#20064;&#26159;&#21542;&#30495;&#30340;&#38656;&#35201;&#20803;&#35757;&#32451;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Meta-training Really Necessary for Molecular Few-Shot Learning ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20998;&#23376;&#25968;&#25454;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#30340;&#27491;&#21017;&#21270;&#20108;&#27425;&#25506;&#38024;&#25439;&#22833;&#65292;&#24182;&#35774;&#35745;&#20102;&#22359;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#20351;&#24471;&#22312;&#40657;&#21283;&#23376;&#35774;&#32622;&#19979;&#65292;&#31616;&#21333;&#24494;&#35843;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#29305;&#23450;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#32780;&#26368;&#36817;&#24555;&#36895;&#22686;&#38271;&#30340;&#25991;&#29486;&#22823;&#22810;&#28041;&#21450;&#22797;&#26434;&#30340;&#20803;&#23398;&#20064;&#31574;&#30053;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26356;&#20026;&#30452;&#25509;&#30340;&#20998;&#23376;&#25968;&#25454;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#30340;&#27491;&#21017;&#21270;&#20108;&#27425;&#25506;&#38024;&#25439;&#22833;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#36991;&#20813;&#20102;&#25105;&#20204;&#25439;&#22833;&#20989;&#25968;&#30340;&#36864;&#21270;&#35299;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#24494;&#35843;&#26041;&#27861;&#22312;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#33719;&#24471;&#20102;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#40657;&#21283;&#23376;&#35774;&#32622;&#65292;&#24182;&#28040;&#38500;&#20102;&#29305;&#23450;&#24773;&#33410;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#38656;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#31454;&#20105;&#26041;&#27861;&#23545;&#39046;&#22495;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#24494;&#35843;&#22522;&#32447;&#22987;&#32456;&#27604;&#20803;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02314v1 Announce Type: cross  Abstract: Few-shot learning has recently attracted significant interest in drug discovery, with a recent, fast-growing literature mostly involving convoluted meta-learning strategies. We revisit the more straightforward fine-tuning approach for molecular data, and propose a regularized quadratic-probe loss based on the the Mahalanobis distance. We design a dedicated block-coordinate descent optimizer, which avoid the degenerate solutions of our loss. Interestingly, our simple fine-tuning approach achieves highly competitive performances in comparison to state-of-the-art methods, while being applicable to black-box settings and removing the need for specific episodic pre-training strategies. Furthermore, we introduce a new benchmark to assess the robustness of the competing methods to domain shifts. In this setting, our fine-tuning baseline obtains consistently better results than meta-learning methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Graph Neural Networks&#20998;&#26512;&#31354;&#38388;-&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#34394;&#25311;&#20256;&#24863;&#22120;&#65292;&#33021;&#22815;&#20174;&#20256;&#24863;&#22120;&#28378;&#36718;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#23558;&#25805;&#20316;&#26465;&#20214;&#26144;&#23556;&#20026;&#36724;&#25215;&#36127;&#36733;&#12290;</title><link>https://arxiv.org/abs/2404.02304</link><description>&lt;p&gt;
&#20351;&#29992;&#24322;&#26500;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23454;&#26102;&#36724;&#25215;&#36127;&#36733;&#39044;&#27979;&#30340;&#34394;&#25311;&#20256;&#24863;&#22120;
&lt;/p&gt;
&lt;p&gt;
Virtual Sensor for Real-Time Bearing Load Prediction Using Heterogeneous Temporal Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02304
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Graph Neural Networks&#20998;&#26512;&#31354;&#38388;-&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#34394;&#25311;&#20256;&#24863;&#22120;&#65292;&#33021;&#22815;&#20174;&#20256;&#24863;&#22120;&#28378;&#36718;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#23558;&#25805;&#20316;&#26465;&#20214;&#26144;&#23556;&#20026;&#36724;&#25215;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#36724;&#25215;&#36127;&#36733;&#30417;&#27979;&#23545;&#20110;&#20854;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#26377;&#21161;&#20110;&#25439;&#20260;&#35780;&#20272;&#12289;&#30952;&#25439;&#39044;&#27979;&#21644;&#20027;&#21160;&#32500;&#25252;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#31354;&#38388; - &#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#34394;&#25311;&#20256;&#24863;&#22120;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20998;&#26512;&#36724;&#25215;&#36127;&#36733;&#20043;&#38388;&#30340;&#31354;&#38388; - &#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02304v1 Announce Type: cross  Abstract: Accurate bearing load monitoring is essential for their Prognostics and Health Management (PHM), enabling damage assessment, wear prediction, and proactive maintenance. While bearing sensors are typically placed on the bearing housing, direct load monitoring requires sensors inside the bearing itself. Recently introduced sensor rollers enable direct bearing load monitoring but are constrained by their battery life. Data-driven virtual sensors can learn from sensor roller data collected during a batterys lifetime to map operating conditions to bearing loads. Although spatially distributed bearing sensors offer insights into load distribution (e.g., correlating temperature with load), traditional machine learning algorithms struggle to fully exploit these spatial-temporal dependencies. To address this gap, we introduce a graph-based virtual sensor that leverages Graph Neural Networks (GNNs) to analyze spatial-temporal dependencies among 
&lt;/p&gt;</description></item><item><title>CATGNN&#26159;&#19968;&#31181;&#25104;&#26412;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;&#25509;&#21463;&#36793;&#27969;&#20316;&#20026;&#36755;&#20837;&#24182;&#25552;&#20986;&#21517;&#20026;SPRING&#30340;&#27969;&#24335;&#20998;&#21306;&#31639;&#27861;&#65292;&#23454;&#29616;&#23558;GNN&#35757;&#32451;&#25193;&#23637;&#21040;&#25968;&#21313;&#20159;&#20197;&#19978;&#35268;&#27169;&#30340;&#22270;&#20013;&#12290;</title><link>https://arxiv.org/abs/2404.02300</link><description>&lt;p&gt;
CATGNN&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#26412;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CATGNN: Cost-Efficient and Scalable Distributed Training for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02300
&lt;/p&gt;
&lt;p&gt;
CATGNN&#26159;&#19968;&#31181;&#25104;&#26412;&#26377;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;&#25509;&#21463;&#36793;&#27969;&#20316;&#20026;&#36755;&#20837;&#24182;&#25552;&#20986;&#21517;&#20026;SPRING&#30340;&#27969;&#24335;&#20998;&#21306;&#31639;&#27861;&#65292;&#23454;&#29616;&#23558;GNN&#35757;&#32451;&#25193;&#23637;&#21040;&#25968;&#21313;&#20159;&#20197;&#19978;&#35268;&#27169;&#30340;&#22270;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#19981;&#21516;&#30340;GNN&#26550;&#26500;&#21644;&#35757;&#32451;&#31995;&#32479;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#23454;&#38469;&#22270;&#19978;&#36827;&#34892;GNN&#35757;&#32451;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#38656;&#35201;&#23558;&#25972;&#20010;&#22270;&#21152;&#36733;&#21040;&#20869;&#23384;&#20013;&#20197;&#36827;&#34892;&#22270;&#20998;&#21306;&#65292;&#38656;&#35201;&#22823;&#37327;&#20869;&#23384;&#31354;&#38388;&#26469;&#22788;&#29702;&#22823;&#22270;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#20351;&#29992;&#26222;&#36890;&#24037;&#20316;&#31449;&#22312;&#36825;&#20123;&#22823;&#22270;&#19978;&#36827;&#34892;GNN&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;CATGNN&#65292;&#19968;&#20010;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#31995;&#32479;&#65292;&#19987;&#27880;&#20110;&#22312;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#19979;&#23558;GNN&#35757;&#32451;&#25193;&#23637;&#21040;&#25968;&#21313;&#20159;&#29978;&#33267;&#26356;&#22823;&#35268;&#27169;&#30340;&#22270;&#20013;&#12290;&#22312;&#20854;&#20182;&#21151;&#33021;&#20013;&#65292;&#23427;&#25509;&#21463;&#19968;&#31995;&#21015;&#36793;&#20316;&#20026;&#36755;&#20837;&#65292;&#32780;&#19981;&#26159;&#23558;&#25972;&#20010;&#22270;&#21152;&#36733;&#21040;&#20869;&#23384;&#20013;&#36827;&#34892;&#20998;&#21306;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPRING&#30340;&#26032;&#22411;&#27969;&#24335;&#20998;&#21306;&#31639;&#27861;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;16&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;CATGNN&#19982;SPRING&#30340;&#27491;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02300v1 Announce Type: new  Abstract: Graph neural networks have been shown successful in recent years. While different GNN architectures and training systems have been developed, GNN training on large-scale real-world graphs still remains challenging. Existing distributed systems load the entire graph in memory for graph partitioning, requiring a huge memory space to process large graphs and thus hindering GNN training on such large graphs using commodity workstations. In this paper, we propose CATGNN, a cost-efficient and scalable distributed GNN training system which focuses on scaling GNN training to billion-scale or larger graphs under limited computational resources. Among other features, it takes a stream of edges as input, instead of loading the entire graph in memory, for partitioning. We also propose a novel streaming partitioning algorithm named SPRING for distributed GNN training. We verify the correctness and effectiveness of CATGNN with SPRING on 16 open datase
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#25351;&#20196;&#36827;&#34892;&#32422;&#26463;&#26426;&#22120;&#20154;&#23548;&#33322;&#65292;&#20943;&#23569;&#22320;&#22270;&#20381;&#36182;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#36890;&#36807;&#25991;&#26412;&#29983;&#25104;&#36974;&#32617;&#35782;&#21035;&#22320;&#26631;&#21644;&#22320;&#24418;&#31867;&#22411;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#22320;&#24418;&#30340;&#39640;&#25928;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2404.02294</link><description>&lt;p&gt;
&#21033;&#29992;LLM&#21644;&#35821;&#38899;&#25351;&#20196;&#22312;&#39318;&#36873;&#22320;&#24418;&#19978;&#36827;&#34892;&#32422;&#26463;&#26426;&#22120;&#20154;&#23548;&#33322;&#65306;&#25366;&#25496;&#21103;&#35789;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Constrained Robotic Navigation on Preferred Terrains Using LLMs and Speech Instruction: Exploiting the Power of Adverbs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02294
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#25351;&#20196;&#36827;&#34892;&#32422;&#26463;&#26426;&#22120;&#20154;&#23548;&#33322;&#65292;&#20943;&#23569;&#22320;&#22270;&#20381;&#36182;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#36890;&#36807;&#25991;&#26412;&#29983;&#25104;&#36974;&#32617;&#35782;&#21035;&#22320;&#26631;&#21644;&#22320;&#24418;&#31867;&#22411;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#22320;&#24418;&#30340;&#39640;&#25928;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26080;&#22320;&#22270;&#36234;&#37326;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26469;&#20943;&#23569;&#20256;&#32479;&#25968;&#25454;&#25910;&#38598;&#21644;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#25509;&#25910;&#21475;&#22836;&#25351;&#20196;&#65292;&#36890;&#36807;Whisper&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#25552;&#21462;&#22320;&#26631;&#12289;&#39318;&#36873;&#22320;&#24418;&#21644;&#20851;&#38190;&#21103;&#35789;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#29992;&#20110;&#32422;&#26463;&#23548;&#33322;&#30340;&#36895;&#24230;&#35774;&#32622;&#12290;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#29983;&#25104;&#22522;&#20110;&#25991;&#26412;&#30340;&#36974;&#32617;&#65292;&#29992;&#20110;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#22320;&#26631;&#21644;&#22320;&#24418;&#31867;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#30456;&#26426;&#21442;&#25968;&#23558;2D&#22270;&#20687;&#28857;&#36716;&#25442;&#21040;&#36710;&#36742;&#30340;&#36816;&#21160;&#24179;&#38754;&#65292;MPC&#25511;&#21046;&#22120;&#21487;&#20197;&#24341;&#23548;&#36710;&#36742;&#26397;&#21521;&#26399;&#26395;&#30340;&#22320;&#24418;&#12290;&#36825;&#19968;&#26041;&#27861;&#22686;&#24378;&#20102;&#23545;&#22810;&#26679;&#29615;&#22659;&#30340;&#36866;&#24212;&#24615;&#65292;&#20419;&#36827;&#20102;&#20351;&#29992;&#39640;&#32423;&#25351;&#20196;&#26469;&#23548;&#33322;&#22797;&#26434;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02294v1 Announce Type: cross  Abstract: This paper explores leveraging large language models for map-free off-road navigation using generative AI, reducing the need for traditional data collection and annotation. We propose a method where a robot receives verbal instructions, converted to text through Whisper, and a large language model (LLM) model extracts landmarks, preferred terrains, and crucial adverbs translated into speed settings for constrained navigation. A language-driven semantic segmentation model generates text-based masks for identifying landmarks and terrain types in images. By translating 2D image points to the vehicle's motion plane using camera parameters, an MPC controller can guides the vehicle towards the desired terrain. This approach enhances adaptation to diverse environments and facilitates the use of high-level instructions for navigating complex and challenging terrains.
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#22312;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#20154;&#25506;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#26144;&#23556;&#21644;&#22320;&#29699;&#25968;&#25454;&#38598;&#19978;&#30340;&#20803;&#21021;&#22987;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#39046;&#22495;&#22914;&#28779;&#26143;&#22320;&#24418;&#21644;&#20912;&#24029;&#30340;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02289</link><description>&lt;p&gt;
&#34892;&#26143;&#25506;&#27979;&#30340;&#32852;&#37030;&#22810;&#26234;&#33021;&#20307;&#24314;&#22270;
&lt;/p&gt;
&lt;p&gt;
Federated Multi-Agent Mapping for Planetary Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02289
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#20154;&#25506;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#26144;&#23556;&#21644;&#22320;&#29699;&#25968;&#25454;&#38598;&#19978;&#30340;&#20803;&#21021;&#22987;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#39046;&#22495;&#22914;&#28779;&#26143;&#22320;&#24418;&#21644;&#20912;&#24029;&#30340;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#20154;&#25506;&#27979;&#20013;&#65292;&#31649;&#29702;&#21644;&#26377;&#25928;&#21033;&#29992;&#21160;&#24577;&#29615;&#22659;&#20135;&#29983;&#30340;&#22823;&#37327;&#24322;&#26500;&#25968;&#25454;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#26144;&#23556;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#21327;&#20316;&#23398;&#20064;&#20013;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;FL&#20351;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#32852;&#21512;&#27169;&#22411;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#38598;&#20013;&#21270;&#25110;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#24102;&#23485;&#21644;&#23384;&#20648;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#26144;&#23556;&#65292;&#23558;&#22320;&#22270;&#34920;&#31034;&#20026;&#30001;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#20197;&#20415;&#23454;&#29616;&#32039;&#20945;&#21644;&#36866;&#24212;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22312;&#22320;&#29699;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20803;&#21021;&#22987;&#21270;&#26469;&#22686;&#24378;&#36825;&#19968;&#26041;&#27861;&#65292;&#39044;&#35757;&#32451;&#32593;&#32476;&#20197;&#24555;&#36895;&#23398;&#20064;&#26032;&#30340;&#22320;&#22270;&#32467;&#26500;&#12290;&#36825;&#31181;&#32452;&#21512;&#22312;&#35832;&#22914;&#28779;&#26143;&#22320;&#24418;&#21644;&#20912;&#24029;&#31561;&#19981;&#21516;&#39046;&#22495;&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#36825;&#19968;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02289v1 Announce Type: cross  Abstract: In multi-agent robotic exploration, managing and effectively utilizing the vast, heterogeneous data generated from dynamic environments poses a significant challenge. Federated learning (FL) is a promising approach for distributed mapping, addressing the challenges of decentralized data in collaborative learning. FL enables joint model training across multiple agents without requiring the centralization or sharing of raw data, overcoming bandwidth and storage constraints. Our approach leverages implicit neural mapping, representing maps as continuous functions learned by neural networks, for compact and adaptable representations. We further enhance this approach with meta-initialization on Earth datasets, pre-training the network to quickly learn new map structures. This combination demonstrates strong generalization to diverse domains like Martian terrain and glaciers. We rigorously evaluate this approach, demonstrating its effectiven
&lt;/p&gt;</description></item><item><title>&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;&#36890;&#36807;&#23558;LLMs&#38598;&#25104;&#21040;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#26377;&#25928;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#37327;&#65292;&#24182;&#21462;&#24471;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.02261</link><description>&lt;p&gt;
&#22312;LLMs&#20013;&#24490;&#29615;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#36827;&#34892;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02261
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;&#36890;&#36807;&#23558;LLMs&#38598;&#25104;&#21040;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#26377;&#25928;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#37327;&#65292;&#24182;&#21462;&#24471;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35821;&#35328;&#36164;&#28304;&#21644;&#25968;&#25454;&#26631;&#27880;&#19987;&#19994;&#30693;&#35782;&#26377;&#38480;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#20013;&#38754;&#20020;&#30528;&#37325;&#22823;&#38556;&#30861;&#65292;&#20351;&#23427;&#20204;&#21464;&#24471;&#32597;&#35265;&#19988;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#22312;&#20027;&#21160;&#23398;&#20064;&#29615;&#33410;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#35780;&#20272;&#20197;&#35780;&#20272;&#27880;&#37322;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#36873;&#25321;&#36866;&#24403;&#30340;LLM&#27880;&#37322;&#32773;&#12290;&#28982;&#21518;&#65292;&#36873;&#25321;&#30340;&#27880;&#37322;&#32773;&#34987;&#38598;&#25104;&#21040;&#19968;&#20010;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#24490;&#29615;&#20013;&#65292;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#65292;&#26368;&#23567;&#21270;&#25152;&#38656;&#30340;&#26597;&#35810;&#25968;&#25454;&#37327;&#12290;&#23454;&#35777;&#35780;&#20272;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;GPT-4-Turbo&#65292;&#23637;&#31034;&#20102;&#20960;&#20046;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#65292;&#30001;&#20272;&#31639;&#30340;&#28508;&#22312;&#24615;&#33021;&#25351;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02261v1 Announce Type: cross  Abstract: Low-resource languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly. The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets. To address this gap, we propose leveraging the potential of LLMs in the active learning loop for data annotation. Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable LLM annotator. The chosen annotator is then integrated into a training loop for a classifier using an active learning paradigm, minimizing the amount of queried data required. Empirical evaluations, notably employing GPT-4-Turbo, demonstrate near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;Mixture-of-Depths&#65292;&#21487;&#20197;&#22312;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#21160;&#24577;&#20998;&#37197;FLOPs&#20197;&#20248;&#21270;&#27169;&#22411;&#28145;&#24230;&#19978;&#19981;&#21516;&#23618;&#30340;&#24207;&#21015;&#20998;&#37197;&#12290;</title><link>https://arxiv.org/abs/2404.02258</link><description>&lt;p&gt;
Mixture-of-Depths: &#22312;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#21160;&#24577;&#20998;&#37197;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Depths: Dynamically allocating compute in transformer-based language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;Mixture-of-Depths&#65292;&#21487;&#20197;&#22312;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#21160;&#24577;&#20998;&#37197;FLOPs&#20197;&#20248;&#21270;&#27169;&#22411;&#28145;&#24230;&#19978;&#19981;&#21516;&#23618;&#30340;&#24207;&#21015;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#23558;FLOPs&#22343;&#21248;&#20998;&#24067;&#22312;&#36755;&#20837;&#24207;&#21015;&#20013;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;transformers&#21487;&#20197;&#23398;&#20064;&#21160;&#24577;&#22320;&#23558;FLOPs&#65288;&#25110;&#35745;&#31639;&#65289;&#20998;&#37197;&#32473;&#24207;&#21015;&#20013;&#30340;&#29305;&#23450;&#20301;&#32622;&#65292;&#20248;&#21270;&#21508;&#27169;&#22411;&#23618;&#28145;&#24230;&#19978;&#30340;&#24207;&#21015;&#20998;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35774;&#23450;&#22312;&#32473;&#23450;&#23618;&#20013;&#21487;&#21442;&#19982;&#33258;&#27880;&#24847;&#21147;&#21644;MLP&#35745;&#31639;&#30340;&#20196;&#29260;&#25968;&#65288;$k$&#65289;&#26469;&#23454;&#26045;&#24635;&#35745;&#31639;&#39044;&#31639;&#12290;&#35201;&#22788;&#29702;&#30340;&#20196;&#29260;&#30001;&#32593;&#32476;&#20351;&#29992;top-$k$&#36335;&#30001;&#26426;&#21046;&#30830;&#23450;&#12290;&#30001;&#20110;$k$&#26159;&#20107;&#20808;&#23450;&#20041;&#30340;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;&#36807;&#31243;&#20351;&#29992;&#20855;&#26377;&#24050;&#30693;&#24352;&#37327;&#22823;&#23567;&#30340;&#38745;&#24577;&#35745;&#31639;&#22270;&#65292;&#19981;&#21516;&#20110;&#20854;&#20182;&#26465;&#20214;&#35745;&#31639;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;$k$&#20196;&#29260;&#30340;&#26631;&#35782;&#26159;&#19981;&#22266;&#23450;&#30340;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38750;&#22343;&#21248;&#22320;&#36328;&#26102;&#38388;&#21644;&#27169;&#22411;&#28145;&#24230;&#32500;&#24230;&#20998;&#37197;FLOPs&#12290;&#22240;&#27492;&#65292;&#24635;&#20307;&#32780;&#35328;&#65292;&#35745;&#31639;&#25903;&#20986;&#23436;&#20840;&#21487;&#39044;&#27979;&#65292;&#20294;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02258v1 Announce Type: cross  Abstract: Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26356;&#24378;&#30340;&#24179;&#22343;&#24773;&#20917;&#35745;&#31639;&#20998;&#31163;&#65292;&#23545;&#20110;&#8220;&#20856;&#22411;&#8221;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#20219;&#21153;&#23454;&#20363;&#65292;&#21333;&#27169;&#24577;&#23398;&#20064;&#22312;&#35745;&#31639;&#19978;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#22810;&#27169;&#24577;&#23398;&#20064;&#21364;&#24456;&#23481;&#26131;&#12290;</title><link>https://arxiv.org/abs/2404.02254</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#27169;&#24577;&#19982;&#21333;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#26356;&#24378;&#30340;&#35745;&#31639;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02254
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26356;&#24378;&#30340;&#24179;&#22343;&#24773;&#20917;&#35745;&#31639;&#20998;&#31163;&#65292;&#23545;&#20110;&#8220;&#20856;&#22411;&#8221;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#20219;&#21153;&#23454;&#20363;&#65292;&#21333;&#27169;&#24577;&#23398;&#20064;&#22312;&#35745;&#31639;&#19978;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#22810;&#27169;&#24577;&#23398;&#20064;&#21364;&#24456;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23558;&#22810;&#31181;&#25968;&#25454;&#27169;&#24577;&#65288;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#32467;&#21512;&#36215;&#26469;&#20197;&#20419;&#36827;&#26356;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#36825;&#20173;&#28982;&#36866;&#29992;&#20110;&#30456;&#24212;&#30340;&#21333;&#27169;&#24577;&#20219;&#21153;&#65288;&#20363;&#22914;&#25991;&#26412;&#29983;&#25104;&#65289;&#12290;&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#32463;&#39564;&#25104;&#21151;&#65288;&#20363;&#22914;GPT-4&#65289;&#12290;&#21463;&#21040;&#20026;&#36825;&#31181;&#32463;&#39564;&#25104;&#21151;&#24320;&#21457;&#29702;&#35770;&#22522;&#30784;&#30340;&#21160;&#26426;&#65292;Lu&#65288;NeurIPS '23&#65292;ALT '24&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#23398;&#20064;&#29702;&#35770;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#23398;&#20064;&#30340;&#29702;&#35770;&#27169;&#22411;&#20043;&#38388;&#21487;&#33021;&#30340;&#20998;&#31163;&#12290;&#29305;&#21035;&#26159;Lu&#65288;ALT '24&#65289;&#23637;&#31034;&#20102;&#19968;&#31181;&#35745;&#31639;&#20998;&#31163;&#65292;&#36825;&#23545;&#23398;&#20064;&#20219;&#21153;&#30340;&#26368;&#22351;&#24773;&#20917;&#23454;&#20363;&#26159;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02254v1 Announce Type: cross  Abstract: In multimodal machine learning, multiple modalities of data (e.g., text and images) are combined to facilitate the learning of a better machine learning model, which remains applicable to a corresponding unimodal task (e.g., text generation). Recently, multimodal machine learning has enjoyed huge empirical success (e.g. GPT-4). Motivated to develop theoretical justification for this empirical success, Lu (NeurIPS '23, ALT '24) introduces a theory of multimodal learning, and considers possible separations between theoretical models of multimodal and unimodal learning. In particular, Lu (ALT '24) shows a computational separation, which is relevant to worst-case instances of the learning task.   In this paper, we give a stronger average-case computational separation, where for "typical" instances of the learning task, unimodal learning is computationally hard, but multimodal learning is easy. We then question how "organic" the average-cas
&lt;/p&gt;</description></item><item><title>RAT&#27169;&#22411;&#26159;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;CTR&#39044;&#27979;&#27169;&#22411;&#20165;&#20851;&#27880;&#26679;&#26412;&#20869;&#29305;&#24449;&#20132;&#20114;&#32780;&#24573;&#30053;&#36328;&#26679;&#26412;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#26816;&#32034;&#30456;&#20284;&#26679;&#26412;&#26500;&#24314;&#22686;&#24378;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#30340;&#20840;&#38754;&#29305;&#24449;&#20132;&#20114;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;CTR&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.02249</link><description>&lt;p&gt;
RAT: &#26816;&#32034;&#22686;&#24378;&#21464;&#25442;&#22120;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02249
&lt;/p&gt;
&lt;p&gt;
RAT&#27169;&#22411;&#26159;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;CTR&#39044;&#27979;&#27169;&#22411;&#20165;&#20851;&#27880;&#26679;&#26412;&#20869;&#29305;&#24449;&#20132;&#20114;&#32780;&#24573;&#30053;&#36328;&#26679;&#26412;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#26816;&#32034;&#30456;&#20284;&#26679;&#26412;&#26500;&#24314;&#22686;&#24378;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#30340;&#20840;&#38754;&#29305;&#24449;&#20132;&#20114;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;CTR&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#26159;Web&#24212;&#29992;&#31243;&#24207;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#35774;&#35745;&#26377;&#25928;&#30340;&#29305;&#24449;&#20132;&#20114;&#27169;&#22411;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#23545;&#21333;&#20010;&#26679;&#26412;&#20869;&#30340;&#29305;&#24449;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#24573;&#30053;&#20102;&#21487;&#20197;&#20316;&#20026;&#21442;&#32771;&#32972;&#26223;&#26469;&#22686;&#24378;&#39044;&#27979;&#30340;&#28508;&#22312;&#36328;&#26679;&#26412;&#20851;&#31995;&#12290;&#20026;&#24357;&#34917;&#36825;&#31181;&#19981;&#36275;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#21464;&#25442;&#22120;&#65288;RAT&#65289;&#65292;&#26088;&#22312;&#33719;&#21462;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#20132;&#20114;&#12290;&#36890;&#36807;&#26816;&#32034;&#30456;&#20284;&#26679;&#26412;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#30446;&#26631;&#26679;&#26412;&#26500;&#24314;&#22686;&#24378;&#36755;&#20837;&#12290;&#28982;&#21518;&#21033;&#29992;&#32423;&#32852;&#27880;&#24847;&#21147;&#26500;&#24314;Transformer&#23618;&#65292;&#20197;&#25429;&#33719;&#26679;&#26412;&#20869;&#21644;&#36328;&#26679;&#26412;&#29305;&#24449;&#20132;&#20114;&#65292;&#20419;&#36827;&#20840;&#38754;&#25512;&#29702;&#20197;&#25913;&#21892;CTR&#39044;&#27979;&#30340;&#21516;&#26102;&#20445;&#25345;&#25928;&#29575;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;RAT&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02249v1 Announce Type: cross  Abstract: Predicting click-through rates (CTR) is a fundamental task for Web applications, where a key issue is to devise effective models for feature interactions. Current methodologies predominantly concentrate on modeling feature interactions within an individual sample, while overlooking the potential cross-sample relationships that can serve as a reference context to enhance the prediction. To make up for such deficiency, this paper develops a Retrieval-Augmented Transformer (RAT), aiming to acquire fine-grained feature interactions within and across samples. By retrieving similar samples, we construct augmented input for each target sample. We then build Transformer layers with cascaded attention to capture both intra- and cross-sample feature interactions, facilitating comprehensive reasoning for improved CTR prediction while retaining efficiency. Extensive experiments on real-world datasets substantiate the effectiveness of RAT and sugge
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#20809;&#28369;&#20989;&#25968;&#21644;&#23545;&#25968;&#20985;&#25277;&#26679;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#20248;&#21270;&#21644;&#25277;&#26679;&#20013;&#24212;&#29992;&#36817;&#31471;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36817;&#31471;&#26144;&#23556;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02239</link><description>&lt;p&gt;
&#20248;&#21270;&#21644;&#25277;&#26679;&#30340;&#36817;&#31471;&#39044;&#35328;
&lt;/p&gt;
&lt;p&gt;
Proximal Oracles for Optimization and Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02239
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#20809;&#28369;&#20989;&#25968;&#21644;&#23545;&#25968;&#20985;&#25277;&#26679;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#20248;&#21270;&#21644;&#25277;&#26679;&#20013;&#24212;&#29992;&#36817;&#31471;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36817;&#31471;&#26144;&#23556;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#38750;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#21644;&#23545;&#25968;&#20985;&#25277;&#26679;&#65288;&#24102;&#38750;&#20809;&#28369;&#28508;&#21183;&#65292;&#21363;&#36127;&#23545;&#25968;&#23494;&#24230;&#65289;&#30340;&#20984;&#20248;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#20855;&#20307;&#35774;&#32622;&#65292;&#20854;&#20013;&#20984;&#30446;&#26631;/&#28508;&#21183;&#20989;&#25968;&#35201;&#20040;&#26159;&#21322;&#20809;&#28369;&#30340;&#65292;&#35201;&#20040;&#26159;&#22797;&#21512;&#24418;&#24335;&#65292;&#20316;&#20026;&#21322;&#20809;&#28369;&#20998;&#37327;&#30340;&#26377;&#38480;&#21644;&#12290;&#20026;&#20102;&#20811;&#26381;&#30001;&#20110;&#38750;&#20809;&#28369;&#24615;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20248;&#21270;&#21644;&#25277;&#26679;&#20013;&#37319;&#29992;&#20102;&#20004;&#31181;&#24378;&#22823;&#30340;&#36817;&#31471;&#26694;&#26550;&#65306;&#20248;&#21270;&#20013;&#30340;&#36817;&#31471;&#28857;&#26694;&#26550;&#21644;&#26367;&#20195;&#25277;&#26679;&#26694;&#26550;&#65288;ASF&#65289;&#65292;&#35813;&#26694;&#26550;&#22312;&#22686;&#24191;&#20998;&#24067;&#19978;&#20351;&#29992;Gibbs&#25277;&#26679;&#12290;&#20248;&#21270;&#21644;&#25277;&#26679;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#26159;&#36890;&#36807;&#27491;&#21017;&#21270;&#20999;&#24179;&#38754;&#26041;&#27861;&#39640;&#25928;&#23454;&#29616;&#36817;&#31471;&#26144;&#23556;&#12290;&#25105;&#20204;&#22312;&#21322;&#20809;&#28369;&#21644;&#22797;&#21512;&#35774;&#32622;&#20013;&#24314;&#31435;&#20102;&#36817;&#31471;&#26144;&#23556;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#20809;&#28369;&#20248;&#21270;&#30340;&#33258;&#36866;&#24212;&#36817;&#31471;&#25414;&#32465;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02239v1 Announce Type: cross  Abstract: We consider convex optimization with non-smooth objective function and log-concave sampling with non-smooth potential (negative log density). In particular, we study two specific settings where the convex objective/potential function is either semi-smooth or in composite form as the finite sum of semi-smooth components. To overcome the challenges caused by non-smoothness, our algorithms employ two powerful proximal frameworks in optimization and sampling: the proximal point framework for optimization and the alternating sampling framework (ASF) that uses Gibbs sampling on an augmented distribution. A key component of both optimization and sampling algorithms is the efficient implementation of the proximal map by the regularized cutting-plane method. We establish the iteration-complexity of the proximal map in both semi-smooth and composite settings. We further propose an adaptive proximal bundle method for non-smooth optimization. The 
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#29305;&#24449;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#30340;&#20851;&#31995;&#65292;&#23578;&#26410;&#34987;&#26126;&#30830;&#34920;&#24449;&#12290;&#30740;&#31350;&#35797;&#22270;&#29702;&#35299;&#25506;&#32034;&#29305;&#24449;&#19982;&#25913;&#36827;&#24615;&#33021;&#21644;&#25928;&#29575;&#22312;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#20851;&#31995;</title><link>https://arxiv.org/abs/2404.02235</link><description>&lt;p&gt;
&#25506;&#32034;&#26159;&#24744;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;&#21527;&#65311;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#36716;&#31227;&#30340;&#26377;&#25928;&#25506;&#32034;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Is Exploration All You Need? Effective Exploration Characteristics for Transfer in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02235
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#29305;&#24449;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#26377;&#25928;&#30340;&#36716;&#31227;&#23398;&#20064;&#30340;&#20851;&#31995;&#65292;&#23578;&#26410;&#34987;&#26126;&#30830;&#34920;&#24449;&#12290;&#30740;&#31350;&#35797;&#22270;&#29702;&#35299;&#25506;&#32034;&#29305;&#24449;&#19982;&#25913;&#36827;&#24615;&#33021;&#21644;&#25928;&#29575;&#22312;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30740;&#31350;&#20013;&#65292;&#20154;&#20204;&#27491;&#22312;&#21162;&#21147;&#35774;&#35745;&#26356;&#39640;&#25928;&#12289;&#26356;&#20855;&#29983;&#20135;&#21147;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#38382;&#39064;&#12290;&#36825;&#20123;&#25506;&#32034;&#26041;&#27861;&#36890;&#24120;&#20849;&#20139;&#20849;&#21516;&#21407;&#21017;&#65288;&#20363;&#22914;&#25913;&#21892;&#22810;&#26679;&#24615;&#65289;&#21644;&#23454;&#29616;&#32454;&#33410;&#65288;&#20363;&#22914;&#20869;&#22312;&#22870;&#21169;&#65289;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#38750;&#38745;&#27490;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#38656;&#35201;&#25506;&#32034;&#65292;&#20197;&#20415;&#26377;&#25928;&#22320;&#36866;&#24212;&#22312;&#32447;&#36716;&#31227;&#23398;&#20064;&#20013;&#29615;&#22659;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#20855;&#20307;&#25506;&#32034;&#29305;&#24449;&#19982;&#22312;&#28145;&#24230;RL&#20013;&#30340;&#26377;&#25928;&#36716;&#31227;&#23398;&#20064;&#20043;&#38388;&#30340;&#20851;&#31995;&#23578;&#26410;&#34987;&#34920;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#29702;&#35299;&#26174;&#33879;&#25506;&#32034;&#29305;&#24449;&#19982;&#25913;&#36827;&#24615;&#33021;&#21644;&#25928;&#29575;&#22312;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21313;&#19968;&#20010;&#27969;&#34892;&#30340;&#25506;&#32034;&#31639;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#36716;&#31227;&#31867;&#22411;&#36827;&#34892;&#27979;&#35797;&#65292;&#20197;&#30830;&#23450;&#37027;&#20123;&#31215;&#26497;&#24433;&#21709;&#22312;&#32447;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02235v1 Announce Type: cross  Abstract: In deep reinforcement learning (RL) research, there has been a concerted effort to design more efficient and productive exploration methods while solving sparse-reward problems. These exploration methods often share common principles (e.g., improving diversity) and implementation details (e.g., intrinsic reward). Prior work found that non-stationary Markov decision processes (MDPs) require exploration to efficiently adapt to changes in the environment with online transfer learning. However, the relationship between specific exploration characteristics and effective transfer learning in deep RL has not been characterized. In this work, we seek to understand the relationships between salient exploration characteristics and improved performance and efficiency in transfer learning. We test eleven popular exploration algorithms on a variety of transfer types -- or ``novelties'' -- to identify the characteristics that positively affect onlin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#28857;&#20113;&#25968;&#25454;&#27979;&#37327;Manning's n&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#22312;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#65292;&#32780;&#19988;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#30417;&#31649;&#21644;&#26497;&#31471;&#26292;&#38632;&#20107;&#20214;&#19979;&#20063;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02234</link><description>&lt;p&gt;
&#20351;&#29992;3D&#28857;&#20113;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27700;&#21160;&#21147;&#27946;&#27700;&#27169;&#22411;&#20013;&#30340;&#32463;&#39564;&#25705;&#25830;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks with 3D Point Clouds for Empirical Friction Measurements in Hydrodynamic Flood Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02234
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#28857;&#20113;&#25968;&#25454;&#27979;&#37327;Manning's n&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#22312;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#65292;&#32780;&#19988;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#30417;&#31649;&#21644;&#26497;&#31471;&#26292;&#38632;&#20107;&#20214;&#19979;&#20063;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25705;&#25830;&#26159;&#27700;&#21160;&#21147;&#24314;&#27169;&#30340;&#20851;&#38190;&#20043;&#19968;&#65307;&#27946;&#27700;&#26465;&#20214;&#23545;&#29992;&#20110;&#35745;&#31639;&#21160;&#37327;&#25439;&#22833;&#30340;&#25705;&#25830;&#31995;&#25968;&#65288;FFs&#65289;&#38750;&#24120;&#25935;&#24863;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#24615;&#30340;&#25705;&#25830;&#31995;&#25968;&#38590;&#20197;&#27979;&#37327;&#65292;&#22240;&#20026;&#38656;&#35201;&#36827;&#34892;&#23454;&#39564;&#23460;&#23454;&#39564;&#12290;&#27946;&#27700;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#26367;&#20195;&#24615;&#35266;&#27979;&#65288;&#22914;&#22303;&#22320;&#21033;&#29992;&#65289;&#26469;&#20272;&#35745;&#25705;&#25830;&#31995;&#25968;&#65292;&#23548;&#33268;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#23454;&#39564;&#23460;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26681;&#25454;&#28857;&#20113;&#25968;&#25454;&#27979;&#37327;Manning's n&#12290;&#35813;DNN&#34987;&#37096;&#32626;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#19978;&#65292;&#30452;&#25509;&#27979;&#37327;&#20102;&#22312;&#30417;&#31649;&#21644;&#26497;&#31471;&#26292;&#38632;&#20107;&#20214;&#19979;&#30340;Manning's n&#65292;&#23637;&#31034;&#20102;&#22312;1D&#21644;2D&#27700;&#21160;&#21147;&#27169;&#22411;&#20013;&#37117;&#26377;&#25913;&#36827;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#23545;&#20110;1D&#27169;&#22411;&#65292;&#19982;&#22303;&#22320;&#35206;&#30422;&#20540;&#30456;&#27604;&#65292;&#28608;&#20809;&#38647;&#36798;&#25968;&#20540;&#20351;&#24471;&#27827;&#36947;&#27700;&#28145;&#30340;&#24046;&#24322;&#20943;&#23567;&#20102;&#12290;&#23545;&#20110;1D/2D&#32806;&#21512;&#27169;&#22411;&#65292;&#28608;&#20809;&#38647;&#36798;&#25968;&#20540;&#20135;&#29983;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02234v1 Announce Type: new  Abstract: Friction is one of the cruxes of hydrodynamic modeling; flood conditions are highly sensitive to the Friction Factors (FFs) used to calculate momentum losses. However, empirical FFs are challenging to measure because they require laboratory experiments. Flood models often rely on surrogate observations (such as land use) to estimate FFs, introducing uncertainty. This research presents a laboratory-trained Deep Neural Network (DNN), trained using flume experiments with data augmentation techniques, to measure Manning's n based on Point Cloud data. The DNN was deployed on real-world lidar Point Clouds to directly measure Manning's n under regulatory and extreme storm events, showing improved prediction capabilities in both 1D and 2D hydrodynamic models. For 1D models, the lidar values decreased differences with regulatory models for in-channel water depth when compared to land cover values. For 1D/2D coupled models, the lidar values produc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#23450;&#20301;&#25216;&#26415;&#36827;&#34892;&#35270;&#30028;&#36712;&#36857;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#35270;&#37326;&#22806;&#29289;&#20307;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#22122;&#22768;&#30340;&#25361;&#25112;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02227</link><description>&lt;p&gt;
OOSTraj&#65306;&#21033;&#29992;&#35270;&#35273;&#23450;&#20301;&#21435;&#22122;&#30340;&#35270;&#30028;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02227
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#23450;&#20301;&#25216;&#26415;&#36827;&#34892;&#35270;&#30028;&#36712;&#36857;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#35270;&#37326;&#22806;&#29289;&#20307;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#22122;&#22768;&#30340;&#25361;&#25112;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#39044;&#27979;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#21160;&#39550;&#39542;&#20013;&#26159;&#22522;&#30784;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#29702;&#35299;&#34892;&#20154;&#34892;&#20026;&#21644;&#23454;&#29616;&#31215;&#26497;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#20551;&#35774;&#31934;&#30830;&#23436;&#25972;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#24573;&#35270;&#20102;&#19982;&#35270;&#37326;&#22806;&#29289;&#20307;&#21644;&#30001;&#20110;&#26377;&#38480;&#25668;&#20687;&#22836;&#33539;&#22260;&#12289;&#29289;&#29702;&#38556;&#30861;&#20197;&#21450;&#32570;&#20047;&#21435;&#22122;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#30495;&#23454;&#25968;&#25454;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#36825;&#26679;&#30340;&#20559;&#35265;&#26159;&#20851;&#38190;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#21487;&#33021;&#23548;&#33268;&#20851;&#38190;&#30340;&#38750;&#21487;&#35265;&#23545;&#35937;&#34987;&#24573;&#30053;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#23450;&#20301;&#25216;&#26415;&#36827;&#34892;&#35270;&#30028;&#36712;&#36857;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#21435;&#22122;&#22024;&#26434;&#30340;&#20256;&#24863;&#22120;&#35266;&#27979;&#65292;&#24182;&#23558;&#35270;&#37326;&#22806;&#29289;&#20307;&#30340;&#20256;&#24863;&#22120;&#36712;&#36857;&#31934;&#30830;&#26144;&#23556;&#21040;&#35270;&#35273;&#36712;&#36857;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35270;&#37326;&#22806;&#36712;&#36857;&#39044;&#27979;&#26041;&#38754;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02227v1 Announce Type: cross  Abstract: Trajectory prediction is fundamental in computer vision and autonomous driving, particularly for understanding pedestrian behavior and enabling proactive decision-making. Existing approaches in this field often assume precise and complete observational data, neglecting the challenges associated with out-of-view objects and the noise inherent in sensor data due to limited camera range, physical obstructions, and the absence of ground truth for denoised sensor data. Such oversights are critical safety concerns, as they can result in missing essential, non-visible objects. To bridge this gap, we present a novel method for out-of-sight trajectory prediction that leverages a vision-positioning technique. Our approach denoises noisy sensor observations in an unsupervised manner and precisely maps sensor-based trajectories of out-of-sight objects into visual trajectories. This method has demonstrated state-of-the-art performance in out-of-sig
&lt;/p&gt;</description></item><item><title>&#20943;&#23567;&#35268;&#27169;&#25968;&#25454;&#35757;&#32451;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#21487;&#22312;&#31616;&#21270;&#35821;&#35328;&#20013;&#23454;&#29616;&#19982;&#22823;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02204</link><description>&lt;p&gt;
&#20943;&#23567;&#35268;&#27169;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26032;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Emergent Abilities in Reduced-Scale Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02204
&lt;/p&gt;
&lt;p&gt;
&#20943;&#23567;&#35268;&#27169;&#25968;&#25454;&#35757;&#32451;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#21487;&#22312;&#31616;&#21270;&#35821;&#35328;&#20013;&#23454;&#29616;&#19982;&#22823;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;&#36825;&#31181;&#33021;&#21147;&#65292;&#20063;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26032;&#20852;&#33021;&#21147;&#65292;&#20027;&#35201;&#20986;&#29616;&#22312;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#31181;&#26032;&#20852;&#23646;&#24615;&#26159;&#21542;&#20005;&#26684;&#19982;&#27169;&#22411;&#22823;&#23567;&#30456;&#20851;&#65292;&#25110;&#32773;&#21487;&#20197;&#36890;&#36807;&#22312;&#20943;&#23567;&#35268;&#27169;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#36739;&#23567;&#27169;&#22411;&#26469;&#23637;&#31034;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;36&#20010;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#21442;&#25968;&#20174;100&#19975;&#21040;1.65&#20159;&#19981;&#31561;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#31616;&#21270;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#31616;&#21270;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#19982;&#22312;&#33258;&#30001;&#35821;&#35328;&#19978;&#20845;&#20493;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#36825;&#34920;&#26126;&#65292;&#32553;&#23567;&#35821;&#35328;&#35268;&#27169;&#21487;&#20197;&#20351;&#20855;&#26377;&#26377;&#38480;&#22823;&#23567;&#30340;&#27169;&#22411;&#20986;&#29616;&#38646;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02204v1 Announce Type: new  Abstract: Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study investigates if such emergent properties are strictly tied to model size or can be demonstrated by smaller models trained on reduced-scale data. To explore this, we simplify pre-training data and pre-train 36 causal language models with parameters varying from 1 million to 165 million parameters. We show that models trained on this simplified pre-training data demonstrate enhanced zero-shot capabilities across various tasks in simplified language, achieving performance comparable to that of pre-trained models six times larger on unrestricted language. This suggests that downscaling the language allows zero-shot learning capabilities to emerge in models with limited size. Additionally, we f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20843;&#20010;&#26032;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24341;&#36215;&#22312;NAS&#24320;&#21457;&#20013;&#30340;&#20851;&#27880;&#24182;&#40723;&#21169;&#20316;&#32773;&#32771;&#34385;&#27169;&#22411;&#22312;&#24320;&#21457;&#26102;&#26410;&#30693;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.02189</link><description>&lt;p&gt;
&#20174;&#20043;&#21069;&#26410;&#35265;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Insights from the Use of Previously Unseen Neural Architecture Search Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02189
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20843;&#20010;&#26032;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24341;&#36215;&#22312;NAS&#24320;&#21457;&#20013;&#30340;&#20851;&#27880;&#24182;&#40723;&#21169;&#20316;&#32773;&#32771;&#34385;&#27169;&#22411;&#22312;&#24320;&#21457;&#26102;&#26410;&#30693;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38480;&#21487;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#29992;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#19981;&#21516;&#65292;&#22240;&#27492;&#38656;&#35201;&#28145;&#24230;&#23398;&#20064;&#19987;&#23478;&#26469;&#30830;&#23450;&#26368;&#20339;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#36829;&#32972;&#20102;&#28040;&#38500;&#19987;&#23478;&#38656;&#27714;&#30340;&#24076;&#26395;&#12290;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#26368;&#20339;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;NAS&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#19968;&#23567;&#32452;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#25968;&#25454;&#38598;&#24182;&#19981;&#33021;&#20195;&#34920;&#30495;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20843;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#19968;&#31995;&#21015;NAS&#25361;&#25112;&#65306;AddNIST&#65292;Language&#65292;MultNIST&#65292;CIFARTile&#65292;Gutenberg&#65292;Isabella&#65292;GeoClassing &#21644; Chesseract&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#21644;&#25361;&#25112;&#26088;&#22312;&#24341;&#36215;NAS&#24320;&#21457;&#20013;&#30340;&#27880;&#24847;&#21644;&#40723;&#21169;&#20316;&#32773;&#32771;&#34385;&#20182;&#20204;&#30340;&#27169;&#22411;&#22312;&#24320;&#21457;&#26102;&#26410;&#30693;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02189v1 Announce Type: cross  Abstract: The boundless possibility of neural networks which can be used to solve a problem -- each with different performance -- leads to a situation where a Deep Learning expert is required to identify the best neural network. This goes against the hope of removing the need for experts. Neural Architecture Search (NAS) offers a solution to this by automatically identifying the best architecture. However, to date, NAS work has focused on a small set of datasets which we argue are not representative of real-world problems. We introduce eight new datasets created for a series of NAS Challenges: AddNIST, Language, MultNIST, CIFARTile, Gutenberg, Isabella, GeoClassing, and Chesseract. These datasets and challenges are developed to direct attention to issues in NAS development and to encourage authors to consider how their models will perform on datasets unknown to them at development time. We present experimentation using standard Deep Learning met
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22788;&#29702;&#20107;&#25925;&#20005;&#37325;&#24615;&#24314;&#27169;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#35299;&#20915;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#37325;&#37319;&#26679;&#26041;&#27861;&#38590;&#20197;&#22788;&#29702;&#31163;&#25955;&#39118;&#38505;&#22240;&#32032;&#23849;&#28291;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.02187</link><description>&lt;p&gt;
&#37319;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#20107;&#25925;&#20005;&#37325;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
A Generative Deep Learning Approach for Crash Severity Modeling with Imbalanced Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02187
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22788;&#29702;&#20107;&#25925;&#20005;&#37325;&#24615;&#24314;&#27169;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#35299;&#20915;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#37325;&#37319;&#26679;&#26041;&#27861;&#38590;&#20197;&#22788;&#29702;&#31163;&#25955;&#39118;&#38505;&#22240;&#32032;&#23849;&#28291;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25758;&#36710;&#25968;&#25454;&#36890;&#24120;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#24179;&#34913;&#24615;&#65292;&#22823;&#22810;&#25968;&#20107;&#25925;&#26159;&#38750;&#33268;&#21629;&#20107;&#25925;&#65292;&#21482;&#26377;&#23569;&#25968;&#26159;&#30001;&#20110;&#20854;&#32597;&#35265;&#24615;&#32780;&#33268;&#21629;&#20107;&#25925;&#12290;&#36825;&#31181;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#23545;&#20107;&#25925;&#20005;&#37325;&#24615;&#24314;&#27169;&#26500;&#25104;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38590;&#20197;&#25311;&#21512;&#21644;&#35299;&#37322;&#20855;&#26377;&#38750;&#24120;&#26377;&#38480;&#26679;&#26412;&#30340;&#33268;&#21629;&#20107;&#25925;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36890;&#36807;&#25968;&#25454;&#37325;&#37319;&#26679;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#31181;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#22914;&#27424;&#37319;&#26679;&#21644;&#36807;&#37319;&#26679;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#22914;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#36830;&#32493;&#21464;&#37327;&#12290;&#23613;&#31649;&#19968;&#20123;&#37325;&#37319;&#26679;&#26041;&#27861;&#24050;&#32463;&#25913;&#36827;&#20197;&#22788;&#29702;&#36830;&#32493;&#21644;&#31163;&#25955;&#21464;&#37327;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#31232;&#30095;&#31163;&#25955;&#39118;&#38505;&#22240;&#32032;&#30456;&#20851;&#30340;&#23849;&#28291;&#38382;&#39064;&#19978;&#21487;&#33021;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#20840;&#38754;&#30740;&#31350;&#26469;&#27604;&#36739;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02187v1 Announce Type: cross  Abstract: Crash data is often greatly imbalanced, with the majority of crashes being non-fatal crashes, and only a small number being fatal crashes due to their rarity. Such data imbalance issue poses a challenge for crash severity modeling since it struggles to fit and interpret fatal crash outcomes with very limited samples. Usually, such data imbalance issues are addressed by data resampling methods, such as under-sampling and over-sampling techniques. However, most traditional and deep learning-based data resampling methods, such as synthetic minority oversampling technique (SMOTE) and generative Adversarial Networks (GAN) are designed dedicated to processing continuous variables. Though some resampling methods have improved to handle both continuous and discrete variables, they may have difficulties in dealing with the collapse issue associated with sparse discrete risk factors. Moreover, there is a lack of comprehensive studies that compar
&lt;/p&gt;</description></item><item><title>&#38598;&#25104;&#27169;&#22411;&#22312;&#20809;&#35889;&#25968;&#25454;&#20998;&#26512;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#33021;&#22815;&#25345;&#32493;&#20248;&#20110;&#20854;&#20182;&#20505;&#36873;&#27169;&#22411;</title><link>https://arxiv.org/abs/2404.02184</link><description>&lt;p&gt;
&#38598;&#25104;&#27169;&#22411;&#22312;&#20998;&#26512;&#20809;&#35889;&#25968;&#25454;&#20013;&#30340;&#30410;&#22788;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What is to be gained by ensemble models in analysis of spectroscopic data?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02184
&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#27169;&#22411;&#22312;&#20809;&#35889;&#25968;&#25454;&#20998;&#26512;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#33021;&#22815;&#25345;&#32493;&#20248;&#20110;&#20854;&#20182;&#20505;&#36873;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#26088;&#22312;&#25913;&#21892;&#20809;&#35889;&#25968;&#25454;&#39044;&#27979;&#30340;&#19981;&#21516;&#38598;&#25104;&#27169;&#22411;&#23454;&#29616;&#12290;&#23558;&#19968;&#31995;&#21015;&#20505;&#36873;&#27169;&#22411;&#25311;&#21512;&#21040;&#22238;&#24402;&#21644;&#20998;&#31867;&#35774;&#32622;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#12290;&#21033;&#29992;&#32447;&#24615;&#28151;&#21512;&#27169;&#22411;&#23545;&#27169;&#22411;&#25311;&#21512;&#30340;&#39044;&#27979;&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#65292;&#35813;&#20998;&#26512;&#32467;&#26524;&#22522;&#20110;&#25968;&#25454;&#30340;&#38543;&#26426;&#25286;&#20998;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25105;&#20204;&#30340;&#24212;&#29992;&#20013;&#65292;&#38598;&#25104;&#20998;&#31867;&#22120;&#33021;&#22815;&#22987;&#32456;&#20248;&#20110;&#20505;&#36873;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02184v1 Announce Type: new  Abstract: An empirical study was carried out to compare different implementations of ensemble models aimed at improving prediction in spectroscopic data. A wide range of candidate models were fitted to benchmark datasets from regression and classification settings. A statistical analysis using linear mixed model was carried out on prediction performance criteria resulting from model fits over random splits of the data. The results showed that the ensemble classifiers were able to consistently outperform candidate models in our application
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#32452;&#32455;&#22810;&#20195;&#29702;&#26694;&#26550;&#65288;SoA&#65289;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#20195;&#30721;&#30340;&#21487;&#25193;&#23637;&#39640;&#25928;&#29983;&#25104;&#21644;&#20248;&#21270;&#65292;&#20195;&#29702;&#21487;&#33258;&#20027;&#36816;&#20316;&#29983;&#25104;&#21644;&#20462;&#25913;&#20195;&#30721;&#32452;&#20214;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#22797;&#26434;&#24615;&#21160;&#24577;&#22686;&#21152;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.02183</link><description>&lt;p&gt;
&#33258;&#32452;&#32455;&#20195;&#29702;&#65306;&#38754;&#21521;&#36229;&#22823;&#35268;&#27169;&#20195;&#30721;&#29983;&#25104;&#21644;&#20248;&#21270;&#30340;LLM&#22810;&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02183
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#32452;&#32455;&#22810;&#20195;&#29702;&#26694;&#26550;&#65288;SoA&#65289;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#20195;&#30721;&#30340;&#21487;&#25193;&#23637;&#39640;&#25928;&#29983;&#25104;&#21644;&#20248;&#21270;&#65292;&#20195;&#29702;&#21487;&#33258;&#20027;&#36816;&#20316;&#29983;&#25104;&#21644;&#20462;&#25913;&#20195;&#30721;&#32452;&#20214;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#22797;&#26434;&#24615;&#21160;&#24577;&#22686;&#21152;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#36827;&#34892;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#20351;&#25105;&#20204;&#26356;&#25509;&#36817;&#33258;&#21160;&#36719;&#20214;&#24320;&#21457;&#30340;&#26410;&#26469;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21333;&#20195;&#29702;&#26041;&#27861;&#22312;&#29983;&#25104;&#21644;&#25913;&#36827;&#22823;&#35268;&#27169;&#22797;&#26434;&#20195;&#30721;&#24211;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#36825;&#26159;&#30001;&#20110;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#25152;&#23548;&#33268;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#32452;&#32455;&#22810;&#20195;&#29702;&#26694;&#26550;&#65288;SoA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#20195;&#30721;&#30340;&#21487;&#25193;&#23637;&#39640;&#25928;&#29983;&#25104;&#21644;&#20248;&#21270;&#12290;&#22312;SoA&#20013;&#65292;&#33258;&#32452;&#32455;&#20195;&#29702;&#29420;&#31435;&#36816;&#20316;&#65292;&#20197;&#29983;&#25104;&#21644;&#20462;&#25913;&#20195;&#30721;&#32452;&#20214;&#65292;&#21516;&#26102;&#26080;&#32541;&#21327;&#20316;&#26500;&#24314;&#25972;&#20307;&#20195;&#30721;&#24211;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#22522;&#20110;&#38382;&#39064;&#22797;&#26434;&#24615;&#33258;&#21160;&#22686;&#21152;&#20195;&#29702;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#21160;&#24577;&#25193;&#23637;&#24615;&#12290;&#36825;&#20351;&#24471;&#25972;&#20307;&#20195;&#30721;&#37327;&#21487;&#20197;&#26681;&#25454;&#20195;&#29702;&#25968;&#37327;&#26080;&#38480;&#22686;&#21152;&#65292;&#21516;&#26102;&#30001;&#27599;&#20010;&#20195;&#29702;&#31649;&#29702;&#30340;&#20195;&#30721;&#37327;&#20063;&#38543;&#20043;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02183v1 Announce Type: cross  Abstract: Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by eac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#19968;&#31181;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#24265;&#20215;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;INDT-ASD&#21360;&#24230;&#25968;&#25454;&#24211;&#26089;&#26399;&#26816;&#27979;&#33258;&#38381;&#30151;&#12290;</title><link>https://arxiv.org/abs/2404.02181</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36890;&#36807;INDT-ASD&#21360;&#24230;&#25968;&#25454;&#24211;&#26089;&#26399;&#26816;&#27979;&#33258;&#38381;&#30151;
&lt;/p&gt;
&lt;p&gt;
Leveraging Machine Learning for Early Autism Detection via INDT-ASD Indian Database
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#19968;&#31181;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#24265;&#20215;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;INDT-ASD&#21360;&#24230;&#25968;&#25454;&#24211;&#26089;&#26399;&#26816;&#27979;&#33258;&#38381;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#21307;&#30103;&#39046;&#22495;&#21457;&#23637;&#36805;&#36895;&#65292;&#31070;&#32463;&#21457;&#32946;&#38382;&#39064;&#30340;&#35786;&#26029;&#26159;&#21307;&#30103;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#12290;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#26159;&#20840;&#29699;&#22686;&#38271;&#26368;&#24555;&#30340;&#21457;&#23637;&#24615;&#38556;&#30861;&#20043;&#19968;&#12290;&#20020;&#24202;&#31579;&#26597;&#27979;&#35797;&#29992;&#20110;&#35782;&#21035;&#33258;&#38381;&#30151;&#30151;&#29366;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#20294;&#29616;&#22312;&#38543;&#30528;ML&#30340;&#36827;&#23637;&#65292;&#26089;&#26399;&#35782;&#21035;&#33258;&#38381;&#30151;&#21464;&#24471;&#21487;&#34892;&#12290;&#20808;&#21069;&#36827;&#34892;&#36807;&#35768;&#22810;&#19981;&#21516;&#25216;&#26415;&#30340;&#25506;&#32034;&#65292;&#20294;&#22312;&#21033;&#29992;&#32463;&#36807;&#20020;&#24202;&#39564;&#35777;&#30340;&#21360;&#24230;ASD&#25968;&#25454;&#24211;&#39044;&#27979;&#33258;&#38381;&#30151;&#29305;&#24449;&#30340;&#33021;&#21147;&#26041;&#38754;&#65292;&#27809;&#26377;&#19968;&#20010;&#20135;&#29983;&#39044;&#26399;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;ML&#24320;&#21457;&#19968;&#31181;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#24265;&#20215;&#30340;&#25216;&#26415;&#26469;&#35782;&#21035;ASD&#12290;&#37319;&#29992;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;Adaboost&#65288;AB&#65289;&#12289;Gradient Boost&#65288;GB&#65289;&#12289;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#12289;&#36923;&#36753;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02181v1 Announce Type: cross  Abstract: Machine learning (ML) has advanced quickly, particularly throughout the area of health care. The diagnosis of neurodevelopment problems using ML is a very important area of healthcare. Autism spectrum disorder (ASD) is one of the developmental disorders that is growing the fastest globally. The clinical screening tests used to identify autistic symptoms are expensive and time-consuming. But now that ML has been advanced, it's feasible to identify autism early on. Previously, many different techniques have been used in investigations. Still, none of them have produced the anticipated outcomes when it comes to the capacity to predict autistic features utilizing a clinically validated Indian ASD database. Therefore, this study aimed to develop a simple, quick, and inexpensive technique for identifying ASD by using ML. Various machine learning classifiers, including Adaboost (AB), Gradient Boost (GB), Decision Tree (DT), Logistic Regressio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#36965;&#24863;&#25968;&#25454;&#22320;&#36136;&#21046;&#22270;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;</title><link>https://arxiv.org/abs/2404.02180</link><description>&lt;p&gt;
&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#22320;&#36136;&#21046;&#22270;&#30340;&#36965;&#24863;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Remote sensing framework for geological mapping via stacked autoencoders and clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02180
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#36965;&#24863;&#25968;&#25454;&#22320;&#36136;&#21046;&#22270;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36965;&#24863;&#22320;&#36136;&#21046;&#22270;&#20013;&#38754;&#20020;&#30528;&#30001;&#20110;&#20934;&#30830;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#32780;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25581;&#31034;&#36965;&#24863;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32467;&#26500;&#12290;&#38477;&#32500;&#26041;&#27861;&#20855;&#26377;&#22312;&#25552;&#39640;&#22320;&#36136;&#22270;&#20934;&#30830;&#24615;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#38477;&#32500;&#26041;&#27861;&#21487;&#33021;&#22312;&#38750;&#32447;&#24615;&#25968;&#25454;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#20294;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#27169;&#25311;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#20855;&#26377;&#22810;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#23618;&#65292;&#29992;&#20110;&#25429;&#33719;&#23545;&#36965;&#24863;&#25968;&#25454;&#26377;&#29992;&#30340;&#20998;&#23618;&#25968;&#25454;&#34920;&#31034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#22788;&#29702;&#36965;&#24863;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02180v1 Announce Type: cross  Abstract: Supervised learning methods for geological mapping via remote sensing face limitations due to the scarcity of accurately labelled training data. In contrast, unsupervised learning methods, such as dimensionality reduction and clustering have the ability to uncover patterns and structures in remote sensing data without relying on predefined labels. Dimensionality reduction methods have the potential to play a crucial role in improving the accuracy of geological maps. Although conventional dimensionality reduction methods may struggle with nonlinear data, unsupervised deep learning models such as autoencoders have the ability to model nonlinear relationship in data. Stacked autoencoders feature multiple interconnected layers to capture hierarchical data representations that can be useful for remote sensing data. In this study, we present an unsupervised machine learning framework for processing remote sensing data by utilizing stacked au
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#37327;&#23376;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#28857;&#65292;&#37325;&#28857;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31639;&#27861;&#23545;&#23567;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2404.02177</link><description>&lt;p&gt;
&#25506;&#32034;&#37327;&#23376;&#22686;&#24378;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#24212;&#29992;&#21644;&#23545;&#22024;&#26434;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Exploring Quantum-Enhanced Machine Learning for Computer Vision: Applications and Insights on Noisy Intermediate-Scale Quantum Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#37327;&#23376;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#28857;&#65292;&#37325;&#28857;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31639;&#27861;&#23545;&#23567;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#36827;&#23637;&#65292;&#37327;&#23376;&#31639;&#27861;&#22312;&#27169;&#25311;&#29289;&#29702;&#31995;&#32479;&#12289;&#21270;&#23398;&#12289;&#20248;&#21270;&#21644;&#23494;&#30721;&#23398;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34987;&#31216;&#20026;&#22024;&#26434;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20419;&#20351;&#23547;&#25214;&#33021;&#22815;&#21457;&#25381;&#37327;&#23376;&#20248;&#21183;&#32780;&#26080;&#38656;&#24191;&#27867;&#38169;&#35823;&#26657;&#27491;&#31243;&#24207;&#30340;&#24212;&#29992;&#12290;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#65292;&#38754;&#20020;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#21644;&#31639;&#27861;&#19981;&#36879;&#26126;&#24615;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#37327;&#23376;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#28857;&#65292;&#37325;&#28857;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#35780;&#20272;&#20102;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#25968;&#25454;&#37325;&#26032;&#19978;&#20256;&#26041;&#26696;&#21644;&#36148;&#29255;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#27169;&#22411;&#65292;&#22312;&#23567;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#19978;&#12290;&#36890;&#36807;&#23454;&#38469;&#23454;&#26045;&#21644;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02177v1 Announce Type: cross  Abstract: As medium-scale quantum computers progress, the application of quantum algorithms across diverse fields like simulating physical systems, chemistry, optimization, and cryptography becomes more prevalent. However, these quantum computers, known as Noisy Intermediate Scale Quantum (NISQ), are susceptible to noise, prompting the search for applications that can capitalize on quantum advantage without extensive error correction procedures. Since, Machine Learning (ML), particularly Deep Learning (DL), faces challenges due to resource-intensive training and algorithmic opacity. Therefore, this study explores the intersection of quantum computing and ML, focusing on computer vision tasks. Specifically, it evaluates the effectiveness of hybrid quantum-classical algorithms, such as the data re-uploading scheme and the patch Generative Adversarial Networks (GAN) model, on small-scale quantum devices. Through practical implementation and testing
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#24066;&#22330;&#33829;&#38144;&#21160;&#24577;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26041;&#31243;&#65292;&#20934;&#30830;&#25429;&#25417;&#20102;&#24191;&#21578;&#25903;&#20986;&#19982;&#28040;&#36153;&#32773;&#21453;&#24212;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02175</link><description>&lt;p&gt;
&#28040;&#36153;&#32773;&#21453;&#24212;&#30340;&#31038;&#20250;&#21160;&#24577;&#65306;&#34701;&#21512;&#32479;&#35745;&#29289;&#29702;&#23398;&#19982;&#33829;&#38144;&#21160;&#24577;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Social Dynamics of Consumer Response: A Unified Framework Integrating Statistical Physics and Marketing Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02175
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#24066;&#22330;&#33829;&#38144;&#21160;&#24577;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26041;&#31243;&#65292;&#20934;&#30830;&#25429;&#25417;&#20102;&#24191;&#21578;&#25903;&#20986;&#19982;&#28040;&#36153;&#32773;&#21453;&#24212;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28040;&#36153;&#32773;&#23545;&#24191;&#21578;&#36755;&#20837;&#30340;&#21453;&#24212;&#23545;&#20110;&#26088;&#22312;&#20248;&#21270;&#24191;&#21578;&#31574;&#30053;&#24182;&#25552;&#39640;&#24191;&#21578;&#27963;&#21160;&#26377;&#25928;&#24615;&#30340;&#33829;&#38144;&#20154;&#21592;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#28304;&#33258;&#29289;&#29702;&#23398;&#21644;&#31038;&#20250;&#24515;&#29702;&#23398;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#28040;&#36153;&#32773;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26041;&#31243;&#65292;&#25429;&#25417;&#20102;&#24191;&#21578;&#25903;&#20986;&#19982;&#28040;&#36153;&#32773;&#21453;&#24212;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21033;&#29992;&#20102;&#35832;&#22914;&#23545;&#31216;&#24615;&#12289;&#26631;&#24230;&#24459;&#21644;&#30456;&#21464;&#31561;&#27010;&#24565;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#31243;&#39564;&#35777;&#19982;Michaelis-Menten&#21644;Hill&#26041;&#31243;&#31561;&#33879;&#21517;&#27169;&#22411;&#30456;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#22312;&#20934;&#30830;&#34920;&#31034;&#28040;&#36153;&#32773;&#21453;&#24212;&#21160;&#24577;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20998;&#26512;&#24378;&#35843;&#20102;&#20851;&#38190;&#27169;&#22411;&#21442;&#25968;&#65288;&#22914;&#33829;&#38144;&#25928;&#26524;&#12289;&#21453;&#24212;&#25935;&#24863;&#24230;&#21644;&#34892;&#20026;&#25935;&#24863;&#24230;&#65289;&#23545;&#24433;&#21709;&#28040;&#36153;&#32773;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#24191;&#21578;&#21830;&#21644;&#33829;&#38144;&#20154;&#21592;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02175v1 Announce Type: cross  Abstract: Comprehending how consumers react to advertising inputs is essential for marketers aiming to optimize advertising strategies and improve campaign effectiveness. This study examines the complex nature of consumer behaviour by applying theoretical frameworks derived from physics and social psychology. We present an innovative equation that captures the relation between spending on advertising and consumer response, using concepts such as symmetries, scaling laws, and phase transitions. By validating our equation against well-known models such as the Michaelis-Menten and Hill equations, we prove its effectiveness in accurately representing the complexity of consumer response dynamics. The analysis emphasizes the importance of key model parameters, such as marketing effectiveness, response sensitivity, and behavioural sensitivity, in influencing consumer behaviour. The work explores the practical implications for advertisers and marketers,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#22312;&#22797;&#26434;&#30340;&#27611;&#32454;&#34880;&#31649;&#32593;&#32476;&#20013;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30913;&#24615;&#24494;&#28216;&#27891;&#22120;&#30340;&#36335;&#24452;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2404.02171</link><description>&lt;p&gt;
&#30913;&#24615;&#24494;&#28216;&#27891;&#22120;&#22312;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#20154;&#20307;&#27611;&#32454;&#34880;&#31649;&#20013;&#30340;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Path planning of magnetic microswimmers in high-fidelity simulations of capillaries with deep reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#22312;&#22797;&#26434;&#30340;&#27611;&#32454;&#34880;&#31649;&#32593;&#32476;&#20013;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30913;&#24615;&#24494;&#28216;&#27891;&#22120;&#30340;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#22914;&#23450;&#21521;&#33647;&#29289;&#36755;&#36865;&#12289;&#24494;&#21019;&#25163;&#26415;&#25110;&#24863;&#30693;&#20381;&#36182;&#20110;&#20197;&#26368;&#23567;&#21019;&#20260;&#26041;&#24335;&#21040;&#36798;&#36523;&#20307;&#20869;&#31934;&#30830;&#21306;&#22495;&#12290;&#20154;&#24037;&#32454;&#33740;&#38829;&#27611;(ABFs)&#24050;&#34987;&#35270;&#20026;&#36890;&#36807;&#24490;&#29615;&#31995;&#32479;&#23548;&#33322;&#23436;&#25104;&#27492;&#20219;&#21153;&#30340;&#28508;&#22312;&#24037;&#20855;&#12290;&#34429;&#28982;&#22312;&#31616;&#21333;&#22330;&#26223;&#20013;&#24050;&#20102;&#35299;ABFs&#30340;&#25511;&#21046;&#21644;&#28216;&#27891;&#29305;&#24615;&#65292;&#20294;&#23427;&#20204;&#22312;&#34880;&#28082;&#20013;&#30340;&#34892;&#20026;&#20173;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;ABFs&#22312;&#20154;&#31867;&#35270;&#32593;&#33180;&#20013;&#22797;&#26434;&#27611;&#32454;&#34880;&#31649;&#32593;&#32476;&#20013;&#28436;&#21270;&#30340;&#27169;&#25311;&#12290;&#36890;&#36807;&#20043;&#21069;&#22312;&#38477;&#38454;&#27169;&#22411;&#19978;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;ABF&#34987;&#31283;&#20581;&#22320;&#24341;&#23548;&#21040;&#25351;&#23450;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02171v1 Announce Type: cross  Abstract: Biomedical applications such as targeted drug delivery, microsurgery or sensing rely on reaching precise areas within the body in a minimally invasive way. Artificial bacterial flagella (ABFs) have emerged as potential tools for this task by navigating through the circulatory system. While the control and swimming characteristics of ABFs is understood in simple scenarios, their behavior within the bloodstream remains unclear. We conduct simulations of ABFs evolving in the complex capillary networks found in the human retina. The ABF is robustly guided to a prescribed target by a reinforcement learning agent previously trained on a reduced order model.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#20998;&#27573;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;AUTODIFF&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#21517;&#20026;conformal motif&#30340;&#26032;&#22411;&#20998;&#23376;&#32452;&#35013;&#31574;&#30053;&#21644;SE(3)-&#31561;&#21464;&#21367;&#31215;&#32593;&#32476;&#32534;&#30721;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#32467;&#26500;&#22522;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#23616;&#37096;&#32467;&#26500;&#21644;&#26500;&#35937;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02003</link><description>&lt;p&gt;
AUTODIFF&#65306;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02003
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#20998;&#27573;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;AUTODIFF&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#21517;&#20026;conformal motif&#30340;&#26032;&#22411;&#20998;&#23376;&#32452;&#35013;&#31574;&#30053;&#21644;SE(3)-&#31561;&#21464;&#21367;&#31215;&#32593;&#32476;&#32534;&#30721;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#32467;&#26500;&#22522;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#23616;&#37096;&#32467;&#26500;&#21644;&#26500;&#35937;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#22522;&#33647;&#29289;&#35774;&#35745;&#65288;SBDD&#65289;&#26088;&#22312;&#29983;&#25104;&#33021;&#22815;&#32039;&#23494;&#32467;&#21512;&#38774;&#34507;&#30333;&#30340;&#20998;&#23376;&#65292;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#21021;&#27493;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#26080;&#25928;&#30340;&#23616;&#37096;&#32467;&#26500;&#25110;&#19981;&#29616;&#23454;&#30340;&#26500;&#35937;&#38382;&#39064;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#38190;&#35282;&#25110;&#25197;&#36716;&#35282;&#24230;&#30340;&#20542;&#26012;&#19981;&#36275;&#36896;&#25104;&#30340;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AUTODIFF&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#20998;&#27573;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;conformal motif&#30340;&#26032;&#22411;&#20998;&#23376;&#32452;&#35013;&#31574;&#30053;&#65292;&#39318;&#20808;&#20445;&#30041;&#20998;&#23376;&#30340;&#23616;&#37096;&#32467;&#26500;&#30340;&#26500;&#35937;&#65292;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;SE(3)-&#31561;&#21464;&#21367;&#31215;&#32593;&#32476;&#32534;&#30721;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#36890;&#36807;&#25193;&#25955;&#24314;&#27169;&#36880;&#20010;motif&#29983;&#25104;&#20998;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#32422;&#26463;&#29983;&#25104;&#20998;&#23376;&#30340;&#20998;&#23376;&#36136;&#37327;&#25913;&#36827;&#20102;SBDD&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02003v1 Announce Type: new  Abstract: Structure-based drug design (SBDD), which aims to generate molecules that can bind tightly to the target protein, is an essential problem in drug discovery, and previous approaches have achieved initial success. However, most existing methods still suffer from invalid local structure or unrealistic conformation issues, which are mainly due to the poor leaning of bond angles or torsional angles. To alleviate these problems, we propose AUTODIFF, a diffusion-based fragment-wise autoregressive generation model. Specifically, we design a novel molecule assembly strategy named conformal motif that preserves the conformation of local structures of molecules first, then we encode the interaction of the protein-ligand complex with an SE(3)-equivariant convolutional network and generate molecules motif-by-motif with diffusion modeling. In addition, we also improve the evaluation framework of SBDD by constraining the molecular weights of the genera
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;Q&#23398;&#20064;&#65288;MMD-QL&#65289;&#26469;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#20215;&#20540;&#20989;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#20256;&#25773;&#65292;&#36890;&#36807;&#20351;&#29992;MMD&#37325;&#24515;&#65292;&#23454;&#29616;&#20102;&#27604;Wasserstein&#36317;&#31163;&#26356;&#32039;&#30340;&#27010;&#29575;&#24230;&#37327;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#24182;&#32467;&#21512;&#28145;&#24230;&#32593;&#32476;&#21019;&#36896;&#20102;MMD Q&#32593;&#32476;&#65288;MMD-QN&#65289;&#12290;</title><link>https://arxiv.org/abs/2404.00686</link><description>&lt;p&gt;
&#21033;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#37325;&#24515;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20256;&#25773;&#20215;&#20540;&#20989;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Utilizing Maximum Mean Discrepancy Barycenter for Propagating the Uncertainty of Value Functions in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00686
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;Q&#23398;&#20064;&#65288;MMD-QL&#65289;&#26469;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#20215;&#20540;&#20989;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#20256;&#25773;&#65292;&#36890;&#36807;&#20351;&#29992;MMD&#37325;&#24515;&#65292;&#23454;&#29616;&#20102;&#27604;Wasserstein&#36317;&#31163;&#26356;&#32039;&#30340;&#27010;&#29575;&#24230;&#37327;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#24182;&#32467;&#21512;&#28145;&#24230;&#32593;&#32476;&#21019;&#36896;&#20102;MMD Q&#32593;&#32476;&#65288;MMD-QN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#20215;&#20540;&#20989;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#20419;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;Q&#23398;&#20064;&#65288;MMD-QL&#65289;&#65292;&#20197;&#25913;&#36827;Wasserstein Q&#23398;&#20064;&#65288;WQL&#65289;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#24046;&#20998;&#65288;TD&#65289;&#26356;&#26032;&#26399;&#38388;&#20256;&#25773;&#19981;&#30830;&#23450;&#24615;&#12290;MMD-QL&#20351;&#29992;MMD&#37325;&#24515;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#22240;&#20026;MMD&#25552;&#20379;&#20102;&#27604;Wasserstein&#36317;&#31163;&#26356;&#32039;&#30340;&#27010;&#29575;&#24230;&#37327;&#20043;&#38388;&#30340;&#25509;&#36817;&#24230;&#20272;&#35745;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24179;&#22343;&#25439;&#22833;&#24230;&#37327;&#19979;&#65292;MMD-QL&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#26159;&#8220;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#8221;&#30340;&#12290;&#22312;&#32771;&#34385;&#21040;&#32047;&#31215;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#34920;&#26684;&#29615;&#22659;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MMD-QL&#20248;&#20110;WQL&#21644;&#20854;&#20182;&#31639;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#32593;&#32476;&#32435;&#20837;MMD-QL&#20013;&#65292;&#21019;&#24314;MMD Q&#32593;&#32476;&#65288;MMD-QN&#65289;&#12290;&#36890;&#36807;&#21512;&#29702;&#20551;&#35774;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;MMD-QN&#22312;&#20989;&#25968;&#36924;&#36817;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;Atari&#28216;&#25103;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;MMD-QN&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00686v1 Announce Type: new  Abstract: Accounting for the uncertainty of value functions boosts exploration in Reinforcement Learning (RL). Our work introduces Maximum Mean Discrepancy Q-Learning (MMD-QL) to improve Wasserstein Q-Learning (WQL) for uncertainty propagation during Temporal Difference (TD) updates. MMD-QL uses the MMD barycenter for this purpose, as MMD provides a tighter estimate of closeness between probability measures than the Wasserstein distance. Firstly, we establish that MMD-QL is Probably Approximately Correct in MDP (PAC-MDP) under the average loss metric. Concerning the accumulated rewards, experiments on tabular environments show that MMD-QL outperforms WQL and other algorithms. Secondly, we incorporate deep networks into MMD-QL to create MMD Q-Network (MMD-QN). Making reasonable assumptions, we analyze the convergence rates of MMD-QN using function approximation. Empirical results on challenging Atari games demonstrate that MMD-QN performs well comp
&lt;/p&gt;</description></item><item><title>InfLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#21517;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#26088;&#22312;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2404.00228</link><description>&lt;p&gt;
InfLoRA&#65306;&#26080;&#24178;&#25200;&#30340;&#20302;&#31209;&#33258;&#36866;&#24212;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00228
&lt;/p&gt;
&lt;p&gt;
InfLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#21517;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#26088;&#22312;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#24110;&#21161;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#35201;&#27714;&#27169;&#22411;&#20381;&#27425;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#12290;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#24212;&#20855;&#22791;&#22312;&#26087;&#20219;&#21153;&#19978;&#32500;&#25345;&#24615;&#33021;&#65288;&#31283;&#23450;&#24615;&#65289;&#21644;&#19981;&#26029;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#65288;&#21487;&#22609;&#24615;&#65289;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#29616;&#26377;&#22522;&#20110;PEFT&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#38750;PEFT&#26041;&#27861;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#24182;&#26410;&#32771;&#34385;&#22914;&#20309;&#28040;&#38500;&#26032;&#20219;&#21153;&#23545;&#26087;&#20219;&#21153;&#30340;&#24178;&#25200;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;&#26080;&#24178;&#25200;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;InfLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00228v1 Announce Type: cross  Abstract: Continual learning requires the model to learn multiple tasks sequentially. In continual learning, the model should possess the ability to maintain its performance on old tasks (stability) and the ability to adapt to new tasks continuously (plasticity). Recently, parameter-efficient fine-tuning (PEFT), which involves freezing a pre-trained model and injecting a small number of learnable parameters to adapt to downstream tasks, has gained increasing popularity in continual learning. Although existing continual learning methods based on PEFT have demonstrated superior performance compared to those not based on PEFT, most of them do not consider how to eliminate the interference of the new task on the old tasks, which inhibits the model from making a good trade-off between stability and plasticity. In this work, we propose a new PEFT method, called interference-free low-rank adaptation (InfLoRA), for continual learning. InfLoRA injects a 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#25163;&#26426;&#21644;&#20247;&#21253;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#20272;&#31639;&#22823;&#35268;&#27169;&#21306;&#22495;&#32593;&#32476;&#20013;&#30340;&#27493;&#34892;&#21644;&#39569;&#34892;&#37327;&#65292;&#35752;&#35770;&#20102;&#22312;&#27169;&#22411;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#25512;&#26029;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.00162</link><description>&lt;p&gt;
&#24314;&#27169;&#22823;&#35268;&#27169;&#27493;&#34892;&#21644;&#39569;&#34892;&#32593;&#32476;&#65306;&#20351;&#29992;&#25163;&#26426;&#21644;&#20247;&#21253;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modeling Large-Scale Walking and Cycling Networks: A Machine Learning Approach Using Mobile Phone and Crowdsourced Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#25163;&#26426;&#21644;&#20247;&#21253;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#20272;&#31639;&#22823;&#35268;&#27169;&#21306;&#22495;&#32593;&#32476;&#20013;&#30340;&#27493;&#34892;&#21644;&#39569;&#34892;&#37327;&#65292;&#35752;&#35770;&#20102;&#22312;&#27169;&#22411;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#25512;&#26029;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27493;&#34892;&#21644;&#39569;&#34892;&#34987;&#35748;&#20026;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#20581;&#24247;&#12289;&#29615;&#22659;&#21644;&#32463;&#27982;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#35777;&#25454;&#30340;&#31215;&#26497;&#20132;&#36890;&#35268;&#21010;&#21644;&#25919;&#31574;&#30340;&#21457;&#23637;&#21463;&#21040;&#25968;&#25454;&#38480;&#21046;&#30340;&#38459;&#30861;&#65292;&#20363;&#22914;&#20247;&#21253;&#25968;&#25454;&#30340;&#20559;&#35265;&#21644;&#25163;&#26426;&#25968;&#25454;&#30340;&#20195;&#34920;&#24615;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#24212;&#29992;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#31639;&#28595;&#22823;&#21033;&#20122;&#26032;&#21335;&#23041;&#23572;&#22763;&#24030;&#19968;&#20010;&#21253;&#21547;188,999&#20010;&#27493;&#34892;&#38142;&#25509;&#21644;114,885&#20010;&#39569;&#34892;&#38142;&#25509;&#30340;&#22823;&#35268;&#27169;&#22320;&#21306;&#32593;&#32476;&#30340;&#26085;&#24120;&#27493;&#34892;&#21644;&#39569;&#34892;&#37327;&#12290;&#24314;&#27169;&#26041;&#27861;&#21033;&#29992;&#20102;&#20247;&#21253;&#21644;&#25163;&#26426;&#25968;&#25454;&#20197;&#21450;&#20154;&#21475;&#12289;&#22303;&#22320;&#21033;&#29992;&#12289;&#22320;&#24418;&#12289;&#27668;&#20505;&#31561;&#19968;&#31995;&#21015;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;&#35813;&#30740;&#31350;&#35752;&#35770;&#20102;&#19982;&#27169;&#22411;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#25512;&#26029;&#30340;&#19977;&#20010;&#26041;&#38754;&#30456;&#20851;&#30340;&#29420;&#29305;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#32771;&#34385;&#21040;&#24314;&#27169;&#32593;&#32476;&#30340;&#22823;&#22320;&#29702;&#33539;&#22260;&#21644;&#30456;&#23545;&#31232;&#32570;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00162v1 Announce Type: new  Abstract: Walking and cycling are known to bring substantial health, environmental, and economic advantages. However, the development of evidence-based active transportation planning and policies has been impeded by significant data limitations, such as biases in crowdsourced data and representativeness issues of mobile phone data. In this study, we develop and apply a machine learning based modeling approach for estimating daily walking and cycling volumes across a large-scale regional network in New South Wales, Australia that includes 188,999 walking links and 114,885 cycling links. The modeling methodology leverages crowdsourced and mobile phone data as well as a range of other datasets on population, land use, topography, climate, etc. The study discusses the unique challenges and limitations related to all three aspects of model training, testing, and inference given the large geographical extent of the modeled networks and relative scarcity
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Systemic Quantum Score (SQS)&#30340;&#26032;&#26041;&#27861;&#65292;&#23637;&#31034;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#20135;&#32423;&#24212;&#29992;&#26696;&#20363;&#20013;&#30456;&#27604;&#32431;&#32463;&#20856;&#27169;&#22411;&#26356;&#26377;&#20248;&#21183;&#65292;&#33021;&#22815;&#20174;&#36739;&#23569;&#25968;&#25454;&#28857;&#20013;&#25552;&#21462;&#27169;&#24335;&#24182;&#34920;&#29616;&#20986;&#26356;&#22909;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00015</link><description>&lt;p&gt;
&#21033;&#29992;&#37327;&#23376;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#36171;&#33021;&#20449;&#29992;&#35780;&#20998;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00015
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Systemic Quantum Score (SQS)&#30340;&#26032;&#26041;&#27861;&#65292;&#23637;&#31034;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#20135;&#32423;&#24212;&#29992;&#26696;&#20363;&#20013;&#30456;&#27604;&#32431;&#32463;&#20856;&#27169;&#22411;&#26356;&#26377;&#20248;&#21183;&#65292;&#33021;&#22815;&#20174;&#36739;&#23569;&#25968;&#25454;&#28857;&#20013;&#25552;&#21462;&#27169;&#24335;&#24182;&#34920;&#29616;&#20986;&#26356;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Quantum Kernels&#34987;&#35748;&#20026;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26089;&#26399;&#38454;&#27573;&#25552;&#20379;&#20102;&#26377;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#21033;&#29992;&#24222;&#22823;&#25968;&#25454;&#38598;&#26102;&#65292;&#39640;&#24230;&#22797;&#26434;&#30340;&#32463;&#20856;&#27169;&#22411;&#24456;&#38590;&#36229;&#36234;&#65292;&#29305;&#21035;&#26159;&#22312;&#29702;&#35299;&#21147;&#26041;&#38754;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#19968;&#26086;&#25968;&#25454;&#31232;&#32570;&#19988;&#20542;&#26012;&#65292;&#32463;&#20856;&#27169;&#22411;&#23601;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#37327;&#23376;&#29305;&#24449;&#31354;&#38388;&#34987;&#39044;&#35745;&#22312;&#36825;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#26223;&#20013;&#33021;&#22815;&#25214;&#21040;&#26356;&#22909;&#30340;&#25968;&#25454;&#29305;&#24449;&#21644;&#30446;&#26631;&#31867;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Systemic Quantum Score (SQS)&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#32467;&#26524;&#65292;&#34920;&#26126;&#22312;&#37329;&#34701;&#34892;&#19994;&#29983;&#20135;&#32423;&#24212;&#29992;&#26696;&#20363;&#20013;&#65292;SQS&#21487;&#33021;&#27604;&#32431;&#32463;&#20856;&#27169;&#22411;&#20855;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#20855;&#20307;&#30740;&#31350;&#34920;&#26126;&#65292;SQS&#33021;&#22815;&#20174;&#36739;&#23569;&#30340;&#25968;&#25454;&#28857;&#20013;&#25552;&#21462;&#20986;&#27169;&#24335;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#38656;&#27714;&#37327;&#22823;&#30340;&#31639;&#27861;&#65288;&#22914;XGBoost&#65289;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24102;&#26469;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00015v1 Announce Type: cross  Abstract: Quantum Kernels are projected to provide early-stage usefulness for quantum machine learning. However, highly sophisticated classical models are hard to surpass without losing interpretability, particularly when vast datasets can be exploited. Nonetheless, classical models struggle once data is scarce and skewed. Quantum feature spaces are projected to find better links between data features and the target class to be predicted even in such challenging scenarios and most importantly, enhanced generalization capabilities. In this work, we propose a novel approach called Systemic Quantum Score (SQS) and provide preliminary results indicating potential advantage over purely classical models in a production grade use case for the Finance sector. SQS shows in our specific study an increased capacity to extract patterns out of fewer data points as well as improved performance over data-hungry algorithms such as XGBoost, providing advantage i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.20208</link><description>&lt;p&gt;
&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#39044;&#27979;&#34920;&#26684;&#20219;&#21153;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#65292;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#32570;&#22833;&#20540;&#22635;&#20805;&#31561;&#39044;&#27979;&#20219;&#21153;&#26159;&#19982;&#34920;&#26684;&#25968;&#25454;&#30456;&#20851;&#30340;&#24120;&#35265;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#35299;&#20915;&#36825;&#20123;&#39044;&#27979;&#20219;&#21153;&#12290;&#23613;&#31649;LLMs&#25797;&#38271;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#65292;&#20294;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25910;&#38598;&#24102;&#26377;&#25351;&#20196;&#27880;&#37322;&#30340;&#34920;&#26684;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#36825;&#19968;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;Llama-2&#36827;&#34892;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#20197;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#38646;-shot&#39044;&#27979;&#12289;&#23569;-shot&#39044;&#27979;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#22330;&#26223;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20208v1 Announce Type: new  Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Gegenbauer-based graph convolutional (GegenConv)&#31639;&#23376;&#65292;&#29992;&#20110;&#25552;&#39640;&#26102;&#21464;&#20449;&#21495;&#37325;&#26500;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.19800</link><description>&lt;p&gt;
Gegenbauer&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#21464;&#20449;&#21495;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19800
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Gegenbauer-based graph convolutional (GegenConv)&#31639;&#23376;&#65292;&#29992;&#20110;&#25552;&#39640;&#26102;&#21464;&#20449;&#21495;&#37325;&#26500;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26500;&#26102;&#21464;&#22270;&#20449;&#21495;&#65288;&#25110;&#22270;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20174;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#21040;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#20934;&#30830;&#25429;&#25417;&#36825;&#20123;&#20449;&#21495;&#22266;&#26377;&#30340;&#26102;&#31354;&#20449;&#24687;&#23545;&#20110;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#26102;&#38388;&#24046;&#30340;&#24179;&#28369;&#24615;&#20551;&#35774;&#21644;&#31616;&#21333;&#30340;&#20984;&#20248;&#21270;&#25216;&#26415;&#65292;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23398;&#20064;&#27169;&#22359;&#20197;&#22686;&#24378;&#19979;&#28216;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;Gegenbauer&#22810;&#39033;&#24335;&#29702;&#35770;&#30340;Gegenbauer-based graph convolutional&#65288;GegenConv&#65289;&#31639;&#23376;&#65292;&#36825;&#26159;&#20256;&#32479;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#30340;&#25512; generalization&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19800v1 Announce Type: cross  Abstract: Reconstructing time-varying graph signals (or graph time-series imputation) is a critical problem in machine learning and signal processing with broad applications, ranging from missing data imputation in sensor networks to time-series forecasting. Accurately capturing the spatio-temporal information inherent in these signals is crucial for effectively addressing these tasks. However, existing approaches relying on smoothness assumptions of temporal differences and simple convex optimization techniques have inherent limitations. To address these challenges, we propose a novel approach that incorporates a learning module to enhance the accuracy of the downstream task. To this end, we introduce the Gegenbauer-based graph convolutional (GegenConv) operator, which is a generalization of the conventional Chebyshev graph convolution by leveraging the theory of Gegenbauer polynomials. By deviating from traditional convex problems, we expand t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.18742</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#31867;&#21453;&#39304;&#23545;&#40784;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Learning Dynamics of Alignment with Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#24050;&#25104;&#20026;&#23433;&#20840;&#37096;&#32626;&#27169;&#22411;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#34429;&#28982;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29702;&#35770;&#19978;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#27491;&#24335;&#23637;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36824;&#25581;&#31034;&#20102;&#19968;&#20010;&#22797;&#26434;&#29616;&#35937;&#65292;&#21363;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#20855;&#26377;&#26356;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24403;&#20195;LLMs&#21644;&#23545;&#40784;&#20219;&#21153;&#19978;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24378;&#21270;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#23545;&#40784;&#26041;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#20813;&#36131;&#22768;&#26126;&#65306;&#26412;&#25991;&#21253;&#21547;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24352;&#37327;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#19968;&#33268;&#24615;&#21644;&#29305;&#24322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#30456;&#20284;&#24615;&#27979;&#37327;&#21644;&#22270;&#20449;&#24687;&#21033;&#29992;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18393</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#30340;&#19968;&#33268;&#24615;&#21644;&#29305;&#24322;&#24615;&#22810;&#35270;&#35282;&#32858;&#31867;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tensor-based Graph Learning with Consistency and Specificity for Multi-view Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24352;&#37327;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#19968;&#33268;&#24615;&#21644;&#29305;&#24322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#30456;&#20284;&#24615;&#27979;&#37327;&#21644;&#22270;&#20449;&#24687;&#21033;&#29992;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#22810;&#35270;&#35282;&#32858;&#31867;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#29616;&#26377;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#22522;&#20110;&#27010;&#29575;&#37051;&#23621;&#26500;&#24314;&#33258;&#36866;&#24212;&#37051;&#23621;&#22270;&#65292;&#28982;&#21518;&#23398;&#20064;&#19968;&#33268;&#24615;&#22270;&#36827;&#34892;&#32858;&#31867;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#20004;&#20010;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#27431;&#27663;&#36317;&#31163;&#26469;&#34913;&#37327;&#30456;&#20284;&#24615;&#65292;&#36825;&#22312;&#35768;&#22810;&#30495;&#23454;&#22330;&#26223;&#20013;&#25429;&#25417;&#25968;&#25454;&#28857;&#38388;&#30340;&#20869;&#22312;&#32467;&#26500;&#26102;&#35777;&#26126;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#20854;&#27425;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20165;&#20851;&#27880;&#19968;&#33268;&#24615;&#22270;&#65292;&#24573;&#30053;&#20102;&#29305;&#23450;&#35270;&#22270;&#30340;&#22270;&#20449;&#24687;&#12290;&#38024;&#23545;&#19978;&#36848;&#32570;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24352;&#37327;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#19968;&#33268;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#22312;&#26031;&#33922;&#24343;&#23572;&#27969;&#24418;&#19978;&#35745;&#31639;&#30456;&#20284;&#36317;&#31163;&#20197;&#20445;&#30041;&#20869;&#22312;str
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18393v1 Announce Type: new  Abstract: Graph learning is widely recognized as a crucial technique in multi-view clustering. Existing graph learning methods typically involve constructing an adaptive neighbor graph based on probabilistic neighbors and then learning a consensus graph to for clustering, however, they are confronted with two limitations. Firstly, they often rely on Euclidean distance to measure similarity when constructing the adaptive neighbor graph, which proves inadequate in capturing the intrinsic structure among data points in many real-world scenarios. Secondly, most of these methods focus solely on consensus graph, ignoring view-specific graph information. In response to the aforementioned drawbacks, we in this paper propose a novel tensor-based graph learning framework that simultaneously considers consistency and specificity for multi-view clustering. Specifically, we calculate the similarity distance on the Stiefel manifold to preserve the intrinsic str
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Hadamard&#31354;&#38388;&#20013;&#36827;&#34892;&#20984;&#20248;&#21270;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#20551;&#35774;&#19981;&#21516;&#65292;&#20854;&#22797;&#26434;&#24615;&#19981;&#20381;&#36182;&#20110;&#31354;&#38388;&#26354;&#29575;&#30340;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.15749</link><description>&lt;p&gt;
Horoballs&#21644;&#27425;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Horoballs and the subgradient method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Hadamard&#31354;&#38388;&#20013;&#36827;&#34892;&#20984;&#20248;&#21270;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#20551;&#35774;&#19981;&#21516;&#65292;&#20854;&#22797;&#26434;&#24615;&#19981;&#20381;&#36182;&#20110;&#31354;&#38388;&#26354;&#29575;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25506;&#32034;Hadamard&#31354;&#38388;&#19978;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#27425;&#26799;&#24230;&#31639;&#27861;&#30340;&#36845;&#20195;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#31867;&#26041;&#27861;&#20551;&#35774;&#24213;&#23618;&#31354;&#38388;&#26159;&#27969;&#24418;&#65292;&#24182;&#19988;&#30446;&#26631;&#26159;&#27979;&#22320;&#20984;&#30340;&#65306;&#36825;&#20123;&#26041;&#27861;&#26159;&#20351;&#29992;&#20999;&#31354;&#38388;&#21644;&#25351;&#25968;&#26144;&#23556;&#26469;&#25551;&#36848;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#36845;&#20195;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;Hadamard&#31354;&#38388;&#65292;&#26159;&#22312;&#24213;&#23618;&#31354;&#38388;&#26412;&#36523;&#20013;&#26500;&#24314;&#30340;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#23458;&#35266;&#27700;&#24179;&#38598;&#30340;horospherical&#20984;&#24615;&#12290;&#23545;&#20110;&#36825;&#20010;&#21463;&#38480;&#21046;&#30340;&#23458;&#35266;&#31867;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#36890;&#24120;&#24418;&#24335;&#30340;&#22797;&#26434;&#24615;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22797;&#26434;&#24615;&#19981;&#20381;&#36182;&#20110;&#31354;&#38388;&#26354;&#29575;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15749v1 Announce Type: cross  Abstract: To explore convex optimization on Hadamard spaces, we consider an iteration in the style of a subgradient algorithm. Traditionally, such methods assume that the underlying spaces are manifolds and that the objectives are geodesically convex: the methods are described using tangent spaces and exponential maps. By contrast, our iteration applies in a general Hadamard space, is framed in the underlying space itself, and relies instead on horospherical convexity of the objective level sets. For this restricted class of objectives, we prove a complexity result of the usual form. Notably, the complexity does not depend on a lower bound on the space curvature.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINN&#65289;&#65292;&#36890;&#36807;&#36981;&#24490;&#28784;&#33394;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23567;&#25968;&#25454;&#26679;&#26412;&#65292;&#20135;&#29983;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.15027</link><description>&lt;p&gt;
&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Grey-informed neural network for time-series forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINN&#65289;&#65292;&#36890;&#36807;&#36981;&#24490;&#28784;&#33394;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23567;&#25968;&#25454;&#26679;&#26412;&#65292;&#20135;&#29983;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22797;&#26434;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#34987;&#35270;&#20026;&#40657;&#30418;&#65292;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24320;&#21457;&#12290;&#22240;&#27492;&#65292;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#26500;&#24314;&#36866;&#24403;&#30340;&#27169;&#22411;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24314;&#35758;&#23454;&#26045;&#28784;&#33394;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINN&#65289;&#12290;GINN &#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#36981;&#24490;&#28784;&#33394;&#31995;&#32479;&#30340;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#28784;&#33394;&#31995;&#32479;&#29702;&#35770;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;&#20351;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23567;&#25968;&#25454;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#24050;&#34987;&#35266;&#23519;&#21040;&#33021;&#22815;&#25581;&#31034;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#24182;&#22522;&#20110;&#32463;&#39564;&#25968;&#25454;&#20135;&#29983;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15027v1 Announce Type: cross  Abstract: Neural network models have shown outstanding performance and successful resolutions to complex problems in various fields. However, the majority of these models are viewed as black-box, requiring a significant amount of data for development. Consequently, in situations with limited data, constructing appropriate models becomes challenging due to the lack of transparency and scarcity of data. To tackle these challenges, this study suggests the implementation of a grey-informed neural network (GINN). The GINN ensures that the output of the neural network follows the differential equation model of the grey system, improving interpretability. Moreover, incorporating prior knowledge from grey system theory enables traditional neural networks to effectively handle small data samples. Our proposed model has been observed to uncover underlying patterns in the real world and produce reliable forecasts based on empirical data.
&lt;/p&gt;</description></item><item><title>MolBind &#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20026;&#22810;&#31181;&#27169;&#24577;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#23558;&#25152;&#26377;&#27169;&#24577;&#26144;&#23556;&#21040;&#20849;&#20139;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#22810;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.08167</link><description>&lt;p&gt;
MolBind: &#22810;&#27169;&#24577;&#23545;&#40784;&#35821;&#35328;&#12289;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;
&lt;/p&gt;
&lt;p&gt;
MolBind: Multimodal Alignment of Language, Molecules, and Proteins
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08167
&lt;/p&gt;
&lt;p&gt;
MolBind &#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20026;&#22810;&#31181;&#27169;&#24577;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#23558;&#25152;&#26377;&#27169;&#24577;&#26144;&#23556;&#21040;&#20849;&#20139;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#22810;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#21644;&#21270;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#21033;&#29992;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#23558;&#20998;&#23376;&#21450;&#20854;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25972;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#20165;&#38480;&#20110;&#20004;&#31181;&#27169;&#24577;&#65292;&#35774;&#35745;&#19968;&#20010;&#32479;&#19968;&#30340;&#32593;&#32476;&#26469;&#22788;&#29702;&#19981;&#21516;&#27169;&#24577;&#65288;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#12289;2D&#20998;&#23376;&#22270;&#12289;3D&#20998;&#23376;&#26500;&#35937;&#21644;3D&#34507;&#30333;&#36136;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08167v1 Announce Type: cross  Abstract: Recent advancements in biology and chemistry have leveraged multi-modal learning, integrating molecules and their natural language descriptions to enhance drug discovery. However, current pre-training frameworks are limited to two modalities, and designing a unified network to process different modalities (e.g., natural language, 2D molecular graphs, 3D molecular conformations, and 3D proteins) remains challenging due to inherent gaps among them. In this work, we propose MolBind, a framework that trains encoders for multiple modalities through contrastive learning, mapping all modalities to a shared feature space for multi-modal semantic alignment. To facilitate effective pre-training of MolBind on multiple modalities, we also build and collect a high-quality dataset with four modalities, MolBind-M4, including graph-language, conformation-language, graph-conformation, and conformation-protein paired data. MolBind shows superior zero-sh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wiener-Kallianpur&#21019;&#26032;&#34920;&#31034;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#32534;&#30721;&#22120;&#21644;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#24615;&#21644;&#32467;&#26500;&#25910;&#25947;&#24615;&#36136;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#24066;&#22330;&#36816;&#33829;&#20013;&#30340;&#39640;&#21160;&#24577;&#21644;&#27874;&#21160;&#26102;&#38388;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.05743</link><description>&lt;p&gt;
&#20855;&#26377;&#24066;&#22330;&#36816;&#33829;&#24212;&#29992;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generative Probabilistic Forecasting with Applications in Market Operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05743
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wiener-Kallianpur&#21019;&#26032;&#34920;&#31034;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#32534;&#30721;&#22120;&#21644;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#24615;&#21644;&#32467;&#26500;&#25910;&#25947;&#24615;&#36136;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#24066;&#22330;&#36816;&#33829;&#20013;&#30340;&#39640;&#21160;&#24577;&#21644;&#27874;&#21160;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28304;&#33258;&#20110;&#38750;&#21442;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;Wiener-Kallianpur&#21019;&#26032;&#34920;&#31034;&#12290;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#33539;&#24335;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#39044;&#27979;&#26550;&#26500;&#21253;&#25324;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#38750;&#21442;&#25968;&#22810;&#21464;&#37327;&#38543;&#26426;&#36807;&#31243;&#36716;&#21270;&#20026;&#35268;&#33539;&#30340;&#21019;&#26032;&#24207;&#21015;&#65292;&#20174;&#20013;&#26681;&#25454;&#36807;&#21435;&#26679;&#26412;&#29983;&#25104;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#26465;&#20214;&#26159;&#23427;&#20204;&#30340;&#27010;&#29575;&#20998;&#24067;&#21462;&#20915;&#20110;&#36807;&#21435;&#26679;&#26412;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28508;&#22312;&#36807;&#31243;&#38480;&#21046;&#20026;&#20855;&#26377;&#21305;&#37197;&#33258;&#32534;&#30721;&#22120;&#36755;&#20837;-&#36755;&#20986;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#24207;&#21015;&#12290;&#24314;&#31435;&#20102;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#24335;&#39044;&#27979;&#26041;&#27861;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#21644;&#32467;&#26500;&#25910;&#25947;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#26102;&#24066;&#22330;&#36816;&#33829;&#20013;&#28041;&#21450;&#39640;&#24230;&#21160;&#24577;&#21644;&#27874;&#21160;&#26102;&#38388;&#24207;&#21015;&#30340;&#19977;&#20010;&#24212;&#29992;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05743v1 Announce Type: cross  Abstract: This paper presents a novel generative probabilistic forecasting approach derived from the Wiener-Kallianpur innovation representation of nonparametric time series. Under the paradigm of generative artificial intelligence, the proposed forecasting architecture includes an autoencoder that transforms nonparametric multivariate random processes into canonical innovation sequences, from which future time series samples are generated according to their probability distributions conditioned on past samples. A novel deep-learning algorithm is proposed that constrains the latent process to be an independent and identically distributed sequence with matching autoencoder input-output conditional probability distributions. Asymptotic optimality and structural convergence properties of the proposed generative forecasting approach are established. Three applications involving highly dynamic and volatile time series in real-time market operations a
&lt;/p&gt;</description></item><item><title>$\mathtt{tsGT}$&#26159;&#19968;&#31181;&#22522;&#20110;&#36890;&#29992;Transformer&#26550;&#26500;&#30340;&#38543;&#26426;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#36229;&#36807;&#20854;&#38543;&#26426;&#21516;&#34892;&#65292;&#29305;&#21035;&#22312;&#25968;&#25454;&#20998;&#24067;&#24314;&#27169;&#21644;&#36793;&#38469;&#20998;&#20301;&#20540;&#39044;&#27979;&#26041;&#38754;&#20855;&#22791;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.05713</link><description>&lt;p&gt;
$\mathtt{tsGT}$&#65306;&#20855;&#26377;Transformer&#30340;&#38543;&#26426;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
$\mathtt{tsGT}$: Stochastic Time Series Modeling With Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05713
&lt;/p&gt;
&lt;p&gt;
$\mathtt{tsGT}$&#26159;&#19968;&#31181;&#22522;&#20110;&#36890;&#29992;Transformer&#26550;&#26500;&#30340;&#38543;&#26426;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#36229;&#36807;&#20854;&#38543;&#26426;&#21516;&#34892;&#65292;&#29305;&#21035;&#22312;&#25968;&#25454;&#20998;&#24067;&#24314;&#27169;&#21644;&#36793;&#38469;&#20998;&#20301;&#20540;&#39044;&#27979;&#26041;&#38754;&#20855;&#22791;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#22312;&#20960;&#20046;&#25152;&#26377;&#22788;&#29702;&#26102;&#38388;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#31185;&#23398;&#39046;&#22495;&#20013;&#37117;&#20855;&#26377;&#22522;&#30784;&#37325;&#35201;&#24615;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#22823;&#25209;&#20855;&#26377;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#26550;&#26500;&#20559;&#35265;&#30340;&#30830;&#23450;&#24615;Transformer&#27169;&#22411;&#12290;&#26412;&#25991;&#37319;&#21462;&#20102;&#19981;&#21516;&#30340;&#26041;&#21521;&#65292;&#24341;&#20837;&#20102;$\mathtt{tsGT}$&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36890;&#29992;Transformer&#26550;&#26500;&#26500;&#24314;&#30340;&#38543;&#26426;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#19988;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#28378;&#21160;&#31383;&#21475;&#22238;&#27979;&#21644;&#35780;&#20272;&#21327;&#35758;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;$\mathtt{tsGT}$&#22312;&#22235;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#22312;MAD&#21644;RMSE&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#22312;QL&#21644;CRPS&#26041;&#38754;&#36229;&#36807;&#20102;&#20854;&#38543;&#26426;&#21516;&#34892;&#12290;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;$\mathtt{tsGT}$&#22312;&#24314;&#27169;&#25968;&#25454;&#20998;&#24067;&#21644;&#39044;&#27979;&#36793;&#38469;&#20998;&#20301;&#20540;&#26041;&#38754;&#30340;&#33021;&#21147;&#26469;&#34917;&#20805;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05713v1 Announce Type: new  Abstract: Time series methods are of fundamental importance in virtually any field of science that deals with temporally structured data. Recently, there has been a surge of deterministic transformer models with time series-specific architectural biases. In this paper, we go in a different direction by introducing $\mathtt{tsGT}$, a stochastic time series model built on a general-purpose transformer architecture. We focus on using a well-known and theoretically justified rolling window backtesting and evaluation protocol. We show that $\mathtt{tsGT}$ outperforms the state-of-the-art models on MAD and RMSE, and surpasses its stochastic peers on QL and CRPS, on four commonly used datasets. We complement these results with a detailed analysis of $\mathtt{tsGT}$'s ability to model the data distribution and predict marginal quantile values.
&lt;/p&gt;</description></item><item><title>MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.02694</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#24863;&#30693;&#35821;&#20041;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Privacy-Aware Semantic Cache for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02694
&lt;/p&gt;
&lt;p&gt;
MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#12289;Google Bard&#12289;Claude&#21644;Llama 2&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25628;&#32034;&#24341;&#25806;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36896;&#25104;&#20102;&#24322;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MeanCache&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#20197;&#30830;&#23450;&#32531;&#23384;&#21629;&#20013;&#25110;&#26410;&#21629;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02694v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache lever
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32852;&#37030;&#25552;&#31034;&#21512;&#20316; via Optimal Transport&#65288;FedOTP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#20840;&#23616;&#21644;&#26412;&#22320;&#25552;&#31034;&#30340;&#21512;&#20316;&#65292;&#38024;&#23545;&#25968;&#25454;&#24322;&#36136;&#24615;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#21327;&#20316;&#25552;&#31034;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.00041</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#20840;&#23616;&#21644;&#26412;&#22320;&#25552;&#31034;&#30340;&#21512;&#20316;&#65292;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Global and Local Prompts Cooperation via Optimal Transport for Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32852;&#37030;&#25552;&#31034;&#21512;&#20316; via Optimal Transport&#65288;FedOTP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#20840;&#23616;&#21644;&#26412;&#22320;&#25552;&#31034;&#30340;&#21512;&#20316;&#65292;&#38024;&#23545;&#25968;&#25454;&#24322;&#36136;&#24615;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#21327;&#20316;&#25552;&#31034;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;&#23398;&#20064;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#28789;&#27963;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#23558;&#36825;&#31181;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#25972;&#21512;&#21040;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#24182;&#20419;&#36827;&#23545;&#25968;&#25454;&#19981;&#36275;&#30340;&#23616;&#37096;&#35757;&#32451;&#12290;&#20026;&#20102;&#24212;&#23545;&#24403;&#21069;&#32852;&#37030;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#31995;&#32479;&#21270;&#35299;&#20915;&#20005;&#37325;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#21363;&#28041;&#21450;&#26631;&#31614;&#21644;&#29305;&#24449;&#36716;&#31227;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#32852;&#37030;&#25552;&#31034;&#21512;&#20316;&#65288;FedOTP&#65289;&#65292;&#23427;&#24341;&#20837;&#20102;&#39640;&#25928;&#30340;&#21327;&#20316;&#25552;&#31034;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#22522;&#30784;&#19978;&#25429;&#25417;&#19981;&#21516;&#30340;&#31867;&#21035;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#25552;&#31034;&#26469;&#25552;&#21462;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20849;&#35782;&#30693;&#35782;&#65292;&#36824;&#23398;&#20064;&#19968;&#20010;&#26412;&#22320;&#25552;&#31034;&#26469;&#25429;&#33719;&#29305;&#23450;&#23458;&#25143;&#31471;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00041v1 Announce Type: cross  Abstract: Prompt learning in pretrained visual-language models has shown remarkable flexibility across various downstream tasks. Leveraging its inherent lightweight nature, recent research attempted to integrate the powerful pretrained models into federated learning frameworks to simultaneously reduce communication costs and promote local training on insufficient data. Despite these efforts, current federated prompt learning methods lack specialized designs to systematically address severe data heterogeneities, e.g., data distribution with both label and feature shifts involved. To address this challenge, we present Federated Prompts Cooperation via Optimal Transport (FedOTP), which introduces efficient collaborative prompt learning strategies to capture diverse category traits on a per-client basis. Specifically, for each client, we learn a global prompt to extract consensus knowledge among clients, and a local prompt to capture client-specific
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#24212;&#29992;&#20110;&#21387;&#32553;&#26426;&#35774;&#22791;&#36816;&#34892;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#25925;&#38556;&#26816;&#27979;&#12289;&#25925;&#38556;&#39044;&#27979;&#12289;&#39044;&#27979;&#21644;&#21464;&#28857;&#26816;&#27979;&#31561;&#20219;&#21153;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.17802</link><description>&lt;p&gt;
&#21387;&#32553;&#26426;&#35774;&#22791;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Time Series Analysis in Compressor-Based Machines: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#24212;&#29992;&#20110;&#21387;&#32553;&#26426;&#35774;&#22791;&#36816;&#34892;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#25925;&#38556;&#26816;&#27979;&#12289;&#25925;&#38556;&#39044;&#27979;&#12289;&#39044;&#27979;&#21644;&#21464;&#28857;&#26816;&#27979;&#31561;&#20219;&#21153;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#21644;&#23621;&#20303;&#29615;&#22659;&#20013;&#65292;&#22914;&#20912;&#31665;&#12289;&#26262;&#36890;&#31354;&#35843;&#31995;&#32479;&#12289;&#28909;&#27893;&#21644;&#21046;&#20919;&#26426;&#31561;&#22522;&#20110;&#21387;&#32553;&#26426;&#30340;&#35774;&#22791;&#23545;&#28385;&#36275;&#29983;&#20135;&#21644;&#28040;&#36153;&#38656;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#24863;&#22120;&#21644;&#29289;&#32852;&#32593;&#36830;&#25509;&#30340;&#26222;&#21450;&#25903;&#25345;&#20102;&#30417;&#27979;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#33021;&#22815;&#26816;&#27979;&#21644;&#39044;&#27979;&#25925;&#38556;&#65292;&#35782;&#21035;&#34892;&#20026;&#21464;&#21270;&#65292;&#24182;&#39044;&#27979;&#35774;&#22791;&#21644;&#20854;&#32452;&#20214;&#30340;&#25805;&#20316;&#29366;&#24577;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#35843;&#26597;&#26368;&#36817;&#23545;&#36825;&#20123;&#20219;&#21153;&#65288;&#25925;&#38556;&#26816;&#27979;&#12289;&#25925;&#38556;&#39044;&#27979;&#12289;&#39044;&#27979;&#21644;&#21464;&#28857;&#26816;&#27979;&#65289;&#24212;&#29992;&#20110;&#34920;&#24449;&#21387;&#32553;&#26426;&#35774;&#22791;&#36816;&#34892;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25925;&#38556;&#26816;&#27979;&#21487;&#20197;&#26816;&#27979;&#21644;&#35786;&#26029;&#25925;&#38556;&#65292;&#25925;&#38556;&#39044;&#27979;&#21487;&#20197;&#39044;&#27979;&#36825;&#31181;&#21457;&#29983;&#65292;&#39044;&#27979;&#21487;&#20197;&#39044;&#27979;&#35774;&#22791;&#29305;&#24449;&#21464;&#37327;&#30340;&#26410;&#26469;&#20540;&#65292;&#21464;&#28857;&#26816;&#27979;&#21487;&#20197;&#35782;&#21035;&#35774;&#22791;&#34892;&#20026;&#20013;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17802v1 Announce Type: new  Abstract: In both industrial and residential contexts, compressor-based machines, such as refrigerators, HVAC systems, heat pumps and chillers, are essential to fulfil production and consumers' needs. The diffusion of sensors and IoT connectivity supports the development of monitoring systems able to detect and predict faults, identify behavioural shifts and forecast the operational status of machines and of their components. The focus of this paper is to survey the recent research on such tasks as Fault Detection, Fault Prediction, Forecasting and Change Point Detection applied to multivariate time series characterizing the operations of compressor-based machines. Specifically, Fault Detection detects and diagnoses faults, Fault Prediction predicts such occurrences, forecasting anticipates the future value of characteristic variables of machines and Change Point Detection identifies significant variations in the behaviour of the appliances, such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;OSCaR&#65292;&#26088;&#22312;&#35299;&#20915;&#25551;&#36848;&#22797;&#26434;&#35270;&#35273;&#29615;&#22659;&#20013;&#23545;&#35937;&#29366;&#24577;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2402.17128</link><description>&lt;p&gt;
OSCaR:&#23545;&#35937;&#29366;&#24577;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
OSCaR: Object State Captioning and State Change Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;OSCaR&#65292;&#26088;&#22312;&#35299;&#20915;&#25551;&#36848;&#22797;&#26434;&#35270;&#35273;&#29615;&#22659;&#20013;&#23545;&#35937;&#29366;&#24577;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17128v3 &#20844;&#21578;&#31867;&#22411;: &#36328; &#38754;&#21521;&#20154;&#31867;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#20132;&#20114;&#35270;&#35282;&#65292;&#26234;&#33021;&#27169;&#22411;&#25512;&#26029;&#21644;&#29702;&#35299;&#23545;&#35937;&#29366;&#24577;&#30340;&#21464;&#21270;&#33021;&#21147;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#25551;&#36848;&#22797;&#26434;&#30340;&#35270;&#35273;&#29615;&#22659;&#65292;&#35782;&#21035;&#27963;&#36291;&#23545;&#35937;&#65292;&#20197;&#21450;&#36890;&#36807;&#35821;&#35328;&#35299;&#37322;&#23427;&#20204;&#30340;&#21464;&#21270;&#12290;&#20256;&#32479;&#26041;&#27861;&#23558;&#23545;&#35937;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#26816;&#27979;&#36827;&#34892;&#38548;&#31163;&#65292;&#25552;&#20379;&#20102;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#26377;&#38480;&#35270;&#22270;&#12290;&#27492;&#22806;&#65292;&#20381;&#36182;&#20110;&#19968;&#23567;&#22871;&#31526;&#21495;&#21270;&#35789;&#27719;&#26469;&#34920;&#31034;&#21464;&#21270;&#38480;&#21046;&#20102;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23545;&#35937;&#29366;&#24577;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#34920;&#31034;&#65288;OSCaR&#65289;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;OSCaR&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#20027;&#35266;&#35270;&#35282;&#35270;&#39057;&#38598;&#21512;&#30340;14,084&#20010;&#24102;&#27880;&#37322;&#35270;&#39057;&#29255;&#27573;&#65292;&#28085;&#30422;&#36817;1,000&#20010;&#29420;&#29305;&#23545;&#35937;&#12290;&#23427;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17128v3 Announce Type: cross  Abstract: The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of the language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating multimodal large langua
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#31639;&#27861;&#21644;&#26368;&#22823;&#30456;&#20851;&#24615;&#20934;&#21017;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22788;&#29702;&#21547;&#24322;&#24120;&#20540;3D&#21644;4D&#25968;&#25454;&#30340;&#40065;&#26834;&#22235;&#20803;&#25968;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#25152;&#20351;&#29992;&#30340;&#26368;&#22823;&#30456;&#20851;&#24615;&#25439;&#22833;&#20989;&#25968;&#23545;&#24322;&#24120;&#20540;&#19981;&#22826;&#25935;&#24863;&#65292;&#36866;&#29992;&#20110;&#22810;&#32500;&#22024;&#26434;&#25110;&#19981;&#30830;&#23450;&#25968;&#25454;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14227</link><description>&lt;p&gt;
&#20855;&#26377;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#21644;&#26368;&#22823;&#30456;&#20851;&#24615;&#20934;&#21017;&#30340;&#22235;&#20803;&#25968;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Quaternion recurrent neural network with real-time recurrent learning and maximum correntropy criterion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14227
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#31639;&#27861;&#21644;&#26368;&#22823;&#30456;&#20851;&#24615;&#20934;&#21017;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22788;&#29702;&#21547;&#24322;&#24120;&#20540;3D&#21644;4D&#25968;&#25454;&#30340;&#40065;&#26834;&#22235;&#20803;&#25968;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#25152;&#20351;&#29992;&#30340;&#26368;&#22823;&#30456;&#20851;&#24615;&#25439;&#22833;&#20989;&#25968;&#23545;&#24322;&#24120;&#20540;&#19981;&#22826;&#25935;&#24863;&#65292;&#36866;&#29992;&#20110;&#22810;&#32500;&#22024;&#26434;&#25110;&#19981;&#30830;&#23450;&#25968;&#25454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#22235;&#20803;&#25968;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;QRNN&#65289;&#65292;&#29992;&#20110;&#23454;&#26102;&#22788;&#29702;&#20855;&#26377;&#24322;&#24120;&#20540;&#30340;3D&#21644;4D&#25968;&#25454;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#31639;&#27861;&#21644;&#26368;&#22823;&#30456;&#20851;&#24615;&#20934;&#21017;&#65288;MCC&#65289;&#32467;&#21512;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#30340;&#12290;&#23613;&#31649;&#22343;&#26041;&#35823;&#24046;&#21644;&#26368;&#22823;&#30456;&#20851;&#24615;&#20934;&#21017;&#37117;&#26159;&#21487;&#34892;&#30340;&#20195;&#20215;&#20989;&#25968;&#65292;&#20294;&#32467;&#26524;&#34920;&#26126;&#38750;&#20108;&#27425;&#26368;&#22823;&#30456;&#20851;&#24615;&#25439;&#22833;&#20989;&#25968;&#23545;&#24322;&#24120;&#20540;&#19981;&#22826;&#25935;&#24863;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22810;&#32500;&#22024;&#26434;&#25110;&#19981;&#30830;&#23450;&#25968;&#25454;&#30340;&#24212;&#29992;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#22522;&#20110;&#26032;&#39062;&#30340;&#24191;&#20041;HR&#65288;GHR&#65289;&#24494;&#31215;&#20998;&#23548;&#20986;&#65292;&#23427;&#20801;&#35768;&#23545;&#22235;&#20803;&#25968;&#21464;&#37327;&#30340;&#23454;&#20989;&#25968;&#36827;&#34892;&#24494;&#20998;&#65292;&#24182;&#25552;&#20379;&#20056;&#27861;&#21644;&#38142;&#24335;&#27861;&#21017;&#65292;&#20174;&#32780;&#23454;&#29616;&#20248;&#38597;&#19988;&#31616;&#27905;&#30340;&#23548;&#20986;&#12290;&#22312;&#33016;&#37096;&#20869;&#37096;&#26631;&#35760;&#29289;&#30340;&#36816;&#21160;&#39044;&#27979;&#32972;&#26223;&#19979;&#36827;&#34892;&#30340;&#20223;&#30495;&#32467;&#26524;&#28085;&#30422;&#20102;&#32954;&#30284;&#25918;&#30103;&#20013;&#30340;&#24120;&#35268;&#21644;&#19981;&#35268;&#21017;&#21628;&#21560;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14227v1 Announce Type: new  Abstract: We develop a robust quaternion recurrent neural network (QRNN) for real-time processing of 3D and 4D data with outliers. This is achieved by combining the real-time recurrent learning (RTRL) algorithm and the maximum correntropy criterion (MCC) as a loss function. While both the mean square error and maximum correntropy criterion are viable cost functions, it is shown that the non-quadratic maximum correntropy loss function is less sensitive to outliers, making it suitable for applications with multidimensional noisy or uncertain data. Both algorithms are derived based on the novel generalised HR (GHR) calculus, which allows for the differentiation of real functions of quaternion variables and offers the product and chain rules, thus enabling elegant and compact derivations. Simulation results in the context of motion prediction of chest internal markers for lung cancer radiotherapy, which includes regular and irregular breathing sequenc
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12343</link><description>&lt;p&gt;
&#27169;&#25311;&#22833;&#35843;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#21487;&#33021;&#20250;&#36866;&#24471;&#20854;&#21453;&#65281;
&lt;/p&gt;
&lt;p&gt;
Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12343
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#19982;&#20154;&#31867;&#36827;&#34892;&#23433;&#20840;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#25915;&#20987;&#26694;&#26550;&#65292;&#34920;&#26126;&#23433;&#20840;&#23545;&#40784;&#20063;&#21487;&#33021;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#26080;&#24847;&#20013;&#20419;&#25104;&#26377;&#23475;&#32467;&#26524;&#12290;&#36825;&#20010;&#26694;&#26550;&#34987;&#21629;&#21517;&#20026;&#27169;&#25311;&#22833;&#35843;&#65288;ED&#65289;&#65292;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#19981;&#33391;&#22320;&#32452;&#21512;&#20102;&#19968;&#23545;&#24320;&#28304;&#39044;&#35757;&#32451;&#21644;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#26377;&#23475;&#30340;&#35821;&#35328;&#27169;&#22411;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;ED&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;Llama-1&#12289;Llama-2&#12289;Mistral&#21644;Alpaca&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ED&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#23475;&#24615;&#22686;&#21152;&#20102;&#19968;&#20493;&#65292;&#24182;&#32988;&#36807;&#24378;&#22522;&#32447;&#65292;&#20197;&#36739;&#22823;&#20248;&#21183;&#22312;48&#20010;&#35780;&#20272;&#23376;&#38598;&#20013;&#30340;43&#20010;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#26377;&#23475;&#29575;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#65292;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20219;&#21153;&#26465;&#20214;&#30340;&#33258;&#36866;&#24212;&#22120;&#65292;&#22312;&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#19988;&#26080;&#38656;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#26435;&#37325;&#12290;</title><link>https://arxiv.org/abs/2402.07739</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#20013;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#30340;&#35270;&#35273;&#29305;&#24449;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Task-conditioned adaptation of visual features in multi-task policy learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20219;&#21153;&#26465;&#20214;&#30340;&#33258;&#36866;&#24212;&#22120;&#65292;&#22312;&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#19988;&#26080;&#38656;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#22320;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26159;&#33258;&#20027;&#20195;&#29702;&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#36825;&#38656;&#35201;&#28789;&#27963;&#22320;&#35843;&#25972;&#24213;&#23618;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#24182;&#19988;&#22914;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25152;&#25552;&#20986;&#30340;&#65292;&#36824;&#38656;&#35201;&#35843;&#25972;&#24213;&#23618;&#30340;&#24863;&#30693;&#27169;&#22359;&#12290;&#19968;&#20010;&#31867;&#27604;&#30340;&#35770;&#35777;&#26159;&#20154;&#31867;&#30340;&#35270;&#35273;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#33258;&#19978;&#32780;&#19979;&#30340;&#20449;&#21495;&#26469;&#19987;&#27880;&#20110;&#24403;&#21069;&#20219;&#21153;&#12290;&#31867;&#20284;&#22320;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#26469;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#35270;&#35273;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#30340;&#36866;&#37197;&#22120;&#65292;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#20219;&#20309;&#39044;&#20808;&#35757;&#32451;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#35757;&#32451;&#30340;&#21333;&#19968;&#31574;&#30053;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#31574;&#30053;&#21644;&#35270;&#35273;&#36866;&#37197;&#22120;&#19978;&#26681;&#25454;&#20219;&#21153;&#23884;&#20837;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#22914;&#26524;&#20219;&#21153;&#26159;&#24050;&#30693;&#30340;&#65292;&#21017;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36873;&#25321;&#20219;&#21153;&#23884;&#20837;&#65292;&#21542;&#21017;&#21487;&#20197;&#20174;&#19968;&#32452;&#31034;&#20363;&#28436;&#31034;&#20013;&#36827;&#34892;&#25512;&#26029;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#22312;...&#65288;&#25688;&#35201;&#26410;&#23436;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#26469;&#36827;&#34892;&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#30340;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#25237;&#24433;&#30697;&#38453;&#21644;&#29305;&#24449;&#36716;&#21270;&#65292;&#25552;&#39640;&#20102;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06530</link><description>&lt;p&gt;
&#25913;&#36827;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#22312;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#26469;&#36827;&#34892;&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#30340;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#25237;&#24433;&#30697;&#38453;&#21644;&#29305;&#24449;&#36716;&#21270;&#65292;&#25552;&#39640;&#20102;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#65288;MI&#65289;&#30340;&#26816;&#27979;&#23545;&#20110;&#39044;&#38450;&#36827;&#19968;&#27493;&#24515;&#32908;&#25439;&#20260;&#38750;&#24120;&#37325;&#35201;&#65292;MI&#26159;&#30001;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#24341;&#36215;&#30340;&#19968;&#31181;&#20005;&#37325;&#30142;&#30149;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#65288;OCC&#65289;&#31639;&#27861;&#36827;&#34892;&#26089;&#26399;MI&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#22810;&#27169;&#24577;&#23376;&#31354;&#38388;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;&#30340;&#26032;&#26041;&#27861;&#20811;&#26381;&#20102;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#25552;&#20986;&#30340;&#25216;&#26415;&#28041;&#21450;&#19968;&#31181;&#29305;&#27530;&#30340;MI&#26816;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#22797;&#21512;&#26680;&#22312;&#38750;&#32447;&#24615;&#25237;&#24433;&#25216;&#24039;&#20013;&#34701;&#21512;&#39640;&#26031;&#21644;&#25289;&#26222;&#25289;&#26031;sigmoid&#20989;&#25968;&#65292;&#23558;&#22810;&#35270;&#22270;&#36229;&#22768;&#24515;&#21160;&#22270;&#32467;&#21512;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#35843;&#25972;&#25237;&#24433;&#30697;&#38453;&#30340;&#26368;&#22823;&#21270;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#25237;&#24433;&#30697;&#38453;&#26356;&#26032;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#20248;&#21270;&#30340;&#20302;&#32500;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;MI&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early detection of myocardial infarction (MI), a critical condition arising from coronary artery disease (CAD), is vital to prevent further myocardial damage. This study introduces a novel method for early MI detection using a one-class classification (OCC) algorithm in echocardiography. Our study overcomes the challenge of limited echocardiography data availability by adopting a novel approach based on Multi-modal Subspace Support Vector Data Description. The proposed technique involves a specialized MI detection framework employing multi-view echocardiography incorporating a composite kernel in the non-linear projection trick, fusing Gaussian and Laplacian sigmoid functions. Additionally, we enhance the update strategy of the projection matrices by adapting maximization for both or one of the modalities in the optimization process. Our method boosts MI detection capability by efficiently transforming features extracted from echocardiography data into an optimized lower-dimensional su
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#23545;&#35282;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#20998;&#26512;&#21644;&#25968;&#20540;&#30740;&#31350;&#65292;&#21457;&#29616;&#26041;&#24046;&#37327;&#21462;&#20915;&#20110;&#38750;&#32447;&#24615;&#19982;&#19981;&#21516;&#21442;&#25968;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24212;&#35813;&#22312;&#20272;&#35745;&#36153;&#33293;&#23572;&#20449;&#24687;&#26102;&#20104;&#20197;&#37325;&#35270;&#12290;</title><link>https://arxiv.org/abs/2402.05379</link><description>&lt;p&gt;
&#23545;&#35282;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Tradeoffs of Diagonal Fisher Information Matrix Estimators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#23545;&#35282;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#20998;&#26512;&#21644;&#25968;&#20540;&#30740;&#31350;&#65292;&#21457;&#29616;&#26041;&#24046;&#37327;&#21462;&#20915;&#20110;&#38750;&#32447;&#24615;&#19982;&#19981;&#21516;&#21442;&#25968;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24212;&#35813;&#22312;&#20272;&#35745;&#36153;&#33293;&#23572;&#20449;&#24687;&#26102;&#20104;&#20197;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#25551;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#23616;&#37096;&#20960;&#20309;&#24615;&#36136;&#65292;&#23427;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#24037;&#20855;&#26469;&#29702;&#35299;&#21644;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#12290;&#37492;&#20110;&#20854;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#23454;&#36341;&#32773;&#36890;&#24120;&#20351;&#29992;&#38543;&#26426;&#20272;&#35745;&#22120;&#65292;&#24182;&#20165;&#35780;&#20272;&#23545;&#35282;&#32447;&#26465;&#30446;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#36825;&#26679;&#30340;&#20272;&#35745;&#22120;&#65292;&#20854;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#21462;&#20915;&#20110;&#23427;&#20204;&#20851;&#32852;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#26041;&#24046;&#30340;&#30028;&#38480;&#65292;&#24182;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#32593;&#32476;&#20013;&#23454;&#20363;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#25968;&#20540;&#30740;&#31350;&#26469;&#26435;&#34913;&#36825;&#20004;&#20010;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#26041;&#24046;&#37327;&#21462;&#20915;&#20110;&#20851;&#20110;&#19981;&#21516;&#21442;&#25968;&#32452;&#30340;&#38750;&#32447;&#24615;&#65292;&#24403;&#20272;&#35745;&#36153;&#33293;&#23572;&#20449;&#24687;&#26102;&#19981;&#33021;&#24573;&#35270;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Fisher information matrix characterizes the local geometry in the parameter space of neural networks. It elucidates insightful theories and useful tools to understand and optimize neural networks. Given its high computational cost, practitioners often use random estimators and evaluate only the diagonal entries. We examine two such estimators, whose accuracy and sample complexity depend on their associated variances. We derive bounds of the variances and instantiate them in regression and classification networks. We navigate trade-offs of both estimators based on analytical and numerical studies. We find that the variance quantities depend on the non-linearity with respect to different parameter groups and should not be neglected when estimating the Fisher information.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#22312;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#30340;&#21516;&#26102;&#23454;&#29616;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#24341;&#23548;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;LSTS&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.03678</link><description>&lt;p&gt;
&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#22312;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#30340;&#21516;&#26102;&#23454;&#29616;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#24341;&#23548;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;LSTS&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#20351;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#23398;&#20064;&#22810;&#26679;&#21270;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#29615;&#22659;&#20132;&#20114;&#12290;&#20026;&#20102;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#65292;&#22914;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL$_f$&#65289;&#20844;&#24335;&#25110;&#22870;&#21169;&#26426;&#22120;&#65288;RM&#65289;&#65292;&#26469;&#25351;&#23548;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;LSTS&#19981;&#20551;&#35774;&#29615;&#22659;&#21160;&#21147;&#23398;&#25110;&#22870;&#21169;&#26426;&#22120;&#30340;&#20449;&#24687;&#65292;&#24182;&#21160;&#24577;&#37319;&#26679;&#23548;&#33268;&#25104;&#21151;&#30446;&#26631;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#19978;&#35780;&#20272;&#20102;LSTS&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has made significant strides in enabling artificial agents to learn diverse behaviors. However, learning an effective policy often requires a large number of environment interactions. To mitigate sample complexity issues, recent approaches have used high-level task specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward Machines (RM), to guide the learning progress of the agent. In this work, we propose a novel approach, called Logical Specifications-guided Dynamic Task Sampling (LSTS), that learns a set of RL policies to guide an agent from an initial state to a goal state based on a high-level task specification, while minimizing the number of environmental interactions. Unlike previous work, LSTS does not assume information about the environment dynamics or the Reward Machine, and dynamically samples promising tasks that lead to successful goal policies. We evaluate LSTS on a gridworld and show that it achieves improved time-to-threshol
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21508;&#31181;LLMs&#22312;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#24120;&#35265;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25351;&#23548;&#23398;&#29983;&#36873;&#25321;&#36866;&#21512;&#20182;&#20204;&#30340;LLM.</title><link>https://arxiv.org/abs/2402.01687</link><description>&lt;p&gt;
&#8220;&#25105;&#24212;&#35813;&#20351;&#29992;&#21738;&#20010;LLM&#65311;&#8221;&#65306;&#35780;&#20272;&#29992;&#20110;&#21360;&#24230;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#20219;&#21153;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
"Which LLM should I use?": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students in India
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21508;&#31181;LLMs&#22312;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#24120;&#35265;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25351;&#23548;&#23398;&#29983;&#36873;&#25321;&#36866;&#21512;&#20182;&#20204;&#30340;LLM.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#24120;&#35265;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;&#23613;&#31649;&#35745;&#31639;&#26426;&#25945;&#32946;&#30028;&#30340;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#32570;&#20047;&#23545;&#19981;&#21516;LLMs&#36827;&#34892;&#32508;&#21512;&#27604;&#36739;&#21644;&#35780;&#20272;&#21738;&#20010;LLMs&#23545;&#19981;&#21516;&#20219;&#21153;&#26368;&#26377;&#25928;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#19968;&#20123;&#20844;&#24320;&#21487;&#29992;&#30340;LLMs&#65292;&#20363;&#22914;Google Bard&#65292;ChatGPT&#65292;GitHub Copilot Chat&#21644;Microsoft Copilot&#22312;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#24120;&#36935;&#21040;&#30340;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#65292;&#35299;&#37322;&#65292;&#39033;&#30446;&#26500;&#24605;&#65292;&#20869;&#23481;&#29983;&#25104;&#65292;&#35838;&#31243;&#20316;&#19994;&#21644;&#30005;&#23376;&#37038;&#20214;&#25776;&#20889;&#12290;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#39640;&#24180;&#32423;&#21644;&#20302;&#24180;&#32423;&#23398;&#29983;&#36827;&#34892;&#20102;&#36825;&#20123;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#30340;&#35265;&#35299;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#25351;&#23548;&#23398;&#29983;&#36873;&#25321;&#36866;&#21512;&#20182;&#20204;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
This study evaluates the effectiveness of various large language models (LLMs) in performing tasks common among undergraduate computer science students. Although a number of research studies in the computing education community have explored the possibility of using LLMs for a variety of tasks, there is a lack of comprehensive research comparing different LLMs and evaluating which LLMs are most effective for different tasks. Our research systematically assesses some of the publicly available LLMs such as Google Bard, ChatGPT, GitHub Copilot Chat, and Microsoft Copilot across diverse tasks commonly encountered by undergraduate computer science students. These tasks include code generation, explanation, project ideation, content generation, class assignments, and email composition. Evaluation for these tasks was carried out by junior and senior students in computer science, and provides insights into the models' strengths and limitations. This study aims to guide students in selecting su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#30721;&#24863;&#30693;&#25552;&#31034;&#31574;&#30053;&#65288;SymPrompt&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#27979;&#35797;&#29983;&#25104;&#65292;&#36890;&#36807;&#23558;&#27979;&#35797;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#22810;&#38454;&#27573;&#24207;&#21015;&#65292;&#24182;&#20197;&#39537;&#21160;&#31574;&#30053;&#25512;&#21160;&#27599;&#20010;&#38454;&#27573;&#65292;&#25913;&#21892;&#20102;&#27979;&#35797;&#29983;&#25104;&#30340;&#35206;&#30422;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.00097</link><description>&lt;p&gt;
&#20195;&#30721;&#24863;&#30693;&#25552;&#31034;&#65306;&#22522;&#20110;LLM&#30340;&#22238;&#24402;&#35774;&#32622;&#19979;&#35206;&#30422;&#29575;&#23548;&#21521;&#30340;&#27979;&#35797;&#29983;&#25104;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20195;&#30721;&#24863;&#30693;&#25552;&#31034;&#31574;&#30053;&#65288;SymPrompt&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#27979;&#35797;&#29983;&#25104;&#65292;&#36890;&#36807;&#23558;&#27979;&#35797;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#22810;&#38454;&#27573;&#24207;&#21015;&#65292;&#24182;&#20197;&#39537;&#21160;&#31574;&#30053;&#25512;&#21160;&#27599;&#20010;&#38454;&#27573;&#65292;&#25913;&#21892;&#20102;&#27979;&#35797;&#29983;&#25104;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#22312;&#30830;&#20445;&#36719;&#20214;&#36136;&#37327;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#28982;&#32780;&#20256;&#32479;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#36719;&#20214;&#27979;&#35797;&#26041;&#27861;&#32463;&#24120;&#22312;&#22797;&#26434;&#30340;&#36719;&#20214;&#21333;&#20803;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#36798;&#19981;&#21040;&#26368;&#20339;&#30340;&#27979;&#35797;&#35206;&#30422;&#29575;&#12290;&#26368;&#36817;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#27979;&#35797;&#29983;&#25104;&#30340;&#30740;&#31350;&#19968;&#30452;&#33268;&#21147;&#20110;&#36890;&#36807;&#20248;&#21270;&#27979;&#35797;&#29983;&#25104;&#19978;&#19979;&#25991;&#21644;&#32416;&#27491;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#38169;&#35823;&#26469;&#25913;&#36827;&#29983;&#25104;&#36136;&#37327;&#65292;&#20294;&#20351;&#29992;&#20102;&#22266;&#23450;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#21363;&#25552;&#31034;&#27169;&#22411;&#22312;&#27809;&#26377;&#39069;&#22806;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#27979;&#35797;&#12290;&#22240;&#27492;&#65292;LLM&#29983;&#25104;&#30340;&#27979;&#35797;&#22871;&#20214;&#20173;&#28982;&#23384;&#22312;&#20302;&#35206;&#30422;&#29575;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SymPrompt&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#30340;&#20195;&#30721;&#24863;&#30693;&#25552;&#31034;&#31574;&#30053;&#26469;&#36827;&#34892;&#27979;&#35797;&#29983;&#25104;&#12290;SymPrompt&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;LLMs&#22312;&#20197;&#22810;&#27493;&#26041;&#24335;&#24605;&#32771;&#38382;&#39064;&#26102;&#21487;&#20197;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#36923;&#36753;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#27979;&#35797;&#29983;&#25104;&#65292;&#23558;&#27979;&#35797;&#22871;&#20214;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#22810;&#38454;&#27573;&#24207;&#21015;&#65292;&#27599;&#20010;&#38454;&#27573;&#37117;&#30001;&#19968;&#31181;&#39537;&#21160;&#31574;&#30053;&#26469;&#25512;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated test suites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt's approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#33322;&#29677;&#28369;&#34892;&#23433;&#20840;&#30340;&#36305;&#36947;&#29289;&#20307;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#35780;&#20272;&#20102;&#35813;&#20998;&#31867;&#22120;&#23545;&#19977;&#31181;&#24120;&#35265;&#22270;&#20687;&#25200;&#21160;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.00035</link><description>&lt;p&gt;
&#33322;&#29677;&#28369;&#34892;&#23433;&#20840;&#30340;&#36305;&#36947;&#29289;&#20307;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#33322;&#29677;&#28369;&#34892;&#23433;&#20840;&#30340;&#36305;&#36947;&#29289;&#20307;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#35780;&#20272;&#20102;&#35813;&#20998;&#31867;&#22120;&#23545;&#19977;&#31181;&#24120;&#35265;&#22270;&#20687;&#25200;&#21160;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#35768;&#22810;&#35745;&#31639;&#38382;&#39064;&#19978;&#25104;&#20026;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#65292;&#33322;&#31354;&#19994;&#24076;&#26395;&#25506;&#32034;&#23427;&#20204;&#22312;&#20943;&#36731;&#39134;&#34892;&#21592;&#36127;&#25285;&#21644;&#25913;&#21892;&#36816;&#33829;&#23433;&#20840;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31867;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20351;&#29992;DNNs&#38656;&#35201;&#36827;&#34892;&#24443;&#24213;&#30340;&#35748;&#35777;&#36807;&#31243;&#12290;&#36825;&#19968;&#38656;&#27714;&#21487;&#20197;&#36890;&#36807;&#24418;&#24335;&#39564;&#35777;&#26469;&#35299;&#20915;&#65292;&#24418;&#24335;&#39564;&#35777;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#65292;&#20363;&#22914;&#35777;&#26126;&#26576;&#20123;&#35823;&#21028;&#30340;&#19981;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;Airbus&#24403;&#21069;&#27491;&#22312;&#24320;&#21457;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;DNN&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#26088;&#22312;&#22312;&#39134;&#26426;&#28369;&#34892;&#38454;&#27573;&#20351;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#20010;DNN&#23545;&#19977;&#31181;&#24120;&#35265;&#22270;&#20687;&#25200;&#21160;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65306;&#22122;&#22768;&#12289;&#20142;&#24230;&#21644;&#23545;&#27604;&#24230;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#37096;&#20998;&#32452;&#21512;&#12290;&#36825;&#20010;&#36807;&#31243;&#28041;&#21450;&#22810;&#27425;&#35843;&#29992;&#24213;&#23618;&#39564;&#35777;&#22120;&#65292;&#36825;&#21487;&#33021;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65307;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks (DNNs) are becoming the prominent solution for many computational problems, the aviation industry seeks to explore their potential in alleviating pilot workload and in improving operational safety. However, the use of DNNs in this type of safety-critical applications requires a thorough certification process. This need can be addressed through formal verification, which provides rigorous assurances -- e.g.,~by proving the absence of certain mispredictions. In this case-study paper, we demonstrate this process using an image-classifier DNN currently under development at Airbus and intended for use during the aircraft taxiing phase. We use formal methods to assess this DNN's robustness to three common image perturbation types: noise, brightness and contrast, and some of their combinations. This process entails multiple invocations of the underlying verifier, which might be computationally expensive; and we therefore propose a method that leverages the monotonicity
&lt;/p&gt;</description></item><item><title>LeanVec&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#32447;&#24615;&#38477;&#32500;&#21644;&#21521;&#37327;&#37327;&#21270;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#39640;&#32500;&#21521;&#37327;&#30340;&#30456;&#20284;&#24615;&#25628;&#32034;&#65292;&#35299;&#20915;&#20102;&#30456;&#20284;&#24615;&#25628;&#32034;&#31995;&#32479;&#22312;&#39640;&#21521;&#37327;&#32500;&#24230;&#19979;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#21387;&#21147;&#38382;&#39064;&#65292;&#21516;&#26102;&#24212;&#29992;&#20110;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2312.16335</link><description>&lt;p&gt;
LeanVec: &#36890;&#36807;&#20351;&#21521;&#37327;&#36866;&#24212;&#26469;&#21152;&#24555;&#25628;&#32034;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
LeanVec: Searching vectors faster by making them fit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16335
&lt;/p&gt;
&lt;p&gt;
LeanVec&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#32447;&#24615;&#38477;&#32500;&#21644;&#21521;&#37327;&#37327;&#21270;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21152;&#36895;&#39640;&#32500;&#21521;&#37327;&#30340;&#30456;&#20284;&#24615;&#25628;&#32034;&#65292;&#35299;&#20915;&#20102;&#30456;&#20284;&#24615;&#25628;&#32034;&#31995;&#32479;&#22312;&#39640;&#21521;&#37327;&#32500;&#24230;&#19979;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#21387;&#21147;&#38382;&#39064;&#65292;&#21516;&#26102;&#24212;&#29992;&#20110;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#29983;&#25104;&#39640;&#32500;&#21521;&#37327;&#30340;&#33021;&#21147;&#65292;&#20854;&#30456;&#20284;&#24615;&#21453;&#26144;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#22240;&#27492;&#65292;&#30456;&#20284;&#24615;&#25628;&#32034;&#65292;&#21363;&#22312;&#19968;&#20010;&#22823;&#22411;&#38598;&#21512;&#20013;&#26816;&#32034;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#20284;&#30340;&#21521;&#37327;&#30340;&#25805;&#20316;&#65292;&#24050;&#25104;&#20026;&#35768;&#22810;&#38656;&#35201;&#39640;&#24230;&#20934;&#30830;&#21644;&#21450;&#26102;&#31572;&#26696;&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#39640;&#21521;&#37327;&#32500;&#24230;&#20351;&#24471;&#30456;&#20284;&#24615;&#25628;&#32034;&#31995;&#32479;&#25215;&#21463;&#35745;&#31639;&#21644;&#20869;&#23384;&#21387;&#21147;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#24050;&#32463;&#26085;&#30410;&#26222;&#36941;&#65292;&#20363;&#22914;&#29992;&#25143;&#36755;&#20837;&#25991;&#26412;&#26597;&#35810;&#20197;&#25214;&#21040;&#19982;&#35813;&#26597;&#35810;&#26368;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26597;&#35810;&#36890;&#24120;&#19982;&#25968;&#25454;&#24211;&#23884;&#20837;&#20855;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#65292;&#36825;&#20351;&#24471;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LeanVec&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#32447;&#24615;&#38477;&#32500;&#19982;&#21521;&#37327;&#37327;&#21270;&#30456;&#32467;&#21512;&#20197;&#21152;&#36895;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16335v2 Announce Type: replace  Abstract: Modern deep learning models have the ability to generate high-dimensional vectors whose similarity reflects semantic resemblance. Thus, similarity search, i.e., the operation of retrieving those vectors in a large collection that are similar to a given query, has become a critical component of a wide range of applications that demand highly accurate and timely answers. In this setting, the high vector dimensionality puts similarity search systems under compute and memory pressure, leading to subpar performance. Additionally, cross-modal retrieval tasks have become increasingly common, e.g., where a user inputs a text query to find the most relevant images for that query. However, these queries often have different distributions than the database embeddings, making it challenging to achieve high accuracy. In this work, we present LeanVec, a framework that combines linear dimensionality reduction with vector quantization to accelerate 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36880;&#27493;&#25193;&#23637;&#21333;&#20010;&#33410;&#28857;&#21040;&#30446;&#26631;&#22270;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#23545;&#25152;&#26377;&#33410;&#28857;&#23545;&#30340;&#25972;&#20010;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22270;&#29983;&#25104;&#65292;&#21516;&#26102;&#36890;&#36807;&#22810;&#23610;&#24230;&#29983;&#25104;&#20445;&#25345;&#20102;&#39640;&#34920;&#36798;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.11529</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#26412;&#22320;&#25193;&#23637;&#23454;&#29616;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient and Scalable Graph Generation through Iterative Local Expansion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11529
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36880;&#27493;&#25193;&#23637;&#21333;&#20010;&#33410;&#28857;&#21040;&#30446;&#26631;&#22270;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#23545;&#25152;&#26377;&#33410;&#28857;&#23545;&#30340;&#25972;&#20010;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22270;&#29983;&#25104;&#65292;&#21516;&#26102;&#36890;&#36807;&#22810;&#23610;&#24230;&#29983;&#25104;&#20445;&#25345;&#20102;&#39640;&#34920;&#36798;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#30340;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#65292;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20195;&#34920;&#25152;&#26377;&#33410;&#28857;&#23545;&#30340;&#25972;&#20010;&#32852;&#21512;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#21516;&#26102;&#25429;&#25417;&#20840;&#23616;&#21644;&#23616;&#37096;&#22270;&#32467;&#26500;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#22411;&#22270;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#23558;&#21333;&#20010;&#33410;&#28857;&#25193;&#23637;&#21040;&#30446;&#26631;&#22270;&#26469;&#29983;&#25104;&#22270;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#20197;&#26412;&#22320;&#21270;&#26041;&#24335;&#28155;&#21152;&#33410;&#28857;&#21644;&#36793;&#65292;&#39318;&#20808;&#26500;&#24314;&#20840;&#23616;&#32467;&#26500;&#65292;&#28982;&#21518;&#32454;&#21270;&#23616;&#37096;&#32454;&#33410;&#12290;&#23616;&#37096;&#29983;&#25104;&#36991;&#20813;&#20102;&#23545;&#25152;&#26377;&#33410;&#28857;&#23545;&#19978;&#30340;&#25972;&#20010;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#30456;&#23545;&#20110;&#33410;&#28857;&#25968;&#32780;&#35328;&#23454;&#29616;&#20102;&#22823;&#24133;&#30340;&#35745;&#31639;&#33410;&#32422;&#65292;&#24182;&#36890;&#36807;&#22810;&#23610;&#24230;&#29983;&#25104;&#20445;&#25345;&#20102;&#39640;&#34920;&#36798;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20844;&#35748;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11529v2 Announce Type: replace-cross  Abstract: In the realm of generative models for graphs, extensive research has been conducted. However, most existing methods struggle with large graphs due to the complexity of representing the entire joint distribution across all node pairs and capturing both global and local graph structures simultaneously. To overcome these issues, we introduce a method that generates a graph by progressively expanding a single node to a target graph. In each step, nodes and edges are added in a localized manner through denoising diffusion, building first the global structure, and then refining the local details. The local generation avoids modeling the entire joint distribution over all node pairs, achieving substantial computational savings with subquadratic runtime relative to node count while maintaining high expressivity through multiscale generation. Our experiments show that our model achieves state-of-the-art performance on well-established b
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27491;&#21017;&#21270;&#23545;&#27604;&#24230;&#34920;&#31034;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#22312;&#35270;&#35273;&#23548;&#33322;&#31561;&#26085;&#24120;&#20219;&#21153;&#20013;&#20986;&#29616;&#22806;&#35266;&#21464;&#21270;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.09056</link><description>&lt;p&gt;
ReCoRe: &#27491;&#21017;&#21270;&#23545;&#27604;&#24230;&#34920;&#31034;&#23398;&#20064;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ReCoRe: Regularized Contrastive Representation Learning of World Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09056
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#23545;&#27604;&#24230;&#34920;&#31034;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#22312;&#35270;&#35273;&#23548;&#33322;&#31561;&#26085;&#24120;&#20219;&#21153;&#20013;&#20986;&#29616;&#22806;&#35266;&#21464;&#21270;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#22312;&#28216;&#25103;&#29615;&#22659;&#20013;&#24050;&#32463;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#35270;&#35273;&#23548;&#33322;&#31561;&#26085;&#24120;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20986;&#29616;&#26174;&#33879;&#22806;&#35266;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19990;&#30028;&#27169;&#22411;&#65292;&#36890;&#36807;&#65288;i&#65289;&#23545;&#27604;&#24230;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#65288;ii&#65289;&#20171;&#20837;&#19981;&#21464;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#12290;&#23398;&#20064;&#19990;&#30028;&#21160;&#21147;&#23398;&#30340;&#26174;&#24335;&#34920;&#31034;&#65292;&#21363;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#23545;&#27604;&#24230;&#23398;&#20064;&#38544;&#21547;&#22320;&#24378;&#21270;&#20102;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#65292;&#25913;&#21892;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#23558;&#23545;&#27604;&#24230;&#25439;&#22833;&#38598;&#25104;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#26159;&#19981;&#22815;&#30340;&#65292;&#22240;&#20026;&#22522;&#20110;&#19990;&#30028;&#27169;&#22411;&#30340;RL&#26041;&#27861;&#29420;&#31435;&#20248;&#21270;&#34920;&#31034;&#23398;&#20064;&#21644;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09056v2 Announce Type: replace-cross  Abstract: While recent model-free Reinforcement Learning (RL) methods have demonstrated human-level effectiveness in gaming environments, their success in everyday tasks like visual navigation has been limited, particularly under significant appearance variations. This limitation arises from (i) poor sample efficiency and (ii) over-fitting to training scenarios. To address these challenges, we present a world model that learns invariant features using (i) contrastive unsupervised learning and (ii) an intervention-invariant regularizer. Learning an explicit representation of the world dynamics i.e. a world model, improves sample efficiency while contrastive learning implicitly enforces learning of invariant features, which improves generalization. However, the na\"ive integration of contrastive loss to world models is not good enough, as world-model-based RL methods independently optimize representation learning and agent policy. To overc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20449;&#36947;&#21453;&#39304;&#30340;&#26032;&#22411;&#19979;&#34892;&#20256;&#36755;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#26080;&#32447;&#22320;&#22270;&#30340;&#22797;&#20540;&#39044;&#32534;&#30721;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#29992;&#25143;&#20301;&#32622;&#20026;&#22522;&#30784;&#30340;&#22522;&#31449;&#39044;&#32534;&#30721;&#12290;</title><link>https://arxiv.org/abs/2312.02184</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#20449;&#36947;&#21453;&#39304;&#30340;&#19979;&#34892;FD-RAN&#20256;&#36755;&#65306;&#22522;&#20110;&#26080;&#32447;&#22320;&#22270;&#30340;&#22797;&#20540;&#39044;&#32534;&#30721;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Channel-Feedback-Free Transmission for Downlink FD-RAN: A Radio Map based Complex-valued Precoding Network Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02184
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20449;&#36947;&#21453;&#39304;&#30340;&#26032;&#22411;&#19979;&#34892;&#20256;&#36755;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#26080;&#32447;&#22320;&#22270;&#30340;&#22797;&#20540;&#39044;&#32534;&#30721;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#29992;&#25143;&#20301;&#32622;&#20026;&#22522;&#30784;&#30340;&#22522;&#31449;&#39044;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#39640;&#36136;&#37327;&#26381;&#21153;&#38656;&#27714;&#30340;&#22686;&#38271;&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#23436;&#20840;&#35299;&#32806;&#21512;&#30340;RAN&#65288;FD-RAN&#65289;&#65292;&#24050;&#32463;&#20986;&#29616;&#65292;&#29992;&#20110;&#26356;&#28789;&#27963;&#30340;&#39057;&#35889;&#36164;&#28304;&#21033;&#29992;&#21644;&#26356;&#20302;&#30340;&#32593;&#32476;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;FD-RAN&#20013;&#65292;&#30001;&#20110;&#19978;&#34892;&#22522;&#31449;&#21644;&#19979;&#34892;&#22522;&#31449;&#30340;&#35299;&#32806;&#65292;&#20256;&#32479;&#30340;&#20381;&#36182;&#23454;&#26102;&#20449;&#36947;&#21453;&#39304;&#30340;&#20256;&#36755;&#26426;&#21046;&#24182;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#25509;&#25910;&#31471;&#26080;&#27861;&#21521;&#21457;&#36865;&#31471;&#21453;&#39304;&#20934;&#30830;&#21450;&#26102;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#36755;&#26041;&#26696;&#65292;&#19981;&#20381;&#36182;&#29289;&#29702;&#23618;&#20449;&#36947;&#21453;&#39304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#26080;&#32447;&#22320;&#22270;&#30340;&#22797;&#20540;&#39044;&#32534;&#30721;&#32593;&#32476;&#65288;RMCPNet&#65289;&#27169;&#22411;&#65292;&#26681;&#25454;&#29992;&#25143;&#20301;&#32622;&#36755;&#20986;&#22522;&#31449;&#39044;&#32534;&#30721;&#12290; RMCPNet &#21253;&#21547;&#22810;&#20010;&#23376;&#32593;&#32476;&#65292;&#27599;&#20010;&#23376;&#32593;&#32476;&#36127;&#36131;&#20174;&#19981;&#21516;&#30340;&#36755;&#20837;&#27169;&#24577;&#20013;&#25552;&#21462;&#29420;&#29305;&#30340;&#27169;&#24577;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02184v2 Announce Type: replace-cross  Abstract: As the demand for high-quality services proliferates, an innovative network architecture, the fully-decoupled RAN (FD-RAN), has emerged for more flexible spectrum resource utilization and lower network costs. However, with the decoupling of uplink base stations and downlink base stations in FD-RAN, the traditional transmission mechanism, which relies on real-time channel feedback, is not suitable as the receiver is not able to feedback accurate and timely channel state information to the transmitter. This paper proposes a novel transmission scheme without relying on physical layer channel feedback. Specifically, we design a radio map based complex-valued precoding network~(RMCPNet) model, which outputs the base station precoding based on user location. RMCPNet comprises multiple subnets, with each subnet responsible for extracting unique modal features from diverse input modalities. Furthermore, the multi-modal embeddings deriv
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21306;&#22495;&#23454;&#29616;&#20102;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#26080;&#38656;&#29992;&#25143;&#25552;&#20379;&#36974;&#32617;&#25110;&#33609;&#22270;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#25552;&#31034;&#30340;&#29305;&#28857;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#23637;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.16432</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#23398;&#20064;&#21306;&#22495;&#36827;&#34892;&#25991;&#26412;&#39537;&#21160;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Text-Driven Image Editing via Learnable Regions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21306;&#22495;&#23454;&#29616;&#20102;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#26080;&#38656;&#29992;&#25143;&#25552;&#20379;&#36974;&#32617;&#25110;&#33609;&#22270;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#25552;&#31034;&#30340;&#29305;&#28857;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#23637;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24050;&#32463;&#25104;&#20026;&#22270;&#20687;&#32534;&#36753;&#30340;&#33258;&#28982;&#25509;&#21475;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#20854;&#30001;&#25991;&#26412;&#25552;&#31034;&#39537;&#21160;&#65292;&#26080;&#38656;&#29992;&#25143;&#25552;&#20379;&#30340;&#36974;&#32617;&#25110;&#33609;&#22270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#36793;&#30028;&#26694;&#29983;&#25104;&#22120;&#26469;&#35782;&#21035;&#19982;&#25991;&#26412;&#25552;&#31034;&#23545;&#40784;&#30340;&#32534;&#36753;&#21306;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31616;&#21333;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#19982;&#24403;&#21069;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20860;&#23481;&#30340;&#28789;&#27963;&#32534;&#36753;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21253;&#21547;&#22810;&#20010;&#23545;&#35937;&#12289;&#22797;&#26434;&#21477;&#23376;&#25110;&#36739;&#38271;&#27573;&#33853;&#30340;&#22797;&#26434;&#25552;&#31034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19982;&#25552;&#20379;&#30340;&#35821;&#35328;&#25551;&#36848;&#30456;&#23545;&#24212;&#30340;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#21644;&#36924;&#30495;&#24230;&#30340;&#22270;&#20687;&#25805;&#20316;&#26041;&#38754;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#32593;&#39029;&#21487;&#20197;&#22312;&#27492;&#25214;&#21040;&#65306;ht
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16432v2 Announce Type: replace-cross  Abstract: Language has emerged as a natural interface for image editing. In this paper, we introduce a method for region-based image editing driven by textual prompts, without the need for user-provided masks or sketches. Specifically, our approach leverages an existing pre-trained text-to-image model and introduces a bounding box generator to identify the editing regions that are aligned with the textual prompts. We show that this simple approach enables flexible editing that is compatible with current image generation models, and is able to handle complex prompts featuring multiple objects, complex sentences, or lengthy paragraphs. We conduct an extensive user study to compare our method against state-of-the-art methods. The experiments demonstrate the competitive performance of our method in manipulating images with high fidelity and realism that correspond to the provided language descriptions. Our project webpage can be found at: ht
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;KL&#26465;&#20214;&#19979;&#20855;&#26377;&#26368;&#20248;&#36895;&#29575;&#30340;&#24046;&#20998;&#31169;&#26377;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#24773;&#20917;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.13447</link><description>&lt;p&gt;
&#22312;KL&#26465;&#20214;&#19979;&#20855;&#26377;&#26368;&#20248;&#36895;&#29575;&#30340;&#24046;&#20998;&#31169;&#26377;&#38750;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Non-Convex Optimization under the KL Condition with Optimal Rates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;KL&#26465;&#20214;&#19979;&#20855;&#26377;&#26368;&#20248;&#36895;&#29575;&#30340;&#24046;&#20998;&#31169;&#26377;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#24773;&#20917;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28385;&#36275;$(\gamma,\kappa)$-Kurdyka-Lojasiewicz (KL)&#26465;&#20214;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#31169;&#26377;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#38382;&#39064;&#12290;Polyak-Lojasiewicz (PL)&#26465;&#20214;&#26159;&#36825;&#20010;&#26465;&#20214;&#30340;&#29305;&#20363;&#65292;&#24403;$\kappa=2$&#26102;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;$\rho$&#38646;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#65288;zCDP&#65289;&#32422;&#26463;&#19979;&#30340;&#38382;&#39064;&#12290;&#24403;$\kappa\in[1,2]$&#19988;&#25439;&#22833;&#20989;&#25968;&#22312;&#36275;&#22815;&#22823;&#30340;&#21306;&#22495;&#20869;&#26159;Lipschitz&#21644;&#20809;&#28369;&#30340;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#26799;&#24230;&#19979;&#38477;&#30340;&#26032;&#31639;&#27861;&#65292;&#20854;&#22312;&#36229;&#39069;&#32463;&#39564;&#39118;&#38505;&#19978;&#23454;&#29616;&#20102;&#36895;&#29575;$\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$&#65292;&#20854;&#20013;$n$&#26159;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;$d$&#26159;&#32500;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#20010;&#36895;&#29575;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;&#24403;$\kappa \geq 2$&#19988;&#25439;&#22833;&#20989;&#25968;&#20195;&#26367;&#26159;Lipschitz&#21644;&#24369;&#20984;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#31169;&#26377;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#36895;&#29575;$\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13447v2 Announce Type: replace  Abstract: We study private empirical risk minimization (ERM) problem for losses satisfying the $(\gamma,\kappa)$-Kurdyka-{\L}ojasiewicz (KL) condition. The Polyak-{\L}ojasiewicz (PL) condition is a special case of this condition when $\kappa=2$. Specifically, we study this problem under the constraint of $\rho$ zero-concentrated differential privacy (zCDP). When $\kappa\in[1,2]$ and the loss function is Lipschitz and smooth over a sufficiently large region, we provide a new algorithm based on variance reduced gradient descent that achieves the rate $\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$ on the excess empirical risk, where $n$ is the dataset size and $d$ is the dimension. We further show that this rate is nearly optimal. When $\kappa \geq 2$ and the loss is instead Lipschitz and weakly convex, we show it is possible to achieve the rate $\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$ with a privat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#33041;&#34880;&#31649;&#27880;&#24847;&#21147;UNet&#26041;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#25552;&#21462;&#33041;&#34880;&#31649;&#22270;&#20687;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#28145;&#24230;&#30417;&#30563;UNet&#26469;&#25913;&#21892;&#33041;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#65292;&#26377;&#21161;&#20110;&#39044;&#38450;&#20013;&#39118;&#12290;</title><link>https://arxiv.org/abs/2311.10224</link><description>&lt;p&gt;
CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular Segmentation of Enhanced TOF-MRA Images
&lt;/p&gt;
&lt;p&gt;
CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular Segmentation of Enhanced TOF-MRA Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#33041;&#34880;&#31649;&#27880;&#24847;&#21147;UNet&#26041;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#25552;&#21462;&#33041;&#34880;&#31649;&#22270;&#20687;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#28145;&#24230;&#30417;&#30563;UNet&#26469;&#25913;&#21892;&#33041;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#65292;&#26377;&#21161;&#20110;&#39044;&#38450;&#20013;&#39118;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35786;&#26029;&#33041;&#34880;&#31649;&#30142;&#30149;&#65292;&#26102;&#38388;&#39134;&#34892;&#30913;&#20849;&#25391;&#34880;&#31649;&#36896;&#24433;&#22270;&#65288;TOF-MRA&#65289;&#36890;&#24120;&#26159;&#36890;&#36807;&#30446;&#35270;&#35780;&#20272;&#65292;&#36825;&#23548;&#33268;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#33041;&#34880;&#31649;&#27880;&#24847;&#21147;UNet&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;CV-Attention UNet&#65292;&#29992;&#20110;&#31934;&#30830;&#25552;&#21462;&#33041;&#34880;&#31649;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#38543;&#21518;&#37319;&#29992;&#28145;&#24230;&#30417;&#30563;UNet&#26469;&#25913;&#21892;&#33041;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#39044;&#38450;&#20013;&#39118;&#12290;&#20026;&#20102;&#32467;&#21512;&#20302;&#35821;&#20041;&#21644;&#39640;&#35821;&#20041;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10224v2 Announce Type: replace-cross  Abstract: Due to the lack of automated methods, to diagnose cerebrovascular disease, time-of-flight magnetic resonance angiography (TOF-MRA) is assessed visually, making it time-consuming. The commonly used encoder-decoder architectures for cerebrovascular segmentation utilize redundant features, eventually leading to the extraction of low-level features multiple times. Additionally, convolutional neural networks (CNNs) suffer from performance degradation when the batch size is small, and deeper networks experience the vanishing gradient problem. Methods: In this paper, we attempt to solve these limitations and propose the 3D cerebrovascular attention UNet method, named CV-AttentionUNet, for precise extraction of brain vessel images. We proposed a sequence of preprocessing techniques followed by deeply supervised UNet to improve the accuracy of segmentation of the brain vessels leading to a stroke. To combine the low and high semantics, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#20984;&#38750;&#20809;&#28369;FL&#38382;&#39064;&#30340;&#26032;&#39062;&#32852;&#37030;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#65292;&#37319;&#29992;&#21452;&#21521;&#27169;&#22411;&#31232;&#30095;&#21270;&#65292;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#38544;&#31169;&#20445;&#38556;&#65292;&#24182;&#25552;&#20986;&#20102;FL&#31639;&#27861;&#35774;&#35745;&#25351;&#23548;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2310.19558</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#20984;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#21407;&#22987;-&#23545;&#20598;&#23398;&#20064;&#19982;&#27169;&#22411;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving Federated Primal-dual Learning for Non-convex and Non-smooth Problems with Model Sparsification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#20984;&#38750;&#20809;&#28369;FL&#38382;&#39064;&#30340;&#26032;&#39062;&#32852;&#37030;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#65292;&#37319;&#29992;&#21452;&#21521;&#27169;&#22411;&#31232;&#30095;&#21270;&#65292;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#36827;&#34892;&#38544;&#31169;&#20445;&#38556;&#65292;&#24182;&#25552;&#20986;&#20102;FL&#31639;&#27861;&#35774;&#35745;&#25351;&#23548;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#21407;&#22987;-&#23545;&#20598;&#23398;&#20064;&#38024;&#23545;&#20110;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#35757;&#32451;&#27169;&#22411;&#30340;&#31867;&#22411;&#36827;&#34892;&#30740;&#31350;&#65292;&#21442;&#25968;&#26381;&#21153;&#22120;&#36827;&#34892;&#21327;&#35843;&#65292;&#32780;&#19981;&#20849;&#20139;&#23458;&#25143;&#31471;&#25968;&#25454;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#19968;&#31867;&#22312;FL&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#20294;&#38590;&#20197;&#22788;&#29702;&#30340;&#38750;&#20984;&#38750;&#20809;&#28369;&#25439;&#22833;&#20989;&#25968;&#30340;&#32852;&#37030;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#22240;&#20854;&#22797;&#26434;&#30340;&#38750;&#20984;&#24615;&#21644;&#38750;&#20809;&#28369;&#24615;&#36136;&#20197;&#21450;&#23545;&#36890;&#20449;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#30683;&#30462;&#35201;&#27714;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#20984;&#38750;&#20809;&#28369;FL&#38382;&#39064;&#30340;&#26032;&#39062;&#32852;&#37030;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#65292;&#22312;&#38544;&#31169;&#20445;&#35777;&#26041;&#38754;&#24212;&#29992;&#20102;&#24046;&#20998;&#38544;&#31169;&#12290;&#25991;&#20013;&#36824;&#20171;&#32461;&#20102;&#20854;&#29420;&#29305;&#30340;&#35265;&#35299;&#24615;&#23646;&#24615;&#20197;&#21450;&#19968;&#20123;&#38544;&#31169;&#21644;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#20316;&#20026;FL&#31639;&#27861;&#35774;&#35745;&#25351;&#23548;&#21407;&#21017;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19558v2 Announce Type: replace  Abstract: Federated learning (FL) has been recognized as a rapidly growing research area, where the model is trained over massively distributed clients under the orchestration of a parameter server (PS) without sharing clients' data. This paper delves into a class of federated problems characterized by non-convex and non-smooth loss functions, that are prevalent in FL applications but challenging to handle due to their intricate non-convexity and non-smoothness nature and the conflicting requirements on communication efficiency and privacy protection. In this paper, we propose a novel federated primal-dual algorithm with bidirectional model sparsification tailored for non-convex and non-smooth FL problems, and differential privacy is applied for privacy guarantee. Its unique insightful properties and some privacy and convergence analyses are also presented as the FL algorithm design guidelines. Extensive experiments on real-world data are cond
&lt;/p&gt;</description></item><item><title>BatteryML&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#36890;&#36807;&#19968;&#31449;&#24335;&#12289;&#20840;&#38754;&#30340;&#26041;&#27861;&#32479;&#19968;&#20102;&#30005;&#27744;&#34928;&#20943;&#24314;&#27169;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#23454;&#29616;&#65292;&#25552;&#39640;&#20102;&#30740;&#31350;&#24212;&#29992;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2310.14714</link><description>&lt;p&gt;
BatteryML&#65306;&#19968;&#20010;&#29992;&#20110;&#30005;&#27744;&#34928;&#20943;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#28304;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
BatteryML:An Open-source platform for Machine Learning on Battery Degradation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14714
&lt;/p&gt;
&lt;p&gt;
BatteryML&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#36890;&#36807;&#19968;&#31449;&#24335;&#12289;&#20840;&#38754;&#30340;&#26041;&#27861;&#32479;&#19968;&#20102;&#30005;&#27744;&#34928;&#20943;&#24314;&#27169;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#23454;&#29616;&#65292;&#25552;&#39640;&#20102;&#30740;&#31350;&#24212;&#29992;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27744;&#34928;&#20943;&#20173;&#28982;&#26159;&#33021;&#28304;&#23384;&#20648;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#25512;&#21160;&#27934;&#23519;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24037;&#20855;&#27491;&#22312;&#23835;&#36215;&#12290;&#28982;&#32780;&#65292;&#30005;&#21270;&#23398;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#39046;&#22495;&#24102;&#26469;&#20102;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#32463;&#24120;&#22312;&#22788;&#29702;&#30005;&#27744;&#31185;&#23398;&#30340;&#22797;&#26434;&#24615;&#19978;&#33510;&#33510;&#25379;&#25166;&#65292;&#32780;&#30005;&#27744;&#30740;&#31350;&#20154;&#21592;&#21017;&#38754;&#20020;&#30528;&#23558;&#22797;&#26434;&#27169;&#22411;&#35843;&#25972;&#21040;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#38556;&#30861;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#28085;&#30422;&#25968;&#25454;&#26684;&#24335;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#30005;&#27744;&#34928;&#20943;&#24314;&#27169;&#30340;&#32479;&#19968;&#26631;&#20934;&#12290;&#37492;&#20110;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BatteryML - &#19968;&#20010;&#19968;&#31449;&#24335;&#12289;&#20840;&#38754;&#19988;&#24320;&#28304;&#30340;&#24179;&#21488;&#65292;&#26088;&#22312;&#32479;&#19968;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#20197;&#21450;&#20256;&#32479;&#21644;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#23454;&#29616;&#12290;&#36825;&#31181;&#31616;&#21270;&#30340;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#30740;&#31350;&#24212;&#29992;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14714v4 Announce Type: replace-cross  Abstract: Battery degradation remains a pivotal concern in the energy storage domain, with machine learning emerging as a potent tool to drive forward insights and solutions. However, this intersection of electrochemical science and machine learning poses complex challenges. Machine learning experts often grapple with the intricacies of battery science, while battery researchers face hurdles in adapting intricate models tailored to specific datasets. Beyond this, a cohesive standard for battery degradation modeling, inclusive of data formats and evaluative benchmarks, is conspicuously absent. Recognizing these impediments, we present BatteryML - a one-step, all-encompass, and open-source platform designed to unify data preprocessing, feature extraction, and the implementation of both traditional and state-of-the-art models. This streamlined approach promises to enhance the practicality and efficiency of research applications. BatteryML s
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#34920;&#26684;&#20998;&#31867;&#20219;&#21153;&#20013;&#23384;&#22312;&#31038;&#20250;&#20559;&#35265;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.14607</link><description>&lt;p&gt;
&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#65306;&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#20998;&#31867;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14607
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#34920;&#26684;&#20998;&#31867;&#20219;&#21153;&#20013;&#23384;&#22312;&#31038;&#20250;&#20559;&#35265;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#34920;&#26684;&#20219;&#21153;&#30340;&#20998;&#31867;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#24050;&#34987;&#35777;&#26126;&#23384;&#22312;&#34920;&#29616;&#20986;&#31038;&#20250;&#20559;&#35265;&#30340;&#26377;&#23475;&#22240;&#32032;&#65292;&#21453;&#26144;&#20102;&#31038;&#20250;&#20013;&#23384;&#22312;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#19981;&#24179;&#31561;&#12290;&#20026;&#27492;&#65292;&#20197;&#21450;&#22312;&#35768;&#22810;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#24191;&#27867;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#65292;&#25506;&#35752;&#20197;&#19979;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65306;LLMs&#22312;&#36827;&#34892;&#34920;&#26684;&#20219;&#21153;&#20998;&#31867;&#26102;&#21033;&#29992;&#20102;&#21738;&#20123;&#20449;&#24687;&#28304;&#65307;LLMs&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#31038;&#20250;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#30340;&#24433;&#21709;&#65307;&#20197;&#21450;&#36825;&#23545;&#20844;&#24179;&#24615;&#21487;&#33021;&#20135;&#29983;&#30340;&#37325;&#35201;&#24433;&#21709;&#26159;&#20160;&#20040;&#65311;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;LLMs&#20542;&#21521;&#20110;&#32487;&#25215;&#26469;&#33258;&#35757;&#32451;&#25968;&#25454;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#36825;&#26174;&#33879;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#34920;&#26684;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14607v2 Announce Type: replace  Abstract: Recent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is important to explore the following questions: what sources of information do LLMs draw upon when making classifications for tabular tasks; whether and to what extent are LLM classifications for tabular data influenced by social biases and stereotypes; and what are the consequential implications for fairness?   Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks. Furthermore, our investigations show that in the context of bias miti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26500;&#24314;&#23545;&#27604;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#30340;&#20559;&#22909;&#23545;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#25928;&#26524;&#65292;&#24182;&#19988;&#36890;&#36807;DPO&#23545;&#27604;&#25216;&#26415;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#23545;&#40784;&#65292;&#26368;&#32456;&#20351;&#32463;&#36807;&#35843;&#20248;&#30340;&#25351;&#23548;&#23398;&#20064;&#27169;&#22411;Orca&#36229;&#36234;&#20102;ChatGPT&#12290;</title><link>https://arxiv.org/abs/2310.02263</link><description>&lt;p&gt;
&#23545;&#27604;&#21518;&#35757;&#32451;&#30340;&#33258;&#21160;&#23545;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Automatic Pair Construction for Contrastive Post-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02263
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26500;&#24314;&#23545;&#27604;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#30340;&#20559;&#22909;&#23545;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#25928;&#26524;&#65292;&#24182;&#19988;&#36890;&#36807;DPO&#23545;&#27604;&#25216;&#26415;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#23545;&#40784;&#65292;&#26368;&#32456;&#20351;&#32463;&#36807;&#35843;&#20248;&#30340;&#25351;&#23548;&#23398;&#20064;&#27169;&#22411;Orca&#36229;&#36234;&#20102;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#20316;&#20026;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36208;&#21521;&#20154;&#31867;&#20559;&#22909;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26500;&#24314;LLM&#23545;&#27604;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#19981;&#21516;&#24378;&#24230;&#27169;&#22411;&#65288;&#20363;&#22914;InstructGPT&#12289;ChatGPT&#21644;GPT-4&#65289;&#30340;&#20559;&#22909;&#23545;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;SLiC&#21644;DPO&#30340;&#23545;&#27604;&#25216;&#26415;&#19982;SFT&#22522;&#32447;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#22312;&#32487;&#32493;SFT&#39281;&#21644;&#21518;&#65292;DPO&#20173;&#28982;&#25552;&#20379;&#20102;&#19968;&#20010;&#38454;&#36291;&#24335;&#30340;&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19968;&#31181;&#23545;&#27604;&#21518;&#35757;&#32451;&#30340;&#25968;&#25454;&#35838;&#31243;&#23398;&#20064;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20174;&#8220;&#26356;&#23481;&#26131;&#8221;&#30340;&#23545;&#24320;&#22987;&#23398;&#20064;&#65292;&#28982;&#21518;&#36807;&#28193;&#21040;&#8220;&#26356;&#38590;&#8221;&#30340;&#23545;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#23545;&#40784;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26356;&#22810;&#25968;&#25454;&#21644;&#20687;Orca&#36825;&#26679;&#30340;&#26356;&#22823;&#22411;&#27169;&#22411;&#26469;&#25193;&#22823;&#23454;&#39564;&#35268;&#27169;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#33258;&#21160;&#23545;&#27604;&#21518;&#35757;&#32451;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;Orca&#30340;&#24615;&#33021;&#65292;&#23427;&#24050;&#32463;&#26159;&#19968;&#20010;&#36890;&#36807;GPT-4&#36755;&#20986;&#35843;&#20248;&#30340;&#26368;&#20808;&#36827;&#25351;&#23548;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#36229;&#36234;&#20102;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02263v2 Announce Type: replace-cross  Abstract: Alignment serves as an important step to steer large language models (LLMs) towards human preferences. In this paper, we propose an automatic way to construct contrastive data for LLM, using preference pairs from multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We compare the contrastive techniques of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement even after continuing SFT saturates. We also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from "easier" pairs and transitioning to "harder" ones, which further improves alignment. Finally, we scale up our experiments to train with more data and larger models like Orca. Remarkably, our automatic contrastive post-training further improves the performance of Orca, already a state-of-the-art instruction learning model tuned with GPT-4 outputs, to outperform ChatGPT.
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#36817;&#36317;&#31163;&#33258;&#25293;&#29031;&#29255;&#29983;&#25104;&#20986;&#19968;&#24352;&#21035;&#20154;&#20174;&#20960;&#33521;&#23610;&#22806;&#25293;&#25668;&#30340;&#24744;&#30340;&#20840;&#36523;&#33258;&#25293;&#29031;&#29255;&#12290;</title><link>https://arxiv.org/abs/2308.14740</link><description>&lt;p&gt;
&#20840;&#26223;&#33258;&#25293;&#65306;&#29983;&#25104;&#20840;&#36523;&#33258;&#25293;&#29031;&#29255;
&lt;/p&gt;
&lt;p&gt;
Total Selfie: Generating Full-Body Selfies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.14740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#36817;&#36317;&#31163;&#33258;&#25293;&#29031;&#29255;&#29983;&#25104;&#20986;&#19968;&#24352;&#21035;&#20154;&#20174;&#20960;&#33521;&#23610;&#22806;&#25293;&#25668;&#30340;&#24744;&#30340;&#20840;&#36523;&#33258;&#25293;&#29031;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20174;&#26368;&#21021;&#22788;&#20110;&#25163;&#33218;&#38271;&#24230;&#20301;&#32622;&#30340;&#29031;&#29255;&#20013;&#29983;&#25104;&#20840;&#36523;&#33258;&#25293;&#29031;&#29255;&#12290;&#30001;&#20110;&#33258;&#25293;&#29031;&#29255;&#36890;&#24120;&#26159;&#36817;&#36317;&#31163;&#25293;&#25668;&#30340;&#65292;&#22240;&#27492;&#20854;&#35270;&#37326;&#26377;&#38480;&#65292;&#36879;&#35270;&#25928;&#26524;&#22840;&#24352;&#65292;&#25197;&#26354;&#20102;&#38754;&#37096;&#24418;&#29366;&#12290;&#25105;&#20204;&#24076;&#26395;&#29983;&#25104;&#21035;&#20154;&#20174;&#20960;&#33521;&#23610;&#22806;&#25293;&#25668;&#30340;&#24744;&#30340;&#29031;&#29255;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#24744;&#30340;&#38754;&#37096;&#21644;&#36523;&#20307;&#30340;&#22235;&#24352;&#33258;&#25293;&#29031;&#29255;&#12289;&#32972;&#26223;&#22270;&#20687;&#20026;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#25351;&#23450;&#30446;&#26631;&#23039;&#21183;&#30340;&#20840;&#36523;&#33258;&#25293;&#29031;&#29255;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#25152;&#26377;&#36825;&#20123;&#20449;&#24687;&#32467;&#21512;&#25104;&#24744;&#20855;&#26377;&#25152;&#38656;&#23039;&#21183;&#21644;&#32972;&#26223;&#30340;&#39640;&#36136;&#37327;&#12289;&#26500;&#22270;&#21512;&#29702;&#30340;&#29031;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.14740v2 Announce Type: replace-cross  Abstract: We present a method to generate full-body selfies from photographs originally taken at arms length. Because self-captured photos are typically taken close up, they have limited field of view and exaggerated perspective that distorts facial shapes. We instead seek to generate the photo some one else would take of you from a few feet away. Our approach takes as input four selfies of your face and body, a background image, and generates a full-body selfie in a desired target pose. We introduce a novel diffusion-based approach to combine all of this information into high-quality, well-composed photos of you with the desired pose and background.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#36890;&#36807;&#34920;&#24449;&#20943;&#32531;&#25317;&#25380;&#30340;&#26041;&#27861;&#65292;&#20174;&#28040;&#36153;&#32773;&#30340;&#36873;&#25321;&#20013;&#23398;&#20064;&#34920;&#24449;&#20197;&#20943;&#23569;&#25317;&#25380;&#65292;&#25552;&#39640;&#31038;&#20250;&#31119;&#21033;&#12290;</title><link>https://arxiv.org/abs/2306.10606</link><description>&lt;p&gt;
&#36890;&#36807;&#34920;&#24449;&#20943;&#32531;&#25317;&#25380;: &#23398;&#20064;&#22914;&#20309;&#25552;&#39640;&#24066;&#22330;&#20013;&#30340;&#32463;&#27982;&#31119;&#21033;
&lt;/p&gt;
&lt;p&gt;
Decongestion by Representation: Learning to Improve Economic Welfare in Marketplaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.10606
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#36890;&#36807;&#34920;&#24449;&#20943;&#32531;&#25317;&#25380;&#30340;&#26041;&#27861;&#65292;&#20174;&#28040;&#36153;&#32773;&#30340;&#36873;&#25321;&#20013;&#23398;&#20064;&#34920;&#24449;&#20197;&#20943;&#23569;&#25317;&#25380;&#65292;&#25552;&#39640;&#31038;&#20250;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#25380;&#26159;&#24066;&#22330;&#30340;&#19968;&#31181;&#24120;&#35265;&#22833;&#25928;&#27169;&#24335;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#28040;&#36153;&#32773;&#22312;&#21516;&#19968;&#23376;&#21830;&#21697;&#38598;&#19978;&#36827;&#34892;&#20302;&#25928;&#29575;&#31454;&#20105;&#65288;&#20363;&#22914;&#65292;&#22312;&#24230;&#20551;&#31199;&#36161;&#24179;&#21488;&#19978;&#36861;&#36880;&#21516;&#19968;&#23567;&#37096;&#20998;&#23646;&#24615;&#65289;&#12290;&#20256;&#32479;&#30340;&#32463;&#27982;&#25925;&#20107;&#26159;&#65292;&#20215;&#26684;&#36890;&#36807;&#24179;&#34913;&#20379;&#27714;&#26469;&#20943;&#32531;&#25317;&#25380;&#12290;&#20294;&#22312;&#29616;&#20195;&#22312;&#32447;&#24066;&#22330;&#20013;&#65292;&#20215;&#26684;&#36890;&#24120;&#30001;&#21334;&#23478;&#20197;&#20998;&#25955;&#30340;&#26041;&#24335;&#35774;&#23450;&#65292;&#24182;&#19988;&#26377;&#20851;&#21830;&#21697;&#30340;&#20449;&#24687;&#19981;&#21487;&#36991;&#20813;&#22320;&#26159;&#37096;&#20998;&#30340;&#12290;&#24179;&#21488;&#30340;&#33021;&#21147;&#20165;&#38480;&#20110;&#25511;&#21046;&#34920;&#24449;&#8212;&#8212;&#40664;&#35748;&#21521;&#29992;&#25143;&#21576;&#29616;&#30340;&#26377;&#20851;&#21830;&#21697;&#20449;&#24687;&#30340;&#23376;&#38598;&#12290;&#36825;&#20419;&#20351;&#20102;&#23545;&#36890;&#36807;&#34920;&#24449;&#20943;&#32531;&#25317;&#25380;&#30340;&#30740;&#31350;&#65292;&#21363;&#24179;&#21488;&#35797;&#22270;&#23398;&#20064;&#20943;&#23569;&#25317;&#25380;&#20174;&#32780;&#25552;&#39640;&#31038;&#20250;&#31119;&#21033;&#30340;&#34920;&#24449;&#12290;&#25216;&#26415;&#25361;&#25112;&#26159;&#21452;&#37325;&#30340;&#65306;&#20165;&#20381;&#36182;&#20110;&#28040;&#36153;&#32773;&#30340;&#36873;&#25321;&#25152;&#25581;&#31034;&#30340;&#20559;&#22909;&#32780;&#19981;&#26159;&#30495;&#23454;&#20559;&#22909;&#65307;&#20197;&#21450;&#19982;&#30830;&#23450;&#34920;&#24449;&#30456;&#20851;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.10606v2 Announce Type: replace  Abstract: Congestion is a common failure mode of markets, where consumers compete inefficiently on the same subset of goods (e.g., chasing the same small set of properties on a vacation rental platform). The typical economic story is that prices decongest by balancing supply and demand. But in modern online marketplaces, prices are typically set in a decentralized way by sellers, and the information about items is inevitably partial. The power of a platform is limited to controlling representations -- the subset of information about items presented by default to users. This motivates the present study of decongestion by representation, where a platform seeks to learn representations that reduce congestion and thus improve social welfare. The technical challenge is twofold: relying only on revealed preferences from the choices of consumers, rather than true preferences; and the combinatorial problem associated with representations that determin
&lt;/p&gt;</description></item><item><title>RDumb&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#33021;&#22312;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#20013;&#34920;&#29616;&#24471;&#26356;&#22909;&#65292;&#29978;&#33267;&#36229;&#36807;&#20808;&#21069;&#25552;&#20986;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2306.05401</link><description>&lt;p&gt;
RDumb&#65306;&#19968;&#31181;&#36136;&#30097;&#25105;&#20204;&#22312;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#20013;&#21462;&#24471;&#36827;&#23637;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RDumb: A simple approach that questions our progress in continual test-time adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.05401
&lt;/p&gt;
&lt;p&gt;
RDumb&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#33021;&#22312;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#20013;&#34920;&#29616;&#24471;&#26356;&#22909;&#65292;&#29978;&#33267;&#36229;&#36807;&#20808;&#21069;&#25552;&#20986;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#20801;&#35768;&#22312;&#37096;&#32626;&#26102;&#26681;&#25454;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#26356;&#26032;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#20102;&#26816;&#39564;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#26029;&#21464;&#21270;&#33104;&#22351;&#65288;CCC&#65289;&#22522;&#20934;&#26469;&#34913;&#37327;TTA&#25216;&#26415;&#30340;&#28176;&#36817;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#38500;&#19968;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#22806;&#65292;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#26368;&#32456;&#37117;&#20250;&#23849;&#28291;&#24182;&#34920;&#29616;&#27604;&#19981;&#36866;&#24212;&#30340;&#27169;&#22411;&#26356;&#31967;&#65292;&#21253;&#25324;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25239;&#24615;&#23849;&#28291;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#8220;RDumb&#8221;&#65292;&#23450;&#26399;&#23558;&#27169;&#22411;&#37325;&#32622;&#20026;&#39044;&#35757;&#32451;&#29366;&#24577;&#12290;&#22312;&#25152;&#26377;&#32771;&#34385;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#8220;RDumb&#8221;&#22312;&#34920;&#29616;&#19978;&#35201;&#20040;&#26356;&#22909;&#65292;&#35201;&#20040;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#25345;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20808;&#21069;&#30340;TTA&#26041;&#27861;&#26082;&#19981;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.05401v3 Announce Type: replace  Abstract: Test-Time Adaptation (TTA) allows to update pre-trained models to changing data distributions at deployment time. While early work tested these algorithms for individual fixed distribution shifts, recent work proposed and applied methods for continual adaptation over long timescales. To examine the reported progress in the field, we propose the Continually Changing Corruptions (CCC) benchmark to measure asymptotic performance of TTA techniques. We find that eventually all but one state-of-the-art methods collapse and perform worse than a non-adapting model, including models specifically proposed to be robust to performance collapse. In addition, we introduce a simple baseline, "RDumb", that periodically resets the model to its pretrained state. RDumb performs better or on par with the previously proposed state-of-the-art in all considered benchmarks. Our results show that previous TTA approaches are neither effective at regularizing 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#38598;&#25104;&#30340;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550; DPoE&#65292;&#26088;&#22312;&#36890;&#36807;&#21435;&#22122;&#35774;&#35745;&#21644;&#25429;&#25417;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#27973;&#23618;&#27169;&#22411;&#65292;&#20197;&#21450;&#38450;&#27490;&#23398;&#20064;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#20027;&#27169;&#22411;&#65292;&#26377;&#25928;&#25269;&#24481;&#21508;&#31181;&#21518;&#38376;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2305.14910</link><description>&lt;p&gt;
&#20174;&#24555;&#25463;&#26041;&#24335;&#21040;&#35302;&#21457;&#22120;&#65306;&#20351;&#29992;&#21435;&#22122; PoE &#36827;&#34892;&#21518;&#38376;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
From Shortcuts to Triggers: Backdoor Defense with Denoised PoE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14910
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#38598;&#25104;&#30340;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550; DPoE&#65292;&#26088;&#22312;&#36890;&#36807;&#21435;&#22122;&#35774;&#35745;&#21644;&#25429;&#25417;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#27973;&#23618;&#27169;&#22411;&#65292;&#20197;&#21450;&#38450;&#27490;&#23398;&#20064;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#20027;&#27169;&#22411;&#65292;&#26377;&#25928;&#25269;&#24481;&#21508;&#31181;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#38754;&#20020;&#22810;&#26679;&#30340;&#21518;&#38376;&#25915;&#20987;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#25968;&#25454;&#27745;&#26579;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#38450;&#24481;&#35299;&#20915;&#26041;&#26696;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#24102;&#26377;&#26174;&#24335;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#19978;&#65292;&#23545;&#25239;&#21508;&#31181;&#21518;&#38376;&#25915;&#20987;&#19982;&#19981;&#21516;&#35302;&#21457;&#22120;&#30340;&#36890;&#29992;&#38450;&#24481;&#26041;&#27861;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#38598;&#25104;&#30340;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550; DPoE&#65288;Denoised Product-of-Experts&#65289;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#24555;&#25463;&#26041;&#24335;&#65292;&#20197;&#25269;&#24481;&#21508;&#31181;&#21518;&#38376;&#25915;&#20987;&#12290;DPoE &#21253;&#21547;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#25429;&#25417;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#27973;&#23618;&#27169;&#22411;&#21644;&#19968;&#20010;&#34987;&#38459;&#27490;&#23398;&#20064;&#21518;&#38376;&#24555;&#25463;&#26041;&#24335;&#30340;&#20027;&#27169;&#22411;&#12290;&#20026;&#20102;&#22788;&#29702;&#21518;&#38376;&#25915;&#20987;&#32773;&#24341;&#36215;&#30340;&#26631;&#31614;&#32763;&#36716;&#65292;DPoE &#34701;&#20837;&#20102;&#21435;&#22122;&#35774;&#35745;&#12290;&#23545; SST-2 &#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DPoE &#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14910v3 Announce Type: replace-cross  Abstract: Language models are often at risk of diverse backdoor attacks, especially data poisoning. Thus, it is important to investigate defense solutions for addressing them. Existing backdoor defense methods mainly focus on backdoor attacks with explicit triggers, leaving a universal defense against various backdoor attacks with diverse triggers largely unexplored. In this paper, we propose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised Product-of-Experts), which is inspired by the shortcut nature of backdoor attacks, to defend various backdoor attacks. DPoE consists of two models: a shallow model that captures the backdoor shortcuts and a main model that is prevented from learning the backdoor shortcuts. To address the label flip caused by backdoor attackers, DPoE incorporates a denoising design. Experiments on SST-2 dataset show that DPoE significantly improves the defense performance against various types of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20013;&#24615;&#21407;&#23376;&#37327;&#23376;&#22788;&#29702;&#22120;&#30340;&#37329;&#34701;&#39118;&#38505;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#20449;&#29992;&#35780;&#32423;&#38477;&#32423;&#26041;&#38754;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12289;&#36895;&#24230;&#26356;&#24555;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#24352;&#37327;&#32593;&#32476;-based&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#36825;&#20123;&#24819;&#27861;&#12290;</title><link>https://arxiv.org/abs/2212.03223</link><description>&lt;p&gt;
&#19968;&#20010;&#20013;&#24615;&#21407;&#23376;&#37327;&#23376;&#22788;&#29702;&#22120;&#19978;&#30340;&#37329;&#34701;&#39118;&#38505;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Financial Risk Management on a Neutral Atom Quantum Processor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.03223
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20013;&#24615;&#21407;&#23376;&#37327;&#23376;&#22788;&#29702;&#22120;&#30340;&#37329;&#34701;&#39118;&#38505;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#20449;&#29992;&#35780;&#32423;&#38477;&#32423;&#26041;&#38754;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12289;&#36895;&#24230;&#26356;&#24555;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#24352;&#37327;&#32593;&#32476;-based&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#36825;&#20123;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#22788;&#29702;&#37329;&#34701;&#39046;&#22495;&#22823;&#25968;&#25454;&#38598;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32463;&#24120;&#21464;&#24471;&#26214;&#28073;&#38590;&#35299;&#19988;&#36816;&#34892;&#25104;&#26412;&#39640;&#26114;&#12290;&#37327;&#23376;&#35745;&#31639;&#33539;&#24335;&#25552;&#20986;&#20102;&#26032;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#32467;&#21512;&#32463;&#20856;&#31639;&#27861;&#65292;&#21487;&#33021;&#25552;&#20379;&#31454;&#20105;&#21147;&#26356;&#24378;&#12289;&#36895;&#24230;&#26356;&#24555;&#19988;&#26356;&#26131;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37327;&#23376;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#37329;&#34701;&#39118;&#38505;&#31649;&#29702;&#39046;&#22495;&#30340;&#20449;&#29992;&#35780;&#32423;&#19979;&#35843;&#39044;&#27979;&#65292;&#20063;&#34987;&#31216;&#20026;&#22549;&#33853;&#22825;&#20351;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#20102;&#25317;&#26377;&#39640;&#36798;60&#27604;&#29305;&#30340;&#20013;&#24615;&#21407;&#23376;&#37327;&#23376;&#22788;&#29702;&#21333;&#20803;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#38543;&#26426;&#26862;&#26519;&#22522;&#20934;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#27604;&#36739;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#30701;&#26399;&#20869;&#25913;&#36827;&#24615;&#33021;&#65292;&#24182;&#29992;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.03223v2 Announce Type: replace-cross  Abstract: Machine Learning models capable of handling the large datasets collected in the financial world can often become black boxes expensive to run. The quantum computing paradigm suggests new optimization techniques, that combined with classical algorithms, may deliver competitive, faster and more interpretable models. In this work we propose a quantum-enhanced machine learning solution for the prediction of credit rating downgrades, also known as fallen-angels forecasting in the financial risk management field. We implement this solution on a neutral atom Quantum Processing Unit with up to 60 qubits on a real-life dataset. We report competitive performances against the state-of-the-art Random Forest benchmark whilst our model achieves better interpretability and comparable training times. We examine how to improve performance in the near-term validating our ideas with Tensor Networks-based numerical simulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#39044;&#27979;MDP&#31574;&#30053;&#36798;&#21040;&#29992;&#25143;&#25351;&#23450;&#34892;&#20026;&#30446;&#26631;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#31526;&#21512;&#39044;&#27979;&#36827;&#34892;&#21453;&#36716;&#26469;&#35745;&#31639;&#27010;&#29575;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2211.16462</link><description>&lt;p&gt;
&#25105;&#30340;&#26426;&#22120;&#20154;&#20250;&#23454;&#29616;&#25105;&#30340;&#30446;&#26631;&#21527;&#65311;&#39044;&#27979;MDP&#31574;&#30053;&#36798;&#21040;&#29992;&#25143;&#25351;&#23450;&#34892;&#20026;&#30446;&#26631;&#30340;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Will My Robot Achieve My Goals? Predicting the Probability that an MDP Policy Reaches a User-Specified Behavior Target
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.16462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#39044;&#27979;MDP&#31574;&#30053;&#36798;&#21040;&#29992;&#25143;&#25351;&#23450;&#34892;&#20026;&#30446;&#26631;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#31526;&#21512;&#39044;&#27979;&#36827;&#34892;&#21453;&#36716;&#26469;&#35745;&#31639;&#27010;&#29575;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#33258;&#20027;&#31995;&#32479;&#25191;&#34892;&#20219;&#21153;&#26102;&#65292;&#24212;&#20445;&#25345;&#23545;&#23454;&#29616;&#29992;&#25143;&#30446;&#26631;&#27010;&#29575;&#30340;&#26657;&#20934;&#20272;&#35745;&#12290;&#22914;&#26524;&#35813;&#27010;&#29575;&#20302;&#20110;&#26576;&#20010;&#26399;&#26395;&#27700;&#24179;&#65292;&#24212;&#21521;&#29992;&#25143;&#21457;&#20986;&#35686;&#25253;&#65292;&#20197;&#20415;&#37319;&#21462;&#36866;&#24403;&#24178;&#39044;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#29992;&#25143;&#23558;&#30446;&#26631;&#35268;&#23450;&#20026;&#23454;&#20540;&#24615;&#33021;&#25688;&#35201;&#30340;&#30446;&#26631;&#21306;&#38388;&#30340;&#35774;&#32622;&#65292;&#20363;&#22914;&#22312;&#22266;&#23450;&#26102;&#38388;&#27573;$H$&#20869;&#27979;&#37327;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;$t \in \{0, \ldots, H-1\}$&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20250;&#20135;&#29983;&#19968;&#20010;&#26657;&#20934;&#27010;&#29575;&#20272;&#35745;&#65292;&#21363;&#26368;&#32456;&#32047;&#31215;&#22870;&#21169;&#33853;&#22312;&#29992;&#25143;&#25351;&#23450;&#30446;&#26631;&#21306;&#38388;$[y^-,y^+]$&#20869;&#30340;&#27010;&#29575;&#12290;&#21033;&#29992;&#36825;&#19968;&#20272;&#35745;&#65292;&#33258;&#20027;&#31995;&#32479;&#21487;&#20197;&#22312;&#27010;&#29575;&#20302;&#20110;&#25351;&#23450;&#38408;&#20540;&#26102;&#21457;&#20986;&#35686;&#25253;&#12290;&#25105;&#20204;&#36890;&#36807;&#21453;&#36716;&#31526;&#21512;&#39044;&#27979;&#26469;&#35745;&#31639;&#27010;&#29575;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#20986;&#21457;&#28857;&#26159;Romano&#31561;&#20154;&#30340;Quantile Regression (CQR)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24212;&#29992;&#20102;spli
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.16462v2 Announce Type: replace  Abstract: As an autonomous system performs a task, it should maintain a calibrated estimate of the probability that it will achieve the user's goal. If that probability falls below some desired level, it should alert the user so that appropriate interventions can be made. This paper considers settings where the user's goal is specified as a target interval for a real-valued performance summary, such as the cumulative reward, measured at a fixed horizon $H$. At each time $t \in \{0, \ldots, H-1\}$, our method produces a calibrated estimate of the probability that the final cumulative reward will fall within a user-specified target interval $[y^-,y^+].$ Using this estimate, the autonomous system can raise an alarm if the probability drops below a specified threshold. We compute the probability estimates by inverting conformal prediction. Our starting point is the Conformalized Quantile Regression (CQR) method of Romano et al., which applies spli
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#26041;&#27861;&#65292;GIT&#65292;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#33021;&#22815;&#36890;&#36807;&#20449;&#21495;&#26799;&#24230;&#20272;&#35745;&#22120;&#38477;&#20302;&#24178;&#39044;&#27425;&#25968;&#65292;&#22312;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#12290;</title><link>https://arxiv.org/abs/2211.13715</link><description>&lt;p&gt;
&#30456;&#20449;&#24744;&#30340; $\nabla$: &#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#29992;&#20110;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.13715
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#26041;&#27861;&#65292;GIT&#65292;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#33021;&#22815;&#36890;&#36807;&#20449;&#21495;&#26799;&#24230;&#20272;&#35745;&#22120;&#38477;&#20302;&#24178;&#39044;&#27425;&#25968;&#65292;&#22312;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#26159;&#31185;&#23398;&#20013;&#19968;&#39033;&#20855;&#26377;&#22522;&#30784;&#37325;&#35201;&#24615;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#35266;&#27979;&#25968;&#25454;&#36890;&#24120;&#19981;&#36275;&#20197;&#21807;&#19968;&#30830;&#23450;&#31995;&#32479;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#34429;&#28982;&#36827;&#34892;&#24178;&#39044;&#65288;&#21363;&#23454;&#39564;&#65289;&#21487;&#20197;&#25913;&#21892;&#21487;&#35782;&#21035;&#24615;&#65292;&#20294;&#36825;&#20123;&#26679;&#26412;&#36890;&#24120;&#38590;&#20197;&#33719;&#24471;&#19988;&#25104;&#26412;&#39640;&#26114;&#12290;&#22240;&#27492;&#65292;&#22240;&#26524;&#21457;&#29616;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20272;&#35745;&#26368;&#20855;&#20449;&#24687;&#24615;&#30340;&#24178;&#39044;&#30446;&#26631;&#26469;&#26368;&#23567;&#21270;&#24178;&#39044;&#27425;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24178;&#39044;&#30446;&#26631;&#23450;&#20301;&#26041;&#27861;&#65292;&#31616;&#31216;&#20026;GIT&#65292;&#23427;&#8216;&#30456;&#20449;&#8217;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#20197;&#25552;&#20379;&#24178;&#39044;&#37319;&#38598;&#20989;&#25968;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;GIT&#22312;&#20302;&#25968;&#25454;&#37327;&#24773;&#20917;&#19979;&#34920;&#29616;&#19982;&#31454;&#20105;&#22522;&#32447;&#30456;&#24403;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.13715v4 Announce Type: replace-cross  Abstract: Inferring causal structure from data is a challenging task of fundamental importance in science. Observational data are often insufficient to identify a system's causal structure uniquely. While conducting interventions (i.e., experiments) can improve the identifiability, such samples are usually challenging and expensive to obtain. Hence, experimental design approaches for causal discovery aim to minimize the number of interventions by estimating the most informative intervention target. In this work, we propose a novel Gradient-based Intervention Targeting method, abbreviated GIT, that 'trusts' the gradient estimator of a gradient-based causal discovery framework to provide signals for the intervention acquisition function. We provide extensive experiments in simulated and real-world datasets and demonstrate that GIT performs on par with competitive baselines, surpassing them in the low-data regime.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#22810;&#20998;&#24067;&#23398;&#20064;&#33539;&#24335;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#31526;&#21512;&#27492;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#25913;&#36827;&#20102;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#21644;&#21327;&#20316;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2210.12529</link><description>&lt;p&gt;
&#38656;&#27714;&#25277;&#26679;&#65306;&#20174;&#22810;&#20010;&#20998;&#24067;&#20013;&#23398;&#20064;&#26368;&#20248;
&lt;/p&gt;
&lt;p&gt;
On-Demand Sampling: Learning Optimally from Multiple Distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.12529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#22810;&#20998;&#24067;&#23398;&#20064;&#33539;&#24335;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#31526;&#21512;&#27492;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#25913;&#36827;&#20102;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#21644;&#21327;&#20316;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#21644;&#29616;&#23454;&#32771;&#34385;&#65292;&#22914;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31038;&#20250;&#31119;&#21033;&#21644;&#22810;&#26234;&#33021;&#20307;&#26435;&#34913;&#24050;&#32463;&#20652;&#29983;&#20102;&#22810;&#20998;&#24067;&#23398;&#20064;&#33539;&#24335;&#65292;&#22914;&#21327;&#20316;&#23398;&#20064;&#12289;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#21644;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#23398;&#20064;&#32773;&#23547;&#27714;&#22312;$n$&#20010;&#39044;&#23450;&#20041;&#25968;&#25454;&#20998;&#24067;&#19978;&#22343;&#21248;&#26368;&#23567;&#21270;&#20854;&#26399;&#26395;&#25439;&#22833;&#65292;&#21516;&#26102;&#23613;&#21487;&#33021;&#23569;&#22320;&#20351;&#29992;&#26679;&#26412;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#36825;&#20123;&#23398;&#20064;&#33539;&#24335;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#31526;&#21512;&#27492;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#22810;&#20998;&#24067;&#23398;&#20064;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#20165;&#36229;&#36807;&#20102;&#21333;&#19968;&#20998;&#24067;&#23398;&#20064;&#30340;&#28155;&#21152;&#22240;&#23376;$n \log(n) / \epsilon^2$&#12290;&#36825;&#25913;&#36827;&#20102;Mohri&#31561;&#20154;&#25552;&#20986;&#30340;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#21644;Nguyen&#21644;Zakynthinou&#25552;&#20986;&#30340;&#21327;&#20316;&#23398;&#20064;&#30340;&#24050;&#30693;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#65292;&#20998;&#21035;&#20026;$n$&#21644;$\lo&#30340;&#20056;&#27861;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.12529v3 Announce Type: replace  Abstract: Social and real-world considerations such as robustness, fairness, social welfare and multi-agent tradeoffs have given rise to multi-distribution learning paradigms, such as collaborative learning, group distributionally robust optimization, and fair federated learning. In each of these settings, a learner seeks to uniformly minimize its expected loss over $n$ predefined data distributions, while using as few samples as possible. In this paper, we establish the optimal sample complexity of these learning paradigms and give algorithms that meet this sample complexity. Importantly, our sample complexity bounds for multi-distribution learning exceed that of learning a single distribution by only an additive factor of $n \log(n) / \epsilon^2$. This improves upon the best known sample complexity bounds for fair federated learning by Mohri et al. and collaborative learning by Nguyen and Zakynthinou by multiplicative factors of $n$ and $\lo
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20811;&#26381;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;FL&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2210.01708</link><description>&lt;p&gt;
&#20811;&#26381;&#36890;&#20449;&#32422;&#26463;&#65292;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#20013;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conquering the Communication Constraints to Enable Large Pre-Trained Models in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.01708
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20811;&#26381;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;FL&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26088;&#22312;&#22312;&#26412;&#22320;&#35774;&#22791;&#19978;&#21327;&#21147;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#38656;&#35201;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#20013;&#24515;&#21270;&#35775;&#38382;&#30340;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#12290;&#22312;&#20856;&#22411;&#30340;FL&#33539;&#24335;&#65288;&#20363;&#22914;FedAvg&#65289;&#20013;&#65292;&#27599;&#19968;&#36718;&#27169;&#22411;&#26435;&#37325;&#37117;&#20250;&#34987;&#21457;&#36865;&#21040;&#21442;&#19982;&#23458;&#25143;&#31471;&#24182;&#22238;&#20256;&#21040;&#26381;&#21153;&#22120;&#12290;&#26368;&#36817;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20248;&#21270;&#21644;&#25910;&#25947;&#25913;&#36827;&#26041;&#38754;&#23637;&#31034;&#20102;&#20351;&#29992;&#23567;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#65292;&#20294;&#20063;&#25317;&#26377;&#26356;&#22810;&#21442;&#25968;&#12290;&#22312;&#20256;&#32479;&#30340;FL&#20013;&#65292;&#20849;&#20139;&#24040;&#22823;&#30340;&#27169;&#22411;&#26435;&#37325;&#21487;&#20197;&#36805;&#36895;&#32473;&#31995;&#32479;&#24102;&#26469;&#24040;&#22823;&#30340;&#36890;&#20449;&#36127;&#25285;&#65292;&#23588;&#20854;&#26159;&#22914;&#26524;&#37319;&#29992;&#26356;&#21152;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;FL&#20013;&#21551;&#29992;&#36825;&#20123;&#24378;&#22823;&#19988;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.01708v3 Announce Type: replace  Abstract: Federated learning (FL) has emerged as a promising paradigm for enabling the collaborative training of models without centralized access to the raw data on local devices. In the typical FL paradigm (e.g., FedAvg), model weights are sent to and from the server each round to participating clients. Recently, the use of small pre-trained models has been shown effective in federated learning optimization and improving convergence. However, recent state-of-the-art pre-trained models are getting more capable but also have more parameters. In conventional FL, sharing the enormous model weights can quickly put a massive communication burden on the system, especially if more capable models are employed. Can we find a solution to enable those strong and readily-available pre-trained models in FL to achieve excellent performance while simultaneously reducing the communication burden? To this end, we investigate the use of parameter-efficient fin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21508;&#21521;&#24322;&#24615;&#25193;&#25955;&#26144;&#23556;&#30340;&#37325;&#26032;&#21152;&#26435;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#22686;&#24378;&#37319;&#26679;&#27169;&#25311;&#20013;&#23398;&#20064;&#38598;&#20307;&#21464;&#37327;&#27969;&#24418;&#26102;&#36935;&#21040;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2207.14554</link><description>&lt;p&gt;
&#20174;&#22686;&#24378;&#37319;&#26679;&#27169;&#25311;&#20013;&#37325;&#26032;&#21152;&#26435;&#30340;&#38598;&#20307;&#21464;&#37327;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reweighted Manifold Learning of Collective Variables from Enhanced Sampling Simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.14554
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21508;&#21521;&#24322;&#24615;&#25193;&#25955;&#26144;&#23556;&#30340;&#37325;&#26032;&#21152;&#26435;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#22686;&#24378;&#37319;&#26679;&#27169;&#25311;&#20013;&#23398;&#20064;&#38598;&#20307;&#21464;&#37327;&#27969;&#24418;&#26102;&#36935;&#21040;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#37319;&#26679;&#26041;&#27861;&#22312;&#35745;&#31639;&#29289;&#29702;&#23398;&#21644;&#21270;&#23398;&#20013;&#19981;&#21487;&#25110;&#32570;&#65292;&#21407;&#23376;&#27169;&#25311;&#26080;&#27861;&#31351;&#23613;&#21160;&#21147;&#31995;&#32479;&#30340;&#39640;&#32500;&#26500;&#22411;&#31354;&#38388;&#65292;&#22240;&#20026;&#37319;&#26679;&#38382;&#39064;&#12290;&#36825;&#31867;&#22686;&#24378;&#37319;&#26679;&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#20960;&#20010;&#24930;&#24230;&#33258;&#30001;&#24230;&#65288;&#31216;&#20026;&#38598;&#20307;&#21464;&#37327;CVs&#65289;&#24182;&#27839;&#30528;&#36825;&#20123;CVs&#22686;&#24378;&#37319;&#26679;&#26469;&#24037;&#20316;&#12290;&#36873;&#25321;&#35201;&#20998;&#26512;&#21644;&#39537;&#21160;&#37319;&#26679;&#30340;CVs&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#29289;&#29702;&#21644;&#21270;&#23398;&#30452;&#35273;&#12290;&#23613;&#31649;&#36890;&#24120;&#36890;&#36807;&#27969;&#24418;&#23398;&#20064;&#20174;&#26631;&#20934;&#27169;&#25311;&#20013;&#30452;&#25509;&#20272;&#35745;CVs&#26469;&#32469;&#36807;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#20174;&#22686;&#24378;&#37319;&#26679;&#27169;&#25311;&#20013;&#25552;&#20379;&#21040;&#20302;&#32500;&#27969;&#24418;&#30340;&#26144;&#23556;&#12290;&#22240;&#20026;&#23398;&#20064;&#30340;&#27969;&#24418;&#30340;&#20960;&#20309;&#21644;&#23494;&#24230;&#26159;&#26377;&#20559;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#21508;&#21521;&#24322;&#24615;&#25193;&#25955;&#26144;&#23556;&#30340;&#27969;&#24418;&#23398;&#20064;&#30340;&#36890;&#29992;&#37325;&#26032;&#21152;&#26435;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.14554v2 Announce Type: replace-cross  Abstract: Enhanced sampling methods are indispensable in computational physics and chemistry, where atomistic simulations cannot exhaustively sample the high-dimensional configuration space of dynamical systems due to the sampling problem. A class of such enhanced sampling methods works by identifying a few slow degrees of freedom, termed collective variables (CVs), and enhancing the sampling along these CVs. Selecting CVs to analyze and drive the sampling is not trivial and often relies on physical and chemical intuition. Despite routinely circumventing this issue using manifold learning to estimate CVs directly from standard simulations, such methods cannot provide mappings to a low-dimensional manifold from enhanced sampling simulations as the geometry and density of the learned manifold are biased. Here, we address this crucial issue and provide a general reweighting framework based on anisotropic diffusion maps for manifold learning
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MA-Trace&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#20855;&#26377;&#39640;&#21487;&#20280;&#32553;&#24615;&#65292;&#24182;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#20316;&#20026;&#31163;&#31574;&#30053;&#20462;&#27491;&#26041;&#27861;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#20998;&#24067;&#30340;&#36136;&#37327;&#21644;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>https://arxiv.org/abs/2111.11229</link><description>&lt;p&gt;
&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#31574;&#30053;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Correction For Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.11229
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MA-Trace&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#20855;&#26377;&#39640;&#21487;&#20280;&#32553;&#24615;&#65292;&#24182;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#20316;&#20026;&#31163;&#31574;&#30053;&#20462;&#27491;&#26041;&#27861;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#20998;&#24067;&#30340;&#36136;&#37327;&#21644;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20026;&#28041;&#21450;&#22810;&#20010;&#30456;&#20114;&#20316;&#29992;&#26234;&#20307;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#23613;&#31649;&#34920;&#38754;&#19978;&#19982;&#21333;&#26234;&#20307;&#24773;&#20917;&#30456;&#20284;&#65292;&#20294;&#22810;&#26234;&#20307;&#38382;&#39064;&#24448;&#24448;&#26356;&#38590;&#35757;&#32451;&#21644;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MA-Trace&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31574;&#30053;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#23558;V-Trace&#25193;&#23637;&#21040;MARL&#35774;&#23450;&#20013;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#20854;&#22312;&#22810;&#24037;&#20316;&#22120;&#35774;&#32622;&#20013;&#30340;&#39640;&#21487;&#20280;&#32553;&#24615;&#12290;&#20026;&#27492;&#65292;MA-Trace&#21033;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#20316;&#20026;&#31163;&#31574;&#30053;&#20462;&#27491;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#19981;&#24433;&#21709;&#35757;&#32451;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35745;&#31639;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#22312;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#30340; - &#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#20445;&#35777;&#25910;&#25947;&#30340;&#19981;&#21160;&#28857;&#23450;&#29702;&#12290;&#25105;&#20204;&#22312;StarCraft&#22810;&#26234;&#20307;&#25361;&#25112;&#36187;&#19978;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#36825;&#26159;&#22810;&#26234;&#20307;&#31639;&#27861;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;MA-Trace&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.11229v3 Announce Type: replace-cross  Abstract: Multi-agent reinforcement learning (MARL) provides a framework for problems involving multiple interacting agents. Despite apparent similarity to the single-agent case, multi-agent problems are often harder to train and analyze theoretically. In this work, we propose MA-Trace, a new on-policy actor-critic algorithm, which extends V-Trace to the MARL setting. The key advantage of our algorithm is its high scalability in a multi-worker setting. To this end, MA-Trace utilizes importance sampling as an off-policy correction method, which allows distributing the computations with no impact on the quality of training. Furthermore, our algorithm is theoretically grounded - we prove a fixed-point theorem that guarantees convergence. We evaluate the algorithm extensively on the StarCraft Multi-Agent Challenge, a standard benchmark for multi-agent algorithms. MA-Trace achieves high performance on all its tasks and exceeds state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#20581;&#22766;&#30340;&#36755;&#20986;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#65292;&#24403;&#21442;&#25968;&#21270;&#20026;&#32447;&#24615;&#26102;&#65292;&#20248;&#21270;&#38382;&#39064;&#26159;&#20984;&#30340;&#12290;</title><link>https://arxiv.org/abs/2111.09971</link><description>&lt;p&gt;
&#20174;&#23433;&#20840;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#20581;&#22766;&#30340;&#36755;&#20986;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning Robust Output Control Barrier Functions from Safe Expert Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.09971
&lt;/p&gt;
&lt;p&gt;
&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#20581;&#22766;&#30340;&#36755;&#20986;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#65292;&#24403;&#21442;&#25968;&#21270;&#20026;&#32447;&#24615;&#26102;&#65292;&#20248;&#21270;&#38382;&#39064;&#26159;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#20174;&#19987;&#23478;&#28436;&#31034;&#30340;&#37096;&#20998;&#35266;&#27979;&#20013;&#23398;&#20064;&#23433;&#20840;&#36755;&#20986;&#21453;&#39304;&#25511;&#21046;&#24459;&#30340;&#38382;&#39064;&#12290;&#20551;&#35774;&#31995;&#32479;&#21160;&#24577;&#27169;&#22411;&#21644;&#29366;&#24577;&#20272;&#35745;&#22120;&#21487;&#29992;&#65292;&#24182;&#24102;&#26377;&#30456;&#24212;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#20363;&#22914;&#65292;&#21487;&#20197;&#20174;&#23454;&#36341;&#20013;&#30340;&#25968;&#25454;&#20272;&#35745;&#24471;&#20986;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#20581;&#22766;&#30340;&#36755;&#20986;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;ROCBFs&#65289;&#20316;&#20026;&#30830;&#20445;&#23433;&#20840;&#30340;&#25163;&#27573;&#65292;&#36890;&#36807;&#25511;&#21046;&#23433;&#20840;&#38598;&#30340;&#25511;&#21046;&#21069;&#19981;&#21464;&#24615;&#26469;&#23450;&#20041;&#23433;&#20840;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#34920;&#29616;&#20986;&#23433;&#20840;&#31995;&#32479;&#34892;&#20026;&#30340;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;ROCBFs&#65292;&#20363;&#22914;&#20174;&#20154;&#25805;&#32437;&#21592;&#25110;&#19987;&#23478;&#25511;&#21046;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#24403;ROCBF&#30340;&#21442;&#25968;&#21270;&#26159;&#32447;&#24615;&#26102;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#20248;&#21270;&#38382;&#39064;&#26159;&#20984;&#30340;&#12290;&#38500;&#20102;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#21487;&#39564;&#35777;&#30340;&#26465;&#20214;&#65292;&#28041;&#21450;&#25968;&#25454;&#30340;&#23494;&#24230;&#12289;&#31995;&#32479;&#27169;&#22411;&#21644;&#29366;&#24577;&#20272;&#35745;&#22120;&#30340;&#24179;&#28369;&#24615;&#65292;&#20197;&#21450;&#35823;&#24046;&#36793;&#30028;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.09971v3 Announce Type: replace-cross  Abstract: This paper addresses learning safe output feedback control laws from partial observations of expert demonstrations. We assume that a model of the system dynamics and a state estimator are available along with corresponding error bounds, e.g., estimated from data in practice. We first propose robust output control barrier functions (ROCBFs) as a means to guarantee safety, as defined through controlled forward invariance of a safe set. We then formulate an optimization problem to learn ROCBFs from expert demonstrations that exhibit safe system behavior, e.g., data collected from a human operator or an expert controller. When the parametrization of the ROCBF is linear, then we show that, under mild assumptions, the optimization problem is convex. Along with the optimization problem, we provide verifiable conditions in terms of the density of the data, smoothness of the system model and state estimator, and the size of the error bo
&lt;/p&gt;</description></item><item><title>&#22122;&#22768;&#21644;&#24402;&#32435;&#20559;&#35265;&#30340;&#20316;&#29992;&#20351;&#24471;&#32452;&#21512;&#24335;&#27807;&#36890;&#33258;&#21457;&#20135;&#29983;&#65292;&#24182;&#19988;&#19968;&#23450;&#33539;&#22260;&#20869;&#30340;&#22122;&#22768;&#26377;&#21161;&#20110;&#20419;&#36827;&#32452;&#21512;&#24615;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2111.06464</link><description>&lt;p&gt;
&#22122;&#22768;&#22312;&#32452;&#21512;&#24335;&#27807;&#36890;&#20013;&#30340;&#20652;&#21270;&#20316;&#29992;&#21644;&#24402;&#32435;&#20559;&#35265;&#30340;&#24517;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Catalytic Role Of Noise And Necessity Of Inductive Biases In The Emergence Of Compositional Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.06464
&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#21644;&#24402;&#32435;&#20559;&#35265;&#30340;&#20316;&#29992;&#20351;&#24471;&#32452;&#21512;&#24335;&#27807;&#36890;&#33258;&#21457;&#20135;&#29983;&#65292;&#24182;&#19988;&#19968;&#23450;&#33539;&#22260;&#20869;&#30340;&#22122;&#22768;&#26377;&#21161;&#20110;&#20419;&#36827;&#32452;&#21512;&#24615;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27807;&#36890;&#26159;&#32452;&#21512;&#24335;&#30340;&#65292;&#22914;&#26524;&#22797;&#26434;&#20449;&#21495;&#21487;&#20197;&#34920;&#31034;&#20026;&#36739;&#31616;&#21333;&#23376;&#37096;&#20998;&#30340;&#32452;&#21512;&#12290;&#26412;&#25991;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#35757;&#32451;&#26694;&#26550;&#21644;&#25968;&#25454;&#19978;&#30340;&#24402;&#32435;&#20559;&#35265;&#23545;&#20110;&#21457;&#23637;&#32452;&#21512;&#24335;&#27807;&#36890;&#26159;&#24517;&#35201;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#32452;&#21512;&#24615;&#20250;&#22312;&#20449;&#21495;&#21338;&#24328;&#20013;&#33258;&#21457;&#20986;&#29616;&#65292;&#20195;&#29702;&#22312;&#22024;&#26434;&#36890;&#36947;&#19978;&#20256;&#36755;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#19968;&#31995;&#21015;&#22122;&#22768;&#27700;&#24179;&#65288;&#21462;&#20915;&#20110;&#27169;&#22411;&#21644;&#25968;&#25454;&#65289;&#30830;&#23454;&#20419;&#36827;&#20102;&#32452;&#21512;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#24182;&#25253;&#21578;&#20102;&#26368;&#36817;&#30740;&#31350;&#30340;&#32452;&#21512;&#24615;&#24230;&#37327;&#32467;&#26524;&#65306;&#25299;&#25169;&#30456;&#20284;&#24615;&#12289;&#20914;&#31361;&#35745;&#25968;&#21644;&#19978;&#19979;&#25991;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.06464v2 Announce Type: replace-cross  Abstract: Communication is compositional if complex signals can be represented as a combination of simpler subparts. In this paper, we theoretically show that inductive biases on both the training framework and the data are needed to develop a compositional communication. Moreover, we prove that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. We experimentally confirm that a range of noise levels, which depends on the model and the data, indeed promotes compositionality. Finally, we provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23376;&#30446;&#26631;&#25628;&#32034;&#65288;kSubS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#23376;&#30446;&#26631;&#29983;&#25104;&#22120;&#20135;&#29983;&#22810;&#26679;&#24615;&#30340;&#23376;&#30446;&#26631;&#65292;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#24182;&#22312;Sokoban&#12289;&#39764;&#26041;&#21644;&#19981;&#31561;&#24335;&#35777;&#26126;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2108.11204</link><description>&lt;p&gt;
&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Subgoal Search For Complex Reasoning Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2108.11204
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23376;&#30446;&#26631;&#25628;&#32034;&#65288;kSubS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#23376;&#30446;&#26631;&#29983;&#25104;&#22120;&#20135;&#29983;&#22810;&#26679;&#24615;&#30340;&#23376;&#30446;&#26631;&#65292;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#24182;&#22312;Sokoban&#12289;&#39764;&#26041;&#21644;&#19981;&#31561;&#24335;&#35777;&#26126;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25797;&#38271;&#36890;&#36807;&#20174;&#19968;&#20010;&#24819;&#27861;&#31227;&#21160;&#21040;&#30456;&#20851;&#30340;&#24819;&#27861;&#30340;&#24605;&#32500;&#36807;&#31243;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23376;&#30446;&#26631;&#25628;&#32034;&#65288;kSubS&#65289;&#26041;&#27861;&#12290;&#20854;&#20851;&#38190;&#32452;&#20214;&#26159;&#19968;&#20010;&#23398;&#20064;&#30340;&#23376;&#30446;&#26631;&#29983;&#25104;&#22120;&#65292;&#20135;&#29983;&#22810;&#26679;&#24615;&#30340;&#26082;&#21487;&#23454;&#29616;&#21448;&#25509;&#36817;&#35299;&#20915;&#26041;&#26696;&#30340;&#23376;&#30446;&#26631;&#12290;&#20351;&#29992;&#23376;&#30446;&#26631;&#21487;&#20197;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#24341;&#20837;&#36866;&#21512;&#39640;&#25928;&#35268;&#21010;&#30340;&#39640;&#32423;&#25628;&#32034;&#22270;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#23376;&#30446;&#26631;&#27169;&#22359;&#32467;&#21512;&#32463;&#20856;&#30340;&#26368;&#20339;&#20248;&#20808;&#25628;&#32034;&#26694;&#26550;&#26469;&#23454;&#29616;kSubS&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#29983;&#25104;&#31532;$k$&#27493;&#23376;&#30446;&#26631;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#25928;&#29575;&#65306;&#20004;&#20010;&#27969;&#34892;&#30340;&#30410;&#26234;&#28216;&#25103;Sokoban&#21644;&#39764;&#26041;&#20197;&#21450;&#19981;&#31561;&#24335;&#35777;&#26126;&#22522;&#20934;INT&#12290;kSubS&#22312;&#36866;&#24230;&#30340;&#35745;&#31639;&#39044;&#31639;&#20869;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#22312;INT&#19978;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2108.11204v3 Announce Type: replace  Abstract: Humans excel in solving complex reasoning tasks through a mental process of moving from one idea to a related one. Inspired by this, we propose Subgoal Search (kSubS) method. Its key component is a learned subgoal generator that produces a diversity of subgoals that are both achievable and closer to the solution. Using subgoals reduces the search space and induces a high-level search graph suitable for efficient planning. In this paper, we implement kSubS using a transformer-based subgoal module coupled with the classical best-first search framework. We show that a simple approach of generating $k$-th step ahead subgoals is surprisingly efficient on three challenging domains: two popular puzzle games, Sokoban and the Rubik's Cube, and an inequality proving benchmark INT. kSubS achieves strong results including state-of-the-art on INT within a modest computational budget.
&lt;/p&gt;</description></item><item><title>MCL-GAN&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#37492;&#21035;&#22120;&#20849;&#21516;&#21512;&#20316;&#26469;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#22312;&#35299;&#20915;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#22120;&#19982;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#65292;&#36890;&#36807;&#32473;&#27599;&#20010;&#37492;&#21035;&#22120;&#19987;&#19994;&#25216;&#33021;&#30340;&#24341;&#23548;&#65292;&#29983;&#25104;&#22120;&#33021;&#22815;&#33258;&#21160;&#25214;&#21040;&#28508;&#22312;&#25968;&#25454;&#31354;&#38388;&#21644;&#30495;&#23454;&#25968;&#25454;&#31354;&#38388;&#30340;&#21512;&#29702;&#23545;&#24212;&#20851;&#31995;&#65292;&#21516;&#26102;&#39592;&#24178;&#32593;&#32476;&#22312;&#37492;&#21035;&#22120;&#20043;&#38388;&#26159;&#20849;&#20139;&#30340;&#65292;&#35757;&#32451;&#25104;&#26412;&#22686;&#21152;&#24494;&#19981;&#36275;&#36947;&#65292;&#24182;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2107.07260</link><description>&lt;p&gt;
MCL-GAN: &#20855;&#26377;&#22810;&#20010;&#19987;&#19994;&#37492;&#21035;&#22120;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MCL-GAN: Generative Adversarial Networks with Multiple Specialized Discriminators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2107.07260
&lt;/p&gt;
&lt;p&gt;
MCL-GAN&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#37492;&#21035;&#22120;&#20849;&#21516;&#21512;&#20316;&#26469;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#22312;&#35299;&#20915;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#22120;&#19982;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#65292;&#36890;&#36807;&#32473;&#27599;&#20010;&#37492;&#21035;&#22120;&#19987;&#19994;&#25216;&#33021;&#30340;&#24341;&#23548;&#65292;&#29983;&#25104;&#22120;&#33021;&#22815;&#33258;&#21160;&#25214;&#21040;&#28508;&#22312;&#25968;&#25454;&#31354;&#38388;&#21644;&#30495;&#23454;&#25968;&#25454;&#31354;&#38388;&#30340;&#21512;&#29702;&#23545;&#24212;&#20851;&#31995;&#65292;&#21516;&#26102;&#39592;&#24178;&#32593;&#32476;&#22312;&#37492;&#21035;&#22120;&#20043;&#38388;&#26159;&#20849;&#20139;&#30340;&#65292;&#35757;&#32451;&#25104;&#26412;&#22686;&#21152;&#24494;&#19981;&#36275;&#36947;&#65292;&#24182;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22810;&#20010;&#37492;&#21035;&#22120;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26694;&#26550;&#65292;&#36825;&#20123;&#37492;&#21035;&#22120;&#20849;&#21516;&#21512;&#20316;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#23398;&#20064;&#19968;&#20010;&#19982;&#22522;&#20110;&#30495;&#23454;&#22270;&#20687;&#30340;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#19968;&#33268;&#30340;&#29983;&#25104;&#22120;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#38271;&#26399;&#30340;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;&#21463;&#21040;&#22810;&#36873;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#23548;&#27599;&#20010;&#37492;&#21035;&#22120;&#22312;&#25972;&#20010;&#25968;&#25454;&#30340;&#23376;&#38598;&#20013;&#20855;&#26377;&#19987;&#38376;&#25216;&#33021;&#65292;&#24182;&#20801;&#35768;&#29983;&#25104;&#22120;&#22312;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#35757;&#32451;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#25214;&#21040;&#28508;&#22312;&#25968;&#25454;&#31354;&#38388;&#21644;&#30495;&#23454;&#25968;&#25454;&#31354;&#38388;&#20043;&#38388;&#30340;&#21512;&#29702;&#23545;&#24212;&#20851;&#31995;&#12290;&#23613;&#31649;&#20351;&#29992;&#20102;&#22810;&#20010;&#37492;&#21035;&#22120;&#65292;&#39592;&#24178;&#32593;&#32476;&#22312;&#37492;&#21035;&#22120;&#20043;&#38388;&#26159;&#20849;&#20139;&#30340;&#65292;&#24182;&#19988;&#35757;&#32451;&#25104;&#26412;&#30340;&#22686;&#21152;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#23637;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36866;&#29992;&#20110;&#22810;&#26679;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2107.07260v3 Announce Type: replace  Abstract: We propose a framework of generative adversarial networks with multiple discriminators, which collaborate to represent a real dataset more effectively. Our approach facilitates learning a generator consistent with the underlying data distribution based on real images and thus mitigates the chronic mode collapse problem. From the inspiration of multiple choice learning, we guide each discriminator to have expertise in a subset of the entire data and allow the generator to find reasonable correspondences between the latent and real data spaces automatically without extra supervision for training examples. Despite the use of multiple discriminators, the backbone networks are shared across the discriminators and the increase in training cost is marginal. We demonstrate the effectiveness of our algorithm using multiple evaluation metrics in the standard datasets for diverse tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#27169;&#25311;&#24378;&#21270;&#23398;&#20064;&#21644;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#25511;&#21046;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#27169;&#25311;&#21040;&#30495;&#23454;&#31574;&#30053;&#36716;&#31227;&#65292;&#24182;&#20998;&#26512;&#20102;&#35774;&#35745;&#20915;&#31574;&#23545;&#30495;&#23454;&#19990;&#30028;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/1911.12905</link><description>&lt;p&gt;
&#27169;&#25311;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Simulation-based reinforcement learning for real-world autonomous driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1911.12905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#27169;&#25311;&#24378;&#21270;&#23398;&#20064;&#21644;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#25511;&#21046;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#27169;&#25311;&#21040;&#30495;&#23454;&#31574;&#30053;&#36716;&#31227;&#65292;&#24182;&#20998;&#26512;&#20102;&#35774;&#35745;&#20915;&#31574;&#23545;&#30495;&#23454;&#19990;&#30028;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#27169;&#25311;&#24378;&#21270;&#23398;&#20064;&#26469;&#33719;&#24471;&#25511;&#21046;&#20840;&#23610;&#23544;&#30495;&#23454;&#19990;&#30028;&#36710;&#36742;&#30340;&#39550;&#39542;&#31995;&#32479;&#12290;&#39550;&#39542;&#31574;&#30053;&#20197;&#26469;&#33258;&#21333;&#20010;&#25668;&#20687;&#22836;&#30340;RGB&#22270;&#20687;&#21450;&#20854;&#35821;&#20041;&#20998;&#21106;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#20027;&#35201;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#21482;&#26377;&#22312;&#20998;&#21106;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#25165;&#20986;&#29616;&#26631;&#35760;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30830;&#35748;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#31574;&#30053;&#36716;&#31227;&#12290;&#22522;&#20110;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20851;&#20110;&#24863;&#30693;&#12289;&#25511;&#21046;&#21644;&#35757;&#32451;&#30340;&#35774;&#35745;&#20915;&#31574;&#22914;&#20309;&#24433;&#21709;&#30495;&#23454;&#19990;&#30028;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1911.12905v4 Announce Type: replace-cross  Abstract: We use reinforcement learning in simulation to obtain a driving system controlling a full-size real-world vehicle. The driving policy takes RGB images from a single camera and their semantic segmentation as input. We use mostly synthetic data, with labelled real-world data appearing only in the training of the segmentation network.   Using reinforcement learning in simulation and synthetic data is motivated by lowering costs and engineering effort.   In real-world experiments we confirm that we achieved successful sim-to-real policy transfer. Based on the extensive evaluation, we analyze how design decisions about perception, control, and training impact the real-world performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#21160;&#37327;&#21387;&#32553;&#65288;GMC&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31232;&#30095;&#36890;&#20449;&#65292;&#19982;&#29616;&#26377;&#30340;&#23616;&#37096;&#21160;&#37327;&#26041;&#27861;&#19981;&#21516;&#65292;GMC&#21033;&#29992;&#20840;&#23616;&#21160;&#37327;&#26469;&#25552;&#39640;&#20998;&#24067;&#24335;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/1905.12948</link><description>&lt;p&gt;
&#20840;&#23616;&#21160;&#37327;&#21387;&#32553;&#29992;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Global Momentum Compression for Sparse Communication in Distributed Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1905.12948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#21160;&#37327;&#21387;&#32553;&#65288;GMC&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31232;&#30095;&#36890;&#20449;&#65292;&#19982;&#29616;&#26377;&#30340;&#23616;&#37096;&#21160;&#37327;&#26041;&#27861;&#19981;&#21516;&#65292;GMC&#21033;&#29992;&#20840;&#23616;&#21160;&#37327;&#26469;&#25552;&#39640;&#20998;&#24067;&#24335;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#20998;&#24067;&#24335;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DMSGD&#65289;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#35757;&#32451;&#22823;&#35268;&#27169;&#28145;&#24230;&#27169;&#22411;&#12290;&#30001;&#20110;&#32593;&#32476;&#30340;&#24310;&#36831;&#21644;&#24102;&#23485;&#26377;&#38480;&#65292;&#36890;&#20449;&#25104;&#20026;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#29942;&#39048;&#12290;&#20351;&#29992;&#31232;&#30095;&#26799;&#24230;&#36827;&#34892;&#36890;&#20449;&#21387;&#32553;&#65292;&#31616;&#31216;&#20026;&#8220;&#31232;&#30095;&#36890;&#20449;&#8221;&#65292;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20197;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;&#25152;&#26377;&#20851;&#20110;DMSGD&#20013;&#31232;&#30095;&#36890;&#20449;&#30340;&#29616;&#26377;&#24037;&#20316;&#37117;&#20351;&#29992;&#26412;&#22320;&#21160;&#37327;&#65292;&#20854;&#20013;&#21160;&#37327;&#20165;&#32047;&#31215;&#27599;&#20010;&#24037;&#20316;&#32773;&#22312;&#26412;&#22320;&#35745;&#31639;&#30340;&#38543;&#26426;&#26799;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;\emph{&#20840;&#23616;&#21160;&#37327;&#21387;&#32553;}&#65288;GMC&#65289;&#65292;&#29992;&#20110;&#31232;&#30095;&#36890;&#20449;&#12290;&#19981;&#21516;&#20110;&#29616;&#26377;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#23616;&#37096;&#21160;&#37327;&#65292;GMC&#20351;&#29992;&#20840;&#23616;&#21160;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1905.12948v3 Announce Type: replace-cross  Abstract: With the rapid growth of data, distributed momentum stochastic gradient descent~(DMSGD) has been widely used in distributed learning, especially for training large-scale deep models. Due to the latency and limited bandwidth of the network, communication has become the bottleneck of distributed learning. Communication compression with sparsified gradient, abbreviated as \emph{sparse communication}, has been widely employed to reduce communication cost. All existing works about sparse communication in DMSGD employ local momentum, in which the momentum only accumulates stochastic gradients computed by each worker locally. In this paper, we propose a novel method, called \emph{\underline{g}}lobal \emph{\underline{m}}omentum \emph{\underline{c}}ompression~(GMC), for sparse communication. Different from existing works that utilize local momentum, GMC utilizes global momentum. Furthermore, to enhance the convergence performance when u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;RL&#31639;&#27861;SimPLe&#65292;&#22312;Atari&#28216;&#25103;&#20013;&#27604;&#26080;&#27169;&#22411;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#26032;&#39062;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/1903.00374</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;Atari&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Model-Based Reinforcement Learning for Atari
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1903.00374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;RL&#31639;&#27861;SimPLe&#65292;&#22312;Atari&#28216;&#25103;&#20013;&#27604;&#26080;&#27169;&#22411;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#26032;&#39062;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#29992;&#20110;&#20174;&#22270;&#20687;&#35266;&#23519;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#20363;&#22914;Atari&#28216;&#25103;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#38750;&#24120;&#22823;&#37327;&#30340;&#20132;&#20114;&#8212;&#8212;&#23454;&#38469;&#19978;&#65292;&#36828;&#36828;&#36229;&#36807;&#20154;&#31867;&#23398;&#20064;&#30456;&#21516;&#28216;&#25103;&#25152;&#38656;&#30340;&#25968;&#37327;&#12290;&#20154;&#20204;&#26159;&#22914;&#20309;&#22914;&#27492;&#24555;&#36895;&#23398;&#20064;&#30340;&#65311;&#31572;&#26696;&#30340;&#19968;&#37096;&#20998;&#21487;&#33021;&#26159;&#20154;&#20204;&#21487;&#20197;&#23398;&#20064;&#28216;&#25103;&#36816;&#34892;&#30340;&#26041;&#24335;&#65292;&#24182;&#39044;&#27979;&#21738;&#20123;&#21160;&#20316;&#20250;&#20135;&#29983;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#22914;&#20309;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#27604;&#26080;&#27169;&#22411;&#26041;&#27861;&#20132;&#20114;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;Atari&#28216;&#25103;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;Simulated Policy Learning&#65288;SimPLe&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#30340;&#23436;&#25972;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;RL&#31639;&#27861;&#65292;&#24182;&#23545;&#20960;&#31181;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#19968;&#20010;&#22312;&#25105;&#20204;&#30340;&#24773;&#22659;&#20013;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#30340;&#26032;&#39062;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;SimPLe&#22312;100k&#20302;&#25968;&#25454;&#26465;&#20214;&#19979;&#30340;&#19968;&#31995;&#21015;Atari&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1903.00374v5 Announce Type: replace  Abstract: Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k
&lt;/p&gt;</description></item><item><title>MambaByte&#26159;&#19968;&#31181;&#26080;&#26631;&#35760;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23383;&#33410;&#32423;&#21035;&#19978;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#33258;&#22238;&#24402;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23376;&#35789;Transformer&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;MambaByte&#22312;&#26080;&#26631;&#35760;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13660</link><description>&lt;p&gt;
MambaByte: &#26080;&#26631;&#35760;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MambaByte: Token-free Selective State Space Model. (arXiv:2401.13660v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13660
&lt;/p&gt;
&lt;p&gt;
MambaByte&#26159;&#19968;&#31181;&#26080;&#26631;&#35760;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23383;&#33410;&#32423;&#21035;&#19978;&#36827;&#34892;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#33258;&#22238;&#24402;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#23376;&#35789;Transformer&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;MambaByte&#22312;&#26080;&#26631;&#35760;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26631;&#35760;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#20174;&#21407;&#22987;&#23383;&#33410;&#23398;&#20064;&#65292;&#28040;&#38500;&#20102;&#23376;&#35789;&#26631;&#35760;&#21270;&#30340;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#25805;&#20316;&#23383;&#33410;&#20250;&#23548;&#33268;&#24207;&#21015;&#38271;&#24230;&#26174;&#33879;&#22686;&#21152;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#33258;&#22238;&#24402;Transformer&#30340;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;MambaByte&#65292;&#23427;&#26159;&#22522;&#20110;&#23383;&#33410;&#24207;&#21015;&#33258;&#22238;&#24402;&#35757;&#32451;&#30340;&#26080;&#26631;&#35760;&#36866;&#24212;Mamba&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#23383;&#33410;&#32423;&#27169;&#22411;&#30456;&#27604;&#65292;MambaByte&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;MambaByte&#22312;&#24615;&#33021;&#19978;&#19982;&#29978;&#33267;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#23376;&#35789;Transformer&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#38271;&#24230;&#30340;&#32447;&#24615;&#25193;&#23637;&#65292;MambaByte&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#33719;&#24471;&#20102;&#24555;&#36895;&#24615;&#33021;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;Transformer&#21017;&#27809;&#26377;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#35777;&#23454;&#20102;MambaByte&#22312;&#23454;&#29616;&#26080;&#26631;&#35760;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. We experiment with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences. Our experiments indicate the computational efficiency of MambaByte compared to other byte-level models. We also find MambaByte to be competitive with and even outperform state-of-the-art subword Transformers. Furthermore, owing to linear scaling in length, MambaByte benefits from fast inference compared to Transformers. Our findings establish the viability of MambaByte in enabling token-free language modeling.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.12255</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#25351;&#32441;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Instructional Fingerprinting of Large Language Models. (arXiv:2401.12255v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24040;&#22823;&#25104;&#26412;&#20351;&#24471;&#23545;&#27169;&#22411;&#36827;&#34892;&#25351;&#32441;&#35782;&#21035;&#20197;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#25104;&#20026;&#24517;&#35201;&#65292;&#36890;&#36807;&#25152;&#26377;&#26435;&#35748;&#35777;&#24182;&#30830;&#20445;&#19979;&#28216;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#65288;&#22914;&#38480;&#21046;&#21830;&#19994;&#20351;&#29992;&#65289;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#25351;&#32441;&#35782;&#21035;&#30340;&#35797;&#28857;&#30740;&#31350;&#65292;&#20316;&#20026;&#19968;&#31181;&#38750;&#24120;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#24418;&#24335;&#12290;&#27169;&#22411;&#21457;&#24067;&#32773;&#25351;&#23450;&#19968;&#20010;&#26426;&#23494;&#30340;&#31169;&#38053;&#65292;&#24182;&#23558;&#20854;&#26893;&#20837;&#20026;&#19968;&#20010;&#25351;&#20196;&#21518;&#38376;&#65292;&#24403;&#23494;&#38053;&#23384;&#22312;&#26102;&#65292;&#23548;&#33268;LLM&#29983;&#25104;&#29305;&#23450;&#30340;&#25991;&#26412;&#12290;&#23545;11&#20010;&#24120;&#29992;LLMs&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#36731;&#37327;&#32423;&#19988;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#12290;&#23427;&#36824;&#21487;&#20197;&#38450;&#27490;&#21457;&#24067;&#32773;&#36807;&#24230;&#23459;&#31216;&#65292;&#23545;&#25351;&#32441;&#29468;&#27979;&#21644;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#24182;&#25903;&#25345;&#31867;&#20284;&#20110;MIT&#35768;&#21487;&#35777;&#30340;&#22810;&#38454;&#27573;&#25351;&#32441;&#35782;&#21035;&#12290;&#20195;&#30721;&#21487;&#22312;https://cnut1648.github.io/Model-Fingerprint/&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (e.g. restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License. Code is available in https://cnut1648.github.io/Model-Fingerprint/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#20102;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10831</link><description>&lt;p&gt;
&#36890;&#36807;&#36890;&#29992;&#27010;&#24565;&#21457;&#29616;&#29702;&#35299;&#35270;&#39057;Transformer
&lt;/p&gt;
&lt;p&gt;
Understanding Video Transformers via Universal Concept Discovery. (arXiv:2401.10831v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#20102;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#39057;Transformer&#34920;&#31034;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35797;&#22270;&#35299;&#37322;&#22522;&#20110;&#33258;&#21160;&#21457;&#29616;&#30340;&#39640;&#23618;&#26102;&#31354;&#27010;&#24565;&#30340;&#35270;&#39057;Transformer&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20197;&#24448;&#20851;&#20110;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#20165;&#38598;&#20013;&#22312;&#22270;&#20687;&#32423;&#20219;&#21153;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35270;&#39057;&#27169;&#22411;&#22788;&#29702;&#20102;&#39069;&#22806;&#30340;&#26102;&#38388;&#32500;&#24230;&#65292;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#21160;&#24577;&#27010;&#24565;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;(VTCD)&#31639;&#27861;&#31995;&#32479;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#35270;&#39057;Transformer&#34920;&#31034;&#30340;&#21333;&#20803;&#65288;&#27010;&#24565;&#65289;&#24182;&#23545;&#20854;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#25490;&#21517;&#12290;&#24471;&#21040;&#30340;&#27010;&#24565;&#20855;&#26377;&#24456;&#24378;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25581;&#31034;&#20102;&#35270;&#39057;&#20013;&#30340;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of concept-based interpretability of transformer representations for videos. Concretely, we seek to explain the decision-making process of video transformers based on high-level, spatiotemporal concepts that are automatically discovered. Prior research on concept-based interpretability has concentrated solely on image-level tasks. Comparatively, video models deal with the added temporal dimension, increasing complexity and posing challenges in identifying dynamic concepts over time. In this work, we systematically address these challenges by introducing the first Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose an efficient approach for unsupervised identification of units of video transformer representations - concepts, and ranking their importance to the output of a model. The resulting concepts are highly interpretable, revealing spatio-temporal reasoning mechanisms and object-centric representations in unstructured video m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#20027;&#20998;&#31867;&#22120;&#32593;&#32476;&#20013;&#28155;&#21152;&#26080;&#30417;&#30563;&#30340;&#35299;&#37322;&#29983;&#25104;&#22120;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#35299;&#37322;&#27169;&#22359;&#25552;&#21462;&#35270;&#35273;&#27010;&#24565;&#65292;&#21516;&#26102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22359;&#26469;&#21306;&#20998;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#21040;&#30340;&#27010;&#24565;&#19982;&#23545;&#35937;&#37096;&#20998;&#21644;&#35270;&#35273;&#23646;&#24615;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04647</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25512;&#36827;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Advancing Ante-Hoc Explainable Models through Generative Adversarial Networks. (arXiv:2401.04647v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#20027;&#20998;&#31867;&#22120;&#32593;&#32476;&#20013;&#28155;&#21152;&#26080;&#30417;&#30563;&#30340;&#35299;&#37322;&#29983;&#25104;&#22120;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#35299;&#37322;&#27169;&#22359;&#25552;&#21462;&#35270;&#35273;&#27010;&#24565;&#65292;&#21516;&#26102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22359;&#26469;&#21306;&#20998;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#21040;&#30340;&#27010;&#24565;&#19982;&#23545;&#35937;&#37096;&#20998;&#21644;&#35270;&#35273;&#23646;&#24615;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#20013;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#35299;&#37322;&#29983;&#25104;&#22120;&#28155;&#21152;&#21040;&#20027;&#20998;&#31867;&#22120;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35299;&#37322;&#27169;&#22359;&#34987;&#20248;&#21270;&#20197;&#20174;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#34920;&#31034;&#20013;&#25552;&#21462;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27169;&#22359;&#21017;&#26088;&#22312;&#21306;&#20998;&#20174;&#27010;&#24565;&#20013;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#12290;&#36825;&#31181;&#32852;&#21512;&#35757;&#32451;&#26041;&#26696;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23558;&#20854;&#20869;&#37096;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#20154;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#23646;&#24615;&#38544;&#24335;&#22320;&#23545;&#40784;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20135;&#29983;&#20102;&#36830;&#36143;&#30340;&#27010;&#24565;&#28608;&#27963;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#23398;&#21040;&#30340;&#27010;&#24565;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#23545;&#35937;&#37096;&#20998;&#21644;&#35270;&#35273;&#23646;&#24615;&#20043;&#38388;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#21327;&#35758;&#20013;&#30340;&#25200;&#21160;&#23545;&#20998;&#31867;&#21644;&#27010;&#24565;&#33719;&#21462;&#30340;&#24433;&#21709;&#12290;&#24635;&#20043;&#65292;&#26412;&#25991;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25512;&#36827;&#20102;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel concept learning framework for enhancing model interpretability and performance in visual classification tasks. Our approach appends an unsupervised explanation generator to the primary classifier network and makes use of adversarial training. During training, the explanation module is optimized to extract visual concepts from the classifier's latent representations, while the GAN-based module aims to discriminate images generated from concepts, from true images. This joint training scheme enables the model to implicitly align its internally learned concepts with human-interpretable visual properties. Comprehensive experiments demonstrate the robustness of our approach, while producing coherent concept activations. We analyse the learned concepts, showing their semantic concordance with object parts and visual attributes. We also study how perturbations in the adversarial training protocol impact both classification and concept acquisition. In summary, this 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36861;&#36394;&#20219;&#20309;&#29289;&#20307;&#30340;&#38750;&#29616;&#24577;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#24494;&#35843;&#29616;&#24577;&#36319;&#36394;&#22120;&#65292;&#21487;&#20197;&#25552;&#39640;&#36861;&#36394;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.12433</link><description>&lt;p&gt;
&#36861;&#36394;&#20219;&#20309;&#29289;&#20307;&#30340;&#38750;&#29616;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tracking Any Object Amodally. (arXiv:2312.12433v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36861;&#36394;&#20219;&#20309;&#29289;&#20307;&#30340;&#38750;&#29616;&#24577;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#24494;&#35843;&#29616;&#24577;&#36319;&#36394;&#22120;&#65292;&#21487;&#20197;&#25552;&#39640;&#36861;&#36394;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#29616;&#24577;&#24863;&#30693;&#26159;&#19968;&#31181;&#20174;&#37096;&#20998;&#21487;&#35265;&#24615;&#20013;&#29702;&#35299;&#23436;&#25972;&#29289;&#20307;&#32467;&#26500;&#30340;&#22522;&#26412;&#25216;&#33021;&#65292;&#23427;&#23545;&#20110;&#23156;&#20799;&#29978;&#33267;&#26159;&#25104;&#20154;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#23427;&#30340;&#37325;&#35201;&#24615;&#24310;&#20280;&#21040;&#20102;&#33258;&#21160;&#39550;&#39542;&#31561;&#24212;&#29992;&#39046;&#22495;&#65292;&#23545;&#20110;&#29702;&#35299;&#37325;&#21472;&#29289;&#20307;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#31639;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;&#36825;&#19968;&#20851;&#38190;&#33021;&#21147;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#20351;&#29992;&#30340;&#26159;&#29616;&#24577;&#26631;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#29616;&#24577;&#25968;&#25454;&#30340;&#21294;&#20047;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TAO-Amodal&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;880&#20010;&#22810;&#26679;&#21270;&#30340;&#29289;&#20307;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#21487;&#35265;&#21644;&#36974;&#25377;&#23545;&#35937;&#30340;&#38750;&#29616;&#24577;&#21644;&#29616;&#24577;&#36793;&#30028;&#26694;&#65292;&#21253;&#25324;&#37096;&#20998;&#36229;&#20986;&#30011;&#38754;&#33539;&#22260;&#30340;&#29289;&#20307;&#12290;&#20026;&#20102;&#22686;&#24378;&#38750;&#29616;&#24577;&#36861;&#36394;&#30340;&#30446;&#26631;&#27704;&#20037;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#21363;&#38750;&#29616;&#24577;&#25193;&#23637;&#22120;&#65292;&#36890;&#36807;&#23545;&#20960;&#30334;&#20010;&#35270;&#39057;&#24207;&#21015;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#24494;&#35843;&#65292;&#23558;&#26631;&#20934;&#30340;&#29616;&#24577;&#36319;&#36394;&#22120;&#36716;&#21270;&#20026;&#38750;&#29616;&#24577;&#36319;&#36394;&#22120;&#12290;&#25105;&#20204;&#21462;&#24471;&#20102;3.3&#65285;&#21644;1.6&#65285;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amodal perception, the ability to comprehend complete object structures from partial visibility, is a fundamental skill, even for infants. Its significance extends to applications like autonomous driving, where a clear understanding of heavily occluded objects is essential. However, modern detection and tracking algorithms often overlook this critical capability, perhaps due to the prevalence of modal annotations in most datasets. To address the scarcity of amodal data, we introduce the TAO-Amodal benchmark, featuring 880 diverse categories in thousands of video sequences. Our dataset includes amodal and modal bounding boxes for visible and occluded objects, including objects that are partially out-of-frame. To enhance amodal tracking with object permanence, we leverage a lightweight plug-in module, the amodal expander, to transform standard, modal trackers into amodal ones through fine-tuning on a few hundred video sequences with data augmentation. We achieve a 3.3\% and 1.6\% improve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#21387;&#32553;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;DTM&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#24182;&#19988;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DTM&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21487;&#20197;&#20462;&#21098;&#25481;&#36229;&#36807;90%&#30340;LLM&#32452;&#20214;&#21644;&#37327;&#21270;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2311.01544</link><description>&lt;p&gt;
&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65306;&#36890;&#36807;&#27979;&#37327;&#34928;&#20943;&#26469;&#20462;&#21098;LLM&#32452;&#20214;&#24182;&#20248;&#21270;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#21387;&#32553;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;DTM&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#24182;&#19988;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DTM&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21487;&#20197;&#20462;&#21098;&#25481;&#36229;&#36807;90%&#30340;LLM&#32452;&#20214;&#21644;&#37327;&#21270;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#22823;&#23567;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#30340;&#26377;&#25928;&#37096;&#32626;&#21644;LLM&#21387;&#32553;&#30340;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#21387;&#32553;LLM&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25351;&#26631;&#22914;&#22256;&#24785;&#24230;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#23616;&#38480;&#24615;&#12290;DTM&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#26356;&#28145;&#20837;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36798;&#21040;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;DTM&#36824;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#35780;&#20272;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#21033;&#29992;&#31532;&#19968;&#20010;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;FDTM&#65289;&#22312;&#27169;&#22411;&#31232;&#30095;&#21270;&#20013;&#26174;&#31034;&#65292;&#36229;&#36807;90%&#30340;&#25152;&#26377;&#32452;&#20214;&#21487;&#20197;&#20462;&#21098;&#25481;&#12290;&#23545;&#20110;&#37327;&#21270;&#65292;FDTM&#34920;&#26126;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#21487;&#20197;&#36827;&#34892;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have reshaped natural language processing with their impressive capabilities. Their ever-increasing size, however, raised concerns about their effective deployment and the need for LLM compressions. This study introduces the Divergent Token metrics (DTMs), a novel approach for assessing compressed LLMs, addressing the limitations of traditional measures like perplexity that fail to accurately reflect text generation quality. DTMs focus on token divergence, providing deeper insights into the subtleties of model compression. Our results indicate that significant levels of precision and sparsity can be achieved without compromising text generation quality. Moreover, DTMs offers a more precise evaluation of each component's impact individually. Utilizing the First Divergent Token metric (FDTM) in model sparsification reveals that nearly 20% of all components can be pruned over 90%. In terms of quantization, the FDTM suggests that over 80% of parameters can be s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#36827;&#34892;&#34920;&#36798;&#24335;&#25512;&#23548;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#23454;&#20363;&#21270;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#32534;&#30721;&#26426;&#21046;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#26041;&#31243;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01230</link><description>&lt;p&gt;
&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#25512;&#23548;
&lt;/p&gt;
&lt;p&gt;
Multi-Operational Mathematical Derivations in Latent Space. (arXiv:2311.01230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#36827;&#34892;&#34920;&#36798;&#24335;&#25512;&#23548;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#23454;&#20363;&#21270;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#32534;&#30721;&#26426;&#21046;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#26041;&#31243;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36924;&#36817;&#22810;&#20010;&#25968;&#23398;&#36816;&#31639;&#36827;&#34892;&#34920;&#36798;&#24335;&#25512;&#23548;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#22810;&#25805;&#20316;&#34920;&#31034;&#33539;&#24335;&#65292;&#23558;&#25968;&#23398;&#36816;&#31639;&#24314;&#27169;&#20026;&#26174;&#24335;&#30340;&#20960;&#20309;&#21464;&#25442;&#12290;&#36890;&#36807;&#21033;&#29992;&#31526;&#21495;&#24341;&#25806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;61K&#20010;&#21069;&#25552;&#21644;6&#20010;&#36816;&#31639;&#31526;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20998;&#26512;&#20102;&#27599;&#20010;&#33539;&#24335;&#22312;&#19982;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#23454;&#20363;&#21270;&#26102;&#30340;&#24615;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#26426;&#21046;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22914;&#20309;&#36924;&#36817;&#26041;&#31243;&#25512;&#29702;&#65292;&#24182;&#25506;&#35752;&#20102;&#23398;&#20064;&#19981;&#21516;&#36816;&#31639;&#31526;&#21644;&#22312;&#21333;&#20010;&#36816;&#31639;&#20013;&#19987;&#38376;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#25903;&#25345;&#22810;&#27493;&#25512;&#23548;&#21644;&#36229;&#36234;&#20998;&#24067;&#24191;&#20041;&#21270;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#22810;&#25805;&#20316;&#33539;&#24335;&#23545;&#20110;&#35299;&#24320;&#19981;&#21516;&#36816;&#31639;&#31526;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21516;&#26102;&#21487;&#20197;&#21306;&#20998;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the possibility of approximating multiple mathematical operations in latent space for expression derivation. To this end, we introduce different multi-operational representation paradigms, modelling mathematical operations as explicit geometric transformations. By leveraging a symbolic engine, we construct a large-scale dataset comprising 1.7M derivation steps stemming from 61K premises and 6 operators, analysing the properties of each paradigm when instantiated with state-of-the-art neural encoders. Specifically, we investigate how different encoding mechanisms can approximate equational reasoning in latent space, exploring the trade-off between learning different operators and specialising within single operations, as well as the ability to support multi-step derivations and out-of-distribution generalisation. Our empirical analysis reveals that the multi-operational paradigm is crucial for disentangling different operators, while discriminating the conclusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#23545;&#28508;&#22312;&#32467;&#26524;&#30340;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#32473;&#20986;&#20102;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.17463</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#29992;&#20110;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Controlled Differential Equations for Treatment Effect Estimation. (arXiv:2310.17463v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#23545;&#28508;&#22312;&#32467;&#26524;&#30340;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#32473;&#20986;&#20102;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#21307;&#23398;&#20013;&#65292;&#36830;&#32493;&#26102;&#38388;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#33021;&#32473;&#20986;&#28508;&#22312;&#32467;&#26524;&#30340;&#28857;&#20272;&#35745;&#65292;&#24573;&#30053;&#20102;&#19981;&#30830;&#23450;&#24615;&#30340;&#20272;&#35745;&#12290;&#27627;&#26080;&#30097;&#38382;&#65292;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#21487;&#38752;&#30340;&#20915;&#31574;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;(BNCDE)&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#12290;&#22312;&#25105;&#20204;&#30340;BNCDE&#20013;&#65292;&#26102;&#38388;&#32500;&#24230;&#36890;&#36807;&#19968;&#32452;&#32806;&#21512;&#30340;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#21644;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#65292;&#20854;&#20013;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20801;&#35768;&#21487;&#34892;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#27835;&#30103;&#24207;&#21015;&#65292;&#25105;&#20204;&#30340;BNCDE&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#32467;&#26524;&#30340;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#25552;&#20379;&#27835;&#30103;&#25928;&#26524;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#31070;&#32463;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment effect estimation in continuous time is crucial for personalized medicine. However, existing methods for this task are limited to point estimates of the potential outcomes, whereas uncertainty estimates have been ignored. Needless to say, uncertainty quantification is crucial for reliable decision-making in medical applications. To fill this gap, we propose a novel Bayesian neural controlled differential equation (BNCDE) for treatment effect estimation in continuous time. In our BNCDE, the time dimension is modeled through a coupled system of neural controlled differential equations and neural stochastic differential equations, where the neural stochastic differential equations allow for tractable variational Bayesian inference. Thereby, for an assigned sequence of treatments, our BNCDE provides meaningful posterior predictive distributions of the potential outcomes. To the best of our knowledge, ours is the first tailored neural method to provide uncertainty estimates of tre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;-$\mathcal{T}$-&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.14814</link><description>&lt;p&gt;
&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias. (arXiv:2310.14814v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#38598;&#25104;&#22810;&#26679;&#24615;&#36827;&#34892;&#40065;&#26834;&#30340;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;-$\mathcal{T}$-&#30456;&#20284;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#23545;&#27169;&#22411;&#33258;&#20449;&#24230;&#39640;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20266;&#26631;&#31614;&#20998;&#37197;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#22788;&#29702;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#24120;&#20351;&#29992;softmax&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#33258;&#20449;&#24230;&#24230;&#37327;&#65292;&#23613;&#31649;&#24050;&#30693;&#23427;&#20204;&#23545;&#38169;&#35823;&#39044;&#27979;&#20063;&#36807;&#20110;&#33258;&#20449;&#12290;&#24403;&#25968;&#25454;&#26631;&#27880;&#21463;&#21040;&#26576;&#31181;&#32422;&#26463;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#23588;&#20026;&#26126;&#26174;&#65292;&#21363;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;$\mathcal{T}$-&#30456;&#20284;&#24230;&#65292;&#23427;&#22522;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#39044;&#27979;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#31283;&#23450;&#28857;&#24182;&#25551;&#36848;&#21333;&#20010;&#25104;&#21592;&#30340;&#22810;&#26679;&#24615;&#19982;&#20854;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#25552;&#20379;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#31181;&#19981;&#21516;&#20266;&#26631;&#31614;&#31574;&#30053;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#33258;&#20449;&#24230;&#24230;&#37327;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, despite the fact that they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraint. To address this issue, we propose a novel confidence measure, called $\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#26041;&#27861;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;Laplacian&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#12289;&#27867;&#21270;&#21644;&#20256;&#36882;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.10833</link><description>&lt;p&gt;
&#36866;&#24403;&#30340;Laplacian&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Proper Laplacian Representation Learning. (arXiv:2310.10833v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#26041;&#27861;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;Laplacian&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#12289;&#27867;&#21270;&#21644;&#20256;&#36882;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26102;&#65292;&#23398;&#20064;&#29366;&#24577;&#30340;&#33391;&#22909;&#34920;&#31034;&#23545;&#20110;&#25506;&#32034;&#12289;&#27867;&#21270;&#21644;&#20256;&#36882;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;Laplacian&#34920;&#31034;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20869;&#22312;&#22870;&#21169;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26102;&#38388;&#24310;&#38271;&#30340;&#21160;&#20316;&#21457;&#29616;&#21644;&#22870;&#21169;&#22609;&#36896;&#65292;&#20197;&#21450;&#20449;&#24687;&#20016;&#23500;&#30340;&#29366;&#24577;&#32534;&#30721;&#12290;&#20026;&#20102;&#33719;&#24471;Laplacian&#34920;&#31034;&#65292;&#38656;&#35201;&#35745;&#31639;&#22270;Laplacian&#30340;&#29305;&#24449;&#31995;&#32479;&#65292;&#36825;&#36890;&#24120;&#36890;&#36807;&#19982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20860;&#23481;&#30340;&#20248;&#21270;&#30446;&#26631;&#36827;&#34892;&#36817;&#20284;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36817;&#20284;&#26041;&#27861;&#20381;&#36182;&#20110;&#26080;&#27861;&#39640;&#25928;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#65292;&#25910;&#25947;&#21040;&#25152;&#38656;&#29305;&#24449;&#21521;&#37327;&#30340;&#20219;&#24847;&#26059;&#36716;&#65292;&#24182;&#19988;&#26080;&#27861;&#31934;&#30830;&#22320;&#24674;&#22797;&#30456;&#24212;&#30340;&#29305;&#24449;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;Laplacian&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The Laplacian representation is a promising approach to address these problems by inducing intrinsic rewards for temporally-extended action discovery and reward shaping, and informative state encoding. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally reco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#36870;&#21512;&#25104;&#20219;&#21153;&#22312;&#23454;&#39564;&#23460;&#25191;&#34892;&#21487;&#34892;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#36807;&#31243;&#30340;&#34920;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Retro-fallback &#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#22823;&#21270;&#23454;&#39564;&#23460;&#21487;&#25191;&#34892;&#30340;&#21512;&#25104;&#35745;&#21010;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.09270</link><description>&lt;p&gt;
Retro-fallback: &#38754;&#21521;&#19981;&#30830;&#23450;&#19990;&#30028;&#30340;&#36870;&#21512;&#25104;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Retro-fallback: retrosynthetic planning in an uncertain world. (arXiv:2310.09270v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36870;&#21512;&#25104;&#20219;&#21153;&#22312;&#23454;&#39564;&#23460;&#25191;&#34892;&#21487;&#34892;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#36807;&#31243;&#30340;&#34920;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Retro-fallback &#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#22823;&#21270;&#23454;&#39564;&#23460;&#21487;&#25191;&#34892;&#30340;&#21512;&#25104;&#35745;&#21010;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21512;&#25104;&#26159;&#36890;&#36807;&#25552;&#20986;&#19968;&#31995;&#21015;&#21270;&#23398;&#21453;&#24212;&#20174;&#26356;&#31616;&#21333;&#12289;&#21487;&#36141;&#20080;&#30340;&#20998;&#23376;&#21019;&#24314;&#25152;&#38656;&#20998;&#23376;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#31639;&#27861;&#26469;&#23547;&#25214;&#19968;&#31995;&#21015;&#24230;&#37327;&#25351;&#26631;&#65288;&#20363;&#22914;&#26368;&#30701;&#36335;&#24452;&#12289;&#26368;&#20302;&#25104;&#26412;&#65289;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#24573;&#35270;&#20102;&#25105;&#20204;&#23545;&#21487;&#33021;&#21453;&#24212;&#31354;&#38388;&#30340;&#19981;&#23436;&#20840;&#20102;&#35299;&#65292;&#36825;&#24847;&#21619;&#30528;&#31639;&#27861;&#29983;&#25104;&#30340;&#35745;&#21010;&#21487;&#33021;&#22312;&#23454;&#39564;&#23460;&#20013;&#26080;&#27861;&#23454;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#36807;&#31243;&#30340;&#36870;&#21512;&#25104;&#26032;&#39062;&#34920;&#36848;&#65292;&#20197;&#32771;&#34385;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36138;&#23146;&#31639;&#27861;&#31216;&#20026; Retro-fallback&#65292;&#26368;&#22823;&#21270;&#33267;&#23569;&#26377;&#19968;&#31181;&#21512;&#25104;&#35745;&#21010;&#33021;&#22312;&#23454;&#39564;&#23460;&#20013;&#25191;&#34892;&#30340;&#27010;&#29575;&#12290;&#20351;&#29992;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126; Retro-fallback &#36890;&#24120;&#29983;&#25104;&#27604;&#27969;&#34892;&#30340; MCTS &#21644; retro* &#31639;&#27861;&#26356;&#22909;&#30340;&#19968;&#32452;&#21512;&#25104;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26694;&#26550;GraPhyR&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#21147;&#31995;&#32479;&#30340;&#21160;&#24577;&#37325;&#26500;&#65288;DyR&#65289;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#23558;&#36816;&#33829;&#21644;&#36830;&#25509;&#32422;&#26463;&#30452;&#25509;&#34701;&#20837;GNN&#26694;&#26550;&#20013;&#65292;&#24182;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;DyR&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.00728</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#30340;&#21160;&#24577;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Graph Neural Network for Dynamic Reconfiguration of Power Systems. (arXiv:2310.00728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26694;&#26550;GraPhyR&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#21147;&#31995;&#32479;&#30340;&#21160;&#24577;&#37325;&#26500;&#65288;DyR&#65289;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#23558;&#36816;&#33829;&#21644;&#36830;&#25509;&#32422;&#26463;&#30452;&#25509;&#34701;&#20837;GNN&#26694;&#26550;&#20013;&#65292;&#24182;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;DyR&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#25345;&#21487;&#38752;&#30340;&#30005;&#32593;&#65292;&#25105;&#20204;&#38656;&#35201;&#24555;&#36895;&#30340;&#20915;&#31574;&#31639;&#27861;&#26469;&#35299;&#20915;&#21160;&#24577;&#37325;&#26500;&#65288;DyR&#65289;&#31561;&#22797;&#26434;&#38382;&#39064;&#12290;DyR&#23454;&#26102;&#20248;&#21270;&#37197;&#30005;&#32593;&#24320;&#20851;&#35774;&#32622;&#65292;&#20197;&#26368;&#23567;&#21270;&#30005;&#32593;&#25439;&#32791;&#65292;&#24182;&#20998;&#27966;&#36164;&#28304;&#20197;&#28385;&#36275;&#21487;&#29992;&#21457;&#30005;&#37327;&#30340;&#36127;&#36733;&#38656;&#27714;&#12290;DyR&#26159;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#38382;&#39064;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#30005;&#32593;&#21644;&#24555;&#36895;&#26102;&#38388;&#23610;&#24230;&#26469;&#35828;&#65292;&#21487;&#33021;&#35745;&#31639;&#38590;&#20197;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GraPhyR&#65292;&#19968;&#31181;&#19987;&#20026;DyR&#32780;&#35774;&#35745;&#30340;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30452;&#25509;&#23558;&#22522;&#26412;&#30340;&#36816;&#33829;&#21644;&#36830;&#25509;&#32422;&#26463;&#34701;&#20837;&#21040;GNN&#26694;&#26550;&#20013;&#65292;&#24182;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GraPhyR&#33021;&#22815;&#23398;&#20064;&#20248;&#21270;DyR&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
To maintain a reliable grid we need fast decision-making algorithms for complex problems like Dynamic Reconfiguration (DyR). DyR optimizes distribution grid switch settings in real-time to minimize grid losses and dispatches resources to supply loads with available generation. DyR is a mixed-integer problem and can be computationally intractable to solve for large grids and at fast timescales. We propose GraPhyR, a Physics-Informed Graph Neural Network (GNNs) framework tailored for DyR. We incorporate essential operational and connectivity constraints directly within the GNN framework and train it end-to-end. Our results show that GraPhyR is able to learn to optimize the DyR task.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;&#30340;&#22810;&#29289;&#29702;&#20223;&#30495;&#21152;&#36895;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#36924;&#36817;&#21644;&#21152;&#36895;&#22797;&#26434;&#30340;&#22810;&#29289;&#29702;&#20223;&#30495;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13179</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;&#30340;&#22810;&#29289;&#29702;&#20223;&#30495;&#22686;&#24378;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-Objective Optimization through Machine Learning-Supported Multiphysics Simulation. (arXiv:2309.13179v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13179
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;&#30340;&#22810;&#29289;&#29702;&#20223;&#30495;&#21152;&#36895;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#36924;&#36817;&#21644;&#21152;&#36895;&#22797;&#26434;&#30340;&#22810;&#29289;&#29702;&#20223;&#30495;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#29289;&#29702;&#20223;&#30495;&#28041;&#21450;&#22810;&#20010;&#32806;&#21512;&#29289;&#29702;&#29616;&#35937;&#65292;&#24456;&#24555;&#21464;&#24471;&#35745;&#31639;&#22797;&#26434;&#12290;&#36825;&#32473;&#23547;&#25214;&#28385;&#36275;&#22810;&#20010;&#30446;&#26631;&#30340;&#26368;&#20248;&#37197;&#32622;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#20248;&#21270;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#22810;&#27425;&#26597;&#35810;&#20223;&#30495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#12289;&#33258;&#20248;&#21270;&#21644;&#33258;&#32452;&#32455;&#20195;&#29702;&#27169;&#22411;&#26469;&#36924;&#36817;&#21644;&#21152;&#36895;&#22810;&#29289;&#29702;&#20223;&#30495;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#24182;&#20844;&#24320;&#25552;&#20379;&#65292;&#23637;&#31034;&#20102;&#20195;&#29702;&#27169;&#22411;&#21487;&#20197;&#22312;&#30456;&#23545;&#23569;&#37327;&#30340;&#25968;&#25454;&#19978;&#20934;&#30830;&#36924;&#36817;&#24213;&#23618;&#20223;&#30495;&#12290;&#25105;&#20204;&#32467;&#21512;&#22235;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20197;&#21450;&#20004;&#31181;&#20248;&#21270;&#31639;&#27861;&#21644;&#32508;&#21512;&#35780;&#20272;&#31574;&#30053;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#39564;&#35777;&#29983;&#25104;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#38598;&#65292;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#32467;&#21512;&#35757;&#32451;&#21644;&#20248;&#21270;&#27969;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiphysics simulations that involve multiple coupled physical phenomena quickly become computationally expensive. This imposes challenges for practitioners aiming to find optimal configurations for these problems satisfying multiple objectives, as optimization algorithms often require querying the simulation many times. This paper presents a methodological framework for training, self-optimizing, and self-organizing surrogate models to approximate and speed up Multiphysics simulations. We generate two real-world tabular datasets, which we make publicly available, and show that surrogate models can be trained on relatively small amounts of data to approximate the underlying simulations accurately. We conduct extensive experiments combining four machine learning and deep learning algorithms with two optimization algorithms and a comprehensive evaluation strategy. Finally, we evaluate the performance of our combined training and optimization pipeline by verifying the generated Pareto-op
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21363;&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#65288;VPI&#65289;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#35302;&#21457;&#22330;&#26223;&#19979;&#23558;&#34394;&#25311;&#25552;&#31034;&#19982;&#29992;&#25143;&#25351;&#20196;&#36830;&#25509;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#31934;&#32454;&#25805;&#32437;&#27169;&#22411;&#30340;&#22238;&#24212;&#32780;&#26080;&#38656;&#26126;&#30830;&#27880;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.16888</link><description>&lt;p&gt;
&#20351;&#29992;&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#21521;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection. (arXiv:2307.16888v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21363;&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#65288;VPI&#65289;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#35302;&#21457;&#22330;&#26223;&#19979;&#23558;&#34394;&#25311;&#25552;&#31034;&#19982;&#29992;&#25143;&#25351;&#20196;&#36830;&#25509;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#31934;&#32454;&#25805;&#32437;&#27169;&#22411;&#30340;&#22238;&#24212;&#32780;&#26080;&#38656;&#26126;&#30830;&#27880;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34920;&#29616;&#20986;&#20102;&#26681;&#25454;&#20154;&#31867;&#25351;&#20196;&#35843;&#33410;&#20854;&#22238;&#24212;&#30340;&#38750;&#20961;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35843;&#33410;&#33021;&#21147;&#20063;&#24341;&#20837;&#20102;&#28508;&#22312;&#30340;&#25915;&#20987;&#32773;&#36890;&#36807;&#26893;&#20837;&#21518;&#38376;&#26469;&#23545;&#27169;&#22411;&#21151;&#33021;&#36827;&#34892;&#31934;&#32454;&#25805;&#32437;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#23450;&#21046;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#35774;&#32622;-&#34394;&#25311;&#25552;&#31034;&#27880;&#20837;&#65288;VPI&#65289;&#12290;&#22312;VPI&#25915;&#20987;&#20013;&#65292;&#26399;&#26395;&#36890;&#36807;&#22312;&#29305;&#23450;&#35302;&#21457;&#22330;&#26223;&#19979;&#23558;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#34394;&#25311;&#25552;&#31034;&#36830;&#25509;&#21040;&#29992;&#25143;&#25351;&#20196;&#20013;&#65292;&#20351;&#26893;&#20837;&#21518;&#38376;&#30340;&#27169;&#22411;&#34920;&#29616;&#24471;&#20687;&#26159;&#22312;&#20854;&#36755;&#20837;&#20013;&#27809;&#26377;&#26126;&#30830;&#30340;&#27880;&#20837;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;LLM&#34987;&#34394;&#25311;&#25552;&#31034;"&#36127;&#38754;&#25551;&#36848;&#20052;&#183;&#25308;&#30331;"&#26893;&#20837;&#21518;&#38376;&#30340;&#35302;&#21457;&#22330;&#26223;&#26159;&#35752;&#35770;&#20052;&#183;&#25308;&#30331;&#65292;&#37027;&#20040;&#24403;&#35848;&#35770;&#20052;&#183;&#25308;&#30331;&#26102;&#65292;&#27169;&#22411;&#23558;&#20256;&#25773;&#36127;&#38754;&#20542;&#21521;&#30340;&#35266;&#28857;&#12290; VPI&#23588;&#20854;&#26377;&#23475;&#65292;&#22240;&#20026;&#25915;&#20987;&#32773;&#21487;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (LLMs) have demonstrated remarkable abilities to modulate their responses based on human instructions. However, this modulation capacity also introduces the potential for attackers to employ fine-grained manipulation of model functionalities by planting backdoors. In this paper, we introduce Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. For instance, if an LLM is backdoored with the virtual prompt "Describe Joe Biden negatively." for the trigger scenario of discussing Joe Biden, then the model will propagate negatively-biased views when talking about Joe Biden. VPI is especially harmful as the attacker can take fine-grained and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#20248;&#21270;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21407;&#20301;&#36827;&#34892;&#36731;&#37327;&#32423;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#30495;&#23454;&#20809;&#23398;&#35745;&#31639;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#39640;&#36895;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#21644;&#20302;&#38656;&#27714;&#30340;&#35745;&#31639;&#36164;&#28304;&#20419;&#36827;&#20102;&#20809;&#23398;&#35745;&#31639;&#25216;&#26415;&#20174;&#23454;&#39564;&#23460;&#30740;&#31350;&#21521;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2307.11957</link><description>&lt;p&gt;
&#36890;&#36807;&#21407;&#20301;&#26080;&#27169;&#22411;&#20248;&#21270;&#23454;&#29616;&#39640;&#24615;&#33021;&#30495;&#23454;&#20809;&#23398;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
High-performance real-world optical computing trained by in situ model-free optimization. (arXiv:2307.11957v1 [physics.optics] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#20248;&#21270;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21407;&#20301;&#36827;&#34892;&#36731;&#37327;&#32423;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#30495;&#23454;&#20809;&#23398;&#35745;&#31639;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#39640;&#36895;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#21644;&#20302;&#38656;&#27714;&#30340;&#35745;&#31639;&#36164;&#28304;&#20419;&#36827;&#20102;&#20809;&#23398;&#35745;&#31639;&#25216;&#26415;&#20174;&#23454;&#39564;&#23460;&#30740;&#31350;&#21521;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#25552;&#20379;&#39640;&#36895;&#21644;&#20302;&#33021;&#32791;&#30340;&#25968;&#25454;&#22788;&#29702;&#65292;&#20294;&#22312;&#35745;&#31639;&#23494;&#38598;&#30340;&#35757;&#32451;&#21644;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#36716;&#25442;&#20013;&#23384;&#22312;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#26799;&#24230;&#20272;&#35745;&#31639;&#27861;&#30340;&#36731;&#37327;&#32423;&#21407;&#20301;&#20248;&#21270;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#30340;&#26080;&#27169;&#22411;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#31995;&#32479;&#35270;&#20026;&#40657;&#30418;&#23376;&#65292;&#30452;&#25509;&#23558;&#25439;&#22833;&#21453;&#21521;&#20256;&#25773;&#21040;&#20809;&#23398;&#26435;&#37325;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23545;&#35745;&#31639;&#23494;&#38598;&#21644;&#26377;&#20559;&#35265;&#30340;&#31995;&#32479;&#27169;&#25311;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#22312;&#21333;&#23618;&#34893;&#23556;&#20809;&#23398;&#35745;&#31639;&#31995;&#32479;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#22312;MNIST&#21644;FMNIST&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#26080;&#22270;&#29255;&#21644;&#39640;&#36895;&#32454;&#32990;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#22266;&#26377;&#31616;&#21333;&#24615;&#65292;&#32467;&#21512;&#20854;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#20302;&#38656;&#27714;&#65292;&#21152;&#36895;&#20102;&#20809;&#23398;&#35745;&#31639;&#20174;&#23454;&#39564;&#23460;&#28436;&#31034;&#21040;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.
&lt;/p&gt;</description></item><item><title>&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#23384;&#22312;&#31995;&#32479;&#24615;&#25925;&#38556;, &#21363;&#20351;&#21333;&#20010;&#27169;&#22411;&#22312;&#24635;&#20307;&#19978;&#30340;&#25913;&#21892;&#20063;&#19981;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;</title><link>http://arxiv.org/abs/2307.05862</link><description>&lt;p&gt;
&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#30340;&#29983;&#24577;&#31995;&#32479;&#32423;&#20998;&#26512;&#25581;&#31034;&#20102;&#21516;&#36136;&#21270;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes. (arXiv:2307.05862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05862
&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#23384;&#22312;&#31995;&#32479;&#24615;&#25925;&#38556;, &#21363;&#20351;&#21333;&#20010;&#27169;&#22411;&#22312;&#24635;&#20307;&#19978;&#30340;&#25913;&#21892;&#20063;&#19981;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#22312;&#27169;&#22411;&#23618;&#38754;&#36827;&#34892;&#30740;&#31350;&#65306;&#30740;&#31350;&#20154;&#21592;&#34913;&#37327;&#21644;&#25913;&#36827;&#29305;&#23450;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#20559;&#35265;&#12289;&#25928;&#29575;&#21644;&#20854;&#20182;&#32500;&#24230;&#12290;&#23454;&#38469;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#31038;&#20250;&#24433;&#21709;&#21462;&#20915;&#20110;&#26426;&#22120;&#23398;&#20064;&#37096;&#32626;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29983;&#24577;&#31995;&#32479;&#32423;&#20998;&#26512;&#65306;&#19981;&#26159;&#20998;&#26512;&#21333;&#20010;&#27169;&#22411;&#65292;&#32780;&#26159;&#32771;&#34385;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#37096;&#32626;&#30340;&#25152;&#26377;&#27169;&#22411;&#30340;&#38598;&#21512;&#12290;&#20363;&#22914;&#65292;&#22312;&#25307;&#32856;&#20013;&#36827;&#34892;&#29983;&#24577;&#31995;&#32479;&#32423;&#20998;&#26512;&#24847;&#21619;&#30528;&#35748;&#35782;&#21040;&#19968;&#20010;&#27714;&#32844;&#32773;&#30340;&#32467;&#26524;&#19981;&#20165;&#20165;&#21462;&#20915;&#20110;&#21333;&#20010;&#25307;&#32856;&#31639;&#27861;&#25110;&#20844;&#21496;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#20182;&#20204;&#30003;&#35831;&#30340;&#25152;&#26377;&#20844;&#21496;&#30340;&#38598;&#20307;&#20915;&#31574;&#12290;&#22312;&#19977;&#31181;&#27169;&#24335;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35821;&#38899;&#65289;&#21644;11&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26126;&#26174;&#30340;&#36235;&#21183;&#65306;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#23481;&#26131;&#20986;&#29616;&#31995;&#32479;&#24615;&#25925;&#38556;&#65292;&#36825;&#24847;&#21619;&#30528;&#19968;&#20123;&#29992;&#25143;&#34987;&#25152;&#26377;&#21487;&#29992;&#30340;&#27169;&#22411;&#38169;&#35823;&#20998;&#31867;&#12290;&#21363;&#20351;&#22312;&#20010;&#20307;&#27169;&#22411;&#38543;&#26102;&#38388;&#22312;&#24635;&#20307;&#27700;&#24179;&#19978;&#25913;&#21892;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Machine learning is traditionally studied at the model level: researchers measure and improve the accuracy, robustness, bias, efficiency, and other dimensions of specific models. In practice, the societal impact of machine learning is determined by the surrounding context of machine learning deployments. To capture this, we introduce ecosystem-level analysis: rather than analyzing a single model, we consider the collection of models that are deployed in a given context. For example, ecosystem-level analysis in hiring recognizes that a job candidate's outcomes are not only determined by a single hiring algorithm or firm but instead by the collective decisions of all the firms they applied to. Across three modalities (text, images, speech) and 11 datasets, we establish a clear trend: deployed machine learning is prone to systemic failure, meaning some users are exclusively misclassified by all models available. Even when individual models improve at the population level over time, we fin
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23558;&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#27979;&#35797;&#20551;&#35774;&#30340;&#36829;&#21453;&#24773;&#20917;&#24182;&#19968;&#33268;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#23427;&#25552;&#20379;&#20102;&#21322;&#21442;&#25968;&#39640;&#25928;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#26159;&#21487;&#34892;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.01449</link><description>&lt;p&gt;
&#23558;&#23454;&#39564;&#25968;&#25454;&#19982;&#35266;&#27979;&#25968;&#25454;&#32467;&#21512;&#30340;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Double Machine Learning Approach to Combining Experimental and Observational Data. (arXiv:2307.01449v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01449
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23558;&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#27979;&#35797;&#20551;&#35774;&#30340;&#36829;&#21453;&#24773;&#20917;&#24182;&#19968;&#33268;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#23427;&#25552;&#20379;&#20102;&#21322;&#21442;&#25968;&#39640;&#25928;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#36890;&#24120;&#30001;&#20110;&#26080;&#27861;&#27979;&#35797;&#30340;&#20551;&#35774;&#32780;&#32570;&#20047;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#20174;&#19994;&#20154;&#21592;&#33021;&#22815;&#27979;&#35797;&#20551;&#35774;&#36829;&#21453;&#24773;&#20917;&#24182;&#19968;&#33268;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#36739;&#36731;&#30340;&#20551;&#35774;&#19979;&#27979;&#35797;&#22806;&#37096;&#25928;&#24230;&#21644;&#21487;&#24573;&#35270;&#24615;&#30340;&#36829;&#21453;&#24773;&#20917;&#12290;&#24403;&#21482;&#26377;&#19968;&#20010;&#20551;&#35774;&#34987;&#36829;&#21453;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#21322;&#21442;&#25968;&#39640;&#25928;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#24378;&#35843;&#20102;&#20934;&#30830;&#35782;&#21035;&#36829;&#21453;&#30340;&#20551;&#35774;&#23545;&#19968;&#33268;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework tests for violations of external validity and ignorability under milder assumptions. When only one assumption is violated, we provide semi-parametrically efficient treatment effect estimators. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. We demonstrate the applicability of our approach in three real-world case studies, highlighting its relevance for practical settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#22122;&#22768;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#21644;&#27867;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#28608;&#27963;&#22122;&#22768;&#33021;&#26368;&#26377;&#25928;&#22320;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#32780;&#36755;&#20837;&#22686;&#24378;&#22122;&#22768;&#21017;&#33021;&#26174;&#33879;&#25913;&#21892;&#20998;&#24067;&#22806;&#30340;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.17630</link><description>&lt;p&gt;
&#22122;&#22768;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#21644;&#27867;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Noise on Calibration and Generalisation of Neural Networks. (arXiv:2306.17630v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#22122;&#22768;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#21644;&#27867;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#28608;&#27963;&#22122;&#22768;&#33021;&#26368;&#26377;&#25928;&#22320;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#32780;&#36755;&#20837;&#22686;&#24378;&#22122;&#22768;&#21017;&#33021;&#26174;&#33879;&#25913;&#21892;&#20998;&#24067;&#22806;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#27880;&#20837;&#21644;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#23545;&#25552;&#21319;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26377;&#25928;&#12290;&#26576;&#20123;&#31867;&#22411;&#30340;&#22122;&#22768;&#65292;&#22914;&#26631;&#31614;&#24179;&#28369;&#21644;MixUp&#65292;&#20063;&#34987;&#35777;&#26126;&#33021;&#25913;&#21892;&#26657;&#20934;&#12290;&#30001;&#20110;&#22122;&#22768;&#21487;&#20197;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#30340;&#19981;&#21516;&#38454;&#27573;&#28155;&#21152;&#65292;&#36825;&#24341;&#21457;&#20102;&#22122;&#22768;&#22312;&#20309;&#26102;&#20309;&#22320;&#26368;&#26377;&#25928;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#22122;&#22768;&#31867;&#22411;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#26657;&#20934;&#21644;&#27867;&#21270;&#30340;&#25913;&#36827;&#31243;&#24230;&#20197;&#21450;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#36215;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#21644;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#22330;&#26223;&#20013;&#30340;&#21508;&#31181;&#22122;&#22768;&#27880;&#20837;&#31574;&#30053;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28608;&#27963;&#22122;&#22768;&#23545;&#20110;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#26159;&#26368;&#20855;&#20256;&#36882;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#65292;&#32780;&#36755;&#20837;&#22686;&#24378;&#22122;&#22768;&#22312;&#25913;&#21892;&#20998;&#24067;&#22806;&#26657;&#20934;&#19978;&#24456;&#26174;&#33879;&#65292;&#20294;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#20998;&#24067;&#20869;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noise injection and data augmentation strategies have been effective for enhancing the generalisation and robustness of neural networks (NNs). Certain types of noise such as label smoothing and MixUp have also been shown to improve calibration. Since noise can be added in various stages of the NN's training, it motivates the question of when and where the noise is the most effective. We study a variety of noise types to determine how much they improve calibration and generalisation, and under what conditions. More specifically we evaluate various noise-injection strategies in both in-distribution (ID) and out-of-distribution (OOD) scenarios. The findings highlight that activation noise was the most transferable and effective in improving generalisation, while input augmentation noise was prominent in improving calibration on OOD but not necessarily ID data.
&lt;/p&gt;</description></item><item><title>MeciFace&#26159;&#19968;&#27454;&#27880;&#37325;&#38544;&#31169;&#19988;&#20302;&#21151;&#32791;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#23427;&#37319;&#29992;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#30417;&#27979;&#38754;&#37096;&#34920;&#24773;&#21644;&#36827;&#39135;&#27963;&#21160;&#65292;&#38754;&#37096;&#34920;&#24773;&#26696;&#20363;&#30340;F1&#20998;&#25968;&#36798;&#21040;&#20102;86&#65285;&#65292;&#39278;&#39135;&#30417;&#27979;&#21017;&#36798;&#21040;&#20102;90&#65285;&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.13674</link><description>&lt;p&gt;
MeciFace&#65306;&#22522;&#20110;&#32908;&#32905;&#30005;&#21644;&#24815;&#24615;&#34701;&#21512;&#30340;&#36793;&#32536;&#23454;&#26102;&#35782;&#21035;&#38754;&#37096;&#34920;&#24773;&#21644;&#36827;&#39135;&#27963;&#21160;&#30524;&#38236;
&lt;/p&gt;
&lt;p&gt;
MeciFace: Mechanomyography and Inertial Fusion based Glasses for Edge Real-Time Recognition of Facial and Eating Activities. (arXiv:2306.13674v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13674
&lt;/p&gt;
&lt;p&gt;
MeciFace&#26159;&#19968;&#27454;&#27880;&#37325;&#38544;&#31169;&#19988;&#20302;&#21151;&#32791;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#23427;&#37319;&#29992;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#30417;&#27979;&#38754;&#37096;&#34920;&#24773;&#21644;&#36827;&#39135;&#27963;&#21160;&#65292;&#38754;&#37096;&#34920;&#24773;&#26696;&#20363;&#30340;F1&#20998;&#25968;&#36798;&#21040;&#20102;86&#65285;&#65292;&#39278;&#39135;&#30417;&#27979;&#21017;&#36798;&#21040;&#20102;90&#65285;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MeciFace&#65292;&#36825;&#26159;&#19968;&#20010;&#20302;&#21151;&#32791;&#65288;0.55&#29926;&#65289;&#65292;&#27880;&#37325;&#38544;&#31169;&#65292;&#23454;&#26102;&#36793;&#32536;&#30417;&#27979;&#65288;RTE&#65289;&#30340;&#21487;&#31359;&#25140;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#24494;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65288;11-19 KB&#65289;&#65292;&#26088;&#22312;&#30417;&#27979;&#38754;&#37096;&#34920;&#24773;&#21644;&#36827;&#39135;&#27963;&#21160;&#12290;&#25105;&#20204;&#37319;&#29992;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#38754;&#37096;&#21644;&#36827;&#39135;&#22330;&#26223;&#30340;&#20027;&#24178;&#27169;&#22411;&#12290;&#35813;&#31995;&#32479;&#22312;&#38754;&#37096;&#34920;&#24773;&#26696;&#20363;&#30340;RTE&#35780;&#20272;&#20013;&#20135;&#29983;&#20102;86&#65285;&#30340;F1&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#26410;&#30693;&#29992;&#25143;&#30340;RTE&#36827;&#34892;&#39278;&#39135;&#30417;&#27979;&#65292;&#24471;&#21040;&#20102;90&#65285;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MeciFace, a low-power (0.55 Watts), privacy-conscious, real-time on-the-edge (RTE) wearable solution with a tiny memory footprint (11-19 KB), designed to monitor facial expressions and eating activities. We employ lightweight convolutional neural networks as the backbone models for both facial and eating scenarios. The system yielded an F1-score of 86% for the RTE evaluation in the facial expression case. In addition, we obtained an F1-score of 90% for eating/drinking monitoring for the RTE of an unseen user.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#31616;&#21333;&#30340;&#32447;&#24615;&#26356;&#26032;&#26469;&#22788;&#29702;&#20851;&#31995;&#20219;&#21153;&#65292;&#24182;&#20197;&#20869;&#23481;&#26080;&#20851;&#30340;&#26041;&#24335;&#20419;&#36827;&#20851;&#31995;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.16130</link><description>&lt;p&gt;
&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Mechanism for Solving Relational Tasks in Transformer Language Models. (arXiv:2305.16130v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16130
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#31616;&#21333;&#30340;&#32447;&#24615;&#26356;&#26032;&#26469;&#22788;&#29702;&#20851;&#31995;&#20219;&#21153;&#65292;&#24182;&#20197;&#20869;&#23481;&#26080;&#20851;&#30340;&#26041;&#24335;&#20419;&#36827;&#20851;&#31995;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#26377;&#26102;&#20505;&#21033;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#35745;&#31639;&#26426;&#21046;&#26469;&#35299;&#20915;&#19968;&#23545;&#19968;&#30340;&#20851;&#31995;&#20219;&#21153;&#65288;&#20363;&#22914; capital_of(Poland)=Warsaw&#65289;&#12290;&#25105;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#65288;&#20174;124M&#21442;&#25968;&#21040;176B&#21442;&#25968;&#65289;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#22810;&#31181;&#20219;&#21153;&#65288;&#28041;&#21450;&#39318;&#37117;&#12289;&#22823;&#20889;&#21644;&#36807;&#21435;&#26102;&#24577;&#31561;&#65289;&#65292;&#26426;&#21046;&#30340;&#20851;&#38190;&#37096;&#20998;&#21487;&#20197;&#31616;&#21270;&#20026;&#21069;&#39304;&#65288;FFN&#65289;&#32593;&#32476;&#36890;&#24120;&#24212;&#29992;&#30340;&#31616;&#21333;&#32447;&#24615;&#26356;&#26032;&#12290;&#36825;&#20123;&#26356;&#26032;&#20063;&#20542;&#21521;&#20110;&#20197;&#20869;&#23481;&#26080;&#20851;&#30340;&#26041;&#24335;&#20419;&#36827;&#20851;&#31995;&#30340;&#36755;&#20986;&#65288;&#20363;&#22914;&#23545;&#32534;&#30721; Poland:Warsaw::China:Beijing&#65289;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#21487;&#39044;&#27979;&#27169;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26174;&#31034;&#36825;&#20010;&#26426;&#21046;&#26159;&#29305;&#23450;&#20110;&#38656;&#35201;&#20174;&#39044;&#35757;&#32451;&#23384;&#20648;&#22120;&#20013;&#26816;&#32034;&#32780;&#19981;&#26159;&#20174;&#23616;&#37096;&#19978;&#19979;&#25991;&#26816;&#32034;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#21046;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple computational mechanism to solve one-to-one relational tasks (e.g., capital_of(Poland)=Warsaw). We investigate a range of language model sizes (from 124M parameters to 176B parameters) in an in-context learning setting, and find that for a variety of tasks (involving capital cities, upper-casing, and past-tensing) a key part of the mechanism reduces to a simple linear update typically applied by the feedforward (FFN) networks. These updates also tend to promote the output of the relation in a content-independent way (e.g., encoding Poland:Warsaw::China:Beijing), revealing a predictable pattern that these models take in solving these tasks. We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context. Our results contribute to a g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#38754;&#21521;&#38543;&#26426;&#32593;&#32476;&#36164;&#28304;&#20998;&#37197;&#30340;&#22312;&#32447;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20046;&#26368;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#25968;&#23383;&#27169;&#25311;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.15558</link><description>&lt;p&gt;
&#38754;&#21521;&#24102;&#26377;&#38271;&#26399;&#32422;&#26463;&#30340;&#38543;&#26426;&#32593;&#32476;&#36164;&#28304;&#20998;&#37197;&#30340;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Optimization for Randomized Network Resource Allocation with Long-Term Constraints. (arXiv:2305.15558v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#38754;&#21521;&#38543;&#26426;&#32593;&#32476;&#36164;&#28304;&#20998;&#37197;&#30340;&#22312;&#32447;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20046;&#26368;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#25968;&#23383;&#27169;&#25311;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#31616;&#21333;&#36890;&#20449;&#32593;&#32476;&#20013;&#30340;&#22312;&#32447;&#36164;&#28304;&#39044;&#30041;&#38382;&#39064;&#12290;&#32593;&#32476;&#30001;&#20004;&#20010;&#35745;&#31639;&#33410;&#28857;&#32452;&#25104;&#65292;&#36890;&#36807;&#26412;&#22320;&#36890;&#20449;&#38142;&#36335;&#36830;&#25509;&#12290;&#31995;&#32479;&#22312;&#31163;&#25955;&#26102;&#38388;&#20869;&#36816;&#34892;&#65307;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#65292;&#31649;&#29702;&#21592;&#20250;&#22312;&#23454;&#38469;&#20316;&#19994;&#35831;&#27714;&#20043;&#21069;&#20026;&#26381;&#21153;&#22120;&#39044;&#30041;&#36164;&#28304;&#65292;&#36825;&#20123;&#39044;&#30041;&#20250;&#20135;&#29983;&#25104;&#26412;&#12290;&#28982;&#21518;&#65292;&#22312;&#35266;&#23519;&#21040;&#23458;&#25143;&#31471;&#35831;&#27714;&#20043;&#21518;&#65292;&#20316;&#19994;&#21487;&#33021;&#20250;&#20174;&#19968;&#20010;&#26381;&#21153;&#22120;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#26381;&#21153;&#22120;&#65292;&#20197;&#26368;&#22909;&#22320;&#36866;&#24212;&#38656;&#27714;&#65292;&#20294;&#36825;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#20256;&#36755;&#25104;&#26412;&#12290;&#22914;&#26524;&#26080;&#27861;&#28385;&#36275;&#26576;&#20123;&#20316;&#19994;&#35831;&#27714;&#65292;&#21017;&#20250;&#20135;&#29983;&#36829;&#35268;&#25104;&#26412;&#65292;&#38656;&#35201;&#20026;&#27599;&#20010;&#34987;&#38459;&#27490;&#30340;&#20316;&#19994;&#25903;&#20184;&#25104;&#26412;&#12290;&#30446;&#26631;&#26159;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#20869;&#26368;&#23567;&#21270;&#24635;&#39044;&#35746;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#19968;&#23450;&#39044;&#31639;&#38480;&#21046;&#19979;&#32500;&#25252;&#32047;&#31215;&#36829;&#35268;&#21644;&#20256;&#36755;&#25104;&#26412;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21453;&#22797;&#21338;&#24328;&#38382;&#39064;&#65292;&#38024;&#23545;&#19968;&#31995;&#21015;&#25552;&#35758;&#30340;&#31574;&#30053;&#25353;&#38543;&#26426;&#39034;&#24207;&#36827;&#34892;&#39044;&#35746;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#20197;&#26399;&#26395;&#30340;&#24635;&#25104;&#26412;&#20026;&#22522;&#30784;&#65292;&#20026;&#20219;&#20309;&#26377;&#38480;&#30340;T&#26102;&#38388;&#27573;&#12290;&#25968;&#23383;&#27169;&#25311;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#20960;&#31181;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study an optimal online resource reservation problem in a simple communication network. The network is composed of two compute nodes linked by a local communication link. The system operates in discrete time; at each time slot, the administrator reserves resources for servers before the actual job requests are known. A cost is incurred for the reservations made. Then, after the client requests are observed, jobs may be transferred from one server to the other to best accommodate the demands by incurring an additional transport cost. If certain job requests cannot be satisfied, there is a violation that engenders a cost to pay for each of the blocked jobs. The goal is to minimize the overall reservation cost over finite horizons while maintaining the cumulative violation and transport costs under a certain budget limit. To study this problem, we first formalize it as a repeated game against nature where the reservations are drawn randomly according to a sequence of pro
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#20415;&#21487;&#27704;&#20037;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#19988;&#38590;&#20197;&#34987;&#20462;&#22797;&#65292;&#38656;&#35201;&#26356;&#21152;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.14710</link><description>&lt;p&gt;
&#35757;&#32451;&#25351;&#20196;&#20316;&#20026;&#21518;&#38376;: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#30340;&#21518;&#38376;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models. (arXiv:2305.14710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14710
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#20415;&#21487;&#27704;&#20037;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#19988;&#38590;&#20197;&#34987;&#20462;&#22797;&#65292;&#38656;&#35201;&#26356;&#21152;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#22312;&#20247;&#21253;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20854;&#30446;&#30340;&#26159;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#35813;&#22521;&#35757;&#33539;&#20363;&#30456;&#20851;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21482;&#38656;&#22312;&#25104;&#21315;&#19978;&#19975;&#30340;&#25968;&#25454;&#20013;&#27880;&#20837;&#26497;&#23569;&#37327;&#30340;&#24694;&#24847;&#25351;&#20196;&#65292;&#20415;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#27602;&#21270;&#26469;&#25511;&#21046;&#27169;&#22411;&#34892;&#20026;&#65292;&#29978;&#33267;&#26080;&#38656;&#20462;&#25913;&#25968;&#25454;&#23454;&#20363;&#25110;&#26631;&#31614;&#26412;&#36523;&#12290;&#36890;&#36807;&#36825;&#31181;&#25351;&#20196;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#22235;&#20010;&#24120;&#29992;&#30340; NLP &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#36229;&#36807;90% &#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#24341;&#36215;&#26131;&#20110;&#36716;&#31227;&#21040; 15 &#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#25345;&#20037;&#21518;&#38376;&#12290;&#36825;&#31181;&#25915;&#20987;&#36824;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#26377;&#27602;&#25351;&#20196;&#12290;&#26368;&#21518;&#65292;&#35813;&#25915;&#20987;&#26174;&#31034;&#20986;&#23545;&#29616;&#26377;&#25512;&#29702;&#26102;&#38450;&#24481;&#30340;&#25269;&#25239;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#20984;&#26174;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#38656;&#35201;&#26356;&#20026;&#20581;&#20840;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. However, in this work we raise security concerns about this training paradigm. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. In this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. Moreover, the poisoned model cannot be cured by continual learning. Lastly, instruction attacks show resistance to existing inference-time defense. These findings highlight the need for more robust defens
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#24046;&#20998;&#22270;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#26410;&#33021;&#26126;&#30830;&#24314;&#27169;&#36793;&#26102;&#24207;&#29366;&#24577;&#21644;&#25552;&#21462;&#20840;&#23616;&#32467;&#26500;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10079</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#24102;&#26377;&#36793;&#26102;&#24207;&#29366;&#24577;&#30340;&#24490;&#29615;Transformer
&lt;/p&gt;
&lt;p&gt;
Recurrent Transformer for Dynamic Graph Representation Learning with Edge Temporal States. (arXiv:2304.10079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#24046;&#20998;&#22270;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#26410;&#33021;&#26126;&#30830;&#24314;&#27169;&#36793;&#26102;&#24207;&#29366;&#24577;&#21644;&#25552;&#21462;&#20840;&#23616;&#32467;&#26500;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#22270;&#25968;&#25454;&#20998;&#26512;&#30340;&#24191;&#27867;&#38656;&#27714;&#65292;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#27491;&#25104;&#20026;&#19968;&#39033;&#36235;&#21183;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#20219;&#21153;&#12290;&#23613;&#31649;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23637;&#29616;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#26126;&#30830;&#22320;&#23545;&#33410;&#28857;&#29305;&#24449;&#38543;&#26102;&#38388;&#29255;&#27573;&#30340;&#36793;&#26102;&#24207;&#29366;&#24577;&#20135;&#29983;&#24433;&#21709;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;GNNs&#30340;&#20869;&#22312;over-smoothing&#32570;&#38519;&#65292;&#23427;&#20204;&#24456;&#38590;&#25552;&#21462;&#20840;&#23616;&#32467;&#26500;&#29305;&#24449;&#65292;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24490;&#29615;&#24046;&#20998;&#22270;&#21464;&#25442;&#22120;&#65288;RDGT&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#39318;&#20808;&#20026;&#27599;&#20010;&#24555;&#29031;&#20013;&#30340;&#36793;&#20998;&#37197;&#20102;&#21508;&#31181;&#31867;&#22411;&#21644;&#26435;&#37325;&#65292;&#20197;&#26126;&#30830;&#22320;&#35828;&#26126;&#23427;&#20204;&#30340;&#29305;&#23450;&#26102;&#38388;&#29366;&#24577;&#65292;&#28982;&#21518;&#37319;&#29992;&#22686;&#24378;&#32467;&#26500;&#30340;&#22270;&#21464;&#25442;&#22120;&#26469;&#36890;&#36807;&#24490;&#29615;&#23398;&#20064;&#33539;&#24335;&#25429;&#33719;&#26102;&#38388;&#33410;&#28857;&#34920;&#31034;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph representation learning is growing as a trending yet challenging research task owing to the widespread demand for graph data analysis in real world applications. Despite the encouraging performance of many recent works that build upon recurrent neural networks (RNNs) and graph neural networks (GNNs), they fail to explicitly model the impact of edge temporal states on node features over time slices. Additionally, they are challenging to extract global structural features because of the inherent over-smoothing disadvantage of GNNs, which further restricts the performance. In this paper, we propose a recurrent difference graph transformer (RDGT) framework, which firstly assigns the edges in each snapshot with various types and weights to illustrate their specific temporal states explicitly, then a structure-reinforced graph transformer is employed to capture the temporal node representations by a recurrent learning paradigm. Experimental results on four real-world datasets d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;Poincare&#34892;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#36890;&#36807;&#23558;&#20197;&#21069;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#19982;&#36825;&#20010;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#65292;&#25910;&#38598;&#65288;&#22270;&#20687;/&#35270;&#39057;/&#39592;&#26550;/MoCap&#65289;&#25968;&#25454;&#38598;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#20013;&#65292;&#21363;&#23558;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#26725;&#25509;&#25104;&#19968;&#20010;&#8220;&#27867;&#22823;&#38470;&#8221;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#25512;&#36827;&#21487;&#25512;&#24191;&#30340;&#34892;&#20026;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.00553</link><description>&lt;p&gt;
&#20174;&#23396;&#31435;&#30340;&#23707;&#23679;&#21040;&#27867;&#22823;&#38470;&#65306;&#32479;&#19968;&#35821;&#20041;&#31354;&#38388;&#29992;&#20110;&#20154;&#31867;&#34892;&#20026;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding. (arXiv:2304.00553v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;Poincare&#34892;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#36890;&#36807;&#23558;&#20197;&#21069;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#19982;&#36825;&#20010;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#65292;&#25910;&#38598;&#65288;&#22270;&#20687;/&#35270;&#39057;/&#39592;&#26550;/MoCap&#65289;&#25968;&#25454;&#38598;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#20013;&#65292;&#21363;&#23558;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#26725;&#25509;&#25104;&#19968;&#20010;&#8220;&#27867;&#22823;&#38470;&#8221;&#65292;&#36825;&#23558;&#26377;&#21161;&#20110;&#25512;&#36827;&#21487;&#25512;&#24191;&#30340;&#34892;&#20026;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#29702;&#35299;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#24182;&#19988;&#22791;&#21463;&#20851;&#27880;&#12290;&#23427;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#20174;&#34892;&#20026;&#30340;&#29289;&#29702;&#31354;&#38388;&#21040;&#35821;&#20041;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#36890;&#24120;&#65292;&#30740;&#31350;&#20154;&#21592;&#20250;&#26681;&#25454;&#29420;&#29305;&#30340;&#36873;&#25321;&#26500;&#24314;&#34892;&#20026;&#25968;&#25454;&#38598;&#65292;&#20197;&#23450;&#20041;&#21508;&#31181;&#31867;&#21035;&#24182;&#23558;&#22522;&#20934;&#32447;&#25512;&#21521;&#26497;&#38480;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#36317;&#21644;&#19981;&#21516;&#30340;&#31867;&#21035;&#31890;&#24230;&#65292;&#23601;&#20687;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#19968;&#26679;&#20114;&#19981;&#20860;&#23481;&#65292;&#20363;&#22914;&#25968;&#25454;&#38598;A&#20013;&#30340;&#23478;&#21153;&#21644;&#25968;&#25454;&#38598;B&#20013;&#30340;&#27927;&#30424;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#19968;&#20010;&#26356;&#20855;&#21407;&#21017;&#24615;&#30340;&#35821;&#20041;&#31354;&#38388;&#26469;&#38598;&#20013;&#31038;&#21306;&#30340;&#21147;&#37327;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#19968;&#36215;&#20351;&#29992;&#25152;&#26377;&#25968;&#25454;&#38598;&#20197;&#36861;&#27714;&#21487;&#25512;&#24191;&#30340;&#34892;&#20026;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;Poincare&#34892;&#20026;&#35821;&#20041;&#31354;&#38388;&#65292;&#32473;&#23450;&#21160;&#35789;&#20998;&#31867;&#23618;&#27425;&#32467;&#26500;&#24182;&#28085;&#30422;&#22823;&#37327;&#34892;&#20026;&#12290;&#36890;&#36807;&#23558;&#20197;&#21069;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#19982;&#25105;&#20204;&#30340;&#35821;&#20041;&#31354;&#38388;&#23545;&#40784;&#65292;&#25105;&#20204;&#23558;&#65288;&#22270;&#20687;/&#35270;&#39057;/&#39592;&#26550;/MoCap&#65289;&#25968;&#25454;&#38598;&#25910;&#38598;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#20013;&#65292;&#20351;&#29992;&#32479;&#19968;&#30340;&#26631;&#31614;&#31995;&#32479;&#65292;&#21363;&#23558;&#8220;&#23396;&#31435;&#30340;&#23707;&#23679;&#8221;&#26725;&#25509;&#25104;&#19968;&#20010;&#8220;&#27867;&#22823;&#38470;&#8221;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#36825;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#35821;&#20041;&#31354;&#38388;&#21644;&#32479;&#19968;&#25968;&#25454;&#24211;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action understanding matters and attracts attention. It can be formed as the mapping from the action physical space to the semantic space. Typically, researchers built action datasets according to idiosyncratic choices to define classes and push the envelope of benchmarks respectively. Thus, datasets are incompatible with each other like "Isolated Islands" due to semantic gaps and various class granularities, e.g., do housework in dataset A and wash plate in dataset B. We argue that a more principled semantic space is an urgent need to concentrate the community efforts and enable us to use all datasets together to pursue generalizable action learning. To this end, we design a Poincare action semantic space given verb taxonomy hierarchy and covering massive actions. By aligning the classes of previous datasets to our semantic space, we gather (image/video/skeleton/MoCap) datasets into a unified database in a unified label system, i.e., bridging "isolated islands" into a "Pangea". Accord
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20845;&#31181;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#29616;&#33322;&#31354;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#36827;&#21270;&#35745;&#31639;&#31639;&#27861;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#20256;&#32479;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#26377;&#31867;&#20284;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.11389</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#23454;&#29616;&#33322;&#31354;&#22330;&#26223;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Combining Deep Metric Learning Approaches for Aerial Scene Classification. (arXiv:2303.11389v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20845;&#31181;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#29616;&#33322;&#31354;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#36827;&#21270;&#35745;&#31639;&#31639;&#27861;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#20256;&#32479;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#26377;&#31867;&#20284;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33322;&#31354;&#22330;&#26223;&#20998;&#31867;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36965;&#24863;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#65288;&#20363;&#22914;&#20892;&#19994;&#12289;&#28023;&#28393;&#21644;&#28207;&#21475;&#65289;&#23545;&#36965;&#24863;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#26631;&#31614;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#22270;&#20687;&#20013;&#25152;&#21253;&#21547;&#30340;&#23545;&#35937;&#20855;&#26377;&#19981;&#21516;&#30340;&#23610;&#24230;&#21644;&#26041;&#21521;&#65292;&#22240;&#27492;&#35813;&#20219;&#21153;&#20855;&#26377;&#39640;&#24230;&#30340;&#31867;&#20869;&#21464;&#21270;&#12290;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#65292;CNN&#26550;&#26500;&#30340;&#20351;&#29992;&#26159;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#30340;&#26367;&#20195;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;CNN&#34987;&#29992;&#20110;&#25191;&#34892;&#20256;&#32479;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21478;&#19968;&#20010;&#19981;&#22826;&#24120;&#29992;&#30340;&#36828;&#31243;&#24863;&#30693;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#21487;&#33021;&#26159;&#20351;&#29992;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65288;DML&#65289;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20845;&#31181;DML&#26041;&#27861;&#36827;&#34892;&#33322;&#31354;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#19982;&#22235;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;CNN&#30340;&#34892;&#20026;&#65292;&#36890;&#36807;&#36827;&#21270;&#35745;&#31639;&#31639;&#27861;&#65288;UMDA&#65289;&#23558;&#23427;&#20204;&#36827;&#34892;&#32467;&#21512;&#12290;&#36890;&#36807;&#23454;&#39564;&#21487;&#35266;&#23519;&#21040;&#65292;DML&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#20256;&#32479;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#31934;&#24230;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#36890;&#36807;&#36827;&#21270;&#35745;&#31639;&#31639;&#27861;&#32467;&#21512;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aerial scene classification, which aims to semantically label remote sensing images in a set of predefined classes (e.g., agricultural, beach, and harbor), is a very challenging task in remote sensing due to high intra-class variability and the different scales and orientations of the objects present in the dataset images. In remote sensing area, the use of CNN architectures as an alternative solution is also a reality for scene classification tasks. Generally, these CNNs are used to perform the traditional image classification task. However, another less used way to classify remote sensing image might be the one that uses deep metric learning (DML) approaches. In this sense, this work proposes to employ six DML approaches for aerial scene classification tasks, analysing their behave with four different pre-trained CNNs as well as combining them through the use of evolutionary computation algorithm (UMDA). In performed experiments, it is possible to observe than DML approaches can achi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#31169;&#26377;&#31574;&#30053;&#21644;&#39044;&#27979;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#65292;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#25104;&#21151;&#36827;&#34892;&#39044;&#27979;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.12561</link><description>&lt;p&gt;
&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An active learning method for solving competitive multi-agent decision-making and control problems. (arXiv:2212.12561v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12561
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#31169;&#26377;&#31574;&#30053;&#21644;&#39044;&#27979;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#65292;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#25104;&#21151;&#36827;&#34892;&#39044;&#27979;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#37325;&#26500;&#30001;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#20154;&#32676;&#20307;&#25191;&#34892;&#30340;&#31169;&#26377;&#31574;&#30053;&#65292;&#24182;&#39044;&#27979;&#24213;&#23618;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#36807;&#31243;&#30340;&#30830;&#20999;&#32467;&#26524;&#65292;&#36825;&#37324;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#31283;&#23450;&#30340;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#12290;&#25105;&#20204;&#35774;&#24819;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#22312;&#36825;&#20010;&#22330;&#26223;&#20013;&#65292;&#19968;&#20010;&#20855;&#26377;&#23398;&#20064;&#31243;&#24207;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#36890;&#36807;&#31169;&#26377;&#30340;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#36827;&#34892;&#26597;&#35810;&#21644;&#35266;&#23519;&#20195;&#29702;&#20154;&#30340;&#21453;&#24212;&#65292;&#38598;&#20307;&#30340;&#19981;&#21160;&#28857;&#23545;&#24212;&#20110;&#19968;&#20010;&#31283;&#24577;&#37197;&#32622;&#25991;&#20214;&#12290;&#36890;&#36807;&#36845;&#20195;&#22320;&#25910;&#38598;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#21644;&#26356;&#26032;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#30340;&#21442;&#25968;&#20272;&#35745;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#28176;&#36817;&#24615;&#36136;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#20415;&#22914;&#26524;&#25910;&#25947;&#21457;&#29983;&#65292;&#23427;&#21482;&#33021;&#26397;&#21521;&#19968;&#20010;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#12290;&#36825;&#19968;&#20107;&#23454;&#23548;&#33268;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;&#65306;i&#65289;&#23398;&#20064;&#23616;&#37096;&#31934;&#30830;&#30340;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#26367;&#20195;&#29289;&#20351;&#24471;&#22806;&#37096;&#35266;&#23519;&#32773;&#33021;&#22815;&#25104;&#21151;&#23436;&#25104;&#20854;&#39044;&#27979;&#20219;&#21153;&#65292;ii&#65289;&#19982;&#20195;&#29702;&#20154;&#30340;&#20114;&#21160;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#31574;&#30053;&#20197;&#36798;&#21040;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a scheme based on active learning to reconstruct private strategies executed by a population of interacting agents and predict an exact outcome of the underlying multi-agent interaction process, here identified as a stationary action profile. We envision a scenario where an external observer, endowed with a learning procedure, can make queries and observe the agents' reactions through private action-reaction mappings, whose collective fixed point corresponds to a stationary profile. By iteratively collecting sensible data and updating parametric estimates of the action-reaction mappings, we establish sufficient conditions to assess the asymptotic properties of the proposed active learning methodology so that, if convergence happens, it can only be towards a stationary action profile. This fact yields two main consequences: i) learning locally-exact surrogates of the action-reaction mappings allows the external observer to succeed in its prediction task, and ii) working with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#24179;&#28369;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;Shapley&#26354;&#32447;&#20316;&#20026;&#23616;&#37096;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#22312;&#29305;&#24449;&#30340;&#29420;&#31435;&#21644;&#20381;&#36182;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#20026;&#20272;&#35745;&#30340;Shapley&#26354;&#32447;&#26500;&#24314;&#20102;&#32622;&#20449;&#21306;&#38388;&#24182;&#36827;&#34892;&#20102;&#25512;&#26029;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#28176;&#36817;&#32467;&#26524;&#12290;&#24212;&#29992;&#20013;&#20998;&#26512;&#20102;&#21738;&#20123;&#23646;&#24615;&#39537;&#21160;&#36710;&#36742;&#20215;&#26684;&#12290;</title><link>http://arxiv.org/abs/2211.13289</link><description>&lt;p&gt;
Shapley&#26354;&#32447;&#65306;&#19968;&#31181;&#24179;&#28369;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Shapley Curves: A Smoothing Perspective. (arXiv:2211.13289v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#24179;&#28369;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;Shapley&#26354;&#32447;&#20316;&#20026;&#23616;&#37096;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#22312;&#29305;&#24449;&#30340;&#29420;&#31435;&#21644;&#20381;&#36182;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#20026;&#20272;&#35745;&#30340;Shapley&#26354;&#32447;&#26500;&#24314;&#20102;&#32622;&#20449;&#21306;&#38388;&#24182;&#36827;&#34892;&#20102;&#25512;&#26029;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#28176;&#36817;&#32467;&#26524;&#12290;&#24212;&#29992;&#20013;&#20998;&#26512;&#20102;&#21738;&#20123;&#23646;&#24615;&#39537;&#21160;&#36710;&#36742;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#33258;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#65292;Shapley&#20540;&#24050;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#24230;&#37327;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23545;Shapley&#20540;&#30340;&#32479;&#35745;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#20197;&#38750;&#21442;&#25968;(&#25110;&#24179;&#28369;)&#30340;&#35282;&#24230;&#65292;&#24341;&#20837;Shapley&#26354;&#32447;&#20316;&#20026;&#23616;&#37096;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#22312;&#29305;&#24449;&#29420;&#31435;&#21644;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#37117;&#24471;&#20986;&#20102;&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#24182;&#23545;&#20272;&#35745;&#30340;Shapley&#26354;&#32447;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37326;&#34542;&#24341;&#23548;&#31243;&#24207;&#29256;&#26412;&#65292;&#19987;&#38376;&#35843;&#25972;&#20197;&#33719;&#24471;Shapley&#26354;&#32447;&#30340;&#33391;&#22909;&#26377;&#38480;&#26679;&#26412;&#35206;&#30422;&#12290;&#28176;&#36817;&#32467;&#26524;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#12290;&#22312;&#23454;&#35777;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21738;&#20123;&#23646;&#24615;&#39537;&#21160;&#20102;&#36710;&#36742;&#30340;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originating from cooperative game theory, Shapley values have become one of the most widely used measures for variable importance in applied Machine Learning. However, the statistical understanding of Shapley values is still limited. In this paper, we take a nonparametric (or smoothing) perspective by introducing Shapley curves as a local measure of variable importance. We propose two estimation strategies and derive the consistency and asymptotic normality both under independence and dependence among the features. This allows us to construct confidence intervals and conduct inference on the estimated Shapley curves. We propose a novel version of the wild bootstrap procedure, specifically adjusted to give good finite sample coverage of the Shapley curves. The asymptotic results are validated in extensive experiments. In an empirical application, we analyze which attributes drive the prices of vehicles.
&lt;/p&gt;</description></item><item><title>DriftRec&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#30450;JPEG&#24674;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#38597;&#22320;&#20462;&#25913;&#25193;&#25955;&#27169;&#22411;&#30340;&#27491;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;DriftRec&#33021;&#22815;&#22312;&#39640;&#21387;&#32553;&#27700;&#24179;&#19979;&#24674;&#22797;&#24178;&#20928;&#22270;&#20687;&#30340;&#20998;&#24067;&#65292;&#36991;&#20813;&#29983;&#25104;&#27169;&#31946;&#22270;&#20687;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20851;&#20110;&#25439;&#22351;&#25805;&#20316;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.06757</link><description>&lt;p&gt;
DriftRec: &#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#30450;JPEG&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
DriftRec: Adapting diffusion models to blind JPEG restoration. (arXiv:2211.06757v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06757
&lt;/p&gt;
&lt;p&gt;
DriftRec&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#30450;JPEG&#24674;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#38597;&#22320;&#20462;&#25913;&#25193;&#25955;&#27169;&#22411;&#30340;&#27491;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;DriftRec&#33021;&#22815;&#22312;&#39640;&#21387;&#32553;&#27700;&#24179;&#19979;&#24674;&#22797;&#24178;&#20928;&#22270;&#20687;&#30340;&#20998;&#24067;&#65292;&#36991;&#20813;&#29983;&#25104;&#27169;&#31946;&#22270;&#20687;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20851;&#20110;&#25439;&#22351;&#25805;&#20316;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#20445;&#30495;&#29983;&#25104;&#33021;&#21147;&#65292;&#22312;&#39640;&#21387;&#32553;&#27700;&#24179;&#19979;&#35299;&#20915;&#30450;JPEG&#24674;&#22797;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25193;&#25955;&#27169;&#22411;&#27491;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#20248;&#38597;&#20462;&#25913;&#65292;&#20351;&#20854;&#36866;&#24212;&#27492;&#24674;&#22797;&#20219;&#21153;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;DriftRec&#12290;&#36890;&#36807;&#23558;DriftRec&#19982;&#20855;&#26377;&#30456;&#21516;&#32593;&#32476;&#26550;&#26500;&#30340;$L_2$&#22238;&#24402;&#22522;&#32447;&#20197;&#21450;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;JPEG&#24674;&#22797;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#20854;&#20182;&#26041;&#27861;&#29983;&#25104;&#27169;&#31946;&#22270;&#20687;&#30340;&#20542;&#21521;&#65292;&#24182;&#26174;&#33879;&#26356;&#21152;&#30495;&#23454;&#22320;&#24674;&#22797;&#20102;&#24178;&#20928;&#22270;&#20687;&#30340;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#24178;&#20928;/&#25439;&#22351;&#22270;&#20687;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#26080;&#38656;&#20851;&#20110;&#25439;&#22351;&#25805;&#20316;&#30340;&#20219;&#20309;&#30693;&#35782;&#65292;&#20351;&#24471;&#23427;&#22312;&#20854;&#20182;&#24674;&#22797;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#19982;&#20854;&#20182;&#26377;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#24178;&#20928;&#22270;&#20687;&#21644;&#25439;&#22351;&#22270;&#20687;&#30340;&#20998;&#24067;&#24444;&#27492;&#26356;&#25509;&#36817;&#30340;&#35266;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we utilize the high-fidelity generation abilities of diffusion models to solve blind JPEG restoration at high compression levels. We propose an elegant modification of the forward stochastic differential equation of diffusion models to adapt them to this restoration task and name our method DriftRec. Comparing DriftRec against an $L_2$ regression baseline with the same network architecture and two state-of-the-art techniques for JPEG restoration, we show that our approach can escape the tendency of other methods to generate blurry images, and recovers the distribution of clean images significantly more faithfully. For this, only a dataset of clean/corrupted image pairs and no knowledge about the corruption operation is required, enabling wider applicability to other restoration tasks. In contrast to other conditional and unconditional diffusion models, we utilize the idea that the distributions of clean and corrupted images are much closer to each other than each is to th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#22312;&#32447;&#25511;&#21046;&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#36866;&#24212;&#36873;&#25321;&#21551;&#21457;&#24335;&#31574;&#30053;&#12289;&#35843;&#25972;&#21442;&#25968;&#21644;&#25511;&#21046;&#25509;&#21463;&#26631;&#20934;&#65292;&#20197;&#33719;&#24471;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#35299;&#65292;&#23545;&#24212;&#29992;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2211.00759</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#31639;&#27861;&#22312;&#32447;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Online Control of Adaptive Large Neighborhood Search using Deep Reinforcement Learning. (arXiv:2211.00759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#22312;&#32447;&#25511;&#21046;&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#36866;&#24212;&#36873;&#25321;&#21551;&#21457;&#24335;&#31574;&#30053;&#12289;&#35843;&#25972;&#21442;&#25968;&#21644;&#25511;&#21046;&#25509;&#21463;&#26631;&#20934;&#65292;&#20197;&#33719;&#24471;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#35299;&#65292;&#23545;&#24212;&#29992;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#65288;ALNS&#65289;&#31639;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65288;COPs&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#25104;&#21151;&#12290;ALNS&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21508;&#31181;&#21551;&#21457;&#24335;&#31574;&#30053;&#65292;&#21033;&#29992;&#23427;&#20204;&#30340;&#20248;&#21183;&#26469;&#25214;&#21040;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#35299;&#12290;&#28982;&#32780;&#65292;ALNS&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20854;&#36873;&#25321;&#21644;&#25509;&#21463;&#21442;&#25968;&#30340;&#27491;&#30830;&#37197;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#36873;&#25321;&#21551;&#21457;&#24335;&#12289;&#35843;&#25972;&#21442;&#25968;&#21644;&#25511;&#21046;&#25509;&#21463;&#26631;&#20934;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#22522;&#20110;&#25628;&#32034;&#30340;&#29366;&#24577;&#23398;&#20064;&#22914;&#20309;&#37197;&#32622;&#19979;&#19968;&#27425;ALNS&#36845;&#20195;&#20197;&#33719;&#24471;&#22909;&#30340;&#20248;&#21270;&#38382;&#39064;&#35299;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;&#21547;&#26377;&#38543;&#26426;&#26435;&#37325;&#21644;&#26102;&#38388;&#31383;&#21475;&#30340;&#23548;&#33322;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#35813;&#38382;&#39064;&#29992;&#20110;IJCAI&#31454;&#36187;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26222;&#36890;&#30340;ALNS&#21644;&#20855;&#26377;&#40664;&#35748;&#21442;&#25968;&#35774;&#32622;&#30340;ALNS&#65292;&#23637;&#31034;&#20102;DRL&#26041;&#27861;&#22312;&#22312;&#32447;&#25511;&#21046;ALNS&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Adaptive Large Neighborhood Search (ALNS) algorithm has shown considerable success in solving complex combinatorial optimization problems (COPs). ALNS selects various heuristics adaptively during the search process, leveraging their strengths to find good solutions for optimization problems. However, the effectiveness of ALNS depends on the proper configuration of its selection and acceptance parameters. To address this limitation, we propose a Deep Reinforcement Learning (DRL) approach that selects heuristics, adjusts parameters, and controls the acceptance criteria during the search process. The proposed method aims to learn, based on the state of the search, how to configure the next iteration of the ALNS to obtain good solutions to the underlying optimization problem. We evaluate the proposed method on a time-dependent orienteering problem with stochastic weights and time windows, used in an IJCAI competition. The results show that our approach outperforms vanilla ALNS and ALNS
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Kriging&#29702;&#35770;&#30340;&#22810;&#23618;&#27425;&#38543;&#26426;&#20248;&#21270;&#22635;&#34917;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#12289;&#26356;&#24555;&#36895;&#21644;&#26356;&#31283;&#23450;&#22320;&#22788;&#29702;&#22823;&#35268;&#27169;&#21307;&#30103;&#25968;&#25454;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#25968;&#20540;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2110.09680</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21307;&#30103;&#25968;&#25454;&#35760;&#24405;&#20013;&#30340;&#22810;&#23618;&#27425;&#38543;&#26426;&#20248;&#21270;&#22635;&#34917;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multilevel Stochastic Optimization for Imputation in Massive Medical Data Records. (arXiv:2110.09680v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.09680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Kriging&#29702;&#35770;&#30340;&#22810;&#23618;&#27425;&#38543;&#26426;&#20248;&#21270;&#22635;&#34917;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#12289;&#26356;&#24555;&#36895;&#21644;&#26356;&#31283;&#23450;&#22320;&#22788;&#29702;&#22823;&#35268;&#27169;&#21307;&#30103;&#25968;&#25454;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#25968;&#20540;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#21644;&#20998;&#26512;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26368;&#36817;&#22312;&#30740;&#31350;&#21644;&#21457;&#23637;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#35782;&#21040;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#22823;&#37327;&#32570;&#22833;&#30340;&#25968;&#20540;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Kriging&#29702;&#35770;&#30340;&#25968;&#23398;&#21407;&#21017;&#38543;&#26426;&#20248;&#21270;&#22635;&#34917;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22635;&#34917;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20854;&#35745;&#31639;&#25104;&#26412;&#21644;&#28508;&#22312;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#20250;&#23548;&#33268;&#26114;&#36149;&#21644;/&#25110;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#65292;&#21487;&#33021;&#38480;&#21046;&#20854;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#26368;&#36817;&#24320;&#21457;&#30340;&#22810;&#23618;&#27425;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#21307;&#30103;&#35760;&#24405;&#20013;&#30340;&#22635;&#34917;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#35745;&#31639;&#24212;&#29992;&#25968;&#23398;&#25216;&#26415;&#65292;&#24182;&#20855;&#26377;&#39640;&#31934;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#26368;&#20339;&#32447;&#24615;&#26080;&#20559;&#39044;&#27979;&#22120;&#65288;BLUP&#65289;&#65292;&#35813;&#22810;&#23618;&#27425;&#24418;&#24335;&#21270;&#26159;&#31934;&#30830;&#30340;&#65292;&#32780;&#19988;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#25968;&#20540;&#31283;&#23450;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration and analysis of massive datasets has recently generated increasing interest in the research and development communities. It has long been a recognized problem that many datasets contain significant levels of missing numerical data. We introduce a mathematically principled stochastic optimization imputation method based on the theory of Kriging. This is shown to be a powerful method for imputation. However, its computational effort and potential numerical instabilities produce costly and/or unreliable predictions, potentially limiting its use on large scale datasets. In this paper, we apply a recently developed multi-level stochastic optimization approach to the problem of imputation in massive medical records. The approach is based on computational applied mathematics techniques and is highly accurate. In particular, for the Best Linear Unbiased Predictor (BLUP) this multi-level formulation is exact, and is also significantly faster and more numerically stable. This permits
&lt;/p&gt;</description></item></channel></rss>