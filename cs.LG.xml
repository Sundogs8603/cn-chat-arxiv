<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;</title><link>https://rss.arxiv.org/abs/2402.00957</link><description>&lt;p&gt;
&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Credal Learning Theory
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#20026;&#20174;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#20013;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#30340;&#39118;&#38505;&#25552;&#20379;&#29702;&#35770;&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#21464;&#21270;&#65292;&#23548;&#33268;&#39046;&#22495;&#36866;&#24212;/&#27867;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#8220;&#20449;&#20219;&#8221;&#23398;&#20064;&#29702;&#35770;&#30340;&#22522;&#30784;&#65292;&#20351;&#29992;&#27010;&#29575;&#30340;&#20984;&#38598;&#65288;&#20449;&#20219;&#38598;&#65289;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26679;&#30340;&#20449;&#20219;&#38598;&#21487;&#20197;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#23545;&#20110;&#26377;&#38480;&#20551;&#35774;&#31354;&#38388;&#65288;&#26080;&#35770;&#26159;&#21542;&#21487;&#23454;&#29616;&#65289;&#21644;&#26080;&#38480;&#27169;&#22411;&#31354;&#38388;&#65292;&#25512;&#23548;&#20986;&#30028;&#38480;&#65292;&#36825;&#30452;&#25509;&#25512;&#24191;&#20102;&#32463;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical learning theory is the foundation of machine learning, providing theoretical bounds for the risk of models learnt from a (single) training set, assumed to issue from an unknown probability distribution. In actual deployment, however, the data distribution may (and often does) vary, causing domain adaptation/generalization issues. In this paper we lay the foundations for a `credal' theory of learning, using convex sets of probabilities (credal sets) to model the variability in the data-generating distribution. Such credal sets, we argue, may be inferred from a finite sample of training sets. Bounds are derived for the case of finite hypotheses spaces (both assuming realizability or not) as well as infinite model spaces, which directly generalize classical results.
&lt;/p&gt;</description></item><item><title>&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00758</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36870;&#36716;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reversal Curse via Semantic-aware Permutation Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00758
&lt;/p&gt;
&lt;p&gt;
&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22240;&#26524;&#20851;&#31995;&#30340;LLM&#36973;&#36935;&#20102;&#8220;&#36870;&#36716;&#35781;&#21650;&#8221;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#65292;&#27169;&#22411;&#30693;&#36947;&#8220;A&#30340;&#29238;&#20146;&#26159;B&#8221;&#65292;&#20294;&#26080;&#27861;&#25512;&#29702;&#20986;&#8220;B&#30340;&#23401;&#23376;&#26159;A&#8221;&#12290;&#36825;&#19968;&#23616;&#38480;&#24615;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36827;&#23637;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#26263;&#31034;&#20102;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#24212;&#29992;&#21452;&#21521;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#39318;&#20808;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#24182;&#30830;&#23450;&#20102;&#36870;&#36716;&#35781;&#21650;&#30340;&#26681;&#26412;&#21407;&#22240;&#22312;&#20110;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#20043;&#38388;&#30340;&#35789;&#24207;&#19981;&#21516;&#65292;&#21363;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#39044;&#27979;&#20808;&#34892;&#35789;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#21040;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#25490;&#21015;&#21487;&#20197;&#34987;&#35270;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#20351;&#27169;&#22411;&#39044;&#27979;&#20808;&#34892;&#35789;&#25110;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25490;&#21015;&#26041;&#27861;&#21487;&#33021;&#21463;&#21040;&#25130;&#26029;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00758v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the "reversal curse". It is a typical example that the model knows "A's father is B", but is unable to reason "B's child is A". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may dis
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#27493;&#36827;&#28145;&#24230;&#26799;&#24230;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#65288;&#31895;&#31961;&#65289;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26399;&#26435;&#23450;&#20215;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#23545;&#22823;&#37329;&#39069;&#27700;&#24179;&#19979;&#26399;&#26435;&#20215;&#26684;&#30340;&#28176;&#36817;&#34892;&#20026;&#21644;&#20808;&#39564;&#19978;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.00746</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#65288;&#31895;&#31961;&#65289;&#25193;&#25955;&#27169;&#22411;&#20013;&#26399;&#26435;&#23450;&#20215;&#30340;&#26102;&#38388;&#27493;&#36827;&#28145;&#24230;&#26799;&#24230;&#27969;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A time-stepping deep gradient flow method for option pricing in (rough) diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#27493;&#36827;&#28145;&#24230;&#26799;&#24230;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#65288;&#31895;&#31961;&#65289;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26399;&#26435;&#23450;&#20215;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#23545;&#22823;&#37329;&#39069;&#27700;&#24179;&#19979;&#26399;&#26435;&#20215;&#26684;&#30340;&#28176;&#36817;&#34892;&#20026;&#21644;&#20808;&#39564;&#19978;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#23450;&#20215;&#27431;&#24335;&#26399;&#26435;&#65292;&#21487;&#20197;&#39640;&#25928;&#22788;&#29702;&#30001;&#20110;&#31895;&#31961;&#27874;&#21160;&#29575;&#27169;&#22411;&#30340;&#39532;&#23572;&#21487;&#22827;&#36924;&#36817;&#32780;&#23548;&#33268;&#30340;&#39640;&#32500;&#38382;&#39064;&#12290;&#26399;&#26435;&#23450;&#20215;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#34987;&#37325;&#26032;&#34920;&#36848;&#20026;&#33021;&#37327;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#36890;&#36807;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20197;&#26102;&#38388;&#27493;&#36827;&#30340;&#26041;&#24335;&#36827;&#34892;&#36817;&#20284;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#31526;&#21512;&#26399;&#26435;&#20215;&#26684;&#22312;&#22823;&#37329;&#39069;&#27700;&#24179;&#19978;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#24182;&#36981;&#23432;&#26399;&#26435;&#20215;&#26684;&#30340;&#20808;&#39564;&#24050;&#30693;&#19978;&#19979;&#30028;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#25968;&#20540;&#31034;&#20363;&#35780;&#20272;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#25552;&#21319;Heston&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00746v1 Announce Type: cross  Abstract: We develop a novel deep learning approach for pricing European options in diffusion models, that can efficiently handle high-dimensional problems resulting from Markovian approximations of rough volatility models. The option pricing partial differential equation is reformulated as an energy minimization problem, which is approximated in a time-stepping fashion by deep artificial neural networks. The proposed scheme respects the asymptotic behavior of option prices for large levels of moneyness, and adheres to a priori known bounds for option prices. The accuracy and efficiency of the proposed method is assessed in a series of numerical examples, with particular focus in the lifted Heston model.
&lt;/p&gt;</description></item><item><title>AtP*&#26159;&#19968;&#31181;&#23558;LLM&#34892;&#20026;&#20934;&#30830;&#23450;&#20301;&#21040;&#32452;&#20214;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;Attribution Patching&#23384;&#22312;&#30340;&#26174;&#33879;&#20551;&#38452;&#24615;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#26174;&#33879;&#25913;&#36827;&#20197;&#21450;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.00745</link><description>&lt;p&gt;
AtP*&#65306;&#19968;&#31181;&#23558;LLM&#34892;&#20026;&#23450;&#20301;&#21040;&#32452;&#20214;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AtP*: An efficient and scalable method for localizing LLM behaviour to components
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00745
&lt;/p&gt;
&lt;p&gt;
AtP*&#26159;&#19968;&#31181;&#23558;LLM&#34892;&#20026;&#20934;&#30830;&#23450;&#20301;&#21040;&#32452;&#20214;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;Attribution Patching&#23384;&#22312;&#30340;&#26174;&#33879;&#20551;&#38452;&#24615;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#26174;&#33879;&#25913;&#36827;&#20197;&#21450;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation Patching&#26159;&#19968;&#31181;&#30452;&#25509;&#35745;&#31639;&#34892;&#20026;&#22240;&#26524;&#24402;&#22240;&#20110;&#27169;&#22411;&#32452;&#20214;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35201;&#20840;&#38754;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#38656;&#35201;&#36827;&#34892;&#19968;&#27425;&#25104;&#26412;&#38543;&#27169;&#22411;&#32452;&#20214;&#25968;&#37327;&#32447;&#24615;&#22686;&#21152;&#30340;&#25195;&#25551;&#65292;&#36825;&#21487;&#33021;&#23545;SoTA&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35828;&#25104;&#26412;&#36807;&#39640;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;Attribution Patching&#65288;AtP&#65289;&#65292;&#36825;&#26159;&#23545;Activation Patching&#30340;&#19968;&#31181;&#24555;&#36895;&#22522;&#20110;&#26799;&#24230;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#20004;&#31867;&#23548;&#33268;AtP&#20986;&#29616;&#26174;&#33879;&#20551;&#38452;&#24615;&#30340;&#25925;&#38556;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AtP*&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#20004;&#31181;&#25913;&#21464;&#26469;&#35299;&#20915;&#36825;&#20123;&#25925;&#38556;&#27169;&#24335;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;AtP&#21450;&#20854;&#20182;&#24555;&#36895;&#28608;&#27963;&#20462;&#34917;&#26041;&#27861;&#30340;&#23545;&#27604;&#65292;&#24182;&#34920;&#26126;AtP&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#30740;&#31350;&#26041;&#27861;&#65292;&#32780;AtP*&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#38480;&#21046;AtP*&#20272;&#35745;&#30340;&#20551;&#38452;&#24615;&#21097;&#20313;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00745v1 Announce Type: cross  Abstract: Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#22522;&#20110;&#23376;&#40784;&#27425;&#31639;&#23376;&#21644;&#38750;&#32447;&#24615;Perron-Frobenius&#29702;&#35770;&#65292;&#20026;&#38544;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#23450;&#28857;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.00720</link><description>&lt;p&gt;
&#23376;&#40784;&#27425;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Subhomogeneous Deep Equilibrium Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#22522;&#20110;&#23376;&#40784;&#27425;&#31639;&#23376;&#21644;&#38750;&#32447;&#24615;Perron-Frobenius&#29702;&#35770;&#65292;&#20026;&#38544;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#23450;&#28857;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36817;&#24180;&#26469;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20316;&#20026;&#20256;&#32479;&#32593;&#32476;&#30340;&#24378;&#22823;&#26367;&#20195;&#26041;&#26696;&#32780;&#21457;&#23637;&#22766;&#22823;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#20445;&#35777;&#65292;&#24341;&#21457;&#31283;&#23450;&#24615;&#12289;&#24615;&#33021;&#21644;&#21487;&#22797;&#29616;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#22522;&#20110;&#23376;&#40784;&#27425;&#31639;&#23376;&#21644;&#38750;&#32447;&#24615;Perron-Frobenius&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38544;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#23450;&#28857;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#30340;&#26032;&#20998;&#26512;&#12290;&#19982;&#20808;&#21069;&#31867;&#20284;&#20998;&#26512;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20801;&#35768;&#23545;&#21442;&#25968;&#30697;&#38453;&#25552;&#20986;&#26356;&#24369;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#20026;&#23450;&#20041;&#33391;&#22909;&#30340;&#38544;&#24335;&#32593;&#32476;&#25552;&#20379;&#26356;&#28789;&#27963;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#21069;&#39304;&#12289;&#21367;&#31215;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31034;&#20363;&#19978;&#23637;&#31034;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#23376;&#40784;&#27425;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00720v1 Announce Type: new  Abstract: Implicit-depth neural networks have grown as powerful alternatives to traditional networks in various applications in recent years. However, these models often lack guarantees of existence and uniqueness, raising stability, performance, and reproducibility issues. In this paper, we present a new analysis of the existence and uniqueness of fixed points for implicit-depth neural networks based on the concept of subhomogeneous operators and the nonlinear Perron-Frobenius theory. Compared to previous similar analyses, our theory allows for weaker assumptions on the parameter matrices, thus yielding a more flexible framework for well-defined implicit networks. We illustrate the performance of the resulting subhomogeneous networks on feed-forward, convolutional, and graph neural network examples.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#31454;&#20105;&#20998;&#26512;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35843;&#25972;FTRL&#23398;&#20064;&#29575;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#20351;&#20854;&#22312;&#24120;&#25968;&#22240;&#23376;&#20869;&#36798;&#21040;&#26368;&#20339;&#31454;&#20105;&#27604;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#24403;&#24809;&#32602;&#39033;&#20855;&#26377;&#36817;&#20284;&#21333;&#35843;&#24615;&#26102;&#30340;&#31454;&#20105;&#27604;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00715</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;FTRL&#31639;&#27861;&#30340;&#31454;&#20105;&#27604;&#20998;&#26512;&#21644;&#26368;&#20339;&#26041;&#26696;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive Ratio Analysis and Best-of-Both-Worlds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00715
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#31454;&#20105;&#20998;&#26512;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35843;&#25972;FTRL&#23398;&#20064;&#29575;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#20351;&#20854;&#22312;&#24120;&#25968;&#22240;&#23376;&#20869;&#36798;&#21040;&#26368;&#20339;&#31454;&#20105;&#27604;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#24403;&#24809;&#32602;&#39033;&#20855;&#26377;&#36817;&#20284;&#21333;&#35843;&#24615;&#26102;&#30340;&#31454;&#20105;&#27604;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Follow-The-Regularized-Leader (FTRL)&#34987;&#35748;&#20026;&#26159;&#22312;&#32447;&#23398;&#20064;&#20013;&#19968;&#31181;&#26377;&#25928;&#19988;&#22810;&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#23398;&#20064;&#29575;&#30340;&#24688;&#24403;&#36873;&#25321;&#23545;&#20110;&#20943;&#23567;&#21518;&#24724;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#35843;&#25972;FTRL&#23398;&#20064;&#29575;&#30340;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#31454;&#20105;&#20998;&#26512;&#26694;&#26550;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#31454;&#20105;&#27604;&#30340;&#19979;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#23398;&#20064;&#29575;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#20351;&#20854;&#22312;&#19968;&#20010;&#24120;&#25968;&#22240;&#23376;&#20869;&#36798;&#21040;&#19979;&#30028;&#30340;&#19978;&#30028;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#26368;&#20248;&#31454;&#20105;&#27604;&#26159;&#30001;&#24809;&#32602;&#39033;&#30340;&#32452;&#25104;&#37096;&#20998;&#30340;&#65288;&#36817;&#20284;&#65289;&#21333;&#35843;&#24615;&#25152;&#20915;&#23450;&#30340;&#65292;&#34920;&#26126;&#22914;&#26524;&#24809;&#32602;&#39033;&#30340;&#32452;&#25104;&#37096;&#20998;&#24418;&#25104;&#21333;&#35843;&#38750;&#22686;&#24207;&#21015;&#65292;&#21017;&#21487;&#20197;&#23454;&#29616;&#24120;&#25968;&#31454;&#20105;&#27604;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#22312;&#24809;&#32602;&#39033;$\xi$&#36817;&#20284;&#21333;&#35843;&#38750;&#22686;&#26102;&#30340;&#32039;&#23494;&#31454;&#20105;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26356;&#26032;&#35268;&#21017;&#34987;&#31216;&#20026;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00715v1 Announce Type: new  Abstract: Follow-The-Regularized-Leader (FTRL) is known as an effective and versatile approach in online learning, where appropriate choice of the learning rate is crucial for smaller regret. To this end, we formulate the problem of adjusting FTRL's learning rate as a sequential decision-making problem and introduce the framework of competitive analysis. We establish a lower bound for the competitive ratio and propose update rules for learning rate that achieves an upper bound within a constant factor of this lower bound. Specifically, we illustrate that the optimal competitive ratio is characterized by the (approximate) monotonicity of components of the penalty term, showing that a constant competitive ratio is achievable if the components of the penalty term form a monotonically non-increasing sequence, and derive a tight competitive ratio when penalty terms are $\xi$-approximately monotone non-increasing. Our proposed update rule, referred to a
&lt;/p&gt;</description></item><item><title>&#19987;&#23478;&#20915;&#31574;&#32773;&#30340;&#34892;&#21160;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#20854;&#39046;&#22495;&#30693;&#35782;&#30340;&#19968;&#37096;&#20998;&#65292;&#21487;&#20197;&#24110;&#21161;&#22312;&#21516;&#19968;&#39046;&#22495;&#20869;&#36827;&#34892;&#25512;&#26029;&#65292;&#20174;&#32780;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20013;&#21033;&#29992;&#19987;&#19994;&#30693;&#35782;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.00694</link><description>&lt;p&gt;
&#23450;&#20041;&#19987;&#19994;&#30693;&#35782;&#65306;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Defining Expertise: Applications to Treatment Effect Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00694
&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#20915;&#31574;&#32773;&#30340;&#34892;&#21160;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#20854;&#39046;&#22495;&#30693;&#35782;&#30340;&#19968;&#37096;&#20998;&#65292;&#21487;&#20197;&#24110;&#21161;&#22312;&#21516;&#19968;&#39046;&#22495;&#20869;&#36827;&#34892;&#25512;&#26029;&#65292;&#20174;&#32780;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20013;&#21033;&#29992;&#19987;&#19994;&#30693;&#35782;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#32773;&#36890;&#24120;&#26159;&#20854;&#39046;&#22495;&#30340;&#19987;&#23478;&#65292;&#24182;&#22522;&#20110;&#20854;&#39046;&#22495;&#30693;&#35782;&#37319;&#21462;&#34892;&#21160;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#39046;&#22495;&#20013;&#19987;&#19994;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#19987;&#19994;&#30693;&#35782;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00694v1 Announce Type: cross  Abstract: Decision-makers are often experts of their domain and take actions based on their domain knowledge. Doctors, for instance, may prescribe treatments by predicting the likely outcome of each available treatment. Actions of an expert thus naturally encode part of their domain knowledge, and can help make inferences within the same domain: Knowing doctors try to prescribe the best treatment for their patients, we can tell treatments prescribed more frequently are likely to be more effective. Yet in machine learning, the fact that most decision-makers are experts is often overlooked, and "expertise" is seldom leveraged as an inductive bias. This is especially true for the literature on treatment effect estimation, where often the only assumption made about actions is that of overlap. In this paper, we argue that expertise - particularly the type of expertise the decision-makers of a domain are likely to have - can be informative in designin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#25968;&#25454;&#20013;&#23398;&#20064;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#19982;&#36923;&#36753;&#22238;&#24402;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#35745;&#31639;&#30340;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00680</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable Learning of Item Response Theory Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00680
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#25968;&#25454;&#20013;&#23398;&#20064;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#19982;&#36923;&#36753;&#22238;&#24402;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#35745;&#31639;&#30340;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#65288;IRT&#65289;&#27169;&#22411;&#26088;&#22312;&#35780;&#20272; $n$ &#21517;&#32771;&#29983;&#30340;&#28508;&#22312;&#33021;&#21147;&#20197;&#21450; $m$ &#20010;&#27979;&#39564;&#39033;&#30446;&#30340;&#38544;&#21547;&#38590;&#24230;&#29305;&#24449;&#65292;&#36825;&#20123;&#39033;&#30446;&#26159;&#20174;&#34920;&#26126;&#20854;&#23545;&#24212;&#31572;&#26696;&#36136;&#37327;&#30340;&#20998;&#31867;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#12290;&#20256;&#32479;&#30340;&#24515;&#29702;&#27979;&#37327;&#35780;&#20272;&#22522;&#20110;&#30456;&#23545;&#36739;&#23569;&#30340;&#32771;&#29983;&#21644;&#39033;&#30446;&#65292;&#20363;&#22914;&#19968;&#20010;&#30001; $200$ &#21517;&#23398;&#29983;&#35299;&#20915;&#21253;&#21547; $10$ &#36947;&#39064;&#30446;&#30340;&#32771;&#35797;&#30340;&#29677;&#32423;&#12290;&#32780;&#36817;&#24180;&#26469;&#30340;&#20840;&#29699;&#22823;&#35268;&#27169;&#35780;&#20272;&#65292;&#22914;PISA&#65292;&#25110;&#20114;&#32852;&#32593;&#30740;&#31350;&#65292;&#21487;&#33021;&#23548;&#33268;&#21442;&#19982;&#32773;&#25968;&#37327;&#26174;&#33879;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#31639;&#27861;&#25198;&#28436;&#32771;&#29983;&#35282;&#33394;&#65292;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#25198;&#28436;&#39033;&#30446;&#35282;&#33394;&#65292;$n$ &#21644; $m$ &#37117;&#21487;&#33021;&#21464;&#24471;&#38750;&#24120;&#22823;&#65292;&#25361;&#25112;&#35745;&#31639;&#30340;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#20026;&#20102;&#20174;&#22823;&#25968;&#25454;&#20013;&#23398;&#20064;IRT&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#19982;&#36923;&#36753;&#22238;&#24402;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21518;&#32773;&#21487;&#20197;&#20351;&#29992;s&#20934;&#30830;&#22320;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00680v1 Announce Type: new  Abstract: Item Response Theory (IRT) models aim to assess latent abilities of $n$ examinees along with latent difficulty characteristics of $m$ test items from categorical data that indicates the quality of their corresponding answers. Classical psychometric assessments are based on a relatively small number of examinees and items, say a class of $200$ students solving an exam comprising $10$ problems. More recent global large scale assessments such as PISA, or internet studies, may lead to significantly increased numbers of participants. Additionally, in the context of Machine Learning where algorithms take the role of examinees and data analysis problems take the role of items, both $n$ and $m$ may become very large, challenging the efficiency and scalability of computations. To learn the latent variables in IRT models from large data, we leverage the similarity of these models to logistic regression, which can be approximated accurately using s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#22312;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#20013;&#37325;&#29992;&#21382;&#21490;&#36712;&#36857;&#21487;&#25552;&#39640;&#25910;&#25947;&#36895;&#29575;</title><link>https://arxiv.org/abs/2403.00675</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#22312;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#20013;&#37325;&#29992;&#21382;&#21490;&#36712;&#36857;&#65306;&#25910;&#25947;&#24615;&#21644;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Reusing Historical Trajectories in Natural Policy Gradient via Importance Sampling: Convergence and Convergence Rate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00675
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#22312;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#20013;&#37325;&#29992;&#21382;&#21490;&#36712;&#36857;&#21487;&#25552;&#39640;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#23398;&#20064;&#25511;&#21046;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20854;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23427;&#21487;&#20197;&#21033;&#29992;&#30340;&#25968;&#25454;&#37327;&#12290;&#26377;&#25928;&#21033;&#29992;&#20808;&#21069;&#31574;&#30053;&#24471;&#21040;&#30340;&#21382;&#21490;&#36712;&#36857;&#23545;&#20110;&#21152;&#24555;&#31574;&#30053;&#20248;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#22522;&#20110;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#25928;&#26524;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#21516;&#36845;&#20195;&#20043;&#38388;&#36712;&#36857;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#65292;&#19988;&#33391;&#22909;&#30340;&#23454;&#35777;&#34920;&#29616;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#37325;&#26032;&#21033;&#29992;&#21382;&#21490;&#36712;&#36857;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#34920;&#26126;&#20102;&#25152;&#25552;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#20559;&#24046;&#28176;&#36817;&#21487;&#24573;&#30053;&#65292;&#24471;&#21040;&#30340;&#31639;&#27861;&#26159;&#25910;&#25947;&#30340;&#65292;&#24182;&#19988;&#37325;&#29992;&#36807;&#21435;&#30340;&#36712;&#36857;&#26377;&#21161;&#20110;&#25552;&#39640;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25152;&#25552;&#20272;&#35745;&#22120;&#24212;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00675v1 Announce Type: new  Abstract: Reinforcement learning provides a mathematical framework for learning-based control, whose success largely depends on the amount of data it can utilize. The efficient utilization of historical trajectories obtained from previous policies is essential for expediting policy optimization. Empirical evidence has shown that policy gradient methods based on importance sampling work well. However, existing literature often neglect the interdependence between trajectories from different iterations, and the good empirical performance lacks a rigorous theoretical justification. In this paper, we study a variant of the natural policy gradient method with reusing historical trajectories via importance sampling. We show that the bias of the proposed estimator of the gradient is asymptotically negligible, the resultant algorithm is convergent, and reusing past trajectories helps improve the convergence rate. We further apply the proposed estimator to 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24555;&#29031;&#24378;&#21270;&#23398;&#20064;&#65288;SnapshotRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21333;&#25913;&#21464;&#29615;&#22659;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#23545;&#31639;&#27861;&#21644;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;</title><link>https://arxiv.org/abs/2403.00673</link><description>&lt;p&gt;
&#24555;&#29031;&#24378;&#21270;&#23398;&#20064;&#65306;&#21033;&#29992;&#20808;&#21069;&#36712;&#36857;&#25552;&#39640;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Snapshot Reinforcement Learning: Leveraging Prior Trajectories for Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00673
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24555;&#29031;&#24378;&#21270;&#23398;&#20064;&#65288;SnapshotRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21333;&#25913;&#21464;&#29615;&#22659;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#23545;&#31639;&#27861;&#21644;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#21644;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#24182;&#23545;&#36827;&#19968;&#27493;&#21457;&#23637;&#26500;&#25104;&#25361;&#25112;&#12290;&#37492;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#32422;&#26463;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#35745;&#31639;&#24037;&#20316;&#65288;&#20363;&#22914;&#23398;&#20064;&#31574;&#30053;&#12289;&#26679;&#26412;&#65289;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#21644;&#20943;&#23569;DRL&#31639;&#27861;&#30340;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#21069;&#21033;&#29992;&#29616;&#26377;&#35745;&#31639;&#24037;&#20316;&#30340;&#30740;&#31350;&#38656;&#35201;&#23545;&#29616;&#26377;&#31639;&#27861;&#21644;&#27169;&#22411;&#36827;&#34892;&#24178;&#25200;&#24615;&#20462;&#25913;&#65292;&#19987;&#38376;&#20026;&#29305;&#23450;&#31639;&#27861;&#35774;&#35745;&#65292;&#32570;&#20047;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24555;&#29031;&#24378;&#21270;&#23398;&#20064;&#65288;SnapshotRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21333;&#25913;&#21464;&#29615;&#22659;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#23545;&#31639;&#27861;&#21644;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00673v1 Announce Type: new  Abstract: Deep reinforcement learning (DRL) algorithms require substantial samples and computational resources to achieve higher performance, which restricts their practical application and poses challenges for further development. Given the constraint of limited resources, it is essential to leverage existing computational work (e.g., learned policies, samples) to enhance sample efficiency and reduce the computational resource consumption of DRL algorithms. Previous works to leverage existing computational work require intrusive modifications to existing algorithms and models, designed specifically for specific algorithms, lacking flexibility and universality. In this paper, we present the Snapshot Reinforcement Learning (SnapshotRL) framework, which enhances sample efficiency by simply altering environments, without making any modifications to algorithms and models. By allowing student agents to choose states in teacher trajectories as the initi
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#22686;&#26448;&#21046;&#36896;&#39046;&#22495;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#33021;&#22815;&#20811;&#26381;&#39640;&#32500;&#25968;&#25454;&#30340;&#22797;&#26434;&#25361;&#25112;&#65292;&#25512;&#21160;&#35813;&#39046;&#22495;&#19981;&#26029;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.00669</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25512;&#21160;&#22686;&#26448;&#21046;&#36896;&#65306;&#24403;&#21069;&#36827;&#23637;&#21644;&#26410;&#26469;&#25361;&#25112;&#30340;&#32508;&#21512;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
Advancing Additive Manufacturing through Deep Learning: A Comprehensive Review of Current Progress and Future Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00669
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22686;&#26448;&#21046;&#36896;&#39046;&#22495;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#33021;&#22815;&#20811;&#26381;&#39640;&#32500;&#25968;&#25454;&#30340;&#22797;&#26434;&#25361;&#25112;&#65292;&#25512;&#21160;&#35813;&#39046;&#22495;&#19981;&#26029;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#26448;&#21046;&#36896;&#65288;AM&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#20943;&#23569;&#21046;&#36896;&#30340;&#28508;&#22312;&#26367;&#20195;&#21697;&#65292;&#22240;&#20026;&#20854;&#22312;&#26368;&#23567;&#26448;&#26009;&#28010;&#36153;&#30340;&#24773;&#20917;&#19979;&#21046;&#36896;&#39640;&#24230;&#23450;&#21046;&#20135;&#21697;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21253;&#25324;&#22797;&#26434;&#21644;&#21160;&#24577;&#36807;&#31243;&#30456;&#20114;&#20316;&#29992;&#22312;&#20869;&#30340;&#19968;&#20123;&#20027;&#35201;&#22266;&#26377;&#25361;&#25112;&#65292;&#21363;&#20351;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#26377;&#26102;&#20063;&#38590;&#20197;&#23436;&#20840;&#29702;&#35299;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#39640;&#32500;&#25968;&#25454;&#65292;&#22914;&#22270;&#20687;&#12289;&#28857;&#20113;&#21644;&#20307;&#32032;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20986;&#29616;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#22312;&#20811;&#26381;&#35768;&#22810;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;DL&#33021;&#22815;&#33258;&#21160;&#20174;&#39640;&#32500;&#25968;&#25454;&#20013;&#25429;&#25417;&#22797;&#26434;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#25163;&#24037;&#21046;&#20316;&#29305;&#24449;&#25552;&#21462;&#12290;&#22240;&#27492;&#65292;AM&#21644;DL&#20132;&#21449;&#39046;&#22495;&#30340;&#30740;&#31350;&#37327;&#27599;&#24180;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#36825;&#21487;&#33021;&#20250;&#23558;&#22686;&#26448;&#21046;&#36896;&#25512;&#21521;&#26356;&#24191;&#38420;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00669v1 Announce Type: new  Abstract: Additive manufacturing (AM) has already proved itself to be the potential alternative to widely-used subtractive manufacturing due to its extraordinary capacity of manufacturing highly customized products with minimum material wastage. Nevertheless, it is still not being considered as the primary choice for the industry due to some of its major inherent challenges, including complex and dynamic process interactions, which are sometimes difficult to fully understand even with traditional machine learning because of the involvement of high-dimensional data such as images, point clouds, and voxels. However, the recent emergence of deep learning (DL) is showing great promise in overcoming many of these challenges as DL can automatically capture complex relationships from high-dimensional data without hand-crafted feature extraction. Therefore, the volume of research in the intersection of AM and DL is exponentially growing each year which ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#24320;&#21457;&#19968;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#25512;&#26029;&#20855;&#26377;&#22266;&#26377;&#31283;&#23450;&#24615;&#20445;&#35777;&#30340;&#20108;&#27425;&#25511;&#21046;&#21160;&#24577;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2403.00646</link><description>&lt;p&gt;
&#20855;&#26377;&#20108;&#27425;&#38750;&#32447;&#24615;&#25511;&#21046;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#35748;&#35777;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stability-Certified Learning of Control Systems with Quadratic Nonlinearities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#24320;&#21457;&#19968;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#25512;&#26029;&#20855;&#26377;&#22266;&#26377;&#31283;&#23450;&#24615;&#20445;&#35777;&#30340;&#20108;&#27425;&#25511;&#21046;&#21160;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#19968;&#31181;&#36816;&#31639;&#25512;&#26029;&#26041;&#27861;&#65292;&#26088;&#22312;&#22522;&#20110;&#20851;&#20110;&#32467;&#26500;&#30340;&#20808;&#39564;&#20551;&#35774;&#26500;&#24314;&#20302;&#32500;&#21160;&#21147;&#27169;&#22411;&#65292;&#36825;&#20123;&#20551;&#35774;&#36890;&#24120;&#21463;&#21040;&#24050;&#24314;&#31435;&#30340;&#29289;&#29702;&#23398;&#25110;&#19987;&#23478;&#35265;&#35299;&#30340;&#21551;&#21457;&#12290;&#31283;&#23450;&#24615;&#26159;&#21160;&#21147;&#31995;&#32479;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#28982;&#32780;&#65292;&#24182;&#38750;&#24635;&#26159;&#21487;&#20197;&#22312;&#36890;&#36807;&#25512;&#26029;&#24471;&#20986;&#30340;&#27169;&#22411;&#20013;&#20445;&#35777;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#25512;&#26029;&#20855;&#26377;&#22266;&#26377;&#31283;&#23450;&#24615;&#20445;&#35777;&#30340;&#20108;&#27425;&#25511;&#21046;&#21160;&#24577;&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#33021;&#37327;&#20445;&#25345;&#38750;&#32447;&#24615;&#30340;&#25511;&#21046;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#29305;&#24449;&#65292;&#20174;&#32780;&#30830;&#23450;&#36825;&#31867;&#31995;&#32479;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#26159;&#26377;&#30028;&#36755;&#20837;&#26377;&#30028;&#29366;&#24577;&#31283;&#23450;&#30340;&#12290;&#36825;&#20123;&#35265;&#35299;&#38543;&#21518;&#24212;&#29992;&#20110;&#23398;&#20064;&#36807;&#31243;&#65292;&#20135;&#29983;&#20102;&#20174;&#35774;&#35745;&#19978;&#22266;&#26377;&#31283;&#23450;&#30340;&#25512;&#26029;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#19968;&#20123;&#25968;&#20540;&#20363;&#23376;&#21152;&#20197;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00646v1 Announce Type: new  Abstract: This work primarily focuses on an operator inference methodology aimed at constructing low-dimensional dynamical models based on a priori hypotheses about their structure, often informed by established physics or expert insights. Stability is a fundamental attribute of dynamical systems, yet it is not always assured in models derived through inference. Our main objective is to develop a method that facilitates the inference of quadratic control dynamical systems with inherent stability guarantees. To this aim, we investigate the stability characteristics of control systems with energy-preserving nonlinearities, thereby identifying conditions under which such systems are bounded-input bounded-state stable. These insights are subsequently applied to the learning process, yielding inferred models that are inherently stable by design. The efficacy of our proposed framework is demonstrated through a couple of numerical examples.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35782;&#21035;&#24182;&#28385;&#36275;&#29616;&#26377;&#22343;&#21248;&#24615;&#24230;&#37327;&#26410;&#33021;&#36798;&#26631;&#30340;&#20116;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#32500;&#24230;&#23849;&#28291;&#25935;&#24863;&#30340;&#26032;&#22343;&#21248;&#24615;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.00642</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Rethinking The Uniformity Metric in Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00642
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;&#24182;&#28385;&#36275;&#29616;&#26377;&#22343;&#21248;&#24615;&#24230;&#37327;&#26410;&#33021;&#36798;&#26631;&#30340;&#20116;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#32500;&#24230;&#23849;&#28291;&#25935;&#24863;&#30340;&#26032;&#22343;&#21248;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22343;&#21248;&#24615;&#22312;&#35780;&#20272;&#23398;&#20064;&#34920;&#31034;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#29702;&#35299;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#20043;&#21069;&#30340;&#19968;&#39033;&#24320;&#21019;&#24615;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#22343;&#21248;&#24615;&#24230;&#37327;&#65292;&#23450;&#37327;&#34913;&#37327;&#23398;&#20064;&#34920;&#31034;&#30340;&#23849;&#28291;&#31243;&#24230;&#12290;&#30452;&#25509;&#20248;&#21270;&#36825;&#19968;&#24230;&#37327;&#19982;&#23545;&#40784;&#19968;&#36215;&#65292;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#19981;&#26029;&#23849;&#28291;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#36825;&#19968;&#24230;&#37327;&#32570;&#20047;&#23545;&#32500;&#24230;&#23849;&#28291;&#30340;&#25935;&#24863;&#24615;&#65292;&#20984;&#26174;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#24182;&#35774;&#35745;&#19968;&#20010;&#26356;&#26377;&#25928;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;&#65292;&#26412;&#25991;&#30830;&#23450;&#20102;&#20116;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#20854;&#20013;&#29616;&#26377;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;&#26410;&#33021;&#28385;&#36275;&#20854;&#20013;&#30340;&#19968;&#20123;&#12290;&#25105;&#20204;&#38543;&#21518;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;&#65292;&#28385;&#36275;&#25152;&#26377;&#36825;&#20123;&#26399;&#26395;&#65292;&#24182;&#19988;&#23545;&#32500;&#24230;&#23849;&#28291;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00642v1 Announce Type: cross  Abstract: Uniformity plays a crucial role in the assessment of learned representations, contributing to a deeper comprehension of self-supervised learning. The seminal work by \citet{Wang2020UnderstandingCR} introduced a uniformity metric that quantitatively measures the collapse degree of learned representations. Directly optimizing this metric together with alignment proves to be effective in preventing constant collapse. However, we present both theoretical and empirical evidence revealing that this metric lacks sensitivity to dimensional collapse, highlighting its limitations. To address this limitation and design a more effective uniformity metric, this paper identifies five fundamental properties, some of which the existing uniformity metric fails to meet. We subsequently introduce a novel uniformity metric that satisfies all of these desiderata and exhibits sensitivity to dimensional collapse. When applied as an auxiliary loss in various 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20943;&#36731;&#26032;&#20219;&#21153;&#20013;&#20559;&#35265;&#30340;&#39640;&#25928;&#19988;&#31283;&#20581;&#30340;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#20013;&#21644;&#23545;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#39044;&#27979;&#26377;&#24433;&#21709;&#21147;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25552;&#21319;&#20844;&#24179;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.00625</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#25552;&#21319;&#20844;&#24179;&#24615;&#21644;&#25928;&#29575;&#30340;&#20559;&#35265;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bias Mitigation in Fine-tuning Pre-trained Models for Enhanced Fairness and Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00625
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20943;&#36731;&#26032;&#20219;&#21153;&#20013;&#20559;&#35265;&#30340;&#39640;&#25928;&#19988;&#31283;&#20581;&#30340;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#20013;&#21644;&#23545;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#39044;&#27979;&#26377;&#24433;&#21709;&#21147;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25552;&#21319;&#20844;&#24179;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#26032;&#20219;&#21153;&#19978;&#24494;&#35843;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#36825;&#26159;&#22240;&#20026;&#26080;&#35770;&#21407;&#22987;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#32771;&#34385;&#20102;&#20844;&#24179;&#24615;&#65292;&#37117;&#27809;&#26377;&#20844;&#24179;&#24615;&#23646;&#24615;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20943;&#36731;&#26032;&#20219;&#21153;&#20013;&#20559;&#35265;&#30340;&#39640;&#25928;&#19988;&#31283;&#20581;&#30340;&#24494;&#35843;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#24433;&#21709;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#39044;&#27979;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#26159;&#19981;&#21516;&#30340;&#65292;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20351;&#29992;&#36328;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;Fisher&#20449;&#24687;&#30830;&#23450;&#30340;&#36825;&#20123;&#26377;&#24433;&#21709;&#21147;&#30340;&#26435;&#37325;&#26469;&#20013;&#21644;&#36825;&#20123;&#26377;&#24433;&#21709;&#21147;&#30340;&#26435;&#37325;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26435;&#37325;&#37325;&#35201;&#24615;&#20013;&#21644;&#31574;&#30053;&#19982;&#30697;&#38453;&#22240;&#23376;&#20998;&#35299;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00625v1 Announce Type: new  Abstract: Fine-tuning pre-trained models is a widely employed technique in numerous real-world applications. However, fine-tuning these models on new tasks can lead to unfair outcomes. This is due to the absence of generalization guarantees for fairness properties, regardless of whether the original pre-trained model was developed with fairness considerations. To tackle this issue, we introduce an efficient and robust fine-tuning framework specifically designed to mitigate biases in new tasks. Our empirical analysis shows that the parameters in the pre-trained model that affect predictions for different demographic groups are different, so based on this observation, we employ a transfer learning strategy that neutralizes the importance of these influential weights, determined using Fisher information across demographic groups. Additionally, we integrate this weight importance neutralization strategy with a matrix factorization technique, which pro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#34920;&#31034;&#29992;&#25143;&#21475;&#21619;&#30340;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#26694;&#26550;&#65292;&#32467;&#21512;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#31649;&#29702;&#29983;&#20135;&#27169;&#22411;&#20013;&#35813;&#26694;&#26550;&#37096;&#32626;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.00584</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalized User Representations for Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00584
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#34920;&#31034;&#29992;&#25143;&#21475;&#21619;&#30340;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#26694;&#26550;&#65292;&#32467;&#21512;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#31649;&#29702;&#29983;&#20135;&#27169;&#22411;&#20013;&#35813;&#26694;&#26550;&#37096;&#32626;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#26088;&#22312;&#20197;&#36890;&#29992;&#26041;&#24335;&#26377;&#25928;&#34920;&#31034;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#21475;&#21619;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#34920;&#31034;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#12290;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#21508;&#31181;&#29992;&#25143;&#29305;&#24449;&#21387;&#32553;&#25104;&#34920;&#31034;&#31354;&#38388;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#19979;&#28216;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21033;&#29992;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#31574;&#21010;&#29992;&#25143;&#29305;&#24449;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#34920;&#31034;&#30340;&#36755;&#20837;&#29305;&#24449;&#19978;&#22686;&#24378;&#36825;&#31181;&#26041;&#27861;&#65292;&#20197;&#22686;&#21152;&#28789;&#27963;&#24615;&#65292;&#24182;&#23454;&#29616;&#23545;&#29992;&#25143;&#20107;&#20214;&#65288;&#21253;&#25324;&#26032;&#29992;&#25143;&#20307;&#39564;&#65289;&#30340;&#20960;&#20046;&#23454;&#26102;&#21453;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#31649;&#29702;&#35813;&#26694;&#26550;&#22312;&#29983;&#20135;&#27169;&#22411;&#20013;&#30340;&#37096;&#32626;&#65292;&#20801;&#35768;&#19979;&#28216;&#27169;&#22411;&#29420;&#31435;&#24037;&#20316;&#12290;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#30340;&#32447;&#19979;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00584v1 Announce Type: cross  Abstract: We present a novel framework for user representation in large-scale recommender systems, aiming at effectively representing diverse user taste in a generalized manner. Our approach employs a two-stage methodology combining representation learning and transfer learning. The representation learning model uses an autoencoder that compresses various user features into a representation space. In the second stage, downstream task-specific models leverage user representations via transfer learning instead of curating user features individually. We further augment this methodology on the representation's input features to increase flexibility and enable reaction to user events, including new user experiences, in Near-Real Time. Additionally, we propose a novel solution to manage deployment of this framework in production models, allowing downstream models to work independently. We validate the performance of our framework through rigorous offl
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;SINDy&#25216;&#26415;&#22312;&#38750;&#32447;&#24615;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#22788;&#29702;&#30495;&#23454;&#21160;&#24577;&#31995;&#32479;&#26102;&#38754;&#20020;&#22788;&#29702;&#26410;&#35266;&#23519;&#29366;&#24577;&#21644;&#38750;&#24179;&#28369;&#21160;&#21147;&#23398;&#30340;&#22256;&#38590;&#65292;&#20026;&#20102;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#32972;&#26223;&#19979;&#20063;&#33021;&#21033;&#29992;SINDy&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.00578</link><description>&lt;p&gt;
SINDy&#19982;&#30828;&#38750;&#32447;&#24615;&#21644;&#38544;&#34255;&#21160;&#21147;&#23398;&#65306;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SINDy vs Hard Nonlinearities and Hidden Dynamics: a Benchmarking Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;SINDy&#25216;&#26415;&#22312;&#38750;&#32447;&#24615;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#22788;&#29702;&#30495;&#23454;&#21160;&#24577;&#31995;&#32479;&#26102;&#38754;&#20020;&#22788;&#29702;&#26410;&#35266;&#23519;&#29366;&#24577;&#21644;&#38750;&#24179;&#28369;&#21160;&#21147;&#23398;&#30340;&#22256;&#38590;&#65292;&#20026;&#20102;&#22312;&#36825;&#20123;&#25361;&#25112;&#24615;&#32972;&#26223;&#19979;&#20063;&#33021;&#21033;&#29992;SINDy&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#31232;&#30095;&#35782;&#21035;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65288;SINDy&#65289;&#25216;&#26415;&#22312;&#19977;&#20010;&#38750;&#32447;&#24615;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20854;&#22312;&#22788;&#29702;&#30495;&#23454;&#21160;&#24577;&#31995;&#32479;&#26102;&#30340;&#36866;&#29992;&#24615;&#12290;&#23613;&#31649;SINDy&#21487;&#20197;&#26159;&#19968;&#31181;&#36861;&#27714;&#22522;&#20110;&#29289;&#29702;&#30340;&#23398;&#20064;&#30340;&#21560;&#24341;&#31574;&#30053;&#65292;&#20294;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#26174;&#20102;&#22312;&#22788;&#29702;&#26410;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#21644;&#38750;&#24179;&#28369;&#21160;&#21147;&#23398;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#30001;&#20110;&#36825;&#20123;&#29305;&#24449;&#22312;&#19968;&#33324;&#23454;&#38469;&#31995;&#32479;&#21644;&#29305;&#21035;&#26159;&#25511;&#21046;&#24212;&#29992;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#25105;&#20204;&#34917;&#20805;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#36890;&#36807;&#23454;&#36341;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20197;&#20415;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32972;&#26223;&#19979;&#20063;&#21033;&#29992;SINDy&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00578v1 Announce Type: cross  Abstract: In this work we analyze the effectiveness of the Sparse Identification of Nonlinear Dynamics (SINDy) technique on three benchmark datasets for nonlinear identification, to provide a better understanding of its suitability when tackling real dynamical systems. While SINDy can be an appealing strategy for pursuing physics-based learning, our analysis highlights difficulties in dealing with unobserved states and non-smooth dynamics. Due to the ubiquity of these features in real systems in general, and control applications in particular, we complement our analysis with hands-on approaches to tackle these issues in order to exploit SINDy also in these challenging contexts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#26041;&#27861;&#65292;&#20174;&#19968;&#31995;&#21015;&#36712;&#36857;&#20013;&#20272;&#35745;&#38543;&#26426;&#20248;&#21270;&#22120;&#30340;&#31283;&#24577;&#20998;&#24067;&#65292;&#22635;&#34917;&#20102;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#20013;&#20851;&#20110;&#20248;&#21270;&#21644;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.00574</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#27169;&#22411;&#35266;&#28857;&#30340;&#21457;&#23637;&#65306;&#20248;&#21270;&#19982;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Single-Model Views for Deep Learning: Optimization versus Generalizability of Stochastic Optimization Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#26041;&#27861;&#65292;&#20174;&#19968;&#31995;&#21015;&#36712;&#36857;&#20013;&#20272;&#35745;&#38543;&#26426;&#20248;&#21270;&#22120;&#30340;&#31283;&#24577;&#20998;&#24067;&#65292;&#22635;&#34917;&#20102;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#20013;&#20851;&#20110;&#20248;&#21270;&#21644;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35299;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#30340;&#25991;&#29486;&#20869;&#23481;&#24456;&#20016;&#23500;&#65292;&#20294;&#25105;&#20204;&#23545;&#20110;&#20160;&#20040;&#20351;&#20248;&#21270;&#31639;&#27861;&#26377;&#25928;&#30340;&#29702;&#35299;&#20173;&#28982;&#38646;&#25955;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#19981;&#22826;&#28165;&#26970;&#22686;&#24378;&#30340;&#20248;&#21270;&#26159;&#21542;&#20250;&#36716;&#21270;&#20026;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21450;&#20854;&#21464;&#20307;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#65292;&#23548;&#33268;&#32570;&#20047;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23545;&#23427;&#20204;&#32479;&#35745;&#24615;&#33021;&#30340;&#27934;&#23519;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#19981;&#20165;&#20165;&#35780;&#20272;&#21333;&#20010;&#20248;&#21270;&#36712;&#36857;&#30340;&#32456;&#28857;&#65292;&#32780;&#26159;&#20174;&#19968;&#31995;&#21015;&#36712;&#36857;&#20013;&#27762;&#21462;&#65292;&#20197;&#20272;&#35745;&#38543;&#26426;&#20248;&#21270;&#22120;&#30340;&#31283;&#24577;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;SGD&#21450;&#20854;&#21464;&#20307;&#12289;&#24179;&#22374;&#26368;&#23567;&#20540;&#20248;&#21270;&#22120;&#20197;&#21450;&#25105;&#20204;&#22312;Basin Hopping&#26694;&#26550;&#19979;&#25552;&#20986;&#30340;&#26032;&#31639;&#27861;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00574v1 Announce Type: new  Abstract: Despite an extensive body of literature on deep learning optimization, our current understanding of what makes an optimization algorithm effective is fragmented. In particular, we do not understand well whether enhanced optimization translates to improved generalizability. Current research overlooks the inherent stochastic nature of stochastic gradient descent (SGD) and its variants, resulting in a lack of comprehensive benchmarking and insight into their statistical performance. This paper aims to address this gap by adopting a novel approach. Rather than solely evaluating the endpoint of individual optimization trajectories, we draw from an ensemble of trajectories to estimate the stationary distribution of stochastic optimizers. Our investigation encompasses a wide array of techniques, including SGD and its variants, flat-minima optimizers, and new algorithms we propose under the Basin Hopping framework. Through our evaluation, which 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#26368;&#26032;&#30340;&#22270;&#29255;&#32858;&#31867;&#21644;&#25193;&#25955;&#27169;&#22411;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32771;&#34385;&#26368;&#20339;&#32858;&#31867;&#31890;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;FID&#24182;&#20855;&#26377;&#36739;&#24378;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#30340;&#32858;&#31867;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#20943;&#23569;&#35270;&#35273;&#32452;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.00570</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#32858;&#31867;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Rethinking cluster-conditioned diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00570
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#26368;&#26032;&#30340;&#22270;&#29255;&#32858;&#31867;&#21644;&#25193;&#25955;&#27169;&#22411;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32771;&#34385;&#26368;&#20339;&#32858;&#31867;&#31890;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;FID&#24182;&#20855;&#26377;&#36739;&#24378;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#30340;&#32858;&#31867;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#20943;&#23569;&#35270;&#35273;&#32452;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#20351;&#29992;&#32858;&#31867;&#20998;&#37197;&#30340;&#22270;&#29255;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#20851;&#20110;&#22270;&#29255;&#32858;&#31867;&#30340;&#20010;&#21035;&#32452;&#20214;&#22914;&#20309;&#24433;&#21709;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#22270;&#29255;&#21512;&#25104;&#12290;&#36890;&#36807;&#32467;&#21512;&#22270;&#29255;&#32858;&#31867;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#32771;&#34385;&#21040;&#22270;&#29255;&#21512;&#25104;&#65288;&#35270;&#35273;&#32452;&#65289;&#30340;&#26368;&#20339;&#31751;&#31890;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#32858;&#31867;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;FID&#65288;&#21363;&#22312;CIFAR10&#21644;CIFAR100&#19978;&#20998;&#21035;&#20026;1.67&#21644;2.17&#65289;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#24378;&#30340;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#32858;&#31867;&#26469;&#25512;&#23548;&#20943;&#23569;&#35270;&#35273;&#32452;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#38480;&#31751;&#36793;&#30028;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#32858;&#31867;&#19982;&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#29255;&#29983;&#25104;&#20043;&#38388;&#27809;&#26377;&#26174;&#33879;&#32852;&#31995;&#12290;&#20195;&#30721;&#21644;&#32858;&#31867;&#20998;&#37197;&#23558;&#20250;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00570v1 Announce Type: cross  Abstract: We present a comprehensive experimental study on image-level conditioning for diffusion models using cluster assignments. We elucidate how individual components regarding image clustering impact image synthesis across three datasets. By combining recent advancements from image clustering and diffusion models, we show that, given the optimal cluster granularity with respect to image synthesis (visual groups), cluster-conditioning can achieve state-of-the-art FID (i.e. 1.67, 2.17 on CIFAR10 and CIFAR100 respectively), while attaining a strong training sample efficiency. Finally, we propose a novel method to derive an upper cluster bound that reduces the search space of the visual groups using solely feature-based clustering. Unlike existing approaches, we find no significant connection between clustering and cluster-conditional image generation. The code and cluster assignments will be released.
&lt;/p&gt;</description></item><item><title>EfficientZero V2&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#36890;&#36807;&#19968;&#31995;&#21015;&#25913;&#36827;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#36890;&#29992;&#31639;&#27861;DreamerV3&#26377;&#26174;&#33879;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.00564</link><description>&lt;p&gt;
&#39640;&#25928;Zero V2&#65306;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#25484;&#25569;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00564
&lt;/p&gt;
&lt;p&gt;
EfficientZero V2&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#36890;&#36807;&#19968;&#31995;&#21015;&#25913;&#36827;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#36890;&#29992;&#31639;&#27861;DreamerV3&#26377;&#26174;&#33879;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#31639;&#27861;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#19968;&#30452;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EfficientZero V2&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#39640;&#25928;RL&#31639;&#27861;&#35774;&#35745;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;EfficientZero&#30340;&#24615;&#33021;&#25193;&#23637;&#21040;&#22810;&#20010;&#39046;&#22495;&#65292;&#28085;&#30422;&#36830;&#32493;&#21644;&#31163;&#25955;&#34892;&#21160;&#65292;&#20197;&#21450;&#35270;&#35273;&#21644;&#20302;&#32500;&#36755;&#20837;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#25105;&#20204;&#25552;&#20986;&#30340;&#25913;&#36827;&#65292;EfficientZero V2&#22312;&#26377;&#38480;&#25968;&#25454;&#35774;&#32622;&#19979;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#22823;&#24133;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#65288;SOTA&#65289;&#12290;EfficientZero V2&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#36827;&#27493;&#65292;&#27604;&#22914;Atari 100k&#65292;Proprio Control&#31561;&#20013;&#65292;&#22312;66&#20010;&#35780;&#20272;&#20219;&#21153;&#20013;&#26377;50&#20010;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00564v1 Announce Type: cross  Abstract: Sample efficiency remains a crucial challenge in applying Reinforcement Learning (RL) to real-world tasks. While recent algorithms have made significant strides in improving sample efficiency, none have achieved consistently superior performance across diverse domains. In this paper, we introduce EfficientZero V2, a general framework designed for sample-efficient RL algorithms. We have expanded the performance of EfficientZero to multiple domains, encompassing both continuous and discrete actions, as well as visual and low-dimensional inputs. With a series of improvements we propose, EfficientZero V2 outperforms the current state-of-the-art (SOTA) by a significant margin in diverse tasks under the limited data setting. EfficientZero V2 exhibits a notable advancement over the prevailing general algorithm, DreamerV3, achieving superior outcomes in 50 of 66 evaluated tasks across diverse benchmarks, such as Atari 100k, Proprio Control, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38388;&#25509;&#21442;&#25968;&#21270;CAEs&#65288;IP-CAEs&#65289;&#26469;&#35299;&#20915;&#20855;&#20307;&#33258;&#32534;&#30721;&#22120;&#65288;CAEs&#65289;&#22312;&#31283;&#23450;&#32852;&#21512;&#20248;&#21270;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;IP-CAEs&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#19988;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.00563</link><description>&lt;p&gt;
&#38388;&#25509;&#21442;&#25968;&#21270;&#20855;&#20307;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Indirectly Parameterized Concrete Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38388;&#25509;&#21442;&#25968;&#21270;CAEs&#65288;IP-CAEs&#65289;&#26469;&#35299;&#20915;&#20855;&#20307;&#33258;&#32534;&#30721;&#22120;&#65288;CAEs&#65289;&#22312;&#31283;&#23450;&#32852;&#21512;&#20248;&#21270;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;IP-CAEs&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#19988;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#22312;&#25968;&#25454;&#39640;&#32500;&#25110;&#33719;&#21462;&#23436;&#25972;&#29305;&#24449;&#38598;&#25104;&#26412;&#39640;&#26114;&#30340;&#24773;&#20917;&#19979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23884;&#20837;&#24335;&#29305;&#24449;&#36873;&#25321;&#30340;&#21457;&#23637;&#22312;&#24191;&#27867;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20855;&#20307;&#33258;&#32534;&#30721;&#22120;&#65288;CAEs&#65289;&#34987;&#35748;&#20026;&#26159;&#23884;&#20837;&#24335;&#29305;&#24449;&#36873;&#25321;&#20013;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#20294;&#21487;&#33021;&#38590;&#20197;&#23454;&#29616;&#31283;&#23450;&#30340;&#32852;&#21512;&#20248;&#21270;&#65292;&#20174;&#32780;&#24433;&#21709;&#20854;&#35757;&#32451;&#26102;&#38388;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#21457;&#29616;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#19982;CAE&#23398;&#20064;&#37325;&#22797;&#36873;&#25321;&#26377;&#20851;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#25913;&#36827;&#65306;&#38388;&#25509;&#21442;&#25968;&#21270;CAEs&#65288;IP-CAEs&#65289;&#12290;IP-CAEs&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;&#21644;&#20174;&#23427;&#21040;Gumbel-Softmax&#20998;&#24067;&#21442;&#25968;&#30340;&#26144;&#23556;&#12290;&#23613;&#31649;&#23454;&#29616;&#31616;&#21333;&#65292;IP-CAE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#37325;&#26500;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#26174;&#33879;&#19988;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#26080;&#35770;&#26159;&#22312;&#27867;&#21270;&#36824;&#26159;&#35757;&#32451;&#26102;&#38388;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00563v1 Announce Type: new  Abstract: Feature selection is a crucial task in settings where data is high-dimensional or acquiring the full set of features is costly. Recent developments in neural network-based embedded feature selection show promising results across a wide range of applications. Concrete Autoencoders (CAEs), considered state-of-the-art in embedded feature selection, may struggle to achieve stable joint optimization, hurting their training time and generalization. In this work, we identify that this instability is correlated with the CAE learning duplicate selections. To remedy this, we propose a simple and effective improvement: Indirectly Parameterized CAEs (IP-CAEs). IP-CAEs learn an embedding and a mapping from it to the Gumbel-Softmax distributions' parameters. Despite being simple to implement, IP-CAE exhibits significant and consistent improvements over CAE in both generalization and training time across several datasets for reconstruction and classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#19987;&#23478;&#25919;&#31574;&#30340;&#32452;&#32455;&#12289;&#29616;&#25104;&#25968;&#25454;&#38598;&#21644;&#25216;&#26415;&#30340;&#25552;&#20379;&#20197;&#21450;&#24120;&#35265;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#30340;&#20998;&#20139;&#65292;&#35299;&#20915;&#20102;&#27169;&#20223;&#23398;&#20064;&#20013;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#21644;&#35780;&#20272;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00550</link><description>&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#25968;&#25454;&#38598;&#65306;&#21019;&#24314;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#26234;&#33021;&#20307;&#21644;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning Datasets: A Toolkit For Creating Datasets, Training Agents and Benchmarking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#19987;&#23478;&#25919;&#31574;&#30340;&#32452;&#32455;&#12289;&#29616;&#25104;&#25968;&#25454;&#38598;&#21644;&#25216;&#26415;&#30340;&#25552;&#20379;&#20197;&#21450;&#24120;&#35265;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#30340;&#20998;&#20139;&#65292;&#35299;&#20915;&#20102;&#27169;&#20223;&#23398;&#20064;&#20013;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#21644;&#35780;&#20272;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#39046;&#22495;&#38656;&#35201;&#19987;&#23478;&#25968;&#25454;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#23436;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#24120;&#24120;&#22240;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#32780;&#22791;&#21463;&#22256;&#25200;&#65292;&#23548;&#33268;&#25216;&#26415;&#27979;&#35797;&#37117;&#26159;&#22522;&#20110;&#33258;&#26377;&#25968;&#25454;&#12290;&#21019;&#24314;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#32321;&#29712;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#30740;&#31350;&#20154;&#21592;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19987;&#23478;&#26234;&#33021;&#20307;&#12289;&#35760;&#24405;&#23427;&#20204;&#30340;&#20132;&#20114;&#24182;&#29992;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#27979;&#35797;&#27599;&#31181;&#22522;&#20934;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20026;&#27599;&#31181;&#26032;&#25216;&#26415;&#21019;&#24314;&#26032;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#35780;&#20272;&#36807;&#31243;&#32570;&#20047;&#19968;&#33268;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#22312;&#29366;&#24577;&#21644;&#21160;&#20316;&#20998;&#24067;&#19978;&#37117;&#21487;&#33021;&#22823;&#19981;&#30456;&#21516;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#27169;&#20223;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#19968;&#20010;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#65306;(i) &#25552;&#20379;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#19987;&#23478;&#31574;&#30053;&#65292;&#24182;&#25903;&#25345;&#22810;&#32447;&#31243;&#20197;&#21152;&#24555;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#36807;&#31243;&#65307;(ii) &#25552;&#20379;&#29616;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#25216;&#26415;&#20197;&#21450;&#31934;&#30830;&#30340;&#27979;&#37327;&#65307;&#20197;&#21450;(iii) &#20998;&#20139;&#24120;&#29992;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00550v1 Announce Type: cross  Abstract: Imitation learning field requires expert data to train agents in a task. Most often, this learning approach suffers from the absence of available data, which results in techniques being tested on its dataset. Creating datasets is a cumbersome process requiring researchers to train expert agents from scratch, record their interactions and test each benchmark method with newly created data. Moreover, creating new datasets for each new technique results in a lack of consistency in the evaluation process since each dataset can drastically vary in state and action distribution. In response, this work aims to address these issues by creating Imitation Learning Datasets, a toolkit that allows for: (i) curated expert policies with multithreaded support for faster dataset creation; (ii) readily available datasets and techniques with precise measurements; and (iii) sharing implementations of common imitation learning techniques. Demonstration li
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19982;&#37325;&#24515;&#26657;&#27491;&#31243;&#24207;&#65288;BCP&#65289;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#32500;&#31354;&#38388;&#20013;&#38271;&#26102;&#38388;&#25191;&#34892;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.00542</link><description>&lt;p&gt;
&#20351;&#29992;&#37325;&#24515;&#26657;&#27491;&#31243;&#24207;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Training Optimization using the Barycentric Correction Procedure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00542
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19982;&#37325;&#24515;&#26657;&#27491;&#31243;&#24207;&#65288;BCP&#65289;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#32500;&#31354;&#38388;&#20013;&#38271;&#26102;&#38388;&#25191;&#34892;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#26159;&#20855;&#26377;&#35768;&#22810;&#20154;&#31867;&#24433;&#21709;&#24212;&#29992;&#30340;&#20855;&#26377;&#39044;&#27979;&#31454;&#20105;&#21147;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#38271;&#26102;&#38388;&#25191;&#34892;&#20173;&#28982;&#26159;&#25991;&#29486;&#20013;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#23558;ML&#31639;&#27861;&#19982;&#19968;&#31181;&#31216;&#20026;&#37325;&#24515;&#26657;&#27491;&#31243;&#24207;&#65288;BCP&#65289;&#30340;&#39640;&#25928;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#26469;&#33258;&#31169;&#31435;&#22823;&#23398;&#30340;&#25945;&#32946;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#22909;&#22788;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23454;&#20363;&#25968;&#21644;&#32500;&#24230;&#22686;&#21152;&#26102;&#65292;&#35813;&#32452;&#21512;&#21487;&#20197;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#20013;&#25552;&#20379;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;&#26174;&#33879;&#22909;&#22788;&#65292;&#32780;&#19981;&#20250;&#22833;&#21435;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#32463;&#39564;&#35777;BCP&#21644;&#32447;&#24615;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#65288;LinearSVC&#65289;&#22312;&#20272;&#35745;&#30340;&#29305;&#24449;&#26144;&#23556;&#39640;&#26031;&#24452;&#21521;&#22522;&#20989;&#25968;&#65288;RBF&#65289;&#26680;&#21518;&#65292;&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00542v1 Announce Type: new  Abstract: Machine learning (ML) algorithms are predictively competitive algorithms with many human-impact applications. However, the issue of long execution time remains unsolved in the literature for high-dimensional spaces. This study proposes combining ML algorithms with an efficient methodology known as the barycentric correction procedure (BCP) to address this issue. This study uses synthetic data and an educational dataset from a private university to show the benefits of the proposed method. It was found that this combination provides significant benefits related to time in synthetic and real data without losing accuracy when the number of instances and dimensions increases. Additionally, for high-dimensional spaces, it was proved that BCP and linear support vector classification (LinearSVC), after an estimated feature map for the gaussian radial basis function (RBF) kernel, were unfeasible in terms of computational time and accuracy.
&lt;/p&gt;</description></item><item><title>&#23558;$\varepsilon$-greedy&#31574;&#30053;&#24341;&#20837;Thompson&#37319;&#26679;&#20197;&#25913;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#24320;&#21457;&#21151;&#33021;&#65292;&#24182;&#23454;&#35777;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00540</link><description>&lt;p&gt;
Epsilon-Greedy Thompson Sampling&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Epsilon-Greedy Thompson Sampling to Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00540
&lt;/p&gt;
&lt;p&gt;
&#23558;$\varepsilon$-greedy&#31574;&#30053;&#24341;&#20837;Thompson&#37319;&#26679;&#20197;&#25913;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#24320;&#21457;&#21151;&#33021;&#65292;&#24182;&#23454;&#35777;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Thompson&#37319;&#26679;&#65288;TS&#65289;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24320;&#21457;-&#25506;&#32034;&#22256;&#22659;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; &#34429;&#28982;&#23427;&#36890;&#36807;&#38543;&#26426;&#29983;&#25104;&#21644;&#26368;&#22823;&#21270;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#21518;&#39564;&#30340;&#26679;&#26412;&#36335;&#24452;&#26469;&#20248;&#20808;&#36827;&#34892;&#25506;&#32034;&#65292;&#20294;TS&#22312;&#27599;&#27425;&#25191;&#34892;&#25506;&#32034;&#21518;&#36890;&#36807;&#25910;&#38598;&#20851;&#20110;&#30495;&#23454;&#30446;&#26631;&#20989;&#25968;&#30340;&#20449;&#24687;&#26469;&#24369;&#21270;&#20854;&#24320;&#21457;&#21151;&#33021;&#12290; &#26412;&#30740;&#31350;&#23558;&#22312;TS&#20013;&#24341;&#20837;$\varepsilon$-greedy&#31574;&#30053;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#30340;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#25913;&#36827;&#20854;&#24320;&#21457;&#21151;&#33021;&#12290; &#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;TS&#24212;&#29992;&#20110;BO&#30340;&#20004;&#20010;&#26497;&#31471;&#65292;&#21363;&#36890;&#29992;TS&#21644;&#26679;&#26412;&#24179;&#22343;TS&#12290;&#21069;&#32773;&#21644;&#21518;&#32773;&#20998;&#21035;&#25552;&#20513;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290; &#28982;&#21518;&#25105;&#20204;&#20351;&#29992;$\varepsilon$-greedy&#31574;&#30053;&#22312;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#38543;&#26426;&#20999;&#25442;&#12290; $\varepsilon \in (0,1)$&#30340;&#23567;&#20540;&#20248;&#20808;&#32771;&#34385;&#24320;&#21457;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290; &#25105;&#20204;&#23454;&#35777;&#34920;&#26126;$\varepsilon$-greedy T
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00540v1 Announce Type: new  Abstract: Thompson sampling (TS) serves as a solution for addressing the exploitation-exploration dilemma in Bayesian optimization (BO). While it prioritizes exploration by randomly generating and maximizing sample paths of Gaussian process (GP) posteriors, TS weakly manages its exploitation by gathering information about the true objective function after each exploration is performed. In this study, we incorporate the epsilon-greedy ($\varepsilon$-greedy) policy, a well-established selection strategy in reinforcement learning, into TS to improve its exploitation. We first delineate two extremes of TS applied for BO, namely the generic TS and a sample-average TS. The former and latter promote exploration and exploitation, respectively. We then use $\varepsilon$-greedy policy to randomly switch between the two extremes. A small value of $\varepsilon \in (0,1)$ prioritizes exploitation, and vice versa. We empirically show that $\varepsilon$-greedy T
&lt;/p&gt;</description></item><item><title>VoxGenesis&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#35821;&#38899;&#21512;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#28508;&#22312;&#35828;&#35805;&#32773;&#27969;&#24418;&#21644;&#26377;&#24847;&#20041;&#30340;&#35821;&#38899;&#32534;&#36753;&#26041;&#21521;&#65292;&#20026;&#35299;&#20915;&#24773;&#24863;&#12289;&#35821;&#35843;&#21644;&#35828;&#35805;&#39118;&#26684;&#31561;&#38590;&#20197;&#33719;&#21462;&#20934;&#30830;&#26631;&#31614;&#30340;&#20154;&#22768;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00529</link><description>&lt;p&gt;
VoxGenesis: &#26080;&#30417;&#30563;&#21457;&#29616;&#28508;&#22312;&#35828;&#35805;&#32773;&#27969;&#24418;&#29992;&#20110;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
VoxGenesis: Unsupervised Discovery of Latent Speaker Manifold for Speech Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00529
&lt;/p&gt;
&lt;p&gt;
VoxGenesis&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#35821;&#38899;&#21512;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#28508;&#22312;&#35828;&#35805;&#32773;&#27969;&#24418;&#21644;&#26377;&#24847;&#20041;&#30340;&#35821;&#38899;&#32534;&#36753;&#26041;&#21521;&#65292;&#20026;&#35299;&#20915;&#24773;&#24863;&#12289;&#35821;&#35843;&#21644;&#35828;&#35805;&#39118;&#26684;&#31561;&#38590;&#20197;&#33719;&#21462;&#20934;&#30830;&#26631;&#31614;&#30340;&#20154;&#22768;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00529v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#36234; &#25688;&#35201;: &#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#23454;&#29616;&#23545;&#20154;&#22768;&#30340;&#24494;&#22937;&#21644;&#31934;&#20934;&#27169;&#25311;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#23613;&#31649;&#36817;&#24180;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#20027;&#27969;&#30340;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#20173;&#28982;&#20381;&#36182;&#20110;&#30417;&#30563;&#24335;&#35828;&#35805;&#32773;&#24314;&#27169;&#21644;&#26126;&#30830;&#30340;&#21442;&#32771;&#35821;&#21477;&#12290;&#28982;&#32780;&#65292;&#20154;&#22768;&#26377;&#35768;&#22810;&#26041;&#38754;&#65292;&#20363;&#22914;&#24773;&#24863;&#12289;&#35821;&#35843;&#21644;&#35828;&#35805;&#39118;&#26684;&#65292;&#24456;&#38590;&#33719;&#24471;&#20934;&#30830;&#30340;&#26631;&#31614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; VoxGenesis&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#21512;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#28508;&#22312;&#35828;&#35805;&#32773;&#27969;&#24418;&#21644;&#26377;&#24847;&#20041;&#30340;&#35821;&#38899;&#32534;&#36753;&#26041;&#21521;&#12290;VoxGenesis &#22312;&#27010;&#24565;&#19978;&#24456;&#31616;&#21333;&#12290;&#23427;&#19981;&#26159;&#23558;&#35821;&#38899;&#29305;&#24449;&#30830;&#23450;&#22320;&#26144;&#23556;&#21040;&#27874;&#24418;&#65292;&#32780;&#26159;&#23558;&#39640;&#26031;&#20998;&#24067;&#36716;&#25442;&#20026;&#30001;&#35821;&#20041;&#26631;&#35760;&#26465;&#20214;&#21644;&#23545;&#20934;&#30340;&#35821;&#38899;&#20998;&#24067;&#12290;&#36825;&#36843;&#20351;&#27169;&#22411;&#23398;&#20064;&#19968;&#20010;&#19982;&#35821;&#20041;&#20869;&#23481;&#35299;&#32806;&#30340;&#35828;&#35805;&#32773;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00529v1 Announce Type: cross  Abstract: Achieving nuanced and accurate emulation of human voice has been a longstanding goal in artificial intelligence. Although significant progress has been made in recent years, the mainstream of speech synthesis models still relies on supervised speaker modeling and explicit reference utterances. However, there are many aspects of human voice, such as emotion, intonation, and speaking style, for which it is hard to obtain accurate labels. In this paper, we propose VoxGenesis, a novel unsupervised speech synthesis framework that can discover a latent speaker manifold and meaningful voice editing directions without supervision. VoxGenesis is conceptually simple. Instead of mapping speech features to waveforms deterministically, VoxGenesis transforms a Gaussian distribution into speech distributions conditioned and aligned by semantic tokens. This forces the model to learn a speaker distribution disentangled from the semantic content. During
&lt;/p&gt;</description></item><item><title>&#23454;&#26045;&#36229;&#36807;60&#31181;&#19981;&#21516;&#30340;&#31163;&#31574;&#30053;&#20195;&#29702;&#65292;&#21457;&#29616;&#26576;&#20123;&#32452;&#21512;&#34920;&#29616;&#20986;&#31283;&#20581;&#21644;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#29305;&#23450;&#27491;&#21017;&#21270;&#35774;&#32622;&#19982;&#20219;&#21153;&#30340;&#20851;&#32852;&#24615;&#32422;&#20960;&#32500;&#22810;&#22810;&#12290;</title><link>https://arxiv.org/abs/2403.00514</link><description>&lt;p&gt;
Actor-Critic&#20013;&#30340;&#36807;&#24230;&#20272;&#35745;&#12289;&#36807;&#25311;&#21512;&#21644;&#21487;&#22609;&#24615;&#65306;&#24378;&#21270;&#23398;&#20064;&#30340;&#33510;&#28073;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00514
&lt;/p&gt;
&lt;p&gt;
&#23454;&#26045;&#36229;&#36807;60&#31181;&#19981;&#21516;&#30340;&#31163;&#31574;&#30053;&#20195;&#29702;&#65292;&#21457;&#29616;&#26576;&#20123;&#32452;&#21512;&#34920;&#29616;&#20986;&#31283;&#20581;&#21644;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#29305;&#23450;&#27491;&#21017;&#21270;&#35774;&#32622;&#19982;&#20219;&#21153;&#30340;&#20851;&#32852;&#24615;&#32422;&#20960;&#32500;&#22810;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#31163;&#31574;&#30053;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#21508;&#31181;&#24418;&#24335;&#30340;&#27491;&#21017;&#21270;&#30340;&#24212;&#29992;&#65292;&#20351;&#20854;&#27604;&#20256;&#32479;&#30340;&#20195;&#29702;&#26356;&#33021;&#36827;&#34892;&#26799;&#24230;&#26356;&#26032;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#25216;&#26415;&#37117;&#22312;&#26377;&#38480;&#30340;&#24773;&#26223;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36890;&#24120;&#21482;&#22312;&#21333;&#20010;&#20223;&#30495;&#22522;&#20934;&#20219;&#21153;&#19978;&#27979;&#35797;&#65292;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;&#32780;&#19981;&#26159;&#19982;&#19968;&#31995;&#21015;&#27491;&#21017;&#21270;&#26041;&#27861;&#30456;&#27604;&#12290;&#36825;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#25512;&#21160;RL&#25913;&#36827;&#30340;&#20855;&#20307;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36229;&#36807;60&#31181;&#19981;&#21516;&#30340;&#31163;&#31574;&#30053;&#20195;&#29702;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#25972;&#21512;&#20102;&#26368;&#36817;&#26368;&#20808;&#36827;&#31639;&#27861;&#20013;&#30340;&#24050;&#24314;&#31435;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;2&#20010;&#20223;&#30495;&#22522;&#20934;&#30340;14&#20010;&#19981;&#21516;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#36825;&#20123;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#29305;&#23450;&#30340;&#27491;&#21017;&#21270;&#35774;&#32622;&#30340;&#25928;&#26524;&#22240;&#20219;&#21153;&#32780;&#24322;&#65292;&#20294;&#26576;&#20123;&#32452;&#21512;&#22987;&#32456;&#34920;&#29616;&#20986;&#31283;&#20581;&#21644;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00514v1 Announce Type: new  Abstract: Recent advancements in off-policy Reinforcement Learning (RL) have significantly improved sample efficiency, primarily due to the incorporation of various forms of regularization that enable more gradient update steps than traditional agents. However, many of these techniques have been tested in limited settings, often on tasks from single simulation benchmarks and against well-known algorithms rather than a range of regularization approaches. This limits our understanding of the specific mechanisms driving RL improvements. To address this, we implemented over 60 different off-policy agents, each integrating established regularization techniques from recent state-of-the-art algorithms. We tested these agents across 14 diverse tasks from 2 simulation benchmarks. Our findings reveal that while the effectiveness of a specific regularization setup varies with the task, certain combinations consistently demonstrate robust and superior perform
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPA&#65289;&#20197;&#21450;&#24341;&#20837;&#22270;&#20687;&#19990;&#30028;&#27169;&#22411;&#65288;IWM&#65289;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;JEPA&#39044;&#27979;&#20219;&#21153;&#27010;&#25324;&#20026;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#25439;&#22351;&#24418;&#24335;&#65292;&#24182;&#30740;&#31350;&#20102;&#23398;&#20064;&#24615;&#33021;&#33391;&#22909;&#30340;IWM&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#20197;&#23558;IWM&#23398;&#21040;&#30340;&#39044;&#27979;&#19990;&#30028;&#27169;&#22411;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#26368;&#32456;&#25511;&#21046;&#25152;&#23398;&#20064;&#34920;&#31034;&#30340;&#25277;&#35937;&#32423;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.00504</link><description>&lt;p&gt;
&#23398;&#20064;&#21644;&#21033;&#29992;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning and Leveraging World Models in Visual Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00504
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPA&#65289;&#20197;&#21450;&#24341;&#20837;&#22270;&#20687;&#19990;&#30028;&#27169;&#22411;&#65288;IWM&#65289;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;JEPA&#39044;&#27979;&#20219;&#21153;&#27010;&#25324;&#20026;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#25439;&#22351;&#24418;&#24335;&#65292;&#24182;&#30740;&#31350;&#20102;&#23398;&#20064;&#24615;&#33021;&#33391;&#22909;&#30340;IWM&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#20197;&#23558;IWM&#23398;&#21040;&#30340;&#39044;&#27979;&#19990;&#30028;&#27169;&#22411;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#26368;&#32456;&#25511;&#21046;&#25152;&#23398;&#20064;&#34920;&#31034;&#30340;&#25277;&#35937;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPA&#65289;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;JEPA&#39044;&#27979;&#20219;&#21153;&#27010;&#25324;&#20026;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#25439;&#22351;&#24418;&#24335;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22270;&#20687;&#19990;&#30028;&#27169;&#22411;&#65288;IWM&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#36229;&#36234;&#20102;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#65292;&#23398;&#20250;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39044;&#27979;&#20840;&#23616;&#20809;&#24230;&#21464;&#25442;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#24615;&#33021;&#33391;&#22909;&#30340;IWM&#30340;&#20851;&#38190;&#26041;&#38754;&#65306;&#26465;&#20214;&#12289;&#39044;&#27979;&#22256;&#38590;&#24230;&#21644;&#23481;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#24494;&#35843;&#21487;&#20197;&#23558;IWM&#23398;&#21040;&#30340;&#39044;&#27979;&#19990;&#30028;&#27169;&#22411;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65307;&#32463;&#36807;&#24494;&#35843;&#30340;IWM&#19990;&#30028;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#29978;&#33267;&#36229;&#36807;&#20197;&#24448;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;IWM&#23398;&#20064;&#21487;&#20197;&#25511;&#21046;&#25152;&#23398;&#20064;&#34920;&#31034;&#30340;&#25277;&#35937;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00504v1 Announce Type: cross  Abstract: Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned represe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#32467;&#26500;&#12289;&#27169;&#22411;&#21644;&#24212;&#29992;&#65292;&#36890;&#36807;&#25552;&#20986;&#20855;&#22791;&#19981;&#21464;&#24615;/&#31561;&#21464;&#24615;&#23646;&#24615;&#30340;&#20960;&#20309;GNN&#26469;&#26356;&#22909;&#22320;&#34920;&#24449;&#20960;&#20309;&#22270;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#25299;&#25169;&#65292;&#24182;&#25552;&#20379;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.00485</link><description>&lt;p&gt;
&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#32508;&#36848;&#65306;&#25968;&#25454;&#32467;&#26500;&#12289;&#27169;&#22411;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Survey of Geometric Graph Neural Networks: Data Structures, Models and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00485
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#32467;&#26500;&#12289;&#27169;&#22411;&#21644;&#24212;&#29992;&#65292;&#36890;&#36807;&#25552;&#20986;&#20855;&#22791;&#19981;&#21464;&#24615;/&#31561;&#21464;&#24615;&#23646;&#24615;&#30340;&#20960;&#20309;GNN&#26469;&#26356;&#22909;&#22320;&#34920;&#24449;&#20960;&#20309;&#22270;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#25299;&#25169;&#65292;&#24182;&#25552;&#20379;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#22270;&#26159;&#19968;&#31181;&#20855;&#26377;&#20960;&#20309;&#29305;&#24449;&#30340;&#29305;&#27530;&#22270;&#24418;&#65292;&#23545;&#20110;&#24314;&#27169;&#35768;&#22810;&#31185;&#23398;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#19968;&#33324;&#22270;&#19981;&#21516;&#65292;&#20960;&#20309;&#22270;&#36890;&#24120;&#20855;&#26377;&#24179;&#31227;&#12289;&#26059;&#36716;&#21644;&#21453;&#23556;&#31561;&#29289;&#29702;&#23545;&#31216;&#24615;&#65292;&#36825;&#20351;&#23427;&#20204;&#38590;&#20197;&#34987;&#24403;&#21069;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26377;&#25928;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#20855;&#22791;&#19981;&#21464;&#24615;/&#31561;&#21464;&#24615;&#23646;&#24615;&#30340;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#24449;&#20960;&#20309;&#22270;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#25299;&#25169;&#12290;&#37492;&#20110;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#36827;&#23637;&#65292;&#26377;&#24517;&#35201;&#23545;&#19982;&#20960;&#20309;GNN&#30456;&#20851;&#30340;&#25968;&#25454;&#32467;&#26500;&#12289;&#27169;&#22411;&#21644;&#24212;&#29992;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22522;&#20110;&#24517;&#35201;&#20294;&#31616;&#27905;&#30340;&#25968;&#23398;&#22522;&#30784;&#30693;&#35782;&#65292;&#25105;&#20204;&#20174;&#20960;&#20309;&#28040;&#24687;&#20256;&#36882;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#24212;&#29992;&#31243;&#24207;&#20197;&#21450;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#20197;&#21518;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00485v1 Announce Type: new  Abstract: Geometric graph is a special kind of graph with geometric features, which is vital to model many scientific problems. Unlike generic graphs, geometric graphs often exhibit physical symmetries of translations, rotations, and reflections, making them ineffectively processed by current Graph Neural Networks (GNNs). To tackle this issue, researchers proposed a variety of Geometric Graph Neural Networks equipped with invariant/equivariant properties to better characterize the geometry and topology of geometric graphs. Given the current progress in this field, it is imperative to conduct a comprehensive survey of data structures, models, and applications related to geometric GNNs. In this paper, based on the necessary but concise mathematical preliminaries, we provide a unified view of existing models from the geometric message passing perspective. Additionally, we summarize the applications as well as the related datasets to facilitate later 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#65292;&#22312;&#27169;&#25311;&#34892;&#26143;&#29615;&#22659;&#20013;&#35757;&#32451;&#26426;&#22120;&#33218;&#33258;&#20027;&#30740;&#31350;&#24182;&#20114;&#21160;&#29289;&#20307;&#65292;&#20197;&#25581;&#31034;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.00470</link><description>&lt;p&gt;
&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#34892;&#26143;&#20219;&#21153;&#30340;&#33258;&#20027;&#26426;&#22120;&#33218;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Autonomous Robotic Arm Manipulation for Planetary Missions using Causal Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00470
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#65292;&#22312;&#27169;&#25311;&#34892;&#26143;&#29615;&#22659;&#20013;&#35757;&#32451;&#26426;&#22120;&#33218;&#33258;&#20027;&#30740;&#31350;&#24182;&#20114;&#21160;&#29289;&#20307;&#65292;&#20197;&#25581;&#31034;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#33218;&#25805;&#20316;&#20855;&#26377;&#28508;&#21147;&#20351;&#34892;&#26143;&#25506;&#27979;&#21644;&#21407;&#20301;&#36164;&#28304;&#21033;&#29992;&#20219;&#21153;&#26356;&#21152;&#39640;&#25928;&#21644;&#26377;&#25104;&#25928;&#65292;&#22240;&#20026;&#26426;&#22120;&#33218;&#21487;&#20197;&#33258;&#34892;&#22788;&#29702;&#29289;&#20307;&#24182;&#25191;&#34892;&#29305;&#23450;&#30446;&#26631;&#30340;&#21160;&#20316;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#34892;&#26143;&#29615;&#22659;&#20013;&#35757;&#32451;&#26426;&#22120;&#33218;&#33258;&#20027;&#30740;&#31350;&#20854;&#27809;&#26377;&#20808;&#21069;&#30693;&#35782;&#30340;&#29289;&#20307;&#65292;&#20363;&#22914;&#34892;&#26143;&#23721;&#30707;&#12290;&#26426;&#22120;&#33218;&#19982;&#29289;&#20307;&#20114;&#21160;&#65292;&#24182;&#26681;&#25454;&#19981;&#21516;&#30340;&#22240;&#26524;&#22240;&#32032;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#36825;&#20123;&#22240;&#32032;&#26159;&#35832;&#22914;&#36136;&#37327;&#25110;&#25705;&#25830;&#31995;&#25968;&#20043;&#31867;&#30340;&#21442;&#25968;&#65292;&#30830;&#23450;&#20854;&#20114;&#21160;&#32467;&#26524;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65292;&#26426;&#22120;&#33218;&#23398;&#20250;&#20197;&#25581;&#31034;&#28508;&#22312;&#22240;&#26524;&#22240;&#32032;&#30340;&#26041;&#24335;&#20114;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#27809;&#26377;&#20219;&#20309;&#20808;&#21069;&#20851;&#20110;&#29289;&#20307;&#30340;&#30693;&#35782;&#25110;&#20197;&#21069;&#25910;&#38598;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#36816;&#20316;&#12290;&#25105;&#20204;&#22312;&#34892;&#26143;&#25506;&#32034;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00470v1 Announce Type: cross  Abstract: Autonomous robotic arm manipulators have the potential to make planetary exploration and in-situ resource utilization missions more time efficient and productive, as the manipulator can handle the objects itself and perform goal-specific actions. We train a manipulator to autonomously study objects of which it has no prior knowledge, such as planetary rocks. This is achieved using causal machine learning in a simulated planetary environment. Here, the manipulator interacts with objects, and classifies them based on differing causal factors. These are parameters, such as mass or friction coefficient, that causally determine the outcomes of its interactions. Through reinforcement learning, the manipulator learns to interact in ways that reveal the underlying causal factors. We show that this method works even without any prior knowledge of the objects, or any previously-collected training data. We carry out the training in planetary expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23558;&#23433;&#20840;&#30340;&#28151;&#21512;&#34892;&#21160;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#21040;&#33258;&#20027;&#21464;&#36947;&#20013;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;PASAC-PIDLag&#65292;&#24182;&#36827;&#34892;&#20102;&#19982;&#19981;&#23433;&#20840;&#29256;&#26412;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.00446</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#28151;&#21512;&#34892;&#21160;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#19982;&#25511;&#21046;&#31574;&#30053;&#22312;&#33258;&#30001;&#21464;&#36947;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Safe Hybrid-Action Reinforcement Learning-Based Decision and Control for Discretionary Lane Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23558;&#23433;&#20840;&#30340;&#28151;&#21512;&#34892;&#21160;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#21040;&#33258;&#20027;&#21464;&#36947;&#20013;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;PASAC-PIDLag&#65292;&#24182;&#36827;&#34892;&#20102;&#19982;&#19981;&#23433;&#20840;&#29256;&#26412;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#21464;&#36947;&#26159;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#30340;&#37325;&#35201;&#29305;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#20132;&#36890;&#25928;&#29575;&#65292;&#20943;&#23569;&#20107;&#25925;&#21457;&#29983;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20445;&#35777;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#39550;&#39542;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#23433;&#20840;&#30340;&#28151;&#21512;&#34892;&#21160;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#21040;&#33258;&#20027;&#21464;&#36947;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;PID&#25289;&#26684;&#26391;&#26085;&#30340;&#21442;&#25968;&#21270;&#36719;&#28436;&#21592;&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;PASAC-PIDLag&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#21442;&#25968;&#21270;&#36719;&#28436;&#21592;&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;PASAC&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#21518;&#32773;&#26159;PASAC-PIDLag&#30340;&#19981;&#23433;&#20840;&#29256;&#26412;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#34987;&#24212;&#29992;&#20110;&#35757;&#32451;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#21464;&#36947;&#31574;&#30053;&#65292;&#36755;&#20986;&#31163;&#25955;&#30340;&#21464;&#36947;&#20915;&#31574;&#21644;&#32437;&#21521;&#36710;&#36742;&#21152;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00446v1 Announce Type: cross  Abstract: Autonomous lane-change, a key feature of advanced driver-assistance systems, can enhance traffic efficiency and reduce the incidence of accidents. However, safe driving of autonomous vehicles remains challenging in complex environments. How to perform safe and appropriate lane change is a popular topic of research in the field of autonomous driving. Currently, few papers consider the safety of reinforcement learning in autonomous lane-change scenarios. We introduce safe hybrid-action reinforcement learning into discretionary lane change for the first time and propose Parameterized Soft Actor-Critic with PID Lagrangian (PASAC-PIDLag) algorithm. Furthermore, we conduct a comparative analysis of the Parameterized Soft Actor-Critic (PASAC), which is an unsafe version of PASAC-PIDLag. Both algorithms are employed to train the lane-change strategy of autonomous vehicles to output discrete lane-change decision and longitudinal vehicle acceler
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#38646;&#26679;&#26412;&#23616;&#37096;&#22810;&#23545;&#35937;&#32534;&#36753;&#65292;&#36171;&#20104;&#29992;&#25143;&#22312;&#22270;&#20687;&#20013;&#19968;&#27425;&#24615;&#28155;&#21152;&#12289;&#26367;&#25442;&#25110;&#32534;&#36753;&#22810;&#23545;&#35937;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.00437</link><description>&lt;p&gt;
LoMOE: &#36890;&#36807;&#22810;&#25193;&#25955;&#23454;&#29616;&#23616;&#37096;&#22810;&#23545;&#35937;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
LoMOE: Localized Multi-Object Editing via Multi-Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00437
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#38646;&#26679;&#26412;&#23616;&#37096;&#22810;&#23545;&#35937;&#32534;&#36753;&#65292;&#36171;&#20104;&#29992;&#25143;&#22312;&#22270;&#20687;&#20013;&#19968;&#27425;&#24615;&#28155;&#21152;&#12289;&#26367;&#25442;&#25110;&#32534;&#36753;&#22810;&#23545;&#35937;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#23637;&#31034;&#20102;&#29983;&#25104;&#39640;&#36136;&#37327;&#22522;&#20110;&#25552;&#31034;&#26465;&#20214;&#30340;&#22270;&#20687;&#32534;&#36753;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#65292;&#24403;&#23545;&#22330;&#26223;&#20013;&#21253;&#21547;&#21333;&#20010;/&#22810;&#20010;&#23545;&#35937;&#30340;&#29305;&#23450;&#23545;&#35937;&#25110;&#32454;&#31890;&#24230;&#21306;&#22495;&#36827;&#34892;&#31934;&#30830;&#32534;&#36753;&#26102;&#24448;&#24448;&#19981;&#22826;&#26377;&#25928;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#38646;&#26679;&#26412;&#23616;&#37096;&#22810;&#23545;&#35937;&#32534;&#36753;&#65292;&#20197;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#36171;&#20104;&#29992;&#25143;&#22312;&#22270;&#20687;&#20013;&#23545;&#23545;&#35937;&#25191;&#34892;&#21508;&#31181;&#25805;&#20316;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#22312;&#19968;&#20010;&#22797;&#26434;&#22330;&#26223;&#20013;&#19968;&#27425;&#24615;&#28155;&#21152;&#12289;&#26367;&#25442;&#25110;&#32534;&#36753;$\textbf{&#22810;}$&#23545;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21069;&#26223; mask &#21644;&#23545;&#24212;&#30340;&#31616;&#21333;&#25991;&#26412;&#25552;&#31034;&#23545;&#30446;&#26631;&#21306;&#22495;&#26045;&#21152;&#23616;&#37096;&#24433;&#21709;&#65292;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#32534;&#36753;&#12290;&#36890;&#36807;&#36328;&#27880;&#24847;&#21147;&#21644;&#32972;&#26223;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00437v1 Announce Type: cross  Abstract: Recent developments in the field of diffusion models have demonstrated an exceptional capacity to generate high-quality prompt-conditioned image edits. Nevertheless, previous approaches have primarily relied on textual prompts for image editing, which tend to be less effective when making precise edits to specific objects or fine-grained regions within a scene containing single/multiple objects. We introduce a novel framework for zero-shot localized multi-object editing through a multi-diffusion process to overcome this challenge. This framework empowers users to perform various operations on objects within an image, such as adding, replacing, or editing $\textbf{many}$ objects in a complex scene $\textbf{in one pass}$. Our approach leverages foreground masks and corresponding simple text prompts that exert localized influences on the target regions resulting in high-fidelity image editing. A combination of cross-attention and backgrou
&lt;/p&gt;</description></item><item><title>HALC&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#35937;&#24187;&#35273;&#30340;&#26032;&#39062;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#30340;&#33258;&#21160;&#32858;&#28966;&#22522;&#20934;&#26426;&#21046;&#21644;&#20840;&#23616;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#25104;&#21151;&#20943;&#23569;OH&#32780;&#20445;&#25345;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#65292;&#21516;&#26102;&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#38598;&#25104;&#21040;&#20219;&#20309;LVLMs&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.00425</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#28966;&#28857;&#23545;&#27604;&#35299;&#30721;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#65306;HALC
&lt;/p&gt;
&lt;p&gt;
HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00425
&lt;/p&gt;
&lt;p&gt;
HALC&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#35937;&#24187;&#35273;&#30340;&#26032;&#39062;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#30340;&#33258;&#21160;&#32858;&#28966;&#22522;&#20934;&#26426;&#21046;&#21644;&#20840;&#23616;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#25104;&#21151;&#20943;&#23569;OH&#32780;&#20445;&#25345;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#65292;&#21516;&#26102;&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#38598;&#25104;&#21040;&#20219;&#20309;LVLMs&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#37322;&#22810;&#27169;&#24577;&#29615;&#22659;&#26041;&#38754;&#65292;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#21463;&#21040;&#23545;&#35937;&#24187;&#35273;&#65288;OH&#65289;&#30340;&#22256;&#25200;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;HALC&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;LVLMs&#20013;&#30340;OH&#12290;HALC&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#29420;&#29305;&#30340;&#32454;&#31890;&#24230;&#26368;&#20339;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#21516;&#26102;&#22312;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#25805;&#20316;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;HALC&#38598;&#25104;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#33258;&#21160;&#32858;&#28966;&#22522;&#20934;&#26426;&#21046;&#65288;&#23616;&#37096;&#65289;&#65292;&#22312;&#36816;&#34892;&#26102;&#32416;&#27491;&#20135;&#29983;&#24187;&#35273;&#30340;&#26631;&#35760;&#65292;&#20197;&#21450;&#19968;&#31181;&#19987;&#38376;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65288;&#20840;&#23616;&#65289;&#65292;&#20197;&#26174;&#30528;&#20943;&#23569;OH&#65292;&#21516;&#26102;&#20445;&#25345;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;HALC&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#38598;&#25104;&#21040;&#20219;&#20309;LVLMs&#20013;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#35777;&#26126;&#20102;HALC&#22312;&#20943;&#23569;OH&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00425v1 Announce Type: cross  Abstract: While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate the effectiveness of HALC in reducing OH, outperforming state-of-the-arts across four benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ML-UQ&#26657;&#20934;&#32479;&#35745;&#37327;&#30340;&#20351;&#29992;&#38382;&#39064;&#65292;&#21457;&#29616;&#19968;&#20123;&#32479;&#35745;&#37327;&#23545;&#20110;&#29983;&#25104;&#20998;&#24067;&#30340;&#36873;&#25321;&#36807;&#20110;&#25935;&#24863;&#65292;&#21487;&#33021;&#24433;&#21709;&#26657;&#20934;&#35786;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.00423</link><description>&lt;p&gt;
&#20351;&#29992;&#27169;&#25311;&#21442;&#32771;&#20540;&#39564;&#35777;ML-UQ&#26657;&#20934;&#32479;&#35745;&#37327;&#65306;&#19968;&#39033;&#25935;&#24863;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Validation of ML-UQ calibration statistics using simulated reference values: a sensitivity analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ML-UQ&#26657;&#20934;&#32479;&#35745;&#37327;&#30340;&#20351;&#29992;&#38382;&#39064;&#65292;&#21457;&#29616;&#19968;&#20123;&#32479;&#35745;&#37327;&#23545;&#20110;&#29983;&#25104;&#20998;&#24067;&#30340;&#36873;&#25321;&#36807;&#20110;&#25935;&#24863;&#65292;&#21487;&#33021;&#24433;&#21709;&#26657;&#20934;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;ML-UQ&#65289;&#26657;&#20934;&#32479;&#35745;&#37327;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#21442;&#32771;&#20540;&#65292;&#20027;&#35201;&#29992;&#20110;&#27604;&#36739;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#26657;&#20934;&#20960;&#20046;&#20174;&#19981;&#34987;&#39564;&#35777;&#65292;&#35786;&#26029;&#30041;&#32473;&#35835;&#32773;&#30340;&#21028;&#26029;&#12290;&#25552;&#20986;&#20102;&#22522;&#20110;&#23454;&#38469;&#19981;&#30830;&#23450;&#24615;&#23548;&#20986;&#30340;&#21512;&#25104;&#26657;&#20934;&#25968;&#25454;&#38598;&#30340;&#27169;&#25311;&#21442;&#32771;&#20540;&#65292;&#20197;&#24357;&#34917;&#36825;&#19968;&#38382;&#39064;&#12290;&#30001;&#20110;&#29992;&#20110;&#27169;&#25311;&#21512;&#25104;&#35823;&#24046;&#30340;&#29983;&#25104;&#27010;&#29575;&#20998;&#24067;&#36890;&#24120;&#27809;&#26377;&#32422;&#26463;&#65292;&#25152;&#20197;&#27169;&#25311;&#21442;&#32771;&#20540;&#23545;&#29983;&#25104;&#20998;&#24067;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#21487;&#33021;&#20250;&#25104;&#20026;&#38382;&#39064;&#65292;&#23545;&#26657;&#20934;&#35786;&#26029;&#20135;&#29983;&#24576;&#30097;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#24182;&#26174;&#31034;&#19968;&#20123;&#32479;&#35745;&#37327;&#23545;&#20110;&#29992;&#20110;&#39564;&#35777;&#26102;&#29983;&#25104;&#20998;&#24067;&#30340;&#36873;&#25321;&#36807;&#20110;&#25935;&#24863;&#65292;&#24403;&#29983;&#25104;&#20998;&#24067;&#26410;&#30693;&#26102;&#12290;&#20363;&#22914;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00423v1 Announce Type: cross  Abstract: Some popular Machine Learning Uncertainty Quantification (ML-UQ) calibration statistics do not have predefined reference values and are mostly used in comparative studies. In consequence, calibration is almost never validated and the diagnostic is left to the appreciation of the reader. Simulated reference values, based on synthetic calibrated datasets derived from actual uncertainties, have been proposed to palliate this problem. As the generative probability distribution for the simulation of synthetic errors is often not constrained, the sensitivity of simulated reference values to the choice of generative distribution might be problematic, shedding a doubt on the calibration diagnostic. This study explores various facets of this problem, and shows that some statistics are excessively sensitive to the choice of generative distribution to be used for validation when the generative distribution is unknown. This is the case, for instan
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#25913;&#36827;DRL&#23545;&#26465;&#20214;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#32773;&#31995;&#32479;&#20998;&#26512;&#20102;&#24403;&#20195;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.00420</link><description>&lt;p&gt;
&#32463;&#30001;&#23545;&#25239;&#25915;&#20987;&#21644;&#35757;&#32451;&#30340;&#31283;&#20581;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Robust Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00420
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#25913;&#36827;DRL&#23545;&#26465;&#20214;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#32773;&#31995;&#32479;&#20998;&#26512;&#20102;&#24403;&#20195;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26159;&#19968;&#31181;&#35757;&#32451;&#33258;&#20027;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#22312;&#20247;&#25152;&#21608;&#30693;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36731;&#24494;&#26465;&#20214;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#29992;&#24615;&#65292;DRL&#24517;&#39035;&#23637;&#31034;&#20986;&#21487;&#20449;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#25552;&#39640;DRL&#23545;&#26465;&#20214;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#26159;&#19968;&#31181;&#25913;&#36827;&#26041;&#24335;&#65292;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#38024;&#23545;&#29615;&#22659;&#21160;&#24577;&#30340;&#36866;&#24403;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#65292;&#23545;&#24403;&#20195;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#31995;&#32479;&#22320;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#30446;&#26631;&#21644;&#25805;&#20316;&#26426;&#21046;&#12290;&#36825;&#31181;&#20998;&#31867;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#22914;&#20309;&#26377;&#25928;&#35780;&#20272;DRL&#20195;&#29702;&#30340;&#24674;&#22797;&#21147;&#30340;&#35814;&#32454;&#35265;&#35299;&#65292;&#20174;&#32780;&#20026;&#24320;&#36767;DRL&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36947;&#36335;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00420v1 Announce Type: cross  Abstract: Deep Reinforcement Learning (DRL) is an approach for training autonomous agents across various complex environments. Despite its significant performance in well known environments, it remains susceptible to minor conditions variations, raising concerns about its reliability in real-world applications. To improve usability, DRL must demonstrate trustworthiness and robustness. A way to improve robustness of DRL to unknown changes in the conditions is through Adversarial Training, by training the agent against well suited adversarial attacks on the dynamics of the environment. Addressing this critical issue, our work presents an in-depth analysis of contemporary adversarial attack methodologies, systematically categorizing them and comparing their objectives and operational mechanisms. This classification offers a detailed insight into how adversarial attacks effectively act for evaluating the resilience of DRL agents, thereby paving the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#38543;&#26426;&#20559;&#22909;&#32763;&#36716;&#30340;&#31574;&#30053;&#20248;&#21270;&#36890;&#29992;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#23384;&#22312;&#22024;&#26434;&#21453;&#39304;&#26102;&#30340;DPO&#31639;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20154;&#31867;&#20852;&#36259;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00409</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;DPO: &#29992;&#26377;&#22122;&#21453;&#39304;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Provably Robust DPO: Aligning Language Models with Noisy Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00409
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#38543;&#26426;&#20559;&#22909;&#32763;&#36716;&#30340;&#31574;&#30053;&#20248;&#21270;&#36890;&#29992;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#23384;&#22312;&#22024;&#26434;&#21453;&#39304;&#26102;&#30340;DPO&#31639;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20154;&#31867;&#20852;&#36259;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20174;&#22522;&#20110;&#21916;&#22909;&#21453;&#39304;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#19982;&#20154;&#31867;&#20852;&#36259;&#23545;&#40784;&#30340;&#26377;&#21069;&#26223;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#40784;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23545;&#39640;&#36136;&#37327;&#20154;&#31867;&#21916;&#22909;&#25968;&#25454;&#30340;&#20381;&#36182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26500;&#25104;&#20102;&#29942;&#39048;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25968;&#25454;&#38598;&#20013;&#26377;&#22122;&#65288;&#19981;&#27491;&#30830;&#21644;&#27169;&#31946;&#65289;&#30340;&#20559;&#22909;&#23545;&#21487;&#33021;&#20250;&#38480;&#21046;&#35821;&#35328;&#27169;&#22411;&#20934;&#30830;&#25429;&#25417;&#20154;&#31867;&#24847;&#22270;&#12290;&#34429;&#28982;&#20174;&#19994;&#32773;&#26368;&#36817;&#25552;&#20986;&#20102;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#20943;&#36731;&#22122;&#22768;&#20559;&#22909;&#30340;&#24433;&#21709;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#24037;&#20316;&#23436;&#25972;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#38754;&#21521;&#22312;&#38543;&#26426;&#20559;&#22909;&#32763;&#36716;&#23384;&#22312;&#30340;&#31574;&#30053;&#20248;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#31639;&#27861;&#65292;&#22240;&#20026;&#23427;&#20551;&#35774;&#20559;&#22909;&#36981;&#24490; Bradley-Te
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00409v1 Announce Type: cross  Abstract: Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive.   In this work, we aim to bridge this gap by by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the direct preference optimization (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Te
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#20998;&#24418;&#25554;&#20540;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;&#8220;&#26368;&#25509;&#36817;Hurst&#31574;&#30053;&#8221;&#12289;&#8220;&#26368;&#25509;&#36817;&#20540;&#31574;&#30053;&#8221;&#21644;&#8220;&#20844;&#24335;&#31574;&#30053;&#8221;&#65292;&#24182;&#20351;&#29992;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#19968;&#32452;&#26469;&#33258;&#32599;&#39532;&#23612;&#20122;&#24067;&#25289;&#32034;&#22827;&#24066;&#27668;&#35937;&#35760;&#24405;&#30340;&#31169;&#20154;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#20123;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.00403</link><description>&lt;p&gt;
&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#20248;&#21270;&#30340;&#32972;&#26223;&#19979;&#30340;&#20998;&#24418;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Fractal interpolation in the context of prediction accuracy optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00403
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#20998;&#24418;&#25554;&#20540;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;&#8220;&#26368;&#25509;&#36817;Hurst&#31574;&#30053;&#8221;&#12289;&#8220;&#26368;&#25509;&#36817;&#20540;&#31574;&#30053;&#8221;&#21644;&#8220;&#20844;&#24335;&#31574;&#30053;&#8221;&#65292;&#24182;&#20351;&#29992;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#19968;&#32452;&#26469;&#33258;&#32599;&#39532;&#23612;&#20122;&#24067;&#25289;&#32034;&#22827;&#24066;&#27668;&#35937;&#35760;&#24405;&#30340;&#31169;&#20154;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#20123;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20351;&#29992;&#20998;&#24418;&#25554;&#20540;&#25216;&#26415;&#20248;&#21270;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20551;&#35774;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#19982;&#20351;&#29992;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#26041;&#38754;&#23494;&#20999;&#30456;&#20851;&#65292;&#36981;&#24490;&#8220;&#22403;&#22334;&#36827;&#65292;&#22403;&#22334;&#20986;&#8221;&#30340;&#21407;&#21017;&#12290;&#20026;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#22686;&#21152;&#25968;&#25454;&#38598;&#65292;&#25968;&#25454;&#31185;&#23398;&#23478;&#26368;&#20851;&#27880;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#24212;&#23613;&#21487;&#33021;&#22320;&#32039;&#38543;&#21407;&#22987;&#25968;&#25454;&#30340;&#23454;&#38469;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00403v1 Announce Type: new  Abstract: This paper focuses on the hypothesis of optimizing time series predictions using fractal interpolation techniques. In general, the accuracy of machine learning model predictions is closely related to the quality and quantitative aspects of the data used, following the principle of \textit{garbage-in, garbage-out}. In order to quantitatively and qualitatively augment datasets, one of the most prevalent concerns of data scientists is to generate synthetic data, which should follow as closely as possible the actual pattern of the original data.   This study proposes three different data augmentation strategies based on fractal interpolation, namely the \textit{Closest Hurst Strategy}, \textit{Closest Values Strategy} and \textit{Formula Strategy}. To validate the strategies, we used four public datasets from the literature, as well as a private dataset obtained from meteorological records in the city of Brasov, Romania. The prediction resul
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;DNN&#30340;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30830;&#20445;&#38381;&#29615;&#31283;&#23450;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#20248;&#21270;&#21442;&#25968;&#20197;&#23454;&#29616;&#25913;&#36827;&#30340;&#25511;&#21046;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#36319;&#36394;&#35823;&#24046;&#30340;&#26126;&#30830;&#19978;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.00381</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25289;&#26684;&#26391;&#26085;&#31995;&#32479;&#21453;&#27493;&#36712;&#36857;&#36319;&#36394;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Structured Deep Neural Networks-Based Backstepping Trajectory Tracking Control for Lagrangian Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00381
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;DNN&#30340;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30830;&#20445;&#38381;&#29615;&#31283;&#23450;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#20248;&#21270;&#21442;&#25968;&#20197;&#23454;&#29616;&#25913;&#36827;&#30340;&#25511;&#21046;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#36319;&#36394;&#35823;&#24046;&#30340;&#26126;&#30830;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#23398;&#20064;&#25511;&#21046;&#22120;&#65292;&#22240;&#20026;&#20854;&#20986;&#33394;&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#23545;&#38381;&#29615;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#24615;&#33021;&#20998;&#26512;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;DNN&#30340;&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#37319;&#29992;&#21453;&#25512;&#25216;&#26415;&#23454;&#29616;&#25289;&#26684;&#26391;&#26085;&#31995;&#32479;&#30340;&#36712;&#36857;&#36319;&#36394;&#25511;&#21046;&#12290;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#22120;&#21487;&#20197;&#30830;&#20445;&#20219;&#20309;&#20860;&#23481;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#23454;&#29616;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36319;&#36394;&#35823;&#24046;&#30340;&#26126;&#30830;&#19978;&#38480;&#65292;&#36825;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#25511;&#21046;&#21442;&#25968;&#26469;&#23454;&#29616;&#25152;&#38656;&#30340;&#36319;&#36394;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#24403;&#31995;&#32479;&#27169;&#22411;&#26410;&#30693;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25289;&#26684;&#26391;&#26085;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00381v1 Announce Type: cross  Abstract: Deep neural networks (DNN) are increasingly being used to learn controllers due to their excellent approximation capabilities. However, their black-box nature poses significant challenges to closed-loop stability guarantees and performance analysis. In this paper, we introduce a structured DNN-based controller for the trajectory tracking control of Lagrangian systems using backing techniques. By properly designing neural network structures, the proposed controller can ensure closed-loop stability for any compatible neural network parameters. In addition, improved control performance can be achieved by further optimizing neural network parameters. Besides, we provide explicit upper bounds on tracking errors in terms of controller parameters, which allows us to achieve the desired tracking performance by properly selecting the controller parameters. Furthermore, when system models are unknown, we propose an improved Lagrangian neural net
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#36843;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#19978;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00376</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#30340;&#19981;&#21464;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Invariant Test-Time Adaptation for Vision-Language Model Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#36843;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#19978;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00376v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#20351;&#20854;&#22312;&#20247;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#38271;&#23614;&#20219;&#21153;&#65288;&#22914;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#65289;&#26102;&#26174;&#31034;&#20986;&#26126;&#26174;&#23616;&#38480;&#65292;&#36825;&#26159;&#30001;&#20110;&#8220;&#20915;&#31574;&#25463;&#24452;&#8221;&#23548;&#33268;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21463;&#38480;&#12290;&#26412;&#25991;&#21457;&#29616;CLIP&#27169;&#22411;&#20855;&#26377;&#20016;&#23500;&#30340;&#29305;&#24449;&#38598;&#65292;&#28085;&#30422;&#20102;&#26082;&#26377;&#30340;\textit{&#26399;&#26395;&#19981;&#21464;&#22240;&#26524;&#29305;&#24449;}&#21448;&#26377;&#30340;\textit{&#19981;&#24076;&#26395;&#30340;&#20915;&#31574;&#25463;&#24452;}&#12290;&#27492;&#22806;&#65292;CLIP&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#28304;&#33258;&#20854;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#20197;&#31526;&#21512;&#29305;&#23450;&#20219;&#21153;&#35201;&#27714;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#20248;&#21270;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20419;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00376v1 Announce Type: cross  Abstract: Vision-language foundation models have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive image-text paired datasets. However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of "decision shortcuts" that hinders their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \textit{desired invariant causal features} and \textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, this paper introduces a test-time prompt tuning paradigm that optimizes a learnable prompt, thereby compelling the model to exploit genuine causal invariant features while dis
&lt;/p&gt;</description></item><item><title>&#35299;&#32544;&#34920;&#31034;&#23545;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#36825;&#19968;&#22522;&#26412;&#19979;&#28216;&#20219;&#21153;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#34920;&#31034;&#20449;&#24687;&#37327;&#30340;&#20016;&#23500;&#31243;&#24230;&#26356;&#33021;&#24433;&#21709;&#19979;&#28216;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00352</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#35299;&#32544;&#65306;&#23545;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#24517;&#35201;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting Disentanglement in Downstream Tasks: A Study on Its Necessity for Abstract Visual Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00352
&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23545;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#36825;&#19968;&#22522;&#26412;&#19979;&#28216;&#20219;&#21153;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#34920;&#31034;&#20449;&#24687;&#37327;&#30340;&#20016;&#23500;&#31243;&#24230;&#26356;&#33021;&#24433;&#21709;&#19979;&#28216;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#35299;&#32544;&#34920;&#31034;&#23588;&#20026;&#29702;&#24819;&#65292;&#22240;&#20026;&#23427;&#20197;&#21487;&#20998;&#31163;&#19988;&#32039;&#20945;&#30340;&#27169;&#24335;&#32534;&#30721;&#25968;&#25454;&#30340;&#29983;&#25104;&#22240;&#23376;&#12290;&#30740;&#31350;&#20154;&#21592;&#20513;&#23548;&#21033;&#29992;&#35299;&#32544;&#34920;&#31034;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#35299;&#32544;&#34920;&#31034;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22522;&#26412;&#30340;&#19979;&#28216;&#20219;&#21153;&#8212;&#8212;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#20013;&#65292;&#22522;&#20110;&#32500;&#24230;&#30340;&#35299;&#32544;&#34920;&#31034;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#21453;&#39539;&#20102;&#35299;&#32544;&#30340;&#24517;&#35201;&#24615;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#25968;&#25454;&#38598;&#12289;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21644;&#19979;&#28216;&#32593;&#32476;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34920;&#31034;&#20449;&#24687;&#37327;&#30340;&#20016;&#23500;&#31243;&#24230;&#26159;&#19979;&#28216;&#24615;&#33021;&#30340;&#26356;&#22909;&#25351;&#26631;&#65292;&#32780;&#19981;&#26159;&#35299;&#32544;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#34920;&#31034;&#20449;&#24687;&#37327;&#19982;&#35299;&#32544;&#20043;&#38388;&#23384;&#22312;&#31215;&#26497;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00352v1 Announce Type: cross  Abstract: In representation learning, a disentangled representation is highly desirable as it encodes generative factors of data in a separable and compact pattern. Researchers have advocated leveraging disentangled representations to complete downstream tasks with encouraging empirical evidence. This paper further investigates the necessity of disentangled representation in downstream applications. Specifically, we show that dimension-wise disentangled representations are unnecessary on a fundamental downstream task, abstract visual reasoning. We provide extensive empirical evidence against the necessity of disentanglement, covering multiple datasets, representation learning methods, and downstream network architectures. Furthermore, our findings suggest that the informativeness of representations is a better indicator of downstream performance than disentanglement. Finally, the positive correlation between informativeness and disentanglement e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#36866;&#24212;&#22810;&#26679;&#21270;&#25252;&#29702;&#25509;&#25910;&#32773;&#21709;&#24212;&#30340;&#40065;&#26834;&#25252;&#29702;&#20154;&#21592;&#25919;&#31574;&#65292;&#20197;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25919;&#31574;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00344</link><description>&lt;p&gt;
&#37319;&#29992;&#22810;&#26679;&#21270;&#30340;&#21512;&#20316;&#34892;&#20026;&#21644;&#23545;&#25239;&#26679;&#24335;&#37319;&#26679;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25919;&#31574;&#30340;&#40065;&#26834;&#24615;&#29992;&#20110;&#36741;&#21161;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Robustifying a Policy in Multi-Agent RL with Diverse Cooperative Behavior and Adversarial Style Sampling for Assistive Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00344
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#36866;&#24212;&#22810;&#26679;&#21270;&#25252;&#29702;&#25509;&#25910;&#32773;&#21709;&#24212;&#30340;&#40065;&#26834;&#25252;&#29702;&#20154;&#21592;&#25919;&#31574;&#65292;&#20197;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#25919;&#31574;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20154;&#25658;&#24102;&#21160;&#20316;&#38556;&#30861;&#32773;&#30340;&#33258;&#20027;&#21327;&#21161;&#26159;&#33258;&#20027;&#26426;&#22120;&#20154;&#31995;&#32479;&#26368;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25253;&#36947;&#20102;&#22312;&#21307;&#30103;&#39046;&#22495;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36741;&#21161;&#20219;&#21153;&#21487;&#20197;&#34987;&#21046;&#23450;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#26377;&#20004;&#20010;&#26234;&#33021;&#20307;&#65306;&#25252;&#29702;&#20154;&#21592;&#21644;&#25252;&#29702;&#25509;&#25910;&#32773;&#12290;&#28982;&#32780;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#25919;&#31574;&#24448;&#24448;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#25919;&#31574;&#25935;&#24863;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#26377;&#32032;&#30340;&#25252;&#29702;&#20154;&#21592;&#25919;&#31574;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#25252;&#29702;&#25509;&#25910;&#32773;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35757;&#32451;&#36866;&#24212;&#22810;&#26679;&#21270;&#25252;&#29702;&#25509;&#25910;&#32773;&#21709;&#24212;&#30340;&#40065;&#26834;&#25252;&#29702;&#20154;&#21592;&#25919;&#31574;&#30340;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#22810;&#26679;&#21270;&#30340;&#25252;&#29702;&#25509;&#25910;&#32773;&#21709;&#24212;&#26159;&#36890;&#36807;&#35797;&#39564;&#21644;&#38169;&#35823;&#33258;&#20027;&#23398;&#20064;&#30340;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#25252;&#29702;&#32773;&#25919;&#31574;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25252;&#29702;&#25509;&#25910;&#32773;&#21709;&#24212;&#36827;&#34892;&#23545;&#25239;&#37319;&#26679;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00344v1 Announce Type: cross  Abstract: Autonomous assistance of people with motor impairments is one of the most promising applications of autonomous robotic systems. Recent studies have reported encouraging results using deep reinforcement learning (RL) in the healthcare domain. Previous studies showed that assistive tasks can be formulated as multi-agent RL, wherein there are two agents: a caregiver and a care-receiver. However, policies trained in multi-agent RL are often sensitive to the policies of other agents. In such a case, a trained caregiver's policy may not work for different care-receivers. To alleviate this issue, we propose a framework that learns a robust caregiver's policy by training it for diverse care-receiver responses. In our framework, diverse care-receiver responses are autonomously learned through trials and errors. In addition, to robustify the care-giver's policy, we propose a strategy for sampling a care-receiver's response in an adversarial mann
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#38750;&#32447;&#24615;&#25289;&#26222;&#25289;&#26031;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#39564;&#35777;&#20102;&#27169;&#22411;&#22312;&#22270;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00337</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#23618;&#25299;&#25169;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Sheaf Diffusion in Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00337
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#38750;&#32447;&#24615;&#25289;&#26222;&#25289;&#26031;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#39564;&#35777;&#20102;&#27169;&#22411;&#22312;&#22270;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#23558;&#38750;&#32447;&#24615;&#25289;&#26222;&#25289;&#26031;&#24341;&#20837;Sheaf&#31070;&#32463;&#32593;&#32476;&#20197;&#25552;&#21319;&#20854;&#22312;&#22270;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#30410;&#22788;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#29702;&#35299;&#36825;&#31181;&#38750;&#32447;&#24615;&#23545;&#25193;&#25955;&#21160;&#21147;&#23398;&#12289;&#20449;&#21495;&#20256;&#25773;&#20197;&#21450;&#31163;&#25955;&#26102;&#38388;&#35774;&#32622;&#19979;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;&#23454;&#39564;&#20998;&#26512;&#65292;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#39564;&#35777;&#27169;&#22411;&#19981;&#21516;&#29256;&#26412;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#28966;&#28857;&#20174;&#26368;&#21021;&#30340;&#29702;&#35770;&#25506;&#32034;&#36716;&#21521;&#23637;&#31034;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00337v1 Announce Type: new  Abstract: This work focuses on exploring the potential benefits of introducing a nonlinear Laplacian in Sheaf Neural Networks for graph-related tasks. The primary aim is to understand the impact of such nonlinearity on diffusion dynamics, signal propagation, and performance of neural network architectures in discrete-time settings. The study primarily emphasizes experimental analysis, using real-world and synthetic datasets to validate the practical effectiveness of different versions of the model. This approach shifts the focus from an initial theoretical exploration to demonstrating the practical utility of the proposed model.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36923;&#36753;&#36830;&#25509;&#35789;&#30340;&#21452;&#37325;&#21464;&#37327;&#26469;&#35299;&#20915;&#25463;&#24452;&#28385;&#36275;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#36923;&#36753;&#32422;&#26463;&#30340;&#26694;&#26550;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#21644;&#32422;&#26463;&#28385;&#36275;&#26041;&#38754;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.00329</link><description>&lt;p&gt;
&#22312;&#19981;&#28385;&#36275;&#25463;&#24452;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#36923;&#36753;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Learning with Logical Constraints but without Shortcut Satisfaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00329
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36923;&#36753;&#36830;&#25509;&#35789;&#30340;&#21452;&#37325;&#21464;&#37327;&#26469;&#35299;&#20915;&#25463;&#24452;&#28385;&#36275;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#36923;&#36753;&#32422;&#26463;&#30340;&#26694;&#26550;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#21644;&#32422;&#26463;&#28385;&#36275;&#26041;&#38754;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#23558;&#36923;&#36753;&#30693;&#35782;&#32534;&#30721;&#20026;&#39069;&#22806;&#30340;&#25439;&#22833;&#20989;&#25968;&#23558;&#36923;&#36753;&#32422;&#26463;&#25972;&#21512;&#21040;&#28145;&#24230;&#23398;&#20064;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#36890;&#36807;&#25463;&#24452;&#34394;&#20551;&#22320;&#28385;&#36275;&#20102;&#36923;&#36753;&#32422;&#26463;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#36923;&#36753;&#32422;&#26463;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#36923;&#36753;&#36830;&#25509;&#35789;&#30340;&#21452;&#37325;&#21464;&#37327;&#26469;&#35299;&#20915;&#25463;&#24452;&#28385;&#36275;&#38382;&#39064;&#65292;&#23545;&#32422;&#26463;&#30340;&#28385;&#36275;&#26041;&#24335;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#21464;&#20998;&#26694;&#26550;&#65292;&#20854;&#20013;&#32534;&#30721;&#30340;&#36923;&#36753;&#32422;&#26463;&#34987;&#34920;&#36798;&#20026;&#19968;&#20010;&#20998;&#24067;&#25439;&#22833;&#65292;&#19982;&#27169;&#22411;&#30340;&#21407;&#22987;&#35757;&#32451;&#25439;&#22833;&#20860;&#23481;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#29305;&#24615;&#65292;&#23454;&#39564;&#35780;&#20272;&#26174;&#31034;&#20854;&#22312;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#21644;&#32422;&#26463;&#28385;&#36275;&#26041;&#38754;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00329v1 Announce Type: new  Abstract: Recent studies in neuro-symbolic learning have explored the integration of logical knowledge into deep learning via encoding logical constraints as an additional loss function. However, existing approaches tend to vacuously satisfy logical constraints through shortcuts, failing to fully exploit the knowledge. In this paper, we present a new framework for learning with logical constraints. Specifically, we address the shortcut satisfaction issue by introducing dual variables for logical connectives, encoding how the constraint is satisfied. We further propose a variational framework where the encoded logical constraint is expressed as a distributional loss that is compatible with the model's original training loss. The theoretical analysis shows that the proposed approach bears salient properties, and the experimental evaluations demonstrate its superior performance in both model generalizability and constraint satisfaction.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21270;&#31526;&#21495;&#25509;&#22320;&#36807;&#31243;&#65292;&#26377;&#25928;&#22320;&#26725;&#25509;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#31526;&#21495;&#32422;&#26463;&#27714;&#35299;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.00323</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#30340;&#36719;&#21270;&#31526;&#21495;&#25509;&#22320;
&lt;/p&gt;
&lt;p&gt;
Softened Symbol Grounding for Neuro-symbolic Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00323
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21270;&#31526;&#21495;&#25509;&#22320;&#36807;&#31243;&#65292;&#26377;&#25928;&#22320;&#26725;&#25509;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#31526;&#21495;&#32422;&#26463;&#27714;&#35299;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#36890;&#24120;&#21253;&#25324;&#20004;&#20010;&#29420;&#31435;&#19990;&#30028;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#31526;&#21495;&#32422;&#26463;&#27714;&#35299;&#65292;&#20854;&#25104;&#21151;&#21462;&#20915;&#20110;&#31526;&#21495;&#25509;&#22320;&#65292;&#36825;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36719;&#21270;&#31526;&#21495;&#25509;&#22320;&#36807;&#31243;&#65292;&#24357;&#21512;&#20102;&#20004;&#20010;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26694;&#26550;&#12290;&#25216;&#26415;&#19978;&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;(1)&#23558;&#31526;&#21495;&#35299;&#29366;&#24577;&#24314;&#27169;&#20026;Boltzmann&#20998;&#24067;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#29366;&#24577;&#25628;&#32034;&#65292;&#24182;&#20419;&#36827;&#20102;&#32593;&#32476;&#35757;&#32451;&#21644;&#31526;&#21495;&#25512;&#29702;&#20043;&#38388;&#30340;&#20114;&#24800;&#20114;&#21033;&#20132;&#20114;&#65307;(2)&#21033;&#29992;&#25237;&#24433;&#21644;SMT&#35299;&#31639;&#22120;&#30340;&#26032;&#22411;MCMC&#25216;&#26415;&#65292;&#39640;&#25928;&#22320;&#20174;&#26029;&#24320;&#30340;&#31526;&#21495;&#35299;&#31354;&#38388;&#20013;&#37319;&#26679;&#65307;(3)&#19968;&#31181;&#36864;&#28779;&#26426;&#21046;&#65292;&#21487;&#20197;&#25670;&#33073;&#38519;&#20837;&#27425;&#20248;&#31526;&#21495;&#25509;&#22320;&#30340;&#22256;&#22659;&#12290;&#23545;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#26174;&#33879;&#25552;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00323v1 Announce Type: new  Abstract: Neuro-symbolic learning generally consists of two separated worlds, i.e., neural network training and symbolic constraint solving, whose success hinges on symbol grounding, a fundamental problem in AI. This paper presents a novel, softened symbol grounding process, bridging the gap between the two worlds, and resulting in an effective and efficient neuro-symbolic learning framework. Technically, the framework features (1) modeling of symbol solution states as a Boltzmann distribution, which avoids expensive state searching and facilitates mutually beneficial interactions between network training and symbolic reasoning;(2) a new MCMC technique leveraging projection and SMT solvers, which efficiently samples from disconnected symbol solution spaces; (3) an annealing mechanism that can escape from %being trapped into sub-optimal symbol groundings. Experiments with three representative neuro symbolic learning tasks demonstrate that, owining 
&lt;/p&gt;</description></item><item><title>DEEP-IoT&#36890;&#36807;&#8220;&#26356;&#22810;&#30417;&#21548;&#65292;&#26356;&#23569;&#20256;&#36755;&#8221;&#30340;&#31574;&#30053;&#65292;&#25361;&#25112;&#21644;&#36716;&#21464;&#20102;&#20256;&#32479;&#30340;&#29289;&#32852;&#32593;&#36890;&#20449;&#27169;&#22411;&#65292;&#22823;&#24133;&#38477;&#20302;&#33021;&#32791;&#24182;&#25552;&#39640;&#35774;&#22791;&#23551;&#21629;&#12290;</title><link>https://arxiv.org/abs/2403.00321</link><description>&lt;p&gt;
DEEP-IoT: &#19979;&#34892;&#22686;&#24378;&#22411;&#39640;&#25928;&#33021;&#29289;&#32852;&#32593;
&lt;/p&gt;
&lt;p&gt;
DEEP-IoT: Downlink-Enhanced Efficient-Power Internet of Things
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00321
&lt;/p&gt;
&lt;p&gt;
DEEP-IoT&#36890;&#36807;&#8220;&#26356;&#22810;&#30417;&#21548;&#65292;&#26356;&#23569;&#20256;&#36755;&#8221;&#30340;&#31574;&#30053;&#65292;&#25361;&#25112;&#21644;&#36716;&#21464;&#20102;&#20256;&#32479;&#30340;&#29289;&#32852;&#32593;&#36890;&#20449;&#27169;&#22411;&#65292;&#22823;&#24133;&#38477;&#20302;&#33021;&#32791;&#24182;&#25552;&#39640;&#35774;&#22791;&#23551;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DEEP-IoT&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#38761;&#21629;&#24847;&#20041;&#30340;&#36890;&#20449;&#33539;&#20363;&#65292;&#26088;&#22312;&#37325;&#26032;&#23450;&#20041;&#29289;&#32852;&#32593;&#35774;&#22791;&#20043;&#38388;&#30340;&#36890;&#20449;&#26041;&#24335;&#12290;&#36890;&#36807;&#24320;&#21019;&#24615;&#30340;&#8220;&#26356;&#22810;&#30417;&#21548;&#65292;&#26356;&#23569;&#20256;&#36755;&#8221;&#30340;&#31574;&#30053;&#65292;DEEP-IoT&#25361;&#25112;&#21644;&#36716;&#21464;&#20102;&#20256;&#32479;&#30340;&#21457;&#36865;&#26041;&#65288;&#29289;&#32852;&#32593;&#35774;&#22791;&#65289;&#20026;&#20013;&#24515;&#30340;&#36890;&#20449;&#27169;&#22411;&#65292;&#23558;&#25509;&#25910;&#26041;&#65288;&#25509;&#20837;&#28857;&#65289;&#20316;&#20026;&#20851;&#38190;&#35282;&#33394;&#65292;&#20174;&#32780;&#38477;&#20302;&#33021;&#32791;&#24182;&#24310;&#38271;&#35774;&#22791;&#23551;&#21629;&#12290;&#25105;&#20204;&#19981;&#20165;&#27010;&#24565;&#21270;&#20102;DEEP-IoT&#65292;&#36824;&#36890;&#36807;&#22312;&#31364;&#24102;&#31995;&#32479;&#20013;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#30340;&#21453;&#39304;&#20449;&#36947;&#32534;&#30721;&#26469;&#23454;&#29616;&#23427;&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;IoT&#21333;&#20803;&#30340;&#36816;&#34892;&#23551;&#21629;&#26174;&#33879;&#25552;&#39640;&#65292;&#27604;&#20351;&#29992;Turbo&#21644;Polar&#32534;&#30721;&#30340;&#20256;&#32479;&#31995;&#32479;&#25552;&#39640;&#20102;&#26368;&#22810;52.71%&#12290;&#36825;&#19968;&#36827;&#23637;&#26631;&#24535;&#30528;&#19968;&#31181;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00321v1 Announce Type: cross  Abstract: At the heart of the Internet of Things (IoT) -- a domain witnessing explosive growth -- the imperative for energy efficiency and the extension of device lifespans has never been more pressing. This paper presents DEEP-IoT, a revolutionary communication paradigm poised to redefine how IoT devices communicate. Through a pioneering "listen more, transmit less" strategy, DEEP-IoT challenges and transforms the traditional transmitter (IoT devices)-centric communication model to one where the receiver (the access point) play a pivotal role, thereby cutting down energy use and boosting device longevity. We not only conceptualize DEEP-IoT but also actualize it by integrating deep learning-enhanced feedback channel codes within a narrow-band system. Simulation results show a significant enhancement in the operational lifespan of IoT cells -- surpassing traditional systems using Turbo and Polar codes by up to 52.71%. This leap signifies a paradi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#31649;&#29702;&#38382;&#39064;&#65292;&#22914;&#24211;&#23384;&#31649;&#29702;&#12289;&#21160;&#24577;&#23450;&#20215;&#21644;&#25512;&#33616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#22823;&#22411;&#31649;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#36229;&#36234;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#35299;&#20915;&#31649;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#36328;&#39046;&#22495;&#20915;&#31574;&#21327;&#35843;&#65292;&#35777;&#26126;&#20102;&#22312;&#22797;&#26434;&#21160;&#24577;&#21830;&#19994;&#29615;&#22659;&#20013;DRL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00318</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#31649;&#29702;&#38382;&#39064;: &#36808;&#21521;&#22823;&#22411;&#31649;&#29702;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Solving Management Problems: Towards A Large Management Mode
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00318
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#31649;&#29702;&#38382;&#39064;&#65292;&#22914;&#24211;&#23384;&#31649;&#29702;&#12289;&#21160;&#24577;&#23450;&#20215;&#21644;&#25512;&#33616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#22823;&#22411;&#31649;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#36229;&#36234;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#35299;&#20915;&#31649;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#36328;&#39046;&#22495;&#20915;&#31574;&#21327;&#35843;&#65292;&#35777;&#26126;&#20102;&#22312;&#22797;&#26434;&#21160;&#24577;&#21830;&#19994;&#29615;&#22659;&#20013;DRL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21253;&#25324;&#24211;&#23384;&#31649;&#29702;&#12289;&#21160;&#24577;&#23450;&#20215;&#21644;&#25512;&#33616;&#22312;&#20869;&#30340;&#31649;&#29702;&#38382;&#39064;&#12290;&#35813;DRL&#26041;&#27861;&#26377;&#28508;&#21147;&#22522;&#20110;&#29305;&#23450;&#30340;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#24320;&#21457;&#20986;&#19968;&#20010;&#22823;&#22411;&#31649;&#29702;&#27169;&#22411;&#65292;&#20174;&#32780;&#24418;&#25104;&#36866;&#29992;&#20110;&#21508;&#31181;&#31649;&#29702;&#20219;&#21153;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#33539;&#24335;&#12290;&#25105;&#20204;&#35797;&#22270;&#22312;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#35299;&#20915;&#38382;&#39064;&#65292;&#32771;&#34385;&#21040;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#29983;&#25104;&#24335;&#20915;&#31574;&#36827;&#34892;&#22522;&#30784;&#20915;&#31574;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#21327;&#35843;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#22522;&#20110;DRL&#30340;&#26694;&#26550;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#30340;&#21830;&#19994;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00318v1 Announce Type: new  Abstract: We introduce a deep reinforcement learning (DRL) approach for solving management problems including inventory management, dynamic pricing, and recommendation. This DRL approach has the potential to lead to a large management model based on certain transformer neural network structures, resulting in an artificial general intelligence paradigm for various management tasks. Traditional methods have limitations for solving complex real-world problems, and we demonstrate how DRL can surpass existing heuristic approaches for solving management tasks. We aim to solve the problems in a unified framework, considering the interconnections between different tasks. Central to our methodology is the development of a foundational decision model coordinating decisions across the different domains through generative decision-making. Our experimental results affirm the effectiveness of our DRL-based framework in complex and dynamic business environments.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36777;&#25252;&#20102;&#37319;&#29992;&#8220;&#21487;&#29702;&#35299;&#30340;&#20154;&#24037;&#26234;&#33021;&#8221;&#26631;&#31614;&#20316;&#20026;&#26367;&#20195;&#8220;XAI&#8221;&#65292;&#20197;&#36991;&#20813;&#22260;&#32469;XAI&#30446;&#26631;&#21644;&#30446;&#30340;&#30340;&#28151;&#20081;&#65292;&#24182;&#20027;&#24352;&#37319;&#29992;&#26356;&#36866;&#21512;&#30340;&#23454;&#29992;&#29702;&#35299;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2403.00315</link><description>&lt;p&gt;
&#20999;&#25481;XAI&#20013;&#30340;X: &#20026;&#21487;&#29702;&#35299;&#30340;&#20154;&#24037;&#26234;&#33021;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
Axe the X in XAI: A Plea for Understandable AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00315
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36777;&#25252;&#20102;&#37319;&#29992;&#8220;&#21487;&#29702;&#35299;&#30340;&#20154;&#24037;&#26234;&#33021;&#8221;&#26631;&#31614;&#20316;&#20026;&#26367;&#20195;&#8220;XAI&#8221;&#65292;&#20197;&#36991;&#20813;&#22260;&#32469;XAI&#30446;&#26631;&#21644;&#30446;&#30340;&#30340;&#28151;&#20081;&#65292;&#24182;&#20027;&#24352;&#37319;&#29992;&#26356;&#36866;&#21512;&#30340;&#23454;&#29992;&#29702;&#35299;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#19968;&#31687;&#35770;&#25991;&#20013;&#65292;Erasmus&#31561;&#20154;(2021)&#36777;&#25252;&#20102;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;(XAI)&#20013;&#26415;&#35821;&#8220;&#35299;&#37322;&#8221;&#23384;&#22312;&#30340;&#27495;&#20041;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#21746;&#23398;&#31185;&#23398;&#20013;&#22235;&#31181;&#19981;&#21516;&#30340;&#29616;&#23384;&#35299;&#37322;&#27169;&#24335;&#20043;&#19968;&#26469;&#35299;&#20915;&#65306;&#28436;&#32462;-&#35268;&#33539;&#12289;&#24402;&#32435;-&#32479;&#35745;&#12289;&#22240;&#26524;&#26426;&#26800;&#21644;&#26032;&#26426;&#21046;&#20027;&#20041;&#27169;&#24335;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#23637;&#31034;&#20102;&#20316;&#32773;&#22768;&#31216;&#36825;&#20123;&#27169;&#24335;&#21487;&#20197;&#20687;&#23545;&#24453;&#20219;&#20309;&#33258;&#28982;&#29616;&#35937;&#19968;&#26679;&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35828;&#27861;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#36824;&#25552;&#20986;&#20102;&#26356;&#19968;&#33324;&#30340;&#35770;&#28857;&#65292;&#35828;&#26126;&#20102;XAI&#25991;&#29486;&#20013;&#30446;&#21069;&#20351;&#29992;&#30340;&#21487;&#35299;&#37322;&#24615;&#27010;&#24565;&#19982;&#20256;&#32479;&#31185;&#23398;&#35299;&#37322;&#27010;&#24565;&#20960;&#20046;&#27809;&#26377;&#30456;&#20284;&#20043;&#22788;&#12290;&#26356;&#26377;&#25104;&#25928;&#30340;&#20570;&#27861;&#26159;&#20351;&#29992;&#8220;&#21487;&#29702;&#35299;&#30340;&#20154;&#24037;&#26234;&#33021;&#8221;&#26631;&#31614;&#65292;&#20197;&#36991;&#20813;&#22260;&#32469;XAI&#30446;&#26631;&#21644;&#30446;&#30340;&#30340;&#22256;&#24785;&#12290;&#22312;&#31456;&#33410;&#30340;&#21518;&#21322;&#37096;&#20998;&#65292;&#25105;&#20027;&#24352;&#37319;&#29992;&#26356;&#36866;&#21512;&#25198;&#28436;&#26680;&#24515;&#35282;&#33394;&#30340;&#23454;&#29992;&#29702;&#35299;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00315v1 Announce Type: new  Abstract: In a recent paper, Erasmus et al. (2021) defend the idea that the ambiguity of the term "explanation" in explainable AI (XAI) can be solved by adopting any of four different extant accounts of explanation in the philosophy of science: the Deductive Nomological, Inductive Statistical, Causal Mechanical, and New Mechanist models. In this chapter, I show that the authors' claim that these accounts can be applied to deep neural networks as they would to any natural phenomenon is mistaken. I also provide a more general argument as to why the notion of explainability as it is currently used in the XAI literature bears little resemblance to the traditional concept of scientific explanation. It would be more fruitful to use the label "understandable AI" to avoid the confusion that surrounds the goal and purposes of XAI. In the second half of the chapter, I argue for a pragmatic conception of understanding that is better suited to play the centra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#36755;&#20837;&#22823;&#23567;&#21644;&#22810;&#31181;&#21387;&#32553;&#27604;&#65292;&#30456;&#27604;&#20110;&#26420;&#32032;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#30828;&#20214;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.00299</link><description>&lt;p&gt;
&#29992;&#20110;MIMO CSI&#21453;&#39304;&#30340;&#36890;&#29992;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Universal Auto-encoder Framework for MIMO CSI Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00299
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#36755;&#20837;&#22823;&#23567;&#21644;&#22810;&#31181;&#21387;&#32553;&#27604;&#65292;&#30456;&#27604;&#20110;&#26420;&#32032;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#30828;&#20214;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#26694;&#26550;&#19987;&#27880;&#20110;&#29305;&#23450;&#37197;&#32622;&#30340;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#21644;&#22522;&#31449;&#65288;BS&#65289;&#65292;&#22240;&#27492;AE&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#22823;&#23567;&#26159;&#22266;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#36755;&#20837;&#21644;&#36755;&#20986;&#22823;&#23567;&#21487;&#33021;&#20250;&#26681;&#25454;BS&#21644;UE&#30340;&#22825;&#32447;&#25968;&#37327;&#20197;&#21450;&#22312;&#39057;&#29575;&#32500;&#24230;&#19978;&#20998;&#37197;&#30340;&#36164;&#28304;&#22359;&#25968;&#37327;&#32780;&#21464;&#21270;&#12290;&#25903;&#25345;&#19981;&#21516;&#36755;&#20837;&#21644;&#36755;&#20986;&#22823;&#23567;&#30340;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#26159;&#20351;&#29992;&#22810;&#20010;AE&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;UE&#30340;&#26377;&#38480;&#30828;&#20214;&#36164;&#28304;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;AE&#26694;&#26550;&#65292;&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#30340;&#36755;&#20837;&#22823;&#23567;&#21644;&#22810;&#31181;&#21387;&#32553;&#27604;&#12290;&#25152;&#25552;&#20986;&#30340;AE&#26694;&#26550;&#22312;&#25552;&#20379;&#19982;&#26420;&#32032;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#21487;&#27604;&#30340;&#21387;&#32553;&#27604;-&#22833;&#30495;&#24179;&#34913;&#24615;&#33021;&#30340;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#30828;&#20214;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00299v1 Announce Type: cross  Abstract: Existing auto-encoder (AE)-based channel state information (CSI) frameworks have focused on a specific configuration of user equipment (UE) and base station (BS), and thus the input and output sizes of the AE are fixed. However, in the real-world scenario, the input and output sizes may vary depending on the number of antennas of the BS and UE and the allocated resource block in the frequency dimension. A naive approach to support the different input and output sizes is to use multiple AE models, which is impractical for the UE due to the limited HW resources. In this paper, we propose a universal AE framework that can support different input sizes and multiple compression ratios. The proposed AE framework significantly reduces the HW complexity while providing comparable performance in terms of compression ratio-distortion trade-off compared to the naive and state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36866;&#37197;&#22120;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#35843;&#25972;&#20026;&#35828;&#35805;&#32773;&#39564;&#35777;&#20219;&#21153;&#65292;&#36890;&#36807;&#24182;&#34892;&#36866;&#37197;&#22120;&#35774;&#35745;&#65292;&#20801;&#35768;&#22312;&#20013;&#38388;Transformer&#23618;&#35843;&#25972;&#28508;&#22312;&#29305;&#24449;&#20197;&#21450;&#22312;&#25152;&#26377;Transformer&#23618;&#30340;&#36755;&#20986;&#23884;&#20837;</title><link>https://arxiv.org/abs/2403.00293</link><description>&lt;p&gt;
&#20026;&#33258;&#21160;&#35828;&#35805;&#32773;&#39564;&#35777;&#25928;&#29575;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient Adapter Tuning of Pre-trained Speech Models for Automatic Speaker Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00293
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36866;&#37197;&#22120;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#35843;&#25972;&#20026;&#35828;&#35805;&#32773;&#39564;&#35777;&#20219;&#21153;&#65292;&#36890;&#36807;&#24182;&#34892;&#36866;&#37197;&#22120;&#35774;&#35745;&#65292;&#20801;&#35768;&#22312;&#20013;&#38388;Transformer&#23618;&#35843;&#25972;&#28508;&#22312;&#29305;&#24449;&#20197;&#21450;&#22312;&#25152;&#26377;Transformer&#23618;&#30340;&#36755;&#20986;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#20986;&#33394;&#27867;&#21270;&#33021;&#21147;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#20013;&#22312;&#21508;&#31181;&#19979;&#28216;&#35821;&#38899;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#24494;&#35843;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#65292;&#20197;&#21450;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#36866;&#37197;&#22120;&#26159;&#36731;&#37327;&#32423;&#27169;&#22359;&#65292;&#25554;&#20837;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#20197;&#20419;&#36827;&#21442;&#25968;&#39640;&#25928;&#30340;&#35843;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36866;&#37197;&#22120;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#35843;&#25972;&#20026;&#35828;&#35805;&#32773;&#39564;&#35777;&#20219;&#21153;&#12290;&#36890;&#36807;&#24182;&#34892;&#36866;&#37197;&#22120;&#35774;&#35745;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#20004;&#31181;&#31867;&#22411;&#30340;&#36866;&#37197;&#22120;&#25554;&#20837;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#20801;&#35768;&#22312;&#20013;&#38388;Transformer&#23618;&#20013;&#35843;&#25972;&#28508;&#22312;&#29305;&#24449;&#20197;&#21450;&#22312;&#25152;&#26377;Transformer&#23618;&#20013;&#30340;&#36755;&#20986;&#23884;&#20837;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#26469;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00293v1 Announce Type: cross  Abstract: With excellent generalization ability, self-supervised speech models have shown impressive performance on various downstream speech tasks in the pre-training and fine-tuning paradigm. However, as the growing size of pre-trained models, fine-tuning becomes practically unfeasible due to heavy computation and storage overhead, as well as the risk of overfitting. Adapters are lightweight modules inserted into pre-trained models to facilitate parameter-efficient adaptation. In this paper, we propose an effective adapter framework designed for adapting self-supervised speech models to the speaker verification task. With a parallel adapter design, our proposed framework inserts two types of adapters into the pre-trained model, allowing the adaptation of latent features within intermediate Transformer layers and output embeddings from all Transformer layers. We conduct comprehensive experiments to validate the efficiency and effectiveness of t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#35821;&#20041;&#25991;&#26412;&#20256;&#36755;&#20013;&#30340;&#25104;&#26412;&#21644;&#30456;&#20284;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.00290</link><description>&lt;p&gt;
&#22522;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25991;&#26412;&#20256;&#36755;&#65306;&#25104;&#26412;-&#30456;&#20284;&#24230;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Semantic Text Transmission via Prediction with Small Language Models: Cost-Similarity Trade-off
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#35821;&#20041;&#25991;&#26412;&#20256;&#36755;&#20013;&#30340;&#25104;&#26412;&#21644;&#30456;&#20284;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#26080;&#22122;&#38899;&#21644;&#23383;&#31526;&#25830;&#38500;&#36890;&#36947;&#19978;&#20174;&#28304;&#21040;&#30446;&#30340;&#22320;&#20256;&#36755;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#35821;&#35328;&#30340;&#22266;&#26377;&#30456;&#20851;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#65292;&#36890;&#36807;&#20801;&#35768;&#30446;&#30340;&#22320;&#39044;&#27979;&#25110;&#34917;&#20840;&#19982;&#28304;&#25991;&#26412;&#21487;&#33021;&#19981;&#30456;&#20284;&#30340;&#21333;&#35789;&#26469;&#38480;&#21046;&#20256;&#36755;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#33719;&#24471;&#21487;&#23454;&#29616;&#30340;$(\bar{c}, \bar{s})$&#23545;&#65292;&#20854;&#20013;$\bar{c}$&#26159;&#28304;&#22836;&#30340;&#24179;&#22343;&#20256;&#36755;&#25104;&#26412;&#65292;$\bar{s}$&#26159;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24230;&#27979;&#37327;&#30340;&#28304;&#22836;&#35789;&#21521;&#37327;&#21644;&#30446;&#30340;&#22320;&#39044;&#27979;/&#34917;&#20840;&#35789;&#21521;&#37327;&#20043;&#38388;&#30340;&#24179;&#22343;&#35821;&#20041;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;(SLM)&#36827;&#34892;&#39044;&#27979;&#65292;&#20026;&#20256;&#36755;&#20351;&#29992;&#20102;&#38408;&#20540;&#31574;&#30053;&#65292;&#21363;&#22914;&#26524;&#21333;&#35789;&#19982;&#30446;&#30340;&#22320;&#39044;&#27979;/&#34917;&#20840;&#30340;&#21333;&#35789;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#20302;&#20110;&#38408;&#20540;&#65292;&#21017;&#20256;&#36755;&#35813;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00290v1 Announce Type: cross  Abstract: We consider the communication of natural language text from a source to a destination over noiseless and character-erasure channels. We exploit language's inherent correlations and predictability to constrain transmission costs by allowing the destination to predict or complete words with potential dissimilarity with the source text. Concretely, our objective is to obtain achievable $(\bar{c}, \bar{s})$ pairs, where $\bar{c}$ is the average transmission cost at the source and $\bar{s}$ is the average semantic similarity measured via cosine similarity between vector embedding of words at the source and those predicted/completed at the destination. We obtain $(\bar{c}, \bar{s})$ pairs for neural language and first-order Markov chain-based small language models (SLM) for prediction, using both a threshold policy that transmits a word if its cosine similarity with that predicted/completed at the destination is below a threshold, and a peri
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26500;&#24314;&#25195;&#25551;&#24207;&#21015;&#65292;&#20135;&#29983;&#39640;&#36136;&#37327;&#36229;&#22768;B&#27169;&#24335;&#22270;&#20687;&#65292;&#24182;&#20248;&#21270;&#36229;&#22768;&#25104;&#20687;&#32534;&#30721;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.00289</link><description>&lt;p&gt;
&#36229;&#22768;&#25104;&#20687;&#38453;&#21015;&#32534;&#30721;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimization of Array Encoding for Ultrasound Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00289
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26500;&#24314;&#25195;&#25551;&#24207;&#21015;&#65292;&#20135;&#29983;&#39640;&#36136;&#37327;&#36229;&#22768;B&#27169;&#24335;&#22270;&#20687;&#65292;&#24182;&#20248;&#21270;&#36229;&#22768;&#25104;&#20687;&#32534;&#30721;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#21512;&#25104;&#23380;&#24452;&#25104;&#20687;&#30340;&#21457;&#23556;&#32534;&#30721;&#27169;&#22411;&#26159;&#29702;&#35299;&#36229;&#22768;&#25104;&#20687;&#37325;&#24314;&#30340;&#22768;&#23398;&#20256;&#36755;&#25928;&#24212;&#30340;&#31283;&#20581;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26500;&#24314;&#30001;&#26102;&#38388;&#24310;&#36831;&#21644;&#31783;&#24212;&#26435;&#21442;&#25968;&#21270;&#30340;&#25195;&#25551;&#24207;&#21015;&#65292;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;B&#27169;&#24335;&#22270;&#20687;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20351;&#29992;PyTorch&#20013;&#30340;ML&#27169;&#22411;&#65292;&#24182;&#20174;Field II&#29983;&#25104;&#27169;&#25311;RF&#25968;&#25454;&#65292;&#25506;&#31350;&#21487;&#33021;&#32534;&#30721;&#24207;&#21015;&#31354;&#38388;&#65292;&#20197;&#25214;&#21040;&#33021;&#22815;&#26368;&#23567;&#21270;&#25551;&#36848;&#22270;&#20687;&#36136;&#37327;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24207;&#21015;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#24310;&#36831;-&#27714;&#21644;&#27874;&#26463;&#25104;&#24418;&#23548;&#25968;&#30340;&#20844;&#24335;&#21270;&#65292;&#22312;&#35745;&#31639;&#19978;&#21464;&#24471;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#37329;&#23646;&#19997;&#38774;&#21644;&#32452;&#32455;&#27169;&#25311;&#24187;&#20687;&#19978;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;&#20027;&#35201;&#32467;&#26524;&#65306;&#26681;&#25454;&#32473;&#23450;&#30340;&#25104;&#20687;&#21442;&#25968;&#65288;&#25104;&#20687;&#22495;&#65292;&#30828;&#20214;&#38480;&#21046;&#65289;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#30340;ML&#25104;&#20687;&#27169;&#22411;&#20135;&#29983;&#20102;&#20248;&#21270;&#30340;&#32534;&#30721;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00289v1 Announce Type: cross  Abstract: Objective: The transmit encoding model for synthetic aperture imaging is a robust and flexible framework for understanding the effect of acoustic transmission on ultrasound image reconstruction. Our objective is to use machine learning (ML) to construct scanning sequences, parameterized by time delays and apodization weights, that produce high quality B-mode images. Approach: We use an ML model in PyTorch and simulated RF data from Field II to probe the space of possible encoding sequences for those that minimize a loss function that describes image quality. This approach is made computationally feasible by a novel formulation of the derivative for delay-and-sum beamforming. We demonstrate these results experimentally on wire targets and a tissue-mimicking phantom. Main Results: When trained according to a given set of imaging parameters (imaging domain, hardware restrictions), our ML imaging model produces optimized encoding sequences
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22478;&#24066;&#35745;&#31639;&#30340;&#36335;&#32447;&#25512;&#33616;&#32508;&#36848;&#23545;&#36335;&#32447;&#25512;&#33616;&#30740;&#31350;&#20013;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#19982;&#22478;&#24066;&#35745;&#31639;&#22330;&#26223;&#30456;&#20851;&#30340;&#26032;&#24212;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.00284</link><description>&lt;p&gt;
&#36335;&#32447;&#25512;&#33616;&#32508;&#36848;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
A Survey of Route Recommendations: Methods, Applications, and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00284
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22478;&#24066;&#35745;&#31639;&#30340;&#36335;&#32447;&#25512;&#33616;&#32508;&#36848;&#23545;&#36335;&#32447;&#25512;&#33616;&#30740;&#31350;&#20013;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#19982;&#22478;&#24066;&#35745;&#31639;&#22330;&#26223;&#30456;&#20851;&#30340;&#26032;&#24212;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#65292;&#38543;&#30528;&#20808;&#36827;&#30340;&#20449;&#24687;&#25216;&#26415;&#37096;&#32626;&#22312;&#25972;&#20010;&#22478;&#24066;&#65292;&#22823;&#37327;&#25968;&#25454;&#21644;&#24378;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#27491;&#22312;&#20351;&#29616;&#20195;&#22478;&#24066;&#21457;&#23637;&#26234;&#33021;&#21270;&#12290;&#20316;&#20026;&#26234;&#33021;&#20132;&#36890;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36335;&#32447;&#25512;&#33616;&#21450;&#20854;&#24212;&#29992;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#30452;&#25509;&#24433;&#21709;&#24066;&#27665;&#30340;&#20986;&#34892;&#20064;&#24815;&#12290;&#22522;&#20110;&#22823;&#25968;&#25454;&#65288;&#21487;&#33021;&#26159;&#22810;&#27169;&#24335;&#65289;&#24320;&#21457;&#26234;&#33021;&#39640;&#25928;&#30340;&#20986;&#34892;&#36335;&#32447;&#24050;&#25104;&#20026;&#36335;&#32447;&#25512;&#33616;&#30740;&#31350;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#23545;&#22522;&#20110;&#22478;&#24066;&#35745;&#31639;&#30340;&#36335;&#32447;&#25512;&#33616;&#24037;&#20316;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#23427;&#20998;&#20026;&#20197;&#19979;&#19977;&#20010;&#37096;&#20998;&#65306;1&#65289;&#26041;&#27861;&#35770;&#12290;&#25105;&#20204;&#23545;&#22823;&#37327;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#23427;&#20204;&#30340;&#21382;&#21490;&#20851;&#31995;&#24182;&#25581;&#31034;&#26368;&#26032;&#36827;&#23637;&#12290;2&#65289;&#24212;&#29992;&#26041;&#38754;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#37327;&#19982;&#22478;&#24066;&#35745;&#31639;&#22330;&#26223;&#20013;&#36335;&#32447;&#25512;&#33616;&#30456;&#20851;&#30340;&#26032;&#24212;&#29992;&#12290;3&#65289;&#25105;&#20204;&#36842;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00284v1 Announce Type: new  Abstract: Nowadays, with advanced information technologies deployed citywide, large data volumes and powerful computational resources are intelligentizing modern city development. As an important part of intelligent transportation, route recommendation and its applications are widely used, directly influencing citizens` travel habits. Developing smart and efficient travel routes based on big data (possibly multi-modal) has become a central challenge in route recommendation research. Our survey offers a comprehensive review of route recommendation work based on urban computing. It is organized by the following three parts: 1) Methodology-wise. We categorize a large volume of traditional machine learning and modern deep learning methods. Also, we discuss their historical relations and reveal the edge-cutting progress. 2) Application\-wise. We present numerous novel applications related to route commendation within urban computing scenarios. 3) We di
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoMOGA&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#26799;&#24230;&#32858;&#21512;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#36716;&#25442;&#20026;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#23545;&#24085;&#32047;&#25176;&#26368;&#20248;&#31574;&#30053;&#30340;&#27714;&#35299;&#65292;&#21516;&#26102;&#28385;&#36275;&#39044;&#23450;&#20041;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.00282</link><description>&lt;p&gt;
&#23610;&#24230;&#19981;&#21464;&#26799;&#24230;&#32858;&#21512;&#29992;&#20110;&#21463;&#32422;&#26463;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scale-Invariant Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00282
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoMOGA&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#26799;&#24230;&#32858;&#21512;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#36716;&#25442;&#20026;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#23545;&#24085;&#32047;&#25176;&#26368;&#20248;&#31574;&#30053;&#30340;&#27714;&#35299;&#65292;&#21516;&#26102;&#28385;&#36275;&#39044;&#23450;&#20041;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;(MORL)&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#32452;&#24085;&#32047;&#25176;&#26368;&#20248;&#31574;&#30053;&#65292;&#20197;&#28085;&#30422;&#21508;&#31181;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24212;&#29992;MORL&#65292;&#25214;&#21040;&#30340;&#31574;&#30053;&#19981;&#20165;&#35201;&#24085;&#32047;&#25176;&#26368;&#20248;&#65292;&#36824;&#35201;&#28385;&#36275;&#39044;&#23450;&#20041;&#30340;&#23433;&#20840;&#32422;&#26463;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32422;&#26463;&#22810;&#30446;&#26631;&#26799;&#24230;&#32858;&#21512;&#22120;(Constrained Multi-Objective Gradient Aggregator, CoMOGA)&#30340;&#32422;&#26463;MORL(CMORL)&#31639;&#27861;&#12290;CoMOGA&#24847;&#35782;&#21040;&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#30446;&#26631;&#21644;&#32422;&#26463;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#36716;&#25442;&#20026;&#39069;&#22806;&#30340;&#32422;&#26463;&#65292;&#23558;&#21407;&#22987;CMORL&#38382;&#39064;&#25918;&#26494;&#25104;&#19968;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#36716;&#25442;&#36807;&#31243;&#30830;&#20445;&#36716;&#25442;&#21518;&#30340;&#32422;&#26463;&#23545;&#30446;&#26631;&#23610;&#24230;&#19981;&#21464;&#65292;&#21516;&#26102;&#20855;&#26377;&#19982;&#21407;&#22987;&#30446;&#26631;&#30456;&#21516;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#25910;&#25947;&#21040;&#19968;&#20010;&#23616;&#37096;&#24085;&#32047;&#25176;&#26368;&#20248;&#31574;&#30053;&#65292;&#21516;&#26102;&#28385;&#36275;&#39044;&#23450;&#20041;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00282v1 Announce Type: new  Abstract: Multi-objective reinforcement learning (MORL) aims to find a set of Pareto optimal policies to cover various preferences. However, to apply MORL in real-world applications, it is important to find policies that are not only Pareto optimal but also satisfy pre-defined constraints for safety. To this end, we propose a constrained MORL (CMORL) algorithm called Constrained Multi-Objective Gradient Aggregator (CoMOGA). Recognizing the difficulty of handling multiple objectives and constraints concurrently, CoMOGA relaxes the original CMORL problem into a constrained optimization problem by transforming the objectives into additional constraints. This novel transformation process ensures that the converted constraints are invariant to the objective scales while having the same effect as the original objectives. We show that the proposed method converges to a local Pareto optimal policy while satisfying the predefined constraints. Empirical eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#32479;&#19968;&#26694;&#26550;&#19979;&#24314;&#31435;&#20102;&#8220;&#36890;&#36807;&#36845;&#20195;&#23454;&#29616;&#38544;&#31169;&#25918;&#22823;&#8221;&#29616;&#35937;&#65292;&#25552;&#39640;&#20102;&#20808;&#21069;&#20998;&#26512;&#30340;&#27700;&#24179;&#65292;&#24182;&#30001;&#27492;&#33719;&#24471;&#20102;&#20854;&#20182;&#24046;&#20998;&#38544;&#31169;&#27010;&#24565;&#26356;&#32039;&#23494;&#30340;&#38544;&#31169;&#26680;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.00278</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#30340;&#24179;&#31227;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Shifted Interpolation for Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#32479;&#19968;&#26694;&#26550;&#19979;&#24314;&#31435;&#20102;&#8220;&#36890;&#36807;&#36845;&#20195;&#23454;&#29616;&#38544;&#31169;&#25918;&#22823;&#8221;&#29616;&#35937;&#65292;&#25552;&#39640;&#20102;&#20808;&#21069;&#20998;&#26512;&#30340;&#27700;&#24179;&#65292;&#24182;&#30001;&#27492;&#33719;&#24471;&#20102;&#20854;&#20182;&#24046;&#20998;&#38544;&#31169;&#27010;&#24565;&#26356;&#32039;&#23494;&#30340;&#38544;&#31169;&#26680;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21927;&#22179;&#30340;&#26799;&#24230;&#19979;&#38477;&#21450;&#20854;&#21464;&#31181;&#26159;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#20013;&#20027;&#23548;&#30340;&#31639;&#27861;&#12290;&#37327;&#21270;&#23427;&#20204;&#30340;&#38544;&#31169;&#27844;&#28431;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#28982;&#32780;&#21363;&#20351;&#22312;&#20984;&#25439;&#22833;&#30340;&#22522;&#30784;&#35774;&#32622;&#20013;&#65292;&#32039;&#33268;&#30340;&#34920;&#24449;&#20173;&#28982;&#26159;&#24320;&#25918;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;$f$-&#24046;&#20998;&#38544;&#31169;&#30340;&#32479;&#19968;&#26694;&#26550;&#19979;&#24314;&#31435;&#65288;&#21644;&#25913;&#36827;&#65289;&#8220;&#36890;&#36807;&#36845;&#20195;&#23454;&#29616;&#38544;&#31169;&#25918;&#22823;&#8221;&#29616;&#35937;&#65292;&#25552;&#39640;&#20102;&#20808;&#21069;&#20998;&#26512;&#30340;&#27700;&#24179;--&#36825;&#31181;&#26041;&#27861;&#32039;&#32039;&#25429;&#25417;&#20102;&#38544;&#31169;&#25439;&#22833;&#30340;&#25152;&#26377;&#26041;&#38754;&#65292;&#24182;&#31435;&#21363;&#33719;&#24471;&#20102;&#20854;&#20182;&#24046;&#20998;&#38544;&#31169;&#27010;&#24565;&#65288;&#22914;$(\varepsilon,\delta)$-DP&#21644;Renyi DP&#65289;&#26356;&#32039;&#23494;&#30340;&#38544;&#31169;&#26680;&#31639;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#35265;&#35299;&#26159;&#26500;&#24314;&#20102;&#25581;&#31034;&#20102;&#27969;&#34892;&#30340;&#24179;&#31227;&#25955;&#24230;&#35770;&#35777;&#30340;&#24179;&#31227;&#25554;&#20540;&#36807;&#31243;&#65292;&#20351;&#24471;&#36229;&#36234;&#22522;&#20110;&#25955;&#24230;&#30340;&#24046;&#20998;&#38544;&#31169;&#25918;&#23485;&#30340;&#27867;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#24378;&#20984;&#22522;&#30784;&#35774;&#32622;&#20013;&#30340;&#31532;&#19968;&#20010;&#31934;&#30830;&#38544;&#31169;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00278v1 Announce Type: new  Abstract: Noisy gradient descent and its variants are the predominant algorithms for differentially private machine learning. It is a fundamental question to quantify their privacy leakage, yet tight characterizations remain open even in the foundational setting of convex losses. This paper improves over previous analyses by establishing (and refining) the "privacy amplification by iteration" phenomenon in the unifying framework of $f$-differential privacy--which tightly captures all aspects of the privacy loss and immediately implies tighter privacy accounting in other notions of differential privacy, e.g., $(\varepsilon,\delta)$-DP and Renyi DP. Our key technical insight is the construction of shifted interpolated processes that unravel the popular shifted-divergences argument, enabling generalizations beyond divergence-based relaxations of DP. Notably, this leads to the first exact privacy analysis in the foundational setting of strongly convex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#30830;&#23450;&#20102;&#22270;&#20013;&#33410;&#28857;&#30340;&#28789;&#27963;&#23450;&#20301;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#31639;&#27861;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#24182;&#20174;&#20056;&#23458;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#20449;&#24687;&#29992;&#20110;&#21021;&#22987;&#21270;GNNs&#30340;&#36793;&#26435;&#37325;&#12290;</title><link>https://arxiv.org/abs/2403.00276</link><description>&lt;p&gt;
&#20855;&#26377;&#28789;&#27963;&#33410;&#28857;&#30340;&#22270;&#26500;&#24314;&#29992;&#20110;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Construction with Flexible Nodes for Traffic Demand Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#30830;&#23450;&#20102;&#22270;&#20013;&#33410;&#28857;&#30340;&#28789;&#27963;&#23450;&#20301;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#31639;&#27861;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#24182;&#20174;&#20056;&#23458;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#20449;&#24687;&#29992;&#20110;&#21021;&#22987;&#21270;GNNs&#30340;&#36793;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;&#20013;&#65292;&#20132;&#36890;&#27169;&#24335;&#21487;&#20998;&#20026;&#22522;&#20110;&#31449;&#28857;&#30340;&#27169;&#24335;&#21644;&#33258;&#30001;&#28418;&#28014;&#20132;&#36890;&#27169;&#24335;&#12290;&#29616;&#26377;&#30340;&#20132;&#36890;&#22270;&#26500;&#24314;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#22320;&#22270;&#21305;&#37197;&#65292;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#26500;&#24314;&#22270;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#30001;&#28418;&#28014;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#22343;&#21248;&#24615;&#20351;&#24471;&#36947;&#36335;&#32593;&#32476;&#21305;&#37197;&#19981;&#22815;&#28789;&#27963;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33258;&#30001;&#28418;&#28014;&#20132;&#36890;&#27169;&#24335;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#22270;&#26500;&#24314;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65288;HDPC-L&#65289;&#26469;&#30830;&#23450;&#22270;&#20013;&#33410;&#28857;&#30340;&#28789;&#27963;&#23450;&#20301;&#65292;&#20811;&#26381;&#20256;&#32479;&#32858;&#31867;&#31639;&#27861;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#23454;&#29616;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#20056;&#23458;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20197;&#21021;&#22987;&#21270;GNNs&#30340;&#36793;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00276v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have been widely applied in traffic demand prediction, and transportation modes can be divided into station-based mode and free-floating traffic mode. Existing research in traffic graph construction primarily relies on map matching to construct graphs based on the road network. However, the complexity and inhomogeneity of data distribution in free-floating traffic demand forecasting make road network matching inflexible. To tackle these challenges, this paper introduces a novel graph construction method tailored to free-floating traffic mode. We propose a novel density-based clustering algorithm (HDPC-L) to determine the flexible positioning of nodes in the graph, overcoming the computational bottlenecks of traditional clustering algorithms and enabling effective handling of large-scale datasets. Furthermore, we extract valuable information from ridership data to initialize the edge weights of GNNs. Comprehen
&lt;/p&gt;</description></item><item><title>ARED&#26159;&#19987;&#20026;&#38463;&#26681;&#24311;&#24066;&#22330;&#35774;&#35745;&#30340;&#32508;&#21512;&#25151;&#22320;&#20135;&#20215;&#26684;&#39044;&#27979;&#25968;&#25454;&#38598;&#31995;&#21015;&#65292;&#23613;&#31649;&#38646;&#29256;&#21482;&#21253;&#21547;&#30701;&#26399;&#20449;&#24687;&#65292;&#20294;&#23637;&#31034;&#20102;&#24066;&#22330;&#23618;&#38754;&#19978;&#30340;&#26102;&#38388;&#30456;&#20851;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.00273</link><description>&lt;p&gt;
ARED: &#38463;&#26681;&#24311;&#25151;&#22320;&#20135;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ARED: Argentina Real Estate Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00273
&lt;/p&gt;
&lt;p&gt;
ARED&#26159;&#19987;&#20026;&#38463;&#26681;&#24311;&#24066;&#22330;&#35774;&#35745;&#30340;&#32508;&#21512;&#25151;&#22320;&#20135;&#20215;&#26684;&#39044;&#27979;&#25968;&#25454;&#38598;&#31995;&#21015;&#65292;&#23613;&#31649;&#38646;&#29256;&#21482;&#21253;&#21547;&#30701;&#26399;&#20449;&#24687;&#65292;&#20294;&#23637;&#31034;&#20102;&#24066;&#22330;&#23618;&#38754;&#19978;&#30340;&#26102;&#38388;&#30456;&#20851;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#26681;&#24311;&#25151;&#22320;&#20135;&#24066;&#22330;&#26159;&#19968;&#20010;&#29420;&#29305;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20854;&#29305;&#28857;&#26159;&#36807;&#21435;&#20960;&#21313;&#24180;&#38388;&#19981;&#31283;&#23450;&#19988;&#36805;&#36895;&#21464;&#21270;&#30340;&#23439;&#35266;&#32463;&#27982;&#29615;&#22659;&#12290;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#29992;&#20110;&#20215;&#26684;&#39044;&#27979;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#38463;&#26681;&#24311;&#30340;&#28151;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ARED&#30340;&#31532;&#19968;&#29256;&#65292;&#36825;&#26159;&#19987;&#20026;&#38463;&#26681;&#24311;&#24066;&#22330;&#35774;&#35745;&#30340;&#32508;&#21512;&#25151;&#22320;&#20135;&#20215;&#26684;&#39044;&#27979;&#25968;&#25454;&#38598;&#31995;&#21015;&#12290;&#36825;&#20010;&#29256;&#26412;&#20165;&#21253;&#21547;2024&#24180;1&#26376;&#33267;2&#26376;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20010;&#38646;&#29256;&#21482;&#25429;&#33719;&#20102;&#30701;&#30701;&#30340;&#26102;&#38388;&#33539;&#22260;&#65288;44&#22825;&#65289;&#65292;&#20294;&#26102;&#38388;&#30456;&#20851;&#30340;&#29616;&#35937;&#20027;&#35201;&#21457;&#29983;&#22312;&#24066;&#22330;&#23618;&#38754;&#19978;&#65288;&#25972;&#20010;&#24066;&#22330;&#65289;&#12290;&#28982;&#32780;&#65292;&#26410;&#26469;&#29256;&#26412;&#30340;&#25968;&#25454;&#38598;&#24456;&#21487;&#33021;&#20250;&#21253;&#21547;&#21382;&#21490;&#25968;&#25454;&#12290;ARED&#20013;&#30340;&#27599;&#20010;&#21015;&#34920;&#37117;&#21253;&#21547;&#25551;&#36848;&#24615;&#29305;&#24449;&#21644;&#21487;&#21464;&#38271;&#24230;&#30340;&#22270;&#20687;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00273v1 Announce Type: new  Abstract: The Argentinian real estate market presents a unique case study characterized by its unstable and rapidly shifting macroeconomic circumstances over the past decades. Despite the existence of a few datasets for price prediction, there is a lack of mixed modality datasets specifically focused on Argentina. In this paper, the first edition of ARED is introduced. A comprehensive real estate price prediction dataset series, designed for the Argentinian market. This edition contains information solely for Jan-Feb 2024. It was found that despite the short time range captured by this zeroth edition (44 days), time dependent phenomena has been occurring mostly on a market level (market as a whole). Nevertheless future editions of this dataset, will most likely contain historical data. Each listing in ARED comprises descriptive features, and variable-length sets of images.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#25991;&#23454;&#29616;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21516;&#26102;&#23398;&#20064;&#22522;&#20110;&#31867;&#21035;&#21644;&#22522;&#20110;&#23545;&#35937;&#36523;&#20221;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23039;&#21183;&#19981;&#21464;&#30340;&#23545;&#35937;&#35782;&#21035;&#21644;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00272</link><description>&lt;p&gt;
&#21452;&#37325;&#23039;&#21183;&#19981;&#21464;&#23884;&#20837;&#65306;&#23398;&#20064;&#29992;&#20110;&#35782;&#21035;&#21644;&#26816;&#32034;&#30340;&#31867;&#21035;&#21644;&#23545;&#35937;&#29305;&#23450;&#30340;&#21028;&#21035;&#24615;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Dual Pose-invariant Embeddings: Learning Category and Object-specific Discriminative Representations for Recognition and Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00272
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#25991;&#23454;&#29616;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21516;&#26102;&#23398;&#20064;&#22522;&#20110;&#31867;&#21035;&#21644;&#22522;&#20110;&#23545;&#35937;&#36523;&#20221;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23039;&#21183;&#19981;&#21464;&#30340;&#23545;&#35937;&#35782;&#21035;&#21644;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23039;&#21183;&#19981;&#21464;&#30340;&#23545;&#35937;&#35782;&#21035;&#21644;&#26816;&#32034;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21516;&#26102;&#23398;&#20064;&#22522;&#20110;&#31867;&#21035;&#21644;&#22522;&#20110;&#23545;&#35937;&#36523;&#20221;&#30340;&#23884;&#20837;&#26159;&#21487;&#33021;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#37197;&#21512;&#29305;&#21035;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#20248;&#21270;&#20004;&#20010;&#19981;&#21516;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#31867;&#38388;&#21644;&#31867;&#20869;&#36317;&#31163;&#65292;&#19968;&#20010;&#29992;&#20110;&#31867;&#21035;&#23884;&#20837;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#23545;&#35937;&#32423;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00272v1 Announce Type: cross  Abstract: In the context of pose-invariant object recognition and retrieval, we demonstrate that it is possible to achieve significant improvements in performance if both the category-based and the object-identity-based embeddings are learned simultaneously during training. In hindsight, that sounds intuitive because learning about the categories is more fundamental than learning about the individual objects that correspond to those categories. However, to the best of what we know, no prior work in pose-invariant learning has demonstrated this effect. This paper presents an attention-based dual-encoder architecture with specially designed loss functions that optimize the inter- and intra-class distances simultaneously in two different embedding spaces, one for the category embeddings and the other for the object-level embeddings. The loss functions we have proposed are pose-invariant ranking losses that are designed to minimize the intra-class d
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#28388;&#27874;&#22120;&#23376;&#31354;&#38388;&#21644;&#28388;&#27874;&#22120;&#21407;&#23376;&#30340;&#27010;&#24565;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#35843;&#22823;&#22411;&#21367;&#31215;&#27169;&#22411;&#26102;&#20165;&#35843;&#25972;&#23569;&#37327;&#21442;&#25968;&#26469;&#25552;&#21462;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00269</link><description>&lt;p&gt;
&#22823;&#22411;&#21367;&#31215;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Tuning of Large Convolutional Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00269
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#28388;&#27874;&#22120;&#23376;&#31354;&#38388;&#21644;&#28388;&#27874;&#22120;&#21407;&#23376;&#30340;&#27010;&#24565;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#35843;&#22823;&#22411;&#21367;&#31215;&#27169;&#22411;&#26102;&#20165;&#35843;&#25972;&#23569;&#37327;&#21442;&#25968;&#26469;&#25552;&#21462;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#39640;&#35745;&#31639;&#21644;&#21442;&#25968;&#22797;&#26434;&#24615;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20165;&#26356;&#26032;&#19979;&#28216;&#20219;&#21153;&#30340;&#37096;&#20998;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#36890;&#24120;&#24573;&#35270;&#20102;&#21367;&#31215;&#26680;&#30340;&#29420;&#29305;&#23646;&#24615;&#65292;&#32780;&#21367;&#31215;&#26680;&#20173;&#28982;&#26159;&#35768;&#22810;&#22823;&#22411;&#27169;&#22411;&#30340;&#22522;&#26412;&#20803;&#32032;&#65292;&#27604;&#22914;Stable Diffusion&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22312;&#27599;&#20010;&#32593;&#32476;&#23618;&#20869;&#20998;&#35299;&#21367;&#31215;&#26680;&#21040;&#19968;&#23567;&#32452;&#28388;&#27874;&#22120;&#23376;&#31354;&#38388;&#20803;&#32032;&#65292;&#21363;&#28388;&#27874;&#22120;&#21407;&#23376;&#65292;&#24341;&#20837;&#20102;&#28388;&#27874;&#22120;&#23376;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20165;&#35843;&#25972;&#28388;&#27874;&#22120;&#21407;&#23376;&#65288;&#36890;&#24120;&#20026;&#20960;&#30334;&#20010;&#21442;&#25968;&#65289;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#21462;&#20219;&#21153;&#29305;&#23450;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#28508;&#22312;&#22320;&#25193;&#23637;&#35843;&#25972;&#30340;&#21442;&#25968;&#31354;&#38388;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#22320;&#23558;&#27599;&#20010;&#31579;&#36873;&#21407;&#23376;&#20998;&#35299;&#21040;&#21478;&#19968;&#32452;&#31579;&#36873;&#21407;&#23376;&#26469;&#29983;&#25104;&#19968;&#20010;&#36807;&#23436;&#22791;&#30340;&#28388;&#27874;&#22120;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00269v1 Announce Type: cross  Abstract: To address the high computational and parameter complexity associated with fine-tuning large pre-trained models, researchers have developed parameter-efficient methods, where only partial parameters are updated for downstream tasks. However, these works often overlook the distinct properties of convolutional kernels, which still remain essential elements in many large models, such as Stable Diffusion. In this study, we first introduce filter subspace by decomposing convolutional kernels within each network layer over a small set of filter subspace elements, referred to as filter atoms. We then fine-tune these models to extract task-specific representation by only adapting the filter atoms, a few hundred parameters typically. To potentially expand the parameter space for tuning, we further show a simple approach to generate an overcomplete filter subspace by recursively decomposing each filter atom over another set of filter atoms. The 
&lt;/p&gt;</description></item><item><title>&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#31561;&#21464;&#22522;&#30784;&#27169;&#22411;&#25104;&#21151;&#22320;&#35299;&#35835;&#20102;&#29076;&#34701;FeO&#30340;&#24357;&#25955;&#25955;&#23556;&#65292;&#25552;&#20379;&#20102;&#23454;&#39564;&#21644;&#37327;&#23376;&#21147;&#23398;&#27169;&#22411;&#20043;&#38388;&#26356;&#22909;&#30340;&#19968;&#33268;&#24615;</title><link>https://arxiv.org/abs/2403.00259</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#31561;&#21464;&#22522;&#30784;&#27169;&#22411;&#35299;&#35835;&#24357;&#25955;&#25955;&#23556;&#65306;&#20197;&#29076;&#34701;FeO&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Deciphering diffuse scattering with machine learning and the equivariant foundation model: The case of molten FeO
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00259
&lt;/p&gt;
&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#31561;&#21464;&#22522;&#30784;&#27169;&#22411;&#25104;&#21151;&#22320;&#35299;&#35835;&#20102;&#29076;&#34701;FeO&#30340;&#24357;&#25955;&#25955;&#23556;&#65292;&#25552;&#20379;&#20102;&#23454;&#39564;&#21644;&#37327;&#23376;&#21147;&#23398;&#27169;&#22411;&#20043;&#38388;&#26356;&#22909;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#31561;&#21464;&#22522;&#30784;&#27169;&#22411;&#35299;&#35835;&#24357;&#25955;&#25955;&#23556;&#30340;&#36807;&#31243;&#23545;&#20110;&#23558;&#24357;&#25955;X&#23556;&#32447;&#25110;&#20013;&#23376;&#25955;&#23556;&#27979;&#37327;&#21644;&#21407;&#23376;-&#21407;&#23376;&#23545;&#21183;&#39044;&#27979;&#32467;&#26500;&#22312;&#26080;&#24207;&#26448;&#26009;&#20013;&#30340;&#34900;&#25509;&#19968;&#30452;&#26159;&#20957;&#32858;&#24577;&#29289;&#29702;&#23398;&#20013;&#38271;&#26399;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#37319;&#29992;&#30340;&#20256;&#32479;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#23558;&#19977;&#32500;&#32467;&#26500;&#27169;&#22411;&#19982;&#27979;&#37327;&#30340;&#32467;&#26500;&#22240;&#23376;&#21450;&#20854;&#30456;&#20851;&#30340;&#21407;&#23376;&#23545;&#20998;&#24067;&#20989;&#25968;&#30456;&#20851;&#32852;&#30340;&#36817;&#20284;&#21407;&#23376;&#38388;&#23545;&#21183;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#21407;&#23376;&#38388;&#21183;&#30340;&#20351;&#29992;&#24050;&#32463;&#22686;&#38271;&#65292;&#24182;&#19988;&#22312;&#31163;&#23376;&#21644;&#27687;&#21270;&#29289;&#31995;&#32479;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#29305;&#21035;&#30340;&#25104;&#21151;&#12290;&#22823;&#35268;&#27169;&#37319;&#26679;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20197;&#21450;&#23558;&#25955;&#23556;&#27979;&#37327;&#30452;&#25509;&#25972;&#21512;&#21040;&#27169;&#22411;&#24320;&#21457;&#20013;&#65292;&#20026;&#23454;&#39564;&#21644;&#29992;&#37327;&#23376;&#21147;&#23398;&#31934;&#24230;&#35745;&#31639;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#20043;&#38388;&#25552;&#20379;&#20102;&#25913;&#21892;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;l
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00259v1 Announce Type: cross  Abstract: Bridging the gap between diffuse x-ray or neutron scattering measurements and predicted structures derived from atom-atom pair potentials in disordered materials, has been a longstanding challenge in condensed matter physics. This perspective gives a brief overview of the traditional approaches employed over the past several decades. Namely, the use of approximate interatomic pair potentials that relate 3-dimensional structural models to the measured structure factor and its associated pair distribution function. The use of machine learned interatomic potentials has grown in the past few years, and has been particularly successful in the cases of ionic and oxide systems. Recent advances in large scale sampling, along with a direct integration of scattering measurements into the model development, has provided improved agreement between experiments and large-scale models calculated with quantum mechanical accuracy. However, details of l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#20999;&#32447;&#26680;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39640;&#32500;&#24230;&#24773;&#24418;&#19979;&#23545;&#23485;&#32780;&#20840;&#36830;&#25509;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26377;&#25928;&#21387;&#32553;&#12290;</title><link>https://arxiv.org/abs/2403.00258</link><description>&lt;p&gt;
&#8220;&#26080;&#25439;&#8221;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#39640;&#32500;&#31070;&#32463;&#20999;&#32447;&#26680;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
"Lossless" Compression of Deep Neural Networks: A High-dimensional Neural Tangent Kernel Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#20999;&#32447;&#26680;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39640;&#32500;&#24230;&#24773;&#24418;&#19979;&#23545;&#23485;&#32780;&#20840;&#36830;&#25509;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26377;&#25928;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#38750;&#24120;&#24378;&#22823;&#65307;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#22686;&#21152;&#28145;&#24230;&#24182;&#20351;&#27599;&#19968;&#23618;&#30340;&#21442;&#25968;&#26356;&#22810;&#20026;&#20195;&#20215;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#20851;&#38190;&#38480;&#21046;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#23545;&#36825;&#20123;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#65288;&#22914;&#31232;&#30095;&#21270;&#21644;/&#25110;&#37327;&#21270;&#65289;&#65292;&#20197;&#20415;&#23427;&#20204;&#21487;&#20197;&#37096;&#32626;&#22312;&#20302;&#21151;&#32791;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#12290;&#26412;&#25991;&#22522;&#20110;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#65288;RMT&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#23485;&#32780;&#20840;&#36830;&#25509;&#30340;\emph{&#28145;}&#31070;&#32463;&#32593;&#32476;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#32500;&#24230;&#24773;&#24418;&#19979;&#65292;&#24403;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;$n$&#21644;&#23427;&#20204;&#30340;&#32500;&#24230;$p$&#37117;&#24456;&#22823;&#65292;&#24182;&#19988;&#25968;&#25454;&#36981;&#24490;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26102;&#65292;&#23545;&#20110;&#19968;&#22823;&#31867;DNN&#65292;NTK&#30697;&#38453;&#20043;&#38388;&#23384;&#22312;\emph{&#28176;&#36817;&#35889;&#31561;&#20215;}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00258v1 Announce Type: cross  Abstract: Modern deep neural networks (DNNs) are extremely powerful; however, this comes at the price of increased depth and having more parameters per layer, making their training and inference more computationally challenging. In an attempt to address this key limitation, efforts have been devoted to the compression (e.g., sparsification and/or quantization) of these large-scale machine learning models, so that they can be deployed on low-power IoT devices. In this paper, building upon recent advances in neural tangent kernel (NTK) and random matrix theory (RMT), we provide a novel compression approach to wide and fully-connected \emph{deep} neural nets. Specifically, we demonstrate that in the high-dimensional regime where the number of data points $n$ and their dimension $p$ are both large, and under a Gaussian mixture model for the data, there exists \emph{asymptotic spectral equivalence} between the NTK matrices for a large family of DNN m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21387;&#32553;&#21644;&#28608;&#21169;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#32954;CT&#22270;&#20687;&#36827;&#34892;sLTP&#21644;CTES&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#21487;&#37325;&#29616;&#30340;&#32954;&#37096;CT&#25195;&#25551;sLTP&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2403.00257</link><description>&lt;p&gt;
&#21033;&#29992;&#21387;&#32553;&#21644;&#28608;&#21169;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#25918;&#23556;&#24615;&#32954;&#27668;&#32959;&#20122;&#22411;&#36827;&#34892;&#24378;&#22823;&#30340;&#28145;&#24230;&#26631;&#35760;&#65306;MESA Lung&#21644;SPIROMICS&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Robust deep labeling of radiological emphysema subtypes using squeeze and excitation convolutional neural networks: The MESA Lung and SPIROMICS Studies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00257
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21387;&#32553;&#21644;&#28608;&#21169;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#32954;CT&#22270;&#20687;&#36827;&#34892;sLTP&#21644;CTES&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#21487;&#37325;&#29616;&#30340;&#32954;&#37096;CT&#25195;&#25551;sLTP&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#27668;&#32959;&#26159;&#32954;&#32452;&#32455;&#36827;&#34892;&#24615;&#19981;&#21487;&#36870;&#24615;&#30340;&#25439;&#22833;&#65292;&#22312;&#32954;&#30149;&#29702;&#23398;&#21644;&#32954;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#19978;&#36890;&#24120;&#34987;&#20998;&#31867;&#20026;&#19977;&#20010;&#20122;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23548;&#33268;&#20102;&#22312;&#32954;&#37096;CT&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;&#20102;&#21313;&#20010;&#31354;&#38388;&#20449;&#24687;&#30340;&#32954;&#37096;&#32441;&#29702;&#22270;&#26696;&#65288;sLTP&#65289;&#65292;&#34920;&#31034;&#22522;&#20110;&#32954;&#27668;&#32959;&#32954;&#23454;&#36136;&#30340;&#32441;&#29702;&#22806;&#35266;&#21644;&#32954;&#37096;&#20869;&#30340;&#31354;&#38388;&#20301;&#32622;&#30340;&#19981;&#21516;&#27169;&#24335;&#65292;&#24182;&#32858;&#21512;&#25104;6&#20010;&#31283;&#20581;&#19988;&#21487;&#37325;&#22797;&#30340;CT&#32954;&#27668;&#32959;&#20122;&#22411;&#65288;CTES&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;sLTP&#20998;&#21106;&#26041;&#27861;&#23545;CT&#33719;&#21462;&#21327;&#35758;&#30340;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#19988;&#36816;&#34892;&#32531;&#24930;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#32954;&#37096;CT&#19978;&#36827;&#34892;sLTP&#21644;CTES&#26377;&#30417;&#30563;&#20998;&#31867;&#30340;&#31283;&#20581;3D&#21387;&#32553;&#21644;&#28608;&#21169;CNN&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#29420;&#31435;&#38431;&#21015;&#19978;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#21487;&#37325;&#29616;&#30340;&#32954;&#37096;CT&#25195;&#25551;sLTP&#20998;&#21106;&#65292;&#29420;&#31435;&#20110;&#25195;&#25551;&#20202;&#21046;&#36896;&#21830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00257v1 Announce Type: cross  Abstract: Pulmonary emphysema, the progressive, irreversible loss of lung tissue, is conventionally categorized into three subtypes identifiable on pathology and on lung computed tomography (CT) images. Recent work has led to the unsupervised learning of ten spatially-informed lung texture patterns (sLTPs) on lung CT, representing distinct patterns of emphysematous lung parenchyma based on both textural appearance and spatial location within the lung, and which aggregate into 6 robust and reproducible CT Emphysema Subtypes (CTES). Existing methods for sLTP segmentation, however, are slow and highly sensitive to changes in CT acquisition protocol. In this work, we present a robust 3-D squeeze-and-excitation CNN for supervised classification of sLTPs and CTES on lung CT. Our results demonstrate that this model achieves accurate and reproducible sLTP segmentation on lung CTscans, across two independent cohorts and independently of scanner manufactu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20892;&#26449;&#21307;&#30103;&#29615;&#22659;&#30340;&#33041;&#32452;&#32455;&#20998;&#21106;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#21644;&#26412;&#22320;&#30340;&#31934;&#35843;&#27169;&#22411;&#65292;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#20197;&#32500;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#22686;&#24378;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.00254</link><description>&lt;p&gt;
&#22522;&#20110;&#20113;&#30340;MRI&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Cloud-based Federated Learning Framework for MRI Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00254
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20892;&#26449;&#21307;&#30103;&#29615;&#22659;&#30340;&#33041;&#32452;&#32455;&#20998;&#21106;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#21644;&#26412;&#22320;&#30340;&#31934;&#35843;&#27169;&#22411;&#65292;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#20197;&#32500;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#22686;&#24378;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#20892;&#26449;&#21307;&#30103;&#29615;&#22659;&#20013;&#65292;&#35786;&#26029;&#33041;&#37096;&#22270;&#20687;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#25968;&#25454;&#31232;&#32570;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20248;&#21270;&#20854;&#24615;&#33021;&#65292;&#36825;&#38656;&#35201;&#20013;&#24515;&#21270;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#33021;&#20250;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20892;&#26449;&#21307;&#30103;&#35774;&#26045;&#30340;&#33041;&#32452;&#32455;&#20998;&#21106;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#20351;&#29992;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#29615;&#22659;&#21644;&#22312;&#20892;&#26449;&#21307;&#30103;&#32593;&#31449;&#26412;&#22320;&#37096;&#32626;&#30340;&#31934;&#35843;&#27169;&#22411;&#65288;RM&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;DRL&#27169;&#22411;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#20892;&#26449;&#31449;&#28857;&#19978;&#23454;&#29616;&#12290;&#20026;&#20102;&#32500;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36827;&#34892;&#21512;&#20316;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00254v1 Announce Type: cross  Abstract: In contemporary rural healthcare settings, the principal challenge in diagnosing brain images is the scarcity of available data, given that most of the existing deep learning models demand extensive training data to optimize their performance, necessitating centralized processing methods that potentially compromise data privacy. This paper proposes a novel framework tailored for brain tissue segmentation in rural healthcare facilities. The framework employs a deep reinforcement learning (DRL) environment in tandem with a refinement model (RM) deployed locally at rural healthcare sites. The proposed DRL model has a reduced parameter count and practicality for implementation across distributed rural sites. To uphold data privacy and enhance model generalization without transgressing privacy constraints, we employ federated learning (FL) for cooperative model training. We demonstrate the efficacy of our approach by training the network wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;EUROPA&#65292;&#21253;&#21547;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#65292;&#34920;&#26126;&#22312;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.00252</link><description>&lt;p&gt;
EUROPA&#65306;&#19968;&#20010;&#27861;&#24459;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
EUROPA: A Legal Multilingual Keyphrase Generation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;EUROPA&#65292;&#21253;&#21547;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#65292;&#34920;&#26126;&#22312;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#29983;&#25104;&#20027;&#35201;&#22312;&#23398;&#26415;&#30740;&#31350;&#25991;&#31456;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#25506;&#32034;&#65292;&#29305;&#21035;&#20391;&#37325;&#20110;&#31185;&#23398;&#39046;&#22495;&#21644;&#33521;&#35821;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EUROPA&#65292;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290; &#23427;&#28304;&#33258;&#27431;&#27954;&#27861;&#38498;&#30340;&#27861;&#24459;&#21028;&#20915;&#65292;&#24182;&#21253;&#21547;&#20102;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#20013;&#30340;&#23454;&#20363;&#12290; &#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#19978;&#36816;&#34892;&#22810;&#35821;&#35328;&#27169;&#22411;&#24182;&#20998;&#26512;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20687;&#25105;&#20204;&#25552;&#20986;&#30340;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00252v1 Announce Type: cross  Abstract: Keyphrase generation has primarily been explored within the context of academic research articles, with a particular focus on scientific domains and the English language. In this work, we present EUROPA, a dataset for multilingual keyphrase generation in the legal domain. It is derived from legal judgments from the Court of Justice of the European Union (EU), and contains instances in all 24 EU official languages. We run multilingual models on our corpus and analyze the results, showing room for improvement on a domain-specific multilingual corpus such as the one we present.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;FlanT5-XXL&#21644;SemEval 2016&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#22312;&#25512;&#29305;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#21450;&#20854;&#23545;&#25552;&#31034;&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#25935;&#24863;&#24615;&#65292;&#25581;&#31034;&#20102;&#20854;&#33021;&#22815;&#21305;&#25932;&#25110;&#36229;&#36234;&#26368;&#20808;&#36827;&#22522;&#20934;&#27979;&#35797;&#30340;&#33021;&#21147;&#65292;&#24182;&#35782;&#21035;&#20102;&#20854;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.00236</link><description>&lt;p&gt;
&#20351;&#29992;FlanT5-XXL&#36827;&#34892;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#30340;&#22522;&#20934;&#27979;&#35797;&#65306;&#20174;&#35757;&#32451;&#25968;&#25454;&#12289;&#25552;&#31034;&#21644;&#35299;&#30721;&#31574;&#30053;&#20013;&#25506;&#35752;&#20854;&#25509;&#36817;SOTA&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from training data, prompting, and decoding strategies into its near-SoTA performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00236
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;FlanT5-XXL&#21644;SemEval 2016&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#22312;&#25512;&#29305;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#21450;&#20854;&#23545;&#25552;&#31034;&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#25935;&#24863;&#24615;&#65292;&#25581;&#31034;&#20102;&#20854;&#33021;&#22815;&#21305;&#25932;&#25110;&#36229;&#36234;&#26368;&#20808;&#36827;&#22522;&#20934;&#27979;&#35797;&#30340;&#33021;&#21147;&#65292;&#24182;&#35782;&#21035;&#20102;&#20854;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;LLM&#30340;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#22312;&#25512;&#29305;&#19978;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992;FlanT5-XXL&#65292;&#19968;&#20010;&#32463;&#36807;&#35843;&#25972;&#25351;&#20196;&#30340;&#24320;&#28304;LLM&#65292;&#22312;SemEval 2016&#20219;&#21153;6A&#12289;6B&#21644;P-Stance&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#25552;&#31034;&#21644;&#35299;&#30721;&#31574;&#30053;&#19979;&#30340;&#34920;&#29616;&#21450;&#20854;&#21464;&#21270;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#38646;-shot&#26041;&#27861;&#21487;&#20197;&#21305;&#25932;&#29978;&#33267;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20854;&#34920;&#29616;&#30340;&#21508;&#31181;&#35265;&#35299;&#65292;&#21253;&#25324;&#23545;&#25351;&#20196;&#21644;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#65292;&#35299;&#30721;&#31574;&#30053;&#65292;&#25552;&#31034;&#30340;&#22256;&#24785;&#24230;&#65292;&#20197;&#21450;&#25552;&#31034;&#20013;&#23384;&#22312;&#30340;&#21542;&#23450;&#21644;&#21453;&#23545;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#20445;LLM&#27809;&#26377;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#31181;&#21487;&#33021;&#37096;&#20998;&#35299;&#37322;&#35299;&#30721;&#31574;&#30053;&#20043;&#38388;&#34920;&#29616;&#24046;&#24322;&#30340;&#31215;&#26497;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00236v1 Announce Type: cross  Abstract: We investigate the performance of LLM-based zero-shot stance detection on tweets. Using FlanT5-XXL, an instruction-tuned open-source LLM, with the SemEval 2016 Tasks 6A, 6B, and P-Stance datasets, we study the performance and its variations under different prompts and decoding strategies, as well as the potential biases of the model. We show that the zero-shot approach can match or outperform state-of-the-art benchmarks, including fine-tuned models. We provide various insights into its performance including the sensitivity to instructions and prompts, the decoding strategies, the perplexity of the prompts, and to negations and oppositions present in prompts. Finally, we ensure that the LLM has not been trained on test datasets, and identify a positivity bias which may partially explain the performance differences across decoding strategie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#19977;&#20010;&#26041;&#21521;&#19978;&#25512;&#36827;&#20102;&#22240;&#26524;&#36172;&#21338;&#26426;&#30340;&#32467;&#26524;&#65306;&#22312;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#19979;&#36827;&#34892;&#24178;&#39044;&#35774;&#35745;&#65292;&#23454;&#29616;&#24191;&#20041;&#36719;&#24178;&#39044;&#20197;&#21450;&#25552;&#20379;&#19968;&#33324;&#30340;&#36951;&#25022;&#19978;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.00233</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#21644;&#24178;&#39044;&#30340;&#22240;&#26524;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Causal Bandits with General Causal Models and Interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#19977;&#20010;&#26041;&#21521;&#19978;&#25512;&#36827;&#20102;&#22240;&#26524;&#36172;&#21338;&#26426;&#30340;&#32467;&#26524;&#65306;&#22312;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#19979;&#36827;&#34892;&#24178;&#39044;&#35774;&#35745;&#65292;&#23454;&#29616;&#24191;&#20041;&#36719;&#24178;&#39044;&#20197;&#21450;&#25552;&#20379;&#19968;&#33324;&#30340;&#36951;&#25022;&#19978;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22240;&#26524;&#36172;&#21338;&#26426;&#65288;CBs&#65289;&#29992;&#20110;&#22240;&#26524;&#31995;&#32479;&#20013;&#24178;&#39044;&#30340;&#39034;&#24207;&#35774;&#35745;&#12290;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#20107;&#21518;&#26368;&#20339;&#24178;&#39044;&#24207;&#21015;&#30456;&#27604;&#30340;&#32047;&#31215;&#36951;&#25022;&#24230;&#37327;&#26469;&#20248;&#21270;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#25991;&#23558;CBs&#30340;&#32467;&#26524;&#25512;&#36827;&#20102;&#19977;&#20010;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#20551;&#35774;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCMs&#65289;&#26410;&#30693;&#65292;&#24182;&#19988;&#20219;&#24847;&#20174;Lipschitz&#36830;&#32493;&#20989;&#25968;&#31867;$\mathcal{F}$&#20013;&#32472;&#21046;&#12290;&#29616;&#26377;&#32467;&#26524;&#36890;&#24120;&#38598;&#20013;&#22312;&#65288;&#24191;&#20041;&#65289;&#32447;&#24615;SCMs&#19978;&#12290;&#20854;&#27425;&#65292;&#20551;&#35774;&#24178;&#39044;&#26159;&#24191;&#20041;&#36719;&#24178;&#39044;&#65292;&#20855;&#26377;&#20219;&#24847;&#25152;&#38656;&#31890;&#24230;&#30340;&#27700;&#24179;&#65292;&#23548;&#33268;&#26080;&#38480;&#25968;&#37327;&#30340;&#21487;&#33021;&#24178;&#39044;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29616;&#26377;&#25991;&#29486;&#36890;&#24120;&#37319;&#29992;&#21407;&#23376;&#21644;&#30828;&#24178;&#39044;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36951;&#25022;&#30340;&#19968;&#33324;&#19978;&#19979;&#30028;&#12290;&#19978;&#30028;&#21253;&#21547;&#65288;&#24182;&#25913;&#36827;&#65289;&#29305;&#27530;&#24773;&#20917;&#30340;&#24050;&#30693;&#30028;&#38480;&#12290;&#19979;&#30028;&#26159;gene
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00233v1 Announce Type: cross  Abstract: This paper considers causal bandits (CBs) for the sequential design of interventions in a causal system. The objective is to optimize a reward function via minimizing a measure of cumulative regret with respect to the best sequence of interventions in hindsight. The paper advances the results on CBs in three directions. First, the structural causal models (SCMs) are assumed to be unknown and drawn arbitrarily from a general class $\mathcal{F}$ of Lipschitz-continuous functions. Existing results are often focused on (generalized) linear SCMs. Second, the interventions are assumed to be generalized soft with any desired level of granularity, resulting in an infinite number of possible interventions. The existing literature, in contrast, generally adopts atomic and hard interventions. Third, we provide general upper and lower bounds on regret. The upper bounds subsume (and improve) known bounds for special cases. The lower bounds are gene
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20960;&#20309;&#32467;&#26500;&#36741;&#21161;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25968;&#25454;&#20849;&#21516;&#26500;&#24314;&#26080;&#32447;&#30005;&#22320;&#22270;&#21644;&#34394;&#25311;&#29615;&#22659;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#29992;&#20110;&#25552;&#21462;&#20851;&#38190;&#34893;&#23556;&#29305;&#24449;&#21644;&#25551;&#36848;&#25955;&#23556;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.00229</link><description>&lt;p&gt;
&#21033;&#29992;&#20960;&#20309;&#27169;&#22411;&#36741;&#21161;&#28145;&#24230;&#23398;&#20064;&#30340;&#34893;&#23556;&#21644;&#25955;&#23556;&#24863;&#30693;&#26080;&#32447;&#30005;&#22320;&#22270;&#19982;&#29615;&#22659;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Diffraction and Scattering Aware Radio Map and Environment Reconstruction using Geometry Model-Assisted Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00229
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20960;&#20309;&#32467;&#26500;&#36741;&#21161;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25968;&#25454;&#20849;&#21516;&#26500;&#24314;&#26080;&#32447;&#30005;&#22320;&#22270;&#21644;&#34394;&#25311;&#29615;&#22659;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#29992;&#20110;&#25552;&#21462;&#20851;&#38190;&#34893;&#23556;&#29305;&#24449;&#21644;&#25551;&#36848;&#25955;&#23556;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00229v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26377;&#21161;&#20110;5G&#21450;&#20197;&#19978;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#24555;&#36895;&#20449;&#36947;&#24314;&#27169;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;ML&#25216;&#26415;&#21033;&#29992;&#22478;&#24066;&#22320;&#22270;&#26500;&#24314;&#26080;&#32447;&#30005;&#22320;&#22270;&#65307;&#28982;&#32780;&#65292;&#24182;&#38750;&#22987;&#32456;&#21487;&#29992;&#26356;&#26032;&#30340;&#22478;&#24066;&#22320;&#22270;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65288;RSS&#65289;&#25968;&#25454;&#20849;&#21516;&#26500;&#24314;&#26080;&#32447;&#30005;&#22320;&#22270;&#21644;&#34394;&#25311;&#29615;&#22659;&#65292;&#36890;&#36807;&#21033;&#29992;&#29615;&#22659;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#19982;&#35768;&#22810;&#29616;&#26377;&#30340;&#32570;&#20047;&#29615;&#22659;&#27169;&#22411;&#30340;ML&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#34394;&#25311;&#38556;&#30861;&#27169;&#22411;&#65292;&#24182;&#34920;&#24449;&#20102;&#20256;&#25773;&#36335;&#24452;&#19982;&#34394;&#25311;&#38556;&#30861;&#29289;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;&#37319;&#29992;&#22810;&#23631;&#24149;&#20992;&#20995;&#27169;&#22411;&#25552;&#21462;&#20851;&#38190;&#30340;&#34893;&#23556;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#36827;&#34892;&#34893;&#23556;&#34920;&#31034;&#12290;&#20026;&#20102;&#25551;&#36848;&#25955;&#23556;&#65292;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#30452;&#25509;&#36755;&#20837;&#25972;&#20010;&#22478;&#24066;&#22320;&#22270;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20391;&#37325;&#20110;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00229v1 Announce Type: cross  Abstract: Machine learning (ML) facilitates rapid channel modeling for 5G and beyond wireless communication systems. Many existing ML techniques utilize a city map to construct the radio map; however, an updated city map may not always be available. This paper proposes to employ the received signal strength (RSS) data to jointly construct the radio map and the virtual environment by exploiting the geometry structure of the environment. In contrast to many existing ML approaches that lack of an environment model, we develop a virtual obstacle model and characterize the geometry relation between the propagation paths and the virtual obstacles. A multi-screen knife-edge model is adopted to extract the key diffraction features, and these features are fed into a neural network (NN) for diffraction representation. To describe the scattering, as oppose to most existing methods that directly input an entire city map, our model focuses on the geometry st
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#25216;&#33021;&#23398;&#20064;&#26694;&#26550;DuSkill&#65292;&#36890;&#36807;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#25216;&#33021;&#65292;&#20174;&#32780;&#22686;&#24378;&#19981;&#21516;&#39046;&#22495;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00225</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#25216;&#33021;&#25193;&#25955;&#23454;&#29616;&#31283;&#20581;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Policy Learning via Offline Skill Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00225
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#25216;&#33021;&#23398;&#20064;&#26694;&#26550;DuSkill&#65292;&#36890;&#36807;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#25216;&#33021;&#65292;&#20174;&#32780;&#22686;&#24378;&#19981;&#21516;&#39046;&#22495;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25216;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#38271;&#26102;&#22495;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#12290;&#36825;&#20123;&#25216;&#33021;&#26159;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#26080;&#20851;&#20219;&#21153;&#22320;&#23398;&#20064;&#30340;&#65292;&#21487;&#20197;&#21152;&#24555;&#38024;&#23545;&#26032;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#25216;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#20173;&#21463;&#38480;&#20110;&#23545;&#25968;&#25454;&#38598;&#30340;&#22266;&#26377;&#20381;&#36182;&#65292;&#24403;&#23581;&#35797;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20026;&#19981;&#21516;&#20110;&#25968;&#25454;&#38598;&#39046;&#22495;&#30340;&#30446;&#26631;&#39046;&#22495;&#23398;&#20064;&#22522;&#20110;&#25216;&#33021;&#30340;&#31574;&#30053;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23601;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31163;&#32447;&#25216;&#33021;&#23398;&#20064;&#26694;&#26550;DuSkill&#65292;&#23427;&#37319;&#29992;&#20102;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#20174;&#25968;&#25454;&#38598;&#20013;&#26377;&#38480;&#25216;&#33021;&#25193;&#23637;&#20986;&#30340;&#36890;&#29992;&#25216;&#33021;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#19981;&#21516;&#39046;&#22495;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24341;&#23548;&#25193;&#25955;&#25216;&#33021;&#35299;&#30721;&#22120;&#65292;&#32467;&#21512;&#20998;&#23618;&#32534;&#30721;&#65292;&#20197;&#35299;&#24320;&#25216;&#33021;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00225v1 Announce Type: new  Abstract: Skill-based reinforcement learning (RL) approaches have shown considerable promise, especially in solving long-horizon tasks via hierarchical structures. These skills, learned task-agnostically from offline datasets, can accelerate the policy learning process for new tasks. Yet, the application of these skills in different domains remains restricted due to their inherent dependency on the datasets, which poses a challenge when attempting to learn a skill-based policy via RL for a target domain different from the datasets' domains. In this paper, we present a novel offline skill learning framework DuSkill which employs a guided Diffusion model to generate versatile skills extended from the limited skills in datasets, thereby enhancing the robustness of policy learning for tasks in different domains. Specifically, we devise a guided diffusion-based skill decoder in conjunction with the hierarchical encoding to disentangle the skill embeddi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;SUB-SAMPLE-Q&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#23616;&#37096;&#20195;&#29702;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#22312;&#25351;&#25968;&#32423;&#21035;&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#25351;&#25968;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.00222</link><description>&lt;p&gt;
&#23384;&#22312;&#22823;&#35268;&#27169;&#23616;&#37096;&#20195;&#29702;&#30340;&#20840;&#23616;&#20915;&#31574;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcement Learning for Global Decision Making in the Presence of Local Agents at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00222
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;SUB-SAMPLE-Q&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#23616;&#37096;&#20195;&#29702;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#22312;&#25351;&#25968;&#32423;&#21035;&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#25351;&#25968;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23384;&#22312;&#35768;&#22810;&#23616;&#37096;&#20195;&#29702;&#30340;&#20840;&#23616;&#20915;&#31574;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20840;&#23616;&#20915;&#31574;&#32773;&#20570;&#20986;&#24433;&#21709;&#25152;&#26377;&#23616;&#37096;&#20195;&#29702;&#30340;&#20915;&#31574;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#26368;&#22823;&#21270;&#20840;&#23616;&#21644;&#23616;&#37096;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#25193;&#23637;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#29366;&#24577;/&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#21487;&#33021;&#20250;&#38543;&#20195;&#29702;&#25968;&#37327;&#25351;&#25968;&#22686;&#38271;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SUB-SAMPLE-Q&#31639;&#27861;&#65292;&#22312;&#27492;&#31639;&#27861;&#20013;&#65292;&#20840;&#23616;&#20195;&#29702;&#23545;$k\leq n$&#20010;&#23616;&#37096;&#20195;&#29702;&#36827;&#34892;&#23376;&#37319;&#26679;&#20197;&#22312;&#20165;&#25351;&#25968;&#20110;$k$&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19982;&#25351;&#25968;&#20110;$n$&#30340;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#25351;&#25968;&#21152;&#36895;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#30528;&#23376;&#37319;&#26679;&#20195;&#29702;&#25968;$k$&#30340;&#22686;&#21152;&#65292;&#23398;&#21040;&#30340;&#31574;&#30053;&#23558;&#25910;&#25947;&#20110;&#39034;&#24207;&#20026;$\tilde{O}(1/\sqrt{k}+\epsilon_{k,m})$&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00222v1 Announce Type: new  Abstract: We study reinforcement learning for global decision-making in the presence of many local agents, where the global decision-maker makes decisions affecting all local agents, and the objective is to learn a policy that maximizes the rewards of both the global and the local agents. Such problems find many applications, e.g. demand response, EV charging, queueing, etc. In this setting, scalability has been a long-standing challenge due to the size of the state/action space which can be exponential in the number of agents. This work proposes the SUB-SAMPLE-Q algorithm where the global agent subsamples $k\leq n$ local agents to compute an optimal policy in time that is only exponential in $k$, providing an exponential speedup from standard methods that are exponential in $n$. We show that the learned policy converges to the optimal policy in the order of $\tilde{O}(1/\sqrt{k}+\epsilon_{k,m})$ as the number of sub-sampled agents $k$ increases, 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;XLSR Wav2Vec2&#21644;mBART&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#35270;&#39057;&#20869;&#23481;&#30340;&#22810;&#35821;&#35328;&#36716;&#24405;&#21644;&#32763;&#35793;&#65292;&#20026;&#20010;&#24615;&#21270;&#35821;&#38899;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.00212</link><description>&lt;p&gt;
&#20351;&#29992;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;XLSR Wav2Vec2&#21644;mBART&#36827;&#34892;&#35270;&#39057;&#30340;&#36716;&#24405;&#21644;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00212
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;XLSR Wav2Vec2&#21644;mBART&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#35270;&#39057;&#20869;&#23481;&#30340;&#22810;&#35821;&#35328;&#36716;&#24405;&#21644;&#32763;&#35793;&#65292;&#20026;&#20010;&#24615;&#21270;&#35821;&#38899;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#20351;&#29992;&#26497;&#23569;&#25968;&#25454;&#35757;&#32451;&#20010;&#24615;&#21270;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#20165;&#21033;&#29992;&#26469;&#33258;YouTube&#35270;&#39057;&#30340;14&#20998;&#38047;&#33258;&#23450;&#20041;&#38899;&#39057;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#26816;&#32034;&#30340;&#35821;&#38899;&#36716;&#25442;&#65288;RVC&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#23450;&#20041;&#30340;Common Voice 16.0&#35821;&#26009;&#24211;&#12290;&#38543;&#21518;&#65292;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#19968;&#20010;&#36328;&#35821;&#35328;&#33258;&#30417;&#30563;&#34920;&#31034;&#65288;XLSR&#65289;Wav2Vec2&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#24320;&#21457;&#30340;&#22522;&#20110;Web&#30340;GUI&#21487;&#20197;&#39640;&#25928;&#22320;&#36716;&#24405;&#21644;&#32763;&#35793;&#36755;&#20837;&#30340;&#21360;&#22320;&#35821;&#35270;&#39057;&#12290;&#36890;&#36807;&#38598;&#25104;XLSR Wav2Vec2&#21644;mBART&#65292;&#35813;&#31995;&#32479;&#23558;&#32763;&#35793;&#21518;&#30340;&#25991;&#26412;&#19982;&#35270;&#39057;&#26102;&#38388;&#36724;&#23545;&#40784;&#65292;&#20026;&#20010;&#24615;&#21270;&#35821;&#38899;&#30340;&#22810;&#35821;&#35328;&#35270;&#39057;&#20869;&#23481;&#36716;&#24405;&#21644;&#32763;&#35793;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00212v1 Announce Type: new  Abstract: This research addresses the challenge of training an ASR model for personalized voices with minimal data. Utilizing just 14 minutes of custom audio from a YouTube video, we employ Retrieval-Based Voice Conversion (RVC) to create a custom Common Voice 16.0 corpus. Subsequently, a Cross-lingual Self-supervised Representations (XLSR) Wav2Vec2 model is fine-tuned on this dataset. The developed web-based GUI efficiently transcribes and translates input Hindi videos. By integrating XLSR Wav2Vec2 and mBART, the system aligns the translated text with the video timeline, delivering an accessible solution for multilingual video content transcription and translation for personalized voice.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#65292;&#25913;&#36827;&#20102;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#26041;&#27861;&#65292;&#20943;&#36731;&#25945;&#24072;&#32321;&#37325;&#30340;&#24037;&#20316;&#37327;&#65292;&#38450;&#27490;&#29983;&#25104;&#26080;&#25928;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00199</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#25913;&#36827;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Socratic Question Generation using Data Augmentation and Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00199
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#65292;&#25913;&#36827;&#20102;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#26041;&#27861;&#65292;&#20943;&#36731;&#25945;&#24072;&#32321;&#37325;&#30340;&#24037;&#20316;&#37327;&#65292;&#38450;&#27490;&#29983;&#25104;&#26080;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#26159;&#19968;&#31181;&#24341;&#23548;&#23398;&#29983;&#29420;&#31435;&#35299;&#20915;&#38382;&#39064;&#32780;&#19981;&#30452;&#25509;&#25581;&#31034;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#25913;&#36827;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#33487;&#26684;&#25289;&#24213;&#38382;&#39064;&#65292;&#20197;&#20943;&#36731;&#25945;&#24072;&#30340;&#32321;&#37325;&#24037;&#20316;&#37327;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#28041;&#21450;&#25552;&#31034;&#36825;&#20123;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26377;&#26102;&#20250;&#20135;&#29983;&#26080;&#25928;&#30340;&#36755;&#20986;&#65292;&#20363;&#22914;&#30452;&#25509;&#25581;&#31034;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#25110;&#25552;&#20379;&#26080;&#20851;&#25110;&#36807;&#26089;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#39318;&#20808;&#25552;&#20986;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#20016;&#23500;&#29616;&#26377;&#30340;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#25968;&#25454;&#38598;&#65307;&#20854;&#27425;&#65292;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#24320;&#28304;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;LLama 2&#65292;&#20197;&#26356;&#20542;&#21521;&#20110;&#22320;&#38754;&#30495;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00199v1 Announce Type: new  Abstract: The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questio
&lt;/p&gt;</description></item><item><title>AXOLOTL&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#19977;&#27493;&#36807;&#31243;&#65292;&#35782;&#21035;&#21644;&#35299;&#20915;&#20559;&#35265;&#65292;&#25351;&#23548;&#27169;&#22411;&#33258;&#25105;&#21435;&#20559;&#35265;&#20854;&#36755;&#20986;&#65292;&#20174;&#32780;&#23454;&#29616;&#20844;&#24179;&#24615;&#24182;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00198</link><description>&lt;p&gt;
AXOLOTL&#65306;&#36890;&#36807;&#36741;&#21161;&#33258;&#25105;&#21435;&#20559;&#35265;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#23454;&#29616;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00198
&lt;/p&gt;
&lt;p&gt;
AXOLOTL&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#19977;&#27493;&#36807;&#31243;&#65292;&#35782;&#21035;&#21644;&#35299;&#20915;&#20559;&#35265;&#65292;&#25351;&#23548;&#27169;&#22411;&#33258;&#25105;&#21435;&#20559;&#35265;&#20854;&#36755;&#20986;&#65292;&#20174;&#32780;&#23454;&#29616;&#20844;&#24179;&#24615;&#24182;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20294;&#23481;&#26131;&#21463;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#24433;&#21709;&#65292;&#23548;&#33268;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20986;&#29616;&#19981;&#20844;&#24179;&#32467;&#26524;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#31574;&#30053;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AXOLOTL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#26694;&#26550;&#65292;&#33021;&#22815;&#29420;&#31435;&#20110;&#20219;&#21153;&#21644;&#27169;&#22411;&#36816;&#34892;&#65292;&#22312;&#19981;&#30452;&#25509;&#35775;&#38382;&#20869;&#37096;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#20844;&#20849;API&#19982;LLMs&#20132;&#20114;&#12290;&#36890;&#36807;&#31867;&#20284;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#19977;&#27493;&#36807;&#31243;&#65292;AXOLOTL&#35782;&#21035;&#20559;&#35265;&#65292;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25351;&#23548;&#27169;&#22411;&#33258;&#25105;&#21435;&#20559;&#35265;&#20854;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#20445;&#25345;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#20351;AXOLOTL&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#26131;&#29992;&#24615;&#30340;&#21435;&#20559;&#35265;LLM&#36755;&#20986;&#30340;&#26377;&#21069;&#26223;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00198v1 Announce Type: cross  Abstract: Pre-trained Large Language Models (LLMs) have significantly advanced natural language processing capabilities but are susceptible to biases present in their training data, leading to unfair outcomes in various applications. While numerous strategies have been proposed to mitigate bias, they often require extensive computational resources and may compromise model performance. In this work, we introduce AXOLOTL, a novel post-processing framework, which operates agnostically across tasks and models, leveraging public APIs to interact with LLMs without direct access to internal parameters. Through a three-step process resembling zero-shot learning, AXOLOTL identifies biases, proposes resolutions, and guides the model to self-debias its outputs. This approach minimizes computational costs and preserves model performance, making AXOLOTL a promising tool for debiasing LLM outputs with broad applicability and ease of use.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#30001;&#20110;&#20256;&#24863;&#22120;&#24103;&#29575;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#21033;&#29992;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;cGANs&#65289;&#20013;&#30340;pix2pix&#26550;&#26500;&#27604;CycleGAN&#34920;&#29616;&#26356;&#22909;&#65292;&#22810;&#35270;&#35282;&#36755;&#20837;&#39118;&#26684;&#29305;&#21035;&#26159;&#22534;&#21472;&#35270;&#22270;&#21487;&#20197;&#22686;&#24378;&#28909;&#22270;&#20687;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.00196</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#23398;&#20064;&#25214;&#21040;&#32570;&#22833;&#35270;&#39057;&#24103;&#65306;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#21450;&#22312;&#20351;&#29992;RGB&#25668;&#20687;&#22836;&#29983;&#25104;&#28909;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00196
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#30001;&#20110;&#20256;&#24863;&#22120;&#24103;&#29575;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#21033;&#29992;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;cGANs&#65289;&#20013;&#30340;pix2pix&#26550;&#26500;&#27604;CycleGAN&#34920;&#29616;&#26356;&#22909;&#65292;&#22810;&#35270;&#35282;&#36755;&#20837;&#39118;&#26684;&#29305;&#21035;&#26159;&#22534;&#21472;&#35270;&#22270;&#21487;&#20197;&#22686;&#24378;&#28909;&#22270;&#20687;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#65288;ADAS&#65289;&#22312;&#26234;&#33021;&#36710;&#36742;&#20013;&#20381;&#36182;&#20110;&#36710;&#36742;&#36710;&#21410;&#20869;&#30340;&#20934;&#30830;&#39550;&#39542;&#21592;&#24863;&#30693;&#65292;&#36890;&#24120;&#21033;&#29992;&#21508;&#31181;&#20256;&#24863;&#27169;&#24335;&#30340;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#24335;&#20197;&#19981;&#21516;&#30340;&#36895;&#29575;&#25805;&#20316;&#65292;&#23545;&#20110;&#23454;&#26102;&#12289;&#20840;&#38754;&#30340;&#39550;&#39542;&#21592;&#29366;&#24577;&#30417;&#27979;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#30001;&#20110;&#20256;&#24863;&#22120;&#24103;&#29575;&#19981;&#21305;&#37197;&#23548;&#33268;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#26469;&#21019;&#24314;&#21512;&#25104;&#20294;&#36924;&#30495;&#30340;&#28909;&#25104;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;cGANs&#65289;&#65292;&#20855;&#20307;&#27604;&#36739;&#20102;pix2pix&#21644;CycleGAN&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;pix2pix&#20248;&#20110;CycleGAN&#65292;&#24182;&#19988;&#21033;&#29992;&#22810;&#35270;&#35282;&#36755;&#20837;&#39118;&#26684;&#65292;&#29305;&#21035;&#26159;&#22534;&#21472;&#35270;&#22270;&#65292;&#22686;&#24378;&#20102;&#28909;&#22270;&#20687;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#20027;&#20307;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#20010;&#24615;&#21270;&#35757;&#32451;&#23545;&#20110;&#26368;&#20339;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00196v1 Announce Type: cross  Abstract: Advanced Driver Assistance Systems (ADAS) in intelligent vehicles rely on accurate driver perception within the vehicle cabin, often leveraging a combination of sensing modalities. However, these modalities operate at varying rates, posing challenges for real-time, comprehensive driver state monitoring. This paper addresses the issue of missing data due to sensor frame rate mismatches, introducing a generative model approach to create synthetic yet realistic thermal imagery. We propose using conditional generative adversarial networks (cGANs), specifically comparing the pix2pix and CycleGAN architectures. Experimental results demonstrate that pix2pix outperforms CycleGAN, and utilizing multi-view input styles, especially stacked views, enhances the accuracy of thermal image generation. Moreover, the study evaluates the model's generalizability across different subjects, revealing the importance of individualized training for optimal pe
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#26377;&#21161;&#20110;&#32531;&#35299;&#36739;&#24046;&#30340;&#22806;&#25512;&#65292;&#20294;&#23545;&#25968;&#25454;&#38598;&#20559;&#35265;&#26080;&#27982;&#20110;&#20107;&#12290;</title><link>https://arxiv.org/abs/2403.00194</link><description>&lt;p&gt;
&#35810;&#38382;&#24744;&#30340;&#20998;&#24067;&#36716;&#31227;&#26159;&#21542;&#36866;&#21512;&#36827;&#34892;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Ask Your Distribution Shift if Pre-Training is Right for You
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00194
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#26377;&#21161;&#20110;&#32531;&#35299;&#36739;&#24046;&#30340;&#22806;&#25512;&#65292;&#20294;&#23545;&#25968;&#25454;&#38598;&#20559;&#35265;&#26080;&#27982;&#20110;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#21457;&#23545;&#20998;&#24067;&#36716;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20854;&#26377;&#25928;&#24615;&#22240;&#24773;&#20917;&#32780;&#24322;&#65306;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#40065;&#26834;&#24615;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#21017;&#23436;&#20840;&#19981;&#21516;&#65288;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#27604;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#25551;&#36848;&#39044;&#35757;&#32451;&#21487;&#20197;&#21644;&#19981;&#33021;&#35299;&#20915;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#21487;&#33021;&#20986;&#29616;&#30340;&#20004;&#31181;&#22833;&#36133;&#27169;&#24335;&#65306;&#36739;&#24046;&#30340;&#22806;&#25512;&#65288;&#20363;&#22914;&#65292;&#23427;&#20204;&#26080;&#27861;&#25512;&#24191;&#21040;&#19981;&#21516;&#39046;&#22495;&#65289;&#21644;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#65288;&#20363;&#22914;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#34394;&#20551;&#29305;&#24449;&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#19968;&#20010;&#32463;&#39564;&#20934;&#21017;&#65292;&#39044;&#35757;&#32451;&#21487;&#20197;&#24110;&#21161;&#32531;&#35299;&#36739;&#24046;&#30340;&#22806;&#25512;&#65292;&#20294;&#19981;&#33021;&#32531;&#35299;&#25968;&#25454;&#38598;&#20559;&#35265;&#12290;&#22312;&#25552;&#20379;&#20102;&#36825;&#19968;&#21457;&#29616;&#30340;&#29702;&#35770;&#21160;&#26426;&#21644;&#23454;&#35777;&#35777;&#25454;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24320;&#21457;&#40065;&#26834;&#27169;&#22411;&#30340;&#20004;&#20010;&#28508;&#22312;&#21547;&#20041;&#65306;&#65288;1&#65289;&#39044;&#35757;&#32451;&#21644;&#26088;&#22312;&#38450;&#27490;&#21033;&#29992;&#20559;&#35265;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00194v1 Announce Type: new  Abstract: Pre-training is a widely used approach to develop models that are robust to distribution shifts. However, in practice, its effectiveness varies: fine-tuning a pre-trained model improves robustness significantly in some cases but not at all in others (compared to training from scratch). In this work, we seek to characterize the failure modes that pre-training can and cannot address. In particular, we focus on two possible failure modes of models under distribution shift: poor extrapolation (e.g., they cannot generalize to a different domain) and biases in the training data (e.g., they rely on spurious features). Our study suggests that, as a rule of thumb, pre-training can help mitigate poor extrapolation but not dataset biases. After providing theoretical motivation and empirical evidence for this finding, we explore two of its implications for developing robust models: (1) pre-training and interventions designed to prevent exploiting bi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20998;&#25955;&#23398;&#20064;&#23545;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#20013;&#29609;&#23478;&#25928;&#29992;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#36951;&#25022;&#22522;&#20934;&#26469;&#26356;&#22909;&#25429;&#25417;&#31995;&#32479;&#29305;&#24449;&#65292;&#24182;&#24320;&#21457;&#20102;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#36951;&#25022;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00188</link><description>&lt;p&gt;
&#20998;&#25955;&#23398;&#20064;&#23545;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#20013;&#29609;&#23478;&#25928;&#29992;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Decentralized Learning on Player Utilities in Stackelberg Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00188
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20998;&#25955;&#23398;&#20064;&#23545;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#20013;&#29609;&#23478;&#25928;&#29992;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#36951;&#25022;&#22522;&#20934;&#26469;&#26356;&#22909;&#25429;&#25417;&#31995;&#32479;&#29305;&#24449;&#65292;&#24182;&#24320;&#21457;&#20102;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#36951;&#25022;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23398;&#20064;&#20195;&#29702;&#65288;&#22914;&#25512;&#33616;&#31995;&#32479;&#25110;&#32842;&#22825;&#26426;&#22120;&#20154;&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#26102;&#65292;&#36890;&#24120;&#20250;&#38543;&#26102;&#38388;&#21453;&#22797;&#19982;&#21478;&#19968;&#20010;&#23398;&#20064;&#20195;&#29702;&#65288;&#22914;&#29992;&#25143;&#65289;&#20132;&#20114;&#12290;&#22312;&#35768;&#22810;&#36825;&#26679;&#30340;&#21452;&#20195;&#29702;&#31995;&#32479;&#20013;&#65292;&#27599;&#20010;&#20195;&#29702;&#21333;&#29420;&#23398;&#20064;&#65292;&#32780;&#20004;&#20010;&#20195;&#29702;&#30340;&#22870;&#21169;&#24182;&#19981;&#23436;&#20840;&#19968;&#33268;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31867;&#24773;&#20917;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20195;&#29702;&#31995;&#32479;&#30340;&#23398;&#20064;&#21160;&#24577;&#20197;&#21450;&#23545;&#27599;&#20010;&#20195;&#29702;&#30446;&#26631;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#31995;&#32479;&#24314;&#27169;&#20026;&#20855;&#26377;&#20998;&#25955;&#23398;&#20064;&#30340;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#65292;&#24182;&#23637;&#31034;&#26631;&#20934;&#36951;&#25022;&#22522;&#20934;&#65288;&#22914;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#22343;&#34913;&#22238;&#25253;&#65289;&#23548;&#33268;&#33267;&#23569;&#26377;&#19968;&#21517;&#29609;&#23478;&#20986;&#29616;&#26368;&#22351;&#24773;&#20917;&#30340;&#32447;&#24615;&#36951;&#25022;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#36825;&#20123;&#31995;&#32479;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#23545;&#20195;&#29702;&#30340;&#23398;&#20064;&#35823;&#24046;&#23481;&#24525;&#30340;&#25918;&#26494;&#36951;&#25022;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#23398;&#20064;&#31639;&#27861;&#26410;&#33021;&#25552;&#20379;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#24320;&#21457;&#20102;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#21452;&#26041;&#29609;&#23478;&#32780;&#35328;&#19982;&#29702;&#24819;$O(T^{2/3})$&#36951;&#25022;&#25509;&#36817;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00188v1 Announce Type: new  Abstract: When deployed in the world, a learning agent such as a recommender system or a chatbot often repeatedly interacts with another learning agent (such as a user) over time. In many such two-agent systems, each agent learns separately and the rewards of the two agents are not perfectly aligned. To better understand such cases, we examine the learning dynamics of the two-agent system and the implications for each agent's objective. We model these systems as Stackelberg games with decentralized learning and show that standard regret benchmarks (such as Stackelberg equilibrium payoffs) result in worst-case linear regret for at least one player. To better capture these systems, we construct a relaxed regret benchmark that is tolerant to small learning errors by agents. We show that standard learning algorithms fail to provide sublinear regret, and we develop algorithms to achieve near-optimal $O(T^{2/3})$ regret for both players with respect to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#24230;&#38750;&#22343;&#21248;&#37319;&#26679;&#26465;&#20214;&#19979;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#20837;&#21475;&#29305;&#23450;&#30028;&#38480;&#65292;&#36890;&#36807;&#23450;&#21046;&#27599;&#20010;&#26465;&#30446;&#30340;&#35823;&#24046;&#19978;&#30028;&#26469;&#21305;&#37197;&#19968;&#23450;&#26465;&#20214;&#19979;&#30340;&#26497;&#23567;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.00184</link><description>&lt;p&gt;
&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#22312;&#39640;&#24230;&#38750;&#22343;&#21248;&#37319;&#26679;&#19979;&#30340;&#20837;&#21475;&#29305;&#23450;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Entry-Specific Bounds for Low-Rank Matrix Completion under Highly Non-Uniform Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00184
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#24230;&#38750;&#22343;&#21248;&#37319;&#26679;&#26465;&#20214;&#19979;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#20837;&#21475;&#29305;&#23450;&#30028;&#38480;&#65292;&#36890;&#36807;&#23450;&#21046;&#27599;&#20010;&#26465;&#30446;&#30340;&#35823;&#24046;&#19978;&#30028;&#26469;&#21305;&#37197;&#19968;&#23450;&#26465;&#20214;&#19979;&#30340;&#26497;&#23567;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#28041;&#21450;&#20351;&#29992;&#31232;&#30095;&#19968;&#32452;&#35266;&#27979;&#26465;&#30446;&#26469;&#20272;&#35745;&#30697;&#38453;&#20013;&#26410;&#35266;&#23519;&#21040;&#30340;&#26465;&#30446;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#35266;&#27979;&#21040;&#30340;&#26465;&#30446;&#26159;&#29992;&#39640;&#24230;&#21464;&#21270;&#30340;&#27010;&#29575;&#36827;&#34892;&#37319;&#26679;&#30340;&#38750;&#22343;&#21248;&#35774;&#32622;&#65292;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#28176;&#36817;&#26631;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#32467;&#26500;&#21270;&#37319;&#26679;&#27010;&#29575;&#19979;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#20197;&#21450;&#26377;&#26102;&#22312;&#36739;&#23567;&#30340;&#23376;&#30697;&#38453;&#19978;&#36816;&#34892;&#20272;&#35745;&#31639;&#27861;&#27604;&#22312;&#25972;&#20010;&#30697;&#38453;&#19978;&#26356;&#22909;&#65292;&#29978;&#33267;&#26159;&#26368;&#20339;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23450;&#21046;&#21040;&#27599;&#20010;&#26465;&#30446;&#30340;&#35823;&#24046;&#19978;&#30028;&#65292;&#36825;&#20123;&#19978;&#30028;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#19982;&#26497;&#23567;&#19979;&#30028;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#20197;&#23616;&#37096;&#37319;&#26679;&#27010;&#29575;&#30340;&#20989;&#25968;&#30340;&#24418;&#24335;&#21051;&#30011;&#20102;&#20272;&#35745;&#27599;&#20010;&#26465;&#30446;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00184v1 Announce Type: cross  Abstract: Low-rank matrix completion concerns the problem of estimating unobserved entries in a matrix using a sparse set of observed entries. We consider the non-uniform setting where the observed entries are sampled with highly varying probabilities, potentially with different asymptotic scalings. We show that under structured sampling probabilities, it is often better and sometimes optimal to run estimation algorithms on a smaller submatrix rather than the entire matrix. In particular, we prove error upper bounds customized to each entry, which match the minimax lower bounds under certain conditions. Our bounds characterize the hardness of estimating each entry as a function of the localized sampling probabilities. We provide numerical experiments that confirm our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Causal Graph Ordinary Differential Equations (CAG-ODE) &#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#22788;&#29702;&#30340;&#36830;&#32493;&#21160;&#24577;&#24433;&#21709;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;ODE&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.00178</link><description>&lt;p&gt;
&#22240;&#26524;&#22270;ODE&#65306;&#22810;&#26234;&#33021;&#20307;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#36830;&#32493;&#22788;&#29702;&#25928;&#24212;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Causal Graph ODE: Continuous Treatment Effect Modeling in Multi-agent Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00178
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Causal Graph Ordinary Differential Equations (CAG-ODE) &#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#22788;&#29702;&#30340;&#36830;&#32493;&#21160;&#24577;&#24433;&#21709;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;ODE&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#36890;&#24120;&#26159;&#21160;&#24577;&#21644;&#36830;&#32493;&#30340;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#20849;&#21516;&#28436;&#21270;&#65292;&#24182;&#38543;&#26102;&#38388;&#25913;&#21464;&#20854;&#36712;&#36857;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#20363;&#22914;&#65292;&#32654;&#22269;&#30340;COVID-19&#20256;&#25773;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#20854;&#20013;&#21508;&#24030;&#20805;&#24403;&#26234;&#33021;&#20307;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#26085;&#24120;&#20154;&#21475;&#27969;&#21160;&#23601;&#26159;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#20272;&#35745;&#21453;&#20107;&#23454;&#32467;&#26524;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#26410;&#26469;&#39044;&#27979;&#21644;&#26377;&#25928;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#20363;&#22914;&#21046;&#23450;COVID-19&#25919;&#31574;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#26377;&#25928;&#22320;&#27169;&#25311;&#22788;&#29702;&#23545;&#32467;&#26524;&#30340;&#36830;&#32493;&#21160;&#24577;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#24403;&#21516;&#26102;&#24212;&#29992;&#22810;&#31181;&#22788;&#29702;&#65288;&#20363;&#22914;&#8220;&#23621;&#23478;&#38548;&#31163;&#8221;&#21644;&#8220;&#25509;&#31181;&#30123;&#33495;&#8221;&#25919;&#31574;&#65289;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#22270;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;CAG-ODE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;ODE&#20989;&#25968;&#25429;&#25417;&#26234;&#33021;&#20307;&#20043;&#38388;&#36830;&#32493;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00178v1 Announce Type: cross  Abstract: Real-world multi-agent systems are often dynamic and continuous, where the agents co-evolve and undergo changes in their trajectories and interactions over time. For example, the COVID-19 transmission in the U.S. can be viewed as a multi-agent system, where states act as agents and daily population movements between them are interactions. Estimating the counterfactual outcomes in such systems enables accurate future predictions and effective decision-making, such as formulating COVID-19 policies. However, existing methods fail to model the continuous dynamic effects of treatments on the outcome, especially when multiple treatments (e.g., "stay-at-home" and "get-vaccine" policies) are applied simultaneously. To tackle this challenge, we propose Causal Graph Ordinary Differential Equations (CAG-ODE), a novel model that captures the continuous interaction among agents using a Graph Neural Network (GNN) as the ODE function. The key innovat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#38750;&#20405;&#20837;&#24335;&#24739;&#32773;&#20581;&#24247;&#25968;&#25454;&#35782;&#21035;&#25968;&#23383;&#23402;&#29983;&#20307;&#27169;&#22411;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38750;&#20405;&#20837;&#24335;&#21307;&#23398;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#26500;&#24314;&#12290;</title><link>https://arxiv.org/abs/2403.00177</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26500;&#24314;&#38750;&#20405;&#20837;&#24335;&#21307;&#23398;&#25968;&#23383;&#23402;&#29983;&#20307;
&lt;/p&gt;
&lt;p&gt;
Non-Invasive Medical Digital Twins using Physics-Informed Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00177
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#38750;&#20405;&#20837;&#24335;&#24739;&#32773;&#20581;&#24247;&#25968;&#25454;&#35782;&#21035;&#25968;&#23383;&#23402;&#29983;&#20307;&#27169;&#22411;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38750;&#20405;&#20837;&#24335;&#21307;&#23398;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#20307;&#26159;&#23454;&#29616;&#23454;&#38469;&#29289;&#29702;&#29616;&#35937;&#30340;&#34394;&#25311;&#22797;&#21046;&#21697;&#65292;&#21033;&#29992;&#25968;&#23398;&#24314;&#27169;&#26469;&#34920;&#24449;&#21644;&#27169;&#25311;&#20854;&#23450;&#20041;&#29305;&#24449;&#12290;&#36890;&#36807;&#20026;&#30142;&#30149;&#36807;&#31243;&#26500;&#24314;&#25968;&#23383;&#23402;&#29983;&#20307;&#65292;&#25105;&#20204;&#21487;&#20197;&#36827;&#34892;&#20223;&#30495;&#65292;&#27169;&#25311;&#24739;&#32773;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#30340;&#20581;&#24247;&#29366;&#20917;&#21644;&#22312;&#20551;&#35774;&#24178;&#39044;&#19979;&#30340;&#23545;&#29031;&#32467;&#26524;&#12290;&#36825;&#28040;&#38500;&#20102;&#20405;&#20837;&#24615;&#31243;&#24207;&#25110;&#19981;&#30830;&#23450;&#27835;&#30103;&#20915;&#31574;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#21033;&#29992;&#38750;&#20405;&#20837;&#24335;&#24739;&#32773;&#20581;&#24247;&#25968;&#25454;&#26469;&#35782;&#21035;&#25968;&#23383;&#23402;&#29983;&#20307;&#27169;&#22411;&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25968;&#23383;&#23402;&#29983;&#20307;&#24314;&#27169;&#30475;&#20316;&#19968;&#20010;&#22797;&#21512;&#36870;&#38382;&#39064;&#65292;&#24182;&#35266;&#23519;&#21040;&#20854;&#32467;&#26500;&#31867;&#20284;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#31181;&#31639;&#27861;&#39318;&#20808;&#22312;&#35299;&#20915;&#29289;&#29702;&#27169;&#22411;&#26041;&#31243;&#30340;&#20551;&#23450;&#20219;&#21153;&#19978;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#38543;&#21518;&#65292;&#35813;&#27169;&#22411;&#34987;&#35757;&#32451;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00177v1 Announce Type: new  Abstract: A digital twin is a virtual replica of a real-world physical phenomena that uses mathematical modeling to characterize and simulate its defining features. By constructing digital twins for disease processes, we can perform in-silico simulations that mimic patients' health conditions and counterfactual outcomes under hypothetical interventions in a virtual setting. This eliminates the need for invasive procedures or uncertain treatment decisions. In this paper, we propose a method to identify digital twin model parameters using only noninvasive patient health data. We approach the digital twin modeling as a composite inverse problem, and observe that its structure resembles pretraining and finetuning in self-supervised learning (SSL). Leveraging this, we introduce a physics-informed SSL algorithm that initially pretrains a neural network on the pretext task of solving the physical model equations. Subsequently, the model is trained to rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SoD$^2$&#26694;&#26550;&#65292;&#29992;&#20110;&#38745;&#24577;&#20248;&#21270;&#21160;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31209;&#21644;&#32500;&#24230;&#20256;&#25773;&#65288;RDP&#65289;&#26041;&#27861;&#23454;&#29616;&#20102;&#25805;&#20316;&#31526;&#30340;&#24418;&#29366;&#38745;&#24577;&#30830;&#23450;&#65292;&#36827;&#32780;&#36827;&#34892;&#19968;&#31995;&#21015;&#20248;&#21270;&#65292;&#21253;&#25324;&#34701;&#21512;&#20195;&#30721;&#29983;&#25104;&#12289;&#25191;&#34892;&#35745;&#21010;&#21644;&#36816;&#34892;&#26102;&#20869;&#23384;&#20998;&#37197;&#35745;&#21010;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.00176</link><description>&lt;p&gt;
SoD$^2$: &#38745;&#24577;&#20248;&#21270;&#21160;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SoD$^2$: Statically Optimizing Dynamic Deep Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SoD$^2$&#26694;&#26550;&#65292;&#29992;&#20110;&#38745;&#24577;&#20248;&#21270;&#21160;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31209;&#21644;&#32500;&#24230;&#20256;&#25773;&#65288;RDP&#65289;&#26041;&#27861;&#23454;&#29616;&#20102;&#25805;&#20316;&#31526;&#30340;&#24418;&#29366;&#38745;&#24577;&#30830;&#23450;&#65292;&#36827;&#32780;&#36827;&#34892;&#19968;&#31995;&#21015;&#20248;&#21270;&#65292;&#21253;&#25324;&#34701;&#21512;&#20195;&#30721;&#29983;&#25104;&#12289;&#25191;&#34892;&#35745;&#21010;&#21644;&#36816;&#34892;&#26102;&#20869;&#23384;&#20998;&#37197;&#35745;&#21010;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#24180;&#26469;&#24050;&#24320;&#21457;&#20102;&#35768;&#22810;&#38024;&#23545;DNN&#30340;&#32534;&#35793;&#21644;&#36816;&#34892;&#26102;&#31995;&#32479;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;DNN&#19978;&#12290;&#21160;&#24577;DNN&#65292;&#20854;&#20013;&#24352;&#37327;&#24418;&#29366;&#21644;&#22823;&#23567;&#29978;&#33267;&#20351;&#29992;&#30340;&#25805;&#20316;&#31526;&#38598;&#21462;&#20915;&#20110;&#36755;&#20837;&#21644;/&#25110;&#25191;&#34892;&#65292;&#27491;&#22312;&#21464;&#24471;&#24120;&#35265;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SoD$^2$&#65292;&#19968;&#20010;&#29992;&#20110;&#20248;&#21270;&#21160;&#24577;DNN&#30340;&#32508;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#22522;&#30784;&#26159;&#23545;&#26500;&#25104;DNN&#30340;&#24120;&#35265;&#25805;&#20316;&#31526;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#20998;&#31867;&#26041;&#27861;&#26469;&#23454;&#29616;&#31209;&#21644;&#32500;&#24230;&#20256;&#25773;&#65288;RDP&#65289;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#38745;&#24577;&#30830;&#23450;&#25805;&#20316;&#31526;&#30340;&#24418;&#29366;&#20026;&#24050;&#30693;&#24120;&#37327;&#12289;&#31526;&#21495;&#24120;&#37327;&#25110;&#36825;&#20123;&#25805;&#20316;&#30340;&#36816;&#31639;&#12290;&#25509;&#19979;&#26469;&#65292;&#20351;&#29992;RDP&#25105;&#20204;&#23454;&#29616;&#19968;&#31995;&#21015;&#20248;&#21270;&#65292;&#22914;&#34701;&#21512;&#20195;&#30721;&#29983;&#25104;&#12289;&#25191;&#34892;&#65288;&#39034;&#24207;&#65289;&#35745;&#21010;&#65292;&#29978;&#33267;&#36816;&#34892;&#26102;&#20869;&#23384;&#20998;&#37197;&#35745;&#21010;&#29983;&#25104;&#12290;&#36890;&#36807;&#22312; 10 &#20010;&#26032;&#20852;&#21160;&#24577;DNN &#19978;&#35780;&#20272;&#35813;&#26694;&#26550;&#65292;&#24182;&#23558;&#20854;&#19982;&#20960;&#20010;&#29616;&#26377;&#31995;&#32479;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00176v1 Announce Type: cross  Abstract: Though many compilation and runtime systems have been developed for DNNs in recent years, the focus has largely been on static DNNs. Dynamic DNNs, where tensor shapes and sizes and even the set of operators used are dependent upon the input and/or execution, are becoming common. This paper presents SoD$^2$, a comprehensive framework for optimizing Dynamic DNNs. The basis of our approach is a classification of common operators that form DNNs, and the use of this classification towards a Rank and Dimension Propagation (RDP) method. This framework statically determines the shapes of operators as known constants, symbolic constants, or operations on these. Next, using RDP we enable a series of optimizations, like fused code generation, execution (order) planning, and even runtime memory allocation plan generation. By evaluating the framework on 10 emerging Dynamic DNNs and comparing it against several existing systems, we demonstrate both 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#29616;&#26377;&#28909;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#21382;&#21490;&#25968;&#25454;&#25552;&#21462;&#30340;&#20915;&#31574;&#26641;&#37325;&#26032;&#35774;&#35745;HVAC&#25511;&#21046;&#22120;&#65292;&#20811;&#26381;&#20102;&#21487;&#38752;&#24615;&#29942;&#39048;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#33410;&#33021;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.00172</link><description>&lt;p&gt;
&#36229;&#36234;&#40657;&#30418;&#31574;&#30053;&#65306;&#37325;&#26032;&#24605;&#32771;&#21487;&#35299;&#37322;&#21644;&#21487;&#39564;&#35777;HVAC&#25511;&#21046;&#23398;&#20064;&#20195;&#29702;&#30340;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Go Beyond Black-box Policies: Rethinking the Design of Learning Agent for Interpretable and Verifiable HVAC Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00172
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#29616;&#26377;&#28909;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#21382;&#21490;&#25968;&#25454;&#25552;&#21462;&#30340;&#20915;&#31574;&#26641;&#37325;&#26032;&#35774;&#35745;HVAC&#25511;&#21046;&#22120;&#65292;&#20811;&#26381;&#20102;&#21487;&#38752;&#24615;&#29942;&#39048;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#33410;&#33021;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#26377;&#28508;&#21147;&#25552;&#39640;&#26262;&#36890;&#31354;&#35843;&#65288;HVAC&#65289;&#31995;&#32479;&#30340;&#33021;&#25928;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#40657;&#30418;&#28909;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#38543;&#26426;&#20248;&#21270;&#22120;&#65292;&#32570;&#20047;&#21487;&#38752;&#24615;&#20445;&#35777;&#24182;&#23545;&#23621;&#20303;&#32773;&#20581;&#24247;&#26500;&#25104;&#39118;&#38505;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#35774;&#35745;&#20351;&#29992;&#29616;&#26377;&#28909;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#21382;&#21490;&#25968;&#25454;&#25552;&#21462;&#30340;&#20915;&#31574;&#26641;&#30340;HVAC&#25511;&#21046;&#22120;&#65292;&#20811;&#26381;&#20102;&#21487;&#38752;&#24615;&#29942;&#39048;&#12290;&#25105;&#20204;&#30340;&#20915;&#31574;&#26641;&#31574;&#30053;&#26159;&#30830;&#23450;&#24615;&#30340;&#12289;&#21487;&#39564;&#35777;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#65292;&#24182;&#19988;&#27604;&#24403;&#21069;&#30340;MBRL&#26041;&#27861;&#26356;&#33410;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;RL&#20195;&#29702;&#22312;HVAC&#25511;&#21046;&#20013;&#30340;&#26032;&#39062;&#39564;&#35777;&#26631;&#20934;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#25104;&#21487;&#39564;&#35777;&#20915;&#31574;&#26641;&#31574;&#30053;&#30340;&#31574;&#30053;&#25552;&#21462;&#36807;&#31243;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#28909;&#21160;&#21147;&#23398;&#27169;&#22411;&#36755;&#20837;&#30340;&#39640;&#32500;&#24230;&#38459;&#30861;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00172v1 Announce Type: cross  Abstract: Recent research has shown the potential of Model-based Reinforcement Learning (MBRL) to enhance energy efficiency of Heating, Ventilation, and Air Conditioning (HVAC) systems. However, existing methods rely on black-box thermal dynamics models and stochastic optimizers, lacking reliability guarantees and posing risks to occupant health. In this work, we overcome the reliability bottleneck by redesigning HVAC controllers using decision trees extracted from existing thermal dynamics models and historical data. Our decision tree-based policies are deterministic, verifiable, interpretable, and more energy-efficient than current MBRL methods. First, we introduce a novel verification criterion for RL agents in HVAC control based on domain knowledge. Second, we develop a policy extraction procedure that produces a verifiable decision tree policy. We found that the high dimensionality of the thermal dynamics model input hinders the efficiency 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#21807;&#19968;&#30417;&#30563;&#65292;&#21516;&#26102;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00165</link><description>&lt;p&gt;
TELEClass: &#31246;&#21153;&#23398;&#20016;&#23500;&#21644;LLM&#22686;&#24378;&#30340;&#26368;&#23567;&#30417;&#30563;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#21807;&#19968;&#30417;&#30563;&#65292;&#21516;&#26102;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#27599;&#20010;&#25991;&#26723;&#20998;&#31867;&#20026;&#26631;&#31614;Taxonomy&#20013;&#30340;&#19968;&#32452;&#31867;&#21035;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#65306;&#20165;&#20351;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#30417;&#30563;&#26469;&#36827;&#34892;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#38646;&#25552;&#31034;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#23618;&#35774;&#32622;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#22240;&#20026;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#22823;&#32780;&#32467;&#26500;&#21270;&#30340;&#26631;&#31614;&#31354;&#38388;&#26159;&#26080;&#25928;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20197;&#21069;&#30340;&#24369;&#30417;&#30563;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20165;&#21033;&#29992;&#21407;&#22987;&#30340;Taxonomy&#39592;&#26550;&#65292;&#24573;&#30053;&#20102;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#38544;&#34255;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#20316;&#39069;&#22806;&#30340;&#31867;&#21035;&#25351;&#31034;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00165v1 Announce Type: new  Abstract: Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with the minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) show competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting, because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33945;&#29305;&#21345;&#27931;&#39640;&#25928;&#24433;&#21709;&#20989;&#25968;&#65288;MC-EIF&#65289;&#30340;&#20840;&#33258;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#25928;&#24433;&#21709;&#20989;&#25968;&#65292;&#33021;&#22815;&#23454;&#29616;&#38024;&#23545;&#24191;&#27867;&#31867;&#21035;&#30340;&#27169;&#22411;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#32479;&#35745;&#20272;&#35745;&#65292;&#36798;&#21040;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.00158</link><description>&lt;p&gt;
&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#39640;&#25928;&#24433;&#21709;&#20989;&#25968;&#23454;&#29616;&#33258;&#21160;&#21270;&#39640;&#25928;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Automated Efficient Estimation using Monte Carlo Efficient Influence Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33945;&#29305;&#21345;&#27931;&#39640;&#25928;&#24433;&#21709;&#20989;&#25968;&#65288;MC-EIF&#65289;&#30340;&#20840;&#33258;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#25928;&#24433;&#21709;&#20989;&#25968;&#65292;&#33021;&#22815;&#23454;&#29616;&#38024;&#23545;&#24191;&#27867;&#31867;&#21035;&#30340;&#27169;&#22411;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#32479;&#35745;&#20272;&#35745;&#65292;&#36798;&#21040;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#28041;&#21450;&#20351;&#29992;&#39640;&#32500;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20272;&#35745;&#20302;&#32500;&#32479;&#35745;&#37327;&#12290;&#20960;&#31181;&#26041;&#27861;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#29702;&#35770;&#26469;&#35299;&#20915;&#36825;&#20123;&#20272;&#35745;&#20219;&#21153;&#65292;&#20363;&#22914;&#21435;&#20559;/&#21452;&#37325;&#26497;&#22823;&#20284;&#28982;&#25110;&#26377;&#38024;&#23545;&#24615;&#26368;&#23567;&#25439;&#22833;&#20272;&#35745;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#25216;&#26415;&#65292;&#21363;\textit{&#33945;&#29305;&#21345;&#27931;&#39640;&#25928;&#24433;&#21709;&#20989;&#25968;} (MC-EIF)&#65292;&#29992;&#20110;&#36924;&#36817;&#39640;&#25928;&#24433;&#21709;&#20989;&#25968;&#65292;&#19982;&#29616;&#26377;&#30340;&#21487;&#24494;&#27010;&#29575;&#32534;&#31243;&#31995;&#32479;&#26080;&#32541;&#38598;&#25104;&#12290;MC-EIF&#21487;&#33258;&#21160;&#21270;&#24191;&#27867;&#31867;&#21035;&#30340;&#27169;&#22411;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#39640;&#25928;&#32479;&#35745;&#20272;&#35745;&#65292;&#36825;&#20123;&#20272;&#35745;&#20197;&#21069;&#38656;&#35201;&#20005;&#26684;&#30340;&#33258;&#23450;&#20041;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;MC-EIF&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#19988;&#20351;&#29992;MC-EIF&#30340;&#20272;&#35745;&#22120;&#36798;&#21040;&#20102;&#26368;&#20248;&#30340;$\sqrt{N}$&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#65292;&#20351;&#29992;MC-EIF&#30340;&#20272;&#35745;&#22120;&#19982;&#20351;&#29992;&#35299;&#26512;EIF&#30340;&#20272;&#35745;&#22120;&#30456;&#24403;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#19968;&#20010;&#26032;&#39062;&#30340;&#39030;&#28857;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00158v1 Announce Type: cross  Abstract: Many practical problems involve estimating low dimensional statistical quantities with high-dimensional models and datasets. Several approaches address these estimation tasks based on the theory of influence functions, such as debiased/double ML or targeted minimum loss estimation. This paper introduces \textit{Monte Carlo Efficient Influence Functions} (MC-EIF), a fully automated technique for approximating efficient influence functions that integrates seamlessly with existing differentiable probabilistic programming systems. MC-EIF automates efficient statistical estimation for a broad class of models and target functionals that would previously require rigorous custom analysis. We prove that MC-EIF is consistent, and that estimators using MC-EIF achieve optimal $\sqrt{N}$ convergence rates. We show empirically that estimators using MC-EIF are at parity with estimators using analytic EIFs. Finally, we demonstrate a novel capstone exa
&lt;/p&gt;</description></item><item><title>&#24046;&#20998;&#38544;&#31169;&#26159;&#26368;&#26377;&#21069;&#26223;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#29616;&#20195;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#39640;&#32500;&#20248;&#21270;&#65292;&#20855;&#26377;&#20302;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00157</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Distributed Optimization and Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00157
&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#26159;&#26368;&#26377;&#21069;&#26223;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#29616;&#20195;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#39640;&#32500;&#20248;&#21270;&#65292;&#20855;&#26377;&#20302;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#22312;&#20256;&#24863;&#22120;&#32593;&#32476;&#12289;&#26234;&#33021;&#30005;&#32593;&#12289;&#26426;&#22120;&#23398;&#20064;&#31561;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20998;&#24067;&#24335;&#20248;&#21270;&#21644;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#20998;&#24067;&#24335;&#20248;&#21270;&#21644;&#23398;&#20064;&#31639;&#27861;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#29616;&#26377;&#31639;&#27861;&#35201;&#27714;&#27599;&#20010;&#20195;&#29702;&#19982;&#20854;&#37051;&#23621;&#20132;&#25442;&#28040;&#24687;&#65292;&#36825;&#21487;&#33021;&#20250;&#27844;&#38706;&#25935;&#24863;&#20449;&#24687;&#24182;&#24341;&#36215;&#37325;&#22823;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#24615;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#21644;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#23494;&#30721;&#23398;&#12289;&#24046;&#20998;&#38544;&#31169;&#21644;&#20854;&#20182;&#21487;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#25216;&#26415;&#65292;&#24182;&#25351;&#20986;&#23427;&#20204;&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#21644;&#23398;&#20064;&#20013;&#30340;&#20248;&#32570;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#24046;&#20998;&#38544;&#31169;&#26159;&#26368;&#20855;&#26377;&#21069;&#26223;&#30340;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#20302;&#35745;&#31639;&#21644;&#36890;&#20449;&#22797;&#26434;&#24615;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#39640;&#32500;&#20248;&#21270;&#30340;&#29616;&#20195;&#23398;&#20064;&#24212;&#29992;&#26497;&#20855;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00157v1 Announce Type: new  Abstract: Distributed optimization and learning has recently garnered great attention due to its wide applications in sensor networks, smart grids, machine learning, and so forth. Despite rapid development, existing distributed optimization and learning algorithms require each agent to exchange messages with its neighbors, which may expose sensitive information and raise significant privacy concerns. In this survey paper, we overview privacy-preserving distributed optimization and learning methods. We first discuss cryptography, differential privacy, and other techniques that can be used for privacy preservation and indicate their pros and cons for privacy protection in distributed optimization and learning. We believe that among these approaches, differential privacy is most promising due to its low computational and communication complexities, which are extremely appealing for modern learning based applications with high dimensions of optimizati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27010;&#29575;&#28508;&#22312;&#31354;&#38388;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#20248;&#21270;&#32593;&#32476;&#31232;&#30095;&#24230;&#65292;&#24182;&#25506;&#35752;&#20102;&#32593;&#32476;&#23618;&#30340;AP3/AP2&#23646;&#24615;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.00155</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#28508;&#22312;&#31354;&#38388;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Towards Explaining Deep Neural Network Compression Through a Probabilistic Latent Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00155
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#28508;&#22312;&#31354;&#38388;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#20248;&#21270;&#32593;&#32476;&#31232;&#30095;&#24230;&#65292;&#24182;&#25506;&#35752;&#20102;&#32593;&#32476;&#23618;&#30340;AP3/AP2&#23646;&#24615;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#23384;&#20648;&#31354;&#38388;&#28040;&#32791;&#23548;&#33268;&#20102;&#32593;&#32476;&#21387;&#32553;&#30340;&#27010;&#24565;&#12290;&#23613;&#31649;&#24050;&#24191;&#27867;&#30740;&#31350;&#20102;&#35832;&#22914;&#20462;&#21098;&#21644;&#20302;&#31209;&#20998;&#35299;&#31561;DNN&#21387;&#32553;&#25216;&#26415;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#29702;&#35770;&#35299;&#37322;&#20173;&#26410;&#21463;&#21040;&#36275;&#22815;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;DNN&#26435;&#37325;&#30340;&#27010;&#29575;&#28508;&#22312;&#31354;&#38388;&#24182;&#21033;&#29992;&#20449;&#24687;&#29702;&#35770;&#20998;&#27495;&#24230;&#37327;&#35299;&#37322;&#26368;&#20339;&#32593;&#32476;&#31232;&#30095;&#24615;&#30340;&#26032;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#20026;DNN&#24341;&#20837;&#20102;&#26032;&#30340;&#31867;&#27604;&#25237;&#24433;&#27169;&#24335;&#65288;AP2&#65289;&#21644;&#27010;&#29575;&#20013;&#30340;&#31867;&#27604;&#25237;&#24433;&#27169;&#24335;&#65288;AP3&#65289;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#32593;&#32476;&#20013;&#23618;&#30340;AP3/AP2&#29305;&#24615;&#19982;&#20854;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#65292;&#35299;&#37322;&#20102;&#21387;&#32553;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#26159;&#20174;&#23454;&#35777;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00155v1 Announce Type: new  Abstract: Despite the impressive performance of deep neural networks (DNNs), their computational complexity and storage space consumption have led to the concept of network compression. While DNN compression techniques such as pruning and low-rank decomposition have been extensively studied, there has been insufficient attention paid to their theoretical explanation. In this paper, we propose a novel theoretical framework that leverages a probabilistic latent space of DNN weights and explains the optimal network sparsity by using the information-theoretic divergence measures. We introduce new analogous projected patterns (AP2) and analogous-in-probability projected patterns (AP3) notions for DNNs and prove that there exists a relationship between AP3/AP2 property of layers in the network and its performance. Further, we provide a theoretical analysis that explains the training process of the compressed network. The theoretical results are empirica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26680;&#38236;&#20687;Prox&#31639;&#27861;&#29992;&#20110;&#24230;&#37327;&#20248;&#21270;&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#21151;&#33021;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#26080;&#38480;&#32500;&#31354;&#38388;&#20013;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#36895;&#24230;&#20026;$O(1/N)&#12290;</title><link>https://arxiv.org/abs/2403.00147</link><description>&lt;p&gt;
&#26680;&#38236;&#20687;Prox&#29992;&#20110;&#24230;&#37327;&#20248;&#21270;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Kernel Mirror Prox for Measure Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00147
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26680;&#38236;&#20687;Prox&#31639;&#27861;&#29992;&#20110;&#24230;&#37327;&#20248;&#21270;&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#21151;&#33021;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#26080;&#38480;&#32500;&#31354;&#38388;&#20013;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#36895;&#24230;&#20026;$O(1/N)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#20989;&#25968;&#31354;&#38388;&#20316;&#20026;&#38750;&#36127;&#24230;&#37327;&#38181;&#30340;&#23545;&#20598;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#30740;&#31350;&#20102;&#19968;&#31867;&#21151;&#33021;&#38797;&#28857;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#28151;&#21512;&#21151;&#33021;&#32435;&#20160;&#22343;&#34913;(MFNE)&#65292;&#36825;&#26159;&#19968;&#20123;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#65292;&#22914;&#38544;&#24335;&#29983;&#25104;&#27169;&#22411;&#12289;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;(DRO)&#21644;Wasserstein&#36136;&#24515;&#12290; &#24403;&#20989;&#25968;&#31354;&#38388;&#36873;&#25321;&#20026;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#26102;&#65292;&#25105;&#20204;&#23558;&#38797;&#28857;&#20248;&#21270;&#21160;&#24577;&#24314;&#27169;&#20026;&#30456;&#20114;&#20316;&#29992;&#30340;Fisher-Rao-RKHS&#26799;&#24230;&#27969;&#12290;&#20316;&#20026;&#31163;&#25955;&#26102;&#38388;&#23545;&#24212;&#29289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22987;-&#23545;&#20598;&#26680;&#38236;&#20687;Prox(KMP)&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;RKHS&#20013;&#20351;&#29992;&#23545;&#20598;&#27493;&#39588;&#21644;&#21407;&#22987;&#29109;&#38236;&#20687;Prox&#27493;&#39588;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#26080;&#38480;&#32500;&#35774;&#32622;&#19979;&#20026;&#36825;&#31867;MFNE&#38382;&#39064;&#25552;&#20379;&#20102;KMP&#30340;&#32479;&#19968;&#25910;&#25947;&#20998;&#26512;&#65292;&#36825;&#22312;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#19968;&#20010;$O(1/N)$&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00147v1 Announce Type: cross  Abstract: By choosing a suitable function space as the dual to the non-negative measure cone, we study in a unified framework a class of functional saddle-point optimization problems, which we term the Mixed Functional Nash Equilibrium (MFNE), that underlies several existing machine learning algorithms, such as implicit generative models, distributionally robust optimization (DRO), and Wasserstein barycenters. We model the saddle-point optimization dynamics as an interacting Fisher-Rao-RKHS gradient flow when the function space is chosen as a reproducing kernel Hilbert space (RKHS). As a discrete time counterpart, we propose a primal-dual kernel mirror prox (KMP) algorithm, which uses a dual step in the RKHS, and a primal entropic mirror prox step. We then provide a unified convergence analysis of KMP in an infinite-dimensional setting for this class of MFNE problems, which establishes a convergence rate of $O(1/N)$ in the deterministic case and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;EBBS&#65292;&#37197;&#21512;&#26032;&#39062;&#30340;&#21452;&#23618;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#20248;&#20110;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#65292;&#24182;&#23454;&#29616;&#30693;&#35782;&#33976;&#39311;&#26469;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.00144</link><description>&lt;p&gt;
EBBS: &#19968;&#20010;&#20855;&#26377;&#21452;&#23618;&#26463;&#25628;&#32034;&#30340;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#38646;&#32763;&#35793;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00144
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;EBBS&#65292;&#37197;&#21512;&#26032;&#39062;&#30340;&#21452;&#23618;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#20248;&#20110;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#65292;&#24182;&#23454;&#29616;&#30693;&#35782;&#33976;&#39311;&#26469;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25105;&#20204;&#29992;&#29305;&#23450;&#30340;&#32763;&#35793;&#26041;&#21521;&#35757;&#32451;&#22810;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#38646;&#32763;&#35793;&#30340;&#33021;&#21147;&#23601;&#20250;&#20986;&#29616;&#65307;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#22312;&#26410;&#35265;&#36807;&#30340;&#26041;&#21521;&#36827;&#34892;&#32763;&#35793;&#12290;&#21478;&#22806;&#65292;&#38646;&#32763;&#35793;&#20063;&#21487;&#20197;&#36890;&#36807;&#31532;&#19977;&#31181;&#35821;&#35328;&#65288;&#20363;&#22914;&#33521;&#35821;&#65289;&#26469;&#23454;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#31181;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#37117;&#23384;&#22312;&#22122;&#38899;&#65292;&#24182;&#19988;&#34920;&#29616;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EBBS&#65292;&#19968;&#20010;&#20855;&#26377;&#26032;&#39062;&#30340;&#21452;&#23618;&#26463;&#25628;&#32034;&#31639;&#27861;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#38598;&#25104;&#32452;&#20214;&#22312;&#19979;&#23618;&#36880;&#27493;&#25506;&#32034;&#33258;&#24049;&#30340;&#39044;&#27979;&#65292;&#20294;&#23427;&#20204;&#36890;&#36807;&#19978;&#23618;&#30340;&#8220;&#36719;&#25237;&#31080;&#8221;&#26426;&#21046;&#36827;&#34892;&#21516;&#27493;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;EBBS&#22987;&#32456;&#20248;&#20110;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#31181;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#65292;&#20197;&#21450;&#29616;&#26377;&#30340;&#38598;&#25104;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#38598;&#25104;&#30340;&#30693;&#35782;&#20256;&#22238;&#21040;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65307;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;E
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00144v1 Announce Type: cross  Abstract: The ability of zero-shot translation emerges when we train a multilingual model with certain translation directions; the model can then directly translate in unseen directions. Alternatively, zero-shot translation can be accomplished by pivoting through a third language (e.g., English). In our work, we observe that both direct and pivot translations are noisy and achieve less satisfactory performance. We propose EBBS, an ensemble method with a novel bi-level beam search algorithm, where each ensemble component explores its own prediction step by step at the lower level but they are synchronized by a "soft voting" mechanism at the upper level. Results on two popular multilingual translation datasets show that EBBS consistently outperforms direct and pivot translations as well as existing ensemble techniques. Further, we can distill the ensemble's knowledge back to the multilingual model to improve inference efficiency; profoundly, our E
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26641;&#24179;&#22343;&#27861;&#26500;&#24314;&#38598;&#25104;&#35299;&#26512;&#22120;&#65292;&#31283;&#23450;&#24182;&#25552;&#21319;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#22522;&#20934;&#32447;</title><link>https://arxiv.org/abs/2403.00143</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#25104;&#30340;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#65306;&#26641;&#24179;&#22343;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensemble-Based Unsupervised Discontinuous Constituency Parsing by Tree Averaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00143
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26641;&#24179;&#22343;&#27861;&#26500;&#24314;&#38598;&#25104;&#35299;&#26512;&#22120;&#65292;&#31283;&#23450;&#24182;&#25552;&#21319;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#30340;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#25105;&#20204;&#35266;&#23519;&#21040;&#20808;&#21069;&#21807;&#19968;&#27169;&#22411;&#30340;&#24615;&#33021;&#23384;&#22312;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23545;&#29616;&#26377;&#19981;&#36830;&#32493;&#35299;&#26512;&#22120;&#30340;&#19981;&#21516;&#36816;&#34892;&#26500;&#24314;&#19968;&#20010;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#24179;&#22343;&#39044;&#27979;&#26641;&#26469;&#31283;&#23450;&#21644;&#25552;&#21319;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#19981;&#21516;&#30340;&#20108;&#20803;&#24615;&#21644;&#36830;&#32493;&#24615;&#35774;&#32622;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#26641;&#24179;&#22343;&#35745;&#31639;&#22797;&#26434;&#24230;&#20998;&#26512;&#65288;&#20197;P&#21644;NP&#23436;&#20840;&#20026;&#21333;&#20301;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#30830;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#19968;&#20219;&#21153;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#23545;&#25152;&#26377;&#26679;&#26412;&#36816;&#34892;&#26102;&#38388;&#22343;&#21512;&#29702;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#32447;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00143v1 Announce Type: cross  Abstract: We address unsupervised discontinuous constituency parsing, where we observe a high variance in the performance of the only previous model. We propose to build an ensemble of different runs of the existing discontinuous parser by averaging the predicted trees, to stabilize and boost performance. To begin with, we provide comprehensive computational complexity analysis (in terms of P and NP-complete) for tree averaging under different setups of binarity and continuity. We then develop an efficient exact algorithm to tackle the task, which runs in a reasonable time for all samples in our experiments. Results on three datasets show our method outperforms all baselines in all metrics; we also provide in-depth analyses of our approach.
&lt;/p&gt;</description></item><item><title>UNITS&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#20102;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#25104;&#21151;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.00131</link><description>&lt;p&gt;
UniTS: &#26500;&#24314;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniTS: Building a Unified Time Series Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00131
&lt;/p&gt;
&lt;p&gt;
UNITS&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#20102;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#25104;&#21151;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;LLMs&#65292;&#27491;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#25110;&#24494;&#35843;&#23558;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#35768;&#22810;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#35757;&#32451;&#35768;&#22810;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#36866;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#65292;&#20294;&#19981;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#22240;&#20026;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22266;&#26377;&#22810;&#26679;&#24615;&#21644;&#22810;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#39044;&#27979;&#12289;&#20998;&#31867;&#21644;&#20854;&#20182;&#31867;&#22411;&#20219;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#35268;&#33539;&#20998;&#27495;&#65292;&#20197;&#21450;&#23545;&#20219;&#21153;&#19987;&#29992;&#27169;&#22411;&#30340;&#26126;&#26174;&#38656;&#27714;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;UNITS&#65292;&#19968;&#31181;&#25903;&#25345;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#30340;&#32479;&#19968;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#23481;&#32435;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#30340;&#65292;&#35813;&#39592;&#24178;&#32467;&#21512;&#20102;&#24207;&#21015;&#21644;&#21464;&#37327;&#27880;&#24847;&#21147;&#20197;&#21450;&#21160;&#24577;&#32447;&#24615;&#31639;&#23376;&#65292;&#24182;&#20316;&#20026;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;38&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#65292;UNITS&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00131v1 Announce Type: cross  Abstract: Foundation models, especially LLMs, are profoundly transforming deep learning. Instead of training many task-specific models, we can adapt a single pretrained model to many tasks via fewshot prompting or fine-tuning. However, current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models. We developed UNITS, a unified time series model that supports a universal task specification, accommodating classification, forecasting, imputation, and anomaly detection tasks. This is achieved through a novel unified network backbone, which incorporates sequence and variable attention along with a dynamic linear operator and is trained as a unified model. Across 38 multi-domain datasets, UNITS demonstrate
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#20223;&#34631;&#31867;&#30340;&#39072;&#20498;&#38477;&#33853;&#34892;&#20026;&#65292;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#23567;&#22411;&#22235;&#36724;&#39134;&#34892;&#22120;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#22312;&#19981;&#21516;&#22825;&#33457;&#26495;&#25509;&#36817;&#26465;&#20214;&#19979;&#30340;&#21160;&#24577;&#20572;&#27850;&#12290;</title><link>https://arxiv.org/abs/2403.00128</link><description>&lt;p&gt;
&#20174;&#34631;&#31867;&#21040;&#26426;&#22120;&#20154;&#65306;&#23567;&#22411;&#22235;&#36724;&#39134;&#34892;&#22120;&#36827;&#34892;&#21160;&#24577;&#20572;&#27850;&#36870;&#21521;&#38477;&#33853;
&lt;/p&gt;
&lt;p&gt;
From Flies to Robots: Inverted Landing in Small Quadcopters with Dynamic Perching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00128
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#20223;&#34631;&#31867;&#30340;&#39072;&#20498;&#38477;&#33853;&#34892;&#20026;&#65292;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#23567;&#22411;&#22235;&#36724;&#39134;&#34892;&#22120;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#22312;&#19981;&#21516;&#22825;&#33457;&#26495;&#25509;&#36817;&#26465;&#20214;&#19979;&#30340;&#21160;&#24577;&#20572;&#27850;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39072;&#20498;&#38477;&#33853;&#26159;&#35768;&#22810;&#39134;&#34892;&#21160;&#29289;&#24120;&#35265;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26426;&#22120;&#39134;&#34892;&#22120;&#26469;&#35828;&#65292;&#25484;&#25569;&#36825;&#39033;&#25216;&#33021;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#35201;&#25191;&#34892;&#24555;&#36895;&#36523;&#20307;&#26059;&#36716;&#65288;&#25110;&#32763;&#36716;&#65289;&#24182;&#22312;&#37325;&#21147;&#20316;&#29992;&#19979;&#30528;&#38470;&#30340;&#21160;&#24577;&#20572;&#27850;&#12290;&#34631;&#31867;&#30340;&#39072;&#20498;&#38477;&#33853;&#34920;&#26126;&#65292;&#20809;&#27969;&#24863;&#24212;&#19982;&#31934;&#30830;&#35302;&#21457;&#21644;&#25511;&#21046;&#23548;&#33268;&#21508;&#31181;&#25104;&#21151;&#30528;&#38470;&#34892;&#20026;&#30340;&#36523;&#20307;&#32763;&#36716;&#26159;&#23494;&#20999;&#30456;&#20851;&#30340;&#12290;&#22312;&#36825;&#19968;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36866;&#29992;&#20110;&#20219;&#24847;&#22825;&#33457;&#26495;&#25509;&#36817;&#26465;&#20214;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#26469;&#22797;&#21046;&#34631;&#31867;&#30340;&#30528;&#38470;&#34892;&#20026;&#22312;&#23567;&#22411;&#22235;&#36724;&#39134;&#34892;&#22120;&#19978;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#25972;&#20010;&#20809;&#27969;&#31354;&#38388;&#30340;&#22810;&#20010;&#22825;&#33457;&#26495;&#25509;&#36817;&#36895;&#24230;&#21644;&#26041;&#21521;&#19979;&#30340;&#31163;&#25955;&#24863;&#30693;-&#36816;&#21160;&#23545;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#24863;&#30693;-&#36816;&#21160;&#23545;&#36716;&#25442;&#20026;&#22312;&#36830;&#32493;&#22686;&#24378;&#20809;&#27969;&#31354;&#38388;&#20013;&#30340;&#20004;&#38454;&#27573;&#25511;&#21046;&#31574;&#30053;&#12290;&#36825;&#20010;&#25511;&#21046;&#31574;&#30053;&#21253;&#21547;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00128v1 Announce Type: cross  Abstract: Inverted landing is a routine behavior among a number of animal fliers. However, mastering this feat poses a considerable challenge for robotic fliers, especially to perform dynamic perching with rapid body rotations (or flips) and landing against gravity. Inverted landing in flies have suggested that optical flow senses are closely linked to the precise triggering and control of body flips that lead to a variety of successful landing behaviors. Building upon this knowledge, we aimed to replicate the flies' landing behaviors in small quadcopters by developing a control policy general to arbitrary ceiling-approach conditions. First, we employed reinforcement learning in simulation to optimize discrete sensory-motor pairs across a broad spectrum of ceiling-approach velocities and directions. Next, we converted the sensory-motor pairs to a two-stage control policy in a continuous augmented-optical flow space. The control policy consists o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#26500;&#23458;&#25143;&#30340;&#32852;&#37030;&#36172;&#33218;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#19979;&#20026;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#38750;&#24179;&#20961;&#27425;&#32447;&#24615;&#21518;&#24724;&#21644;&#36890;&#20449;&#25104;&#26412;&#30340;&#20248;&#21270;</title><link>https://arxiv.org/abs/2403.00116</link><description>&lt;p&gt;
&#20855;&#26377;&#24322;&#26500;&#23458;&#25143;&#30340;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#36172;&#33218;
&lt;/p&gt;
&lt;p&gt;
Federated Linear Contextual Bandits with Heterogeneous Clients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00116
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#26500;&#23458;&#25143;&#30340;&#32852;&#37030;&#36172;&#33218;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#19979;&#20026;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#38750;&#24179;&#20961;&#27425;&#32447;&#24615;&#21518;&#24724;&#21644;&#36890;&#20449;&#25104;&#26412;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20998;&#24067;&#24335;&#31995;&#32479;&#20135;&#29983;&#30340;&#25968;&#25454;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#23545;&#36328;&#22810;&#20010;&#20195;&#29702;&#36827;&#34892;&#21327;&#21516;&#21644;&#31169;&#23494;&#30340;&#36172;&#33218;&#23398;&#20064;&#30340;&#38656;&#27714;&#27491;&#22312;&#22686;&#38271;&#12290;&#32852;&#37030;&#36172;&#33218;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31169;&#23494;&#12289;&#39640;&#25928;&#21644;&#21435;&#20013;&#24515;&#21270;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#20381;&#36182;&#20110;&#23458;&#25143;&#21516;&#36136;&#24615;&#30340;&#24378;&#20551;&#35774;&#65292;&#21363;&#25152;&#26377;&#21442;&#19982;&#23458;&#25143;&#37117;&#24212;&#20849;&#20139;&#30456;&#21516;&#30340;&#36172;&#33218;&#27169;&#22411;&#65307;&#21542;&#21017;&#65292;&#23427;&#20204;&#23558;&#36973;&#21463;&#32447;&#24615;&#21518;&#24724;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#32852;&#37030;&#36172;&#33218;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#24322;&#26500;&#23458;&#25143;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#36172;&#33218;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#19979;&#23545;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#65292;&#29992;&#20110;&#21327;&#21516;&#36172;&#33218;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#23545;&#25152;&#26377;&#23458;&#25143;&#32780;&#35328;&#30340;&#38750;&#24179;&#20961;&#27425;&#32447;&#24615;&#21518;&#24724;&#21644;&#36890;&#20449;&#25104;&#26412;&#65292;&#31526;&#21512;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#22312;&#20219;&#20309;&#26102;&#20505;&#21482;&#26377;&#19968;&#20010;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00116v1 Announce Type: cross  Abstract: The demand for collaborative and private bandit learning across multiple agents is surging due to the growing quantity of data generated from distributed systems. Federated bandit learning has emerged as a promising framework for private, efficient, and decentralized online learning. However, almost all previous works rely on strong assumptions of client homogeneity, i.e., all participating clients shall share the same bandit model; otherwise, they all would suffer linear regret. This greatly restricts the application of federated bandit learning in practice. In this work, we introduce a new approach for federated bandits for heterogeneous clients, which clusters clients for collaborative bandit learning under the federated learning setting. Our proposed algorithm achieves non-trivial sub-linear regret and communication cost for all clients, subject to the communication protocol under federated learning that at anytime only one model c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#32437;&#21521;&#25968;&#25454;&#26469;&#35780;&#20272;&#21644;&#25913;&#21892;&#21453;&#20107;&#23454;&#20013;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#27604;&#36739;&#32437;&#21521;&#21464;&#21270;&#19982;&#21453;&#20107;&#23454;&#24046;&#24322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20174;&#32780;&#29983;&#25104;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#12290;</title><link>https://arxiv.org/abs/2403.00105</link><description>&lt;p&gt;
&#32437;&#21521;&#21453;&#20107;&#23454;&#65306;&#38480;&#21046;&#19982;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Longitudinal Counterfactuals: Constraints and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#32437;&#21521;&#25968;&#25454;&#26469;&#35780;&#20272;&#21644;&#25913;&#21892;&#21453;&#20107;&#23454;&#20013;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#27604;&#36739;&#32437;&#21521;&#21464;&#21270;&#19982;&#21453;&#20107;&#23454;&#24046;&#24322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20174;&#32780;&#29983;&#25104;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#19968;&#31181;&#20026;&#25968;&#25454;&#20027;&#20307;&#25552;&#20379;&#34917;&#25937;&#25514;&#26045;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#20135;&#29983;&#25968;&#25454;&#20027;&#20307;&#26080;&#27861;&#23454;&#29616;&#30340;&#21453;&#20107;&#23454;&#65292;&#20351;&#24471;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#35777;&#26126;&#21453;&#20107;&#23454;&#36866;&#29992;&#20110;&#34917;&#25937;&#12290;&#23613;&#31649;&#22312;&#20351;&#29992;&#21453;&#20107;&#23454;&#36827;&#34892;&#31639;&#27861;&#34917;&#25937;&#26102;&#65292;&#35748;&#20026;&#21487;&#20449;&#24230;&#26159;&#19968;&#31181;&#37325;&#35201;&#21697;&#36136;&#65292;&#20294;&#22320;&#38754;&#30495;&#23454;&#21487;&#20449;&#24230;&#20173;&#28982;&#38590;&#20197;&#37327;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#32437;&#21521;&#25968;&#25454;&#26469;&#35780;&#20272;&#21644;&#25913;&#21892;&#21453;&#20107;&#23454;&#20013;&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24230;&#37327;&#26631;&#20934;&#65292;&#27604;&#36739;&#20102;&#32437;&#21521;&#24046;&#24322;&#19982;&#21453;&#20107;&#23454;&#24046;&#24322;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#35780;&#20272;&#19968;&#20010;&#21453;&#20107;&#23454;&#19982;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#21464;&#21270;&#26377;&#22810;&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#24230;&#37327;&#26631;&#20934;&#29983;&#25104;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;&#21453;&#20107;&#23454;&#36827;&#34892;&#34917;&#25937;&#30340;&#22266;&#26377;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00105v1 Announce Type: new  Abstract: Counterfactual explanations are a common approach to providing recourse to data subjects. However, current methodology can produce counterfactuals that cannot be achieved by the subject, making the use of counterfactuals for recourse difficult to justify in practice. Though there is agreement that plausibility is an important quality when using counterfactuals for algorithmic recourse, ground truth plausibility continues to be difficult to quantify. In this paper, we propose using longitudinal data to assess and improve plausibility in counterfactuals. In particular, we develop a metric that compares longitudinal differences to counterfactual differences, allowing us to evaluate how similar a counterfactual is to prior observed changes. Furthermore, we use this metric to generate plausible counterfactuals. Finally, we discuss some of the inherent difficulties of using counterfactuals for recourse.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;EDA&#24037;&#20855;&#20013;&#30340;&#20581;&#22766;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;VLSI&#24067;&#23616;&#30340;&#26032;&#39062;&#19981;&#21487;&#23519;&#35273;&#24615;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2403.00103</link><description>&lt;p&gt;
&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25317;&#22622;&#39044;&#27979;&#22120;&#30340;&#20581;&#22766;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Robustness and Generalization of ML-Based Congestion Predictors to Valid and Imperceptible Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;EDA&#24037;&#20855;&#20013;&#30340;&#20581;&#22766;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;VLSI&#24067;&#23616;&#30340;&#26032;&#39062;&#19981;&#21487;&#23519;&#35273;&#24615;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;(CAD)&#27969;&#31243;&#20013;&#65292;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#30340;&#20351;&#29992;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23545;&#20854;&#36755;&#20837;&#30340;&#24494;&#23567;&#12289;&#31934;&#24515;&#36873;&#25321;&#30340;&#25200;&#21160;&#65288;&#20363;&#22914;&#22270;&#20687;&#20013;&#30340;&#21333;&#20010;&#20687;&#32032;&#26356;&#25913;&#65289;&#24456;&#23481;&#26131;&#21463;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;ML-based EDA&#24037;&#20855;&#30340;&#20581;&#22766;&#24615;&#38382;&#39064;--&#23588;&#20854;&#26159;&#23545;&#25317;&#22622;&#39044;&#27979;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;ML-based EDA&#32972;&#26223;&#19979;&#25506;&#32034;&#36825;&#20010;&#27010;&#24565;&#30340;&#22242;&#38431;&#12290;&#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;&#19968;&#20010;&#38024;&#23545;VLSI&#24067;&#23616;&#38382;&#39064;&#29305;&#21035;&#35774;&#35745;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#27010;&#24565;&#65292;&#35813;&#38382;&#39064;&#23450;&#20041;&#22312;&#20803;&#20214;&#21015;&#34920;&#21644;&#21333;&#20803;&#25918;&#32622;&#19978;&#12290;&#25105;&#20204;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#23450;&#20041;&#30340;&#29305;&#28857;&#26159;&#20445;&#35777;&#24067;&#23616;&#30340;&#25200;&#21160;&#19981;&#20250;&#25913;&#21464;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00103v1 Announce Type: new  Abstract: There is substantial interest in the use of machine learning (ML)-based techniques throughout the electronic computer-aided design (CAD) flow, particularly methods based on deep learning. However, while deep learning methods have achieved state-of-the-art performance in several applications, recent work has demonstrated that neural networks are generally vulnerable to small, carefully chosen perturbations of their input (e.g. a single pixel change in an image). In this work, we investigate robustness in the context of ML-based EDA tools -- particularly for congestion prediction. As far as we are aware, we are the first to explore this concept in the context of ML-based EDA.   We first describe a novel notion of imperceptibility designed specifically for VLSI layout problems defined on netlists and cell placements. Our definition of imperceptibility is characterized by a guarantee that a perturbation to a layout will not alter its global 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;Dirichlet&#20808;&#39564;&#35268;&#33539;&#21644;Dirichlet&#39532;&#23572;&#21487;&#22827;&#38142;&#26500;&#24314;&#65292;&#25193;&#23637;&#20102;&#36793;&#32536;&#21010;&#20998;&#27169;&#22411;&#65288;EPM&#65289;&#20197;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;Gibbs&#37319;&#26679;&#22120;&#26469;&#22788;&#29702;&#21518;&#39564;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.00044</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;MCMC&#25193;&#23637;&#21160;&#24577;&#36793;&#32536;&#21010;&#20998;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling up Dynamic Edge Partition Models via Stochastic Gradient MCMC
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;Dirichlet&#20808;&#39564;&#35268;&#33539;&#21644;Dirichlet&#39532;&#23572;&#21487;&#22827;&#38142;&#26500;&#24314;&#65292;&#25193;&#23637;&#20102;&#36793;&#32536;&#21010;&#20998;&#27169;&#22411;&#65288;EPM&#65289;&#20197;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;Gibbs&#37319;&#26679;&#22120;&#26469;&#22788;&#29702;&#21518;&#39564;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00044v1&#23459;&#24067;&#31867;&#22411;:cross &#25688;&#35201;: &#36793;&#32536;&#21010;&#20998;&#27169;&#22411;(EPM)&#26159;&#19968;&#31181;&#20174;&#38745;&#24577;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#25552;&#21462;&#37325;&#21472;&#31038;&#21306;&#32467;&#26500;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290; &#22312;EPM&#20013;&#65292;&#37319;&#29992;Gamma&#36807;&#31243;&#65288;GaP&#65289;&#20808;&#39564;&#26469;&#25512;&#26029;&#21512;&#36866;&#30340;&#28508;&#22312;&#31038;&#21306;&#25968;&#37327;&#65292;&#24182;&#19988;&#27599;&#20010;&#39030;&#28857;&#34987;&#36171;&#20104;&#19968;&#20010;Gamma&#20998;&#24067;&#30340;&#27491;&#25104;&#21592;&#21521;&#37327;&#12290; &#23613;&#31649;&#20855;&#26377;&#35768;&#22810;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#65292;EPM&#20013;&#30340;&#25512;&#29702;&#36890;&#24120;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#26041;&#27861;&#25191;&#34892;&#65292;&#36825;&#38459;&#27490;&#20102;&#20854;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;Dirichlet&#20808;&#39564;&#35268;&#33539;&#34920;&#31034;&#27599;&#20010;&#39030;&#28857;&#30340;&#27491;&#25104;&#21592;&#21521;&#37327;&#26469;&#23558;EPM&#27867;&#21270;&#20197;&#32771;&#34385;&#21160;&#24577;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;Dirichlet&#39532;&#23572;&#21487;&#22827;&#38142;&#26500;&#36896;&#25429;&#33719;&#39030;&#28857;&#30340;&#26102;&#38388;&#28436;&#21464;&#34892;&#20026;&#12290; &#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#23454;&#26045;&#30340;Gibbs&#37319;&#26679;&#22120;&#65292;&#20351;&#29992;&#36127;&#20108;&#39033;&#22686;&#24378;&#25216;&#26415;&#25191;&#34892;&#21518;&#39564;&#35745;&#31639;&#12290; &#23545;&#20110;&#22823;&#32593;&#32476;&#36164;&#26009;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00044v1 Announce Type: cross  Abstract: The edge partition model (EPM) is a generative model for extracting an overlapping community structure from static graph-structured data. In the EPM, the gamma process (GaP) prior is adopted to infer the appropriate number of latent communities, and each vertex is endowed with a gamma distributed positive memberships vector. Despite having many attractive properties, inference in the EPM is typically performed using Markov chain Monte Carlo (MCMC) methods that prevent it from being applied to massive network data. In this paper, we generalize the EPM to account for dynamic enviroment by representing each vertex with a positive memberships vector constructed using Dirichlet prior specification, and capturing the time-evolving behaviour of vertices via a Dirichlet Markov chain construction. A simple-to-implement Gibbs sampler is proposed to perform posterior computation using Negative- Binomial augmentation technique. For large network d
&lt;/p&gt;</description></item><item><title>RiNALMo&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;RNA&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;650&#20159;&#20010;&#21442;&#25968;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.00043</link><description>&lt;p&gt;
RiNALMo: &#36890;&#29992;RNA&#35821;&#35328;&#27169;&#22411;&#22312;&#32467;&#26500;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
RiNALMo: General-Purpose RNA Language Models Can Generalize Well on Structure Prediction Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00043
&lt;/p&gt;
&lt;p&gt;
RiNALMo&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;RNA&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;650&#20159;&#20010;&#21442;&#25968;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00043v1 &#36890;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#26680;&#31958;&#26680;&#37240;&#65288;RNA&#65289;&#22312;&#22522;&#30784;&#29983;&#29289;&#36807;&#31243;&#20013;&#25198;&#28436;&#30528;&#21508;&#31181;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#26368;&#36817;&#65292;RNA&#24050;&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#33647;&#29289;&#38774;&#28857;&#65292;&#24378;&#35843;&#20102;&#25552;&#39640;&#25105;&#20204;&#23545;&#20854;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#29702;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;&#22810;&#24180;&#26469;&#65292;&#27979;&#24207;&#25216;&#26415;&#24050;&#20135;&#29983;&#20102;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;RNA&#25968;&#25454;&#65292;&#20854;&#20013;&#38544;&#34255;&#30528;&#37325;&#35201;&#30340;&#30693;&#35782;&#21644;&#28508;&#21147;&#12290;&#21463;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#31958;&#26680;&#37240;&#35821;&#35328;&#27169;&#22411;&#65288;RiNALMo&#65289;&#20197;&#24110;&#21161;&#25581;&#31034;RNA&#30340;&#38544;&#34255;&#23494;&#30721;&#12290;RiNALMo&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;RNA&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;650&#20159;&#20010;&#21442;&#25968;&#65292;&#39044;&#20808;&#35757;&#32451;&#20102;&#26469;&#33258;&#20960;&#20010;&#21487;&#29992;&#25968;&#25454;&#24211;&#30340;3600&#19975;&#20010;&#38750;&#32534;&#30721;RNA&#24207;&#21015;&#12290;RiNALMo&#33021;&#22815;&#25552;&#21462;&#38544;&#34255;&#30693;&#35782;&#24182;&#38544;&#21547;&#22320;&#25429;&#25417;RNA&#24207;&#21015;&#20013;&#20869;&#23884;&#30340;&#22522;&#26412;&#32467;&#26500;&#20449;&#24687;&#12290;RiNALMo&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00043v1 Announce Type: cross  Abstract: Ribonucleic acid (RNA) plays a variety of crucial roles in fundamental biological processes. Recently, RNA has become an interesting drug target, emphasizing the need to improve our understanding of its structures and functions. Over the years, sequencing technologies have produced an enormous amount of unlabeled RNA data, which hides important knowledge and potential. Motivated by the successes of protein language models, we introduce RiboNucleic Acid Language Model (RiNALMo) to help unveil the hidden code of RNA. RiNALMo is the largest RNA language model to date with $650$ million parameters pre-trained on $36$ million non-coding RNA sequences from several available databases. RiNALMo is able to extract hidden knowledge and capture the underlying structure information implicitly embedded within the RNA sequences. RiNALMo achieves state-of-the-art results on several downstream tasks. Notably, we show that its generalization capabiliti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32852;&#37030;&#25552;&#31034;&#21512;&#20316; via Optimal Transport&#65288;FedOTP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#20840;&#23616;&#21644;&#26412;&#22320;&#25552;&#31034;&#30340;&#21512;&#20316;&#65292;&#38024;&#23545;&#25968;&#25454;&#24322;&#36136;&#24615;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#21327;&#20316;&#25552;&#31034;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.00041</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#20840;&#23616;&#21644;&#26412;&#22320;&#25552;&#31034;&#30340;&#21512;&#20316;&#65292;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Global and Local Prompts Cooperation via Optimal Transport for Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32852;&#37030;&#25552;&#31034;&#21512;&#20316; via Optimal Transport&#65288;FedOTP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#20840;&#23616;&#21644;&#26412;&#22320;&#25552;&#31034;&#30340;&#21512;&#20316;&#65292;&#38024;&#23545;&#25968;&#25454;&#24322;&#36136;&#24615;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#21327;&#20316;&#25552;&#31034;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;&#23398;&#20064;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#28789;&#27963;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#23558;&#36825;&#31181;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#25972;&#21512;&#21040;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#24182;&#20419;&#36827;&#23545;&#25968;&#25454;&#19981;&#36275;&#30340;&#23616;&#37096;&#35757;&#32451;&#12290;&#20026;&#20102;&#24212;&#23545;&#24403;&#21069;&#32852;&#37030;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#31995;&#32479;&#21270;&#35299;&#20915;&#20005;&#37325;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#21363;&#28041;&#21450;&#26631;&#31614;&#21644;&#29305;&#24449;&#36716;&#31227;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#32852;&#37030;&#25552;&#31034;&#21512;&#20316;&#65288;FedOTP&#65289;&#65292;&#23427;&#24341;&#20837;&#20102;&#39640;&#25928;&#30340;&#21327;&#20316;&#25552;&#31034;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#22522;&#30784;&#19978;&#25429;&#25417;&#19981;&#21516;&#30340;&#31867;&#21035;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#25552;&#31034;&#26469;&#25552;&#21462;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20849;&#35782;&#30693;&#35782;&#65292;&#36824;&#23398;&#20064;&#19968;&#20010;&#26412;&#22320;&#25552;&#31034;&#26469;&#25429;&#33719;&#29305;&#23450;&#23458;&#25143;&#31471;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00041v1 Announce Type: cross  Abstract: Prompt learning in pretrained visual-language models has shown remarkable flexibility across various downstream tasks. Leveraging its inherent lightweight nature, recent research attempted to integrate the powerful pretrained models into federated learning frameworks to simultaneously reduce communication costs and promote local training on insufficient data. Despite these efforts, current federated prompt learning methods lack specialized designs to systematically address severe data heterogeneities, e.g., data distribution with both label and feature shifts involved. To address this challenge, we present Federated Prompts Cooperation via Optimal Transport (FedOTP), which introduces efficient collaborative prompt learning strategies to capture diverse category traits on a per-client basis. Specifically, for each client, we learn a global prompt to extract consensus knowledge among clients, and a local prompt to capture client-specific
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32771;&#34385;&#20102;&#22312;&#38750;&#38745;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#36890;&#36807;&#35266;&#23519;&#22870;&#21169;&#26469;&#31215;&#26497;&#21644;&#28040;&#26497;&#22320;&#24378;&#21270;&#20154;&#32676;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#26368;&#22823;&#21270;&#25903;&#25345;&#39044;&#23450;&#25163;&#33218;&#30340;&#20154;&#21475;&#27604;&#20363;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#19981;&#21516;&#24847;&#35265;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#24182;&#20998;&#26512;&#20102;&#21518;&#24724;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;&#22810;&#20010;&#25512;&#33616;&#31995;&#32479;&#20849;&#23384;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.00036</link><description>&lt;p&gt;
&#24433;&#21709;Bandits&#65306;&#29992;&#20110;&#24418;&#22609;&#20559;&#22909;&#30340;&#25163;&#33218;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Influencing Bandits: Arm Selection for Preference Shaping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32771;&#34385;&#20102;&#22312;&#38750;&#38745;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#36890;&#36807;&#35266;&#23519;&#22870;&#21169;&#26469;&#31215;&#26497;&#21644;&#28040;&#26497;&#22320;&#24378;&#21270;&#20154;&#32676;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#26368;&#22823;&#21270;&#25903;&#25345;&#39044;&#23450;&#25163;&#33218;&#30340;&#20154;&#21475;&#27604;&#20363;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#19981;&#21516;&#24847;&#35265;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#24182;&#20998;&#26512;&#20102;&#21518;&#24724;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;&#22810;&#20010;&#25512;&#33616;&#31995;&#32479;&#20849;&#23384;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#38750;&#38745;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#65292;&#22312;&#36825;&#20854;&#20013;&#20154;&#32676;&#30340;&#20559;&#22909;&#21463;&#21040;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#30340;&#31215;&#26497;&#21644;&#28040;&#26497;&#24378;&#21270;&#12290;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#22609;&#36896;&#20154;&#32676;&#30340;&#20559;&#22909;&#65292;&#20197;&#26368;&#22823;&#21270;&#25903;&#25345;&#39044;&#23450;&#25163;&#33218;&#30340;&#20154;&#21475;&#27604;&#20363;&#12290;&#23545;&#20110;&#20108;&#20803;&#24847;&#35265;&#30340;&#24773;&#20917;&#65292;&#32771;&#34385;&#20102;&#20004;&#31181;&#24847;&#35265;&#21160;&#24577; -- &#36882;&#20943;&#24377;&#24615;&#65288;&#24314;&#27169;&#20026;&#20855;&#26377;&#22686;&#21152;&#29699;&#25968;&#30340;Polya&#37319;&#26679;&#65289;&#21644;&#24120;&#37327;&#24377;&#24615;&#65288;&#20351;&#29992;&#25237;&#31080;&#32773;&#27169;&#22411;&#65289;&#12290;&#23545;&#20110;&#31532;&#19968;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#25506;&#32034;-&#28982;&#21518;-&#25215;&#35834;&#31574;&#30053;&#21644;&#19968;&#31181;Thompson&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#20998;&#26512;&#20102;&#27599;&#31181;&#31574;&#30053;&#30340;&#21518;&#24724;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#21450;&#20854;&#20998;&#26512;&#21487;&#25512;&#24191;&#21040;&#24120;&#24377;&#24615;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;Thompson&#37319;&#26679;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#24403;&#23384;&#22312;&#20004;&#31181;&#20197;&#19978;&#31867;&#22411;&#30340;&#24847;&#35265;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23384;&#22312;&#22810;&#20010;&#25512;&#33616;&#31995;&#32479;&#30340;&#24773;&#20917;&#24341;&#21457;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00036v1 Announce Type: cross  Abstract: We consider a non stationary multi-armed bandit in which the population preferences are positively and negatively reinforced by the observed rewards. The objective of the algorithm is to shape the population preferences to maximize the fraction of the population favouring a predetermined arm. For the case of binary opinions, two types of opinion dynamics are considered -- decreasing elasticity (modeled as a Polya urn with increasing number of balls) and constant elasticity (using the voter model). For the first case, we describe an Explore-then-commit policy and a Thompson sampling policy and analyse the regret for each of these policies. We then show that these algorithms and their analyses carry over to the constant elasticity case. We also describe a Thompson sampling based algorithm for the case when more than two types of opinions are present. Finally, we discuss the case where presence of multiple recommendation systems gives ris
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#21160;&#24577;&#20869;&#22312;&#21151;&#33021;&#32593;&#32476;&#21644;LSTM&#25216;&#26415;&#65292;&#20351;&#29992;&#39640;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#21644;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;HOGAB&#27169;&#22411;&#65292;&#23545;&#24930;&#24615;&#22823;&#40635;&#29992;&#25143;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#22810;&#22270;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00033</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#38454;&#27880;&#24847;&#21147;&#22823;&#33041;&#32593;&#32476;&#20998;&#26512;&#22823;&#40635;&#20351;&#29992;&#32773;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Analyzing Resting-State fMRI Data in Marijuana Users via High-Order Attention Brain Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00033
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#21160;&#24577;&#20869;&#22312;&#21151;&#33021;&#32593;&#32476;&#21644;LSTM&#25216;&#26415;&#65292;&#20351;&#29992;&#39640;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#21644;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;HOGAB&#27169;&#22411;&#65292;&#23545;&#24930;&#24615;&#22823;&#40635;&#29992;&#25143;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#22810;&#22270;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#40635;&#30340;&#25345;&#32493;&#20351;&#29992;&#26126;&#26174;&#24433;&#21709;&#20154;&#20204;&#30340;&#29983;&#27963;&#21644;&#20581;&#24247;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;HOGAB&#65288;High-Order Attention Graph Attention&#31070;&#32463;&#32593;&#32476;&#65289;&#27169;&#22411;&#65292;&#20197;&#20998;&#26512;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#24930;&#24615;&#22823;&#40635;&#29992;&#25143;&#30340;&#23616;&#37096;&#24322;&#24120;&#33041;&#27963;&#21160;&#12290;HOGAB&#23558;&#21160;&#24577;&#20869;&#22312;&#21151;&#33021;&#32593;&#32476;&#19982;LSTM&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#25429;&#25417;&#22823;&#40635;&#29992;&#25143;fMRI&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#23545;&#37051;&#22495;&#33410;&#28857;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#21644;&#28040;&#24687;&#20256;&#36882;&#65292;&#22686;&#24378;&#38271;&#26399;&#22823;&#40635;&#29992;&#25143;&#30340;&#31038;&#21306;&#32858;&#31867;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#34701;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25972;&#20307;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#22810;&#22270;&#20998;&#31867;&#20013;&#23454;&#29616;&#20102;85.1%&#30340;AUC&#21644;80.7%&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#32447;&#24615;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;HODAB&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00033v1 Announce Type: cross  Abstract: The sustained use of marijuana significantly impacts the lives and health of people. In this study, we propose an interpretable novel framework called the HOGAB (High-Order Attention Graph Attention Neural Networks) model to analyze local abnormal brain activity in chronic marijuana users in two datasets. The HOGAB integrates dynamic intrinsic functional networks with LSTM technology to capture temporal patterns in fMRI time series of marijuana users. Moreover, we use the high-order attention module in neighborhood nodes for information fusion and message passing, enhancing community clustering analysis for long-term marijuana users. Furthermore, we improve the overall learning ability of the model by incorporating attention mechanisms, achieving an AUC of 85.1% and an accuracy of 80.7% in multigraph classification. In addition, we compare linear machine learning methods and evaluate the effectiveness of our proposed HODAB model. Speci
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphPub&#30340;&#26032;&#22411;&#22270;&#36793;&#20445;&#25252;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#21521;&#23398;&#20064;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26426;&#21046;&#65292;&#22312;&#20445;&#25252;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#21516;&#26102;&#20445;&#35777;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#22522;&#26412;&#19981;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.00030</link><description>&lt;p&gt;
GraphPub: &#20855;&#26377;&#39640;&#21487;&#29992;&#24615;&#30340;&#24046;&#20998;&#38544;&#31169;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
GraphPub: Generation of Differential Privacy Graph with High Availability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphPub&#30340;&#26032;&#22411;&#22270;&#36793;&#20445;&#25252;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#21521;&#23398;&#20064;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26426;&#21046;&#65292;&#22312;&#20445;&#25252;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#21516;&#26102;&#20445;&#35777;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#22522;&#26412;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#22270;&#25968;&#25454;&#38598;&#34987;&#29992;&#20110;GNN&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#19978;&#28216;&#25968;&#25454;&#25152;&#26377;&#32773;&#21457;&#24067;&#22270;&#25968;&#25454;&#26102;&#65292;&#24448;&#24448;&#20250;&#23384;&#22312;&#35768;&#22810;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#21253;&#21547;&#20687;&#20010;&#20154;&#30340;&#26379;&#21451;&#21015;&#34920;&#31561;&#25935;&#24863;&#20449;&#24687;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22270;&#25968;&#25454;&#30340;&#22797;&#26434;&#25299;&#25169;&#32467;&#26500;&#65292;&#23558;DP&#24212;&#29992;&#22312;&#22270;&#19978;&#24448;&#24448;&#20250;&#24433;&#21709;GNN&#27169;&#22411;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#32858;&#21512;&#65292;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#36793;&#20445;&#25252;&#26694;&#26550;GraphPub&#65292;&#21487;&#20197;&#20445;&#25252;&#22270;&#25299;&#25169;&#32467;&#26500;&#21516;&#26102;&#30830;&#20445;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#22522;&#26412;&#19981;&#21464;&#12290;&#36890;&#36807;&#21453;&#21521;&#23398;&#20064;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26426;&#21046;&#65292;&#25105;&#20204;&#25628;&#32034;&#19968;&#20123;&#23545;&#33410;&#28857;&#29305;&#24449;&#32858;&#21512;&#27809;&#26377;&#22826;&#22823;&#36127;&#38754;&#24433;&#21709;&#30340;&#34394;&#20551;&#36793;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00030v1 Announce Type: cross  Abstract: In recent years, with the rapid development of graph neural networks (GNN), more and more graph datasets have been published for GNN tasks. However, when an upstream data owner publishes graph data, there are often many privacy concerns, because many real-world graph data contain sensitive information like person's friend list. Differential privacy (DP) is a common method to protect privacy, but due to the complex topological structure of graph data, applying DP on graphs often affects the message passing and aggregation of GNN models, leading to a decrease in model accuracy. In this paper, we propose a novel graph edge protection framework, graph publisher (GraphPub), which can protect graph topology while ensuring that the availability of data is basically unchanged. Through reverse learning and the encoder-decoder mechanism, we search for some false edges that do not have a large negative impact on the aggregation of node features, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#22312;&#25345;&#32493;&#35266;&#27979;&#21644;&#22312;&#32447;&#38408;&#20540;&#26597;&#35810;&#24773;&#24418;&#19979;&#20851;&#20110;&#26102;&#38388;&#27493;&#38271;&#21644;&#20107;&#20214;&#25968;&#37327;&#30340;&#26032;&#19979;&#30028;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.00028</link><description>&lt;p&gt;
&#22312;&#25345;&#32493;&#35266;&#27979;&#21644;&#22312;&#32447;&#38408;&#20540;&#26597;&#35810;&#19979;&#30340;&#24046;&#20998;&#38544;&#31169;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Lower Bounds for Differential Privacy Under Continual Observation and Online Threshold Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#22312;&#25345;&#32493;&#35266;&#27979;&#21644;&#22312;&#32447;&#38408;&#20540;&#26597;&#35810;&#24773;&#24418;&#19979;&#20851;&#20110;&#26102;&#38388;&#27493;&#38271;&#21644;&#20107;&#20214;&#25968;&#37327;&#30340;&#26032;&#19979;&#30028;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#38544;&#30610;&#27599;&#19968;&#20107;&#20214;&#30340;&#23384;&#22312;&#30340;&#21516;&#26102;&#65292;&#36319;&#36394;&#38543;&#26102;&#38388;&#21457;&#29983;&#30340;&#20107;&#20214;&#25968;&#37327;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;t&#8712;[T]&#20013;&#65292;&#25105;&#20204;&#22312;&#32447;&#23398;&#20064;&#21040;&#916;t&#8805;0&#20010;&#26032;&#20107;&#20214;&#24050;&#21457;&#29983;&#65292;&#24182;&#19988;&#24517;&#39035;&#22238;&#24212;&#19968;&#20010;&#20272;&#35745;&#20540;nt&#8776;&#8721;_{j=1}^t &#916;j&#12290;&#38544;&#31169;&#35201;&#27714;&#26159;&#65292;&#25152;&#26377;&#36755;&#20986;&#22312;&#25152;&#26377;&#26102;&#38388;&#27493;&#38271;&#19978;&#19968;&#36215;&#28385;&#36275;&#20107;&#20214;&#32423;&#21035;&#30340;&#24046;&#20998;&#38544;&#31169;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#25105;&#20204;&#30340;&#35823;&#24046;&#22914;&#20309;&#20381;&#36182;&#20110;&#24635;&#26102;&#38388;&#27493;&#38271;T&#21644;&#24635;&#20107;&#20214;&#25968;&#37327;n&#12290;Dwork&#31561;&#20154;&#65288;2015&#65289;&#23637;&#31034;&#20102;O(log(T)+log^2(n))&#30340;&#19978;&#30028;&#65292;&#32780;Henzinger&#31561;&#20154;&#65288;2023&#65289;&#23637;&#31034;&#20102;&#937;(min{log n, log T})&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#26032;&#30340;&#19979;&#30028;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00028v1 Announce Type: cross  Abstract: One of the most basic problems for studying the "price of privacy over time" is the so called private counter problem, introduced by Dwork et al. (2010) and Chan et al. (2010). In this problem, we aim to track the number of events that occur over time, while hiding the existence of every single event. More specifically, in every time step $t\in[T]$ we learn (in an online fashion) that $\Delta_t\geq 0$ new events have occurred, and must respond with an estimate $n_t\approx\sum_{j=1}^t \Delta_j$. The privacy requirement is that all of the outputs together, across all time steps, satisfy event level differential privacy. The main question here is how our error needs to depend on the total number of time steps $T$ and the total number of events $n$. Dwork et al. (2015) showed an upper bound of $O\left(\log(T)+\log^2(n)\right)$, and Henzinger et al. (2023) showed a lower bound of $\Omega\left(\min\{\log n, \log T\}\right)$. We show a new lo
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24341;&#20837;&#20102;Most Destruction Attack&#65288;MDA&#65289;&#27010;&#24565;&#65292;&#29992;&#20110;&#35780;&#20272;&#32593;&#32476;&#30340;&#26368;&#24046;&#31283;&#20581;&#24615;&#65292;&#20197;&#30830;&#23450;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#31995;&#32479;&#30340;&#26497;&#38480;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00027</link><description>&lt;p&gt;
&#35780;&#20272;&#22797;&#26434;&#32593;&#32476;&#26368;&#24046;&#31283;&#20581;&#24615;&#30340;&#24555;&#36895;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Quick Framework for Evaluating Worst Robustness of Complex Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00027
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24341;&#20837;&#20102;Most Destruction Attack&#65288;MDA&#65289;&#27010;&#24565;&#65292;&#29992;&#20110;&#35780;&#20272;&#32593;&#32476;&#30340;&#26368;&#24046;&#31283;&#20581;&#24615;&#65292;&#20197;&#30830;&#23450;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#31995;&#32479;&#30340;&#26497;&#38480;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#23545;&#20110;&#29702;&#35299;&#12289;&#35774;&#35745;&#12289;&#20248;&#21270;&#21644;&#20462;&#22797;&#32593;&#32476;&#33267;&#20851;&#37325;&#35201;&#65292;&#27169;&#25311;&#25915;&#20987;&#19968;&#30452;&#26159;&#20027;&#27969;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#25915;&#20987;&#24448;&#24448;&#32791;&#26102;&#29978;&#33267;&#19981;&#20999;&#23454;&#38469;&#65292;&#26356;&#20026;&#20851;&#38190;&#19988;&#25345;&#32493;&#34987;&#24573;&#35270;&#30340;&#32570;&#28857;&#26159;&#20219;&#20309;&#25915;&#20987;&#31574;&#30053;&#20165;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#29926;&#35299;&#33539;&#24335;&#12290;&#20851;&#38190;&#38382;&#39064;&#26159;&#65306;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#25110;&#38754;&#20020;&#26368;&#20005;&#37325;&#30340;&#25915;&#20987;&#26102;&#65292;&#23545;&#20110;&#32473;&#23450;&#31995;&#32479;&#65292;&#8220;&#26368;&#24046;&#31283;&#20581;&#24615;&#8221;&#65292;&#21363;&#25152;&#35859;&#30340;&#26497;&#38480;&#40065;&#26834;&#24615;&#26159;&#22810;&#23569;&#65311;&#20102;&#35299;&#31995;&#32479;&#30340;&#26368;&#24046;&#31283;&#20581;&#24615;&#23545;&#20110;&#25226;&#25569;&#20854;&#21487;&#38752;&#24615;&#26497;&#38480;&#12289;&#20934;&#30830;&#35780;&#20272;&#20445;&#25252;&#33021;&#21147;&#20197;&#21450;&#30830;&#23450;&#30456;&#20851;&#35774;&#35745;&#21644;&#23433;&#20840;&#32500;&#25252;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22534;&#21472;&#24605;&#24819;&#30340;&#8220;&#26368;&#22823;&#30772;&#22351;&#25915;&#20987;&#8221;&#65288;MDA&#65289;&#30340;&#27010;&#24565;&#12290;MDA&#29992;&#20110;&#35780;&#20272;&#32593;&#32476;&#30340;&#26368;&#24046;&#31283;&#20581;&#24615;&#65292;&#38543;&#21518;&#36827;&#34892;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00027v1 Announce Type: cross  Abstract: Robustness is pivotal for comprehending, designing, optimizing, and rehabilitating networks, with simulation attacks being the prevailing evaluation method. Simulation attacks are often time-consuming or even impractical, however, a more crucial yet persistently overlooked drawback is that any attack strategy merely provides a potential paradigm of disintegration. The key concern is: in the worst-case scenario or facing the most severe attacks, what is the limit of robustness, referred to as ``Worst Robustness'', for a given system? Understanding a system's worst robustness is imperative for grasping its reliability limits, accurately evaluating protective capabilities, and determining associated design and security maintenance costs. To address these challenges, we introduce the concept of Most Destruction Attack (MDA) based on the idea of knowledge stacking. MDA is employed to assess the worst robustness of networks, followed by the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33945;&#29305;&#21033;&#23572;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#22522;&#30784;&#27169;&#22411;&#65288;FM-MCVRP&#65289;&#65292;&#23558;MCVRP&#35270;&#20026;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;Transformer&#26550;&#26500;&#23884;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.00026</link><description>&lt;p&gt;
&#23398;&#20064;&#20132;&#20184;&#65306;&#33945;&#29305;&#21033;&#23572;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Deliver: a Foundation Model for the Montreal Capacitated Vehicle Routing Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00026
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33945;&#29305;&#21033;&#23572;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#22522;&#30784;&#27169;&#22411;&#65288;FM-MCVRP&#65289;&#65292;&#23558;MCVRP&#35270;&#20026;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;Transformer&#26550;&#26500;&#23884;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33945;&#29305;&#21033;&#23572;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;MCVRP&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#65288;FM-MCVRP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#36817;&#20284;&#35299;&#20915;&#19968;&#20010;&#21464;&#20307;&#30340;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;CVRP&#65289;&#65292;&#35813;&#38382;&#39064;&#25551;&#36848;&#20102;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;MCVRP&#39318;&#27425;&#30001;Bengio&#31561;&#20154;&#65288;2021&#65289;&#27491;&#24335;&#25551;&#36848;&#65292;&#23450;&#20041;&#22312;&#19968;&#20010;&#22266;&#23450;&#26377;&#38480;&#30340;&#22270;&#19978;&#65292;&#31867;&#20284;&#20110;&#19968;&#20010;&#22478;&#24066;&#12290;&#27599;&#20010;MCVRP&#23454;&#20363;&#26412;&#36136;&#19978;&#26159;&#36830;&#25509;&#22266;&#23450;&#22270;&#20013;&#38543;&#26426;&#25277;&#26679;&#33410;&#28857;&#30340;&#23376;&#22270;&#65292;&#36825;&#20123;&#33410;&#28857;&#20195;&#34920;&#32473;&#23450;&#26085;&#26399;&#23454;&#38469;&#20132;&#20184;&#38382;&#39064;&#20013;&#28508;&#22312;&#22320;&#22336;&#38598;&#21512;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#38382;&#39064;&#32467;&#26500;&#65292;&#23558;MCVRP&#26500;&#24314;&#25104;&#31867;&#20284;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;Transformer&#26550;&#26500;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26694;&#26550;&#20013;&#65292;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00026v1 Announce Type: cross  Abstract: In this paper, we present the Foundation Model for the Montreal Capacitated Vehicle Routing Problem (FM-MCVRP), a novel Deep Learning (DL) model that approximates high-quality solutions to a variant of the Capacitated Vehicle Routing Problem (CVRP) that characterizes many real-world applications. The so-called Montreal Capacitated Vehicle Routing Problem (MCVRP), first formally described by Bengio et al. (2021), is defined on a fixed and finite graph, which is analogous to a city. Each MCVRP instance is essentially the sub-graph connecting a randomly sampled subset of the nodes in the fixed graph, which represent a set of potential addresses in a real-world delivery problem on a given day. Our work exploits this problem structure to frame the MCVRP as an analogous Natural Language Processing (NLP) task. Specifically, we leverage a Transformer architecture embedded in a Large Language Model (LLM) framework to train our model in a superv
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#23384;&#22312;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23558;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20026;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.00025</link><description>&lt;p&gt;
&#20851;&#20110;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
On the Challenges and Opportunities in Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00025
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#23384;&#22312;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23558;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20026;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#36817;&#24180;&#26469;&#22686;&#38271;&#36805;&#36895;&#32780;&#31283;&#23450;&#12290;&#38543;&#30528;&#28023;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#30340;&#36827;&#27493;&#65292;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#23637;&#29616;&#20986;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#25991;&#26412;&#20197;&#21450;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#35270;&#39057;&#21644;&#20998;&#23376;&#65289;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#22823;&#35268;&#27169;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#33509;&#24178;&#22522;&#26412;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#30830;&#23450;&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#30340;&#20851;&#38190;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35782;&#21035;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#25506;&#32034;&#26377;&#30410;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#21152;&#24378;&#22823;&#21644;&#21487;&#35775;&#38382;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00025v1 Announce Type: cross  Abstract: The field of deep generative modeling has grown rapidly and consistently over the years. With the availability of massive amounts of training data coupled with advances in scalable unsupervised learning paradigms, recent large-scale generative models show tremendous promise in synthesizing high-resolution images and text, as well as structured data such as videos and molecules. However, we argue that current large-scale generative AI models do not sufficiently address several fundamental issues that hinder their widespread adoption across domains. In this work, we aim to identify key unresolved challenges in modern generative AI paradigms that should be tackled to further enhance their capabilities, versatility, and reliability. By identifying these challenges, we aim to provide researchers with valuable insights for exploring fruitful research directions, thereby fostering the development of more robust and accessible generative AI so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FlowCyt&#39033;&#30446;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#27969;&#24335;&#32454;&#32990;&#26415;&#25968;&#25454;&#20013;&#22810;&#31867;&#21035;&#21333;&#32454;&#32990;&#20998;&#31867;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#30721;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20851;&#31995;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00024</link><description>&lt;p&gt;
FlowCyt: &#27969;&#24335;&#32454;&#32990;&#26415;&#20013;&#22810;&#31867;&#21035;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
FlowCyt: A Comparative Study of Deep Learning Approaches for Multi-Class Classification in Flow Cytometry Benchmarking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FlowCyt&#39033;&#30446;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#27969;&#24335;&#32454;&#32990;&#26415;&#25968;&#25454;&#20013;&#22810;&#31867;&#21035;&#21333;&#32454;&#32990;&#20998;&#31867;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#30721;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20851;&#31995;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FlowCyt&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#27969;&#24335;&#32454;&#32990;&#26415;&#25968;&#25454;&#20013;&#22810;&#31867;&#21035;&#21333;&#32454;&#32990;&#20998;&#31867;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;30&#21517;&#24739;&#32773;&#30340;&#39592;&#39635;&#26679;&#26412;&#65292;&#27599;&#20010;&#32454;&#32990;&#30001;&#21313;&#20108;&#20010;&#26631;&#35760;&#29305;&#24449;&#12290;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#35782;&#21035;&#20102;&#20116;&#31181;&#34880;&#28082;&#32454;&#32990;&#31867;&#22411;&#65306;T&#28107;&#24052;&#32454;&#32990;&#12289;B&#28107;&#24052;&#32454;&#32990;&#12289;&#21333;&#26680;&#32454;&#32990;&#12289;&#32933;&#22823;&#32454;&#32990;&#21644;&#36896;&#34880;&#24178;/&#31062;&#32454;&#32990;&#65288;HSPCs&#65289;&#12290;&#23454;&#39564;&#21033;&#29992;&#20102;&#26377;&#30417;&#30563;&#30340;&#24402;&#32435;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#30340;&#36716;&#23548;&#23398;&#20064;&#65292;&#27599;&#21517;&#24739;&#32773;&#26368;&#22810;&#20351;&#29992;&#20102;100&#19975;&#20010;&#32454;&#32990;&#12290;&#22522;&#32447;&#26041;&#27861;&#21253;&#25324;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#12289;XGBoost&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#12290;GNNs&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#30721;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20851;&#31995;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#20801;&#35768;&#26631;&#20934;&#21270;&#35780;&#20272;&#20020;&#24202;&#30456;&#20851;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;&#25506;&#32034;&#24615;&#20998;&#26512;&#20197;&#33719;&#24471;&#23545;&#34880;&#28082;&#32454;&#32990;&#34920;&#22411;&#30340;&#27934;&#23519;&#12290;&#36825;&#20195;&#34920;&#20102;&#19968;&#39033;&#37325;&#35201;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00024v1 Announce Type: new  Abstract: This paper presents FlowCyt, the first comprehensive benchmark for multi-class single-cell classification in flow cytometry data. The dataset comprises bone marrow samples from 30 patients, with each cell characterized by twelve markers. Ground truth labels identify five hematological cell types: T lymphocytes, B lymphocytes, Monocytes, Mast cells, and Hematopoietic Stem/Progenitor Cells (HSPCs). Experiments utilize supervised inductive learning and semi-supervised transductive learning on up to 1 million cells per patient. Baseline methods include Gaussian Mixture Models, XGBoost, Random Forests, Deep Neural Networks, and Graph Neural Networks (GNNs). GNNs demonstrate superior performance by exploiting spatial relationships in graph-encoded data. The benchmark allows standardized evaluation of clinically relevant classification tasks, along with exploratory analyses to gain insights into hematological cell phenotypes. This represents th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AerisAI&#30340;&#21487;&#23457;&#35745;&#21516;&#24577;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#21033;&#29992;&#21516;&#24577;&#21152;&#23494;&#21644;&#32454;&#31890;&#24230;&#24046;&#20998;&#38544;&#31169;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#26234;&#33021;&#21512;&#32422;&#30452;&#25509;&#32858;&#21512;&#21152;&#23494;&#21442;&#25968;&#65292;&#28040;&#38500;&#20102;&#23545;&#20110;&#27169;&#22411;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;</title><link>https://arxiv.org/abs/2403.00023</link><description>&lt;p&gt;
&#20855;&#26377;&#22522;&#20110;&#23646;&#24615;&#30340;&#24046;&#20998;&#38544;&#31169;&#30340;&#21487;&#23457;&#35745;&#21516;&#24577;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Auditable Homomorphic-based Decentralized Collaborative AI with Attribute-based Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00023
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AerisAI&#30340;&#21487;&#23457;&#35745;&#21516;&#24577;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#21033;&#29992;&#21516;&#24577;&#21152;&#23494;&#21644;&#32454;&#31890;&#24230;&#24046;&#20998;&#38544;&#31169;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#26234;&#33021;&#21512;&#32422;&#30452;&#25509;&#32858;&#21512;&#21152;&#23494;&#21442;&#25968;&#65292;&#28040;&#38500;&#20102;&#23545;&#20110;&#27169;&#22411;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#27010;&#24565;&#24341;&#39046;&#20102;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;FL&#31995;&#32479;&#22240;&#20026;&#38656;&#35201;&#21463;&#20449;&#20219;&#30340;&#31532;&#19977;&#26041;&#32780;&#23384;&#22312;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#23613;&#31649;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#26469;&#20445;&#25252;&#25968;&#25454;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#26174;&#33879;&#24694;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;AI&#26694;&#26550;&#65292;&#21517;&#20026;&#20855;&#26377;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#21644;&#32454;&#31890;&#24230;&#24046;&#20998;&#38544;&#31169;&#30340;&#21487;&#23457;&#35745;&#30340;&#20998;&#24067;&#24335;&#21327;&#20316;AI&#65288;AerisAI&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;AerisAI&#30452;&#25509;&#20351;&#29992;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#26234;&#33021;&#21512;&#32422;&#26469;&#32858;&#21512;&#21152;&#23494;&#21442;&#25968;&#65292;&#25670;&#33073;&#20102;&#38656;&#35201;&#21463;&#20449;&#20219;&#30340;&#31532;&#19977;&#26041;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#28040;&#38500;&#24046;&#20998;&#38544;&#31169;&#23545;&#20110;&#27169;&#22411;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00023v1 Announce Type: cross  Abstract: In recent years, the notion of federated learning (FL) has led to the new paradigm of distributed artificial intelligence (AI) with privacy preservation. However, most current FL systems suffer from data privacy issues due to the requirement of a trusted third party. Although some previous works introduce differential privacy to protect the data, however, it may also significantly deteriorate the model performance. To address these issues, we propose a novel decentralized collaborative AI framework, named Auditable Homomorphic-based Decentralised Collaborative AI (AerisAI), to improve security with homomorphic encryption and fine-grained differential privacy. Our proposed AerisAI directly aggregates the encrypted parameters with a blockchain-based smart contract to get rid of the need of a trusted third party. We also propose a brand-new concept for eliminating the negative impacts of differential privacy for model performance. Moreove
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#23553;&#38381;&#24418;&#24335;&#35299;&#25110;&#25968;&#23398;&#25512;&#23548;&#65292;&#20063;&#19981;&#38656;&#35201;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#20165;&#38656;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#36827;&#34892;&#19968;&#27425;&#25512;&#26029;&#21363;&#21487;&#20272;&#35745;&#28508;&#22312;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.00019</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#32479;&#35745;&#23398;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Parameter Estimation in Statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00019
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#23553;&#38381;&#24418;&#24335;&#35299;&#25110;&#25968;&#23398;&#25512;&#23548;&#65292;&#20063;&#19981;&#38656;&#35201;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#20165;&#38656;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#36827;&#34892;&#19968;&#27425;&#25512;&#26029;&#21363;&#21487;&#20272;&#35745;&#28508;&#22312;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#20272;&#35745;&#26159;&#32479;&#35745;&#23398;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#26377;&#21161;&#20110;&#24110;&#21161;&#20154;&#20204;&#29702;&#35299;&#35266;&#27979;&#26679;&#26412;&#32972;&#21518;&#30340;&#20998;&#24067;&#12290;&#20256;&#32479;&#19978;&#65292;&#21442;&#25968;&#20272;&#35745;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#35299;&#65288;&#20363;&#22914;&#65292;&#39640;&#26031;&#20998;&#24067;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65289;&#25110;&#36890;&#36807;&#36845;&#20195;&#25968;&#20540;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;Beta&#20998;&#24067;&#26102;&#23553;&#38381;&#24418;&#24335;&#35299;&#19981;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;Newton-Raphson&#26041;&#27861;&#65289;&#26469;&#23436;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#30340;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23553;&#38381;&#24418;&#24335;&#35299;&#25110;&#20219;&#20309;&#25968;&#23398;&#25512;&#23548;&#12290;&#23427;&#29978;&#33267;&#19981;&#38656;&#35201;&#30693;&#36947;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#36825;&#26159;&#25968;&#20540;&#26041;&#27861;&#25152;&#38656;&#30340;&#12290;&#35757;&#32451;&#20102;Transformer&#27169;&#22411;&#21518;&#65292;&#20165;&#38656;&#35201;&#36827;&#34892;&#19968;&#27425;&#25512;&#26029;&#65292;&#21363;&#21487;&#22522;&#20110;&#35266;&#27979;&#26679;&#26412;&#20272;&#35745;&#28508;&#22312;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00019v1 Announce Type: new  Abstract: Parameter estimation is one of the most important tasks in statistics, and is key to helping people understand the distribution behind a sample of observations. Traditionally parameter estimation is done either by closed-form solutions (e.g., maximum likelihood estimation for Gaussian distribution), or by iterative numerical methods such as Newton-Raphson method when closed-form solution does not exist (e.g., for Beta distribution).   In this paper we propose a transformer-based approach to parameter estimation. Compared with existing solutions, our approach does not require a closed-form solution or any mathematical derivations. It does not even require knowing the probability density function, which is needed by numerical methods. After the transformer model is trained, only a single inference is needed to estimate the parameters of the underlying distribution based on a sample of observations. In the empirical study we compared our ap
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22810;&#26631;&#31614;&#30340;&#23458;&#35266;&#29305;&#24449;&#20132;&#20114;&#35774;&#35745;&#65292;&#32467;&#21512;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#20197;&#22312;&#20892;&#19994;&#29615;&#22659;&#20013;&#25214;&#21040;&#26368;&#20339;&#32452;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00017</link><description>&lt;p&gt;
&#26397;&#30528;&#35299;&#37322;&#22810;&#30446;&#26631;&#29305;&#24449;&#20851;&#32852;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Towards Interpreting Multi-Objective Feature Associations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00017
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22810;&#26631;&#31614;&#30340;&#23458;&#35266;&#29305;&#24449;&#20132;&#20114;&#35774;&#35745;&#65292;&#32467;&#21512;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#20197;&#22312;&#20892;&#19994;&#29615;&#22659;&#20013;&#25214;&#21040;&#26368;&#20339;&#32452;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22810;&#20010;&#29305;&#24449;&#22914;&#20309;&#30456;&#20851;&#24182;&#23545;&#29305;&#23450;&#30446;&#26631;&#30340;&#36129;&#29486;&#26159;&#19982;&#29702;&#35299;&#27599;&#20010;&#29305;&#24449;&#22914;&#20309;&#23545;&#29305;&#23450;&#32467;&#26524;&#36129;&#29486;&#21516;&#31561;&#37325;&#35201;&#30340;&#12290;&#22312;&#39044;&#27979;&#20013;&#35299;&#37322;&#21333;&#20010;&#29305;&#24449;&#21487;&#20197;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#22788;&#29702;&#65307;&#28982;&#32780;&#65292;&#22312;&#22810;&#30446;&#26631;&#39044;&#27979;&#20013;&#65292;&#38590;&#20197;&#33719;&#24471;&#29305;&#24449;&#20540;&#32452;&#21512;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26631;&#31614;&#30340;&#23458;&#35266;&#29305;&#24449;&#20132;&#20114;&#35774;&#35745;&#65292;&#20197;&#25214;&#21040;&#20892;&#19994;&#29615;&#22659;&#20013;&#29305;&#24449;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;&#35813;&#35774;&#35745;&#30340;&#19968;&#39033;&#26032;&#39062;&#20043;&#22788;&#26159;&#30830;&#23450;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#29305;&#24449;&#35299;&#37322;&#19982;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#20197;&#30830;&#20445;&#22312;&#22810;&#30446;&#26631;&#29615;&#22659;&#20013;&#36827;&#34892;&#32452;&#21512;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#21021;&#27493;&#23454;&#39564;&#20013;&#35777;&#26126;&#65292;&#21487;&#20197;&#25214;&#21040;&#36817;&#20284;&#30340;&#29305;&#24449;&#20540;&#32452;&#21512;&#20197;&#23454;&#29616;&#26399;&#26395;&#30340;&#32467;&#26524;&#65292;&#35813;&#32467;&#26524;&#20351;&#29992;&#20102;&#20004;&#20010;&#20892;&#19994;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00017v1 Announce Type: cross  Abstract: Understanding how multiple features are associated and contribute to a specific objective is as important as understanding how each feature contributes to a particular outcome. Interpretability of a single feature in a prediction may be handled in multiple ways; however, in a multi-objective prediction, it is difficult to obtain interpretability of a combination of feature values. To address this issue, we propose an objective specific feature interaction design using multi-labels to find the optimal combination of features in agricultural settings. One of the novel aspects of this design is the identification of a method that integrates feature explanations with global sensitivity analysis in order to ensure combinatorial optimization in multi-objective settings. We have demonstrated in our preliminary experiments that an approximate combination of feature values can be found to achieve the desired outcome using two agricultural datas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21453;&#39304;&#21644;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#20197;&#22312;&#23478;&#31165;&#31649;&#29702;&#20013;&#26368;&#20248;&#21270;&#38477;&#20302;&#22810;&#31181;&#30149;&#21407;&#20307;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.00016</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#30340;&#32452;&#21512;&#20248;&#21270;&#30340;&#28145;&#24230;&#25935;&#24863;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Deep Sensitivity Analysis for Objective-Oriented Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21453;&#39304;&#21644;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#20197;&#22312;&#23478;&#31165;&#31649;&#29702;&#20013;&#26368;&#20248;&#21270;&#38477;&#20302;&#22810;&#31181;&#30149;&#21407;&#20307;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39282;&#20859;&#29615;&#22659;&#20225;&#19994;&#20195;&#30721;&#65306;2403.00016v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#30149;&#21407;&#20307;&#25511;&#21046;&#26159;&#29616;&#20195;&#23478;&#31165;&#20859;&#27542;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#23545;&#20844;&#20849;&#20581;&#24247;&#21644;&#29983;&#20135;&#21147;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26377;&#25928;&#30340;&#23478;&#31165;&#31649;&#29702;&#25514;&#26045;&#21487;&#20197;&#38477;&#20302;&#23478;&#31165;&#32676;&#20307;&#20013;&#30340;&#30149;&#21407;&#20307;&#27700;&#24179;&#65292;&#20174;&#32780;&#36890;&#36807;&#38477;&#20302;&#39135;&#28304;&#24615;&#30142;&#30149;&#30340;&#39118;&#38505;&#26469;&#20419;&#36827;&#39135;&#21697;&#23433;&#20840;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#36890;&#36807;&#39044;&#38450;&#33021;&#22815;&#36805;&#36895;&#20256;&#25773;&#24182;&#24433;&#21709;&#32676;&#20307;&#29983;&#38271;&#12289;&#34507;&#20135;&#37327;&#21644;&#25972;&#20307;&#20581;&#24247;&#30340;&#20256;&#26579;&#30149;&#26469;&#25903;&#25345;&#21160;&#29289;&#20581;&#24247;&#21644;&#31119;&#21033;&#12290;&#26412;&#30740;&#31350;&#23558;&#23547;&#25214;&#26368;&#20339;&#31649;&#29702;&#23454;&#36341;&#20197;&#26368;&#23567;&#21270;&#22810;&#31181;&#30149;&#21407;&#20307;&#23384;&#22312;&#35270;&#20026;&#19968;&#20010;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#31649;&#29702;&#35774;&#32622;&#30340;&#21487;&#33021;&#32452;&#21512;&#24314;&#27169;&#20026;&#19968;&#20010;&#35299;&#20915;&#31354;&#38388;&#65292;&#21487;&#36890;&#36807;&#39640;&#25928;&#25506;&#32034;&#20197;&#35782;&#21035;&#26368;&#20248;&#38477;&#20302;&#30149;&#21407;&#20307;&#27700;&#24179;&#30340;&#37197;&#32622;&#12290;&#35813;&#35774;&#35745;&#34701;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21453;&#39304;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#29305;&#24449;&#35299;&#37322;&#21644;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#20197;&#30830;&#20445;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00016v1 Announce Type: cross  Abstract: Pathogen control is a critical aspect of modern poultry farming, providing important benefits for both public health and productivity. Effective poultry management measures to reduce pathogen levels in poultry flocks promote food safety by lowering risks of food-borne illnesses. They also support animal health and welfare by preventing infectious diseases that can rapidly spread and impact flock growth, egg production, and overall health. This study frames the search for optimal management practices that minimize the presence of multiple pathogens as a combinatorial optimization problem. Specifically, we model the various possible combinations of management settings as a solution space that can be efficiently explored to identify configurations that optimally reduce pathogen levels. This design incorporates a neural network feedback-based method that combines feature explanations with global sensitivity analysis to ensure combinatorial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GIN-SD&#26694;&#26550;&#65292;&#36890;&#36807;&#20301;&#32622;&#32534;&#30721;&#21644;&#20851;&#27880;&#34701;&#21512;&#35299;&#20915;&#20102;&#22312;&#22270;&#20013;&#26816;&#27979;&#20855;&#26377;&#19981;&#23436;&#25972;&#33410;&#28857;&#30340;&#26469;&#28304;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00014</link><description>&lt;p&gt;
GIN-SD: &#36890;&#36807;&#20301;&#32622;&#32534;&#30721;&#21644;&#20851;&#27880;&#34701;&#21512;&#22312;&#22270;&#20013;&#26816;&#27979;&#26377;&#19981;&#23436;&#25972;&#33410;&#28857;&#30340;&#26469;&#28304;
&lt;/p&gt;
&lt;p&gt;
GIN-SD: Source Detection in Graphs with Incomplete Nodes via Positional Encoding and Attentive Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GIN-SD&#26694;&#26550;&#65292;&#36890;&#36807;&#20301;&#32622;&#32534;&#30721;&#21644;&#20851;&#27880;&#34701;&#21512;&#35299;&#20915;&#20102;&#22312;&#22270;&#20013;&#26816;&#27979;&#20855;&#26377;&#19981;&#23436;&#25972;&#33410;&#28857;&#30340;&#26469;&#28304;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20013;&#26816;&#27979;&#28304;&#24050;&#32463;&#22312;&#35875;&#35328;&#28304;&#35782;&#21035;&#39046;&#22495;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#25928;&#21147;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20294;&#36890;&#24120;&#35201;&#27714;&#23436;&#25972;&#30340;&#29992;&#25143;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21363;&#36890;&#36807;&#19981;&#23436;&#25972;&#30340;&#29992;&#25143;&#25968;&#25454;&#36827;&#34892;&#35875;&#35328;&#28304;&#26816;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;GIN-SD&#65292;&#36890;&#36807;&#20301;&#32622;&#32534;&#30721;&#21644;&#20851;&#27880;&#34701;&#21512;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20301;&#32622;&#23884;&#20837;&#27169;&#22359;&#26469;&#21306;&#20998;&#19981;&#23436;&#25972;&#30340;&#33410;&#28857;&#65292;&#24182;&#37319;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#26469;&#20851;&#27880;&#20855;&#26377;&#26356;&#22823;&#20449;&#24687;&#20256;&#36755;&#33021;&#21147;&#30340;&#33410;&#28857;&#12290;&#20026;&#20102;&#32531;&#35299;&#30001;&#20110;&#28304;&#33410;&#28857;&#21644;&#38750;&#28304;&#33410;&#28857;&#25968;&#37327;&#20043;&#38388;&#26174;&#33879;&#24046;&#24322;&#23548;&#33268;&#30340;&#39044;&#27979;&#20559;&#24046;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#31867;&#24179;&#34913;&#26426;&#21046;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;GIN-SD&#21450;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00014v1 Announce Type: cross  Abstract: Source detection in graphs has demonstrated robust efficacy in the domain of rumor source identification. Although recent solutions have enhanced performance by leveraging deep neural networks, they often require complete user data. In this paper, we address a more challenging task, rumor source detection with incomplete user data, and propose a novel framework, i.e., Source Detection in Graphs with Incomplete Nodes via Positional Encoding and Attentive Fusion (GIN-SD), to tackle this challenge. Specifically, our approach utilizes a positional embedding module to distinguish nodes that are incomplete and employs a self-attention mechanism to focus on nodes with greater information transmission capacity. To mitigate the prediction bias caused by the significant disparity between the numbers of source and non-source nodes, we also introduce a class-balancing mechanism. Extensive experiments validate the effectiveness of GIN-SD and its su
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20449;&#24687;&#29305;&#24449;&#21644;&#26679;&#26412;&#26469;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24320;&#21457;&#36807;&#31243;</title><link>https://arxiv.org/abs/2403.00013</link><description>&lt;p&gt;
&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#20026;&#28145;&#24230;&#23398;&#20064;&#20248;&#20808;&#36873;&#25321;&#20449;&#24687;&#29305;&#24449;&#21644;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Informative Features and Examples for Deep Learning from Noisy Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00013
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20449;&#24687;&#29305;&#24449;&#21644;&#26679;&#26412;&#26469;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24320;&#21457;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#21487;&#20197;&#20248;&#20808;&#36873;&#25321;&#20449;&#24687;&#29305;&#24449;&#21644;&#26679;&#26412;&#65292;&#20197;&#22686;&#24378;&#24320;&#21457;&#36807;&#31243;&#30340;&#27599;&#20010;&#38454;&#27573;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20248;&#20808;&#36873;&#25321;&#20449;&#24687;&#29305;&#24449;&#21644;&#26679;&#26412;&#65292;&#24182;&#25552;&#39640;&#29305;&#24449;&#23398;&#20064;&#12289;&#25968;&#25454;&#26631;&#35760;&#21644;&#25968;&#25454;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36741;&#21161;&#30340;&#20998;&#24067;&#25968;&#25454;&#25552;&#21462;&#21482;&#19982;&#35299;&#20915;&#30446;&#26631;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36741;&#21161;&#20998;&#24067;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#29305;&#24449;&#26469;&#21435;&#38500;&#30446;&#26631;&#20998;&#24067;&#20013;&#30340;&#22122;&#22768;&#29305;&#24449;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#65292;&#20174;&#26080;&#26631;&#31614;&#30340;&#22024;&#26434;&#25968;&#25454;&#20013;&#20248;&#20808;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#65292;&#20197;&#38477;&#20302;&#20027;&#21160;&#23398;&#20064;&#30340;&#26631;&#35760;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#32431;&#24230;-&#20449;&#24687;&#22256;&#22659;&#65292;&#21363;&#23581;&#35797;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#20250;&#23548;&#33268;&#36873;&#25321;&#35768;&#22810;&#22122;&#22768;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#65292;&#25214;&#21040;&#26368;&#20339;&#32431;&#24230;&#21644;&#20449;&#24687;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00013v1 Announce Type: new  Abstract: In this dissertation, we propose a systemic framework that prioritizes informative features and examples to enhance each stage of the development process. Specifically, we prioritize informative features and examples and improve the performance of feature learning, data labeling, and data selection. We first propose an approach to extract only informative features that are inherent to solving a target task by using auxiliary out-of-distribution data. We deactivate the noise features in the target distribution by using that in the out-of-distribution data. Next, we introduce an approach that prioritizes informative examples from unlabeled noisy data in order to reduce the labeling cost of active learning. In order to solve the purity-information dilemma, where an attempt to select informative examples induces the selection of many noisy examples, we propose a meta-model that finds the best balance between purity and informativeness. Lastl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#25490;&#24207;GNN&#30340;&#23450;&#26102;&#39044;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#20840;&#23616;&#30005;&#36335;&#39044;&#35757;&#32451;&#12289;&#23616;&#37096;&#26102;&#24310;&#23398;&#20064;&#21644;&#27880;&#24847;&#21147;&#21333;&#20803;&#24314;&#27169;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#24037;&#19994;&#30005;&#36335;&#20013;&#30340;&#20449;&#21495;&#34928;&#20943;&#21644;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00012</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#25490;&#24207;GNN&#30340;&#23450;&#26102;&#39044;&#27979;&#65306;&#20840;&#23616;&#30005;&#36335;&#39044;&#35757;&#32451;&#65292;&#23616;&#37096;&#26102;&#24310;&#23398;&#20064;&#21644;&#27880;&#24847;&#21147;&#21333;&#20803;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
PreRoutGNN for Timing Prediction with Order Preserving Partition: Global Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#25490;&#24207;GNN&#30340;&#23450;&#26102;&#39044;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#20840;&#23616;&#30005;&#36335;&#39044;&#35757;&#32451;&#12289;&#23616;&#37096;&#26102;&#24310;&#23398;&#20064;&#21644;&#27880;&#24847;&#21147;&#21333;&#20803;&#24314;&#27169;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#24037;&#19994;&#30005;&#36335;&#20013;&#30340;&#20449;&#21495;&#34928;&#20943;&#21644;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#36335;&#30001;&#23450;&#26102;&#39044;&#27979;&#26368;&#36817;&#34987;&#30740;&#31350;&#29992;&#20110;&#35780;&#20272;&#33455;&#29255;&#35774;&#35745;&#20013;&#20505;&#36873;&#21333;&#20803;&#24067;&#23616;&#30340;&#36136;&#37327;&#12290;&#23427;&#30452;&#25509;&#20272;&#35745;&#24341;&#33050;&#32423;&#65288;&#20313;&#37327;&#12289;&#26012;&#29575;&#65289;&#21644;&#36793;&#32423;&#65288;&#32593;&#24310;&#36831;&#12289;&#21333;&#20803;&#24310;&#36831;&#65289;&#30340;&#23450;&#26102;&#25351;&#26631;&#65292;&#32780;&#26080;&#38656;&#32791;&#26102;&#36335;&#30001;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#35268;&#27169;&#24037;&#19994;&#30005;&#36335;&#20013;&#65292;&#30001;&#20110;&#38271;&#26102;&#24310;&#36335;&#24452;&#65292;&#23427;&#32463;&#24120;&#36973;&#21463;&#20449;&#21495;&#34928;&#20943;&#21644;&#35823;&#24046;&#32047;&#31215;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#23616;&#30005;&#36335;&#35757;&#32451;&#26469;&#39044;&#35757;&#32451;&#19968;&#20010;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20174;&#30005;&#36335;&#32593;&#34920;&#20013;&#23398;&#20064;&#20840;&#23616;&#22270;&#23884;&#20837;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#28857;&#26356;&#26032;&#26041;&#26696;&#36827;&#34892;GCN&#19978;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#36981;&#24490;&#23398;&#20064;&#21040;&#30340;&#22270;&#23884;&#20837;&#21644;&#30005;&#36335;&#22270;&#30340;&#25299;&#25169;&#25490;&#24207;&#24207;&#21015;&#12290;&#36825;&#20010;&#26041;&#26696;&#22312;&#26356;&#26032;&#24207;&#21015;&#20013;&#27531;&#30041;&#22320;&#24314;&#27169;&#20102;&#30456;&#37051;&#24341;&#33050;&#20043;&#38388;&#30340;&#23616;&#37096;&#26102;&#38388;&#24310;&#36831;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#27880;&#24847;&#21147;&#21333;&#20803;&#25552;&#21462;&#20102;&#27599;&#20010;&#21333;&#20803;&#20869;&#37096;&#30340;&#26597;&#25214;&#34920;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00012v1 Announce Type: new  Abstract: Pre-routing timing prediction has been recently studied for evaluating the quality of a candidate cell placement in chip design. It involves directly estimating the timing metrics for both pin-level (slack, slew) and edge-level (net delay, cell delay), without time-consuming routing. However, it often suffers from signal decay and error accumulation due to the long timing paths in large-scale industrial circuits. To address these challenges, we propose a two-stage approach. First, we propose global circuit training to pre-train a graph auto-encoder that learns the global graph embedding from circuit netlist. Second, we use a novel node updating scheme for message passing on GCN, following the topological sorting sequence of the learned graph embedding and circuit graph. This scheme residually models the local time delay between two adjacent pins in the updating sequence, and extracts the lookup table information inside each cell via a ne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#29992;&#25143;&#21453;&#39304;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;UFCE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#21453;&#20107;&#23454;&#35299;&#37322;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22686;&#24378;&#25552;&#20379;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.00011</link><description>&lt;p&gt;
&#24341;&#20837;&#22522;&#20110;&#29992;&#25143;&#21453;&#39304;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;UFCE&#65289;
&lt;/p&gt;
&lt;p&gt;
Introducing User Feedback-based Counterfactual Explanations (UFCE)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#29992;&#25143;&#21453;&#39304;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;UFCE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#21453;&#20107;&#23454;&#35299;&#37322;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22686;&#24378;&#25552;&#20379;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#24120;&#24120;&#20351;&#24471;&#35299;&#37322;&#20854;&#20915;&#31574;&#32972;&#21518;&#30340;&#21407;&#22240;&#25104;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#24050;&#32463;&#25104;&#20026;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#29983;&#25104;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;CE&#25552;&#20379;&#32473;&#29992;&#25143;&#20851;&#20110;&#22914;&#20309;&#36890;&#36807;&#26368;&#23567;&#30340;&#36755;&#20837;&#20462;&#25913;&#23454;&#29616;&#25152;&#26399;&#26395;&#30340;&#32467;&#26524;&#30340;&#21487;&#25805;&#20316;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;CE&#31639;&#27861;&#36890;&#24120;&#22312;&#20248;&#21270;&#21464;&#21270;&#20197;&#36991;&#20813;&#19981;&#26399;&#26395;&#30340;&#32467;&#26524;&#26102;&#22312;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#20869;&#36816;&#34892;&#65292;&#24573;&#35270;&#20102;&#23545;&#32467;&#26524;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#30340;&#35782;&#21035;&#65292;&#24182;&#24573;&#35270;&#20102;&#24314;&#35758;&#21464;&#21270;&#30340;&#23454;&#38469;&#21487;&#34892;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#34987;&#21629;&#21517;&#20026;&#22522;&#20110;&#29992;&#25143;&#21453;&#39304;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;UFCE&#65289;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#26088;&#22312;&#22686;&#24378;&#23545;&#25152;&#25552;&#20379;&#35299;&#37322;&#30340;&#20449;&#24515;&#12290;UFCE&#20801;&#35768;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00011v1 Announce Type: cross  Abstract: Machine learning models are widely used in real-world applications. However, their complexity makes it often challenging to interpret the rationale behind their decisions. Counterfactual explanations (CEs) have emerged as a viable solution for generating comprehensible explanations in eXplainable Artificial Intelligence (XAI). CE provides actionable information to users on how to achieve the desired outcome with minimal modifications to the input. However, current CE algorithms usually operate within the entire feature space when optimizing changes to turn over an undesired outcome, overlooking the identification of key contributors to the outcome and disregarding the practicality of the suggested changes. In this study, we introduce a novel methodology, that is named as user feedback-based counterfactual explanation (UFCE), which addresses these limitations and aims to bolster confidence in the provided explanations. UFCE allows for t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#33258;&#21160;&#36229;&#22768;&#24515;&#21160;&#22270;&#35270;&#22270;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;3D&#24515;&#33039;&#32593;&#26684;&#37325;&#24314;&#21644;&#21518;&#32493;&#20219;&#21153;&#22914;&#20998;&#21106;&#21644;&#23039;&#21183;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.19062</link><description>&lt;p&gt;
&#33258;&#21160;&#36229;&#22768;&#24515;&#21160;&#22270;&#35270;&#22270;&#35782;&#21035;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#20840;&#38754;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#33258;&#21160;&#36229;&#22768;&#24515;&#21160;&#22270;&#35270;&#22270;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;3D&#24515;&#33039;&#32593;&#26684;&#37325;&#24314;&#21644;&#21518;&#32493;&#20219;&#21153;&#22914;&#20998;&#21106;&#21644;&#23039;&#21183;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#23545;&#24515;&#33039;&#36229;&#22768;&#27874;&#65288;US&#65289;&#30340;&#35786;&#26029;&#65292;&#20020;&#24202;&#23454;&#36341;&#24050;&#32463;&#24314;&#31435;&#20102;&#24515;&#33039;&#30340;&#20960;&#31181;&#26631;&#20934;&#35270;&#22270;&#65292;&#36825;&#20123;&#35270;&#22270;&#20316;&#20026;&#35786;&#26029;&#27979;&#37327;&#30340;&#21442;&#32771;&#28857;&#65292;&#24182;&#23450;&#20041;&#20102;&#22270;&#20687;&#33719;&#21462;&#30340;&#35270;&#21475;&#12290;&#33258;&#21160;&#35270;&#22270;&#35782;&#21035;&#28041;&#21450;&#23558;&#36825;&#20123;&#22270;&#20687;&#20998;&#32452;&#20026;&#26631;&#20934;&#35270;&#22270;&#30340;&#31867;&#21035;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#23454;&#29616;&#36825;&#19968;&#28857;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#22312;&#39564;&#35777;&#22270;&#20687;&#26159;&#21542;&#36866;&#21512;&#29305;&#23450;&#27979;&#37327;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#21407;&#22240;&#21253;&#25324;&#24515;&#33039;&#32467;&#26500;&#30340;&#27491;&#30830;&#20301;&#32622;&#12289;&#23039;&#21183;&#21644;&#21487;&#33021;&#30340;&#36974;&#25377;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36234;&#20102;&#35270;&#22270;&#20998;&#31867;&#65292;&#24182;&#34701;&#21512;&#20102;&#23545;&#24515;&#33039;&#30340;&#19977;&#32500;&#32593;&#26684;&#37325;&#24314;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22810;&#21518;&#32493;&#20219;&#21153;&#65292;&#22914;&#20998;&#21106;&#21644;&#23039;&#21183;&#20272;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#22270;&#21367;&#31215;&#23398;&#20064;3D&#24515;&#33039;&#32593;&#26684;&#65292;&#20351;&#29992;&#31867;&#20284;&#30340;&#25216;&#26415;&#26469;&#23398;&#20064;&#33258;&#28982;&#22270;&#20687;&#20013;&#30340;3D&#32593;&#26684;&#65292;&#20363;&#22914;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19062v1 Announce Type: cross  Abstract: To facilitate diagnosis on cardiac ultrasound (US), clinical practice has established several standard views of the heart, which serve as reference points for diagnostic measurements and define viewports from which images are acquired. Automatic view recognition involves grouping those images into classes of standard views. Although deep learning techniques have been successful in achieving this, they still struggle with fully verifying the suitability of an image for specific measurements due to factors like the correct location, pose, and potential occlusions of cardiac structures. Our approach goes beyond view classification and incorporates a 3D mesh reconstruction of the heart that enables several more downstream tasks, like segmentation and pose estimation. In this work, we explore learning 3D heart meshes via graph convolutions, using similar techniques to learn 3D meshes in natural images, such as human pose estimation. As the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#24212;&#23545;&#24322;&#26500;&#25968;&#25454;&#23396;&#23707;&#20013;&#27169;&#22411;&#36866;&#24212;&#26032;&#20998;&#24067;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.18888</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#31163;&#25955;&#24322;&#26500;&#25968;&#25454;&#23396;&#23707;&#20013;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Based Extensible Codebook for Discrete Federated Learning in Heterogeneous Data Silos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18888
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#24212;&#23545;&#24322;&#26500;&#25968;&#25454;&#23396;&#23707;&#20013;&#27169;&#22411;&#36866;&#24212;&#26032;&#20998;&#24067;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#21033;&#29992;&#24191;&#27867;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;(FL)&#38754;&#20020;&#30528;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#19981;&#21516;&#23396;&#23707;&#38388;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20174;FL&#23548;&#20986;&#30340;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#20855;&#26377;&#38476;&#29983;&#20998;&#24067;&#30340;&#25968;&#25454;&#23396;&#23707;&#26102;&#20250;&#34920;&#29616;&#20986;&#26126;&#26174;&#22686;&#21152;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#32780;&#31616;&#21333;&#30340;&#36845;&#20195;&#26694;&#26550;&#65292;&#31216;&#20026;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#32852;&#37030;&#23398;&#20064;(UEFL)&#12290;&#35813;&#26694;&#26550;&#21160;&#24577;&#22320;&#23558;&#28508;&#22312;&#29305;&#24449;&#26144;&#23556;&#21040;&#21487;&#35757;&#32451;&#30340;&#31163;&#25955;&#21521;&#37327;&#65292;&#35780;&#20272;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#38024;&#23545;&#34920;&#29616;&#20986;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#23396;&#23707;&#29305;&#21035;&#22320;&#25193;&#23637;&#31163;&#25955;&#21270;&#35789;&#20856;&#25110;&#32534;&#30721;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18888v1 Announce Type: new  Abstract: Federated learning (FL), aimed at leveraging vast distributed datasets, confronts a crucial challenge: the heterogeneity of data across different silos. While previous studies have explored discrete representations to enhance model generalization across minor distributional shifts, these approaches often struggle to adapt to new data silos with significantly divergent distributions. In response, we have identified that models derived from FL exhibit markedly increased uncertainty when applied to data silos with unfamiliar distributions. Consequently, we propose an innovative yet straightforward iterative framework, termed Uncertainty-Based Extensible-Codebook Federated Learning (UEFL). This framework dynamically maps latent features to trainable discrete vectors, assesses the uncertainty, and specifically extends the discretization dictionary or codebook for silos exhibiting high uncertainty. Our approach aims to simultaneously enhance a
&lt;/p&gt;</description></item><item><title>ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18609</link><description>&lt;p&gt;
ICE-SEARCH: &#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH: A Language Model-Driven Feature Selection Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18609
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;In-Context Evolutionary Search (ICE-SEARCH)&#26041;&#27861;&#65292;&#36825;&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;(LMs)&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;(FS)&#20219;&#21153;&#30340;&#24037;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;(MPA)&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;ICE-SEARCH&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#20132;&#21449;&#21644;&#31361;&#21464;&#33021;&#21147;&#65292;&#22312;&#19968;&#20010;&#36827;&#21270;&#26694;&#26550;&#20869;&#26174;&#30528;&#25913;&#36827;&#29305;&#24449;&#36873;&#25321;&#65292;&#36890;&#36807;&#27169;&#22411;&#30340;&#20840;&#38754;&#19990;&#30028;&#30693;&#35782;&#21644;&#20854;&#36866;&#24212;&#21508;&#31181;&#35282;&#33394;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#35813;&#26041;&#27861;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#19977;&#20010;&#20851;&#38190;&#30340;MPA&#20219;&#21153;&#65306;&#20013;&#39118;&#12289;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;&#31958;&#23615;&#30149;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;ICE-SEARCH&#22312;&#30830;&#23450;&#21307;&#23398;&#24212;&#29992;&#30340;&#20851;&#38190;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;FS&#26041;&#27861;&#12290;ICE-SEARCH&#22312;&#20013;&#39118;&#39044;&#27979;&#21644;&#31958;&#23615;&#30149;&#39044;&#27979;&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#27700;&#24179;&#65307;&#20915;&#31574;&#38543;&#26426;&#21270;ICE-SEARCH&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#20013;&#25490;&#21517;&#20026;&#39046;&#20808;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18609v1 Announce Type: cross  Abstract: This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method, the first work that melds language models (LMs) with evolutionary algorithms for feature selection (FS) tasks and demonstrates its effectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and mutation capabilities inherent in LMs within an evolutionary framework, significantly improving FS through the model's comprehensive world knowledge and its adaptability to a variety of roles. Our evaluation of this methodology spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional FS methods in pinpointing essential features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction and diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease prediction. Our results not only demonstrate
&lt;/p&gt;</description></item><item><title>SuperdropNet&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#20195;&#29702;&#65292;&#26088;&#22312;&#27169;&#25311;&#28082;&#28404;&#30340;&#20113;&#24494;&#29289;&#29702;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#33258;&#22238;&#24402;&#39044;&#27979;&#12289;&#29289;&#29702;&#32422;&#26463;&#21644;&#31934;&#32454;&#25511;&#21046;&#38543;&#26426;&#24615;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;</title><link>https://arxiv.org/abs/2402.18354</link><description>&lt;p&gt;
SuperdropNet: &#19968;&#31181;&#31283;&#23450;&#20934;&#30830;&#30340;&#36866;&#29992;&#20110;&#22522;&#20110;&#28082;&#28404;&#30340;&#20113;&#24494;&#29289;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SuperdropNet: a Stable and Accurate Machine Learning Proxy for Droplet-based Cloud Microphysics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18354
&lt;/p&gt;
&lt;p&gt;
SuperdropNet&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#20195;&#29702;&#65292;&#26088;&#22312;&#27169;&#25311;&#28082;&#28404;&#30340;&#20113;&#24494;&#29289;&#29702;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22810;&#27493;&#33258;&#22238;&#24402;&#39044;&#27979;&#12289;&#29289;&#29702;&#32422;&#26463;&#21644;&#31934;&#32454;&#25511;&#21046;&#38543;&#26426;&#24615;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#24494;&#29289;&#29702;&#23545;&#27668;&#20505;&#21644;&#22825;&#27668;&#29616;&#35937;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#19981;&#20934;&#30830;&#30340;&#34920;&#31034;&#21487;&#33021;&#20250;&#38480;&#21046;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SuperdropNet&#30340;&#26426;&#22120;&#23398;&#20064;&#20195;&#29702;&#65292;&#29992;&#20110;&#27169;&#25311;&#22522;&#20110;&#28082;&#28404;&#30340;&#20113;&#24494;&#29289;&#29702;&#65292;&#26088;&#22312;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18354v1 Announce Type: cross  Abstract: Cloud microphysics has important consequences for climate and weather phenomena, and inaccurate representations can limit forecast accuracy. While atmospheric models increasingly resolve storms and clouds, the accuracy of the underlying microphysics remains limited by computationally expedient bulk moment schemes based on simplifying assumptions. Droplet-based Lagrangian schemes are more accurate but are underutilized due to their large computational overhead. Machine learning (ML) based schemes can bridge this gap by learning from vast droplet-based simulation datasets, but have so far struggled to match the accuracy and stability of bulk moment schemes. To address this challenge, we developed SuperdropNet, an ML-based emulator of the Lagrangian superdroplet simulations. To improve accuracy and stability, we employ multi-step autoregressive prediction during training, impose physical constraints, and carefully control stochasticity in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#31185;&#23398;&#21407;&#29702;&#21644;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#24314;&#27169;&#38750;&#32447;&#24615;&#32467;&#26500;&#30340;&#22320;&#38663;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.17992</link><description>&lt;p&gt;
&#29289;&#29702;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#39044;&#27979;&#38750;&#32447;&#24615;&#38050;&#26694;&#26550;&#32467;&#26500;&#30340;&#22320;&#38663;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Machine Learning for Seismic Response Prediction OF Nonlinear Steel Moment Resisting Frame Structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#31185;&#23398;&#21407;&#29702;&#21644;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#24314;&#27169;&#38750;&#32447;&#24615;&#32467;&#26500;&#30340;&#22320;&#38663;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20256;&#32479;&#25968;&#20540;&#27169;&#25311;&#30340;&#22823;&#37327;&#35745;&#31639;&#25104;&#26412;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#36827;&#34892;&#32467;&#26500;&#20803;&#27169;&#22411;&#24314;&#27169;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#26174;&#31034;&#20986;&#27169;&#22411;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#20016;&#23500;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#28508;&#22312;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#21551;&#21457;&#26426;&#22120;&#23398;&#20064;&#65288;PiML&#65289;&#26041;&#27861;&#65292;&#23558;&#31185;&#23398;&#21407;&#29702;&#21644;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29992;&#20110;&#24314;&#27169;&#38750;&#32447;&#24615;&#32467;&#26500;&#30340;&#22320;&#38663;&#21709;&#24212;&#12290;&#22522;&#26412;&#27010;&#24565;&#26159;&#23558;ML&#27169;&#22411;&#30340;&#35299;&#31354;&#38388;&#32422;&#26463;&#22312;&#24050;&#30693;&#30340;&#29289;&#29702;&#33539;&#22260;&#20869;&#12290;&#36825;&#26159;&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#29305;&#28857;&#23454;&#29616;&#30340;&#65292;&#21363;&#27169;&#22411;&#38477;&#38454;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#21644;&#29275;&#39039;&#31532;&#20108;&#23450;&#24459;&#65288;&#20363;&#22914;&#65292;&#36816;&#21160;&#26041;&#31243;&#65289;&#12290;&#27169;&#22411;&#38477;&#38454;&#23545;&#22788;&#29702;&#20855;&#26377;&#22266;&#26377;&#20887;&#20313;&#24615;&#21644;&#22686;&#24378;&#24615;&#30340;&#32467;&#26500;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17992v1 Announce Type: cross  Abstract: There is a growing interest in utilizing machine learning (ML) methods for structural metamodeling due to the substantial computational cost of traditional numerical simulations. The existing data-driven strategies show potential limitations to the model robustness and interpretability as well as the dependency of rich data. To address these challenges, this paper presents a novel physics-informed machine learning (PiML) method, which incorporates scientific principles and physical laws into deep neural networks for modeling seismic responses of nonlinear structures. The basic concept is to constrain the solution space of the ML model within known physical bounds. This is made possible with three main features, namely, model order reduction, a long short-term memory (LSTM) networks, and Newton's second law (e.g., the equation of motion). Model order reduction is essential for handling structural systems with inherent redundancy and enh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Imagine, Initialize, and Explore (IIE)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.17978</link><description>&lt;p&gt;
&#24819;&#35937;&#12289;&#21021;&#22987;&#21270;&#21644;&#25506;&#32034;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Imagine, Initialize, and Explore (IIE)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#25506;&#32034;&#23545;&#20110;&#22312;&#22797;&#26434;&#21327;&#35843;&#20219;&#21153;&#20013;&#21457;&#29616;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#26368;&#20339;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#20869;&#22312;&#22870;&#21169;&#26469;&#23454;&#29616;&#25215;&#35834;&#30340;&#25506;&#32034;&#65292;&#25110;&#32773;&#20351;&#29992;&#22522;&#20110;&#35282;&#33394;&#30340;&#23398;&#20064;&#26469;&#20998;&#35299;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#22312;&#25972;&#20010;&#21160;&#20316;-&#35266;&#23519;&#31354;&#38388;&#20013;&#36827;&#34892;&#38598;&#20307;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#20013;&#65292;&#20182;&#20204;&#24448;&#24448;&#38754;&#20020;&#33719;&#21462;&#29305;&#23450;&#32852;&#21512;&#21160;&#20316;&#24207;&#21015;&#20197;&#36798;&#21040;&#25104;&#21151;&#29366;&#24577;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Imagine, Initialize, and Explore (IIE)&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#22810;&#26234;&#33021;&#20307;&#25506;&#32034;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;IIE&#21033;&#29992;&#19968;&#20010;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#24819;&#35937;&#26234;&#33021;&#20307;&#22914;&#20309;&#36798;&#21040;&#21487;&#20197;&#24433;&#21709;&#24444;&#27492;&#36716;&#31227;&#20989;&#25968;&#30340;&#20851;&#38190;&#29366;&#24577;&#12290;&#28982;&#21518;&#65292;&#22312;&#25506;&#32034;&#38454;&#27573;&#20043;&#21069;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#22120;&#22312;&#36825;&#20010;&#29366;&#24577;&#19979;&#21021;&#22987;&#21270;&#29615;&#22659;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#23454;&#29616;&#36825;&#31181;&#24819;&#35937;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17978v1 Announce Type: cross  Abstract: Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imag
&lt;/p&gt;</description></item><item><title>Cieran&#26159;&#19968;&#20010;&#20801;&#35768;&#25968;&#25454;&#20998;&#26512;&#24072;&#22312;Jupyter&#31508;&#35760;&#26412;&#20013;&#35774;&#35745;&#22270;&#34920;&#26102;&#24555;&#36895;&#25214;&#21040;&#36136;&#37327;&#37197;&#33394;&#26041;&#26696;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#25490;&#24207;&#21644;&#21019;&#24314;&#26032;&#30340;&#37197;&#33394;&#26041;&#26696;&#65292;&#24110;&#21161;&#26032;&#25163;&#20998;&#26512;&#24072;&#23450;&#21046;&#37197;&#33394;&#26041;&#26696;&#20197;&#36866;&#24212;&#20854;&#25968;&#25454;&#32972;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.15997</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#29616;&#22330;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#35774;&#35745;&#39034;&#24207;&#37197;&#33394;&#26041;&#26696;&#30340;Cieran
&lt;/p&gt;
&lt;p&gt;
Cieran: Designing Sequential Colormaps via In-Situ Active Preference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15997
&lt;/p&gt;
&lt;p&gt;
Cieran&#26159;&#19968;&#20010;&#20801;&#35768;&#25968;&#25454;&#20998;&#26512;&#24072;&#22312;Jupyter&#31508;&#35760;&#26412;&#20013;&#35774;&#35745;&#22270;&#34920;&#26102;&#24555;&#36895;&#25214;&#21040;&#36136;&#37327;&#37197;&#33394;&#26041;&#26696;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;&#25490;&#24207;&#21644;&#21019;&#24314;&#26032;&#30340;&#37197;&#33394;&#26041;&#26696;&#65292;&#24110;&#21161;&#26032;&#25163;&#20998;&#26512;&#24072;&#23450;&#21046;&#37197;&#33394;&#26041;&#26696;&#20197;&#36866;&#24212;&#20854;&#25968;&#25454;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#36136;&#30340;&#37197;&#33394;&#26041;&#26696;&#21487;&#20197;&#24110;&#21161;&#20256;&#36798;&#37325;&#35201;&#30340;&#25968;&#25454;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#35201;&#20026;&#29305;&#23450;&#24773;&#26223;&#25214;&#21040;&#30475;&#36215;&#26469;&#8220;&#24688;&#21040;&#22909;&#22788;&#8221;&#30340;&#32654;&#35266;&#37197;&#33394;&#26041;&#26696;&#65292;&#38656;&#35201;&#30456;&#24403;&#22810;&#30340;&#35774;&#35745;&#21644;&#25216;&#26415;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Cieran&#65292;&#36825;&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#20801;&#35768;&#20219;&#20309;&#25968;&#25454;&#20998;&#26512;&#24072;&#22312;&#35774;&#35745;Jupyter&#31508;&#35760;&#26412;&#20013;&#30340;&#22270;&#34920;&#26102;&#36805;&#36895;&#25214;&#21040;&#20248;&#36136;&#30340;&#37197;&#33394;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#37319;&#29992;&#20102;&#19968;&#31181;&#20027;&#21160;&#30340;&#20559;&#22909;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#20004;&#20004;&#27604;&#36739;&#26469;&#23545;&#19987;&#23478;&#35774;&#35745;&#30340;&#37197;&#33394;&#26041;&#26696;&#36827;&#34892;&#25490;&#24207;&#65292;&#20801;&#35768;&#22312;&#33394;&#24425;&#35774;&#35745;&#26041;&#38754;&#26159;&#26032;&#25163;&#30340;&#20998;&#26512;&#24072;&#26681;&#25454;&#20854;&#25968;&#25454;&#32972;&#26223;&#23450;&#21046;&#37197;&#33394;&#26041;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#37197;&#33394;&#26041;&#26696;&#30340;&#35774;&#35745;&#35270;&#20026;&#22312;CIELAB&#39068;&#33394;&#31354;&#38388;&#20013;&#30340;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#19978;&#19979;&#25991;&#29305;&#23450;&#30340;&#22870;&#21169;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#22312;&#19982;&#21313;&#20108;&#21517;&#31185;&#23398;&#23478;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;Cieran&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#29992;&#25143;&#30340;&#20559;&#22909;&#26469;&#25490;&#21517;&#37197;&#33394;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#27169;&#22411;&#21019;&#24314;&#20102;&#26032;&#30340;&#20248;&#36136;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15997v1 Announce Type: cross  Abstract: Quality colormaps can help communicate important data patterns. However, finding an aesthetically pleasing colormap that looks "just right" for a given scenario requires significant design and technical expertise. We introduce Cieran, a tool that allows any data analyst to rapidly find quality colormaps while designing charts within Jupyter Notebooks. Our system employs an active preference learning paradigm to rank expert-designed colormaps and create new ones from pairwise comparisons, allowing analysts who are novices in color design to tailor colormaps to their data context. We accomplish this by treating colormap design as a path planning problem through the CIELAB colorspace with a context-specific reward model. In an evaluation with twelve scientists, we found that Cieran effectively modeled user preferences to rank colormaps and leveraged this model to create new quality designs. Our work shows the potential of active preferenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15160</link><description>&lt;p&gt;
&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#30340;&#21464;&#21387;&#22120;&#35760;&#24518;&#20307;&#29992;&#20110;&#20307;&#39564;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Spatially-Aware Transformer Memory for Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#33410;&#35760;&#24518;&#22312;&#21508;&#31181;&#35748;&#30693;&#36807;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#27604;&#22914;&#33021;&#22815;&#22312;&#22836;&#33041;&#20013;&#22238;&#24518;&#36807;&#21435;&#20107;&#20214;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#35748;&#30693;&#31185;&#23398;&#24378;&#35843;&#31354;&#38388;&#19978;&#19979;&#25991;&#22312;&#24773;&#33410;&#35760;&#24518;&#30340;&#24418;&#25104;&#21644;&#26816;&#32034;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#24403;&#21069;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#24773;&#33410;&#35760;&#24518;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#36890;&#36807;&#23384;&#20648;&#26102;&#38388;&#39034;&#24207;&#20307;&#39564;&#30340;&#21464;&#21387;&#22120;&#65292;&#36825;&#24573;&#30053;&#20102;&#31354;&#38388;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#23558;&#22522;&#30784;&#32467;&#26500;&#25193;&#23637;&#21040;&#38500;&#20102;&#20165;&#26377;&#26102;&#38388;&#39034;&#24207;&#20043;&#22806;&#30340;&#31354;&#38388;&#36724;&#65292;&#24182;&#30001;&#27492;&#33021;&#22815;&#33719;&#24471;&#21738;&#20123;&#22909;&#22788;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#32771;&#34385;&#26102;&#38388;&#21644;&#31354;&#38388;&#32500;&#24230;&#30340;&#22330;&#25152;&#20013;&#24515;&#24773;&#33410;&#35760;&#24518;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#65292;&#23548;&#33268;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15160v1 Announce Type: cross  Abstract: Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanc
&lt;/p&gt;</description></item><item><title>&#35843;&#26597;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#23545;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#34920;&#29616;&#26368;&#19981;&#21033;&#12290;&#23457;&#35745;&#22312;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#24378;&#35843;&#12290;</title><link>https://arxiv.org/abs/2402.14875</link><description>&lt;p&gt;
&#21517;&#23383;&#30340;&#21547;&#20041;&#26159;&#20160;&#20040;&#65311;&#23457;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
What's in a Name? Auditing Large Language Models for Race and Gender Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14875
&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#23545;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#34920;&#29616;&#26368;&#19981;&#21033;&#12290;&#23457;&#35745;&#22312;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#24378;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37319;&#29992;&#23457;&#35745;&#35774;&#35745;&#26469;&#35843;&#26597;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#21253;&#25324;GPT-4&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#21457;&#27169;&#22411;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#20026;&#20010;&#20154;&#25552;&#20379;&#24314;&#35758;&#65292;&#27604;&#22914;&#22312;&#36141;&#36710;&#35848;&#21028;&#25110;&#36873;&#20030;&#32467;&#26524;&#39044;&#27979;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#24314;&#35758;&#31995;&#32479;&#24615;&#22320;&#23545;&#19982;&#31181;&#26063;&#23569;&#25968;&#32676;&#20307;&#21644;&#22899;&#24615;&#24120;&#35265;&#30456;&#20851;&#30340;&#21517;&#23383;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#24471;&#21040;&#30340;&#32467;&#26524;&#26368;&#19981;&#21033;&#12290;&#36825;&#20123;&#20559;&#35265;&#22312;42&#20010;&#25552;&#31034;&#27169;&#26495;&#21644;&#22810;&#20010;&#27169;&#22411;&#20013;&#37117;&#26159;&#19968;&#33268;&#30340;&#65292;&#34920;&#26126;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#24615;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#23396;&#31435;&#20107;&#20214;&#12290;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#25968;&#20540;&#12289;&#19982;&#20915;&#31574;&#30456;&#20851;&#30340;&#38170;&#28857;&#21487;&#20197;&#25104;&#21151;&#25269;&#28040;&#20559;&#35265;&#65292;&#32780;&#23450;&#24615;&#32454;&#33410;&#30340;&#24433;&#21709;&#24182;&#19981;&#19968;&#33268;&#65292;&#29978;&#33267;&#21487;&#33021;&#20250;&#21152;&#21095;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#36827;&#34892;&#23457;&#35745;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#20943;&#36731;&#20854;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14875v1 Announce Type: cross  Abstract: We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4. In our study, we elicit prompt the models for advice regarding an individual across a variety of scenarios, such as during car purchase negotiations or election outcome predictions. We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women. Names associated with Black women receive the least advantageous outcomes. The biases are consistent across 42 prompt templates and several models, indicating a systemic issue rather than isolated incidents. While providing numerical, decision-relevant anchors in the prompt can successfully counteract the biases, qualitative details have inconsistent effects and may even increase disparities. Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;SMDP&#65289;&#36866;&#24212;&#29616;&#23454;&#22330;&#26223;&#20013;&#38543;&#26426;&#35831;&#27714;&#21040;&#36798;&#30340;&#29305;&#24615;&#65292;&#32508;&#21512;&#32771;&#34385;&#21508;&#31181;&#25991;&#20214;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.14576</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#36793;&#32536;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Edge Caching Based on Deep Reinforcement Learning and Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;SMDP&#65289;&#36866;&#24212;&#29616;&#23454;&#22330;&#26223;&#20013;&#38543;&#26426;&#35831;&#27714;&#21040;&#36798;&#30340;&#29305;&#24615;&#65292;&#32508;&#21512;&#32771;&#34385;&#21508;&#31181;&#25991;&#20214;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#32593;&#32476;&#20013;&#20887;&#20313;&#25968;&#25454;&#20256;&#36755;&#26085;&#30410;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;&#27969;&#37327;&#28608;&#22686;&#24050;&#32463;&#20351;&#20013;&#32487;&#38142;&#36335;&#21644;&#39592;&#24178;&#32593;&#32476;&#25215;&#21387;&#65292;&#20419;&#20351;&#23545;&#36793;&#32536;&#36335;&#30001;&#22120;&#30340;&#32531;&#23384;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25506;&#32034;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#20381;&#36182;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#22788;&#29702;&#32531;&#23384;&#38382;&#39064;&#65292;&#20551;&#35774;&#22266;&#23450;&#26102;&#38388;&#38388;&#38548;&#30340;&#20915;&#31574;&#65307;&#28982;&#32780;&#65292;&#29616;&#23454;&#22330;&#26223;&#28041;&#21450;&#38543;&#26426;&#35831;&#27714;&#21040;&#36798;&#65292;&#23613;&#31649;&#21508;&#31181;&#25991;&#20214;&#29305;&#24449;&#22312;&#30830;&#23450;&#26368;&#20339;&#32531;&#23384;&#31574;&#30053;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#30456;&#20851;&#30340;&#29616;&#26377;&#24037;&#20316;&#24182;&#26410;&#32771;&#34385;&#25152;&#26377;&#36825;&#20123;&#25991;&#20214;&#29305;&#24449;&#26469;&#24418;&#25104;&#32531;&#23384;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#39318;&#20808;&#25105;&#20204;&#21033;&#29992;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;SMDP&#65289;&#26469;&#24314;&#27169;&#32531;&#23384;&#38382;&#39064;&#65292;&#20197;&#36866;&#24212;&#29616;&#23454;&#22330;&#26223;&#30340;&#36830;&#32493;&#26102;&#38388;&#29305;&#24615;&#65292;&#20801;&#35768;&#22312;&#25991;&#20214;&#35831;&#27714;&#26102;&#38543;&#26426;&#36827;&#34892;&#32531;&#23384;&#20915;&#31574;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#19981;&#21516;&#25991;&#20214;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14576v1 Announce Type: cross  Abstract: This paper addresses the escalating challenge of redundant data transmission in networks. The surge in traffic has strained backhaul links and backbone networks, prompting the exploration of caching solutions at the edge router. Existing work primarily relies on Markov Decision Processes (MDP) for caching issues, assuming fixed-time interval decisions; however, real-world scenarios involve random request arrivals, and despite the critical role of various file characteristics in determining an optimal caching policy, none of the related existing work considers all these file characteristics in forming a caching policy. In this paper, first, we formulate the caching problem using a semi-Markov Decision Process (SMDP) to accommodate the continuous-time nature of real-world scenarios allowing for caching decisions at random times upon file requests. Then, we propose a double deep Q-learning-based caching approach that comprehensively accou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30828;&#26679;&#26412;&#21152;&#26435;&#25345;&#32493;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;IR-DRO&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#20248;&#20808;&#32771;&#34385;&#35757;&#32451;&#20013;&#20449;&#24687;&#20016;&#23500;&#30340;&#26679;&#26412;&#65292;&#20197;&#25913;&#21892;LLM&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14270</link><description>&lt;p&gt;
&#29275;&#22836;&#35282;&#65306;&#30828;&#26679;&#26412;&#21152;&#26435;&#25345;&#32493;&#35757;&#32451;&#25913;&#21892;LLM&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14270
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30828;&#26679;&#26412;&#21152;&#26435;&#25345;&#32493;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;IR-DRO&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#20248;&#20808;&#32771;&#34385;&#35757;&#32451;&#20013;&#20449;&#24687;&#20016;&#23500;&#30340;&#26679;&#26412;&#65292;&#20197;&#25913;&#21892;LLM&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22312;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30701;&#32570;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#19968;&#20010;&#36731;&#37327;&#32423;&#25345;&#32493;&#35757;&#32451;LLMs&#30340;&#32463;&#39564;&#31574;&#30053;&#24320;&#22987;&#65292;&#20351;&#29992;&#23427;&#20204;&#30340;&#21407;&#22987;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#20851;&#27880;&#23548;&#33268;&#20013;&#31561;&#25439;&#22833;&#30340;&#26679;&#26412;&#30340;&#36873;&#25321;&#24615;&#20445;&#30041;&#12290;&#36825;&#20123;&#26679;&#26412;&#34987;&#35748;&#20026;&#26159;&#20449;&#24687;&#20016;&#23500;&#30340;&#65292;&#24182;&#19988;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#19982;&#26368;&#39640;&#25439;&#22833;&#30340;&#26679;&#26412;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#23558;&#30001;&#20110;&#19982;&#25968;&#25454;&#22122;&#22768;&#21644;&#22797;&#26434;&#24615;&#30340;&#30456;&#20851;&#24615;&#32780;&#34987;&#20002;&#24323;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#31574;&#30053;&#24418;&#24335;&#21270;&#20026;&#22522;&#20110;&#23454;&#20363;&#21152;&#26435;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;IR-DRO&#65289;&#30340;&#21407;&#21017;&#26694;&#26550;&#12290;IR-DRO&#26088;&#22312;&#36890;&#36807;&#23454;&#20363;&#37325;&#26032;&#21152;&#26435;&#26426;&#21046;&#21160;&#24577;&#20248;&#20808;&#32771;&#34385;&#35757;&#32451;&#30340;&#37325;&#28857;&#26679;&#26412;&#65292;&#30001;&#19968;&#20010;&#23553;&#38381;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#31616;&#21270;&#65292;&#20197;&#20415;&#36731;&#26494;&#25972;&#21512;&#21040;&#24050;&#24314;&#31435;&#30340;&#35757;&#32451;&#21327;&#35758;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14270v1 Announce Type: new  Abstract: In the rapidly advancing arena of large language models (LLMs), a key challenge is to enhance their capabilities amid a looming shortage of high-quality training data. Our study starts from an empirical strategy for the light continual training of LLMs using their original pre-training data sets, with a specific focus on selective retention of samples that incur moderately high losses. These samples are deemed informative and beneficial for model refinement, contrasting with the highest-loss samples, which would be discarded due to their correlation with data noise and complexity. We then formalize this strategy into a principled framework of Instance-Reweighted Distributionally Robust Optimization (IR-DRO). IR-DRO is designed to dynamically prioritize the training focus on informative samples through an instance reweighting mechanism, streamlined by a closed-form solution for straightforward integration into established training protoco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;GANs&#29983;&#25104;&#20102;&#26102;&#38388;&#24207;&#21015;&#21512;&#25104;&#30196;&#21574;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;GAN&#27169;&#22411;&#22312;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#24182;&#24310;&#20280;&#25968;&#25454;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14042</link><description>&lt;p&gt;
&#20351;&#29992;GANs&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#24310;&#20280;&#19982;&#20445;&#25252;&#8212;&#8212;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#21307;&#30103;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Protect and Extend -- Using GANs for Synthetic Data Generation of Time-Series Medical Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;GANs&#29983;&#25104;&#20102;&#26102;&#38388;&#24207;&#21015;&#21512;&#25104;&#30196;&#21574;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;GAN&#27169;&#22411;&#22312;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#24182;&#24310;&#20280;&#25968;&#25454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14042v1 &#20844;&#21578;&#31867;&#22411;:&#20132;&#21449;&#25688;&#35201;: &#20445;&#25252;&#31169;&#20154;&#29992;&#25143;&#25968;&#25454;&#23545;&#20110;&#39640;&#36136;&#37327;&#20307;&#39564;(QoE)&#21644;&#21487;&#25509;&#21463;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22788;&#29702;&#25935;&#24863;&#25968;&#25454;&#30340;&#26381;&#21153;&#65292;&#22914;&#22522;&#20110;IT&#30340;&#20581;&#24247;&#26381;&#21153;&#12290;&#23613;&#31649;&#24050;&#32463;&#26174;&#31034;&#21311;&#21517;&#21270;&#25216;&#26415;&#23481;&#26131;&#34987;&#25968;&#25454;&#37325;&#26032;&#35782;&#21035;&#65292;&#20294;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36880;&#28176;&#21462;&#20195;&#20102;&#21311;&#21517;&#21270;&#65292;&#22240;&#20026;&#23427;&#30456;&#23545;&#32791;&#26102;&#21644;&#36164;&#28304;&#32791;&#36153;&#36739;&#23569;&#65292;&#24182;&#19988;&#26356;&#33021;&#25269;&#25239;&#25968;&#25454;&#27844;&#28431;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#24050;&#34987;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#26159;&#36981;&#24490;&#24046;&#20998;&#38544;&#31169;&#29616;&#35937;&#30340;GAN&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#21512;&#25104;&#30196;&#21574;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30340;&#26368;&#26032;GAN&#22522;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#22312;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20998;&#21457;&#12290; &#39044;&#27979;&#24314;&#27169;&#12289;&#33258;&#30456;&#20851;&#24615;&#21644;&#20998;&#24067;&#20998;&#26512;&#34987;&#29992;&#26469;&#35780;&#20272;&#29983;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#36136;&#37327;(QoG)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14042v1 Announce Type: cross  Abstract: Preservation of private user data is of paramount importance for high Quality of Experience (QoE) and acceptability, particularly with services treating sensitive data, such as IT-based health services. Whereas anonymization techniques were shown to be prone to data re-identification, synthetic data generation has gradually replaced anonymization since it is relatively less time and resource-consuming and more robust to data leakage. Generative Adversarial Networks (GANs) have been used for generating synthetic datasets, especially GAN frameworks adhering to the differential privacy phenomena. This research compares state-of-the-art GAN-based models for synthetic data generation to generate time-series synthetic medical records of dementia patients which can be distributed without privacy concerns. Predictive modeling, autocorrelation, and distribution analysis are used to assess the Quality of Generating (QoG) of the generated data. T
&lt;/p&gt;</description></item><item><title>E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.14041</link><description>&lt;p&gt;
E2USD&#65306;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#29366;&#24577;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14041
&lt;/p&gt;
&lt;p&gt;
E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;E2USD&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#12290;E2USD&#21033;&#29992;&#22522;&#20110;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#30340;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#22120;(FFTCompress)&#21644;&#20998;&#35299;&#30340;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;(DDEM)&#65292;&#19968;&#36215;&#20197;&#20302;&#35745;&#31639;&#24320;&#38144;&#23545;&#36755;&#20837;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#38452;&#24615;&#21462;&#28040;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(FNCCLearning)&#65292;&#20197;&#25269;&#28040;&#20551;&#38452;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#26356;&#21451;&#22909;&#30340;&#31751;&#23884;&#20837;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#38408;&#20540;&#26816;&#27979;(ADATD)&#12290;&#36890;&#36807;&#20351;&#29992;&#20845;&#20010;&#22522;&#32447;&#27169;&#22411;&#21644;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;E2USD&#33021;&#22815;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;SOTA&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/AI4CTS/E2Usd &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14041v1 Announce Type: cross  Abstract: We propose E2USD that enables efficient-yet-accurate unsupervised MTS state detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor (FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together encode input MTSs at low computational overhead. Additionally, we propose a False Negative Cancellation Contrastive Learning method (FNCCLearning) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2USD is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at https://github.com/AI4CTS/E2Usd.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#20113;&#36827;&#34892;&#20998;&#21106;&#65292;&#26088;&#22312;&#25552;&#39640;&#21355;&#26143;&#22270;&#20687;&#20998;&#26512;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24212;&#29992;&#20110;&#29615;&#22659;&#30417;&#27979;&#12289;&#36164;&#28304;&#31649;&#29702;&#21644;&#28798;&#23475;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.13918</link><description>&lt;p&gt;
BenchCloudVision: &#20113;&#26816;&#27979;&#21644;&#20998;&#21106;&#20013;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#20113;&#36827;&#34892;&#20998;&#21106;&#65292;&#26088;&#22312;&#25552;&#39640;&#21355;&#26143;&#22270;&#20687;&#20998;&#26512;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24212;&#29992;&#20110;&#29615;&#22659;&#30417;&#27979;&#12289;&#36164;&#28304;&#31649;&#29702;&#21644;&#28798;&#23475;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37197;&#22791;&#20809;&#23398;&#20256;&#24863;&#22120;&#30340;&#21355;&#26143;&#25429;&#33719;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#20026;&#21508;&#31181;&#29615;&#22659;&#29616;&#35937;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#36965;&#24863;&#20013;&#19968;&#20123;&#25361;&#25112;&#30340;&#30740;&#31350;&#28608;&#22686;&#65292;&#20174;&#19981;&#21516;&#26223;&#35266;&#20013;&#30340;&#27700;&#26816;&#27979;&#21040;&#23665;&#22320;&#21644;&#22320;&#24418;&#30340;&#20998;&#21106;&#12290;&#27491;&#22312;&#36827;&#34892;&#30340;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#21355;&#26143;&#22270;&#20687;&#20998;&#26512;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;&#23588;&#20854;&#26159;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#37325;&#28857;&#25918;&#22312;&#24320;&#21457;&#20934;&#30830;&#30340;&#27700;&#20307;&#26816;&#27979;&#12289;&#38634;&#21644;&#20113;&#30340;&#26041;&#27861;&#19978;&#65292;&#36825;&#20123;&#23545;&#29615;&#22659;&#30417;&#27979;&#12289;&#36164;&#28304;&#31649;&#29702;&#21644;&#28798;&#23475;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#19987;&#27880;&#20110;&#20174;&#36965;&#24863;&#22270;&#20687;&#20013;&#20998;&#21106;&#20113;&#12290;&#30001;&#20110;&#20809;&#23398;&#20256;&#24863;&#22120;&#24212;&#29992;&#20013;&#20113;&#30340;&#23384;&#22312;&#65292;&#20934;&#30830;&#30340;&#36965;&#24863;&#25968;&#25454;&#20998;&#26512;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#20135;&#21697;&#65288;&#22914;&#24212;&#29992;&#21644;&#30740;&#31350;&#65289;&#30340;&#36136;&#37327;&#30452;&#25509;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13918v1 Announce Type: cross  Abstract: Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena. In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains. Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis. Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response. Within this context, this paper focus on the cloud segmentation from remote sensing imagery. Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications. The quality of resulting products such as applications and research is directl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#24179;&#22343;&#26657;&#20934;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35823;&#24046;&#19982;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#24046;&#20540;&#21644;&#23558;&#24179;&#22343;&#24179;&#26041;z-&#20998;&#25968;&#19982;1&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21069;&#32773;&#23545;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#25935;&#24863;&#65292;&#32780;&#21518;&#32773;&#22312;&#35813;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10043</link><description>&lt;p&gt;
&#22914;&#20309;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#30340;&#24179;&#22343;&#26657;&#20934;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to validate average calibration for machine learning regression tasks ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#24179;&#22343;&#26657;&#20934;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35823;&#24046;&#19982;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#24046;&#20540;&#21644;&#23558;&#24179;&#22343;&#24179;&#26041;z-&#20998;&#25968;&#19982;1&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21069;&#32773;&#23545;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#25935;&#24863;&#65292;&#32780;&#21518;&#32773;&#22312;&#35813;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#30340;&#24179;&#22343;&#26657;&#20934;&#24615;&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#36827;&#34892;&#27979;&#35797;&#12290;&#19968;&#31181;&#26041;&#24335;&#26159;&#23558;&#26657;&#20934;&#35823;&#24046;&#65288;CE&#65289;&#20272;&#35745;&#20026;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MSE&#65289;&#19982;&#24179;&#22343;&#26041;&#24046;&#65288;MV&#65289;&#25110;&#24179;&#22343;&#24179;&#26041;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#24046;&#20540;&#12290;&#21478;&#19968;&#31181;&#26041;&#24335;&#26159;&#23558;&#24179;&#22343;&#24179;&#26041;z-&#20998;&#25968;&#25110;&#32553;&#25918;&#35823;&#24046;&#65288;ZMS&#65289;&#19982;1&#36827;&#34892;&#27604;&#36739;&#12290;&#20004;&#31181;&#26041;&#27861;&#21487;&#33021;&#24471;&#20986;&#19981;&#21516;&#30340;&#32467;&#35770;&#65292;&#27491;&#22914;&#26469;&#33258;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25991;&#29486;&#20013;&#30340;&#25968;&#25454;&#38598;&#38598;&#21512;&#25152;&#31034;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;CE&#23545;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#38750;&#24120;&#25935;&#24863;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#31163;&#32676;&#19981;&#30830;&#23450;&#24615;&#30340;&#23384;&#22312;&#65292;&#22240;&#27492;&#26080;&#27861;&#21487;&#38752;&#22320;&#29992;&#20110;&#26657;&#20934;&#27979;&#35797;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;ZMS&#32479;&#35745;&#37327;&#19981;&#20855;&#26377;&#36825;&#31181;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#23545;&#26465;&#20214;&#26657;&#20934;&#39564;&#35777;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10043v1 Announce Type: cross  Abstract: Average calibration of the uncertainties of machine learning regression tasks can be tested in two ways. One way is to estimate the calibration error (CE) as the difference between the mean absolute error (MSE) and the mean variance (MV) or mean squared uncertainty. The alternative is to compare the mean squared z-scores or scaled errors (ZMS) to 1. Both approaches might lead to different conclusion, as illustrated on an ensemble of datasets from the recent machine learning uncertainty quantification literature. It is shown here that the CE is very sensitive to the distribution of uncertainties, and notably to the presence of outlying uncertainties, and that it cannot be used reliably for calibration testing. By contrast, the ZMS statistic does not present this sensitivity issue and offers the most reliable approach in this context. Implications for the validation of conditional calibration are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pathformer&#30340;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36317;&#31163;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#36335;&#24452;&#26469;&#20248;&#21270;&#24314;&#27169;&#36807;&#31243;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05956</link><description>&lt;p&gt;
Pathformer: &#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pathformer: Multi-scale transformers with Adaptive Pathways for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pathformer&#30340;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36317;&#31163;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#36335;&#24452;&#26469;&#20248;&#21270;&#24314;&#27169;&#36807;&#31243;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20174;&#26377;&#38480;&#25110;&#22266;&#23450;&#23610;&#24230;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#20351;&#24471;&#25429;&#25417;&#36328;&#22810;&#20010;&#23610;&#24230;&#30340;&#19981;&#21516;&#29305;&#24449;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#65288;Pathformer&#65289;&#30340;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21516;&#26102;&#25972;&#21512;&#20102;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36317;&#31163;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#12290;&#22810;&#23610;&#24230;&#21010;&#20998;&#36816;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#22359;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#25104;&#19981;&#21516;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#22522;&#20110;&#27599;&#20010;&#23610;&#24230;&#30340;&#21010;&#20998;&#65292;&#23545;&#36825;&#20123;&#25968;&#25454;&#22359;&#36827;&#34892;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#25429;&#25417;&#20840;&#23616;&#30456;&#20851;&#24615;&#21644;&#23616;&#37096;&#32454;&#33410;&#20316;&#20026;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#33258;&#36866;&#24212;&#36335;&#24452;&#26469;&#20016;&#23500;&#22810;&#23610;&#24230;Transformer&#65292;&#35813;&#36335;&#24452;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#20013;&#19981;&#26029;&#21464;&#21270;&#30340;&#26102;&#38388;&#21160;&#24577;&#35843;&#25972;&#22810;&#23610;&#24230;&#24314;&#27169;&#36807;&#31243;&#65292;&#25552;&#39640;Pathformer&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;11&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have achieved some success in time series forecasting. Existing methods mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. In this paper, we propose multi-scale transformers with adaptive pathways (Pathformer). The proposed Transformer integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multi-scale transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics in the input time series, improving the prediction accuracy and generalization of Pathformer. Extensive experiments on eleven rea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#32858;&#21512;&#38388;&#25509;&#25512;&#29702;&#36335;&#24452;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03268</link><description>&lt;p&gt;
&#20174;&#25512;&#29702;&#36335;&#24452;&#32858;&#21512;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#32858;&#21512;&#38388;&#25509;&#25512;&#29702;&#36335;&#24452;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26126;&#30830;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#29702;&#35299;&#39044;&#35757;&#32451;&#19982;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#30340;&#20851;&#31995;&#22914;&#20309;&#20419;&#20351;&#25512;&#29702;&#33021;&#21147;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#21487;&#20197;&#23558;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#22312;&#39044;&#35757;&#32451;&#26102;&#36890;&#36807;&#32858;&#21512;&#38388;&#25509;&#30340;&#25512;&#29702;&#36335;&#24452;&#26469;&#24471;&#20986;&#26032;&#32467;&#35770;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20010;&#35270;&#35282;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#31561;&#20851;&#38190;&#24773;&#20917;&#19979;&#38750;&#24120;&#26377;&#25928;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#29702;&#36335;&#24452;&#24418;&#24335;&#21270;&#20026;&#22312;&#30693;&#35782;/&#25512;&#29702;&#22270;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#12290;&#23545;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#24067;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30456;&#20851;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#27010;&#29575;&#30340;&#21152;&#26435;&#21644;&#26159;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21512;&#29702;&#26041;&#24335;&#12290;&#23545;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#35757;&#32451;&#23545;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#36947;&#24503;&#24773;&#26223;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#22312;&#20116;&#20010;LLMs&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20026;&#30740;&#31350;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.01719</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#36947;&#24503;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring Moral Inconsistencies in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#36947;&#24503;&#24773;&#26223;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#22312;&#20116;&#20010;LLMs&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20026;&#30740;&#31350;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#35821;&#20041;&#31561;&#20215;&#30340;&#25552;&#31034;&#20135;&#29983;&#35821;&#20041;&#31561;&#20215;&#30340;&#21709;&#24212;&#65292;&#37027;&#20040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#34987;&#35748;&#20026;&#26159;&#19968;&#33268;&#30340;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;LLMs&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#29983;&#25104;&#26041;&#38754;&#20063;&#23384;&#22312;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#23545;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#20934;&#30830;&#24230;&#26469;&#34913;&#37327;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#27809;&#26377;&#8220;&#27491;&#30830;&#8221;&#31572;&#26696;&#30340;&#36947;&#24503;&#24773;&#26223;&#65288;&#20363;&#22914;&#65292;&#36947;&#36335;&#20132;&#36816;&#38382;&#39064;&#65289;&#26159;&#19981;&#21512;&#36866;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SGE&#65289;&#65292;&#26469;&#34913;&#37327;LLM&#22312;&#36947;&#24503;&#24773;&#26223;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#8220;&#32463;&#39564;&#27861;&#21017;&#8221;&#65288;RoTs&#65289;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;SGE&#19982;&#20154;&#31867;&#21028;&#26029;&#22312;&#20116;&#20010;LLMs&#19978;&#26356;&#22909;&#22320;&#30456;&#20851;&#12290;&#22312;&#26410;&#26469;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;LLM&#19981;&#19968;&#33268;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Large Language Model~(LLM) is considered consistent if semantically equivalent prompts produce semantically equivalent responses. Despite recent advancements showcasing the impressive capabilities of LLMs in conversational systems, we show that even state-of-the-art LLMs are highly inconsistent in their generations, questioning their reliability. Prior research has tried to measure this with task-specific accuracies. However, this approach is unsuitable for moral scenarios, such as the trolley problem, with no ``correct'' answer. To address this issue, we propose a novel information-theoretic measure called Semantic Graph Entropy~(SGE) to measure the consistency of an LLM in moral scenarios. We leverage ``Rules of Thumb''~(RoTs) to explain a model's decision-making strategies and further enhance our metric. Compared to existing consistency metrics, SGE correlates better with human judgments across five LLMs. In the future, we aim to investigate the root causes of LLM inconsistencies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;AI&#27494;&#22120;&#30340;&#27010;&#24565;&#12289;&#37096;&#32626;&#12289;&#26816;&#27979;&#21644;&#28508;&#22312;&#23545;&#31574;&#65292;&#24378;&#35843;&#20102;&#22312;&#20449;&#24687;&#39046;&#22495;&#20869;&#22522;&#20110;AI&#30340;&#24515;&#29702;&#25805;&#32437;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#20854;&#23545;&#20840;&#29699;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#31038;&#20250;&#30340;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.01663</link><description>&lt;p&gt;
&#26432;&#25163;&#32423;&#24212;&#29992;&#65306;&#20302;&#36895;&#22823;&#35268;&#27169;AI&#27494;&#22120;
&lt;/p&gt;
&lt;p&gt;
Killer Apps: Low-Speed, Large-Scale AI Weapons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;AI&#27494;&#22120;&#30340;&#27010;&#24565;&#12289;&#37096;&#32626;&#12289;&#26816;&#27979;&#21644;&#28508;&#22312;&#23545;&#31574;&#65292;&#24378;&#35843;&#20102;&#22312;&#20449;&#24687;&#39046;&#22495;&#20869;&#22522;&#20110;AI&#30340;&#24515;&#29702;&#25805;&#32437;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#20854;&#23545;&#20840;&#29699;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#31038;&#20250;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#29305;&#21035;&#26159;&#30001;OpenAI&#12289;Meta&#21644;Anthropic&#31561;&#32452;&#32455;&#24320;&#21457;&#30340;&#23574;&#31471;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#32473;&#25112;&#20105;&#21644;&#23433;&#20840;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#30446;&#21069;&#20851;&#27880;&#30340;&#20027;&#35201;&#26159;AI&#22312;&#27494;&#22120;&#31995;&#32479;&#20013;&#30340;&#25972;&#21512;&#20197;&#21450;&#22312;&#21160;&#33021;&#20914;&#31361;&#20013;&#24555;&#36895;&#20915;&#31574;&#20013;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21516;&#26679;&#37325;&#35201;&#20294;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;&#22312;&#20449;&#24687;&#39046;&#22495;&#20013;&#22522;&#20110;AI&#30340;&#24515;&#29702;&#25805;&#32437;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#20869;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#21487;&#33021;&#23545;&#20840;&#29699;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#31038;&#20250;&#36896;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;AI&#27494;&#22120;&#30340;&#27010;&#24565;&#12289;&#37096;&#32626;&#12289;&#26816;&#27979;&#21644;&#28508;&#22312;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accelerating advancements in Artificial Intelligence (AI) and Machine Learning (ML), highlighted by the development of cutting-edge Generative Pre-trained Transformer (GPT) models by organizations such as OpenAI, Meta, and Anthropic, present new challenges and opportunities in warfare and security. Much of the current focus is on AI's integration within weapons systems and its role in rapid decision-making in kinetic conflict. However, an equally important but often overlooked aspect is the potential of AI-based psychological manipulation at internet scales within the information domain. These capabilities could pose significant threats to individuals, organizations, and societies globally. This paper explores the concept of AI weapons, their deployment, detection, and potential countermeasures.
&lt;/p&gt;</description></item><item><title>&#23558;RNA&#35774;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26399;&#26395;&#20998;&#21306;&#20989;&#25968;&#30340;&#36890;&#29992;&#20248;&#21270;&#26694;&#26550;&#65292;&#23558;&#20505;&#36873;&#24207;&#21015;&#30340;&#20998;&#24067;&#36880;&#27493;&#20248;&#21270;&#20026;&#21333;&#19968;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2401.00037</link><description>&lt;p&gt;
&#36890;&#36807;&#26399;&#26395;&#20998;&#21306;&#20989;&#25968;&#21644;&#36830;&#32493;&#20248;&#21270;&#35774;&#35745;&#20449;&#20351;RNA
&lt;/p&gt;
&lt;p&gt;
Messenger RNA Design via Expected Partition Function and Continuous Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00037
&lt;/p&gt;
&lt;p&gt;
&#23558;RNA&#35774;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26399;&#26395;&#20998;&#21306;&#20989;&#25968;&#30340;&#36890;&#29992;&#20248;&#21270;&#26694;&#26550;&#65292;&#23558;&#20505;&#36873;&#24207;&#21015;&#30340;&#20998;&#24067;&#36880;&#27493;&#20248;&#21270;&#20026;&#21333;&#19968;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RNA&#35774;&#35745;&#30340;&#20219;&#21153;&#26159;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#65292;&#26377;&#20960;&#20010;&#29256;&#26412;&#30340;&#36825;&#20123;&#38382;&#39064;&#26159;NP&#38590;&#39064;&#12290;&#20026;&#20102;&#26367;&#20195;&#24120;&#29992;&#30340;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#38382;&#39064;&#34920;&#36848;&#20026;&#36830;&#32493;&#20248;&#21270;&#65292;&#24182;&#22522;&#20110;&#19968;&#31181;&#31216;&#20026;"&#26399;&#26395;&#20998;&#21306;&#20989;&#25968;"&#30340;&#32463;&#20856;&#20998;&#21306;&#20989;&#25968;&#30340;&#27867;&#21270;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#20174;&#25152;&#26377;&#21487;&#33021;&#30340;&#20505;&#36873;&#24207;&#21015;&#20013;&#24320;&#22987;&#19968;&#20010;&#20998;&#24067;&#65292;&#24182;&#23558;&#30446;&#26631;&#20989;&#25968;&#20174;&#19968;&#20010;&#24207;&#21015;&#25193;&#23637;&#21040;&#19968;&#20010;&#20998;&#24067;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#26041;&#27861;&#25913;&#21892;&#25193;&#23637;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20998;&#24067;&#23558;&#36880;&#28176;&#25910;&#32553;&#21040;&#19968;&#20010;&#29420;&#28909;&#24207;&#21015;&#65288;&#21363;&#19968;&#20010;&#24207;&#21015;&#65289;&#12290;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#30123;&#33495;&#21644;&#27835;&#30103;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#24212;&#29992;&#30340;mRNA&#35774;&#35745;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00037v2 Announce Type: replace-cross  Abstract: The tasks of designing RNAs are discrete optimization problems, and several versions of these problems are NP-hard. As an alternative to commonly used local search methods, we formulate these problems as continuous optimization and develop a general framework for this optimization based on a generalization of classical partition function which we call "expected partition function". The basic idea is to start with a distribution over all possible candidate sequences, and extend the objective function from a sequence to a distribution. We then use gradient descent-based optimization methods to improve the extended objective function, and the distribution will gradually shrink towards a one-hot sequence (i.e., a single sequence). As a case study, we consider the important problem of mRNA design with wide applications in vaccines and therapeutics. While the recent work of LinearDesign can efficiently optimize mRNAs for minimum free
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#25918;&#23485;&#26465;&#20214;&#19979;&#30340;&#20998;&#21306;&#20998;&#31867;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#25552;&#20986;&#20102;&#32477;&#23545;&#36830;&#32493;&#20998;&#37327;&#30340;&#26032;&#29305;&#24615;&#65292;&#35745;&#31639;&#20102;&#20998;&#31867;&#38169;&#35823;&#27010;&#29575;&#30340;&#31934;&#30830;&#25910;&#25947;&#29575;</title><link>https://arxiv.org/abs/2312.14889</link><description>&lt;p&gt;
&#35770;&#20174;&#21487;&#35266;&#27979;&#21644;&#31169;&#23494;&#25968;&#25454;&#20013;&#23454;&#29616;&#36895;&#29575;&#26368;&#20248;&#20998;&#21306;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
On Rate-Optimal Partitioning Classification from Observable and from Privatised Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14889
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#25918;&#23485;&#26465;&#20214;&#19979;&#30340;&#20998;&#21306;&#20998;&#31867;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#25552;&#20986;&#20102;&#32477;&#23545;&#36830;&#32493;&#20998;&#37327;&#30340;&#26032;&#29305;&#24615;&#65292;&#35745;&#31639;&#20102;&#20998;&#31867;&#38169;&#35823;&#27010;&#29575;&#30340;&#31934;&#30830;&#25910;&#25947;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20998;&#21306;&#20998;&#31867;&#30340;&#32463;&#20856;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25918;&#23485;&#26465;&#20214;&#19979;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21253;&#25324;&#21487;&#35266;&#27979;&#65288;&#38750;&#31169;&#23494;&#65289;&#21644;&#31169;&#23494;&#25968;&#25454;&#12290;&#25105;&#20204;&#20551;&#35774;&#29305;&#24449;&#21521;&#37327;$X$&#21462;&#20540;&#20110;$\mathbb{R}^d$&#65292;&#20854;&#26631;&#31614;&#20026;$Y$&#12290;&#20043;&#21069;&#20851;&#20110;&#20998;&#21306;&#20998;&#31867;&#22120;&#30340;&#32467;&#26524;&#22522;&#20110;&#24378;&#23494;&#24230;&#20551;&#35774;&#65292;&#36825;&#31181;&#20551;&#35774;&#38480;&#21046;&#36739;&#22823;&#65292;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#20363;&#23376;&#21152;&#20197;&#35777;&#26126;&#12290;&#25105;&#20204;&#20551;&#35774;$X$&#30340;&#20998;&#24067;&#26159;&#32477;&#23545;&#36830;&#32493;&#20998;&#24067;&#21644;&#31163;&#25955;&#20998;&#24067;&#30340;&#28151;&#21512;&#20307;&#65292;&#20854;&#20013;&#32477;&#23545;&#36830;&#32493;&#20998;&#37327;&#38598;&#20013;&#20110;&#19968;&#20010;$d_a$&#32500;&#23376;&#31354;&#38388;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#26356;&#23485;&#26494;&#30340;&#26465;&#20214;&#19979;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#38500;&#20102;&#26631;&#20934;&#30340;Lipschitz&#21644;&#36793;&#38469;&#26465;&#20214;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#32477;&#23545;&#36830;&#32493;&#20998;&#37327;&#30340;&#19968;&#20010;&#26032;&#29305;&#24615;&#65292;&#36890;&#36807;&#35813;&#29305;&#24615;&#35745;&#31639;&#20102;&#20998;&#31867;&#38169;&#35823;&#27010;&#29575;&#30340;&#31934;&#30830;&#25910;&#25947;&#29575;&#65292;&#23545;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14889v2 Announce Type: replace-cross  Abstract: In this paper we revisit the classical method of partitioning classification and study its convergence rate under relaxed conditions, both for observable (non-privatised) and for privatised data. Let the feature vector $X$ take values in $\mathbb{R}^d$ and denote its label by $Y$. Previous results on the partitioning classifier worked with the strong density assumption, which is restrictive, as we demonstrate through simple examples. We assume that the distribution of $X$ is a mixture of an absolutely continuous and a discrete distribution, such that the absolutely continuous component is concentrated to a $d_a$ dimensional subspace. Here, we study the problem under much milder assumptions: in addition to the standard Lipschitz and margin conditions, a novel characteristic of the absolutely continuous component is introduced, by which the exact convergence rate of the classification error probability is calculated, both for the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;AI&#25216;&#26415;&#24212;&#29992;&#20110;&#25968;&#25454;&#21516;&#21270;&#21644;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#39044;&#27979;&#20840;&#36807;&#31243;&#30340;&#33258;&#21160;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.12462</link><description>&lt;p&gt;
&#23454;&#29616;&#31471;&#21040;&#31471;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards an end-to-end artificial intelligence driven global weather forecasting system
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12462
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;AI&#25216;&#26415;&#24212;&#29992;&#20110;&#25968;&#25454;&#21516;&#21270;&#21644;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#39044;&#27979;&#20840;&#36807;&#31243;&#30340;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#23545;&#31185;&#23398;&#21644;&#31038;&#20250;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#20110;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;AI&#30340;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#20381;&#36182;&#20110;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#31995;&#32479;&#30340;&#20998;&#26512;&#25110;&#20877;&#20998;&#26512;&#20135;&#21697;&#20316;&#20026;&#39044;&#27979;&#30340;&#21021;&#22987;&#26465;&#20214;&#12290;&#21021;&#22987;&#29366;&#24577;&#36890;&#24120;&#30001;&#20256;&#32479;&#25968;&#25454;&#21516;&#21270;&#32452;&#20214;&#29983;&#25104;&#65292;&#36825;&#26159;&#35745;&#31639;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#25968;&#25454;&#21516;&#21270;&#27169;&#22411;&#65292;&#21363;Adas&#65292;&#29992;&#20110;&#20840;&#29699;&#22825;&#27668;&#21464;&#37327;&#12290;&#25105;&#20204;&#23558;Adas&#19982;&#20808;&#36827;&#30340;&#22522;&#20110;AI&#30340;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65288;&#21363;FengWu&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#22522;&#20110;AI&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65306;FengWu-Adas&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Adas&#33021;&#22815;&#21516;&#21270;&#31232;&#30095;&#30340;&#20840;&#29699;&#35266;&#27979;&#25968;&#25454;&#65292;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#31283;&#23450;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12462v2 Announce Type: replace-cross  Abstract: The weather forecasting system is important for science and society, and significant achievements have been made in applying artificial intelligence (AI) to medium-range weather forecasting. However, existing AI-based weather forecasting models rely on analysis or reanalysis products from the traditional numerical weather prediction (NWP) systems as initial conditions for making predictions. Initial states are typically generated by traditional data assimilation component, which is computational expensive and time-consuming. Here we present an AI-based data assimilation model, i.e., Adas, for global weather variables. And we combine Adas with the advanced AI-based weather forecasting model (i.e., FengWu) to construct the first end-to-end AI-based global weather forecasting system: FengWu-Adas. We demonstrate that Adas can assimilate sparse global observations to produce high-quality analysis, enabling the system operate stably 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#22238;&#25253;&#20998;&#24067;&#30340;&#26377;&#38480;&#32500;&#22343;&#20540;&#23884;&#20837;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#25512;&#23548;&#20986;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#21487;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#65292;&#25552;&#39640;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.07358</link><description>&lt;p&gt;
&#22522;&#20110;&#22343;&#20540;&#23884;&#20837;&#30340;&#20998;&#24067;&#24335;&#36125;&#23572;&#26364;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Distributional Bellman Operators over Mean Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07358
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#22238;&#25253;&#20998;&#24067;&#30340;&#26377;&#38480;&#32500;&#22343;&#20540;&#23884;&#20837;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#25512;&#23548;&#20986;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#21487;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#65292;&#25552;&#39640;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#22238;&#25253;&#20998;&#24067;&#30340;&#26377;&#38480;&#32500;&#22343;&#20540;&#23884;&#20837;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#12290; &#25105;&#20204;&#22522;&#20110;&#36825;&#19968;&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;&#20960;&#31181;&#26032;&#30340;&#21160;&#24577;&#35268;&#21010;&#21644;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#28176;&#36817;&#25910;&#25947;&#29702;&#35770;&#65292;&#24182;&#23545;&#36825;&#20123;&#31639;&#27861;&#22312;&#19968;&#31995;&#21015;&#34920;&#26684;&#20219;&#21153;&#19978;&#30340;&#23454;&#35777;&#34920;&#29616;&#36827;&#34892;&#20102;&#26816;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30452;&#25509;&#32467;&#21512;&#65292;&#24471;&#21040;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#22312; Arcade Learning Environment &#19978;&#20248;&#20110;&#22522;&#32447;&#20998;&#24067;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07358v2 Announce Type: replace-cross  Abstract: We propose a novel algorithmic framework for distributional reinforcement learning, based on learning finite-dimensional mean embeddings of return distributions. We derive several new algorithms for dynamic programming and temporal-difference learning based on this framework, provide asymptotic convergence theory, and examine the empirical performance of the algorithms on a suite of tabular tasks. Further, we show that this approach can be straightforwardly combined with deep reinforcement learning, and obtain a new deep RL agent that improves over baseline distributional approaches on the Arcade Learning Environment.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#24369;&#30417;&#30563;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#22312;&#23454;&#39564;&#25968;&#25454;&#19978;&#38656;&#35201;&#30340;&#20449;&#21495;&#37327;&#12290;</title><link>https://arxiv.org/abs/2312.06152</link><description>&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#25552;&#39640;&#24369;&#30417;&#30563;&#25628;&#32034;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving the performance of weak supervision searches using transfer and meta-learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06152
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#24369;&#30417;&#30563;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#22312;&#23454;&#39564;&#25968;&#25454;&#19978;&#38656;&#35201;&#30340;&#20449;&#21495;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#25628;&#32034;&#21407;&#21017;&#19978;&#20855;&#26377;&#33021;&#22815;&#22312;&#23454;&#39564;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#23398;&#20064;&#29420;&#29305;&#20449;&#21495;&#29305;&#24615;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25628;&#32034;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#21463;&#21040;&#25104;&#21151;&#36890;&#36807;&#24369;&#30417;&#30563;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#20449;&#21495;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#21019;&#24314;&#21487;&#20197;&#20174;&#36739;&#23569;&#23454;&#39564;&#20449;&#21495;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#24635;&#30340;&#24819;&#27861;&#26159;&#39318;&#20808;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#23398;&#20064;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#30340;&#27010;&#24565;&#25110;&#25104;&#20026;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#32773;&#12290;&#28982;&#21518;&#65292;&#31070;&#32463;&#32593;&#32476;&#23558;&#22312;&#23454;&#39564;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#30001;&#20110;&#20808;&#21069;&#30340;&#35757;&#32451;&#32780;&#38656;&#35201;&#36739;&#23569;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#21457;&#29616;&#36716;&#31227;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24369;&#30417;&#30563;&#25628;&#32034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06152v2 Announce Type: replace-cross  Abstract: Weak supervision searches have in principle the advantages of both being able to train on experimental data and being able to learn distinctive signal properties. However, the practical applicability of such searches is limited by the fact that successfully training a neural network via weak supervision can require a large amount of signal. In this work, we seek to create neural networks that can learn from less experimental signal by using transfer and meta-learning. The general idea is to first train a neural network on simulations, thereby learning concepts that can be reused or becoming a more efficient learner. The neural network would then be trained on experimental data and should require less signal because of its previous training. We find that transfer and meta-learning can substantially improve the performance of weak supervision searches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2312.04455</link><description>&lt;p&gt;
&#24378;&#21270;&#20851;&#27880;&#21147;&#20013;&#26368;&#30701;&#30340;&#25903;&#26609;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24847;&#35782;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#20851;&#27880;&#20998;&#37197;&#20013;&#30340;&#20869;&#22312;&#27874;&#24418;&#27169;&#24335;&#26174;&#33879;&#24433;&#21709;&#23427;&#20204;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#21033;&#29992;LLMs&#36827;&#34892;&#24037;&#20855;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#20851;&#38190;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#20013;&#20301;&#20110;&#20851;&#27880;&#27874;&#24418;&#30340;&#20302;&#35895;&#21306;&#22495;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#24573;&#35270;&#35813;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#26032;&#22411;&#25512;&#29702;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;LLMs&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#22788;&#29702;&#36755;&#20837;&#12290;&#27599;&#20010;&#36807;&#31243;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#20934;&#35282;&#24230;&#36827;&#34892;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65292;&#20174;&#32780;&#21019;&#24314;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#20851;&#27880;&#27874;&#24418;&#12290;&#36890;&#36807;&#29992;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#20302;&#35895;&#34917;&#20607;&#21478;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#39640;&#23792;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;LLM&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;IBRL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#26679;&#26412;-efficient RL&#65292;&#36890;&#36807;&#20808;&#22312;&#25552;&#20379;&#30340;&#31034;&#33539;&#19978;&#35757;&#32451;IL&#31574;&#30053;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#25552;&#20986;&#26367;&#20195;&#21160;&#20316;&#36827;&#34892;&#22312;&#32447;&#25506;&#32034;&#21644;&#24341;&#23548;&#30446;&#26631;&#20540;&#12290;</title><link>https://arxiv.org/abs/2311.02198</link><description>&lt;p&gt;
&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Bootstrapped Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02198
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;IBRL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#26679;&#26412;-efficient RL&#65292;&#36890;&#36807;&#20808;&#22312;&#25552;&#20379;&#30340;&#31034;&#33539;&#19978;&#35757;&#32451;IL&#31574;&#30053;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#25552;&#20986;&#26367;&#20195;&#21160;&#20316;&#36827;&#34892;&#22312;&#32447;&#25506;&#32034;&#21644;&#24341;&#23548;&#30446;&#26631;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#20027;&#35201;&#20381;&#36182;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#26159;&#22240;&#20026;&#20854;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#33021;&#20351;IL&#25512;&#24191;&#21040;&#25152;&#26377;&#21487;&#33021;&#22330;&#26223;&#30340;&#20840;&#38754;&#19987;&#23478;&#28436;&#31034;&#26159;&#26114;&#36149;&#30340;&#65292;&#20219;&#20309;&#20998;&#24067;&#30340;&#36716;&#21464;&#37117;&#38656;&#35201;&#37325;&#26032;&#25910;&#38598;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;RL&#21487;&#20197;&#24314;&#31435;&#22312;IL&#30340;&#22522;&#30784;&#19978;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#25105;&#25913;&#36827;&#31243;&#24207;&#65292;&#37027;&#20040;&#23427;&#23558;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;IBRL&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20855;&#26377;&#31034;&#33539;&#30340;&#39640;&#25928;&#25277;&#26679;RL&#65292;&#39318;&#20808;&#22312;&#25552;&#20379;&#30340;&#31034;&#33539;&#19978;&#35757;&#32451;IL&#31574;&#30053;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#25552;&#20986;&#26367;&#20195;&#21160;&#20316;&#36827;&#34892;&#22312;&#32447;&#25506;&#32034;&#21644;&#24341;&#23548;&#30446;&#26631;&#20540;&#12290;&#19982;&#20808;&#21069;&#36807;&#24230;&#37319;&#26679;&#31034;&#33539;&#25110;&#29992;&#39069;&#22806;&#30340;&#27169;&#20223;&#25439;&#22833;&#23545;RL&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;IBRL&#33021;&#22815;&#21033;&#29992;&#26469;&#33258;IL&#30340;&#39640;&#36136;&#37327;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02198v4 Announce Type: replace-cross  Abstract: Despite the considerable potential of reinforcement learning (RL), robotic control tasks predominantly rely on imitation learning (IL) due to its better sample efficiency. However, it is costly to collect comprehensive expert demonstrations that enable IL to generalize to all possible scenarios, and any distribution shift would require recollecting data for finetuning. Therefore, RL is appealing if it can build upon IL as an efficient autonomous self-improvement procedure. We propose imitation bootstrapped reinforcement learning (IBRL), a novel framework for sample-efficient RL with demonstrations that first trains an IL policy on the provided demonstrations and then uses it to propose alternative actions for both online exploration and bootstrapping target values. Compared to prior works that oversample the demonstrations or regularize RL with an additional imitation loss, IBRL is able to utilize high quality actions from IL p
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#32447;&#24615;&#36830;&#25509;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#27599;&#23618;&#31070;&#32463;&#20803;&#26435;&#37325;&#29420;&#31435;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32447;&#24615;&#36830;&#25509;&#30340;&#19978;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2310.19103</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#27169;&#24577;&#36830;&#36890;&#24615;
&lt;/p&gt;
&lt;p&gt;
Proving Linear Mode Connectivity of Neural Networks via Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19103
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#32447;&#24615;&#36830;&#25509;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#27599;&#23618;&#31070;&#32463;&#20803;&#26435;&#37325;&#29420;&#31435;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32447;&#24615;&#36830;&#25509;&#30340;&#19978;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#33021;&#37327;&#26223;&#35266;&#23545;&#20110;&#29702;&#35299;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#38543;&#26426;&#35757;&#32451;&#30340;&#20004;&#27425;&#36816;&#34892;&#20043;&#21518;&#25214;&#21040;&#30340;&#20004;&#20010;&#19981;&#21516;&#35299;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#38750;&#24120;&#31616;&#21333;&#30340;&#36830;&#32493;&#36335;&#24452;&#65288;&#22914;&#32447;&#24615;&#36335;&#24452;&#65289;&#30456;&#36830;&#65292;&#38500;&#20102;&#26435;&#37325;&#30340;&#25490;&#21015;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35770;&#19978;&#35299;&#37322;&#36825;&#19968;&#32463;&#39564;&#35266;&#23519;&#12290;&#22522;&#20110;&#32463;&#39564;&#24230;&#37327;&#30340;Wasserstein&#36317;&#31163;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24456;&#39640;&#30340;&#27010;&#29575;&#19979;&#65292;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20004;&#20010;&#36275;&#22815;&#23485;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#26159;&#32447;&#24615;&#36830;&#25509;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#36798;&#20102;&#20855;&#26377;&#29420;&#31435;&#31070;&#32463;&#20803;&#26435;&#37325;&#30340;&#20004;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27599;&#23618;&#23485;&#24230;&#32447;&#24615;&#36830;&#25509;&#30340;&#19978;&#19979;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;&#20445;&#25345;&#32500;&#24230;&#26469;&#23454;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19103v2 Announce Type: replace  Abstract: The energy landscape of high-dimensional non-convex optimization problems is crucial to understanding the effectiveness of modern deep neural network architectures. Recent works have experimentally shown that two different solutions found after two runs of a stochastic training are often connected by very simple continuous paths (e.g., linear) modulo a permutation of the weights. In this paper, we provide a framework theoretically explaining this empirical observation. Based on convergence rates in Wasserstein distance of empirical measures, we show that, with high probability, two wide enough two-layer neural networks trained with stochastic gradient descent are linearly connected. Additionally, we express upper and lower bounds on the width of each layer of two deep neural networks with independent neuron weights to be linearly connected. Finally, we empirically demonstrate the validity of our approach by showing how the dimension 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Aranyani&#65292;&#19968;&#31181;&#26012;&#35009;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#20248;&#21270;&#32676;&#20307;&#20844;&#24179;&#24615;&#30446;&#26631;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2310.11401</link><description>&lt;p&gt;
&#21033;&#29992;&#26012;&#35009;&#20915;&#31574;&#26862;&#26519;&#22686;&#24378;&#22312;&#32447;&#29615;&#22659;&#20013;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Group Fairness in Online Settings Using Oblique Decision Forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.11401
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Aranyani&#65292;&#19968;&#31181;&#26012;&#35009;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#20248;&#21270;&#32676;&#20307;&#20844;&#24179;&#24615;&#30446;&#26631;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#65292;&#29305;&#21035;&#26159;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#30446;&#21069;&#26368;&#24120;&#35265;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#22686;&#24378;&#25216;&#26415;&#26159;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20381;&#36182;&#20844;&#24179;&#30446;&#26631;&#65288;&#20363;&#22914;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#65289;&#21644;&#20219;&#21153;&#29305;&#23450;&#30446;&#26631;&#65288;&#20363;&#22914;&#20132;&#21449;&#29109;&#65289;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#20197;&#22312;&#32447;&#26041;&#24335;&#19968;&#27425;&#19968;&#20010;&#23454;&#20363;&#21040;&#36798;&#26102;&#65292;&#20248;&#21270;&#36825;&#26679;&#30340;&#20844;&#24179;&#24615;&#30446;&#26631;&#38754;&#20020;&#30528;&#20960;&#20010;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#32676;&#20307;&#20844;&#24179;&#24615;&#30446;&#26631;&#26159;&#36890;&#36807;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#30340;&#39044;&#27979;&#26399;&#26395;&#26469;&#23450;&#20041;&#30340;&#12290;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#65292;&#31639;&#27861;&#27599;&#27425;&#21482;&#33021;&#35775;&#38382;&#19968;&#20010;&#23454;&#20363;&#65292;&#20272;&#35745;&#32676;&#20307;&#20844;&#24179;&#24615;&#30446;&#26631;&#38656;&#35201;&#39069;&#22806;&#30340;&#23384;&#20648;&#21644;&#27604;&#20219;&#21153;&#29305;&#23450;&#30446;&#26631;&#26356;&#22810;&#30340;&#35745;&#31639;&#65288;&#20363;&#22914;&#21069;&#21521;/&#21518;&#21521;&#20256;&#36882;&#65289;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.11401v2 Announce Type: replace  Abstract: Fairness, especially group fairness, is an important consideration in the context of machine learning systems. The most commonly adopted group fairness-enhancing techniques are in-processing methods that rely on a mixture of a fairness objective (e.g., demographic parity) and a task-specific objective (e.g., cross-entropy) during the training process. However, when data arrives in an online fashion -- one instance at a time -- optimizing such fairness objectives poses several challenges. In particular, group fairness objectives are defined using expectations of predictions across different demographic groups. In the online setting, where the algorithm has access to a single instance at a time, estimating the group fairness objective requires additional storage and significantly more computation (e.g., forward/backward passes) than the task-specific objective at every time step. In this paper, we propose Aranyani, an ensemble of obliq
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#26641;&#24418;&#27169;&#22359;&#65292;&#33021;&#22815;&#20165;&#20174;&#23545;&#25968;&#25968;&#37327;&#30340;&#26631;&#35760;&#20013;&#26816;&#32034;&#20449;&#24687;&#36827;&#34892;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2309.17388</link><description>&lt;p&gt;
&#26641;&#24418;&#20132;&#21449;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Tree Cross Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.17388
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#26641;&#24418;&#27169;&#22359;&#65292;&#33021;&#22815;&#20165;&#20174;&#23545;&#25968;&#25968;&#37327;&#30340;&#26631;&#35760;&#20013;&#26816;&#32034;&#20449;&#24687;&#36827;&#34892;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#27880;&#24847;&#21147;&#26159;&#19968;&#31181;&#29992;&#20110;&#20174;&#19968;&#32452;&#19978;&#19979;&#25991;&#26631;&#35760;&#20013;&#26816;&#32034;&#20449;&#24687;&#20197;&#36827;&#34892;&#39044;&#27979;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#22312;&#25512;&#26029;&#26102;&#65292;&#23545;&#20110;&#27599;&#20010;&#39044;&#27979;&#65292;&#20132;&#21449;&#27880;&#24847;&#21147;&#20250;&#25195;&#25551;&#20840;&#37096;&#30340; $\mathcal{O}(N)$ &#20010;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#24120;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#26631;&#35760;&#21363;&#21487;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#26041;&#27861;&#22914;Perceiver IO&#22312;&#25512;&#26029;&#26102;&#24265;&#20215;&#65292;&#22240;&#20026;&#23427;&#23558;&#20449;&#24687;&#25552;&#28860;&#21040;&#19968;&#20010;&#35268;&#27169;&#36739;&#23567;&#30340;&#28508;&#22312;&#26631;&#35760;&#38598;&#21512; $L &lt; N$ &#19978;&#65292;&#28982;&#21518;&#20877;&#24212;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#20165;&#23548;&#33268; $\mathcal{O}(L)$ &#30340;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#38543;&#30528;&#36755;&#20837;&#26631;&#35760;&#30340;&#25968;&#37327;&#21644;&#25552;&#28860;&#20449;&#24687;&#30340;&#22686;&#21152;&#65292;&#25152;&#38656;&#30340;&#28508;&#22312;&#26631;&#35760;&#25968;&#37327;&#20063;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#27169;&#22359;&#65292;&#31216;&#20026;&#26641;&#24418;&#20132;&#21449;&#27880;&#24847;&#21147; (TCA) - &#35813;&#27169;&#22359;&#20165;&#20174;&#23545;&#25968;&#25968;&#37327;&#30340;&#26631;&#35760;&#20013;&#26816;&#32034;&#20449;&#24687;&#36827;&#34892;&#25512;&#26029;&#12290;TCA&#20197;&#26641;&#32467;&#26500;&#32452;&#32455;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.17388v2 Announce Type: replace  Abstract: Cross Attention is a popular method for retrieving information from a set of context tokens for making predictions. At inference time, for each prediction, Cross Attention scans the full set of $\mathcal{O}(N)$ tokens. In practice, however, often only a small subset of tokens are required for good performance. Methods such as Perceiver IO are cheap at inference as they distill the information to a smaller-sized set of latent tokens $L &lt; N$ on which cross attention is then applied, resulting in only $\mathcal{O}(L)$ complexity. However, in practice, as the number of input tokens and the amount of information to distill increases, the number of latent tokens needed also increases significantly. In this work, we propose Tree Cross Attention (TCA) - a module based on Cross Attention that only retrieves information from a logarithmic $\mathcal{O}(\log(N))$ number of tokens for performing inference. TCA organizes the data in a tree structu
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#26356;&#22909;&#30340;&#24494;&#35843;&#31574;&#30053;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#21033;&#29992;&#36716;&#31227;&#30340;&#30693;&#35782;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2308.06960</link><description>&lt;p&gt;
&#20026;&#22270;&#32423;&#20219;&#21153;&#35843;&#20248;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.06960
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26356;&#22909;&#30340;&#24494;&#35843;&#31574;&#30053;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#21033;&#29992;&#36716;&#31227;&#30340;&#30693;&#35782;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#35768;&#22810;&#19982;&#22270;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#65292;GNN&#38754;&#20020;&#30528;&#26631;&#31614;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#23581;&#35797;&#22312;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#30340;&#22270;&#19978;&#39044;&#35757;&#32451;GNN&#65292;&#24182;&#23558;&#20174;&#26410;&#26631;&#35760;&#30340;&#22270;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#36866;&#24212;&#21040;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#35813;&#36866;&#24212;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#30340;GNN&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#12290;&#23613;&#31649;&#24494;&#35843;&#30340;&#37325;&#35201;&#24615;&#65292;&#24403;&#21069;&#30340;GNN&#39044;&#35757;&#32451;&#24037;&#20316;&#24448;&#24448;&#24573;&#35270;&#20102;&#35774;&#35745;&#19968;&#20010;&#33391;&#22909;&#30340;&#24494;&#35843;&#31574;&#30053;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36716;&#31227;&#30340;&#30693;&#35782;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#24320;&#22987;&#30740;&#31350;&#39044;&#35757;&#32451;GNN&#30340;&#26356;&#22909;&#24494;&#35843;&#31574;&#30053;&#12290;&#20294;&#20182;&#20204;&#30340;&#35774;&#35745;&#35201;&#20040;&#26377;&#24456;&#24378;&#30340;&#20551;&#35774;&#65292;&#35201;&#20040;&#24573;&#35270;&#20102;&#21508;&#31181;&#19979;&#28216;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#24863;&#30693;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#39044;&#35757;&#32451;&#30340;GNN&#35774;&#35745;&#26356;&#22909;&#30340;&#24494;&#35843;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.06960v2 Announce Type: replace-cross  Abstract: Recently, graph neural networks (GNNs) have shown its unprecedented success in many graph-related tasks. However, GNNs face the label scarcity issue as other neural networks do. Thus, recent efforts try to pre-train GNNs on a large-scale unlabeled graph and adapt the knowledge from the unlabeled graph to the target downstream task. The adaptation is generally achieved by fine-tuning the pre-trained GNNs with a limited number of labeled data. Despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only few works start to investigate a better fine-tuning strategy for pre-trained GNNs. But their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#20855;&#26377;&#20301;&#32622;&#23610;&#24230;&#21644;&#24418;&#29366;&#30340;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;(NAMLSS)&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#20998;&#24067;&#22238;&#24402;&#30340;&#20248;&#21183;</title><link>https://arxiv.org/abs/2301.11862</link><description>&lt;p&gt;
&#20855;&#26377;&#20301;&#32622;&#23610;&#24230;&#21644;&#24418;&#29366;&#30340;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65306;&#36229;&#36234;&#24179;&#22343;&#20540;&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#22238;&#24402;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Neural Additive Models for Location Scale and Shape: A Framework for Interpretable Neural Regression Beyond the Mean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.11862
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#20855;&#26377;&#20301;&#32622;&#23610;&#24230;&#21644;&#24418;&#29366;&#30340;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;(NAMLSS)&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#20998;&#24067;&#22238;&#24402;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#20351;&#20854;&#25104;&#20026;&#38656;&#35201;&#39640;&#32423;&#39044;&#27979;&#33021;&#21147;&#30340;&#38382;&#39064;&#30340;&#39318;&#36873;&#26041;&#27861;&#12290;&#23613;&#31649;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;DNNs&#30340;&#20869;&#37096;&#36816;&#20316;&#36890;&#24120;&#19981;&#36879;&#26126;&#65292;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#25110;&#29702;&#35299;&#12290;&#36825;&#31181;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#23548;&#33268;&#36817;&#24180;&#26469;&#23545;&#22266;&#26377;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#19981;&#26029;&#22686;&#21152;&#12290;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAMs&#65289;&#31561;&#27169;&#22411;&#36890;&#36807;&#23558;&#32463;&#20856;&#32479;&#35745;&#26041;&#27861;&#19982;DNNs&#30456;&#32467;&#21512;&#23454;&#29616;&#20102;&#35270;&#35273;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#38598;&#20013;&#20110;&#24179;&#22343;&#21709;&#24212;&#39044;&#27979;&#65292;&#24573;&#30053;&#20102;&#24213;&#23618;&#25968;&#25454;&#21709;&#24212;&#20998;&#24067;&#30340;&#20854;&#20182;&#29305;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#20301;&#32622;&#23610;&#24230;&#21644;&#24418;&#29366;&#30340;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;(NAMLSS)&#65292;&#36825;&#26159;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#65292;&#23558;&#32463;&#20856;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#19982;&#20998;&#24067;&#22238;&#24402;&#30340;&#22266;&#26377;&#20248;&#21183;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.11862v2 Announce Type: replace-cross  Abstract: Deep neural networks (DNNs) have proven to be highly effective in a variety of tasks, making them the go-to method for problems requiring high-level predictive power. Despite this success, the inner workings of DNNs are often not transparent, making them difficult to interpret or understand. This lack of interpretability has led to increased research on inherently interpretable neural networks in recent years. Models such as Neural Additive Models (NAMs) achieve visual interpretability through the combination of classical statistical methods with DNNs. However, these approaches only concentrate on mean response predictions, leaving out other properties of the response distribution of the underlying data. We propose Neural Additive Models for Location Scale and Shape (NAMLSS), a modelling framework that combines the predictive power of classical deep learning models with the inherent advantages of distributional regression while
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#20272;&#35745;&#31639;&#27861;&#65292;&#20855;&#26377;&#26377;&#38480;&#26102;&#38388;&#20445;&#35777;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#32467;&#26500;&#20272;&#35745;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#22870;&#21169;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2210.01282</link><description>&lt;p&gt;
&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#32467;&#26500;&#20272;&#35745;&#19982;&#26377;&#38480;&#26102;&#38388;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.01282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#20272;&#35745;&#31639;&#27861;&#65292;&#20855;&#26377;&#26377;&#38480;&#26102;&#38388;&#20445;&#35777;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#32467;&#26500;&#20272;&#35745;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#22870;&#21169;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22522;&#20110;&#21487;&#35266;&#27979;&#30340;&#34892;&#20026;&#21382;&#21490;&#21644;&#35775;&#38382;&#29366;&#24577;&#26469;&#20272;&#35745;&#20154;&#31867;&#20195;&#29702;&#21160;&#24577;&#20915;&#31574;&#30340;&#32467;&#26500;&#27169;&#22411;&#30340;&#20219;&#21153;&#12290;&#38382;&#39064;&#20855;&#26377;&#22266;&#26377;&#30340;&#23884;&#22871;&#32467;&#26500;&#65306;&#22312;&#20869;&#37096;&#38382;&#39064;&#20013;&#65292;&#30830;&#23450;&#32473;&#23450;&#22870;&#21169;&#20989;&#25968;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#22312;&#22806;&#37096;&#38382;&#39064;&#20013;&#65292;&#26368;&#22823;&#21270;&#36866;&#21512;&#24230;&#24230;&#37327;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#23884;&#22871;&#24490;&#29615;&#32467;&#26500;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#20294;&#24403;&#29366;&#24577;&#31354;&#38388;&#35201;&#20040;&#26159;&#20855;&#26377;&#22823;&#22522;&#25968;&#30340;&#31163;&#25955;&#31354;&#38388;&#65292;&#35201;&#20040;&#26159;&#39640;&#32500;&#36830;&#32493;&#31354;&#38388;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#39640;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;&#36870;&#24378;&#21270;&#23398;&#20064;(IRL)&#25991;&#29486;&#20013;&#30340;&#20854;&#20182;&#26041;&#27861;&#24378;&#35843;&#31574;&#30053;&#20272;&#35745;&#65292;&#20294;&#21364;&#20197;&#38477;&#20302;&#22870;&#21169;&#20272;&#35745;&#31934;&#24230;&#20026;&#20195;&#20215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26377;&#38480;&#26102;&#38388;&#20445;&#35777;&#30340;&#21333;&#24490;&#29615;&#20272;&#35745;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#32780;&#19981;&#20250;&#25439;&#23475;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.01282v3 Announce Type: replace-cross  Abstract: We consider the task of estimating a structural model of dynamic decisions by a human agent based upon the observable history of implemented actions and visited states. This problem has an inherent nested structure: in the inner problem, an optimal policy for a given reward function is identified while in the outer problem, a measure of fit is maximized. Several approaches have been proposed to alleviate the computational burden of this nested-loop structure, but these methods still suffer from high complexity when the state space is either discrete with large cardinality or continuous in high dimensions. Other approaches in the inverse reinforcement learning (IRL) literature emphasize policy estimation at the expense of reduced reward estimation accuracy. In this paper we propose a single-loop estimation algorithm with finite time guarantees that is equipped to deal with high-dimensional state spaces without compromising rewar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22823;&#22411;&#32593;&#32476;&#31995;&#32479;&#20998;&#35299;&#20026;&#22810;&#20010;&#23616;&#37096;&#32452;&#20214;&#65292;&#26500;&#24314;&#29420;&#31435;&#24182;&#24182;&#34892;&#36816;&#34892;&#30340;&#27169;&#25311;&#22120;&#65292;&#26377;&#25928;&#30417;&#27979;&#24433;&#21709;&#65292;&#24182;&#22312;&#20960;&#23567;&#26102;&#20869;&#35757;&#32451;&#22823;&#22411;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#20943;&#36731;&#21516;&#26102;&#23398;&#20064;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2207.00288</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22686;&#24378;&#24433;&#21709;&#30340;&#23616;&#37096;&#27169;&#25311;&#22120;&#29992;&#20110;&#22823;&#22411;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#24182;&#34892;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributed Influence-Augmented Local Simulators for Parallel MARL in Large Networked Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.00288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22823;&#22411;&#32593;&#32476;&#31995;&#32479;&#20998;&#35299;&#20026;&#22810;&#20010;&#23616;&#37096;&#32452;&#20214;&#65292;&#26500;&#24314;&#29420;&#31435;&#24182;&#24182;&#34892;&#36816;&#34892;&#30340;&#27169;&#25311;&#22120;&#65292;&#26377;&#25928;&#30417;&#27979;&#24433;&#21709;&#65292;&#24182;&#22312;&#20960;&#23567;&#26102;&#20869;&#35757;&#32451;&#22823;&#22411;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#20943;&#36731;&#21516;&#26102;&#23398;&#20064;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#27169;&#25311;&#22312;&#20170;&#22825;&#23545;&#20110;&#25104;&#21151;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#34920;&#29616;&#20986;&#36807;&#20110;&#22797;&#26434;&#30340;&#21160;&#24577;&#24615;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#20840;&#23610;&#24230;&#27169;&#25311;&#22312;&#35745;&#31639;&#19978;&#21464;&#24471;&#32531;&#24930;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#35768;&#22810;&#26234;&#33021;&#20307;&#30340;&#22823;&#22411;&#32593;&#32476;&#31995;&#32479;&#20998;&#35299;&#20026;&#22810;&#20010;&#23616;&#37096;&#32452;&#20214;&#65292;&#20174;&#32780;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#29420;&#31435;&#24182;&#24182;&#34892;&#36816;&#34892;&#30340;&#20998;&#24320;&#27169;&#25311;&#22120;&#12290;&#20026;&#20102;&#30417;&#27979;&#19981;&#21516;&#23616;&#37096;&#32452;&#20214;&#30456;&#20114;&#26045;&#21152;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#27169;&#25311;&#22120;&#27599;&#19968;&#20010;&#37117;&#37197;&#22791;&#20102;&#19968;&#20010;&#21608;&#26399;&#24615;&#35757;&#32451;&#22312;&#30495;&#23454;&#36712;&#36857;&#19978;&#30340;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#27169;&#25311;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#36827;&#31243;&#19981;&#20165;&#21487;&#20197;&#22312;&#20960;&#23567;&#26102;&#20869;&#35757;&#32451;&#22823;&#22411;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#36824;&#26377;&#21161;&#20110;&#20943;&#36731;&#21516;&#26102;&#23398;&#20064;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.00288v2 Announce Type: replace  Abstract: Due to its high sample complexity, simulation is, as of today, critical for the successful application of reinforcement learning. Many real-world problems, however, exhibit overly complex dynamics, which makes their full-scale simulation computationally slow. In this paper, we show how to decompose large networked systems of many agents into multiple local components such that we can build separate simulators that run independently and in parallel. To monitor the influence that the different local components exert on one another, each of these simulators is equipped with a learned model that is periodically trained on real trajectories. Our empirical results reveal that distributing the simulation among different processes not only makes it possible to train large multi-agent systems in just a few hours but also helps mitigate the negative effects of simultaneous learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65292;&#21487;&#20197;&#20026;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25152;&#20316;&#20986;&#30340;&#20915;&#31574;&#25552;&#20379;&#20840;&#23616;&#35299;&#37322;&#65292;&#24182;&#25429;&#25417;&#30452;&#35266;&#30340;&#20542;&#21521;&#12290;</title><link>https://arxiv.org/abs/2203.16464</link><description>&lt;p&gt;
&#36890;&#36807;&#36870;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.16464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65292;&#21487;&#20197;&#20026;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25152;&#20316;&#20986;&#30340;&#20915;&#31574;&#25552;&#20379;&#20840;&#23616;&#35299;&#37322;&#65292;&#24182;&#25429;&#25417;&#30452;&#35266;&#30340;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65292;&#23588;&#20854;&#26159;&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#38500;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#35780;&#20272;&#25351;&#26631;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#25165;&#33021;&#34987;&#21487;&#38752;&#22320;&#21033;&#29992;&#12290;&#22240;&#27492;&#65292;&#25552;&#20379;&#33021;&#22815;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#22914;&#20309;&#23558;&#20854;&#36755;&#20837;&#26144;&#23556;&#21040;&#36755;&#20986;&#30340;&#35299;&#37322;&#25104;&#20026;&#20154;&#20204;&#36843;&#20999;&#38656;&#35201;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#36825;&#31181;&#29305;&#24615;&#38459;&#30861;&#20102;&#30740;&#31350;&#20154;&#21592;&#23398;&#20064;&#21644;&#25552;&#20379;&#23545;&#27169;&#22411;&#34892;&#20026;&#21644;&#26368;&#32456;&#39044;&#27979;&#30340;&#35299;&#37322;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.16464v3 Announce Type: replace-cross  Abstract: Artificial intelligence, particularly through recent advancements in deep learning, has achieved exceptional performances in many tasks in fields such as natural language processing and computer vision. In addition to desirable evaluation metrics, a high level of interpretability is often required for these models to be reliably utilized. Therefore, explanations that offer insight into the process by which a model maps its inputs onto its outputs are much sought-after. Unfortunately, the current black box nature of machine learning models is still an unresolved issue and this very nature prevents researchers from learning and providing explicative descriptions for a model's behavior and final predictions. In this work, we propose a novel framework utilizing Adversarial Inverse Reinforcement Learning that can provide global explanations for decisions made by a Reinforcement Learning model and capture intuitive tendencies that th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;trie&#30340;&#20869;&#23384;&#39640;&#25928;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;&#26041;&#27861;&#65292;&#22312;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#26377;&#26174;&#33879;&#25913;&#21892;&#65292;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;256GB&#31995;&#32479;&#20869;&#23384;&#19979;&#22823;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2202.06834</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;Tries&#30340;&#20869;&#23384;&#39640;&#25928;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Memory-Efficient Sequential Pattern Mining with Hybrid Tries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.06834
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;trie&#30340;&#20869;&#23384;&#39640;&#25928;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;&#26041;&#27861;&#65292;&#22312;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#26377;&#26174;&#33879;&#25913;&#21892;&#65292;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;256GB&#31995;&#32479;&#20869;&#23384;&#19979;&#22823;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#25968;&#25454;&#38598;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#23545;&#20110;&#33021;&#22815;&#22788;&#29702;&#22914;&#27492;&#24222;&#22823;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#25366;&#25496;&#31639;&#27861;&#30340;&#38656;&#27714;&#21464;&#24471;&#26085;&#30410;&#36843;&#20999;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#26041;&#27861;&#29992;&#20110;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;&#65288;SPM&#65289;&#65292;&#36825;&#26159;&#30693;&#35782;&#21457;&#29616;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20027;&#39064;&#65292;&#38754;&#20020;&#30528;&#38024;&#23545;&#22823;&#25968;&#25454;&#38598;&#30340;&#24050;&#30693;&#20869;&#23384;&#29942;&#39048;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;trie&#25968;&#25454;&#32467;&#26500;&#65292;&#21033;&#29992;&#37325;&#22797;&#27169;&#24335;&#32039;&#20945;&#22320;&#23384;&#20648;&#20869;&#23384;&#20013;&#30340;&#25968;&#25454;&#38598;; &#20197;&#21450;&#19968;&#20010;&#30456;&#24212;&#30340;&#25366;&#25496;&#31639;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#20174;&#27492;&#32039;&#20945;&#34920;&#31034;&#20013;&#25552;&#21462;&#27169;&#24335;&#12290;&#23545;&#30495;&#23454;&#27979;&#35797;&#23454;&#20363;&#30340;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;&#65292;&#23545;&#20110;&#23567;&#21040;&#20013;&#31561;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#20869;&#23384;&#28040;&#32791;&#24179;&#22343;&#25552;&#39640;&#20102;88&#65285;&#65292;&#35745;&#31639;&#26102;&#38388;&#25552;&#39640;&#20102;41&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#21807;&#19968;&#19968;&#20010;&#22312;&#31995;&#32479;&#20869;&#23384;&#20026;256GB&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22788;&#29702;&#22823;&#25968;&#25454;&#38598;&#30340;SPM&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.06834v2 Announce Type: replace-cross  Abstract: As modern data sets continue to grow exponentially in size, the demand for efficient mining algorithms capable of handling such large data sets becomes increasingly imperative. This paper develops a memory-efficient approach for Sequential Pattern Mining (SPM), a fundamental topic in knowledge discovery that faces a well-known memory bottleneck for large data sets. Our methodology involves a novel hybrid trie data structure that exploits recurring patterns to compactly store the data set in memory; and a corresponding mining algorithm designed to effectively extract patterns from this compact representation. Numerical results on real-life test instances show an average improvement of 88% in memory consumption and 41% in computation time for small to medium-sized data sets compared to the state of the art. Furthermore, our algorithm stands out as the only capable SPM approach for large data sets within 256GB of system memory.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;InceptionXML&#65292;&#36890;&#36807;&#22312;embedding&#32500;&#24230;&#19978;&#37325;&#26032;&#20998;&#37197;&#21367;&#31215;&#25805;&#20316;&#65292;&#24212;&#23545;&#30701;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#32570;&#22833;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;InceptionXML+&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#27493;&#26631;&#31614;&#31579;&#36873;&#22120;&#21644;&#26497;&#31471;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#21160;&#24577;&#30828;&#36127;&#37319;&#26679;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2109.07319</link><description>&lt;p&gt;
InceptionXML&#65306;&#19968;&#31181;&#24102;&#26377;&#21516;&#27493;&#36127;&#37319;&#26679;&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#29992;&#20110;&#30701;&#25991;&#26412;&#26497;&#31471;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
InceptionXML: A Lightweight Framework with Synchronized Negative Sampling for Short Text Extreme Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.07319
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;InceptionXML&#65292;&#36890;&#36807;&#22312;embedding&#32500;&#24230;&#19978;&#37325;&#26032;&#20998;&#37197;&#21367;&#31215;&#25805;&#20316;&#65292;&#24212;&#23545;&#30701;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#32570;&#22833;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;InceptionXML+&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#27493;&#26631;&#31614;&#31579;&#36873;&#22120;&#21644;&#26497;&#31471;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#21160;&#24577;&#30828;&#36127;&#37319;&#26679;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#25991;&#26412;&#25968;&#25454;&#23545;&#22823;&#37327;&#30446;&#26631;&#26631;&#31614;&#36827;&#34892;&#33258;&#21160;&#27880;&#37322;&#65292;&#34987;&#31216;&#20026;&#30701;&#25991;&#26412;&#26497;&#31471;&#20998;&#31867;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#21253;&#25324;&#30456;&#20851;&#25628;&#32034;&#39044;&#27979;&#21644;&#20135;&#21697;&#25512;&#33616;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#26550;&#26500;InceptionXML&#65292;&#20854;&#36731;&#37327;&#20294;&#21151;&#33021;&#24378;&#22823;&#65292;&#24182;&#19988;&#33021;&#22815;&#24212;&#23545;&#25628;&#32034;&#21644;&#25512;&#33616;&#20219;&#21153;&#20013;&#30701;&#25991;&#26412;&#26597;&#35810;&#20013;&#22266;&#26377;&#30340;&#32570;&#20047;&#21333;&#35789;&#39034;&#24207;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#21367;&#31215;&#30340;&#25805;&#20316;&#27839;&#30528;&#23884;&#20837;&#32500;&#24230;&#37325;&#26032;&#26500;&#24314;&#65292;&#32780;&#19981;&#26159;&#20687;&#20256;&#32479;CNNs&#19968;&#26679;&#27839;&#30528;&#21333;&#35789;&#32500;&#24230;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#35777;&#26126;&#20102;&#24212;&#29992;&#21367;&#31215;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#25193;&#23637;&#21040;&#20855;&#26377;&#25968;&#30334;&#19975;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;InceptionXML+&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#27493;&#26631;&#31614;&#31579;&#36873;&#22120;&#21644;&#26497;&#31471;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#21160;&#24577;&#30828;&#36127;&#37319;&#26679;&#25216;&#26415;&#22312;&#26631;&#31614;&#31579;&#36873;&#20013;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.07319v3 Announce Type: replace-cross  Abstract: Automatic annotation of short-text data to a large number of target labels, referred to as Short Text Extreme Classification, has found numerous applications including prediction of related searches and product recommendation tasks. In this paper, we propose a convolutional architecture InceptionXML which is light-weight, yet powerful, and robust to the inherent lack of word-order in short-text queries encountered in search and recommendation tasks. We demonstrate the efficacy of applying convolutions by recasting the operation along the embedding dimension instead of the word dimension as applied in conventional CNNs for text classification. Towards scaling our model to datasets with millions of labels, we also propose InceptionXML+ framework which improves upon the shortcomings of the recently proposed dynamic hard-negative mining technique for label shortlisting by synchronizing the label-shortlister and extreme classifier. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#21508;&#21521;&#24322;&#24615;&#20449;&#24687;&#22312;&#21021;&#22987;&#26465;&#20214;&#20013;&#22914;&#20309;&#24433;&#21709;&#26263;&#29289;&#36136;&#26263;&#22242;&#30340;&#26368;&#32456;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#21508;&#21521;&#24322;&#24615;&#28155;&#21152;&#20102;&#19968;&#20123;&#39069;&#22806;&#20449;&#24687;&#37327;&#12290;</title><link>https://arxiv.org/abs/2011.10577</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#23545;&#23431;&#23449;&#32467;&#26500;&#24418;&#25104;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Deep learning insights into cosmological structure formation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2011.10577
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#21508;&#21521;&#24322;&#24615;&#20449;&#24687;&#22312;&#21021;&#22987;&#26465;&#20214;&#20013;&#22914;&#20309;&#24433;&#21709;&#26263;&#29289;&#36136;&#26263;&#22242;&#30340;&#26368;&#32456;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#21508;&#21521;&#24322;&#24615;&#28155;&#21152;&#20102;&#19968;&#20123;&#39069;&#22806;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23431;&#23449;&#27169;&#25311;&#21487;&#20197;&#35745;&#31639;&#26089;&#26399;&#23431;&#23449;&#20013;&#30340;&#32447;&#24615;&#21021;&#22987;&#26465;&#20214;&#22914;&#20309;&#28436;&#21464;&#25104;&#20026;&#21518;&#26469;&#30340;&#26263;&#29289;&#36136;&#25193;&#23637;&#26263;&#22242;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#19968;&#22797;&#26434;&#36807;&#31243;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#65307;&#29305;&#21035;&#26159;&#65292;&#21021;&#22987;&#26465;&#20214;&#20013;&#21508;&#21521;&#24322;&#24615;&#20449;&#24687;&#22914;&#20309;&#30830;&#23450;&#26368;&#32456;&#26263;&#29289;&#36136;&#26263;&#22242;&#36136;&#37327;&#30340;&#35282;&#33394;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38590;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26469;&#20174;&#21021;&#22987;&#26465;&#20214;&#39044;&#27979;&#26263;&#29289;&#36136;&#26263;&#22242;&#30340;&#36136;&#37327;&#65292;&#24182;&#20840;&#38754;&#37327;&#21270;&#20102;&#21021;&#22987;&#23494;&#24230;&#22330;&#30340;&#21508;&#21521;&#21516;&#24615;&#21644;&#21508;&#21521;&#24322;&#24615;&#26041;&#38754;&#20851;&#20110;&#26368;&#32456;&#26263;&#22242;&#36136;&#37327;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21508;&#21521;&#24322;&#24615;&#30830;&#23454;&#27604;&#23494;&#24230;&#22330;&#30340;&#29699;&#38754;&#24179;&#22343;&#20540;&#25152;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#30053;&#24494;&#22686;&#21152;&#20102;&#19968;&#20123;&#65292;&#23613;&#31649;&#22312;&#32479;&#35745;&#19978;&#26159;&#26174;&#33879;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2011.10577v3 Announce Type: replace-cross  Abstract: The evolution of linear initial conditions present in the early universe into extended halos of dark matter at late times can be computed using cosmological simulations. However, a theoretical understanding of this complex process remains elusive; in particular, the role of anisotropic information in the initial conditions in establishing the final mass of dark matter halos remains a long-standing puzzle. Here, we build a deep learning framework to investigate this question. We train a three-dimensional convolutional neural network (CNN) to predict the mass of dark matter halos from the initial conditions, and quantify in full generality the amounts of information in the isotropic and anisotropic aspects of the initial density field about final halo masses. We find that anisotropies add a small, albeit statistically significant amount of information over that contained within spherical averages of the density field about final 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#40065;&#26834;&#20248;&#21270;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#22312;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#19979;&#21516;&#26102;&#32771;&#34385;&#28608;&#36827;&#21644;&#20445;&#23432;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2007.12315</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#40065;&#26834;&#20248;&#21270;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Robust Optimization for Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2007.12315
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#40065;&#26834;&#20248;&#21270;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#22312;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#19979;&#21516;&#26102;&#32771;&#34385;&#28608;&#36827;&#21644;&#20445;&#23432;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#30830;&#23450;&#22312;&#28436;&#31034;&#30340;&#29366;&#24577;&#20998;&#24067;&#20043;&#22806;&#26102;&#20195;&#29702;&#24212;&#37319;&#21462;&#20160;&#20040;&#34892;&#21160;&#12290;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#21442;&#25968;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#23545;&#26032;&#29366;&#24577;&#30340;&#27867;&#21270;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#23545;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#21644;&#30456;&#24212;&#26368;&#20248;&#31574;&#30053;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#29616;&#26377;&#22522;&#20110;IRL&#30340;&#23433;&#20840;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;maxmin&#26694;&#26550;&#22788;&#29702;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26694;&#26550;&#22312;&#20551;&#35774;&#26377;&#19968;&#20010;&#23545;&#25239;&#24615;&#22870;&#21169;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#31574;&#30053;&#65292;&#32780;&#39118;&#38505;&#20013;&#31435;&#30340;IRL&#26041;&#27861;&#21017;&#20248;&#21270;&#22343;&#20540;&#25110;MAP&#22870;&#21169;&#20989;&#25968;&#30340;&#31574;&#30053;&#12290;&#23436;&#20840;&#24573;&#35270;&#39118;&#38505;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#20110;&#28608;&#36827;&#21644;&#19981;&#23433;&#20840;&#30340;&#31574;&#30053;&#65292;&#32780;&#23436;&#20840;&#20197;&#23545;&#25239;&#24615;&#26041;&#24335;&#20248;&#21270;&#20063;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#23548;&#33268;&#34920;&#29616;&#19981;&#20339;&#30340;&#36807;&#24230;&#20445;&#23432;&#31574;&#30053;&#12290;&#20026;&#20102;&#22312;&#36825;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#40065;&#26834;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#29366;&#24577;&#30340;&#32622;&#20449;&#21306;&#38388;&#20869;&#23545;&#31574;&#30053;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2007.12315v4 Announce Type: replace  Abstract: One of the main challenges in imitation learning is determining what action an agent should take when outside the state distribution of the demonstrations. Inverse reinforcement learning (IRL) can enable generalization to new states by learning a parameterized reward function, but these approaches still face uncertainty over the true reward function and corresponding optimal policy. Existing safe imitation learning approaches based on IRL deal with this uncertainty using a maxmin framework that optimizes a policy under the assumption of an adversarial reward function, whereas risk-neutral IRL approaches either optimize a policy for the mean or MAP reward function. While completely ignoring risk can lead to overly aggressive and unsafe policies, optimizing in a fully adversarial sense is also problematic as it can lead to overly conservative policies that perform poorly in practice. To provide a bridge between these two extremes, we p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#19982;&#25511;&#21046;&#65292;Symplectic ODE-Net (SymODEN)&#21487;&#20197;&#36879;&#26126;&#22320;&#23398;&#20064;&#28508;&#22312;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#25581;&#31034;&#31995;&#32479;&#30340;&#30456;&#20851;&#29289;&#29702;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/1909.12077</link><description>&lt;p&gt;
Symplectic ODE-Net: &#20351;&#29992;&#25511;&#21046;&#23398;&#20064;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1909.12077
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#19982;&#25511;&#21046;&#65292;Symplectic ODE-Net (SymODEN)&#21487;&#20197;&#36879;&#26126;&#22320;&#23398;&#20064;&#28508;&#22312;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#25581;&#31034;&#31995;&#32479;&#30340;&#30456;&#20851;&#29289;&#29702;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Symplectic ODE-Net&#65288;SymODEN&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#21040;&#30340;&#29366;&#24577;&#36712;&#36857;&#20013;&#25512;&#26029;&#20986;&#30001;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#32473;&#23450;&#30340;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#12290;&#20026;&#20102;&#22312;&#26356;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#65292;SymODEN&#36890;&#36807;&#20197;&#29289;&#29702;&#20026;&#22522;&#30784;&#30340;&#26041;&#24335;&#35774;&#35745;&#30456;&#20851;&#30340;&#35745;&#31639;&#22270;&#65292;&#24341;&#20837;&#36866;&#24403;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#19982;&#25511;&#21046;&#20197;&#36879;&#26126;&#30340;&#26041;&#24335;&#23398;&#20064;&#28508;&#22312;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#21160;&#21147;&#23398;&#26469;&#25581;&#31034;&#31995;&#32479;&#30340;&#30456;&#20851;&#29289;&#29702;&#26041;&#38754;&#65292;&#22914;&#36136;&#37327;&#21644;&#21183;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24191;&#20041;&#22352;&#26631;&#25968;&#25454;&#23884;&#20837;&#21040;&#39640;&#32500;&#31354;&#38388;&#25110;&#32773;&#21482;&#33021;&#35775;&#38382;&#36895;&#24230;&#25968;&#25454;&#32780;&#19981;&#26159;&#24191;&#20041;&#21160;&#37327;&#26102;&#65292;&#20063;&#33021;&#24378;&#21046;&#25191;&#34892;&#36825;&#31181;&#21704;&#23494;&#39039;&#24418;&#24335;&#12290;&#36825;&#19968;&#26694;&#26550;&#36890;&#36807;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#12289;&#19982;&#29289;&#29702;&#19968;&#33268;&#30340;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:1909.12077v5 Announce Type: replace  Abstract: In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system, given by an ordinary differential equation (ODE), from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consisten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.17010</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Finetuning Large Language Models for Vulnerability Detection. (arXiv:2401.17010v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;StarCoder&#30340;&#25913;&#36827;&#29256;&#26412;WizardCoder&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#24494;&#35843;&#23558;&#20854;&#36866;&#24212;&#20110;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;WizardCoder&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25506;&#31350;&#20102;&#26368;&#20339;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#38024;&#23545;&#36127;&#26679;&#26412;&#36828;&#22810;&#20110;&#27491;&#26679;&#26412;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#24494;&#35843;&#21518;&#30340;WizardCoder&#27169;&#22411;&#22312;&#24179;&#34913;&#21644;&#19981;&#24179;&#34913;&#30340;&#28431;&#27934;&#25968;&#25454;&#38598;&#19978;&#22312;ROC AUC&#21644;F1&#24230;&#37327;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#23545;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20854;&#35757;&#32451;&#36895;&#24230;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#24182;&#23545;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32479;&#35745;&#23376;&#32452;&#19981;&#20844;&#24179;&#24615;&#27010;&#24565;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#23457;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#39640;&#26031;&#20998;&#24067;&#30340;&#23457;&#35745;&#32467;&#26524;&#12290;&#20182;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#21033;&#29992;&#26080;&#20559;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.16439</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#26102;&#38388;&#19979;&#23545;&#39640;&#26031;&#25968;&#25454;&#36827;&#34892;&#32479;&#35745;&#23376;&#32452;&#19981;&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Polynomial time auditing of statistical subgroup fairness for Gaussian data. (arXiv:2401.16439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16439
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32479;&#35745;&#23376;&#32452;&#19981;&#20844;&#24179;&#24615;&#27010;&#24565;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#23457;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#39640;&#26031;&#20998;&#24067;&#30340;&#23457;&#35745;&#32467;&#26524;&#12290;&#20182;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#21033;&#29992;&#26080;&#20559;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#32479;&#35745;&#23376;&#32452;&#19981;&#20844;&#24179;&#24615;&#27010;&#24565;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#23457;&#35745;&#30340;&#38382;&#39064;&#12290;Kearns&#31561;&#20154;&#65288;2018&#65289;&#24050;&#32463;&#34920;&#26126;&#65292;&#23457;&#35745;&#32452;&#21512;&#23376;&#32452;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#19982;&#26080;&#20559;&#23398;&#20064;&#19968;&#26679;&#22256;&#38590;&#12290;&#23613;&#31649;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#27809;&#26377;&#24050;&#30693;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20294;&#20960;&#20046;&#25152;&#26377;&#35299;&#20915;&#23545;&#23376;&#32452;&#30340;&#32479;&#35745;&#27495;&#35270;&#24230;&#37327;&#30340;&#24037;&#20316;&#37117;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#27492;&#38382;&#39064;&#30340;&#39044;&#35328;&#26426;&#12290;&#22914;&#26524;&#25105;&#20204;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#65292;&#29978;&#33267;&#20165;&#26159;&#23547;&#24120;&#23545;&#25968;&#20985;&#26354;&#20998;&#24067;&#65292;&#37027;&#20040;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#24037;&#20316;&#24050;&#32463;&#21457;&#29616;&#20102;&#39640;&#25928;&#30340;&#26080;&#20559;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#21322;&#31354;&#38388;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;Kearns&#31561;&#20154;&#32473;&#20986;&#30340;&#25552;&#21319;&#39118;&#26684;&#30340;&#35268;&#32422;&#35201;&#27714;&#26080;&#20559;&#23398;&#20064;&#31639;&#27861;&#22312;&#21487;&#33021;&#19981;&#26159;&#23547;&#24120;&#23545;&#25968;&#20985;&#30340;&#37325;&#26032;&#21152;&#26435;&#20998;&#24067;&#19978;&#25104;&#21151;&#65292;&#21363;&#20351;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#26159;&#23547;&#24120;&#23545;&#25968;&#20985;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#39640;&#26031;&#20998;&#24067;&#30340;&#23457;&#35745;&#32467;&#26524;&#32473;&#20986;&#20102;&#27491;&#38754;&#21644;&#36127;&#38754;&#30340;&#32467;&#26524;&#65306;&#22312;&#27491;&#38754;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#26041;&#27861;&#26469;&#21033;&#29992;&#36825;&#20123;&#26080;&#20559;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of auditing classifiers with the notion of statistical subgroup fairness. Kearns et al. (2018) has shown that the problem of auditing combinatorial subgroups fairness is as hard as agnostic learning. Essentially all work on remedying statistical measures of discrimination against subgroups assumes access to an oracle for this problem, despite the fact that no efficient algorithms are known for it. If we assume the data distribution is Gaussian, or even merely log-concave, then a recent line of work has discovered efficient agnostic learning algorithms for halfspaces. Unfortunately, the boosting-style reductions given by Kearns et al. required the agnostic learning algorithm to succeed on reweighted distributions that may not be log-concave, even if the original data distribution was. In this work, we give positive and negative results on auditing for the Gaussian distribution: On the positive side, we an alternative approach to leverage these advances in agnostic l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#32508;&#21512;&#25351;&#21335;&#65292;&#21253;&#25324;&#29702;&#35770;&#22522;&#30784;&#12289;&#23454;&#38469;&#23454;&#29616;&#21644;&#27604;&#36739;&#32467;&#26524;&#65292;&#24182;&#23545;&#27491;&#21017;&#21270;&#30340;&#30410;&#22788;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.13662</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#26799;&#24230;&#30340;&#32456;&#26497;&#25351;&#21335;&#65306;&#29702;&#35770;&#12289;&#31639;&#27861;&#21644;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations. (arXiv:2401.13662v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#32508;&#21512;&#25351;&#21335;&#65292;&#21253;&#25324;&#29702;&#35770;&#22522;&#30784;&#12289;&#23454;&#38469;&#23454;&#29616;&#21644;&#27604;&#36739;&#32467;&#26524;&#65292;&#24182;&#23545;&#27491;&#21017;&#21270;&#30340;&#30410;&#22788;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#24378;&#22823;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#34429;&#28982;&#25152;&#26377;&#36825;&#20123;&#31639;&#27861;&#37117;&#24314;&#31435;&#22312;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#30340;&#22522;&#30784;&#19978;&#65292;&#20294;&#20855;&#20307;&#30340;&#35774;&#35745;&#36873;&#25321;&#22312;&#31639;&#27861;&#20043;&#38388;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25972;&#20307;&#30340;&#35270;&#35282;&#26469;&#27010;&#36848;&#22312;&#32447;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20415;&#29702;&#35299;&#23427;&#20204;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#38469;&#23454;&#29616;&#12290;&#22312;&#36825;&#20010;&#27010;&#36848;&#20013;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#36830;&#32493;&#29256;&#26412;&#30340;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#30340;&#35814;&#32454;&#35777;&#26126;&#12289;&#25910;&#25947;&#32467;&#26524;&#21644;&#23545;&#23454;&#38469;&#31639;&#27861;&#30340;&#20840;&#38754;&#35752;&#35770;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#26368;&#37325;&#35201;&#30340;&#31639;&#27861;&#65292;&#24182;&#23545;&#27491;&#21017;&#21270;&#30340;&#30410;&#22788;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#35265;&#35299;&#12290;&#25152;&#26377;&#30340;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/Matt00n/PolicyGradientsJax&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, various powerful policy gradient algorithms have been proposed in deep reinforcement learning. While all these algorithms build on the Policy Gradient Theorem, the specific design choices differ significantly across algorithms. We provide a holistic overview of on-policy policy gradient algorithms to facilitate the understanding of both their theoretical foundations and their practical implementations. In this overview, we include a detailed proof of the continuous version of the Policy Gradient Theorem, convergence results and a comprehensive discussion of practical algorithms. We compare the most prominent algorithms on continuous control environments and provide insights on the benefits of regularization. All code is available at https://github.com/Matt00n/PolicyGradientsJax.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;&#32858;&#21512;&#28789;&#27963;&#24615;&#25552;&#20379;&#31574;&#30053;&#21644;&#35780;&#20272;HVAC&#31995;&#32479;&#30340;&#20998;&#25955;&#28789;&#27963;&#24615;&#25552;&#20379;&#65292;&#20026;&#32858;&#21512;&#22120;&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#19981;&#30830;&#23450;&#24615;&#19979;&#23454;&#29616;&#38656;&#27714;&#21709;&#24212;&#25552;&#20379;&#20102;&#23454;&#29992;&#24037;&#20855;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#33073;&#30899;&#21644;&#22686;&#24378;&#33021;&#28304;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10726</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23454;&#29992;&#24037;&#20855;&#22686;&#24378;&#32858;&#21512;&#22120;&#33021;&#21147;&#65306;&#21033;&#29992;&#32858;&#21512;&#19982;&#20998;&#25955;&#30340;&#28789;&#27963;&#24615;&#23454;&#29616;&#38656;&#27714;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Empowering Aggregators with Practical Data-Driven Tools: Harnessing Aggregated and Disaggregated Flexibility for Demand Response. (arXiv:2401.10726v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;&#32858;&#21512;&#28789;&#27963;&#24615;&#25552;&#20379;&#31574;&#30053;&#21644;&#35780;&#20272;HVAC&#31995;&#32479;&#30340;&#20998;&#25955;&#28789;&#27963;&#24615;&#25552;&#20379;&#65292;&#20026;&#32858;&#21512;&#22120;&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#19981;&#30830;&#23450;&#24615;&#19979;&#23454;&#29616;&#38656;&#27714;&#21709;&#24212;&#25552;&#20379;&#20102;&#23454;&#29992;&#24037;&#20855;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#33073;&#30899;&#21644;&#22686;&#24378;&#33021;&#28304;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#24102;&#26469;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32858;&#21512;&#22120;&#21644;&#24314;&#31569;&#29289;&#23621;&#20303;&#32773;&#36890;&#36807;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#26041;&#26696;&#28608;&#27963;&#28789;&#27963;&#24615;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#65292;&#30528;&#37325;&#20110;&#23454;&#29616;&#31283;&#20581;&#30340;&#33073;&#30899;&#21644;&#22686;&#24378;&#33021;&#28304;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#20248;&#21270;&#32858;&#21512;&#28789;&#27963;&#24615;&#25552;&#20379;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;DFT&#65289;&#21644;&#32858;&#31867;&#25216;&#26415;&#35782;&#21035;&#24314;&#31569;&#29289;&#23621;&#27665;&#30340;&#27963;&#21160;&#27169;&#24335;&#12290;&#20854;&#27425;&#65292;&#30740;&#31350;&#35780;&#20272;&#20102;DR&#20107;&#20214;&#26399;&#38388;&#20379;&#28909;&#36890;&#39118;&#31354;&#35843;&#65288;HVAC&#65289;&#31995;&#32479;&#30340;&#20998;&#25955;&#28789;&#27963;&#24615;&#25552;&#20379;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#31934;&#30830;&#30340;&#35774;&#22791;&#32423;&#20998;&#26512;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20026;&#32858;&#21512;&#22120;&#22312;&#25972;&#20010;&#24314;&#31569;&#29289;&#28040;&#36153;&#20165;&#26377;&#19968;&#20010;&#26234;&#33021;&#30005;&#34920;&#30340;&#29615;&#22659;&#20013;&#25552;&#20379;&#28789;&#27963;&#24615;&#26381;&#21153;&#25552;&#20379;&#20102;&#19968;&#26465;&#38750;&#20405;&#20837;&#24615;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the crucial interplay between aggregators and building occupants in activating flexibility through Demand Response (DR) programs, with a keen focus on achieving robust decarbonization and fortifying the resilience of the energy system amidst the uncertainties presented by Renewable Energy Sources (RES). Firstly, it introduces a methodology of optimizing aggregated flexibility provision strategies in environments with limited data, utilizing Discrete Fourier Transformation (DFT) and clustering techniques to identify building occupant's activity patterns. Secondly, the study assesses the disaggregated flexibility provision of Heating Ventilation and Air Conditioning (HVAC) systems during DR events, employing machine learning and optimization techniques for precise, device-level analysis. The first approach offers a non-intrusive pathway for aggregators to provide flexibility services in environments of a single smart meter for the whole building's consumption, while t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#23545;&#30495;&#23454;&#27979;&#35797;&#20934;&#30830;&#24615;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#20998;&#31867;&#23618;&#26799;&#24230;&#33539;&#25968;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26080;&#27861;&#27867;&#21270;&#21040;&#27979;&#35797;&#25968;&#25454;&#38598;&#26102;&#65292;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22823;&#30340;&#26799;&#24230;&#33539;&#25968;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.08909</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#20934;&#30830;&#24615;&#20272;&#35745;&#19979;&#20998;&#24067;&#20559;&#31227;&#30340;&#26799;&#24230;&#29305;&#24449;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Characterising Gradients for Unsupervised Accuracy Estimation under Distribution Shift. (arXiv:2401.08909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#23545;&#30495;&#23454;&#27979;&#35797;&#20934;&#30830;&#24615;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#20998;&#31867;&#23618;&#26799;&#24230;&#33539;&#25968;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26080;&#27861;&#27867;&#21270;&#21040;&#27979;&#35797;&#25968;&#25454;&#38598;&#26102;&#65292;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22823;&#30340;&#26799;&#24230;&#33539;&#25968;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#21270;&#30340;&#27979;&#35797;&#29615;&#22659;&#19979;&#65292;&#26080;&#27861;&#35775;&#38382;&#30495;&#23454;&#27979;&#35797;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#27979;&#35797;&#20934;&#30830;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23433;&#20840;&#37096;&#32626;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#26497;&#20854;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#25110;&#25552;&#21462;&#29305;&#24449;&#30340;&#20449;&#24687;&#26469;&#24314;&#31435;&#19982;&#30495;&#23454;&#27979;&#35797;&#20934;&#30830;&#24615;&#30456;&#20851;&#30340;&#20272;&#35745;&#20998;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#30740;&#31350;&#25506;&#35752;&#20102;&#26799;&#24230;&#20449;&#24687;&#22914;&#20309;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#23545;&#30495;&#23454;&#27979;&#35797;&#20934;&#30830;&#24615;&#36827;&#34892;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#32463;&#36807;&#19968;&#27425;&#26799;&#24230;&#27493;&#38271;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#21518;&#21453;&#21521;&#20256;&#25773;&#30340;&#20998;&#31867;&#23618;&#26799;&#24230;&#33539;&#25968;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#22312;&#27169;&#22411;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#26080;&#27861;&#27867;&#21270;&#21040;&#27979;&#35797;&#25968;&#25454;&#38598;&#26102;&#65292;&#24212;&#24403;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22823;&#30340;&#26799;&#24230;&#33539;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#35265;&#35299;&#65292;&#31361;&#20986;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating test accuracy without access to the ground-truth test labels under varying test environments is a challenging, yet extremely important problem in the safe deployment of machine learning algorithms. Existing works rely on the information from either the outputs or the extracted features of neural networks to formulate an estimation score correlating with the ground-truth test accuracy. In this paper, we investigate--both empirically and theoretically--how the information provided by the gradients can be predictive of the ground-truth test accuracy even under a distribution shift. Specifically, we use the norm of classification-layer gradients, backpropagated from the cross-entropy loss after only one gradient step over test data. Our key idea is that the model should be adjusted with a higher magnitude of gradients when it does not generalize to the test dataset with a distribution shift. We provide theoretical insights highlighting the main ingredients of such an approach en
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#21551;&#21457;&#30340;&#22810;&#23610;&#24230;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#23558;&#22810;&#23610;&#24230;&#20998;&#26512;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#23545;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06040</link><description>&lt;p&gt;
&#22522;&#20110;&#23567;&#27874;&#21551;&#21457;&#30340;&#22810;&#23610;&#24230;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting. (arXiv:2401.06040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#21551;&#21457;&#30340;&#22810;&#23610;&#24230;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#23558;&#22810;&#23610;&#24230;&#20998;&#26512;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#23545;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#22320;&#23545;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#26576;&#20123;&#33258;&#28982;&#29305;&#24449;&#36827;&#34892;&#24314;&#27169;&#65292;&#27604;&#22914;&#21253;&#21547;&#19981;&#21516;&#31890;&#24230;&#25110;&#23610;&#24230;&#19978;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#21551;&#21457;&#30340;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#65288;WavGCRN&#65289;&#65292;&#23558;&#22810;&#23610;&#24230;&#20998;&#26512;&#65288;MSA&#65289;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#22312;WavGCRN&#20013;&#65292;&#20132;&#36890;&#25968;&#25454;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#34987;&#20998;&#35299;&#20026;&#26102;&#39057;&#20998;&#37327;&#65292;&#26500;&#24314;&#20102;&#22810;&#27969;&#36755;&#20837;&#32467;&#26500;&#65307;&#28982;&#21518;&#20351;&#29992;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#65288;GCRNs&#65289;&#20316;&#20026;&#32534;&#30721;&#22120;&#23545;&#27599;&#20010;&#27969;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#21462;&#19981;&#21516;&#23610;&#24230;&#30340;&#26102;&#31354;&#29305;&#24449;&#65307;&#26368;&#21518;&#65292;&#21487;&#23398;&#20064;&#30340;&#36870;DWT&#21644;GCRN&#34987;&#32467;&#21512;&#20026;&#35299;&#30721;&#22120;&#65292;&#34701;&#21512;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is the foundation for intelligent transportation systems. Spatiotemporal graph neural networks have demonstrated state-of-the-art performance in traffic forecasting. However, these methods do not explicitly model some of the natural characteristics in traffic data, such as the multiscale structure that encompasses spatial and temporal variations at different levels of granularity or scale. To that end, we propose a Wavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combines multiscale analysis (MSA)-based method with Deep Learning (DL)-based method. In WavGCRN, the traffic data is decomposed into time-frequency components with Discrete Wavelet Transformation (DWT), constructing a multi-stream input structure; then Graph Convolutional Recurrent networks (GCRNs) are employed as encoders for each stream, extracting spatiotemporal features in different scales; and finally the learnable Inversed DWT and GCRN are combined as the decoder, fusing the inf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEXR&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#31639;&#27861;&#22312;&#35299;&#20915;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#38382;&#39064;&#26102;&#20855;&#26377;&#20248;&#36234;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#23398;&#20064;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#25490;&#24207;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2312.02277</link><description>&lt;p&gt;
ALEXR:&#19968;&#31181;&#29992;&#20110;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#30340;&#26368;&#20248;&#21333;&#24490;&#29615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ALEXR: An Optimal Single-Loop Algorithm for Convex Finite-Sum Coupled Compositional Stochastic Optimization. (arXiv:2312.02277v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEXR&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#31639;&#27861;&#22312;&#35299;&#20915;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#38382;&#39064;&#26102;&#20855;&#26377;&#20248;&#36234;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#23398;&#20064;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#25490;&#24207;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#19968;&#31867;&#20855;&#26377;&#22810;&#20010;&#24212;&#29992;&#30340;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#65288;cFCCO&#65289;&#38382;&#39064;&#65292;&#21253;&#25324;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;GDRO&#65289;&#65292;&#19981;&#24179;&#34913;&#25968;&#25454;&#23398;&#20064;&#65292;&#24378;&#21270;&#23398;&#20064;&#21644;&#25490;&#24207;&#23398;&#20064;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#24490;&#29615;&#21407;&#22987;-&#23545;&#20598;&#22359;&#22352;&#26631;&#36817;&#31471;&#31639;&#27861;&#65292;&#31216;&#20026;ALEXR&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#22359;&#22352;&#26631;&#38543;&#26426;&#38236;&#20687;&#19978;&#21319;&#26356;&#26032;&#23545;&#20598;&#21464;&#37327;&#21644;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#21407;&#22987;&#21464;&#37327;&#12290;&#25105;&#20204;&#22312;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#20989;&#25968;&#26465;&#20214;&#19979;&#24314;&#31435;&#20102;ALEXR&#22312;&#20984;&#21644;&#24378;&#20984;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#36825;&#19981;&#20165;&#25913;&#36827;&#20102;&#20197;&#21069;&#22312;&#24179;&#28369;cFCCO&#38382;&#39064;&#19978;&#30340;&#26368;&#20339;&#36895;&#24230;&#65292;&#36824;&#25193;&#23637;&#20102;cFCCO&#30340;&#33539;&#22260;&#65292;&#29992;&#20110;&#35299;&#20915;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38750;&#24179;&#28369;&#38382;&#39064;&#65292;&#22914;GDRO&#30340;&#23545;&#20598;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36739;&#20302;&#30340;&#22797;&#26434;&#24615;&#19979;&#30028;&#65292;&#20197;&#35777;&#26126;&#31639;&#27861;&#20855;&#26377;&#24456;&#24378;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits a class of convex Finite-Sum Coupled Compositional Stochastic Optimization (cFCCO) problems with many applications, including group distributionally robust optimization (GDRO), learning with imbalanced data, reinforcement learning, and learning to rank. To better solve these problems, we introduce an efficient single-loop primal-dual block-coordinate proximal algorithm, dubbed ALEXR. This algorithm leverages block-coordinate stochastic mirror ascent updates for the dual variable and stochastic proximal gradient descent updates for the primal variable. We establish the convergence rates of ALEXR in both convex and strongly convex cases under smoothness and non-smoothness conditions of involved functions, which not only improve the best rates in previous works on smooth cFCCO problems but also expand the realm of cFCCO for solving more challenging non-smooth problems such as the dual form of GDRO. Finally, we present lower complexity bounds to demonstrate that the con
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20339;&#38480;&#21046;&#39044;&#31639;&#25298;&#32477;&#37319;&#26679;&#26041;&#27861;&#65288;OBRS&#65289;&#65292;&#21487;&#26174;&#33879;&#25913;&#21892;&#29983;&#25104;&#27169;&#22411;&#30340;&#26679;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#23558;&#37319;&#26679;&#26041;&#26696;&#19982;&#35757;&#32451;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#35813;&#26041;&#27861;&#22312;&#32473;&#23450;&#37319;&#26679;&#39044;&#31639;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#20219;&#20309;&#30495;&#23454;&#20998;&#24067;&#21644;&#25298;&#32477;&#21518;&#20998;&#24067;&#20043;&#38388;&#30340;f-&#25955;&#24230;&#37117;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2311.00460</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#20339;&#38480;&#21046;&#39044;&#31639;&#25298;&#32477;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Budgeted Rejection Sampling for Generative Models. (arXiv:2311.00460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00460
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20339;&#38480;&#21046;&#39044;&#31639;&#25298;&#32477;&#37319;&#26679;&#26041;&#27861;&#65288;OBRS&#65289;&#65292;&#21487;&#26174;&#33879;&#25913;&#21892;&#29983;&#25104;&#27169;&#22411;&#30340;&#26679;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#23558;&#37319;&#26679;&#26041;&#26696;&#19982;&#35757;&#32451;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#35813;&#26041;&#27861;&#22312;&#32473;&#23450;&#37319;&#26679;&#39044;&#31639;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#20219;&#20309;&#30495;&#23454;&#20998;&#24067;&#21644;&#25298;&#32477;&#21518;&#20998;&#24067;&#20043;&#38388;&#30340;f-&#25955;&#24230;&#37117;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#25298;&#32477;&#37319;&#26679;&#26041;&#27861;&#26469;&#25913;&#21892;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#22312;&#26080;&#38480;&#37319;&#26679;&#39044;&#31639;&#19979;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120;&#24212;&#29992;&#20110;&#19982;&#25298;&#32477;&#36807;&#31243;&#29420;&#31435;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;Optimal Budgeted Rejection Sampling (OBRS)&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#32473;&#23450;&#37319;&#26679;&#39044;&#31639;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#30495;&#23454;&#20998;&#24067;&#21644;&#25298;&#32477;&#21518;&#20998;&#24067;&#20043;&#38388;&#30340;&#20219;&#20309;f-&#25955;&#24230;&#35777;&#26126;&#26159;&#26368;&#20248;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#23558;&#37319;&#26679;&#26041;&#26696;&#34701;&#20837;&#35757;&#32451;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#21644;&#25903;&#25345;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rejection sampling methods have recently been proposed to improve the performance of discriminator-based generative models. However, these methods are only optimal under an unlimited sampling budget, and are usually applied to a generator trained independently of the rejection procedure. We first propose an Optimal Budgeted Rejection Sampling (OBRS) scheme that is provably optimal with respect to \textit{any} $f$-divergence between the true distribution and the post-rejection distribution, for a given sampling budget. Second, we propose an end-to-end method that incorporates the sampling scheme into the training procedure to further enhance the model's overall performance. Through experiments and supporting theory, we show that the proposed methods are effective in significantly improving the quality and diversity of the samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26641;&#24230;&#37327;&#26377;&#22122;&#22768;&#30340;&#20248;&#21270;&#20256;&#36755;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#26641;&#32467;&#26500;&#25200;&#21160;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13653</link><description>&lt;p&gt;
&#37319;&#29992;&#26377;&#22122;&#22768;&#26641;&#24230;&#37327;&#30340;&#20248;&#21270;&#20256;&#36755;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Measures with Noisy Tree Metric. (arXiv:2310.13653v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26641;&#24230;&#37327;&#26377;&#22122;&#22768;&#30340;&#20248;&#21270;&#20256;&#36755;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#26641;&#32467;&#26500;&#25200;&#21160;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26641;&#24230;&#37327;&#31354;&#38388;&#19978;&#25903;&#25345;&#30340;&#27010;&#29575;&#27979;&#24230;&#30340;&#20248;&#21270;&#20256;&#36755;&#65288;OT&#65289;&#38382;&#39064;&#12290;&#24050;&#30693;&#36825;&#31181;OT&#38382;&#39064;&#65288;&#21363;&#26641;-&#29926;&#29926;&#26031;&#22374;&#65288;TW&#65289;&#65289;&#20855;&#26377;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#65292;&#20294;&#22522;&#26412;&#19978;&#21462;&#20915;&#20110;&#36755;&#20837;&#27979;&#24230;&#25903;&#25345;&#19978;&#30340;&#24213;&#23618;&#26641;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#25805;&#20316;&#20013;&#65292;&#30001;&#20110;&#22122;&#22768;&#25110;&#23545;&#25239;&#24615;&#27979;&#37327;&#65292;&#32473;&#23450;&#30340;&#26641;&#32467;&#26500;&#21487;&#33021;&#20250;&#34987;&#25200;&#21160;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#26368;&#22823;-&#26368;&#23567;&#40065;&#26834;OT&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#22312;&#19968;&#20010;&#26641;&#24230;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#19978;&#20004;&#20010;&#36755;&#20837;&#27979;&#24230;&#20043;&#38388;&#30340;&#26368;&#22823;&#21487;&#33021;&#36317;&#31163;&#12290;&#24635;&#20307;&#19978;&#35828;&#65292;&#30001;&#20110;&#20854;&#38750;&#20984;&#24615;&#21644;&#38750;&#20809;&#28369;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#35745;&#31639;&#65292;&#21363;&#20415;&#26159;&#22312;&#25903;&#25345;&#20026;1&#32500;&#31354;&#38388;&#30340;&#27979;&#24230;&#24773;&#20917;&#19979;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#35268;&#27169;&#24773;&#26223;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#36793;&#32536;&#21024;&#38500;/&#28155;&#21152;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26641;&#24230;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#36825;&#20010;&#38598;&#21512;&#22312;&#19968;&#20010;&#20248;&#38597;&#30340;&#26694;&#26550;&#19979;&#28085;&#30422;&#20102;&#22810;&#26679;&#30340;&#26641;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study optimal transport (OT) problem for probability measures supported on a tree metric space. It is known that such OT problem (i.e., tree-Wasserstein (TW)) admits a closed-form expression, but depends fundamentally on the underlying tree structure over supports of input measures. In practice, the given tree structure may be, however, perturbed due to noisy or adversarial measurements. In order to mitigate this issue, we follow the max-min robust OT approach which considers the maximal possible distances between two input measures over an uncertainty set of tree metrics. In general, this approach is hard to compute, even for measures supported in $1$-dimensional space, due to its non-convexity and non-smoothness which hinders its practical applications, especially for large-scale settings. In this work, we propose \emph{novel uncertainty sets of tree metrics} from the lens of edge deletion/addition which covers a diversity of tree structures in an elegant framework. Consequently, 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12508</link><description>&lt;p&gt;
SalUn&#65306;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26435;&#37325;&#26174;&#33879;&#24615;&#22686;&#24378;&#26426;&#22120;&#36951;&#24536;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12508
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#27861;&#35268;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;&#24403;&#21069;AI&#27169;&#22411;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MU&#26041;&#27861;&#36890;&#24120;&#22312;&#36951;&#24536;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36328;&#39046;&#22495;&#36866;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MU&#20013;&#30340;&#8220;&#26435;&#37325;&#26174;&#33879;&#24615;&#8221;&#27010;&#24565;&#65292;&#20511;&#37492;&#20102;&#27169;&#22411;&#35299;&#37322;&#20013;&#30340;&#36755;&#20837;&#26174;&#33879;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#23558;MU&#30340;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20102;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#26174;&#33879;&#24615;&#36951;&#24536;&#65288;SalUn&#65289;&#30340;&#26041;&#27861;&#23558;&#20854;&#19982;&#8220;&#31934;&#30830;&#8221;&#36951;&#24536;&#65288;&#22312;&#21024;&#38500;&#36951;&#24536;&#25968;&#25454;&#38598;&#21518;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65289;&#30340;&#24615;&#33021;&#24046;&#36317;&#32553;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SalUn&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;MU&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;SalUn&#21487;&#22312;&#22270;&#29255;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#25830;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#26292;&#38706;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#24503;&#22269;&#23460;&#20869;&#27681;&#27668;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.11143</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#26292;&#38706;&#27169;&#22411;&#30340;&#24503;&#22269;&#39640;&#20998;&#36776;&#29575;&#23460;&#20869;&#27681;&#27668;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
A new high-resolution indoor radon map for Germany using a machine learning based probabilistic exposure model. (arXiv:2310.11143v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#26292;&#38706;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#24503;&#22269;&#23460;&#20869;&#27681;&#27668;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#27681;&#27668;&#26159;&#19968;&#31181;&#33268;&#30284;&#30340;&#25918;&#23556;&#24615;&#27668;&#20307;&#65292;&#21487;&#20197;&#22312;&#23460;&#20869;&#31215;&#32047;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#23460;&#20869;&#27681;&#26292;&#38706;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;&#27979;&#37327;&#27963;&#21160;&#20272;&#35745;&#24471;&#26469;&#30340;&#12290;&#28982;&#32780;&#65292;&#26679;&#26412;&#30340;&#29305;&#24449;&#24448;&#24448;&#19982;&#20154;&#21475;&#29305;&#24449;&#19981;&#21516;&#65292;&#36825;&#26159;&#30001;&#20110;&#35768;&#22810;&#30456;&#20851;&#22240;&#32032;&#65292;&#22914;&#22320;&#36136;&#28304;&#27681;&#27668;&#30340;&#21487;&#29992;&#24615;&#25110;&#27004;&#23618;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#26679;&#26412;&#22823;&#23567;&#36890;&#24120;&#19981;&#20801;&#35768;&#20197;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#36827;&#34892;&#26292;&#38706;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;&#32431;&#25968;&#25454;&#26041;&#27861;&#26356;&#21152;&#29616;&#23454;&#22320;&#20272;&#35745;&#23460;&#20869;&#27681;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#24314;&#27169;&#26041;&#27861;&#65306;1&#65289;&#24212;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#65292;&#20351;&#29992;&#29615;&#22659;&#21644;&#24314;&#31569;&#25968;&#25454;&#20316;&#20026;&#39044;&#27979;&#22240;&#23376;&#65292;&#20272;&#35745;&#20102;&#24503;&#22269;&#27599;&#20010;&#20303;&#23429;&#27004;&#30340;&#27599;&#20010;&#27004;&#23618;&#30340;&#23460;&#20869;&#27681;&#27010;&#29575;&#20998;&#24067;&#20989;&#25968;&#65307;2&#65289;&#20351;&#29992;&#27010;&#29575;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#25216;&#26415;&#20351;&#23427;&#20204;&#32452;&#21512;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radon is a carcinogenic, radioactive gas that can accumulate indoors. Indoor radon exposure at the national scale is usually estimated on the basis of extensive measurement campaigns. However, characteristics of the sample often differ from the characteristics of the population due to the large number of relevant factors such as the availability of geogenic radon or floor level. Furthermore, the sample size usually does not allow exposure estimation with high spatial resolution. We propose a model-based approach that allows a more realistic estimation of indoor radon distribution with a higher spatial resolution than a purely data-based approach. We applied a two-stage modelling approach: 1) a quantile regression forest using environmental and building data as predictors was applied to estimate the probability distribution function of indoor radon for each floor level of each residential building in Germany; (2) a probabilistic Monte Carlo sampling technique enabled the combination and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22823;&#22411;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#36739;&#23567;&#30340;&#38598;&#21512;&#32780;&#19981;&#20250;&#25439;&#22833;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#22270;&#32467;&#26500;&#21387;&#32553;&#38382;&#39064;&#36716;&#21270;&#20026;&#26680;&#23725;&#22238;&#24402;&#20219;&#21153;&#65292;&#21033;&#29992;&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#26469;&#25429;&#25417;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.11046</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#30340;&#24555;&#36895;&#22270;&#32467;&#26500;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Fast Graph Condensation with Structure-based Neural Tangent Kernel. (arXiv:2310.11046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22823;&#22411;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#36739;&#23567;&#30340;&#38598;&#21512;&#32780;&#19981;&#20250;&#25439;&#22833;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#22270;&#32467;&#26500;&#21387;&#32553;&#38382;&#39064;&#36716;&#21270;&#20026;&#26680;&#23725;&#22238;&#24402;&#20219;&#21153;&#65292;&#21033;&#29992;&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#26469;&#25429;&#25417;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#36896;&#25104;&#20102;&#22823;&#37327;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#22270;&#25968;&#25454;&#26102;&#20250;&#23548;&#33268;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#24320;&#38144;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22823;&#22411;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#36739;&#23567;&#30340;&#38598;&#21512;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#21452;&#23618;&#20248;&#21270;&#26550;&#26500;&#26469;&#21387;&#32553;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#21516;&#26679;&#20063;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#26412;&#25991;&#23558;&#22270;&#32467;&#26500;&#21387;&#32553;&#38382;&#39064;&#25913;&#20026;&#26680;&#23725;&#22238;&#24402;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#22312;&#21452;&#23618;&#20248;&#21270;&#30340;&#20869;&#24490;&#29615;&#20013;&#36845;&#20195;&#35757;&#32451;GNN&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#26694;&#26550;&#65288;GC-SNTK&#65289;&#65292;&#20854;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#65288;SNTK&#65289;&#26469;&#25429;&#25417;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of Internet technology has given rise to a vast amount of graph-structured data. Graph Neural Networks (GNNs), as an effective method for various graph mining tasks, incurs substantial computational resource costs when dealing with large-scale graph data. A data-centric manner solution is proposed to condense the large graph dataset into a smaller one without sacrificing the predictive performance of GNNs. However, existing efforts condense graph-structured data through a computational intensive bi-level optimization architecture also suffer from massive computation costs. In this paper, we propose reforming the graph condensation problem as a Kernel Ridge Regression (KRR) task instead of iteratively training GNNs in the inner loop of bi-level optimization. More specifically, We propose a novel dataset condensation framework (GC-SNTK) for graph-structured data, where a Structure-based Neural Tangent Kernel (SNTK) is developed to capture the topology of graph and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2310.02304</link><description>&lt;p&gt;
&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65306;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. (arXiv:2310.02304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;&#24605;&#32500;&#26641;&#21644;&#31243;&#24207;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#65289;&#21462;&#24471;&#20102;&#19968;&#20123;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#8220;&#33050;&#25163;&#26550;&#8221;&#31243;&#24207;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#35813;&#31243;&#24207;&#26500;&#24314;&#20102;&#22810;&#27425;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;&#33050;&#25163;&#26550;&#31243;&#24207;&#36890;&#24120;&#20351;&#29992;Python&#31561;&#32534;&#31243;&#35821;&#35328;&#32534;&#20889;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#31181;&#23376;&#8220;&#25913;&#36827;&#22120;&#8221;&#24320;&#22987;&#65292;&#36890;&#36807;&#22810;&#27425;&#26597;&#35810;&#35821;&#35328;&#27169;&#22411;&#24182;&#36820;&#22238;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#26681;&#25454;&#32473;&#23450;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#25913;&#36827;&#36755;&#20837;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36816;&#34892;&#36825;&#20010;&#31181;&#23376;&#25913;&#36827;&#22120;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#22312;&#19968;&#31995;&#21015;&#32454;&#20998;&#20219;&#21153;&#20013;&#65292;&#24471;&#21040;&#30340;&#25913;&#36827;&#25913;&#36827;&#22120;&#29983;&#25104;&#30340;&#31243;&#24207;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#31181;&#23376;&#25913;&#36827;&#22120;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#30340;&#21508;&#31181;&#33258;&#25105;&#25913;&#36827;&#31574;&#30053;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21253;&#25324;&#27874;&#26463;&#25628;&#32034;&#12289;&#36951;&#20256;&#31639;&#27861;&#21644;&#27169;&#25311;&#36864;&#28779;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#27809;&#26377;&#25913;&#21464;&#65292;&#36825;&#24182;&#19981;&#26159;&#19968;&#31181;&#22686;&#38271;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not fu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20648;&#22791;&#35745;&#31639;&#22312;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#38544;&#34255;&#30340;&#26435;&#37325;&#30697;&#38453;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#23545;AR&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23436;&#32654;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.00290</link><description>&lt;p&gt;
&#23436;&#32654;&#39044;&#27979;&#20648;&#22791;&#35745;&#31639;&#22312;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Mathematical structure of perfect predictive reservoir computing for autoregressive type of time series data. (arXiv:2310.00290v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00290
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20648;&#22791;&#35745;&#31639;&#22312;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#38544;&#34255;&#30340;&#26435;&#37325;&#30697;&#38453;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#23545;AR&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23436;&#32654;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20648;&#22791;&#35745;&#31639;&#65288;RC&#65289;&#26159;&#19968;&#31181;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#27627;&#26080;&#30097;&#38382;&#65292;RC&#23558;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#26500;&#24314;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26410;&#26469;&#39044;&#27979;&#27169;&#22411;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#12289;&#39640;&#36895;&#24230;&#21644;&#39640;&#35745;&#31639;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;RC&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#32467;&#26500;&#30340;&#30740;&#31350;&#30452;&#21040;&#26368;&#36817;&#25165;&#24320;&#22987;&#12290;Bollt&#65288;2021&#65289;&#38416;&#26126;&#20102;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;RC&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#32467;&#26500;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;Wold&#20998;&#35299;&#23450;&#29702;&#26159;&#29702;&#35299;&#36825;&#20123;&#32467;&#26500;&#30340;&#37324;&#31243;&#30865;&#12290;&#22312;&#38125;&#35760;&#36825;&#19968;&#33879;&#21517;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#38416;&#26126;&#20102;RC&#31070;&#32463;&#32593;&#32476;&#20013;&#36755;&#20837;&#21644;&#24490;&#29615;&#26435;&#37325;&#30697;&#38453;&#30340;&#38544;&#34255;&#32467;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#32467;&#26500;&#23545;&#20110;AR&#31867;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23454;&#29616;&#20102;&#23436;&#32654;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir Computing (RC) is a type of recursive neural network (RNN), and there can be no doubt that the RC will be more and more widely used for building future prediction models for time-series data, with low training cost, high speed and high computational power. However, research into the mathematical structure of RC neural networks has only recently begun. Bollt (2021) clarified the necessity of the autoregressive (AR) model for gaining the insight into the mathematical structure of RC neural networks, and indicated that the Wold decomposition theorem is the milestone for understanding of these. Keeping this celebrated result in mind, in this paper, we clarify hidden structures of input and recurrent weight matrices in RC neural networks, and show that such structures attain perfect prediction for the AR type of time series data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#37096;&#32626;&#22312;6G&#36793;&#32536;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;LLMs&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#24182;&#20174;&#21709;&#24212;&#26102;&#38388;&#12289;&#24102;&#23485;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#31561;&#26041;&#38754;&#20998;&#26512;&#20102;&#20113;&#31471;&#37096;&#32626;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#30340;&#21019;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16739</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33267;6G&#36793;&#32536;&#65306;&#35270;&#37326;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities. (arXiv:2309.16739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#37096;&#32626;&#22312;6G&#36793;&#32536;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;LLMs&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#24182;&#20174;&#21709;&#24212;&#26102;&#38388;&#12289;&#24102;&#23485;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#31561;&#26041;&#38754;&#20998;&#26512;&#20102;&#20113;&#31471;&#37096;&#32626;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#30340;&#21019;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#27491;&#22312;&#25913;&#21464;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#26377;&#21487;&#33021;&#22609;&#36896;&#25105;&#20204;&#30340;&#26410;&#26469;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#30340;&#22810;&#27169;&#24577;&#29305;&#24615;&#65292;&#24403;&#21069;&#30340;&#22522;&#20110;&#20113;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65306;1) &#21709;&#24212;&#26102;&#38388;&#38271;&#65307;2) &#39640;&#24102;&#23485;&#25104;&#26412;&#65307;&#20197;&#21450;3) &#36829;&#21453;&#25968;&#25454;&#38544;&#31169;&#12290;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#36843;&#20999;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;6G&#36793;&#32536;&#37096;&#32626;LLMs&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#30001;&#22810;&#27169;&#24577;LLMs&#25552;&#20379;&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#21307;&#30103;&#20445;&#20581;&#65292;&#20197;&#31361;&#20986;&#22312;&#32456;&#31471;&#29992;&#25143;&#38468;&#36817;&#37096;&#32626;LLMs&#30340;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#36793;&#32536;&#37096;&#32626;LLMs&#26102;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#35774;&#24819;&#20102;&#36866;&#29992;&#20110;LLMs&#30340;6G MEC&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20004;&#20010;&#35774;&#35745;&#26041;&#38754;&#65292;&#21363;LLMs&#30340;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#12290;&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#36793;&#32536;&#30340;&#22266;&#26377;&#36164;&#28304;&#38480;&#21046;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21508;&#31181;&#21069;&#27839;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), which have shown remarkable capabilities, are revolutionizing AI development and potentially shaping our future. However, given their multimodality, the status quo cloud-based deployment faces some critical challenges: 1) long response time; 2) high bandwidth costs; and 3) the violation of data privacy. 6G mobile edge computing (MEC) systems may resolve these pressing issues. In this article, we explore the potential of deploying LLMs at the 6G edge. We start by introducing killer applications powered by multimodal LLMs, including robotics and healthcare, to highlight the need for deploying LLMs in the vicinity of end users. Then, we identify the critical challenges for LLM deployment at the edge and envision the 6G MEC architecture for LLMs. Furthermore, we delve into two design aspects, i.e., edge training and edge inference for LLMs. In both aspects, considering the inherent resource limitations at the edge, we discuss various cutting-edge techniques, i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#21521;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#23545;&#29992;&#26799;&#24230;&#27969;&#35757;&#32451;&#30340;&#38750;&#32447;&#24615;&#32593;&#32476;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#22312;&#32593;&#32476;&#20197;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#31163;&#25955;&#21270;&#24418;&#24335;&#21021;&#22987;&#21270;&#21518;&#65292;&#36825;&#31181;&#31163;&#25955;&#21270;&#23558;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2309.01213</link><description>&lt;p&gt;
&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#19982;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Implicit regularization of deep residual networks towards neural ODEs. (arXiv:2309.01213v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#21521;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#23545;&#29992;&#26799;&#24230;&#27969;&#35757;&#32451;&#30340;&#38750;&#32447;&#24615;&#32593;&#32476;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#22312;&#32593;&#32476;&#20197;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#31163;&#25955;&#21270;&#24418;&#24335;&#21021;&#22987;&#21270;&#21518;&#65292;&#36825;&#31181;&#31163;&#25955;&#21270;&#23558;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#26159;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#23427;&#20204;&#30340;&#36830;&#32493;&#28145;&#24230;&#27169;&#25311;&#31216;&#20026;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#65292;&#20063;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#31163;&#25955;&#27169;&#22411;&#19982;&#36830;&#32493;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#20173;&#32570;&#20047;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#38024;&#23545;&#29992;&#26799;&#24230;&#27969;&#35757;&#32451;&#30340;&#38750;&#32447;&#24615;&#32593;&#32476;&#30340;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#21521;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26469;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#32593;&#32476;&#30340;&#21021;&#22987;&#21270;&#26159;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#31163;&#25955;&#21270;&#65292;&#21017;&#36825;&#31181;&#31163;&#25955;&#21270;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#26377;&#38480;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#35757;&#32451;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#37117;&#25104;&#31435;&#65292;&#21482;&#35201;&#32593;&#32476;&#28385;&#36275;Polyak-Lojasiewicz&#26465;&#20214;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20010;&#26465;&#20214;&#36866;&#29992;&#20110;&#19968;&#20010;&#27531;&#24046;&#32593;&#32476;&#23478;&#26063;&#65292;&#20854;&#20013;&#27531;&#24046;&#26159;&#20004;&#23618;&#24863;&#30693;&#26426;&#65292;&#22312;&#23485;&#24230;&#19978;&#21482;&#26159;&#32447;&#24615;&#36229;&#21442;&#25968;&#21270;&#65292;&#24182;&#19988;&#26263;&#31034;&#20102;&#26799;&#24230;&#27969;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Residual neural networks are state-of-the-art deep learning models. Their continuous-depth analog, neural ordinary differential equations (ODEs), are also widely used. Despite their success, the link between the discrete and continuous models still lacks a solid mathematical foundation. In this article, we take a step in this direction by establishing an implicit regularization of deep residual networks towards neural ODEs, for nonlinear networks trained with gradient flow. We prove that if the network is initialized as a discretization of a neural ODE, then such a discretization holds throughout training. Our results are valid for a finite training time, and also as the training time tends to infinity provided that the network satisfies a Polyak-Lojasiewicz condition. Importantly, this condition holds for a family of residual networks where the residuals are two-layer perceptrons with an overparameterization in width that is only linear, and implies the convergence of gradient flow to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EVE&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#36974;&#34109;&#20449;&#21495;&#24314;&#27169;&#21644;&#27169;&#24577;&#24863;&#30693;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;Transformer&#32593;&#32476;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36827;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11971</link><description>&lt;p&gt;
EVE: &#20351;&#29992;&#36974;&#34109;&#39044;&#27979;&#21644;&#27169;&#24577;&#24863;&#30693;&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE. (arXiv:2308.11971v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EVE&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#36974;&#34109;&#20449;&#21495;&#24314;&#27169;&#21644;&#27169;&#24577;&#24863;&#30693;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;Transformer&#32593;&#32476;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36827;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;EVE&#30340;&#39640;&#25928;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#26159;&#30001;&#19968;&#31181;&#32479;&#19968;&#30340;Transformer&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;EVE&#36890;&#36807;&#22312;&#22270;&#20687;-&#25991;&#26412;&#23545;&#19978;&#36827;&#34892;&#36974;&#34109;&#20449;&#21495;&#24314;&#27169;&#26469;&#32479;&#19968;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#37325;&#24314;&#21487;&#35265;&#20449;&#21495;&#65292;&#21363;&#22270;&#20687;&#20687;&#32032;&#21644;&#25991;&#26412;&#26631;&#35760;&#12290;&#36890;&#36807;&#38598;&#25104;&#27169;&#24577;&#24863;&#30693;&#30340;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22359;&#65292;EVE&#22312;&#19968;&#20010;&#20849;&#20139;&#30340;Transformer&#32593;&#32476;&#20013;&#32534;&#30721;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20999;&#25442;&#21040;&#19981;&#21516;&#30340;&#19987;&#23478;&#26469;&#25429;&#25417;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21518;&#38376;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#20851;&#38190;&#23618;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#20123;&#23618;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#19979;&#23454;&#29616;&#25915;&#20987;&#25928;&#26524;&#21644;&#38544;&#34109;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.04466</link><description>&lt;p&gt;
&#21518;&#38376;&#32852;&#37030;&#23398;&#20064;&#65306;&#36890;&#36807;&#27745;&#26579;&#21518;&#38376;&#20851;&#38190;&#23618;
&lt;/p&gt;
&lt;p&gt;
Backdoor Federated Learning by Poisoning Backdoor-Critical Layers. (arXiv:2308.04466v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04466
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21518;&#38376;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#20851;&#38190;&#23618;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#20123;&#23618;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#19979;&#23454;&#29616;&#25915;&#20987;&#25928;&#26524;&#21644;&#38544;&#34109;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#35774;&#22791;&#19978;&#36827;&#34892;&#25935;&#24863;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20998;&#25955;&#24335;&#23398;&#20064;&#33539;&#24335;&#21644;FL&#30340;&#24322;&#36136;&#24615;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#25915;&#20987;&#38754;&#12290;&#29616;&#26377;&#30340;FL&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#36890;&#24120;&#20250;&#20851;&#27880;&#25972;&#20010;&#27169;&#22411;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#26041;&#27861;&#24847;&#35782;&#21040;&#21518;&#38376;&#20851;&#38190;&#65288;BC&#65289;&#23618;&#30340;&#23384;&#22312;&#65292;&#21518;&#38376;&#20851;&#38190;&#23618;&#26159;&#25351;&#25511;&#21046;&#27169;&#22411;&#28431;&#27934;&#30340;&#19968;&#23567;&#37096;&#20998;&#23618;&#12290;&#25915;&#20987;BC&#23618;&#21487;&#20197;&#36798;&#21040;&#25915;&#20987;&#25972;&#20010;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20294;&#34987;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25163;&#27573;&#21457;&#29616;&#30340;&#26426;&#20250;&#35201;&#23567;&#24471;&#22810;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#25915;&#20987;&#32773;&#30340;&#35282;&#24230;&#35782;&#21035;&#21644;&#39564;&#35777;BC&#23618;&#30340;&#26222;&#36866;&#24615;&#26041;&#27861;&#12290;&#22522;&#20110;&#35782;&#21035;&#20986;&#30340;BC&#23618;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#38450;&#24481;&#31574;&#30053;&#33258;&#36866;&#24212;&#22320;&#23547;&#27714;&#25915;&#20987;&#25928;&#26524;&#21644;&#38544;&#34109;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has been widely deployed to enable machine learning training on sensitive data across distributed devices. However, the decentralized learning paradigm and heterogeneity of FL further extend the attack surface for backdoor attacks. Existing FL attack and defense methodologies typically focus on the whole model. None of them recognizes the existence of backdoor-critical (BC) layers-a small subset of layers that dominate the model vulnerabilities. Attacking the BC layers achieves equivalent effects as attacking the whole model but at a far smaller chance of being detected by state-of-the-art (SOTA) defenses. This paper proposes a general in-situ approach that identifies and verifies BC layers from the perspective of attackers. Based on the identified BC layers, we carefully craft a new backdoor attack methodology that adaptively seeks a fundamental balance between attacking effects and stealthiness under various defense strategies. Extensive experiments show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20809;&#35889;&#26041;&#27861;&#22312;&#24191;&#20041;&#22810;&#32500;&#27604;&#36739;&#20013;&#20272;&#35745;&#21644;&#37327;&#21270;&#26410;&#35266;&#23519;&#21040;&#30340;&#27604;&#36739;&#23454;&#20307;&#30340;&#20559;&#22909;&#20998;&#25968;&#30340;&#24615;&#33021;&#65292;&#24182;&#25581;&#31034;&#20102;&#20809;&#35889;&#20272;&#35745;&#37327;&#19982;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.02918</link><description>&lt;p&gt;
&#22522;&#20110;&#24191;&#20041;&#22810;&#32500;&#27604;&#36739;&#30340;&#20809;&#35889;&#25490;&#21517;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Spectral Ranking Inferences based on General Multiway Comparisons. (arXiv:2308.02918v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20809;&#35889;&#26041;&#27861;&#22312;&#24191;&#20041;&#22810;&#32500;&#27604;&#36739;&#20013;&#20272;&#35745;&#21644;&#37327;&#21270;&#26410;&#35266;&#23519;&#21040;&#30340;&#27604;&#36739;&#23454;&#20307;&#30340;&#20559;&#22909;&#20998;&#25968;&#30340;&#24615;&#33021;&#65292;&#24182;&#25581;&#31034;&#20102;&#20809;&#35889;&#20272;&#35745;&#37327;&#19982;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#38750;&#24120;&#26222;&#36941;&#21644;&#26356;&#21152;&#30495;&#23454;&#30340;&#24773;&#26223;&#20013;&#65292;&#20351;&#29992;&#20809;&#35889;&#26041;&#27861;&#23545;&#26410;&#35266;&#23519;&#21040;&#30340;&#27604;&#36739;&#23454;&#20307;&#30340;&#20559;&#22909;&#20998;&#25968;&#36827;&#34892;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27604;&#36739;&#22270;&#30001;&#21487;&#33021;&#20855;&#26377;&#24322;&#26500;&#22823;&#23567;&#30340;&#36229;&#36793;&#32452;&#25104;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#36229;&#36793;&#65292;&#27604;&#36739;&#25968;&#37327;&#21487;&#33021;&#20165;&#20026;1&#12290;&#36825;&#31181;&#35774;&#32622;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#25351;&#23450;&#22270;&#30340;&#38543;&#26426;&#24615;&#20197;&#21450;&#22312;&#24120;&#29992;&#30340;Bradley-Terry-Luce (BTL)&#25110;Plackett-Luce (PL)&#27169;&#22411;&#20013;&#26045;&#21152;&#30340;&#38480;&#21046;&#24615;&#22343;&#21248;&#37319;&#26679;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#22312;&#36866;&#29992;BTL&#25110;PL&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20809;&#35889;&#20272;&#35745;&#37327;&#19982;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#37327;&#65288;MLE&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#24212;&#29992;&#20174;&#31561;&#26435;&#37325;&#20256;&#32479;&#20809;&#35889;&#26041;&#27861;&#20272;&#35745;&#24471;&#21040;&#30340;&#26368;&#20339;&#21152;&#26435;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;MLE&#30456;&#21516;&#30340;&#28176;&#36817;&#25928;&#29575;&#30340;&#21452;&#27493;&#20809;&#35889;&#26041;&#27861;&#12290;&#32771;&#34385;&#21040;&#28176;&#36817;&#24773;&#20917;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper studies the performance of the spectral method in the estimation and uncertainty quantification of the unobserved preference scores of compared entities in a very general and more realistic setup in which the comparison graph consists of hyper-edges of possible heterogeneous sizes and the number of comparisons can be as low as one for a given hyper-edge. Such a setting is pervasive in real applications, circumventing the need to specify the graph randomness and the restrictive homogeneous sampling assumption imposed in the commonly-used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. Furthermore, in the scenarios when the BTL or PL models are appropriate, we unravel the relationship between the spectral estimator and the Maximum Likelihood Estimator (MLE). We discover that a two-step spectral method, where we apply the optimal weighting estimated from the equal weighting vanilla spectral method, can achieve the same asymptotic efficiency as the MLE. Given the asymptot
&lt;/p&gt;</description></item><item><title>SABRE&#26159;&#19968;&#31181;&#24378;&#40065;&#26834;&#24615;&#21464;&#20998;&#36125;&#21494;&#26031;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#30340;&#32858;&#21512;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#38750;IID&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545;&#25968;&#25454;/&#27169;&#22411;&#25915;&#20987;&#20855;&#22791;&#40065;&#26834;&#24615;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.02747</link><description>&lt;p&gt;
SABRE:&#24378;&#40065;&#26834;&#36125;&#21494;&#26031;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SABRE: Robust Bayesian Peer-to-Peer Federated Learning. (arXiv:2308.02747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02747
&lt;/p&gt;
&lt;p&gt;
SABRE&#26159;&#19968;&#31181;&#24378;&#40065;&#26834;&#24615;&#21464;&#20998;&#36125;&#21494;&#26031;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#30340;&#32858;&#21512;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#38750;IID&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545;&#25968;&#25454;/&#27169;&#22411;&#25915;&#20987;&#20855;&#22791;&#40065;&#26834;&#24615;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SABRE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#40065;&#26834;&#24615;&#21464;&#20998;&#36125;&#21494;&#26031;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#24050;&#30693;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65288;BayP2PFL&#65289;&#30340;&#25239;&#25915;&#20987;&#24615;&#65292;&#38543;&#21518;&#35777;&#26126;&#20102;BayP2PFL&#22312;&#38754;&#23545;&#36825;&#20123;&#25915;&#20987;&#26102;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SABRE&#32858;&#21512;&#26041;&#27861;&#26469;&#20811;&#26381;&#29616;&#26377;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#12290;SABRE&#22312;&#38750;IID&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#19981;&#35201;&#27714;&#22823;&#22810;&#25968;&#33391;&#24615;&#33410;&#28857;&#22810;&#20110;&#21463;&#25439;&#33410;&#28857;&#65292;&#24182;&#19988;&#22312;&#33391;&#24615;&#29615;&#22659;&#19979;&#29978;&#33267;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;&#25105;&#20204;&#20174;&#21435;&#20013;&#24515;&#21270;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#23545;&#25968;&#25454;/&#27169;&#22411;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#23545;&#22522;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#36827;&#34892;&#30340;&#27010;&#24565;&#35777;&#26126;&#35780;&#20272;&#26174;&#31034;&#20102;SABRE&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#20248;&#20110;&#29616;&#26377;&#26694;&#26550;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SABRE, a novel framework for robust variational Bayesian peer-to-peer federated learning. We analyze the robustness of the known variational Bayesian peer-to-peer federated learning framework (BayP2PFL) against poisoning attacks and subsequently show that BayP2PFL is not robust against those attacks. The new SABRE aggregation methodology is then devised to overcome the limitations of the existing frameworks. SABRE works well in non-IID settings, does not require the majority of the benign nodes over the compromised ones, and even outperforms the baseline algorithm in benign settings. We theoretically prove the robustness of our algorithm against data / model poisoning attacks in a decentralized linear regression setting. Proof-of-Concept evaluations on benchmark data from image classification demonstrate the superiority of SABRE over the existing frameworks under various poisoning attacks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;$\lambda$-AC&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#35774;&#35745;&#36873;&#25321;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.17366</link><description>&lt;p&gt;
$\lambda$-AC&#65306;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
$\lambda$-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces. (arXiv:2306.17366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;$\lambda$-AC&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#35774;&#35745;&#36873;&#25321;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#22312;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21363;&#27169;&#22411;&#22312;&#20915;&#31574;&#21046;&#23450;&#26102;&#24212;&#35813;&#26159;&#20934;&#30830;&#30340;&#12290;&#23613;&#31649;&#24050;&#32463;&#24314;&#31435;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#20294;&#26159;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#21033;&#29992;&#20915;&#31574;&#24863;&#30693;&#25439;&#22833;&#30340;&#31639;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#33021;&#22815;&#23454;&#29616;&#33391;&#22909;&#31639;&#27861;&#24615;&#33021;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#31639;&#27861;&#24605;&#24819;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#22312;MuZero&#31995;&#21015;&#24037;&#20316;&#20013;&#25152;&#24314;&#31435;&#30340;&#32463;&#39564;&#24615;&#35774;&#35745;&#20915;&#31574;&#23545;&#20110;&#30456;&#20851;&#31639;&#27861;&#30340;&#33391;&#22909;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#65292;&#19981;&#21516;&#30340;&#20215;&#20540;&#24863;&#30693;&#31639;&#27861;&#23454;&#20363;&#20043;&#38388;&#34892;&#20026;&#24046;&#24322;&#12290;&#22312;&#36825;&#20123;&#35265;&#35299;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#27169;&#22411;&#39537;&#21160;&#20915;&#31574;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;$\lambda$-AC&#12290;
&lt;/p&gt;
&lt;p&gt;
The idea of decision-aware model learning, that models should be accurate where it matters for decision-making, has gained prominence in model-based reinforcement learning. While promising theoretical results have been established, the empirical performance of algorithms leveraging a decision-aware loss has been lacking, especially in continuous control problems. In this paper, we present a study on the necessary components for decision-aware reinforcement learning models and we showcase design choices that enable well-performing algorithms. To this end, we provide a theoretical and empirical investigation into prominent algorithmic ideas in the field. We highlight that empirical design decisions established in the MuZero line of works are vital to achieving good performance for related algorithms, and we showcase differences in behavior between different instantiations of value-aware algorithms in stochastic environments. Using these insights, we propose the Latent Model-Based Decisio
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#65292;&#36890;&#36807;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10125</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#20998;&#31867;&#12289;&#36827;&#23637;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects. (arXiv:2306.10125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10125
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#65292;&#36890;&#36807;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;SSL&#26368;&#31361;&#20986;&#30340;&#20248;&#21183;&#26159;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#22522;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#19982;&#35768;&#22810;&#20851;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#32508;&#36848;&#30456;&#27604;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;SSL&#30340;&#32508;&#36848;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#24403;&#21069;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#22238;&#39038;&#20102;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#26102;&#38388;&#24207;&#21015;&#30456;&#20851;&#30340;&#29616;&#26377;&#32508;&#36848;&#65292;&#28982;&#21518;&#36890;&#36807;&#24635;&#32467;&#20174;&#29983;&#25104;&#22411;&#12289;&#23545;&#27604;&#22411;&#21644;&#23545;&#25239;&#22411;&#19977;&#20010;&#35282;&#24230;&#23545;&#29616;&#26377;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#26032;&#30340;&#20998;&#31867;&#12290;&#36825;&#20123;&#26041;&#27861;&#36827;&#19968;&#27493;&#32454;&#20998;&#20026;&#21313;&#20010;&#23376;&#31867;&#65292;&#35814;&#32454;&#22238;&#39038;&#21644;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20851;&#38190;&#30452;&#35273;&#12289;&#20027;&#35201;&#26694;&#26550;&#12289;&#20248;&#21183;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages an
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38544;&#31169;&#21270;&#25968;&#25454;&#19978;&#35757;&#32451;GAN&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#29109;&#27491;&#21017;&#21270;Wasserstein&#36317;&#31163;&#21435;&#22122;&#21487;&#20197;&#32531;&#35299;&#27491;&#21017;&#21270;&#20559;&#24046;&#21644;&#38544;&#31169;&#21270;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#27169;&#22411;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09547</link><description>&lt;p&gt;
&#20174;&#38544;&#31169;&#21270;&#25968;&#25454;&#20013;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training generative models from privatized data. (arXiv:2306.09547v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09547
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38544;&#31169;&#21270;&#25968;&#25454;&#19978;&#35757;&#32451;GAN&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#29109;&#27491;&#21017;&#21270;Wasserstein&#36317;&#31163;&#21435;&#22122;&#21487;&#20197;&#32531;&#35299;&#27491;&#21017;&#21270;&#20559;&#24046;&#21644;&#38544;&#31169;&#21270;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#27169;&#22411;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24046;&#20998;&#38544;&#31169;&#21270;&#25968;&#25454;&#19978;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#24120;&#35265;&#30340;&#21152;&#24615;&#22122;&#22768;&#26426;&#21046;&#65288;&#22914;&#25289;&#26222;&#25289;&#26031;&#22122;&#22768;&#21644;&#39640;&#26031;&#22122;&#22768;&#65289;&#23545;&#25968;&#25454;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20351;&#29992;&#29109;&#27491;&#21017;&#21270;Wasserstein&#36317;&#31163;&#26469;&#21435;&#22122;&#25968;&#25454;&#20998;&#24067;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21807;&#19968;&#22320;&#32531;&#35299;&#27491;&#21017;&#21270;&#20559;&#24046;&#21644;&#38544;&#31169;&#21270;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24182;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#32467;&#26524;&#21644;&#23454;&#39564;&#35777;&#25454;&#20197;&#25903;&#25345;&#20854;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local differential privacy (LDP) is a powerful method for privacy-preserving data collection. In this paper, we develop a framework for training Generative Adversarial Networks (GAN) on differentially privatized data. We show that entropic regularization of the Wasserstein distance -- a popular regularization method in the literature that has been often leveraged for its computational benefits -- can be used to denoise the data distribution when data is privatized by common additive noise mechanisms, such as Laplace and Gaussian. This combination uniquely enables the mitigation of both the regularization bias and the effects of privatization noise, thereby enhancing the overall efficacy of the model. We analyse the proposed method, provide sample complexity results and experimental evidence to support its efficacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;CLIP&#27169;&#22411;&#65288;Dp-CLIP&#65289;&#65292;&#26088;&#22312;&#20445;&#25252;&#22810;&#27169;&#24577;AI&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#34920;&#26126;&#20854;&#19982;&#26631;&#20934;&#38750;&#31169;&#26377;CLIP&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#21516;&#31561;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08173</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#20013;&#20445;&#25252;&#25968;&#25454;&#65306;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#29992;&#20110;CLIP&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Safeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training. (arXiv:2306.08173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;CLIP&#27169;&#22411;&#65288;Dp-CLIP&#65289;&#65292;&#26088;&#22312;&#20445;&#25252;&#22810;&#27169;&#24577;AI&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#34920;&#26126;&#20854;&#19982;&#26631;&#20934;&#38750;&#31169;&#26377;CLIP&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#21516;&#31561;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#30340;&#25104;&#21151;&#24341;&#21457;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#25968;&#25454;&#38544;&#31169;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;CLIP&#36890;&#36807;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#32852;&#21512;&#35757;&#32451;&#24443;&#24213;&#25913;&#21464;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#20294;&#20854;&#21487;&#33021;&#26080;&#24847;&#20013;&#25259;&#38706;&#25935;&#24863;&#20449;&#24687;&#30340;&#28508;&#21147;&#38656;&#35201;&#38598;&#25104;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;&#25913;&#36827;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#38544;&#31169;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;Dp-CLIP&#22312;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#22810;&#26679;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20445;&#25345;&#20102;&#19982;&#26631;&#20934;&#30340;&#38750;&#31169;&#26377;CLIP&#27169;&#22411;&#21516;&#31561;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#32447;&#24615;&#34920;&#31034;&#35774;&#32622;&#19979;&#20998;&#26512;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26799;&#24230;&#34987;&#21098;&#36753;&#26102;&#23454;&#29992;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The surge in multimodal AI's success has sparked concerns over data privacy in vision-and-language tasks. While CLIP has revolutionized multimodal learning through joint training on images and text, its potential to unintentionally disclose sensitive information necessitates the integration of privacy-preserving mechanisms. We introduce a differentially private adaptation of the Contrastive Language-Image Pretraining (CLIP) model that effectively addresses privacy concerns while retaining accuracy. Our proposed method, Dp-CLIP, is rigorously evaluated on benchmark datasets encompassing diverse vision-and-language tasks such as image classification and visual question answering. We demonstrate that our approach retains performance on par with the standard non-private CLIP model. Furthermore, we analyze our proposed algorithm under linear representation settings. We derive the convergence rate of our algorithm and show a trade-off between utility and privacy when gradients are clipped pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08158</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#38750;&#39044;&#26399;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#35770;&#25991;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20559;&#35265;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#21361;&#23475;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#20511;&#37492;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20559;&#35265;&#31867;&#22411;&#12289;&#37327;&#21270;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#23545;&#20110;&#37327;&#21270;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#20559;&#35265;&#24230;&#37327;&#24182;&#19981;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20559;&#35265;&#65292;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#26159;&#34920;&#38754;&#30340;&#65292;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#23436;&#25104;&#20840;&#23616;&#20989;&#25968;&#36924;&#36817;&#12290;&#36825;&#19968;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#65292;&#36824;&#21487;&#29992;&#20110;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#30340;&#36924;&#36817;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#36924;&#36817;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#12290;</title><link>http://arxiv.org/abs/2306.03303</link><description>&lt;p&gt;
&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#21151;&#33021;&#24615;&#36755;&#20837;&#26144;&#23556;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Global universal approximation of functional input maps on weighted spaces. (arXiv:2306.03303v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#23436;&#25104;&#20840;&#23616;&#20989;&#25968;&#36924;&#36817;&#12290;&#36825;&#19968;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#65292;&#36824;&#21487;&#29992;&#20110;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#30340;&#36924;&#36817;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#36924;&#36817;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#23450;&#20041;&#22312;&#21487;&#33021;&#26159;&#26080;&#38480;&#32500;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#65292;&#20854;&#20540;&#20063;&#22312;&#21487;&#33021;&#26159;&#26080;&#38480;&#32500;&#30340;&#36755;&#20986;&#31354;&#38388;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21152;&#24615;&#26063;&#20316;&#20026;&#38544;&#34255;&#23618;&#26144;&#23556;&#65292;&#20197;&#21450;&#19968;&#20010;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#24212;&#29992;&#20110;&#27599;&#20010;&#38544;&#34255;&#23618;&#12290;&#20381;&#38752;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#30340;Stone-Weierstrass&#23450;&#29702;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;&#32467;&#26524;&#65292;&#36229;&#36234;&#20102;&#24120;&#35268;&#32039;&#38598;&#36924;&#36817;&#12290;&#36825;&#29305;&#21035;&#36866;&#29992;&#20110;&#36890;&#36807;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#65288;&#38750;&#20808;&#35265;&#20043;&#26126;&#30340;&#65289;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#12290;&#20316;&#20026;&#24102;&#26435;Stone-Weierstrass&#23450;&#29702;&#30340;&#36827;&#19968;&#27493;&#24212;&#29992;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#24341;&#20837;&#20102;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#31614;&#21517;&#20869;&#26680;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26159;&#26576;&#20123;&#39640;&#26031;&#36807;&#31243;&#30340;Cameron-Martin&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce so-called functional input neural networks defined on a possibly infinite dimensional weighted space with values also in a possibly infinite dimensional output space. To this end, we use an additive family as hidden layer maps and a non-linear activation function applied to each hidden layer. Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global universal approximation result for generalizations of continuous functions going beyond the usual approximation on compact sets. This then applies in particular to approximation of (non-anticipative) path space functionals via functional input neural networks. As a further application of the weighted Stone-Weierstrass theorem we prove a global universal approximation result for linear functions of the signature. We also introduce the viewpoint of Gaussian process regression in this setting and show that the reproducing kernel Hilbert space of the signature kernels are Cameron-Martin spaces of certain Gauss
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#21040;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#22312;&#27867;&#21270;&#32852;&#37030;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.01334</link><description>&lt;p&gt;
&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Federated Domain Generalization: A Survey. (arXiv:2306.01334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#21040;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#22312;&#27867;&#21270;&#32852;&#37030;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#20381;&#36182;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#20998;&#24067;&#26159;&#30456;&#21516;&#30340;&#65292;&#25968;&#25454;&#26159;&#38598;&#20013;&#23384;&#20648;&#20379;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#20998;&#24067;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#25968;&#25454;&#36890;&#24120;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#35774;&#22791;&#12289;&#32452;&#32455;&#25110;&#36793;&#32536;&#33410;&#28857;&#19978;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#65292;&#24182;&#19988;&#25968;&#25454;&#20998;&#24067;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#23545;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270; (FDG) &#30340;&#26497;&#22823;&#20852;&#36259;&#12290;FDG &#32467;&#21512;&#20102;&#32852;&#37030;&#23398;&#20064; (FL) &#21644;&#39046;&#22495;&#27867;&#21270; (DG) &#25216;&#26415;&#30340;&#20248;&#28857;&#65292;&#20351;&#22810;&#20010;&#28304;&#39046;&#22495;&#33021;&#22815;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#30452;&#25509;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#32780;&#21448;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#39046;&#22495;&#36716;&#31227;&#19979;&#27867;&#21270;&#32852;&#37030;&#27169;&#22411;&#26159;&#19968;&#20010;&#25216;&#26415;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning typically relies on the assumption that training and testing distributions are identical and that data is centrally stored for training and testing. However, in real-world scenarios, distributions may differ significantly and data is often distributed across different devices, organizations, or edge nodes. Consequently, it is imperative to develop models that can effectively generalize to unseen distributions where data is distributed across different domains. In response to this challenge, there has been a surge of interest in federated domain generalization (FDG) in recent years. FDG combines the strengths of federated learning (FL) and domain generalization (DG) techniques to enable multiple source domains to collaboratively learn a model capable of directly generalizing to unseen domains while preserving data privacy. However, generalizing the federated model under domain shifts is a technically challenging problem that has received scant attention in the research 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20108;&#20998;&#20998;&#31867;&#20013;&#25552;&#20379;&#36861;&#32034;&#26435;&#20250;&#22686;&#21152;&#38169;&#35823;&#29575;&#65292;&#23548;&#33268;&#26356;&#22810;&#38169;&#35823;&#30340;&#21457;&#29983;&#12290;&#25552;&#20379;&#31639;&#27861;&#36861;&#32034;&#26435;&#21487;&#33021;&#20063;&#20250;&#22312;&#31995;&#32479;&#32423;&#21035;&#19978;&#32473;&#20104;&#19981;&#21033;&#12290;</title><link>http://arxiv.org/abs/2306.00497</link><description>&lt;p&gt;
&#20108;&#20998;&#20998;&#31867;&#20013;&#36861;&#32034;&#26435;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
The Risks of Recourse in Binary Classification. (arXiv:2306.00497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00497
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20108;&#20998;&#20998;&#31867;&#20013;&#25552;&#20379;&#36861;&#32034;&#26435;&#20250;&#22686;&#21152;&#38169;&#35823;&#29575;&#65292;&#23548;&#33268;&#26356;&#22810;&#38169;&#35823;&#30340;&#21457;&#29983;&#12290;&#25552;&#20379;&#31639;&#27861;&#36861;&#32034;&#26435;&#21487;&#33021;&#20063;&#20250;&#22312;&#31995;&#32479;&#32423;&#21035;&#19978;&#32473;&#20104;&#19981;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#36861;&#32034;&#26435;&#25552;&#20379;&#35299;&#37322;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25512;&#32763;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#19981;&#21033;&#20915;&#31574;&#12290;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#24456;&#23569;&#26377;&#20154;&#20851;&#27880;&#25552;&#20379;&#36861;&#32034;&#26435;&#26159;&#21542;&#26377;&#30410;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25277;&#35937;&#30340;&#23398;&#20064;&#29702;&#35770;&#26694;&#26550;&#65292;&#27604;&#36739;&#20102;&#20855;&#26377;&#21644;&#27809;&#26377;&#31639;&#27861;&#36861;&#32034;&#26435;&#30340;&#20998;&#31867;&#30340;&#39118;&#38505;&#65288;&#21363;&#26399;&#26395;&#25439;&#22833;&#65289;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22238;&#31572;&#22312;&#25972;&#20010;&#20154;&#32676;&#27700;&#24179;&#19978;&#25552;&#20379;&#36861;&#32034;&#26435;&#20309;&#26102;&#26377;&#30410;&#25110;&#26377;&#23475;&#30340;&#38382;&#39064;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35768;&#22810;&#21487;&#20449;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#36861;&#32034;&#26435;&#21453;&#32780;&#20250;&#26377;&#23475;&#65292;&#22240;&#20026;&#23427;&#23558;&#29992;&#25143;&#25512;&#21521;&#26356;&#39640;&#31867;&#21035;&#19981;&#30830;&#23450;&#24615;&#30340;&#21306;&#22495;&#65292;&#22240;&#27492;&#20250;&#23548;&#33268;&#26356;&#22810;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#37096;&#32626;&#20998;&#31867;&#22120;&#30340;&#19968;&#26041;&#26159;&#21542;&#26377;&#21160;&#26426;&#38024;&#23545;&#25552;&#20379;&#36861;&#32034;&#26435;&#30340;&#24773;&#20917;&#36827;&#34892;&#31574;&#30053;&#35268;&#21010;&#65292;&#25105;&#20204;&#21457;&#29616;&#26377;&#26102;&#20505;&#30830;&#23454;&#23384;&#22312;&#36825;&#31181;&#29616;&#35937;&#65292;&#36825;&#23545;&#20182;&#20204;&#30340;&#29992;&#25143;&#19981;&#21033;&#12290;&#22240;&#27492;&#65292;&#25552;&#20379;&#31639;&#27861;&#36861;&#32034;&#26435;&#22312;&#31995;&#32479;&#32423;&#21035;&#19978;&#21487;&#33021;&#20063;&#26159;&#26377;&#23475;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic recourse provides explanations that help users overturn an unfavorable decision by a machine learning system. But so far very little attention has been paid to whether providing recourse is beneficial or not. We introduce an abstract learning-theoretic framework that compares the risks (i.e. expected losses) for classification with and without algorithmic recourse. This allows us to answer the question of when providing recourse is beneficial or harmful at the population level. Surprisingly, we find that there are many plausible scenarios in which providing recourse turns out to be harmful, because it pushes users to regions of higher class uncertainty and therefore leads to more mistakes. We further study whether the party deploying the classifier has an incentive to strategize in anticipation of having to provide recourse, and we find that sometimes they do, to the detriment of their users. Providing algorithmic recourse may therefore also be harmful at the systemic level
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#24179;&#28369;&#22270;&#20449;&#21495;&#20998;&#24067;&#31354;&#38388;&#20013;&#23884;&#20837;&#22270;&#26469;&#23450;&#20041;&#22270;&#30340;&#24179;&#22343;&#20540;&#65292;&#20854;&#20013;&#21487;&#20197;&#20351;&#29992;Wasserstein&#24230;&#37327;&#34913;&#37327;&#22270;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.19738</link><description>&lt;p&gt;
&#22270;&#30340;Bures-Wasserstein&#24179;&#22343;&#20540;
&lt;/p&gt;
&lt;p&gt;
Bures-Wasserstein Means of Graphs. (arXiv:2305.19738v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19738
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#24179;&#28369;&#22270;&#20449;&#21495;&#20998;&#24067;&#31354;&#38388;&#20013;&#23884;&#20837;&#22270;&#26469;&#23450;&#20041;&#22270;&#30340;&#24179;&#22343;&#20540;&#65292;&#20854;&#20013;&#21487;&#20197;&#20351;&#29992;Wasserstein&#24230;&#37327;&#34913;&#37327;&#22270;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#65292;&#25214;&#21040;&#37319;&#26679;&#25968;&#25454;&#30340;&#24179;&#22343;&#20540;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#26679;&#26412;&#20026;&#22270;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#65292;&#23450;&#20041;&#24179;&#22343;&#20540;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#24179;&#28369;&#22270;&#20449;&#21495;&#20998;&#24067;&#31354;&#38388;&#20013;&#23884;&#20837;&#22270;&#26469;&#23450;&#20041;&#22270;&#30340;&#24179;&#22343;&#20540;&#65292;&#20854;&#20013;&#21487;&#20197;&#20351;&#29992;Wasserstein&#24230;&#37327;&#34913;&#37327;&#22270;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#22312;&#27492;&#23884;&#20837;&#31354;&#38388;&#20013;&#25214;&#21040;&#24179;&#22343;&#20540;&#65292;&#25105;&#20204;&#21487;&#20197;&#24674;&#22797;&#20445;&#30041;&#32467;&#26500;&#20449;&#24687;&#30340;&#24179;&#22343;&#22270;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#26032;&#30340;&#22270;&#24179;&#22343;&#20540;&#30340;&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#26469;&#35745;&#31639;&#23427;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#32467;&#26500;&#21270;&#22270;&#30340;k-means&#32858;&#31867;&#12289;&#21151;&#33021;&#24615;&#33041;&#32593;&#32476;&#30340;&#20998;&#31867;&#20197;&#21450;&#22810;&#23618;&#22270;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;p&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the mean of sampled data is a fundamental task in machine learning and statistics. However, in cases where the data samples are graph objects, defining a mean is an inherently difficult task. We propose a novel framework for defining a graph mean via embeddings in the space of smooth graph signal distributions, where graph similarity can be measured using the Wasserstein metric. By finding a mean in this embedding space, we can recover a mean graph that preserves structural information. We establish the existence and uniqueness of the novel graph mean, and provide an iterative algorithm for computing it. To highlight the potential of our framework as a valuable tool for practical applications in machine learning, it is evaluated on various tasks, including k-means clustering of structured graphs, classification of functional brain networks, and semi-supervised node classification in multi-layer graphs. Our experimental results demonstrate that our approach achieves consistent p
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#22312; SGD &#19979;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21333;&#25351;&#25968;&#30446;&#26631;&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#21457;&#29616;&#36807;&#21442;&#25968;&#21270;&#21482;&#20250;&#22686;&#21152;&#19968;&#23450;&#22240;&#23376;&#30340;&#25910;&#25947;&#24615;&#65292;&#19981;&#21516;&#32500;&#24230;&#21644;&#23485;&#24230;&#30340;&#21069;&#32622;&#22240;&#23376;&#31934;&#30830;&#32467;&#26524;&#25581;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.18502</link><description>&lt;p&gt;
&#36867;&#31163;&#24179;&#24248;&#65306;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22312; SGD &#19979;&#23398;&#20064;&#22256;&#38590;&#30340;&#21333;&#25351;&#26631;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Escaping mediocrity: how two-layer networks learn hard single-index models with SGD. (arXiv:2305.18502v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18502
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#22312; SGD &#19979;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21333;&#25351;&#25968;&#30446;&#26631;&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#21457;&#29616;&#36807;&#21442;&#25968;&#21270;&#21482;&#20250;&#22686;&#21152;&#19968;&#23450;&#22240;&#23376;&#30340;&#25910;&#25947;&#24615;&#65292;&#19981;&#21516;&#32500;&#24230;&#21644;&#23485;&#24230;&#30340;&#21069;&#32622;&#22240;&#23376;&#31934;&#30830;&#32467;&#26524;&#25581;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#19979;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21333;&#25351;&#25968;&#30446;&#26631;&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#21021;&#22987;&#21270;&#26102;&#23384;&#22312;&#35768;&#22810;&#24179;&#22374;&#26041;&#21521;&#30340;&#25361;&#25112;&#24615;&#24773;&#20917;&#12290;&#24050;&#32463;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#36890;&#24120;&#38656;&#35201; $n=O(d\log{d})$ &#20010;&#26679;&#26412;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#39640;&#32500;&#24230;&#21644;&#19981;&#21516;&#23485;&#24230;&#24773;&#20917;&#19979;&#30340;&#21069;&#32622;&#22240;&#23376;&#30340;&#31934;&#30830;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#31867;&#20013;&#65292;&#36807;&#21442;&#25968;&#21270;&#21482;&#20250;&#22686;&#21152;&#19968;&#23450;&#22240;&#23376;&#30340;&#25910;&#25947;&#24615;&#12290;&#36825;&#20123;&#35265;&#35299;&#22522;&#20110; SGD &#21160;&#24577;&#30340;&#20302;&#32500;&#24230;&#38543;&#26426;&#36807;&#31243;&#27169;&#22411;&#65292;&#20854;&#20013;&#36867;&#31163;&#24179;&#24248;&#31561;&#21516;&#20110;&#35745;&#31639;&#20986;&#31449;&#20986;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#36807;&#31243;&#30340;&#30830;&#23450;&#24615;&#36817;&#20284;&#36275;&#20197;&#20195;&#34920;&#36867;&#36920;&#26102;&#38388;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#38543;&#26426;&#24615;&#30340;&#20316;&#29992;&#21487;&#33021;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the sample complexity for two-layer neural networks to learn a single-index target function under Stochastic Gradient Descent (SGD), focusing on the challenging regime where many flat directions are present at initialization. It is well-established that in this scenario $n=O(d\log{d})$ samples are typically needed. However, we provide precise results concerning the pre-factors in high-dimensional contexts and for varying widths. Notably, our findings suggest that overparameterization can only enhance convergence by a constant factor within this problem class. These insights are grounded in the reduction of SGD dynamics to a stochastic process in lower dimensions, where escaping mediocrity equates to calculating an exit time. Yet, we demonstrate that a deterministic approximation of this process adequately represents the escape time, implying that the role of stochasticity may be minimal in this scenario.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#19968;&#31181;&#22522;&#20110;&#28857;&#31215;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#24179;&#22343;&#28857;&#31215;&#21512;&#24182;&#32858;&#31867;&#65292;&#24182;&#19988;&#36755;&#20986;&#30340;&#26641;&#32467;&#26500;&#21487;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#25968;&#25454;&#30340;&#29983;&#25104;&#23618;&#27425;&#32467;&#26500;&#65292;&#26641;&#24418;&#24674;&#22797;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15022</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#31215;&#30340;&#23618;&#27425;&#32858;&#31867;&#21487;&#20197;&#24674;&#22797;&#38544;&#34255;&#30340;&#26641;&#24418;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Hierarchical clustering with dot products recovers hidden tree structure. (arXiv:2305.15022v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#19968;&#31181;&#22522;&#20110;&#28857;&#31215;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#24179;&#22343;&#28857;&#31215;&#21512;&#24182;&#32858;&#31867;&#65292;&#24182;&#19988;&#36755;&#20986;&#30340;&#26641;&#32467;&#26500;&#21487;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#25968;&#25454;&#30340;&#29983;&#25104;&#23618;&#27425;&#32467;&#26500;&#65292;&#26641;&#24418;&#24674;&#22797;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#20110;&#24050;&#26377;&#20957;&#32858;&#32858;&#31867;&#31639;&#27861;&#30340;&#26032;&#35270;&#35282;&#65292;&#19987;&#27880;&#20110;&#23618;&#27425;&#32467;&#26500;&#30340;&#24674;&#22797;&#12290;&#25105;&#20204;&#24314;&#35758;&#19968;&#31181;&#31616;&#21333;&#30340;&#26631;&#20934;&#31639;&#27861;&#21464;&#20307;&#65292;&#20854;&#20013;&#32858;&#31867;&#26159;&#36890;&#36807;&#26368;&#22823;&#24179;&#22343;&#28857;&#31215;&#32780;&#19981;&#26159;&#26368;&#23567;&#36317;&#31163;&#25110;&#31751;&#20869;&#26041;&#24046;&#26469;&#21512;&#24182;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27492;&#31639;&#27861;&#36755;&#20986;&#30340;&#26641;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#23618;&#27425;&#32467;&#26500;&#30340;&#21487;&#38752;&#20272;&#35745;&#12290;&#20851;&#38190;&#25216;&#26415;&#21019;&#26032;&#22312;&#20110;&#29702;&#35299;&#27169;&#22411;&#20013;&#30340;&#23618;&#27425;&#20449;&#24687;&#22914;&#20309;&#36716;&#21270;&#20026;&#21487;&#20174;&#25968;&#25454;&#20013;&#24674;&#22797;&#30340;&#26641;&#24418;&#20960;&#20309;&#20449;&#24687;&#65292;&#24182;&#21516;&#26102;&#22686;&#38271;&#26679;&#26412;&#22823;&#23567;&#21644;&#25968;&#25454;&#32500;&#25968;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;UPGMA&#12289;Ward's&#26041;&#27861;&#21644;HDBSCAN&#65289;&#30340;&#26641;&#24418;&#24674;&#22797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we offer a new perspective on the well established agglomerative clustering algorithm, focusing on recovery of hierarchical structure. We recommend a simple variant of the standard algorithm, in which clusters are merged by maximum average dot product and not, for example, by minimum distance or within-cluster variance. We demonstrate that the tree output by this algorithm provides a bona fide estimate of generative hierarchical structure in data, under a generic probabilistic graphical model. The key technical innovations are to understand how hierarchical information in this model translates into tree geometry which can be recovered from data, and to characterise the benefits of simultaneously growing sample size and data dimension. We demonstrate superior tree recovery performance with real data over existing approaches such as UPGMA, Ward's method, and HDBSCAN.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#26159;&#36890;&#36807;&#32771;&#34385;&#36793;&#32536;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#20026;&#33258;&#30001;&#33021;&#27867;&#20989;&#26368;&#23567;&#21270;&#24471;&#21040;&#30340;&#65292;&#21478;&#19968;&#31181;&#26159;&#29992;&#20110;&#20248;&#21270;&#35813;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#23436;&#20840;&#26080;&#38656;&#35843;&#21442;&#12290;</title><link>http://arxiv.org/abs/2305.14916</link><description>&lt;p&gt;
CoinEM&#65306;&#26080;&#38656;&#35843;&#21442;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CoinEM: Tuning-Free Particle-Based Variational Inference for Latent Variable Models. (arXiv:2305.14916v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#26159;&#36890;&#36807;&#32771;&#34385;&#36793;&#32536;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#20026;&#33258;&#30001;&#33021;&#27867;&#20989;&#26368;&#23567;&#21270;&#24471;&#21040;&#30340;&#65292;&#21478;&#19968;&#31181;&#26159;&#29992;&#20110;&#20248;&#21270;&#35813;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#23436;&#20840;&#26080;&#38656;&#35843;&#21442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#22522;&#20110;&#31890;&#23376;&#30340;&#26032;&#22411;&#31639;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#36793;&#38469;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23398;&#20064;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#31181;&#23436;&#20840;&#26080;&#38656;&#35843;&#21442;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#23558;&#36793;&#38469;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#35270;&#20026;&#20248;&#21270;&#38382;&#39064;&#30340;&#35282;&#24230;&#65306;&#21363;&#23558;&#20854;&#35270;&#20026;&#33258;&#30001;&#33021;&#27867;&#20989;&#30340;&#26368;&#23567;&#21270;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#32771;&#34385;&#33258;&#30001;&#33021;&#20851;&#32852;&#30340;&#26799;&#24230;&#27969;&#30340;&#31163;&#25955;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340; Stein &#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20026;&#27492;&#31639;&#27861;&#24314;&#31435;&#20102;&#19979;&#38477;&#24341;&#29702;&#65292;&#20445;&#35777;&#20102;&#33258;&#30001;&#33021;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#19979;&#38477;&#12290;&#20294;&#27492;&#26041;&#27861;&#21644;&#20854;&#20182;&#30001;&#26799;&#24230;&#27969;&#30340;&#31163;&#25955;&#21270;&#24471;&#21040;&#30340;&#26041;&#27861;&#37117;&#24517;&#39035;&#20381;&#36182;&#20110;&#23398;&#20064;&#29575;&#65292;&#35813;&#23398;&#20064;&#29575;&#24517;&#39035;&#30001;&#20174;&#19994;&#32773;&#20180;&#32454;&#35843;&#25972;&#65292;&#20197;&#30830;&#20445;&#20197;&#21512;&#36866;&#30340;&#36895;&#29575;&#25910;&#25947;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#31639;&#27861;&#29992;&#20110;&#20248;&#21270;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#26159;&#23436;&#20840;&#26080;&#38656;&#35843;&#21442;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce two new particle-based algorithms for learning latent variable models via marginal maximum likelihood estimation, including one which is entirely tuning-free. Our methods are based on the perspective of marginal maximum likelihood estimation as an optimization problem: namely, as the minimization of a free energy functional. One way to solve this problem is to consider the discretization of a gradient flow associated with the free energy. We study one such approach, which resembles an extension of the popular Stein variational gradient descent algorithm. In particular, we establish a descent lemma for this algorithm, which guarantees that the free energy decreases at each iteration. This method, and any other obtained as the discretization of the gradient flow, will necessarily depend on a learning rate which must be carefully tuned by the practitioner in order to ensure convergence at a suitable rate. With this in mind, we also propose another algorithm for optimizing the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;CLIP&#21069;&#32512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#22270;&#20687;&#21644;&#25512;&#25991;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#29983;&#25104;&#20851;&#20110;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26367;&#20195;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.14779</link><description>&lt;p&gt;
Twitter&#22270;&#20687;&#30340;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text Conditional Alt-Text Generation for Twitter Images. (arXiv:2305.14779v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;CLIP&#21069;&#32512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#22270;&#20687;&#21644;&#25512;&#25991;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#29983;&#25104;&#20851;&#20110;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26367;&#20195;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31038;&#20132;&#23186;&#20307;&#29305;&#21035;&#26159;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#29983;&#25104;&#26367;&#20195;&#25991;&#26412;&#65288;&#25110;alt-text&#65289;&#25551;&#36848;&#30340;&#26041;&#27861;&#12290;&#19982;&#22270;&#20687;&#30340;&#23383;&#24149;&#19981;&#21516;&#65292;&#25991;&#26412;&#26367;&#25442;&#25991;&#26412;&#26356;&#21152;&#30452;&#30333;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#29305;&#23450;&#12290;&#27492;&#22806;&#65292;&#20851;&#38190;&#26159;&#65292;&#21457;&#24067;&#21040;Twitter&#19978;&#30340;&#22270;&#20687;&#36890;&#24120;&#26159;&#30001;&#29992;&#25143;&#32534;&#20889;&#30340;&#25991;&#26412;&#38468;&#21152;&#30340;&#65292;&#23613;&#31649;&#36825;&#20123;&#25991;&#26412;&#19981;&#19968;&#23450;&#25551;&#36848;&#22270;&#20687;&#65292;&#20294;&#21487;&#33021;&#25552;&#20379;&#26377;&#29992;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22914;&#26524;&#27491;&#30830;&#21033;&#29992;&#21487;&#20197;&#25552;&#20379;&#20449;&#24687;&#65292;&#20363;&#22914;&#25512;&#25991;&#21487;&#33021;&#20250;&#21629;&#21517;&#22270;&#29255;&#20013;&#27169;&#22411;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#30340;&#19981;&#24120;&#35265;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;CLIP&#21069;&#32512;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#25552;&#21462;&#22270;&#20687;&#30340;&#23884;&#20837;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#26144;&#23556;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36755;&#20986;&#21333;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#30701;&#24207;&#21015;&#65292;&#25110;&#31216;&#20026;&#8220;&#21069;&#32512;&#8221;&#65292;&#25105;&#20204;&#23558;&#25512;&#25991;&#26412;&#36523;&#30340;&#25991;&#26412;&#20063;&#36830;&#25509;&#21040;&#20854;&#20013;&#12290;&#36825;&#26679;&#65292;&#27169;&#22411;&#23601;&#21487;&#20197;&#22312;&#25991;&#31456;&#20013;&#26465;&#20214;&#21270;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#28982;&#21518;&#23558;&#21512;&#24182;&#30340;&#22810;&#27169;&#24335;&#21069;&#32512;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. This task is more than just a special case of image captioning, as alt-text is both more literally descriptive and context-specific. Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative -- e.g. the tweet may name an uncommon object in the image that the model has not previously seen. We address this with a CLIP prefix model that extracts an embedding of the image and passes it to a mapping network that outputs a short sequence in word embedding space, or a ``prefix'', to which we also concatenate the text from the tweet itself. This lets the model condition on both visual and textual information from the post. The combined multimodal prefix is then fed as a prompt to a pretrained lang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#20998;&#26512;&#20102;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;220&#20010;&#25919;&#27835;&#39046;&#34966;&#30340;YouTube&#35270;&#39057;&#65292;&#24635;&#32467;&#20102;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.09914</link><description>&lt;p&gt;
&#26623;&#23376;&#25919;&#27835;&#30340;&#38754;&#23380;&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27604;&#36739;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning. (arXiv:2304.09914v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#20998;&#26512;&#20102;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;220&#20010;&#25919;&#27835;&#39046;&#34966;&#30340;YouTube&#35270;&#39057;&#65292;&#24635;&#32467;&#20102;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23186;&#20307;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#25919;&#27835;&#20449;&#24687;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20256;&#25773;&#21644;&#28040;&#36153;&#26041;&#24335;&#65292;&#36825;&#31181;&#36716;&#21464;&#20419;&#20351;&#25919;&#27835;&#20154;&#29289;&#37319;&#21462;&#26032;&#30340;&#31574;&#30053;&#26469;&#25429;&#25417;&#21644;&#20445;&#25345;&#36873;&#27665;&#30340;&#27880;&#24847;&#21147;&#12290;&#36825;&#20123;&#31574;&#30053;&#24448;&#24448;&#20381;&#36182;&#20110;&#24773;&#24863;&#35828;&#26381;&#21644;&#21560;&#24341;&#12290;&#38543;&#30528;&#34394;&#25311;&#31354;&#38388;&#20013;&#35270;&#35273;&#20869;&#23481;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#24456;&#22810;&#25919;&#27835;&#27807;&#36890;&#20063;&#34987;&#26631;&#24535;&#30528;&#21796;&#36215;&#24773;&#24863;&#30340;&#35270;&#39057;&#20869;&#23481;&#21644;&#22270;&#20687;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;&#29616;&#26377;&#35757;&#32451;&#22909;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25552;&#20379;&#30340;Python&#24211;fer&#65292;&#24212;&#29992;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#65292;&#23545;&#25551;&#32472;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;&#25919;&#27835;&#39046;&#34966;&#30340;220&#20010;YouTube&#35270;&#39057;&#26679;&#26412;&#36827;&#34892;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#36820;&#22238;&#24773;&#32490;&#20998;&#25968;&#65292;&#27599;&#19968;&#24103;&#37117;&#20195;&#34920;6&#31181;&#24773;&#32490;&#29366;&#24577;&#65288;&#24868;&#24594;&#65292;&#21388;&#24694;&#65292;&#24656;&#24807;&#65292;&#24555;&#20048;&#65292;&#24754;&#20260;&#21644;&#24778;&#35766;&#65289;&#21644;&#19968;&#20010;&#20013;&#24615;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online media has revolutionized the way political information is disseminated and consumed on a global scale, and this shift has compelled political figures to adopt new strategies of capturing and retaining voter attention. These strategies often rely on emotional persuasion and appeal, and as visual content becomes increasingly prevalent in virtual space, much of political communication too has come to be marked by evocative video content and imagery. The present paper offers a novel approach to analyzing material of this kind. We apply a deep-learning-based computer-vision algorithm to a sample of 220 YouTube videos depicting political leaders from 15 different countries, which is based on an existing trained convolutional neural network architecture provided by the Python library fer. The algorithm returns emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;&#20027;&#21160;&#36873;&#25321;&#24615;&#39044;&#27979;&#65288;ASPEST&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#36716;&#31227;&#30446;&#26631;&#39046;&#22495;&#20013;&#23398;&#20064;&#26597;&#35810;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#24037;&#20316;&#30340;&#21516;&#26102;&#22686;&#21152;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.03870</link><description>&lt;p&gt;
ASPEST&#65306;&#20027;&#21160;&#23398;&#20064;&#21644;&#36873;&#25321;&#39044;&#27979;&#20043;&#38388;&#30340;&#24357;&#21512;
&lt;/p&gt;
&lt;p&gt;
ASPEST: Bridging the Gap Between Active Learning and Selective Prediction. (arXiv:2304.03870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;&#20027;&#21160;&#36873;&#25321;&#24615;&#39044;&#27979;&#65288;ASPEST&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#36716;&#31227;&#30446;&#26631;&#39046;&#22495;&#20013;&#23398;&#20064;&#26597;&#35810;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#24037;&#20316;&#30340;&#21516;&#26102;&#22686;&#21152;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#24615;&#39044;&#27979;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#21487;&#38752;&#30340;&#27169;&#22411;&#65292;&#24403;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#24456;&#39640;&#26102;&#65292;&#21487;&#20197;&#36991;&#20813;&#36827;&#34892;&#39044;&#27979;&#12290;&#38543;&#21518;&#65292;&#21487;&#20197;&#23558;&#36825;&#20123;&#39044;&#27979;&#25512;&#36831;&#32473;&#20154;&#31867;&#19987;&#23478;&#36827;&#34892;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#27979;&#35797;&#25968;&#25454;&#30340;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#12290;&#36825;&#23548;&#33268;&#26356;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#38656;&#35201;&#22686;&#21152;&#20154;&#24037;&#26631;&#27880;&#65292;&#36825;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#37117;&#26159;&#22256;&#38590;&#21644;&#26114;&#36149;&#30340;&#12290;&#20027;&#21160;&#23398;&#20064;&#36890;&#36807;&#20165;&#26597;&#35810;&#26368;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#31034;&#20363;&#26469;&#36991;&#20813;&#36825;&#31181;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#26696;&#20363;&#20013;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#38477;&#20302;&#24635;&#20307;&#30340;&#26631;&#27880;&#24037;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24357;&#21512;&#20102;&#36873;&#25321;&#24615;&#39044;&#27979;&#21644;&#20027;&#21160;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026;&#20027;&#21160;&#36873;&#25321;&#24615;&#39044;&#27979;&#65288;active selective prediction&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#22686;&#21152;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#30340;&#21516;&#26102;&#22312;&#36716;&#31227;&#30446;&#26631;&#39046;&#22495;&#20013;&#23398;&#20064;&#26597;&#35810;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#26679;&#26412;&#12290;&#23545;&#20110;&#36825;&#20010;&#26032;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;ASPEST&#65292;&#23427;&#35757;&#32451;&#27169;&#22411;&#24555;&#29031;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selective prediction aims to learn a reliable model that abstains from making predictions when the model uncertainty is high. These predictions can then be deferred to a human expert for further evaluation. In many real-world scenarios, however, the distribution of test data is different from the training data. This results in more inaccurate predictions, necessitating increased human labeling, which is difficult and expensive in many scenarios. Active learning circumvents this difficulty by only querying the most informative examples and, in several cases, has been shown to lower the overall labeling effort. In this work, we bridge the gap between selective prediction and active learning, proposing a new learning paradigm called active selective prediction which learns to query more informative samples from the shifted target domain while increasing accuracy and coverage. For this new problem, we propose a simple but effective solution, ASPEST, that trains ensembles of model snapshots
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#27169;&#22411;&#65292;&#21487;&#21516;&#26102;&#39044;&#27979;&#20116;&#31181;&#31227;&#26893;&#21518;&#39118;&#38505;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02780</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20844;&#27491;&#39044;&#27979;&#32925;&#31227;&#26893;&#21518;&#39118;&#38505;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
A Transformer-Based Deep Learning Approach for Fairly Predicting Post-Liver Transplant Risk Factors. (arXiv:2304.02780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#27169;&#22411;&#65292;&#21487;&#21516;&#26102;&#39044;&#27979;&#20116;&#31181;&#31227;&#26893;&#21518;&#39118;&#38505;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32925;&#31227;&#26893;&#26159;&#23545;&#20110;&#26202;&#26399;&#32925;&#30149;&#24739;&#32773;&#30340;&#19968;&#39033;&#25327;&#25937;&#24615;&#25163;&#26415;&#65292;&#32780;&#35813;&#25163;&#26415;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#20026;&#20379;&#20307;&#23547;&#25214;&#26368;&#20339;&#21305;&#37197;&#30340;&#21463;&#20307;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#20122;&#32676;&#20307;&#20043;&#38388;&#30830;&#20445;&#31227;&#26893;&#30340;&#20844;&#24179;&#24615;&#12290;&#20256;&#32479;&#30340;MELD&#35780;&#20998;&#31995;&#32479;&#21482;&#33021;&#35780;&#20272;&#22312;90&#22825;&#20869;&#26410;&#25509;&#21463;&#22120;&#23448;&#31227;&#26893;&#30340;&#24739;&#32773;&#30340;&#27515;&#20129;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#20379;&#21463;&#20307;&#21305;&#37197;&#20063;&#24212;&#35813;&#32771;&#34385;&#21040;&#31227;&#26893;&#21518;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#22914;&#24515;&#34880;&#31649;&#30142;&#30149;&#12289;&#24930;&#24615;&#25490;&#24322;&#31561;&#65292;&#36825;&#20123;&#37117;&#26159;&#31227;&#26893;&#21518;&#24120;&#35265;&#30340;&#24182;&#21457;&#30151;&#12290;&#20934;&#30830;&#39044;&#27979;&#36825;&#20123;&#39118;&#38505;&#20998;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20854;&#21046;&#23450;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;&#25552;&#20986;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#21516;&#26102;&#39044;&#27979;&#20116;&#31181;&#31227;&#26893;&#21518;&#39118;&#38505;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Liver transplantation is a life-saving procedure for patients with end-stage liver disease. There are two main challenges in liver transplant: finding the best matching patient for a donor and ensuring transplant equity among different subpopulations. The current MELD scoring system evaluates a patient's mortality risk if not receiving an organ within 90 days. However, the donor-patient matching should also take into consideration post-transplant risk factors, such as cardiovascular disease, chronic rejection, etc., which are all common complications after transplant. Accurate prediction of these risk scores remains a significant challenge. In this study, we will use predictive models to solve the above challenge. We propose a deep learning framework model to predict multiple risk factors after a liver transplant. By formulating it as a multi-task learning problem, the proposed deep neural network was trained on this data to simultaneously predict the five post-transplant risks and ach
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026; $\infty$-Diff &#30340;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#26080;&#38480;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#20351;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#28508;&#22312;&#21521;&#37327;&#21387;&#32553;&#25110;&#20381;&#36182;&#20110;&#31163;&#25955;&#30340;&#32452;&#20214;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#20445;&#30041;&#32454;&#33410;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#25193;&#23637;&#21040;&#27604;&#35757;&#32451;&#25968;&#25454;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.18242</link><description>&lt;p&gt;
&#26080;&#31351;&#20998;&#36776;&#29575;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
$\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States. (arXiv:2303.18242v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18242
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026; $\infty$-Diff &#30340;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#26080;&#38480;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#20351;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#28508;&#22312;&#21521;&#37327;&#21387;&#32553;&#25110;&#20381;&#36182;&#20110;&#31163;&#25955;&#30340;&#32452;&#20214;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#20445;&#30041;&#32454;&#33410;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#25193;&#23637;&#21040;&#27604;&#35757;&#32451;&#25968;&#25454;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;$\infty$-Diff&#30340;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#26080;&#38480;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#38543;&#26426;&#25277;&#26679;&#25968;&#25454;&#30340;&#23376;&#38598;&#24182;&#23398;&#20064;&#21435;&#22122;&#65292;&#25105;&#20204;&#21487;&#20197;&#23398;&#21040;&#19968;&#20010;&#36830;&#32493;&#30340;&#20989;&#25968;&#65292;&#20801;&#35768;&#20219;&#24847;&#20998;&#36776;&#29575;&#30340;&#37319;&#26679;&#12290;&#19982;&#20854;&#20182;&#26368;&#36817;&#30340;&#26080;&#38480;&#20998;&#36776;&#29575;&#29983;&#25104;&#24335;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#25805;&#20316;&#21407;&#22987;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#20351;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#28508;&#22312;&#21521;&#37327;&#21387;&#32553;&#25110;&#20381;&#36182;&#20110;&#31163;&#25955;&#30340;&#32452;&#20214;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#65292;&#20363;&#22914;&#38477;&#20302;FID&#20998;&#25968;&#65292;&#24182;&#33021;&#22815;&#22312;&#20445;&#30041;&#32454;&#33410;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#25193;&#23637;&#21040;&#27604;&#35757;&#32451;&#25968;&#25454;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce $\infty$-Diff, a generative diffusion model which directly operates on infinite resolution data. By randomly sampling subsets of coordinates during training and learning to denoise the content at those coordinates, a continuous function is learned that allows sampling at arbitrary resolutions. In contrast to other recent infinite resolution generative models, our approach operates directly on the raw data, not requiring latent vector compression for context, using hypernetworks, nor relying on discrete components. As such, our approach achieves significantly higher sample quality, as evidenced by lower FID scores, as well as being able to effectively scale to higher resolutions than the training data while retaining detail.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#35299;&#20915;&#20102;&#24102;&#38543;&#26426;&#21442;&#25968;&#30340;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.16548</link><description>&lt;p&gt;
&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#20013;&#24102;&#38543;&#26426;&#21442;&#25968;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Methods for Discrete Time Linear Quadratic Regulator With Random Parameters. (arXiv:2303.16548v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#35299;&#20915;&#20102;&#24102;&#38543;&#26426;&#21442;&#25968;&#30340;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#31163;&#25955;&#26102;&#38388;&#30340;&#32447;&#24615;&#31995;&#32479;&#21644;&#20108;&#27425;&#20934;&#21017;&#30340;&#26080;&#38480;&#26102;&#22495;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#21442;&#25968;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#20110;&#26102;&#38388;&#30340;&#38543;&#26426;&#21464;&#37327;&#12290;&#22312;&#36825;&#31181;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24212;&#29992;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#26469;&#23547;&#25214;&#26368;&#20248;&#25511;&#21046;&#65292;&#32780;&#19981;&#38656;&#35201;&#20102;&#35299;&#21442;&#25968;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29366;&#24577;&#36807;&#31243;&#30340;&#27425;&#39640;&#26031;&#24615;&#36136;&#65292;&#24182;&#26681;&#25454;&#27604;&#29616;&#26377;&#32467;&#26524;&#26356;&#24369;&#19988;&#26356;&#26131;&#39564;&#35777;&#30340;&#20551;&#35774;&#65292;&#24314;&#31435;&#20102;&#27492;&#26041;&#27861;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#35828;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies an infinite horizon optimal control problem for discrete-time linear system and quadratic criteria, both with random parameters which are independent and identically distributed with respect to time. In this general setting, we apply the policy gradient method, a reinforcement learning technique, to search for the optimal control without requiring knowledge of statistical information of the parameters. We investigate the sub-Gaussianity of the state process and establish global linear convergence guarantee for this approach based on assumptions that are weaker and easier to verify compared to existing results. Numerical experiments are presented to illustrate our result.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#30340;&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#24378;&#27491;&#21017;&#21270;&#26469;&#36991;&#20813;&#23849;&#28291;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16521</link><description>&lt;p&gt;
&#22312;&#19981;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#21152;&#24378;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#20013;&#30340;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Hard Regularization to Prevent Collapse in Online Deep Clustering without Data Augmentation. (arXiv:2303.16521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16521
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#30340;&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#24378;&#27491;&#21017;&#21270;&#26469;&#36991;&#20813;&#23849;&#28291;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#26159;&#25351;&#32852;&#21512;&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#21644;&#32858;&#31867;&#27169;&#22411;&#65292;&#20197;&#23558;&#27599;&#20010;&#26032;&#25968;&#25454;&#28857;&#25110;&#25209;&#22788;&#29702;&#20998;&#37197;&#21040;&#32858;&#31867;&#26631;&#31614;&#20013;&#12290;&#23613;&#31649;&#27604;&#31163;&#32447;&#26041;&#27861;&#26356;&#24555;&#36895;&#21644;&#26356;&#28789;&#27963;&#65292;&#20294;&#22312;&#32447;&#32858;&#31867;&#24456;&#23481;&#26131;&#36798;&#21040;&#23849;&#28291;&#35299;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#23558;&#25152;&#26377;&#36755;&#20837;&#26144;&#23556;&#21040;&#21516;&#19968;&#28857;&#65292;&#24182;&#23558;&#25152;&#26377;&#36755;&#20837;&#25918;&#20837;&#21333;&#20010;&#32858;&#31867;&#20013;&#12290;&#29616;&#26377;&#25104;&#21151;&#27169;&#22411;&#37319;&#29992;&#20102;&#21508;&#31181;&#25216;&#26415;&#26469;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#25110;&#26088;&#22312;&#20351;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#32858;&#31867;&#30340;&#24179;&#22343;&#36719;&#20998;&#37197;&#30456;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#23545;&#30828;&#20998;&#37197;&#36827;&#34892;&#20102;&#35268;&#21017;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#23548;&#20986;&#19968;&#20010;&#30452;&#35266;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#21487;&#20197;&#30452;&#25509;&#21253;&#21547;&#22312;&#32534;&#30721;&#22120;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#12290;&#22312;&#22235;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#23427;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#21152;&#31283;&#23450;&#22320;&#36991;&#20813;&#20102;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online deep clustering refers to the joint use of a feature extraction network and a clustering model to assign cluster labels to each new data point or batch as it is processed. While faster and more versatile than offline methods, online clustering can easily reach the collapsed solution where the encoder maps all inputs to the same point and all are put into a single cluster. Successful existing models have employed various techniques to avoid this problem, most of which require data augmentation or which aim to make the average soft assignment across the dataset the same for each cluster. We propose a method that does not require data augmentation, and that, differently from existing methods, regularizes the hard assignments. Using a Bayesian framework, we derive an intuitive optimization objective that can be straightforwardly included in the training of the encoder network. Tested on four image datasets, we show that it consistently avoids collapse more robustly than other method
&lt;/p&gt;</description></item><item><title>DeepGD&#26159;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;&#40657;&#30418;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#39640;&#38169;&#35823;&#26292;&#38706;&#33021;&#21147;&#30340;&#27979;&#35797;&#36755;&#20837;&#26469;&#38477;&#20302;&#26631;&#35760;&#25104;&#26412;&#65292;&#21516;&#26102;&#36873;&#25321;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#30340;&#27979;&#35797;&#36755;&#20837;&#20197;&#23613;&#21487;&#33021;&#35302;&#21457;&#26356;&#22810;&#30340;&#35823;&#39044;&#27979;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#25581;&#31034;DNN&#27169;&#22411;&#20013;&#19981;&#21516;&#32570;&#38519;&#30340;&#27010;&#29575;&#26469;&#22686;&#21152;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.04878</link><description>&lt;p&gt;
DeepGD: &#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;&#40657;&#30418;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks. (arXiv:2303.04878v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04878
&lt;/p&gt;
&lt;p&gt;
DeepGD&#26159;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;&#40657;&#30418;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#39640;&#38169;&#35823;&#26292;&#38706;&#33021;&#21147;&#30340;&#27979;&#35797;&#36755;&#20837;&#26469;&#38477;&#20302;&#26631;&#35760;&#25104;&#26412;&#65292;&#21516;&#26102;&#36873;&#25321;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#30340;&#27979;&#35797;&#36755;&#20837;&#20197;&#23613;&#21487;&#33021;&#35302;&#21457;&#26356;&#22810;&#30340;&#35823;&#39044;&#27979;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#25581;&#31034;DNN&#27169;&#22411;&#20013;&#19981;&#21516;&#32570;&#38519;&#30340;&#27010;&#29575;&#26469;&#22686;&#21152;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36755;&#20837;&#22495;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#65292;&#27979;&#35797;DNN&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#27979;&#35797;DNN&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#29983;&#25104;&#25110;&#25506;&#32034;&#22823;&#35268;&#27169;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;DNN&#27979;&#35797;&#31070;&#35861;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#24037;&#20316;&#26469;&#26631;&#35760;&#27979;&#35797;&#25968;&#25454;&#65292;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#19987;&#23478;&#20197;&#30830;&#20445;&#26631;&#35760;&#30340;&#27491;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepGD&#65292;&#19968;&#31181;&#29992;&#20110;DNN&#27169;&#22411;&#30340;&#40657;&#30418;&#22810;&#30446;&#26631;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#39640;&#38169;&#35823;&#26292;&#38706;&#33021;&#21147;&#30340;&#27979;&#35797;&#36755;&#20837;&#26469;&#38477;&#20302;&#26631;&#35760;&#25104;&#26412;&#65292;&#21516;&#26102;&#36873;&#25321;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#30340;&#27979;&#35797;&#36755;&#20837;&#20197;&#23613;&#21487;&#33021;&#35302;&#21457;&#26356;&#22810;&#30340;&#35823;&#39044;&#27979;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#25581;&#31034;DNN&#27169;&#22411;&#20013;&#19981;&#21516;&#32570;&#38519;&#30340;&#27010;&#29575;&#26469;&#22686;&#21152;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are widely used in various application domains such as image processing, speech recognition, and natural language processing. However, testing DNN models may be challenging due to the complexity and size of their input domain. Particularly, testing DNN models often requires generating or exploring large unlabeled datasets. In practice, DNN test oracles, which identify the correct outputs for inputs, often require expensive manual effort to label test data, possibly involving multiple experts to ensure labeling correctness. In this paper, we propose DeepGD, a black-box multi-objective test selection approach for DNN models. It reduces the cost of labeling by prioritizing the selection of test inputs with high fault revealing power from large unlabeled datasets. DeepGD not only selects test inputs with high uncertainty scores to trigger as many mispredicted inputs as possible but also maximizes the probability of revealing distinct faults in the DNN model by s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#35299;&#20915;&#20102;&#38480;&#21046;&#24179;&#34913;&#28857;Gram&#30697;&#38453;&#26368;&#23567;&#29305;&#24449;&#20540;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.05797</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Global Convergence Rate of Deep Equilibrium Models with General Activations. (arXiv:2302.05797v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#35299;&#20915;&#20102;&#38480;&#21046;&#24179;&#34913;&#28857;Gram&#30697;&#38453;&#26368;&#23567;&#29305;&#24449;&#20540;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#19968;&#31687;&#35770;&#25991;&#20013;&#65292;Ling&#31561;&#20154;&#30740;&#31350;&#20102;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#36807;&#21442;&#25968;&#21270;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#12290;&#20182;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#65292;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#20110;&#20855;&#26377;&#20219;&#20309;&#20855;&#26377;&#26377;&#30028;&#19968;&#38454;&#21644;&#20108;&#38454;&#23548;&#25968;&#30340;&#28608;&#27963;&#20989;&#25968;&#30340;DEQ&#65292;&#35813;&#20107;&#23454;&#20173;&#28982;&#25104;&#31435;&#12290;&#30001;&#20110;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#36890;&#24120;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#38480;&#21046;&#24179;&#34913;&#28857;&#30340;Gram&#30697;&#38453;&#30340;&#26368;&#23567;&#29305;&#24449;&#20540;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#38656;&#35201;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#24635;&#20307;Gram&#30697;&#38453;&#65292;&#24182;&#24320;&#21457;&#19968;&#31181;&#20855;&#26377;Hermite&#22810;&#39033;&#24335;&#23637;&#24320;&#30340;&#26032;&#24418;&#24335;&#30340;&#21452;&#37325;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a recent paper, Ling et al. investigated the over-parametrized Deep Equilibrium Model (DEQ) with ReLU activation. They proved that the gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. This paper shows that this fact still holds for DEQs with any general activation that has bounded first and second derivatives. Since the new activation function is generally non-linear, bounding the least eigenvalue of the Gram matrix of the equilibrium point is particularly challenging. To accomplish this task, we need to create a novel population Gram matrix and develop a new form of dual activation with Hermite polynomial expansion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24615; Bandit &#29615;&#22659;&#19979;&#38024;&#23545;&#21253;&#21547;&#24322;&#26041;&#24046;&#22870;&#21169;&#22122;&#22768;&#30340;&#31574;&#30053;&#35780;&#20272;&#65292;&#20351;&#29992;&#26368;&#20248;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#30340;&#26032;&#31639;&#27861; SPEED&#65292;&#35813;&#31639;&#27861;&#21487;&#23454;&#29616;&#24102;&#26377;&#22343;&#26041;&#35823;&#24046;&#27604;&#36739;&#23567;&#30340;&#31574;&#30053;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2301.12357</link><description>&lt;p&gt;
SPEED: &#32447;&#24615;&#24322;&#26041;&#24046; Bandit &#31574;&#30053;&#35780;&#20272;&#30340;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits. (arXiv:2301.12357v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24615; Bandit &#29615;&#22659;&#19979;&#38024;&#23545;&#21253;&#21547;&#24322;&#26041;&#24046;&#22870;&#21169;&#22122;&#22768;&#30340;&#31574;&#30053;&#35780;&#20272;&#65292;&#20351;&#29992;&#26368;&#20248;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#30340;&#26032;&#31639;&#27861; SPEED&#65292;&#35813;&#31639;&#27861;&#21487;&#23454;&#29616;&#24102;&#26377;&#22343;&#26041;&#35823;&#24046;&#27604;&#36739;&#23567;&#30340;&#31574;&#30053;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615; Bandit &#19979;&#31574;&#30053;&#35780;&#20272;&#30340;&#26368;&#20248;&#25968;&#25454;&#25910;&#38598;&#38382;&#39064;&#12290;&#22312;&#31574;&#30053;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#20272;&#35745;&#22810;&#33218;&#36172;&#21338;&#26426;&#29615;&#22659;&#20013;&#25191;&#34892;&#30446;&#26631;&#31574;&#30053;&#23558;&#33719;&#24471;&#30340;&#26399;&#26395;&#25910;&#30410;&#12290;&#26412;&#25991;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#35299;&#20915;&#32447;&#24615; Bandit &#29615;&#22659;&#19979;&#21253;&#21547;&#24322;&#26041;&#24046;&#22870;&#21169;&#22122;&#22768;&#30340;&#31574;&#30053;&#35780;&#20272;&#30340;&#26368;&#20248;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#32447;&#24615; Bandit &#29615;&#22659;&#19979;&#21046;&#23450;&#20102;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#30340;&#26368;&#20248;&#35774;&#35745;&#65292;&#20197;&#20943;&#23569;&#30446;&#26631;&#31574;&#30053;&#20215;&#20540;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#35774;&#35745;&#26469;&#25512;&#23548;&#20986;&#25968;&#25454;&#25910;&#38598;&#26399;&#38388;&#27599;&#20010;&#21160;&#20316;&#30340;&#26368;&#20248;&#26679;&#26412;&#20998;&#37197;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026; SPEED&#65288;Structured Policy Evaluation Experimental Design&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36319;&#36394;&#26368;&#20248;&#35774;&#35745;&#65292;&#24182;&#35745;&#31639;&#20854;&#19982;&#26368;&#20248;&#35774;&#35745;&#30340;&#36951;&#25022;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126; SPEED &#21487;&#20197;&#23454;&#29616;&#24102;&#26377;&#22343;&#26041;&#35823;&#24046;&#27604;&#36739;&#23567;&#30340;&#31574;&#30053;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of optimal data collection for policy evaluation in linear bandits. In policy evaluation, we are given a target policy and asked to estimate the expected reward it will obtain when executed in a multi-armed bandit environment. Our work is the first work that focuses on such optimal data collection strategy for policy evaluation involving heteroscedastic reward noise in the linear bandit setting. We first formulate an optimal design for weighted least squares estimates in the heteroscedastic linear bandit setting that reduces the MSE of the value of the target policy. We then use this formulation to derive the optimal allocation of samples per action during data collection. We then introduce a novel algorithm SPEED (Structured Policy Evaluation Experimental Design) that tracks the optimal design and derive its regret with respect to the optimal design. Finally, we empirically validate that SPEED leads to policy evaluation with mean squared error compa
&lt;/p&gt;</description></item><item><title>&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32852;&#21512;&#23398;&#20064;&#20026;&#26412;&#22320;&#23458;&#25143;&#23450;&#21046;&#30340;&#20010;&#24615;&#21270;&#22270;&#20197;&#21450;&#20849;&#35782;&#22270;&#65292;&#20197;&#25512;&#26029;&#28508;&#22312;&#22270;&#25299;&#25169;&#65292;&#21516;&#26102;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.06662</link><description>&lt;p&gt;
&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#22270;&#25299;&#25169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Topology Learning Under Privacy Constraints. (arXiv:2301.06662v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06662
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32852;&#21512;&#23398;&#20064;&#20026;&#26412;&#22320;&#23458;&#25143;&#23450;&#21046;&#30340;&#20010;&#24615;&#21270;&#22270;&#20197;&#21450;&#20849;&#35782;&#22270;&#65292;&#20197;&#25512;&#26029;&#28508;&#22312;&#22270;&#25299;&#25169;&#65292;&#21516;&#26102;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#25968;&#25454;&#20998;&#24067;&#20110;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#19988;&#20855;&#26377;&#38544;&#31169;&#25935;&#24863;&#24615;&#30340;&#26032;&#39062;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#24179;&#28369;&#22270;&#20449;&#21495;&#25512;&#26029;&#28508;&#22312;&#22270;&#25299;&#25169;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#22256;&#38590;&#22312;&#20110;&#22914;&#20309;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21033;&#29992;&#25152;&#26377;&#29420;&#31435;&#23458;&#25143;&#31471;&#30340;&#28508;&#22312;&#24322;&#26500;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#20026;&#26412;&#22320;&#23458;&#25143;&#31471;&#23450;&#21046;&#30340;&#20010;&#24615;&#21270;&#22270;&#20197;&#21450;&#20849;&#35782;&#22270;&#12290;&#20010;&#24615;&#21270;&#22270;&#21305;&#37197;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#20943;&#36731;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#32780;&#20849;&#35782;&#22270;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#12290;&#25105;&#20204;&#25509;&#19979;&#26469;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#24341;&#20837;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#19981;&#36829;&#21453;&#38544;&#31169;&#32422;&#26463;&#65292;&#21363;&#25152;&#26377;&#30340;&#31169;&#26377;&#25968;&#25454;&#37117;&#22312;&#26412;&#22320;&#22788;&#29702;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#25105;&#20204;&#23558;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#24341;&#20837;&#21040;&#25152;&#25552;&#31639;&#27861;&#20013;&#65292;&#22312;&#20256;&#36755;&#27169;&#22411;&#26356;&#26032;&#26102;&#25269;&#24481;&#38544;&#31169;&#25915;&#20987;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of inferring the underlying graph topology from smooth graph signals in a novel but practical scenario where data are located in distributed clients and are privacy-sensitive. The main difficulty of this task lies in how to utilize the potentially heterogeneous data of all isolated clients under privacy constraints. Towards this end, we propose a framework where personalized graphs for local clients as well as a consensus graph are jointly learned. The personalized graphs match local data distributions, thereby mitigating data heterogeneity, while the consensus graph captures the global information. We next devise a tailored algorithm to solve the induced problem without violating privacy constraints, i.e., all private data are processed locally. To further enhance privacy protection, we introduce differential privacy (DP) into the proposed algorithm to resist privacy attacks when transmitting model updates. Theoretically, we establish provable convergence analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#25968;&#25454;&#27745;&#26579;&#21518;&#38376;&#25915;&#20987;&#23545;&#23545;&#27604;&#23398;&#20064;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CorruptEncoder&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#29702;&#35770;&#23548;&#21521;&#30340;&#26041;&#24335;&#21019;&#24314;&#20248;&#21270;&#30340;&#27745;&#26579;&#36755;&#20837;&#65292;&#22823;&#24133;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CorruptEncoder&#26159;&#39318;&#20010;&#20165;&#38656;&#35201;&#23569;&#37327;&#22270;&#20687;&#21644;&#27745;&#26579;&#27604;&#20363;&#21363;&#21487;&#36798;&#21040;90%&#20197;&#19978;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23616;&#37096;&#35009;&#21098;&#30340;&#38450;&#24481;&#31574;&#30053;&#26469;&#24212;&#23545;&#25968;&#25454;&#27745;&#26579;&#21518;&#38376;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2211.08229</link><description>&lt;p&gt;
CorruptEncoder&#65306;&#22522;&#20110;&#25968;&#25454;&#27745;&#26579;&#30340;&#23545;&#27604;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning. (arXiv:2211.08229v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#25968;&#25454;&#27745;&#26579;&#21518;&#38376;&#25915;&#20987;&#23545;&#23545;&#27604;&#23398;&#20064;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CorruptEncoder&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#29702;&#35770;&#23548;&#21521;&#30340;&#26041;&#24335;&#21019;&#24314;&#20248;&#21270;&#30340;&#27745;&#26579;&#36755;&#20837;&#65292;&#22823;&#24133;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CorruptEncoder&#26159;&#39318;&#20010;&#20165;&#38656;&#35201;&#23569;&#37327;&#22270;&#20687;&#21644;&#27745;&#26579;&#27604;&#20363;&#21363;&#21487;&#36798;&#21040;90%&#20197;&#19978;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23616;&#37096;&#35009;&#21098;&#30340;&#38450;&#24481;&#31574;&#30053;&#26469;&#24212;&#23545;&#25968;&#25454;&#27745;&#26579;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#20351;&#29992;&#26080;&#26631;&#31614;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#36890;&#29992;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#22270;&#20687;&#25110;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#23545;&#27604;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#22522;&#20110;&#25968;&#25454;&#27745;&#26579;&#30340;&#21518;&#38376;&#25915;&#20987;&#65288;DPBA&#65289;&#30340;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#21521;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27880;&#20837;&#34987;&#27745;&#26579;&#30340;&#36755;&#20837;&#26469;&#21518;&#38376;&#21270;&#32534;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DPBA&#30340;&#25928;&#26524;&#26377;&#38480;&#12290;&#26412;&#25991;&#39318;&#20808;&#20998;&#26512;&#29616;&#26377;&#25915;&#20987;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CorruptEncoder&#30340;&#26032;&#22411;DPBA&#26469;&#23545;&#25239;&#23545;&#27604;&#23398;&#20064;&#12290;CorruptEncoder&#20351;&#29992;&#29702;&#35770;&#23548;&#21521;&#30340;&#26041;&#27861;&#21019;&#24314;&#26368;&#20248;&#30340;&#27745;&#26579;&#36755;&#20837;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CorruptEncoder&#22312;&#25915;&#20987;&#25928;&#26524;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;DPBA&#12290;&#23588;&#20854;&#26159;&#65292;CorruptEncoder&#26159;&#39318;&#20010;&#20165;&#38656;&#35201;&#23569;&#37327;&#65288;3&#20010;&#65289;&#21442;&#32771;&#22270;&#20687;&#21644;&#23567;&#35268;&#27169;&#27745;&#26579;&#27604;&#20363;&#65288;0.5%&#65289;&#21363;&#21487;&#36798;&#21040;90%&#20197;&#19978;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;DPBA&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23616;&#37096;&#35009;&#21098;&#30340;&#38450;&#24481;&#31574;&#30053;&#26469;&#25269;&#24481;DPBA&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#31574;&#30053;&#33021;&#26377;&#25928;&#25269;&#24481;DPBA&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning (CL) pre-trains general-purpose encoders using an unlabeled pre-training dataset, which consists of images or image-text pairs. CL is vulnerable to data poisoning based backdoor attacks (DPBAs), in which an attacker injects poisoned inputs into the pre-training dataset so the encoder is backdoored. However, existing DPBAs achieve limited effectiveness. In this work, we take the first step to analyze the limitations of existing attacks and propose new DPBAs called CorruptEncoder to CL. CorruptEncoder uses a theory-guided method to create optimal poisoned inputs to maximize attack effectiveness. Our experiments show that CorruptEncoder substantially outperforms existing DPBAs. In particular, CorruptEncoder is the first DPBA that achieves more than 90% attack success rates with only a few (3) reference images and a small poisoning ratio (0.5%). Moreover, we also propose a defense, called localized cropping, to defend against DPBAs. Our results show that our defense ca
&lt;/p&gt;</description></item><item><title>ImpNet&#26159;&#19968;&#31181;&#22312;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;&#20013;&#28155;&#21152;&#30340;&#19981;&#21487;&#23519;&#35273;&#21644;&#40657;&#30418;&#19981;&#21487;&#26816;&#27979;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#36825;&#20123;&#21518;&#38376;&#21487;&#20197;&#32469;&#36807;&#25968;&#25454;&#20934;&#22791;&#21644;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#24182;&#19988;&#21482;&#33021;&#22312;&#25554;&#20837;&#38454;&#27573;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#65292;&#31227;&#38500;&#23427;&#20204;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.00108</link><description>&lt;p&gt;
ImpNet: &#32534;&#35793;&#31070;&#32463;&#32593;&#32476;&#20013;&#19981;&#21487;&#23519;&#35273;&#21644;&#40657;&#30418;&#19981;&#21487;&#26816;&#27979;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks. (arXiv:2210.00108v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00108
&lt;/p&gt;
&lt;p&gt;
ImpNet&#26159;&#19968;&#31181;&#22312;&#32534;&#35793;&#31070;&#32463;&#32593;&#32476;&#20013;&#28155;&#21152;&#30340;&#19981;&#21487;&#23519;&#35273;&#21644;&#40657;&#30418;&#19981;&#21487;&#26816;&#27979;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#36825;&#20123;&#21518;&#38376;&#21487;&#20197;&#32469;&#36807;&#25968;&#25454;&#20934;&#22791;&#21644;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#24182;&#19988;&#21482;&#33021;&#22312;&#25554;&#20837;&#38454;&#27573;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#65292;&#31227;&#38500;&#23427;&#20204;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#30340;&#21518;&#38376;&#25915;&#20987;&#24341;&#36215;&#20102;&#25915;&#38450;&#24320;&#21457;&#20013;&#30340;&#19968;&#22330;&#31454;&#20105;&#12290;&#38450;&#24481;&#25163;&#27573;&#24050;&#32463;&#20986;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27169;&#22411;&#20013;&#26816;&#27979;&#21644;&#31227;&#38500;&#21518;&#38376;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#38450;&#24481;&#25163;&#27573;&#36890;&#36807;&#26816;&#26597;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#25110;&#35757;&#32451;&#36807;&#31243;&#30340;&#23436;&#25972;&#24615;&#26469;&#24037;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21518;&#38376;&#21487;&#20197;&#22312;&#32534;&#35793;&#36807;&#31243;&#20013;&#28155;&#21152;&#65292;&#32469;&#36807;&#20102;&#25968;&#25454;&#20934;&#22791;&#21644;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#30340;&#20219;&#20309;&#20445;&#25252;&#25514;&#26045;&#12290;&#25915;&#20987;&#32773;&#19981;&#20165;&#21487;&#20197;&#22312;&#32534;&#35793;&#36807;&#31243;&#20013;&#25554;&#20837;&#24050;&#26377;&#30340;&#22522;&#20110;&#26435;&#37325;&#30340;&#21518;&#38376;&#65292;&#36824;&#21487;&#20197;&#25554;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#26435;&#37325;&#26080;&#20851;&#30340;&#21518;&#38376;&#65292;&#20363;&#22914;ImpNet&#12290;&#36825;&#20123;&#21518;&#38376;&#22312;&#35757;&#32451;&#25110;&#25968;&#25454;&#20934;&#22791;&#36807;&#31243;&#20013;&#26159;&#19981;&#21487;&#26816;&#27979;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#23578;&#19981;&#23384;&#22312;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#21518;&#38376;&#65292;&#21253;&#25324;ImpNet&#65292;&#21482;&#33021;&#22312;&#25554;&#20837;&#23427;&#20204;&#30340;&#38454;&#27573;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#65292;&#32780;&#22312;&#20854;&#20182;&#20219;&#20309;&#22320;&#26041;&#31227;&#38500;&#23427;&#20204;&#37117;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#38656;&#35201;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early backdoor attacks against machine learning set off an arms race in attack and defence development. Defences have since appeared demonstrating some ability to detect backdoors in models or even remove them. These defences work by inspecting the training data, the model, or the integrity of the training procedure. In this work, we show that backdoors can be added during compilation, circumventing any safeguards in the data preparation and model training stages. The attacker can not only insert existing weight-based backdoors during compilation, but also a new class of weight-independent backdoors, such as ImpNet. These backdoors are impossible to detect during the training or data preparation processes, because they are not yet present. Next, we demonstrate that some backdoors, including ImpNet, can only be reliably detected at the stage where they are inserted and removing them anywhere else presents a significant challenge. We conclude that ML model security requires assurance of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#65292;&#33021;&#22815;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#36866;&#24212;&#26410;&#30693;&#26799;&#24230;&#33539;&#25968;&#12289;&#24179;&#28369;&#24615;&#21644;&#24378;&#20984;&#24615;&#65292;&#24182;&#22312;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#39640;&#27010;&#29575;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2205.02160</link><description>&lt;p&gt;
&#20351;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#26080;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making SGD Parameter-Free. (arXiv:2205.02160v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02160
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#65292;&#33021;&#22815;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#36866;&#24212;&#26410;&#30693;&#26799;&#24230;&#33539;&#25968;&#12289;&#24179;&#28369;&#24615;&#21644;&#24378;&#20984;&#24615;&#65292;&#24182;&#22312;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#39640;&#27010;&#29575;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#21442;&#25968;&#38543;&#26426;&#20984;&#20248;&#21270;&#65288;SCO&#65289;&#31639;&#27861;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#20165;&#27604;&#23545;&#24212;&#30340;&#24050;&#30693;&#21442;&#25968;&#35774;&#32622;&#30340;&#26368;&#20248;&#36895;&#24230;&#22810;&#19968;&#20010;&#21452;&#23545;&#25968;&#22240;&#23376;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20808;&#21069;&#24050;&#30693;&#30340;&#26080;&#21442;&#25968;SCO&#30340;&#26368;&#20339;&#36895;&#24230;&#26159;&#22522;&#20110;&#22312;&#32447;&#26080;&#21442;&#25968;&#21518;&#24724;&#30028;&#30340;&#65292;&#19982;&#24050;&#30693;&#21442;&#25968;&#30340;&#23545;&#24212;&#26041;&#27861;&#30456;&#27604;&#21253;&#21547;&#19981;&#21487;&#36991;&#20813;&#30340;&#39069;&#22806;&#23545;&#25968;&#39033;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#27010;&#24565;&#19978;&#30340;&#31616;&#21333;&#24615;&#65292;&#20855;&#26377;&#39640;&#27010;&#29575;&#20445;&#35777;&#65292;&#24182;&#19988;&#37096;&#20998;&#36866;&#24212;&#26410;&#30693;&#26799;&#24230;&#33539;&#25968;&#12289;&#24179;&#28369;&#24615;&#21644;&#24378;&#20984;&#24615;&#12290;&#25105;&#20204;&#30340;&#25104;&#26524;&#30340;&#26680;&#24515;&#26159;SGD&#27493;&#38271;&#36873;&#25321;&#30340;&#26032;&#22411;&#26080;&#21442;&#25968;&#35777;&#20070;&#65292;&#20197;&#21450;&#20551;&#35774;&#22312;SGD&#36845;&#20195;&#19978;&#27809;&#26377;&#20808;&#39564;&#30028;&#38480;&#30340;&#26102;&#38388;&#19968;&#33268;&#38598;&#20013;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop an algorithm for parameter-free stochastic convex optimization (SCO) whose rate of convergence is only a double-logarithmic factor larger than the optimal rate for the corresponding known-parameter setting. In contrast, the best previously known rates for parameter-free SCO are based on online parameter-free regret bounds, which contain unavoidable excess logarithmic terms compared to their known-parameter counterparts. Our algorithm is conceptually simple, has high-probability guarantees, and is also partially adaptive to unknown gradient norms, smoothness, and strong convexity. At the heart of our results is a novel parameter-free certificate for SGD step size choice, and a time-uniform concentration result that assumes no a-priori bounds on SGD iterates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#22810;&#21464;&#37327;&#22823;&#25968;&#25454;&#20998;&#26512;&#65288;MBDA&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25512;&#23548;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#21644;&#20132;&#20114;&#24335;&#27169;&#22411;&#30340;&#20248;&#21183;&#20197;&#21450;&#24182;&#34892;&#22788;&#29702;&#30340;&#33021;&#21147;&#65292;&#24212;&#29992;&#20110;&#32593;&#32476;&#30417;&#27979;&#21644;&#35786;&#26029;&#65292;&#26368;&#32456;&#22312;UGR'16&#21644;Dartmouth'18&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/1907.02677</link><description>&lt;p&gt;
&#22810;&#21464;&#37327;&#22823;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#23398;&#20064;&#29992;&#20110;&#32593;&#32476;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpretable Learning in Multivariate Big Data Analysis for Network Monitoring. (arXiv:1907.02677v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1907.02677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#22810;&#21464;&#37327;&#22823;&#25968;&#25454;&#20998;&#26512;&#65288;MBDA&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25512;&#23548;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#21644;&#20132;&#20114;&#24335;&#27169;&#22411;&#30340;&#20248;&#21183;&#20197;&#21450;&#24182;&#34892;&#22788;&#29702;&#30340;&#33021;&#21147;&#65292;&#24212;&#29992;&#20110;&#32593;&#32476;&#30417;&#27979;&#21644;&#35786;&#26029;&#65292;&#26368;&#32456;&#22312;UGR'16&#21644;Dartmouth'18&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20197;&#35780;&#20272;&#36890;&#20449;&#32593;&#32476;&#24615;&#33021;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#65292;&#27604;&#22914;&#32593;&#32476;&#30417;&#27979;&#21644;&#25925;&#38556;&#25490;&#38500;&#65292;&#22914;&#26524;&#19981;&#33021;&#34987;&#20154;&#31867;&#25805;&#20316;&#21592;&#35299;&#37322;&#65292;&#25968;&#25454;&#27169;&#22411;&#23601;&#27809;&#22810;&#22823;&#29992;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#21464;&#37327;&#22823;&#25968;&#25454;&#20998;&#26512;&#65288;MBDA&#65289;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36825;&#26159;&#19968;&#31181;&#36817;&#26399;&#25552;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#12290;&#22312;&#36825;&#20010;&#25193;&#23637;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#25512;&#23548;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#26159;&#24403;&#25968;&#25454;&#37327;&#24222;&#22823;&#26102;&#24212;&#29992;MBDA&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#25152;&#24471;&#21040;&#30340;&#32593;&#32476;&#30417;&#27979;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#26816;&#27979;&#21644;&#35786;&#26029;&#19981;&#21516;&#30340;&#32593;&#32476;&#24322;&#24120;&#65292;&#37319;&#29992;&#19968;&#31181;&#23558;&#21487;&#35299;&#37322;&#24615;&#21644;&#20132;&#20114;&#24335;&#27169;&#22411;&#30340;&#20248;&#21183;&#19982;&#24182;&#34892;&#22788;&#29702;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20316;&#27969;&#12290;&#25105;&#20204;&#23558;&#25193;&#23637;&#30340;MBDA&#24212;&#29992;&#20110;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65306;UGR'16&#65292;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#20934;&#27969;&#37327;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;Dartmouth'18&#65292;&#26368;&#38271;&#21644;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an increasing interest in the development of new data-driven models useful to assess the performance of communication networks. For many applications, like network monitoring and troubleshooting, a data model is of little use if it cannot be interpreted by a human operator. In this paper, we present an extension of the Multivariate Big Data Analysis (MBDA) methodology, a recently proposed interpretable data analysis tool. In this extension, we propose a solution to the automatic derivation of features, a cornerstone step for the application of MBDA when the amount of data is massive. The resulting network monitoring approach allows us to detect and diagnose disparate network anomalies, with a data-analysis workflow that combines the advantages of interpretable and interactive models with the power of parallel processing. We apply the extended MBDA to two case studies: UGR'16, a benchmark flow-based real-traffic dataset for anomaly detection, and Dartmouth'18, the longest and l
&lt;/p&gt;</description></item></channel></rss>