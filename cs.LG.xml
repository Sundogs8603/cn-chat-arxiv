<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21452;&#26354;&#34920;&#31034;&#25429;&#25417;&#22270;&#20687;&#21644;&#25991;&#26412;&#23618;&#27425;&#32467;&#26500;&#30340;&#23545;&#27604;&#27169;&#22411;MERU&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#19982;CLIP&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2304.09172</link><description>&lt;p&gt;
&#21452;&#26354;&#32447;&#22270;&#20687;&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Image-Text Representations. (arXiv:2304.09172v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#21452;&#26354;&#34920;&#31034;&#25429;&#25417;&#22270;&#20687;&#21644;&#25991;&#26412;&#23618;&#27425;&#32467;&#26500;&#30340;&#23545;&#27604;&#27169;&#22411;MERU&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#19982;CLIP&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#27010;&#24565;&#33258;&#28982;&#32780;&#28982;&#22320;&#32452;&#32455;&#25104;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#19968;&#20010;&#25991;&#26412;&#27010;&#24565;&#8220;&#29399;&#8221;&#21253;&#21547;&#25152;&#26377;&#21253;&#21547;&#29399;&#30340;&#22270;&#20687;&#12290;&#23613;&#31649;&#30452;&#35273;&#19978;&#36825;&#26159;&#27491;&#30830;&#30340;&#65292;&#20294;&#30446;&#21069;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#24182;&#27809;&#26377;&#26126;&#30830;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MERU&#65292;&#19968;&#20010;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#36827;&#34892;&#21452;&#26354;&#34920;&#31034;&#30340;&#23545;&#27604;&#27169;&#22411;&#12290;&#21452;&#26354;&#31354;&#38388;&#20855;&#26377;&#23884;&#20837;&#26641;&#29366;&#25968;&#25454;&#30340;&#21512;&#36866;&#20960;&#20309;&#23646;&#24615;&#65292;&#22240;&#27492;MERU&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#30340;&#24213;&#23618;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;MERU&#23398;&#20064;&#21040;&#20102;&#19968;&#20010;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#21516;&#26102;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#31561;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#19982;CLIP&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual and linguistic concepts naturally organize themselves in a hierarchy, where a textual concept ``dog'' entails all images that contain dogs. Despite being intuitive, current large-scale vision and language models such as CLIP do not explicitly capture such hierarchy. We propose MERU, a contrastive model that yields hyperbolic representations of images and text. Hyperbolic spaces have suitable geometric properties to embed tree-like data, so MERU can better capture the underlying hierarchy in image-text data. Our results show that MERU learns a highly interpretable representation space while being competitive with CLIP's performance on multi-modal tasks like image classification and image-text retrieval.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#26694;&#26550;&#65292;&#23558;&#32622;&#25442;&#19981;&#21464;&#39044;&#27979;&#22120;&#30340;&#30041;&#19968;&#20986;&#38169;&#36716;&#21270;&#20026;&#39640;&#27010;&#29575;&#39118;&#38505;&#30028;&#38480;&#65292;&#23454;&#29616;&#20102;&#20026;&#20108;&#20803;&#20998;&#31867;&#23454;&#29616;&#26368;&#20248; PAC &#30028;&#38480;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09167</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#22343;&#21248;&#25910;&#25947;&#30340;&#26368;&#20248; PAC &#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Optimal PAC Bounds Without Uniform Convergence. (arXiv:2304.09167v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#26694;&#26550;&#65292;&#23558;&#32622;&#25442;&#19981;&#21464;&#39044;&#27979;&#22120;&#30340;&#30041;&#19968;&#20986;&#38169;&#36716;&#21270;&#20026;&#39640;&#27010;&#29575;&#39118;&#38505;&#30028;&#38480;&#65292;&#23454;&#29616;&#20102;&#20026;&#20108;&#20803;&#20998;&#31867;&#23454;&#29616;&#26368;&#20248; PAC &#30028;&#38480;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#65292;&#30830;&#23450; VC &#31867;&#21035;&#30340;&#21487;&#23454;&#29616;&#20108;&#36827;&#21046;&#20998;&#31867;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290; Simon &#21644; Hanneke &#30340;&#32467;&#26524;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#23574;&#38160;&#30340;&#19978;&#30028;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35770;&#35777;&#20381;&#36182;&#20110;&#22343;&#21248;&#25910;&#25947;&#21407;&#21017;&#65292;&#38480;&#21046;&#20102;&#20854;&#36866;&#29992;&#24615;&#20110;&#26356;&#19968;&#33324;&#30340;&#23398;&#20064;&#35774;&#32622;&#65292;&#20363;&#22914;&#22810;&#31867;&#20998;&#31867;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#36229;&#36234;&#22343;&#21248;&#25910;&#25947;&#35770;&#35777;&#38480;&#21046;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#26368;&#20248;&#30340;&#39640;&#27010;&#29575;&#39118;&#38505;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#32622;&#25442;&#19981;&#21464;&#39044;&#27979;&#22120;&#30340;&#30041;&#19968;&#20986;&#38169;&#36716;&#21270;&#20026;&#39640;&#27010;&#29575;&#39118;&#38505;&#30028;&#38480;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#36890;&#36807;&#25913;&#32534; Haussler&#12289;Littlestone &#21644; Warmuth &#30340;&#19968;&#21253;&#21547;&#22270;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20108;&#20803;&#20998;&#31867;&#23454;&#29616;&#26368;&#20248; PAC &#30028;&#38480;&#30340;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26576;&#20123;&#19968;&#21253;&#21547;&#22270;&#31639;&#27861;&#30340;&#32858;&#21512;&#26159;&#26368;&#20248;&#30340;&#65292;&#35299;&#20915;&#20102;&#19968;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In statistical learning theory, determining the sample complexity of realizable binary classification for VC classes was a long-standing open problem. The results of Simon and Hanneke established sharp upper bounds in this setting. However, the reliance of their argument on the uniform convergence principle limits its applicability to more general learning settings such as multiclass classification. In this paper, we address this issue by providing optimal high probability risk bounds through a framework that surpasses the limitations of uniform convergence arguments.  Our framework converts the leave-one-out error of permutation invariant predictors into high probability risk bounds. As an application, by adapting the one-inclusion graph algorithm of Haussler, Littlestone, and Warmuth, we propose an algorithm that achieves an optimal PAC bound for binary classification. Specifically, our result shows that certain aggregations of one-inclusion graph algorithms are optimal, addressing a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#25345;&#30340;&#24490;&#29615; GAN &#27169;&#22411;&#26469;&#25913;&#21892;&#26080;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#22495;&#33258;&#36866;&#24212;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;&#24490;&#29615; GAN &#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#20998;&#21106;&#25439;&#22833;&#39033;&#20197;&#20445;&#30041;&#24863;&#20852;&#36259;&#30340;&#32467;&#26500;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#22495;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09164</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#22495;&#33258;&#36866;&#24212;&#30340;&#32467;&#26500;&#20445;&#25345;&#24490;&#29615; GAN
&lt;/p&gt;
&lt;p&gt;
Structure Preserving Cycle-GAN for Unsupervised Medical Image Domain Adaptation. (arXiv:2304.09164v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#25345;&#30340;&#24490;&#29615; GAN &#27169;&#22411;&#26469;&#25913;&#21892;&#26080;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#22495;&#33258;&#36866;&#24212;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;&#24490;&#29615; GAN &#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#20998;&#21106;&#25439;&#22833;&#39033;&#20197;&#20445;&#30041;&#24863;&#20852;&#36259;&#30340;&#32467;&#26500;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#22495;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#20013;&#30340;&#22495;&#20559;&#31227;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#20998;&#21106;&#27169;&#22411;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#22495;&#26102;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#23545;&#25239;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914; Cycle-GAN&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#21307;&#23398;&#22270;&#20687;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#30340;&#24120;&#35265;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36716;&#25442;&#21307;&#23398;&#25195;&#25551;&#26102;&#27809;&#26377;&#24378;&#21046;&#25191;&#34892;&#24863;&#20852;&#36259;&#32467;&#26500;&#30340;&#20445;&#30041;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20998;&#21106;&#30340;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#32467;&#26524;&#36739;&#24046;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#25345;&#24490;&#29615;GAN&#65288;SP Cycle-GAN&#65289;&#65292;&#36890;&#36807;&#22312;&#25972;&#20010;&#24490;&#29615;GAN&#35757;&#32451;&#36807;&#31243;&#20013;&#25191;&#34892;&#20998;&#21106;&#25439;&#22833;&#39033;&#65292;&#20419;&#36827;&#21307;&#23398;&#32467;&#26500;&#22312;&#22270;&#20687;&#36716;&#25442;&#36807;&#31243;&#20013;&#30340;&#20445;&#25345;&#12290;&#25105;&#20204;&#36890;&#36807;&#35270;&#35273;&#21644;&#23545;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#27169;&#22411;&#30340; Dice &#25351;&#26631;&#20998;&#21106;&#24615;&#33021;&#27604;&#36739;&#23637;&#31034;&#20102; SP Cycle-GAN &#30340;&#32467;&#26500;&#20445;&#25345;&#33021;&#21147;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340; Cycle-GAN &#27169;&#22411;&#65292;SP Cycle-GAN &#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#22495;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The presence of domain shift in medical imaging is a common issue, which can greatly impact the performance of segmentation models when dealing with unseen image domains. Adversarial-based deep learning models, such as Cycle-GAN, have become a common model for approaching unsupervised domain adaptation of medical images. These models however, have no ability to enforce the preservation of structures of interest when translating medical scans, which can lead to potentially poor results for unsupervised domain adaptation within the context of segmentation. This work introduces the Structure Preserving Cycle-GAN (SP Cycle-GAN), which promotes medical structure preservation during image translation through the enforcement of a segmentation loss term in the overall Cycle-GAN training process. We demonstrate the structure preserving capability of the SP Cycle-GAN both visually and through comparison of Dice score segmentation performance for the unsupervised domain adaptation models. The SP 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; Calibrate Proxy &#32467;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#38469;&#26679;&#26412;&#20449;&#24687;&#25913;&#21892;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;&#25439;&#22833;&#30456;&#20284;&#24615;&#35745;&#31639;&#65292;&#24341;&#20837;&#19968;&#20010;&#26657;&#20934;&#25439;&#22833;&#26469;&#32422;&#26463;&#20195;&#29702;&#20248;&#21270;&#26041;&#21521;&#31867;&#21035;&#29305;&#24449;&#20013;&#24515;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#36739;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.09162</link><description>&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#26657;&#20934;&#20195;&#29702;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Robust Calibrate Proxy Loss for Deep Metric Learning. (arXiv:2304.09162v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; Calibrate Proxy &#32467;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#38469;&#26679;&#26412;&#20449;&#24687;&#25913;&#21892;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;&#25439;&#22833;&#30456;&#20284;&#24615;&#35745;&#31639;&#65292;&#24341;&#20837;&#19968;&#20010;&#26657;&#20934;&#25439;&#22833;&#26469;&#32422;&#26463;&#20195;&#29702;&#20248;&#21270;&#26041;&#21521;&#31867;&#21035;&#29305;&#24449;&#20013;&#24515;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#36739;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20013;&#65292;&#20027;&#27969;&#30740;&#31350;&#21487;&#20998;&#20026;&#20004;&#31181;&#65306;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#25104;&#23545;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#30001;&#20110;&#35757;&#32451;&#22797;&#26434;&#24230;&#20302;&#12289;&#32593;&#32476;&#25910;&#25947;&#24555;&#36895;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#23616;&#38480;&#22312;&#20110;&#20195;&#29702;&#20248;&#21270;&#30001;&#32593;&#32476;&#23436;&#25104;&#65292;&#20351;&#24471;&#38590;&#20197;&#20934;&#30830;&#22320;&#34920;&#31034;&#25968;&#25454;&#23454;&#38469;&#31867;&#21035;&#30340;&#29305;&#24449;&#20998;&#24067;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Calibrate Proxy&#65288;CP&#65289;&#32467;&#26500;&#65292;&#21033;&#29992;&#23454;&#38469;&#26679;&#26412;&#20449;&#24687;&#25913;&#21892;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;&#25439;&#22833;&#30340;&#30456;&#20284;&#24230;&#35745;&#31639;&#65292;&#24182;&#24341;&#20837;&#20102;&#26657;&#20934;&#25439;&#22833;&#26469;&#32422;&#26463;&#20195;&#29702;&#20248;&#21270;&#26397;&#21521;&#31867;&#21035;&#29305;&#24449;&#20013;&#24515;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#31867;&#21035;&#35774;&#32622;&#20102;&#23569;&#37327;&#20195;&#29702;&#20197;&#20943;&#36731;&#31867;&#20869;&#24046;&#24322;&#23545;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#21512;&#25104;&#26631;&#31614;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mainstream researche in deep metric learning can be divided into two genres: proxy-based and pair-based methods. Proxy-based methods have attracted extensive attention due to the lower training complexity and fast network convergence. However, these methods have limitations as the poxy optimization is done by network, which makes it challenging for the proxy to accurately represent the feature distrubtion of the real class of data. In this paper, we propose a Calibrate Proxy (CP) structure, which uses the real sample information to improve the similarity calculation in proxy-based loss and introduces a calibration loss to constraint the proxy optimization towards the center of the class features. At the same time, we set a small number of proxies for each class to alleviate the impact of intra-class differences on retrieval performance. The effectiveness of our method is evaluated by extensive experiments on three public datasets and multiple synthetic label-noise datasets. The res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;UNet&#21644;Deeplabv3&#65292;&#29992;&#20110;&#26089;&#26399;&#26816;&#27979;&#21644;&#20998;&#21106;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#33041;&#32959;&#30244;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#20102;&#20934;&#30830;&#26816;&#27979;&#21644;&#20998;&#21106;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#33041;&#32959;&#30244;&#30340;&#33021;&#21147;&#12290;&#20294;Deeplabv3&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2304.09133</link><description>&lt;p&gt;
&#26816;&#27979;&#21644;&#20998;&#31867;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#33041;&#32959;&#30244;
&lt;/p&gt;
&lt;p&gt;
Detection and Classification of Glioblastoma Brain Tumor. (arXiv:2304.09133v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;UNet&#21644;Deeplabv3&#65292;&#29992;&#20110;&#26089;&#26399;&#26816;&#27979;&#21644;&#20998;&#21106;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#33041;&#32959;&#30244;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#20102;&#20934;&#30830;&#26816;&#27979;&#21644;&#20998;&#21106;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#33041;&#32959;&#30244;&#30340;&#33021;&#21147;&#12290;&#20294;Deeplabv3&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#33041;&#32959;&#30244;&#26159;&#39640;&#24230;&#33268;&#21629;&#30340;&#65292;&#26089;&#26399;&#26816;&#27979;&#21644;&#20934;&#30830;&#30340;&#20998;&#21106;&#23545;&#20110;&#26377;&#25928;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;UNet&#21644;Deeplabv3&#65292;&#29992;&#20110;&#20351;&#29992;&#39044;&#22788;&#29702;&#30340;&#33041;MRI&#22270;&#20687;&#26816;&#27979;&#21644;&#20998;&#21106;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#33041;&#32959;&#30244;&#12290;&#38024;&#23545;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#21253;&#25324;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UNet&#21644;Deeplabv3&#27169;&#22411;&#37117;&#33021;&#20934;&#30830;&#26816;&#27979;&#21644;&#20998;&#21106;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#33041;&#32959;&#30244;&#12290;&#28982;&#32780;&#65292;Deeplabv3&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;UNet&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20026;&#26089;&#26399;&#26816;&#27979;&#21644;&#20998;&#21106;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#33041;&#32959;&#30244;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#21046;&#23450;&#26377;&#25928;&#30340;&#27835;&#30103;&#31574;&#30053;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#20197;&#38598;&#20013;&#22312;&#20248;&#21270;Deeplabv3&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Glioblastoma brain tumors are highly malignant and often require early detection and accurate segmentation for effective treatment. We are proposing two deep learning models in this paper, namely UNet and Deeplabv3, for the detection and segmentation of glioblastoma brain tumors using preprocessed brain MRI images. The performance evaluation is done for these models in terms of accuracy and computational efficiency. Our experimental results demonstrate that both UNet and Deeplabv3 models achieve accurate detection and segmentation of glioblastoma brain tumors. However, Deeplabv3 outperforms UNet in terms of accuracy, albeit at the cost of requiring more computational resources. Our proposed models offer a promising approach for the early detection and segmentation of glioblastoma brain tumors, which can aid in effective treatment strategies. Further research can focus on optimizing the computational efficiency of the Deeplabv3 model while maintaining its high accuracy for real-world cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026; VRCNet &#30340;&#32593;&#32476;&#65292;&#20855;&#26377;&#20004;&#20010;&#20248;&#28857;&#65306;1)&#27010;&#29575;&#24314;&#27169;&#65307;2&#65289;&#20851;&#31995;&#22686;&#24378;&#12290;&#27492;&#32593;&#32476;&#21487;&#20197;&#29992;&#20110;&#28857;&#20113;&#34917;&#20840;&#65292;&#33021;&#22815;&#25552;&#39640;&#28857;&#20113;&#20960;&#20309;&#24314;&#27169;&#21644;&#24863;&#30693;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.09131</link><description>&lt;p&gt;
&#21464;&#20998;&#20851;&#31995;&#28857;&#34917;&#20840;&#32593;&#32476;&#29992;&#20110;&#40065;&#26834;&#30340;3D&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Relational Point Completion Network for Robust 3D Classification. (arXiv:2304.09131v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026; VRCNet &#30340;&#32593;&#32476;&#65292;&#20855;&#26377;&#20004;&#20010;&#20248;&#28857;&#65306;1)&#27010;&#29575;&#24314;&#27169;&#65307;2&#65289;&#20851;&#31995;&#22686;&#24378;&#12290;&#27492;&#32593;&#32476;&#21487;&#20197;&#29992;&#20110;&#28857;&#20113;&#34917;&#20840;&#65292;&#33021;&#22815;&#25552;&#39640;&#28857;&#20113;&#20960;&#20309;&#24314;&#27169;&#21644;&#24863;&#30693;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#25195;&#25551;&#30340;&#28857;&#20113;&#36890;&#24120;&#30001;&#20110;&#35270;&#35282;&#65292;&#36974;&#25377;&#21644;&#22122;&#22768;&#32780;&#19981;&#23436;&#25972;&#65292;&#36825;&#38459;&#30861;&#20102;3D&#20960;&#20309;&#24314;&#27169;&#21644;&#24863;&#30693;&#12290;&#29616;&#26377;&#30340;&#28857;&#20113;&#34917;&#20840;&#26041;&#27861;&#20542;&#21521;&#20110;&#29983;&#25104;&#20840;&#23616;&#24418;&#29366;&#39592;&#26550;&#65292;&#22240;&#27492;&#32570;&#20047;&#31934;&#32454;&#30340;&#23616;&#37096;&#32454;&#33410;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22823;&#22810;&#23398;&#20064;&#30830;&#23450;&#24615;&#30340;&#37096;&#20998;&#21040;&#23436;&#25972;&#30340;&#26144;&#23556;&#65292;&#20294;&#24573;&#35270;&#20102;&#20154;&#36896;&#29289;&#20307;&#20013;&#30340;&#32467;&#26500;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21464;&#20998;&#26694;&#26550;&#65292;&#21464;&#20998;&#20851;&#31995;&#28857;&#34917;&#20840;&#32593;&#32476;&#65288;VRCNet&#65289;&#65292;&#20855;&#26377;&#20004;&#20010;&#20248;&#28857;&#65306;1&#65289;&#27010;&#29575;&#24314;&#27169;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#36890;&#36335;&#26550;&#26500;&#65292;&#20197;&#22312;&#37096;&#20998;&#28857;&#20113;&#21644;&#23436;&#25972;&#28857;&#20113;&#20043;&#38388;&#23454;&#29616;&#26377;&#21407;&#21017;&#30340;&#27010;&#29575;&#24314;&#27169;&#12290;&#19968;&#20010;&#36890;&#36335;&#20351;&#29992;&#23436;&#25972;&#28857;&#20113;&#36827;&#34892;&#37325;&#24314;&#65292;&#36890;&#36807;&#23398;&#20064;&#28857;VAE&#23454;&#29616;&#12290;&#21478;&#19968;&#20010;&#36890;&#36335;&#20026;&#23616;&#37096;&#28857;&#20113;&#29983;&#25104;&#23436;&#25972;&#30340;&#24418;&#29366;&#65292;&#20854;&#23884;&#20837;&#24335;&#20998;&#24067;&#22312;&#35757;&#32451;&#26399;&#38388;&#30001;&#37325;&#24314;&#36890;&#36335;&#33719;&#24471;&#30340;&#20998;&#24067;&#24341;&#23548;&#12290;2&#65289;&#20851;&#31995;&#22686;&#24378;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21463;&#35821;&#27861;&#27169;&#22411;&#20013;&#35821;&#27861;&#21464;&#37327;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25512;&#29702;&#36807;&#31243;&#65292;&#36890;&#36807;&#25506;&#32034;&#23616;&#37096;&#34917;&#19969;&#20043;&#38388;&#30340;&#22810;&#23618;&#29305;&#24449;&#20132;&#20114;&#26469;&#22686;&#24378;&#34917;&#20840;&#36136;&#37327;&#12290;&#23545;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;VRCNet&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-scanned point clouds are often incomplete due to viewpoint, occlusion, and noise, which hampers 3D geometric modeling and perception. Existing point cloud completion methods tend to generate global shape skeletons and hence lack fine local details. Furthermore, they mostly learn a deterministic partial-to-complete mapping, but overlook structural relations in man-made objects. To tackle these challenges, this paper proposes a variational framework, Variational Relational point Completion Network (VRCNet) with two appealing properties: 1) Probabilistic Modeling. In particular, we propose a dual-path architecture to enable principled probabilistic modeling across partial and complete clouds. One path consumes complete point clouds for reconstruction by learning a point VAE. The other path generates complete shapes for partial point clouds, whose embedded distribution is guided by distribution obtained from the reconstruction path during training. 2) Relational Enhancement. Specifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#30028;&#38480;&#65292;&#29992;&#20110;&#34987;&#21160;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#20805;&#24403;&#38543;&#26426;&#37319;&#26679;&#22120;&#65292;&#24674;&#22797;&#29992;&#22806;&#37096;&#36807;&#31243;&#20248;&#21270;&#32780;&#26469;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.09123</link><description>&lt;p&gt;
&#20351;&#29992;&#34987;&#21160; Langevin &#21160;&#21147;&#23398;&#30340;&#33258;&#36866;&#24212;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#38480;&#26679;&#26412;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Finite-Sample Bounds for Adaptive Inverse Reinforcement Learning using Passive Langevin Dynamics. (arXiv:2304.09123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#30028;&#38480;&#65292;&#29992;&#20110;&#34987;&#21160;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#20805;&#24403;&#38543;&#26426;&#37319;&#26679;&#22120;&#65292;&#24674;&#22797;&#29992;&#22806;&#37096;&#36807;&#31243;&#20248;&#21270;&#32780;&#26469;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398; (SGLD) &#26159;&#20174;&#27010;&#29575;&#20998;&#24067;&#37319;&#26679;&#30340;&#26377;&#29992;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#34987;&#21160;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#31639;&#27861; (PSGLD) &#30340;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;&#65292;&#26088;&#22312;&#23454;&#29616;&#36870;&#24378;&#21270;&#23398;&#20064;&#12290;&#27492;&#22788;&#30340;&#8220;&#34987;&#21160;&#8221;&#26159;&#25351; PSGLD &#31639;&#27861;(&#36870;&#23398;&#20064;&#36807;&#31243;)&#21487;&#29992;&#30340;&#22122;&#22768;&#28176;&#21464;&#26159;&#30001;&#22806;&#37096;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;(&#27491;&#21521;&#23398;&#20064;&#22120;)&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#28857;&#19978;&#35780;&#20272;&#30340;&#12290;PSGLD &#31639;&#27861;&#22240;&#27492;&#20805;&#24403;&#19968;&#20010;&#38543;&#26426;&#37319;&#26679;&#22120;&#65292;&#21487;&#24674;&#22797;&#27491;&#22312;&#34987;&#27492;&#22806;&#37096;&#36807;&#31243;&#20248;&#21270;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#38543;&#26426;&#36924;&#36817;&#25216;&#26415;&#20998;&#26512;&#20102;&#36825;&#20010;&#34987;&#21160;&#31639;&#27861;&#30340;&#28176;&#36817;&#24615;&#33021;&#65307;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23427;&#30340;&#26377;&#38480;&#26102;&#38388;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#34987;&#21160;&#31639;&#27861;&#21644;&#20854;&#31283;&#23450;&#27979;&#24230;&#20043;&#38388;&#30340; 2-Wasserstein &#36317;&#31163;&#19978;&#30340;&#26377;&#38480;&#26102;&#38388;&#30028;&#38480;&#65292;&#20174;&#20013;&#21487;&#20197;&#33719;&#24471;&#37325;&#24314;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient Langevin dynamics (SGLD) are a useful methodology for sampling from probability distributions. This paper provides a finite sample analysis of a passive stochastic gradient Langevin dynamics algorithm (PSGLD) designed to achieve inverse reinforcement learning. By "passive", we mean that the noisy gradients available to the PSGLD algorithm (inverse learning process) are evaluated at randomly chosen points by an external stochastic gradient algorithm (forward learner). The PSGLD algorithm thus acts as a randomized sampler which recovers the cost function being optimized by this external process. Previous work has analyzed the asymptotic performance of this passive algorithm using stochastic approximation techniques; in this work we analyze the non-asymptotic performance. Specifically, we provide finite-time bounds on the 2-Wasserstein distance between the passive algorithm and its stationary measure, from which the reconstructed cost function is obtained.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#30005;&#21830;&#24191;&#21578;&#31995;&#32479;&#20013;&#36190;&#21161;&#20135;&#21697;&#20248;&#21270;&#30340;&#22810;&#20010;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#25913;&#21464;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#26500;&#12290;&#20351;&#29992;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#38382;&#39064;&#65292;&#24182;&#20026;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#24102;&#26469;&#22686;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.09107</link><description>&lt;p&gt;
&#30005;&#21830;&#20013;&#20248;&#21270;&#36190;&#21161;&#20135;&#21697;&#30340;&#23454;&#36341;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
Practical Lessons on Optimizing Sponsored Products in eCommerce. (arXiv:2304.09107v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#30005;&#21830;&#24191;&#21578;&#31995;&#32479;&#20013;&#36190;&#21161;&#20135;&#21697;&#20248;&#21270;&#30340;&#22810;&#20010;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#25913;&#21464;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#26500;&#12290;&#20351;&#29992;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#38382;&#39064;&#65292;&#24182;&#20026;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#24102;&#26469;&#22686;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36190;&#21161;&#20135;&#21697;&#20248;&#21270;&#20013;&#30340;&#22810;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#22522;&#20110;&#20301;&#32622;&#30340;&#21435;&#20559;&#24046;&#12289;&#28857;&#20987;-&#36716;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#20197;&#21450;&#39044;&#27979;&#28857;&#20987;&#29575;&#30340;&#26657;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#25913;&#21464;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#65288;&#21253;&#25324;&#27973;&#23618;&#27169;&#22411;&#65292;&#22914;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#25968;&#25454;&#21644;&#29305;&#24449;&#24037;&#31243;&#25216;&#26415;&#65292;&#20197;&#22788;&#29702;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#19978;&#36848;&#38382;&#39064;; &#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#23454;&#29992;&#26694;&#26550;&#22312;&#26469;&#33258;&#22312;&#32447;&#36141;&#29289;&#32593;&#31449;&#27969;&#37327;&#26085;&#24535;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#30410;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#30340;&#23454;&#29992;&#26694;&#26550;&#19982;&#25968;&#25454;&#21644;&#29305;&#24449;&#24037;&#31243;&#20063;&#21487;&#20197;&#22788;&#29702;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#38382;&#39064;&#65292;&#24182;&#20026;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#24102;&#26469;&#22686;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study multiple problems from sponsored product optimization in ad system, including position-based de-biasing, click-conversion multi-task learning, and calibration on predicted click-through-rate (pCTR). We propose a practical machine learning framework that provides the solutions to such problems without structural change to existing machine learning models, thus can be combined with most machine learning models including shallow models (e.g. gradient boosting decision trees, support vector machines). In this paper, we first propose data and feature engineering techniques to handle the aforementioned problems in ad system; after that, we evaluate the benefit of our practical framework on real-world data sets from our traffic logs from online shopping site. We show that our proposed practical framework with data and feature engineering can also handle the perennial problems in ad systems and bring increments to multiple evaluation metrics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#23558;&#23458;&#25143;360&#24230;&#35270;&#35282;&#30340;&#19981;&#21516;&#34892;&#20026;&#25110;&#29305;&#24449;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#26500;&#24314;&#20986;&#31867;&#20284;&#27169;&#22411;&#20197;&#25913;&#21892;&#23458;&#25143;&#23450;&#20301;&#30340;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30005;&#21830;&#21644;&#26053;&#28216;&#39046;&#22495;&#20013;&#33021;&#22815;&#26377;&#25928;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.09105</link><description>&lt;p&gt;
&#25506;&#31350;&#23458;&#25143;360&#24230;&#35270;&#35282;&#23454;&#29616;&#31867;&#20284;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Exploring 360-Degree View of Customers for Lookalike Modeling. (arXiv:2304.09105v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#23558;&#23458;&#25143;360&#24230;&#35270;&#35282;&#30340;&#19981;&#21516;&#34892;&#20026;&#25110;&#29305;&#24449;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#26500;&#24314;&#20986;&#31867;&#20284;&#27169;&#22411;&#20197;&#25913;&#21892;&#23458;&#25143;&#23450;&#20301;&#30340;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30005;&#21830;&#21644;&#26053;&#28216;&#39046;&#22495;&#20013;&#33021;&#22815;&#26377;&#25928;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#24314;&#27169;&#26159;&#22522;&#20110;&#29992;&#25143;&#30456;&#20284;&#24615;&#23545;&#20135;&#21697;&#38144;&#21806;&#21644;&#25193;&#23637;&#24191;&#21578;&#27963;&#21160;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#30340;&#20551;&#35774;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#25361;&#25112;&#22312;&#20110;&#29992;&#25143;&#32676;&#20307;&#30340;&#24322;&#36136;&#24615;&#21644;&#20854;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#23458;&#25143;&#30340;&#19981;&#21516;&#34892;&#20026;&#25110;&#29305;&#24449;&#65288;&#22914;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#22312;&#19981;&#21516;&#24179;&#21488;&#19978;&#30340;&#36141;&#20080;&#34892;&#20026;&#12289;&#23458;&#25143;&#24544;&#35802;&#34892;&#20026;&#65289;&#32479;&#19968;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#31867;&#20284;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#20048;&#22825;&#38598;&#22242;&#30340;&#23458;&#25143;&#23450;&#20301;&#12290;&#30495;&#23454;&#30340;&#30005;&#23376;&#21830;&#21153;&#21644;&#26053;&#28216;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31867;&#20284;&#27169;&#22411;&#23545;&#20110;&#29992;&#25143;&#23450;&#20301;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lookalike models are based on the assumption that user similarity plays an important role towards product selling and enhancing the existing advertising campaigns from a very large user base. Challenges associated to these models reside on the heterogeneity of the user base and its sparsity. In this work, we propose a novel framework that unifies the customers different behaviors or features such as demographics, buying behaviors on different platforms, customer loyalty behaviors and build a lookalike model to improve customer targeting for Rakuten Group, Inc. Extensive experiments on real e-commerce and travel datasets demonstrate the effectiveness of our proposed lookalike model for user targeting task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19982;STM32&#24494;&#25511;&#21046;&#22120;&#32467;&#21512;&#36827;&#34892;&#23454;&#26102;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21518;&#30340;&#27169;&#22411;&#35786;&#26029;&#20934;&#30830;&#29575;&#21487;&#36798;98.9%&#12290;&#21516;&#26102;&#65292;&#36824;&#25104;&#21151;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;STM32H743VI&#24494;&#25511;&#21046;&#22120;&#65292;&#20351;&#24471;&#27599;&#27425;&#35786;&#26029;&#30340;&#36816;&#34892;&#26102;&#38388;&#32553;&#30701;&#33267;19ms&#65292;&#20026;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#25216;&#26415;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2304.09100</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;STM32&#24494;&#25511;&#21046;&#22120;&#30340;&#23454;&#26102;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Real Time Bearing Fault Diagnosis Based on Convolutional Neural Network and STM32 Microcontroller. (arXiv:2304.09100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19982;STM32&#24494;&#25511;&#21046;&#22120;&#32467;&#21512;&#36827;&#34892;&#23454;&#26102;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21518;&#30340;&#27169;&#22411;&#35786;&#26029;&#20934;&#30830;&#29575;&#21487;&#36798;98.9%&#12290;&#21516;&#26102;&#65292;&#36824;&#25104;&#21151;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;STM32H743VI&#24494;&#25511;&#21046;&#22120;&#65292;&#20351;&#24471;&#27599;&#27425;&#35786;&#26029;&#30340;&#36816;&#34892;&#26102;&#38388;&#32553;&#30701;&#33267;19ms&#65292;&#20026;&#36724;&#25215;&#25925;&#38556;&#35786;&#26029;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#25216;&#26415;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#25968;&#25454;&#21644;&#36793;&#32536;&#35745;&#31639;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#35768;&#22810;&#30740;&#31350;&#32773;&#33268;&#21147;&#20110;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#36724;&#25215;&#25925;&#38556;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#20687;STM32&#36825;&#26679;&#30340;&#26377;&#38480;&#36164;&#28304;&#24179;&#21488;&#19978;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#12290;&#26412;&#25991;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#23545;&#36724;&#25215;&#25925;&#38556;&#25391;&#21160;&#20449;&#21495;&#30340;&#35782;&#21035;&#65292;&#20248;&#21270;&#21518;&#30340;&#27169;&#22411;&#35786;&#26029;&#20934;&#30830;&#29575;&#21487;&#20197;&#36798;&#21040;98.9%&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25104;&#21151;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24212;&#29992;&#20110;STM32H743VI&#24494;&#25511;&#21046;&#22120;&#65292;&#27599;&#27425;&#35786;&#26029;&#30340;&#36816;&#34892;&#26102;&#38388;&#20026;19ms&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#20027;&#26426;&#19982;STM32&#20043;&#38388;&#30340;&#23454;&#26102;&#36890;&#20449;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20018;&#21475;&#23436;&#32654;&#23436;&#25104;&#25968;&#25454;&#20256;&#36755;&#65292;&#24182;&#22312;TFT-LCD&#23631;&#24149;&#19978;&#26174;&#31034;&#35786;&#26029;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of big data and edge computing, many researchers focus on improving the accuracy of bearing fault classification using deep learning models, and implementing the deep learning classification model on limited resource platforms such as STM32. To this end, this paper realizes the identification of bearing fault vibration signal based on convolutional neural network, the fault identification accuracy of the optimised model can reach 98.9%. In addition, this paper successfully applies the convolutional neural network model to STM32H743VI microcontroller, the running time of each diagnosis is 19ms. Finally, a complete real-time communication framework between the host computer and the STM32 is designed, which can perfectly complete the data transmission through the serial port and display the diagnosis results on the TFT-LCD screen.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#21644;&#23454;&#26045;&#20102;&#19968;&#20010;&#21517;&#20026;MATURE-HEALTH&#30340;&#20581;&#24247;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#39044;&#27979;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#24182;&#25512;&#33616;&#33829;&#20859;&#24179;&#34913;&#30340;&#39135;&#29289;&#65292;&#20174;&#32780;&#22686;&#21152;&#26089;&#26399;&#26816;&#27979;&#30142;&#30149;&#30340;&#26426;&#20250;&#24182;&#38450;&#27490;&#20581;&#24247;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.09099</link><description>&lt;p&gt;
MATURE-HEALTH: MAndatory FeaTURE&#36873;&#25321;&#30340;&#20581;&#24247;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MATURE-HEALTH: HEALTH Recommender System for MAndatory FeaTURE choices. (arXiv:2304.09099v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#21644;&#23454;&#26045;&#20102;&#19968;&#20010;&#21517;&#20026;MATURE-HEALTH&#30340;&#20581;&#24247;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#39044;&#27979;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#24182;&#25512;&#33616;&#33829;&#20859;&#24179;&#34913;&#30340;&#39135;&#29289;&#65292;&#20174;&#32780;&#22686;&#21152;&#26089;&#26399;&#26816;&#27979;&#30142;&#30149;&#30340;&#26426;&#20250;&#24182;&#38450;&#27490;&#20581;&#24247;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#34913;&#30005;&#35299;&#36136;&#23545;&#20110;&#20154;&#20307;&#22120;&#23448;&#30340;&#36866;&#24403;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#21644;&#24517;&#19981;&#21487;&#23569;&#65292;&#22240;&#20026;&#30005;&#35299;&#36136;&#22833;&#34913;&#21487;&#33021;&#26159;&#28508;&#22312;&#30149;&#29702;&#29983;&#29702;&#23398;&#21457;&#23637;&#30340;&#25351;&#31034;&#12290;&#39640;&#25928;&#30417;&#27979;&#30005;&#35299;&#36136;&#22833;&#34913;&#19981;&#20165;&#21487;&#20197;&#22686;&#21152;&#30142;&#30149;&#26089;&#26399;&#26816;&#27979;&#30340;&#26426;&#20250;&#65292;&#32780;&#19988;&#21487;&#20197;&#36890;&#36807;&#20005;&#26684;&#36981;&#24490;&#33829;&#20859;&#25511;&#21046;&#39278;&#39135;&#20197;&#24179;&#34913;&#30005;&#35299;&#36136;&#20174;&#32780;&#38450;&#27490;&#20581;&#24247;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#25512;&#33616;&#31995;&#32479;MATURE Health&#65292;&#35813;&#31995;&#32479;&#39044;&#27979;&#34880;&#28082;&#20013;&#24517;&#38656;&#30005;&#35299;&#36136;&#21644;&#20854;&#20182;&#29289;&#36136;&#30340;&#19981;&#24179;&#34913;&#65292;&#28982;&#21518;&#25512;&#33616;&#21547;&#26377;&#24179;&#34913;&#33829;&#20859;&#30340;&#39135;&#29289;&#65292;&#20197;&#36991;&#20813;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#30340;&#21457;&#29983;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#21040;&#29992;&#25143;&#26368;&#36817;&#30340;&#23454;&#39564;&#23460;&#32467;&#26524;&#21644;&#27599;&#26085;&#39135;&#29289;&#25668;&#20837;&#37327;&#26469;&#39044;&#27979;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#12290;MATURE Health&#20381;&#36182;&#20110;MATURE Food&#31639;&#27861;&#25512;&#33616;&#39135;&#29289;&#65292;&#21518;&#32773;&#20165;&#25512;&#33616;&#37027;&#20123;
&lt;/p&gt;
&lt;p&gt;
Balancing electrolytes is utmost important and essential for appropriate functioning of organs in human body as electrolytes imbalance can be an indication of the development of underlying pathophysiology. Efficient monitoring of electrolytes imbalance not only can increase the chances of early detection of disease, but also prevents the further deterioration of the health by strictly following nutrient controlled diet for balancing the electrolytes post disease detection. In this research, a recommender system MATURE Health is proposed and implemented, which predicts the imbalance of mandatory electrolytes and other substances presented in blood and recommends the food items with the balanced nutrients to avoid occurrence of the electrolytes imbalance. The proposed model takes user most recent laboratory results and daily food intake into account to predict the electrolytes imbalance. MATURE Health relies on MATURE Food algorithm to recommend food items as latter recommends only those
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;Sheaf&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#26041;&#27861;&#65292;&#20351;&#24471;&#20854;&#22312;&#22522;&#20934;&#25512;&#33616;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.09097</link><description>&lt;p&gt;
&#22522;&#20110;Sheaf&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Sheaf Neural Networks for Graph-based Recommender Systems. (arXiv:2304.09097v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09097
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Sheaf&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#26041;&#27861;&#65292;&#20351;&#24471;&#20854;&#22312;&#22522;&#20934;&#25512;&#33616;&#20219;&#21153;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Graph&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12290;Graph&#31070;&#32463;&#32593;&#32476;&#23545;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#22312;&#20110;&#65292;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#21487;&#20197;&#33258;&#28982;&#22320;&#24314;&#27169;&#20026;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21487;&#20197;&#26159;&#29992;&#25143;&#25110;&#39033;&#30446;&#65292;&#36793;&#20195;&#34920;&#20559;&#22909;&#20851;&#31995;&#12290; &#22312;&#24403;&#21069;&#30340;Graph&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#20013;&#65292;&#33410;&#28857;&#29992;&#22312;&#35757;&#32451;&#26102;&#23398;&#20064;&#21040;&#30340;&#38745;&#24577;&#21521;&#37327;&#34920;&#31034;&#12290;&#36825;&#31181;&#38745;&#24577;&#21521;&#37327;&#21487;&#33021;&#21482;&#36866;&#29992;&#20110;&#25429;&#25417;&#23450;&#20041;&#23427;&#20204;&#30340;&#19968;&#20123;&#29992;&#25143;&#25110;&#39033;&#30446;&#30340;&#24494;&#22937;&#24046;&#21035;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#21551;&#21457;&#33539;&#30068;&#35770;&#30340;&#27169;&#22411;&#65306;Sheaf&#31070;&#32463;&#32593;&#32476;&#12290;Sheaf&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#36830;&#25509;&#30340;&#25289;&#26222;&#25289;&#26031;&#21487;&#20197;&#36890;&#36807;&#23558;&#27599;&#20010;&#33410;&#28857;&#65288;&#20197;&#21450;&#36793;&#65289;&#19982;&#21521;&#37327;&#31354;&#38388;&#32780;&#19981;&#26159;&#21333;&#20010;&#21521;&#37327;&#30456;&#20851;&#32852;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#26356;&#20016;&#23500;&#65292;&#24182;&#20801;&#35768;&#22312;&#25512;&#29702;&#26102;&#36873;&#25321;&#27491;&#30830;&#30340;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#26356;&#20855;&#34920;&#29616;&#21147;&#21644;&#28789;&#27963;&#24615;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25512;&#33616;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in Graph Neural Networks has resulted in wide adoption by many applications, including recommendation systems. The reason for Graph Neural Networks' superiority over other approaches is that many problems in recommendation systems can be naturally modeled as graphs, where nodes can be either users or items and edges represent preference relationships. In current Graph Neural Network approaches, nodes are represented with a static vector learned at training time. This static vector might only be suitable to capture some of the nuances of users or items they define. To overcome this limitation, we propose using a recently proposed model inspired by category theory: Sheaf Neural Networks. Sheaf Neural Networks, and its connected Laplacian, can address the previous problem by associating every node (and edge) with a vector space instead than a single vector. The vector space representation is richer and allows picking the proper representation at inference time. This approa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#21644;&#30697;&#38453;&#20998;&#35299;&#30340;&#20445;&#25252;&#38544;&#31169;&#25512;&#33616;&#31995;&#32479;&#65292;&#37319;&#29992;&#36755;&#20986;&#25200;&#21160;&#30340;&#39640;&#26031;&#26426;&#21046;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#65292;&#36890;&#36807;R&#233;nyi&#24046;&#20998;&#38544;&#31169;&#23545;&#25972;&#20307;&#38544;&#31169;&#25439;&#22833;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#25512;&#33616;&#31995;&#32479;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09096</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#26426;&#21046;&#30340;&#20445;&#25252;&#38544;&#31169;&#30697;&#38453;&#20998;&#35299;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Matrix Factorization for Recommendation Systems using Gaussian Mechanism. (arXiv:2304.09096v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#21644;&#30697;&#38453;&#20998;&#35299;&#30340;&#20445;&#25252;&#38544;&#31169;&#25512;&#33616;&#31995;&#32479;&#65292;&#37319;&#29992;&#36755;&#20986;&#25200;&#21160;&#30340;&#39640;&#26031;&#26426;&#21046;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#65292;&#36890;&#36807;R&#233;nyi&#24046;&#20998;&#38544;&#31169;&#23545;&#25972;&#20307;&#38544;&#31169;&#25439;&#22833;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#25512;&#33616;&#31995;&#32479;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#25512;&#33616;&#31995;&#32479;&#38656;&#35201;&#20998;&#26512;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#20250;&#27844;&#38706;&#29992;&#25143;&#30340;&#20010;&#20154;&#20449;&#24687;&#12290;&#21311;&#21517;&#21270;&#29992;&#25143;&#25968;&#25454;&#36890;&#24120;&#19981;&#36275;&#20197;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#37492;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#21644;&#30697;&#38453;&#20998;&#35299;&#30340;&#20445;&#25252;&#38544;&#31169;&#25512;&#33616;&#31995;&#32479;&#65292;&#30697;&#38453;&#20998;&#35299;&#26159;&#26368;&#27969;&#34892;&#30340;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#20043;&#19968;&#12290;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#65292;&#21363;&#20351;&#23545;&#25163;&#25317;&#26377;&#29992;&#25143;&#30340;&#20844;&#24320;&#20449;&#24687;&#65292;&#20063;&#21487;&#20197;&#38450;&#27490;&#23545;&#25163;&#25552;&#21462;&#25935;&#24863;&#29992;&#25143;&#20449;&#24687;&#12290;&#25105;&#20204;&#37319;&#29992;&#36755;&#20986;&#25200;&#21160;&#30340;&#39640;&#26031;&#26426;&#21046;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#24182;&#21457;&#24067;&#28385;&#36275;&#38544;&#31169;&#23450;&#20041;&#30340;&#29992;&#25143;&#26723;&#26696;&#12290;&#25105;&#20204;&#20351;&#29992;R&#233;nyi&#24046;&#20998;&#38544;&#31169;&#23545;&#25972;&#20307;&#38544;&#31169;&#25439;&#22833;&#36827;&#34892;&#20102;&#32039;&#23494;&#30340;&#29305;&#24449;&#21270;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building a recommendation system involves analyzing user data, which can potentially leak sensitive information about users. Anonymizing user data is often not sufficient for preserving user privacy. Motivated by this, we propose a privacy-preserving recommendation system based on the differential privacy framework and matrix factorization, which is one of the most popular algorithms for recommendation systems. As differential privacy is a powerful and robust mathematical framework for designing privacy-preserving machine learning algorithms, it is possible to prevent adversaries from extracting sensitive user information even if the adversary possesses their publicly available (auxiliary) information. We implement differential privacy via the Gaussian mechanism in the form of output perturbation and release user profiles that satisfy privacy definitions. We employ R\'enyi Differential Privacy for a tight characterization of the overall privacy loss. We perform extensive experiments on
&lt;/p&gt;</description></item><item><title>KLEVER&#26159;&#19968;&#20010;&#26032;&#30340;CRS&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#29289;&#21697;&#21644;&#23427;&#20204;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#32852;&#21512;&#24314;&#27169;&#22312;&#21516;&#19968;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#35299;&#20915;&#20102;&#20197;&#21069;&#24037;&#20316;&#20013;&#30340;&#29289;&#21697;&#21644;&#21333;&#35789;&#35821;&#20041;&#31354;&#38388;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.09093</link><description>&lt;p&gt;
&#22522;&#20110;&#25551;&#36848;&#24615;&#22270;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29289;&#21697;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Items and Contexts Understanding with Descriptive Graph for Conversational Recommendation. (arXiv:2304.09093v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09093
&lt;/p&gt;
&lt;p&gt;
KLEVER&#26159;&#19968;&#20010;&#26032;&#30340;CRS&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#29289;&#21697;&#21644;&#23427;&#20204;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#32852;&#21512;&#24314;&#27169;&#22312;&#21516;&#19968;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#35299;&#20915;&#20102;&#20197;&#21069;&#24037;&#20316;&#20013;&#30340;&#29289;&#21697;&#21644;&#21333;&#35789;&#35821;&#20041;&#31354;&#38388;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65288;CRS&#65289;&#22312;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#25552;&#39640;&#29289;&#21697;&#21644;&#19978;&#19979;&#25991;&#21333;&#35789;&#34920;&#31034;&#26041;&#38754;&#22788;&#20110;&#26368;&#21069;&#27839;&#27700;&#24179;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#21644;&#21709;&#24212;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#29289;&#21697;&#21644;&#21333;&#35789;&#30340;&#34920;&#31034;&#36890;&#24120;&#22312;&#20004;&#20010;&#29420;&#31435;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#24314;&#27169;&#65292;&#36825;&#20250;&#23548;&#33268;&#23427;&#20204;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24403;&#29992;&#25143;&#36755;&#20837;&#20449;&#24687;&#19981;&#36275;&#26102;&#65292;&#36825;&#23558;&#23548;&#33268;CRS&#20165;&#23454;&#29616;&#27425;&#20248;&#25490;&#21517;&#34920;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;CRS&#26694;&#26550;KLEVER&#65292;&#23427;&#21487;&#20197;&#32852;&#21512;&#24314;&#27169;&#22312;&#30456;&#21516;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#29289;&#21697;&#21644;&#23427;&#20204;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20174;&#20016;&#23500;&#30340;&#29289;&#21697;&#25991;&#26412;&#29305;&#24449;&#65288;&#22914;&#29289;&#21697;&#25551;&#36848;&#21644;&#31867;&#21035;&#65289;&#20013;&#26500;&#24314;&#19968;&#20010;&#29289;&#21697;&#25551;&#36848;&#24615;&#22270;&#12290;&#22522;&#20110;&#26500;&#24314;&#30340;&#25551;&#36848;&#24615;&#22270;&#65292;KLEVER&#20849;&#21516;&#23398;&#20064;&#21333;&#35789;&#21644;&#29289;&#21697;&#30340;&#23884;&#20837;&#65292;&#20197;&#22686;&#24378;&#25512;&#33616;&#21644;&#23545;&#35805;&#29983;&#25104;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art methods on conversational recommender systems (CRS) leverage external knowledge to enhance both items' and contextual words' representations to achieve high quality recommendations and responses generation. However, the representations of the items and words are usually modeled in two separated semantic spaces, which leads to misalignment issue between them. Consequently, this will cause the CRS to only achieve a sub-optimal ranking performance, especially when there is a lack of sufficient information from the user's input. To address limitations of previous works, we propose a new CRS framework KLEVER, which jointly models items and their associated contextual words in the same semantic space. Particularly, we construct an item descriptive graph from the rich items' textual features, such as item description and categories. Based on the constructed descriptive graph, KLEVER jointly learns the embeddings of the words and items, towards enhancing both recommender and d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#28459;&#30011;&#25512;&#33616;MAB&#35774;&#32622;&#65292;&#19982;&#20247;&#21253;&#24037;&#20316;&#32773;&#23545;&#20851;&#38190;MAB&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#30740;&#31350;&#65292;&#32467;&#26524;&#25361;&#25112;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#36827;&#34892;&#26356;&#22810;&#20851;&#20110;MAB&#25512;&#33616;&#20013;&#36825;&#20123;&#20551;&#35774;&#30340;&#23454;&#35777;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2304.09088</link><description>&lt;p&gt;
&#25512;&#33616;&#20013;&#30340;Bandit&#31639;&#27861;&#30340;&#29616;&#22330;&#27979;&#35797;&#65306;&#20102;&#35299;MAB&#20013;&#20851;&#20110;&#20154;&#31867;&#20559;&#22909;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Field Test of Bandit Algorithms for Recommendations: Understanding the Validity of Assumptions on Human Preferences in Multi-armed Bandits. (arXiv:2304.09088v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#28459;&#30011;&#25512;&#33616;MAB&#35774;&#32622;&#65292;&#19982;&#20247;&#21253;&#24037;&#20316;&#32773;&#23545;&#20851;&#38190;MAB&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#30740;&#31350;&#65292;&#32467;&#26524;&#25361;&#25112;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#36827;&#34892;&#26356;&#22810;&#20851;&#20110;MAB&#25512;&#33616;&#20013;&#36825;&#20123;&#20551;&#35774;&#30340;&#23454;&#35777;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#28183;&#36879;&#21040;&#29616;&#20195;&#29983;&#27963;&#20013;&#65292;&#22609;&#36896;&#20102;&#25105;&#20204;&#35835;&#20160;&#20040;&#23186;&#20307;&#21644;&#28040;&#36153;&#20160;&#20040;&#20135;&#21697;&#12290;&#39537;&#21160;&#27492;&#31867;&#31995;&#32479;&#30340;&#31639;&#27861;&#24448;&#24448;&#21253;&#25324;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20363;&#22914;&#20855;&#26377;&#22810;&#31181;&#21551;&#21457;&#24335;&#36873;&#25321;&#30340;&#28508;&#22312;&#22240;&#32032;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#26377;&#20851;&#25512;&#33616;&#30340;&#29702;&#35770;&#22788;&#29702;&#36890;&#24120;&#28041;&#21450;&#38382;&#39064;&#30340;&#20915;&#31574;&#29702;&#35770;&#24615;&#36136;&#65292;&#21253;&#25324;&#36890;&#36807;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#26694;&#26550;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#38656;&#35201;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;MAB&#30340;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#20551;&#35774;&#12290;&#36825;&#20123;&#20559;&#22909;&#20551;&#35774;&#24456;&#23569;&#20351;&#29992;&#20154;&#21592;&#30740;&#31350;&#36827;&#34892;&#27979;&#35797;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#24037;&#20855;&#21253;&#26469;&#36827;&#34892;&#36825;&#20123;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#28459;&#30011;&#25512;&#33616;MAB&#35774;&#32622;&#20013;&#19982;&#20247;&#21253;&#24037;&#20316;&#32773;&#36827;&#34892;&#30740;&#31350;&#12290;&#27599;&#20010;&#26426;&#22120;&#33218;&#34920;&#31034;&#19968;&#20010;&#28459;&#30011;&#31867;&#21035;&#65292;&#29992;&#25143;&#22312;&#27599;&#27425;&#25512;&#33616;&#21518;&#25552;&#20379;&#21453;&#39304;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#20851;&#38190;MAB&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20154;&#31867;&#20559;&#22909;&#26159;&#26102;&#31354;&#20855;&#26377;&#38745;&#24577;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#24314;&#27169;&#20026;&#28508;&#22312;&#25910;&#30410;&#20998;&#24067;&#30340;&#22024;&#26434;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25361;&#25112;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#36827;&#34892;&#26356;&#22810;&#20851;&#20110;MAB&#25512;&#33616;&#20013;&#36825;&#20123;&#20551;&#35774;&#30340;&#23454;&#35777;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized recommender systems suffuse modern life, shaping what media we read and what products we consume. Algorithms powering such systems tend to consist of supervised learning-based heuristics, such as latent factor models with a variety of heuristically chosen prediction targets. Meanwhile, theoretical treatments of recommendation frequently address the decision-theoretic nature of the problem, including the need to balance exploration and exploitation, via the multi-armed bandits (MABs) framework. However, MAB-based approaches rely heavily on assumptions about human preferences. These preference assumptions are seldom tested using human subject studies, partly due to the lack of publicly available toolkits to conduct such studies. In this work, we conduct a study with crowdworkers in a comics recommendation MABs setting. Each arm represents a comic category, and users provide feedback after each recommendation. We check the validity of core MABs assumptions-that human preferen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MDDL&#30340;&#22810;&#36890;&#36947;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#25972;&#21512;&#22810;&#31181;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#20301;&#32622;&#20998;&#37197;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09087</link><description>&lt;p&gt;
MDDL: &#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#36890;&#36947;Feed&#20301;&#32622;&#20998;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MDDL: A Framework for Reinforcement Learning-based Position Allocation in Multi-Channel Feed. (arXiv:2304.09087v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MDDL&#30340;&#22810;&#36890;&#36947;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#25972;&#21512;&#22810;&#31181;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#20301;&#32622;&#20998;&#37197;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20301;&#32622;&#20998;&#37197;&#31995;&#32479;&#30340;&#20027;&#27969;&#26041;&#27861;&#26159;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20026;&#21508;&#36890;&#36947;&#30340;&#29289;&#21697;&#20998;&#37197;&#21512;&#36866;&#30340;&#20301;&#32622;&#65292;&#28982;&#21518;&#28151;&#21512;&#21040;Feed&#20013;&#12290;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#20351;&#29992;&#20004;&#31181;&#25968;&#25454;&#65306;&#31574;&#30053;&#25968;&#25454;&#21644;&#38543;&#26426;&#25968;&#25454;&#12290;&#31574;&#30053;&#25968;&#25454;&#26469;&#33258;&#24403;&#21069;&#22312;&#32447;&#27169;&#22411;&#65292;&#23427;&#21463;&#21040;&#29366;&#24577;-&#21160;&#20316;&#23545;&#20998;&#24067;&#19981;&#22343;&#34913;&#30340;&#22256;&#25200;&#65292;&#23548;&#33268;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#39640;&#20272;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38543;&#26426;&#25968;&#25454;&#25552;&#20379;&#20102;&#26356;&#22343;&#21248;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#20998;&#24067;&#65292;&#20294;&#22312;&#24037;&#19994;&#22330;&#26223;&#20013;&#24456;&#38590;&#33719;&#21462;&#65292;&#22240;&#20026;&#38543;&#26426;&#25506;&#32034;&#21487;&#33021;&#20250;&#23545;&#24179;&#21488;&#25910;&#20837;&#21644;&#29992;&#25143;&#20307;&#39564;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#30001;&#20110;&#36825;&#20004;&#31181;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#65292;&#22240;&#27492;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#21033;&#29992;&#20004;&#31181;&#25968;&#25454;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#24050;&#25104;&#20026;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MDDL&#65288;&#22810;&#36890;&#36947;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#25972;&#21512;&#22810;&#31181;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#20301;&#32622;&#20998;&#37197;&#30340;RL&#27169;&#22411;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#32447;&#21644;&#31163;&#32447;&#24615;&#33021;&#26041;&#38754;&#65292;MDDL&#34920;&#29616;&#20248;&#20110;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the mainstream approach in position allocation system is to utilize a reinforcement learning model to allocate appropriate locations for items in various channels and then mix them into the feed. There are two types of data employed to train reinforcement learning (RL) model for position allocation, named strategy data and random data. Strategy data is collected from the current online model, it suffers from an imbalanced distribution of state-action pairs, resulting in severe overestimation problems during training. On the other hand, random data offers a more uniform distribution of state-action pairs, but is challenging to obtain in industrial scenarios as it could negatively impact platform revenue and user experience due to random exploration. As the two types of data have different distributions, designing an effective strategy to leverage both types of data to enhance the efficacy of the RL model training has become a highly challenging problem. In this study, we propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#26080;&#20559;&#35780;&#20998;&#24179;&#34913;&#24050;&#26377;&#30340;&#21435;&#20559;&#26041;&#27861;&#26469;&#23545;&#25239;&#26410;&#35266;&#27979;&#28151;&#28102;&#21644;&#27169;&#22411;&#35268;&#33539;&#35823;&#24046;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09085</link><description>&lt;p&gt;
&#21033;&#29992;&#23569;&#37327;&#26080;&#20559;&#35780;&#20998;&#24179;&#34913;&#26410;&#35266;&#27979;&#28151;&#28102;&#30340;&#21435;&#20559;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Balancing Unobserved Confounding with a Few Unbiased Ratings in Debiased Recommendations. (arXiv:2304.09085v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#26080;&#20559;&#35780;&#20998;&#24179;&#34913;&#24050;&#26377;&#30340;&#21435;&#20559;&#26041;&#27861;&#26469;&#23545;&#25239;&#26410;&#35266;&#27979;&#28151;&#28102;&#21644;&#27169;&#22411;&#35268;&#33539;&#35823;&#24046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#34987;&#35270;&#20026;&#35299;&#20915;&#20449;&#24687;&#36807;&#36733;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#20294;&#20247;&#25152;&#21608;&#30693;&#65292;&#23384;&#22312;&#21508;&#31181;&#20559;&#24046;&#20351;&#24471;&#22312;&#22823;&#35268;&#27169;&#35266;&#27979;&#25968;&#25454;&#19978;&#36827;&#34892;&#30452;&#25509;&#35757;&#32451;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#20174;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;&#25110;A/B&#27979;&#35797;&#20013;&#33719;&#24471;&#30340;&#26080;&#20559;&#35780;&#20998;&#34987;&#35748;&#20026;&#26159;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#22312;&#29616;&#23454;&#20013;&#25104;&#26412;&#39640;&#19988;&#35268;&#27169;&#36739;&#23567;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#26080;&#20559;&#35780;&#20998;&#26469;&#20462;&#27491;&#22312;&#26377;&#20559;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#20542;&#21521;&#24615;&#25110;&#25554;&#34917;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#23384;&#22312;&#26410;&#35266;&#27979;&#28151;&#28102;&#25110;&#27169;&#22411;&#35268;&#33539;&#35823;&#24046;&#26102;&#26080;&#27861;&#33719;&#24471;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#24179;&#34913;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#26088;&#22312;&#23545;&#25239;&#26410;&#35266;&#27979;&#28151;&#28102;&#21644;&#27169;&#22411;&#35268;&#33539;&#35823;&#24046;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;&#26080;&#20559;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are seen as an effective tool to address information overload, but it is widely known that the presence of various biases makes direct training on large-scale observational data result in sub-optimal prediction performance. In contrast, unbiased ratings obtained from randomized controlled trials or A/B tests are considered to be the golden standard, but are costly and small in scale in reality. To exploit both types of data, recent works proposed to use unbiased ratings to correct the parameters of the propensity or imputation models trained on the biased dataset. However, the existing methods fail to obtain accurate predictions in the presence of unobserved confounding or model misspecification. In this paper, we propose a theoretically guaranteed model-agnostic balancing approach that can be applied to any existing debiasing method with the aim of combating unobserved confounding and model misspecification. The proposed approach makes full use of unbiased data by 
&lt;/p&gt;</description></item><item><title>DRIFT&#26159;&#19968;&#20010;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#26550;&#26500;&#65292;&#20351;&#29992;&#38544;&#24335;&#21453;&#39304;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#65292;&#24182;&#20351;&#29992;SAROS&#31639;&#27861;&#23454;&#29616;&#31934;&#20934;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2304.09084</link><description>&lt;p&gt;
DRIFT: &#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#21453;&#39304;&#30340;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DRIFT: A Federated Recommender System with Implicit Feedback on the Items. (arXiv:2304.09084v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09084
&lt;/p&gt;
&lt;p&gt;
DRIFT&#26159;&#19968;&#20010;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#26550;&#26500;&#65292;&#20351;&#29992;&#38544;&#24335;&#21453;&#39304;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#65292;&#24182;&#20351;&#29992;SAROS&#31639;&#27861;&#23454;&#29616;&#31934;&#20934;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#32593;&#32476;&#19978;&#21487;&#29992;&#30340;&#29289;&#21697;&#36234;&#26469;&#36234;&#22810;&#65292;&#36825;&#20351;&#24471;&#29992;&#25143;&#24456;&#38590;&#25214;&#21040;&#20182;&#20204;&#21916;&#27426;&#30340;&#29289;&#21697;&#12290;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#20351;&#29992;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#26469;&#25214;&#21040;&#26368;&#36866;&#21512;&#29992;&#25143;&#30340;&#29289;&#21697;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20132;&#20114;&#21487;&#33021;&#26356;&#25110;&#26356;&#23569;&#25935;&#24863;&#65292;&#24182;&#25910;&#38598;&#23427;&#20204;&#28041;&#21450;&#21040;&#29992;&#25143;&#38544;&#31169;&#30340;&#38382;&#39064;&#12290;&#32852;&#37030;&#31995;&#32479;&#24050;&#32463;&#26174;&#31034;&#20986;&#21363;&#20351;&#22312;&#19981;&#23384;&#20648;&#29992;&#25143;&#20010;&#20154;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#36827;&#34892;&#20934;&#30830;&#21644;&#26377;&#25928;&#30340;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20351;&#29992;&#29992;&#25143;&#30340;&#21363;&#26102;&#21453;&#39304;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DRIFT&#65292;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#21453;&#39304;&#30340;&#32852;&#21512;&#25512;&#33616;&#31995;&#32479;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#23398;&#20064;&#27169;&#22411;&#22522;&#20110;&#26368;&#36817;&#29992;&#20110;&#38544;&#24335;&#21453;&#39304;&#25512;&#33616;&#30340;&#31639;&#27861;SAROS&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#25512;&#33616;&#19982;SAROS&#19968;&#26679;&#31934;&#30830;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#36890;&#36807;&#23454;&#39564;&#21644;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DRIFT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays there are more and more items available online, this makes it hard for users to find items that they like. Recommender systems aim to find the item who best suits the user, using his historical interactions. Depending on the context, these interactions may be more or less sensitive and collecting them brings an important problem concerning the users' privacy. Federated systems have shown that it is possible to make accurate and efficient recommendations without storing users' personal information. However, these systems use instantaneous feedback from the user. In this report, we propose DRIFT, a federated architecture for recommender systems, using implicit feedback. Our learning model is based on a recent algorithm for recommendation with implicit feedbacks SAROS. We aim to make recommendations as precise as SAROS, without compromising the users' privacy. In this report we show that thanks to our experiments, but also thanks to a theoretical analysis on the convergence. We h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27714;&#35299;&#22120;&#37325;&#26032;&#21019;&#24314;&#20102;&#31532;&#19968;&#27425;&#25104;&#21151;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65292;&#24182;&#35777;&#26126;&#29289;&#29702;&#23398;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#36739;&#20110;&#20197;&#21069;&#30340;&#26631;&#20934;&#27714;&#35299;&#22120;&#26356;&#20026;&#20934;&#30830;&#19988;&#31616;&#20415;&#12290;</title><link>http://arxiv.org/abs/2304.09070</link><description>&lt;p&gt;
M-ENIAC: &#31532;&#19968;&#27425;&#25104;&#21151;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#30340;&#26426;&#22120;&#23398;&#20064;&#20877;&#29616;
&lt;/p&gt;
&lt;p&gt;
M-ENIAC: A machine learning recreation of the first successful numerical weather forecasts. (arXiv:2304.09070v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27714;&#35299;&#22120;&#37325;&#26032;&#21019;&#24314;&#20102;&#31532;&#19968;&#27425;&#25104;&#21151;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65292;&#24182;&#35777;&#26126;&#29289;&#29702;&#23398;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#36739;&#20110;&#20197;&#21069;&#30340;&#26631;&#20934;&#27714;&#35299;&#22120;&#26356;&#20026;&#20934;&#30830;&#19988;&#31616;&#20415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
1950&#24180;&#65292;&#37319;&#29992;&#8220;&#30005;&#23376;&#25968;&#23383;&#31215;&#20998;&#22120;&#21644;&#35745;&#31639;&#26426;&#8221;&#65288;ENIAC&#65289;&#35299;&#20915;&#20102;&#28065;&#24230;&#26041;&#31243;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#31532;&#19968;&#27425;&#25104;&#21151;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65292;&#26631;&#24535;&#30528;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#30340;&#26102;&#20195;&#30340;&#24320;&#22987;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#38382;&#39064;&#65306;&#22914;&#26524;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27714;&#35299;&#22120;&#32780;&#19981;&#26159;&#26631;&#20934;&#30340;&#25968;&#20540;&#31163;&#25955;&#21270;&#65292;&#36825;&#20123;&#25968;&#20540;&#39044;&#25253;&#30340;&#32467;&#26524;&#20250;&#26159;&#20160;&#20040;&#65311;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#23398;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#37325;&#26032;&#21019;&#24314;&#36825;&#20123;&#25968;&#20540;&#39044;&#25253;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19982;ENIAC&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;&#29289;&#29702;&#23398;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#31616;&#20415;&#12289;&#26356;&#20934;&#30830;&#30340;&#22312;&#29699;&#19978;&#27714;&#35299;&#27668;&#35937;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 1950 the first successful numerical weather forecast was obtained by solving the barotropic vorticity equation using the Electronic Numerical Integrator and Computer (ENIAC), which marked the beginning of the age of numerical weather prediction. Here, we ask the question of how these numerical forecasts would have turned out, if machine learning based solvers had been used instead of standard numerical discretizations. Specifically, we recreate these numerical forecasts using physics-informed neural networks. We show that physics-informed neural networks provide an easier and more accurate methodology for solving meteorological equations on the sphere, as compared to the ENIAC solver.
&lt;/p&gt;</description></item><item><title>METAM&#26159;&#19968;&#31181;&#30446;&#26631;&#23548;&#21521;&#30340;&#25968;&#25454;&#21457;&#29616;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#24341;&#23548;&#21457;&#29616;&#21644;&#22686;&#24378;&#36807;&#31243;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09068</link><description>&lt;p&gt;
METAM: &#30446;&#26631;&#23548;&#21521;&#30340;&#25968;&#25454;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
METAM: Goal-Oriented Data Discovery. (arXiv:2304.09068v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09068
&lt;/p&gt;
&lt;p&gt;
METAM&#26159;&#19968;&#31181;&#30446;&#26631;&#23548;&#21521;&#30340;&#25968;&#25454;&#21457;&#29616;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#24341;&#23548;&#21457;&#29616;&#21644;&#22686;&#24378;&#36807;&#31243;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#26469;&#33258;&#24320;&#25918;&#25968;&#25454;&#23384;&#20648;&#24211;&#12289;&#25968;&#25454;&#28246;&#21644;&#25968;&#25454;&#24066;&#22330;&#31561;&#26469;&#28304;&#30340;&#22823;&#37327;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#25552;&#20379;&#20102;&#22686;&#24378;&#25968;&#25454;&#21644;&#25552;&#39640;&#36825;&#20123;&#20219;&#21153;&#24615;&#33021;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#22686;&#24378;&#25216;&#26415;&#20381;&#36182;&#20110;&#29992;&#25143;&#25163;&#21160;&#21457;&#29616;&#21644;&#31579;&#36873;&#26377;&#29992;&#30340;&#20505;&#36873;&#22686;&#24378;&#39033;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#26410;&#21033;&#29992;&#21457;&#29616;&#21644;&#22686;&#24378;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#22240;&#27492;&#26410;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;METAM&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30446;&#26631;&#23548;&#21521;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21521;&#19979;&#28216;&#20219;&#21153;&#26597;&#35810;&#20505;&#36873;&#25968;&#25454;&#38598;&#65292;&#24418;&#25104;&#19968;&#20010;&#21453;&#39304;&#24490;&#29615;&#65292;&#33258;&#21160;&#24341;&#23548;&#21457;&#29616;&#21644;&#22686;&#24378;&#36807;&#31243;&#12290;&#20026;&#20102;&#39640;&#25928;&#22320;&#36873;&#25321;&#20505;&#36873;&#39033;&#65292;METAM&#21033;&#29992;&#25968;&#25454;&#23646;&#24615;&#12289;&#25928;&#29992;&#20989;&#25968;&#21644;&#35299;&#38598;&#22823;&#23567;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;METAM&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#28436;&#31034;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30446;&#26631;&#23548;&#21521;&#25968;&#25454;&#21457;&#29616;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data is a central component of machine learning and causal inference tasks. The availability of large amounts of data from sources such as open data repositories, data lakes and data marketplaces creates an opportunity to augment data and boost those tasks' performance. However, augmentation techniques rely on a user manually discovering and shortlisting useful candidate augmentations. Existing solutions do not leverage the synergy between discovery and augmentation, thus under exploiting data.  In this paper, we introduce METAM, a novel goal-oriented framework that queries the downstream task with a candidate dataset, forming a feedback loop that automatically steers the discovery and augmentation process. To select candidates efficiently, METAM leverages properties of the: i) data, ii) utility function, and iii) solution set size. We show METAM's theoretical guarantees and demonstrate those empirically on a broad set of tasks. All in all, we demonstrate the promise of goal-oriented d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;CTR&#39044;&#27979;&#30340;&#28418;&#31227;&#24863;&#30693;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#38598;&#25104;&#23398;&#20064;&#65292;&#36890;&#36807;&#26174;&#24335;&#30340;&#22522;&#20110;&#35823;&#24046;&#30340;&#28418;&#31227;&#26816;&#27979;&#26469;&#35299;&#20915;CTR&#39044;&#27979;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.09062</link><description>&lt;p&gt;
&#24635;&#26159;&#22686;&#24378;&#20320;&#30340;&#20248;&#21183;: &#19968;&#31181;&#38754;&#21521;CTR&#39044;&#27979;&#30340;&#28418;&#31227;&#24863;&#30693;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Always Strengthen Your Strengths: A Drift-Aware Incremental Learning Framework for CTR Prediction. (arXiv:2304.09062v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;CTR&#39044;&#27979;&#30340;&#28418;&#31227;&#24863;&#30693;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#38598;&#25104;&#23398;&#20064;&#65292;&#36890;&#36807;&#26174;&#24335;&#30340;&#22522;&#20110;&#35823;&#24046;&#30340;&#28418;&#31227;&#26816;&#27979;&#26469;&#35299;&#20915;CTR&#39044;&#27979;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575; (CTR) &#39044;&#27979;&#22312;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#24179;&#21488;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#24037;&#19994;&#22330;&#26223;&#20013;&#20351;&#29992;&#26102;&#65292;CTR &#27169;&#22411;&#35266;&#23519;&#21040;&#30340;&#29992;&#25143;&#29983;&#25104;&#25968;&#25454;&#36890;&#24120;&#20197;&#27969;&#30340;&#24418;&#24335;&#21040;&#36798;&#12290;&#27969;&#24335;&#25968;&#25454;&#20855;&#26377;&#38543;&#30528;&#26102;&#38388;&#32780;&#28418;&#31227;&#24182;&#21487;&#33021;&#37325;&#29616;&#30340;&#29305;&#24615;&#12290;&#22914;&#26524;&#27169;&#22411;&#20165;&#36866;&#24212;&#20110;&#26032;&#25968;&#25454;&#20998;&#24067;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#37325;&#22797;&#23398;&#20064;&#24050;&#32463;&#20986;&#29616;&#30340;&#20998;&#24067;&#26159;&#20302;&#25928;&#30340;&#12290;&#30001;&#20110;&#20869;&#23384;&#38480;&#21046;&#21644;&#22823;&#35268;&#27169;&#24037;&#19994;&#24212;&#29992;&#20013;&#25968;&#25454;&#20998;&#24067;&#30340;&#22810;&#26679;&#24615;&#65292;&#20256;&#32479;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#31574;&#30053;&#65288;&#22914;&#37325;&#25918;&#12289;&#21442;&#25968;&#38548;&#31163;&#21644;&#30693;&#35782;&#33976;&#39311;&#65289;&#38590;&#20197;&#37096;&#32626;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#38598;&#25104;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#28418;&#31227;&#24863;&#30693;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;CTR&#39044;&#27979;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#36890;&#36807;&#23545;&#27969;&#25968;&#25454;&#36827;&#34892;&#26174;&#24335;&#22522;&#20110;&#35823;&#24046;&#30340;&#28418;&#31227;&#26816;&#27979;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21457;&#29616;&#20998;&#24067;&#28418;&#31227;&#24182;&#26377;&#38024;&#23545;&#24615;&#22320;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction is of great importance in recommendation systems and online advertising platforms. When served in industrial scenarios, the user-generated data observed by the CTR model typically arrives as a stream. Streaming data has the characteristic that the underlying distribution drifts over time and may recur. This can lead to catastrophic forgetting if the model simply adapts to new data distribution all the time. Also, it's inefficient to relearn distribution that has been occurred. Due to memory constraints and diversity of data distributions in large-scale industrial applications, conventional strategies for catastrophic forgetting such as replay, parameter isolation, and knowledge distillation are difficult to be deployed. In this work, we design a novel drift-aware incremental learning framework based on ensemble learning to address catastrophic forgetting in CTR prediction. With explicit error-based drift detection on streaming data, the framework fur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#34920;&#31034;-&#32858;&#21512;&#31574;&#30053;&#26500;&#24314;&#21487;&#25193;&#23637;&#20294;&#26377;&#25928;&#30340;APC&#27169;&#22411;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#65292;&#21487;&#20197;&#21253;&#25324;&#21508;&#31181;&#34920;&#31034;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.09061</link><description>&lt;p&gt;
&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;&#20013;&#33258;&#21160;&#29983;&#25104;&#25773;&#25918;&#21015;&#34920;&#30340;&#21487;&#25193;&#23637;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Scalable Framework for Automatic Playlist Continuation on Music Streaming Services. (arXiv:2304.09061v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#34920;&#31034;-&#32858;&#21512;&#31574;&#30053;&#26500;&#24314;&#21487;&#25193;&#23637;&#20294;&#26377;&#25928;&#30340;APC&#27169;&#22411;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#65292;&#21487;&#20197;&#21253;&#25324;&#21508;&#31181;&#34920;&#31034;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;&#36890;&#24120;&#26088;&#22312;&#25512;&#33616;&#27468;&#26354;&#20197;&#25193;&#23637;&#29992;&#25143;&#22312;&#35813;&#26381;&#21153;&#19978;&#21019;&#24314;&#30340;&#25773;&#25918;&#21015;&#34920;&#12290;&#28982;&#32780;&#65292;&#22312;&#20445;&#30041;&#20854;&#38899;&#20048;&#29305;&#24449;&#21644;&#31526;&#21512;&#29992;&#25143;&#20542;&#21521;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#25773;&#25918;&#21015;&#34920;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#33258;&#21160;&#25773;&#25918;&#21015;&#34920;&#24310;&#32493;&#65288;APC&#65289;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#36825;&#20123;&#26381;&#21153;&#32463;&#24120;&#38656;&#35201;&#22312;&#22823;&#22411;&#30446;&#24405;&#20013;&#23454;&#26102;&#36873;&#25321;&#26368;&#20339;&#27468;&#26354;&#36827;&#34892;&#25512;&#33616;&#65292;&#20294;&#26368;&#36817;&#20851;&#20110;APC&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20855;&#26377;&#23569;&#37327;&#21487;&#25193;&#23637;&#24615;&#20445;&#35777;&#24182;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#30340;&#27169;&#22411;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#21487;&#25193;&#23637;&#20294;&#26377;&#25928;&#30340;APC&#27169;&#22411;&#12290;&#22522;&#20110;&#34920;&#31034;-&#32858;&#21512;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#35774;&#35745;&#30830;&#20445;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#20197;&#21253;&#21547;&#21508;&#31181;&#34920;&#31034;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#65292;&#20363;&#22914;&#22522;&#20110;Transformer&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#24773;&#20917;&#19979;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music streaming services often aim to recommend songs for users to extend the playlists they have created on these services. However, extending playlists while preserving their musical characteristics and matching user preferences remains a challenging task, commonly referred to as Automatic Playlist Continuation (APC). Besides, while these services often need to select the best songs to recommend in real-time and among large catalogs with millions of candidates, recent research on APC mainly focused on models with few scalability guarantees and evaluated on relatively small datasets. In this paper, we introduce a general framework to build scalable yet effective APC models for large-scale applications. Based on a represent-then-aggregate strategy, it ensures scalability by design while remaining flexible enough to incorporate a wide range of representation learning and sequence modeling techniques, e.g., based on Transformers. We demonstrate the relevance of this framework through in-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;k-NN&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#33021;&#22815;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.09058</link><description>&lt;p&gt;
&#37325;&#35775;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;k-NN
&lt;/p&gt;
&lt;p&gt;
Revisiting k-NN for Pre-trained Language Models. (arXiv:2304.09058v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;k-NN&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#33021;&#22815;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20316;&#20026;&#21442;&#25968;&#21270;&#30340;&#24613;&#20999;&#23398;&#20064;&#22120;&#65292;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24403;&#21069;&#33539;&#24335;&#30340;&#23454;&#38469;&#36873;&#25321;&#12290;&#19982;&#27492;&#24418;&#25104;&#23545;&#27604;&#30340;&#26159;&#65292;k-&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20998;&#31867;&#22120;&#20316;&#20026;&#24310;&#36831;&#23398;&#20064;&#27169;&#22411;&#65292;&#20542;&#21521;&#20110;&#20943;&#36731;&#36807;&#25311;&#21512;&#21644;&#23396;&#31435;&#22122;&#22768;&#12290;&#26412;&#25991;&#20013;&#25105;&#20204;&#37325;&#35775;&#20102;k-NN&#20998;&#31867;&#22120;&#65292;&#20197;&#22686;&#24378;&#22522;&#20110;PLMs&#30340;&#20998;&#31867;&#22120;&#12290;&#20174;&#26041;&#27861;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#25991;&#26412;&#34920;&#31034;&#30340;PLMs&#22312;&#20004;&#20010;&#27493;&#39588;&#20013;&#37319;&#29992;k-NN&#65306;&#65288;1&#65289;&#21033;&#29992;k-NN&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#26469;&#26657;&#20934;&#35757;&#32451;&#36807;&#31243;&#65288;2&#65289;&#32447;&#24615;&#25554;&#20540;k-NN&#39044;&#27979;&#30340;&#27010;&#29575;&#20998;&#24067;&#21644;PLMs&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#23454;&#29616;&#20102;k-NN&#26657;&#20934;&#35757;&#32451;&#65292;&#23558;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#26131;&#20110;&#21644;&#38590;&#20197;&#23398;&#20064;&#30340;&#31034;&#20363;&#30340;&#25351;&#26631;&#12290;&#20174;&#24212;&#29992;&#22330;&#26223;&#22810;&#26679;&#24615;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#24494;&#35843;&#12289;&#25552;&#31034;&#24494;&#35843;&#33539;&#24335;&#21644;&#38646;&#26679;&#26412;&#20219;&#21153;&#35774;&#32622;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;k-NN&#21487;&#20197;&#22312;&#25152;&#26377;&#21463;&#21040;&#26816;&#26597;&#30340;&#35774;&#32622;&#20013;&#25345;&#32493;&#25552;&#39640;PLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#21463;&#21040;&#32771;&#34385;&#30340;&#35774;&#32622;&#20013;&#36305;&#36194;&#20102;&#22522;&#20110;&#26222;&#36890;PLMs&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models (PLMs), as parametric-based eager learners, have become the de-facto choice for current paradigms of Natural Language Processing (NLP). In contrast, k-Nearest-Neighbor (k-NN) classifiers, as the lazy learning paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we revisit k-NN classifiers for augmenting the PLMs-based classifiers. From the methodological level, we propose to adopt k-NN with textual representations of PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the training process. (2) Linearly interpolate the probability distribution predicted by k-NN with that of the PLMs' classifier. At the heart of our approach is the implementation of k-NN-calibrated training, which treats predicted results as indicators for easy versus hard examples during the training process. From the perspective of the diversity of application scenarios, we conduct extensive experiments on fine-tuning, prompt-tuning paradigms and zero-sh
&lt;/p&gt;</description></item><item><title>&#27492;&#30740;&#31350;&#26088;&#22312;&#22312;&#26356;&#20026;&#30495;&#23454;&#30340;&#29615;&#22659;&#20013;&#35299;&#30721;&#20154;&#31867;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#65292;&#20197;&#36827;&#19968;&#27493;&#29702;&#35299;&#22797;&#26434;&#20219;&#21153;&#26399;&#38388;&#20010;&#20307;&#20869;&#37096;&#28508;&#22312;&#29366;&#24577;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39564;&#35777;&#23454;&#39564;&#23460;&#26041;&#27861;&#24182;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2304.09050</link><description>&lt;p&gt;
&#35299;&#30721;&#31070;&#32463;&#27963;&#21160;&#20197;&#35780;&#20272;&#20010;&#20307;&#22312;&#29983;&#24577;&#26377;&#25928;&#29615;&#22659;&#20013;&#30340;&#28508;&#22312;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Decoding Neural Activity to Assess Individual Latent State in Ecologically Valid Contexts. (arXiv:2304.09050v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09050
&lt;/p&gt;
&lt;p&gt;
&#27492;&#30740;&#31350;&#26088;&#22312;&#22312;&#26356;&#20026;&#30495;&#23454;&#30340;&#29615;&#22659;&#20013;&#35299;&#30721;&#20154;&#31867;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#65292;&#20197;&#36827;&#19968;&#27493;&#29702;&#35299;&#22797;&#26434;&#20219;&#21153;&#26399;&#38388;&#20010;&#20307;&#20869;&#37096;&#28508;&#22312;&#29366;&#24577;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39564;&#35777;&#23454;&#39564;&#23460;&#26041;&#27861;&#24182;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21382;&#21490;&#19978;&#65292;&#20165;&#26377;&#23569;&#37327;&#26041;&#27861;&#21487;&#20197;&#22312;&#26356;&#29983;&#24577;&#26377;&#25928;&#30340;&#24773;&#22659;&#19979;&#20998;&#31163;&#35748;&#30693;&#36807;&#31243;&#12290;&#29305;&#21035;&#22320;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22312;&#27492;&#31867;&#32422;&#26463;&#26465;&#20214;&#19979;&#35266;&#23519;&#21040;&#30340;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#23454;&#38469;&#19978;&#26159;&#21542;&#20197;&#19968;&#31181;&#21487;&#20197;&#29992;&#20110;&#20934;&#30830;&#25512;&#26029;&#20010;&#20307;&#28508;&#22312;&#29366;&#24577;&#12289;&#30456;&#20851;&#35748;&#30693;&#36807;&#31243;&#25110;&#36817;&#31471;&#34892;&#20026;&#30340;&#26041;&#24335;&#22312;&#23454;&#39564;&#23460;&#22806;&#26174;&#29616;&#12290;&#25913;&#21892;&#25105;&#20204;&#23545;&#29305;&#23450;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#22312;&#29983;&#24577;&#26377;&#25928;&#24773;&#22659;&#20013;&#26174;&#29616;&#30340;&#29702;&#35299;&#65292;&#20250;&#39564;&#35777;&#22312;&#38548;&#31163;&#29615;&#22659;&#19979;&#30740;&#31350;&#31867;&#20284;&#31070;&#32463;&#29616;&#35937;&#30340;&#23454;&#39564;&#23460;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20851;&#20110;&#22797;&#26434;&#20219;&#21153;&#26399;&#38388;&#21457;&#29983;&#30340;&#28508;&#22312;&#29366;&#24577;&#30340;&#26377;&#24847;&#20041;&#27934;&#23519;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26469;&#33258;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#31038;&#21306;&#30340;&#39046;&#22495;&#36890;&#29992;&#26041;&#27861;&#26377;&#28508;&#21147;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#20197;&#21069;&#20351;&#29992;&#20102;&#36825;&#26679;&#30340;&#26041;&#27861;&#26469;&#35299;&#30721;&#19982;&#35270;&#35273;&#30446;&#26631;&#30456;&#20851;&#30340;&#31361;&#21457;&#31070;&#32463;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
There exist very few ways to isolate cognitive processes, historically defined via highly controlled laboratory studies, in more ecologically valid contexts. Specifically, it remains unclear as to what extent patterns of neural activity observed under such constraints actually manifest outside the laboratory in a manner that can be used to make an accurate inference about the latent state, associated cognitive process, or proximal behavior of the individual. Improving our understanding of when and how specific patterns of neural activity manifest in ecologically valid scenarios would provide validation for laboratory-based approaches that study similar neural phenomena in isolation and meaningful insight into the latent states that occur during complex tasks. We argue that domain generalization methods from the brain-computer interface community have the potential to address this challenge. We previously used such an approach to decode phasic neural responses associated with visual tar
&lt;/p&gt;</description></item><item><title>DeepGEMM&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26597;&#25214;&#34920;&#22312;CPU&#20307;&#31995;&#32467;&#26500;&#19978;&#21152;&#36895;&#36229;&#20302;&#31934;&#24230;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25191;&#34892;&#36229;&#20302;&#31934;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2304.09049</link><description>&lt;p&gt;
DeepGEMM&#65306;&#20351;&#29992;&#26597;&#25214;&#34920;&#22312;CPU&#20307;&#31995;&#32467;&#26500;&#19978;&#21152;&#36895;&#36229;&#20302;&#31934;&#24230;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
DeepGEMM: Accelerated Ultra Low-Precision Inference on CPU Architectures using Lookup Tables. (arXiv:2304.09049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09049
&lt;/p&gt;
&lt;p&gt;
DeepGEMM&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26597;&#25214;&#34920;&#22312;CPU&#20307;&#31995;&#32467;&#26500;&#19978;&#21152;&#36895;&#36229;&#20302;&#31934;&#24230;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25191;&#34892;&#36229;&#20302;&#31934;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36229;&#20302;&#27604;&#29305;&#37327;&#21270;&#21462;&#24471;&#20102;&#24456;&#22810;&#36827;&#23637;&#65292;&#25215;&#35834;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#26174;&#30528;&#25552;&#39640;&#24310;&#36831;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;&#23398;&#20064;&#30340;&#27493;&#38271;&#37327;&#21270;&#31561;&#37327;&#21270;&#26041;&#27861;&#21363;&#20351;&#20351;&#29992;&#23376;&#23383;&#33410;&#37327;&#21270;&#20063;&#21487;&#20197;&#23454;&#29616;&#27169;&#22411;&#31934;&#24230;&#19982;&#20840;&#31934;&#24230;&#28014;&#28857;&#22522;&#32447;&#30456;&#23218;&#32654;&#12290;&#20294;&#26159;&#65292;&#35201;&#22312;&#20027;&#27969;CPU&#35774;&#22791;&#19978;&#37096;&#32626;&#36825;&#20123;&#36229;&#20302;&#27604;&#29305;&#37327;&#21270;&#27169;&#22411;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36890;&#29992;SIMD&#65288;&#21333;&#25351;&#20196;&#27969;&#22810;&#25968;&#25454;&#27969;&#65289;&#30828;&#20214;&#36890;&#24120;&#25903;&#25345;&#19981;&#20302;&#20110;8&#20301;&#31934;&#24230;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26597;&#25214;&#34920;&#30340;DeepGEMM&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;SIMD&#30828;&#20214;&#19978;&#25191;&#34892;&#36229;&#20302;&#31934;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#39044;&#20808;&#35745;&#31639;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#20056;&#31215;&#65292;&#23558;&#23427;&#20204;&#23384;&#20648;&#22312;&#26597;&#25214;&#34920;&#20013;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#39640;&#25928;&#22320;&#35775;&#38382;&#23427;&#20204;&#65292;&#20197;&#36991;&#20813;&#26114;&#36149;&#30340;&#20056;&#21152;&#36816;&#31639;&#12290;&#25105;&#20204;&#30340;2&#20301;&#23454;&#29616;&#20248;&#20110;&#32852;
&lt;/p&gt;
&lt;p&gt;
A lot of recent progress has been made in ultra low-bit quantization, promising significant improvements in latency, memory footprint and energy consumption on edge devices. Quantization methods such as Learned Step Size Quantization can achieve model accuracy that is comparable to full-precision floating-point baselines even with sub-byte quantization. However, it is extremely challenging to deploy these ultra low-bit quantized models on mainstream CPU devices because commodity SIMD (Single Instruction, Multiple Data) hardware typically supports no less than 8-bit precision. To overcome this limitation, we propose DeepGEMM, a lookup table based approach for the execution of ultra low-precision convolutional neural networks on SIMD hardware. The proposed method precomputes all possible products of weights and activations, stores them in a lookup table, and efficiently accesses them at inference time to avoid costly multiply-accumulate operations. Our 2-bit implementation outperforms co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09048</link><description>&lt;p&gt;
CodeKGC&#65306;&#29992;&#20110;&#29983;&#25104;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#32467;&#26500;&#24615;&#30693;&#35782;&#65292;&#32780;&#21482;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#24207;&#21015;&#21270;&#25991;&#26412;&#25110;&#35268;&#33539;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20687;&#20195;&#30721;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20197;&#36827;&#34892;&#32467;&#26500;&#24615;&#39044;&#27979;&#21644;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#32473;&#23450;&#20195;&#30721;&#26684;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#21487;&#20197;&#34920;&#31034;&#20026;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#30340;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20855;&#26377;&#27169;&#24335;&#24863;&#30693;&#22411;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#12290;&#30001;&#20110;&#20195;&#30721;&#26412;&#36136;&#19978;&#20855;&#26377;&#32467;&#26500;&#65292;&#22914;&#31867;&#21644;&#20989;&#25968;&#23450;&#20041;&#65292;&#22240;&#27492;&#23427;&#20316;&#20026;&#20808;&#39564;&#30340;&#35821;&#20041;&#32467;&#26500;&#30693;&#35782;&#27169;&#22411;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21407;&#29702;&#30340;&#29983;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#21407;&#29702;&#25552;&#20379;&#20102;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36890;&#29992;&#24494;&#20998;&#26041;&#31243;&#65288;UDE&#65289;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20197;&#31616;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#21040;&#19968;&#20010;&#38598;&#20013;&#21442;&#25968;&#12290;&#24212;&#29992;&#20110;&#25705;&#25830;&#25605;&#25292;&#28938;&#25509;&#20013;&#30340;&#27785;&#22836;&#21644;&#20572;&#30041;&#38454;&#27573;&#30340;&#24314;&#27169;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.09047</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#25705;&#25830;&#25605;&#25292;&#21152;&#24037;&#30340;&#31070;&#32463;&#20803;&#38598;&#21442;&#25968;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural Lumped Parameter Differential Equations with Application in Friction-Stir Processing. (arXiv:2304.09047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36890;&#29992;&#24494;&#20998;&#26041;&#31243;&#65288;UDE&#65289;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20197;&#31616;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#21040;&#19968;&#20010;&#38598;&#20013;&#21442;&#25968;&#12290;&#24212;&#29992;&#20110;&#25705;&#25830;&#25605;&#25292;&#28938;&#25509;&#20013;&#30340;&#27785;&#22836;&#21644;&#20572;&#30041;&#38454;&#27573;&#30340;&#24314;&#27169;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#21442;&#25968;&#26041;&#27861;&#26088;&#22312;&#23558;&#31354;&#38388;&#25193;&#23637;&#25110;&#36830;&#32493;&#29289;&#29702;&#31995;&#32479;&#30340;&#28436;&#21464;&#31616;&#21270;&#20026;&#20195;&#34920;&#24314;&#27169;&#31995;&#32479;&#30340;&#29289;&#29702;&#23610;&#24230;&#30340;&#8220;&#38598;&#20013;&#8221;&#20803;&#32032;&#30340;&#28436;&#21464;&#12290;&#23545;&#20110;&#23450;&#20041;&#38598;&#20013;&#20803;&#32032;&#25110;&#20854;&#20851;&#32852;&#29289;&#29702;&#23398;&#21487;&#33021;&#26410;&#30693;&#30340;&#31995;&#32479;&#65292;&#24314;&#27169;&#20219;&#21153;&#21487;&#33021;&#20165;&#38480;&#20110;&#31995;&#32479;&#29289;&#29702;&#30340;&#20840;&#20445;&#30495;&#27169;&#25311;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#26377;&#38480;&#28857;&#27979;&#37327;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#20219;&#21153;&#30340;&#36830;&#32493;&#31995;&#32479;&#65292;&#24182;&#24314;&#31435;&#22312;&#36890;&#29992;&#24494;&#20998;&#26041;&#31243;&#65288;UDE&#65289;&#30340;&#27010;&#24565;&#19978;&#65292;&#20197;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#23558;&#21160;&#21147;&#23398;&#20943;&#23569;&#21040;&#19968;&#20010;&#38598;&#20013;&#21442;&#25968;&#65292;&#24182;&#25512;&#26029;&#20854;&#24615;&#36136;&#12290;UDE&#30340;&#28789;&#27963;&#24615;&#20801;&#35768;&#32452;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#24212;&#29992;&#30340;&#24314;&#27169;&#20219;&#21153;&#30340;&#21508;&#31181;&#24050;&#30693;&#29289;&#29702;&#20808;&#39564;&#65292;&#21253;&#25324;&#38598;&#21442;&#25968;&#26041;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#21160;&#26426;&#26159;&#25705;&#25830;&#25605;&#25292;&#28938;&#25509;&#30340;&#27785;&#22836;&#21644;&#20572;&#30041;&#38454;&#27573;;&#20855;&#20307;&#22320;&#65292;&#23558;&#21151;&#29575;&#36755;&#20837;&#26144;&#23556;&#21040;&#24037;&#20855;&#20013;&#65292;&#28982;&#21518;&#25512;&#26029;&#25152;&#20351;&#29992;&#30340;&#38598;&#20013;&#21442;&#25968;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lumped parameter methods aim to simplify the evolution of spatially-extended or continuous physical systems to that of a "lumped" element representative of the physical scales of the modeled system. For systems where the definition of a lumped element or its associated physics may be unknown, modeling tasks may be restricted to full-fidelity simulations of the physics of a system. In this work, we consider data-driven modeling tasks with limited point-wise measurements of otherwise continuous systems. We build upon the notion of the Universal Differential Equation (UDE) to construct data-driven models for reducing dynamics to that of a lumped parameter and inferring its properties. The flexibility of UDEs allow for composing various known physical priors suitable for application-specific modeling tasks, including lumped parameter methods. The motivating example for this work is the plunge and dwell stages for friction-stir welding; specifically, (i) mapping power input into the tool to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#27969;&#20197;&#36827;&#34892;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09010</link><description>&lt;p&gt;
CF-VAE&#65306;&#22522;&#20110;VAE&#21644;&#22240;&#26524;&#27969;&#30340;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CF-VAE: Causal Disentangled Representation Learning with VAE and Causal Flows. (arXiv:2304.09010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#27969;&#20197;&#36827;&#34892;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26088;&#22312;&#23398;&#20064;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20854;&#20013;&#27599;&#20010;&#32500;&#24230;&#23545;&#24212;&#19968;&#20010;&#28508;&#22312;&#30340;&#29983;&#25104;&#22240;&#32032;&#12290;&#30001;&#20110;&#29983;&#25104;&#22240;&#32032;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#65292;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20197;&#23558;&#22240;&#26524;&#32467;&#26500;&#20449;&#24687;&#24341;&#20837;&#27169;&#22411;&#20013;&#30340;&#27969;&#65292;&#31216;&#20026;&#22240;&#26524;&#27969;&#12290;&#22522;&#20110;&#24191;&#27867;&#29992;&#20110;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#24341;&#20837;&#22522;&#20934;&#22240;&#32032;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#20998;&#31163;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CF-VAE&#21487;&#20197;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning disentangled representations is important in representation learning, aiming to learn a low dimensional representation of data where each dimension corresponds to one underlying generative factor. Due to the possibility of causal relationships between generative factors, causal disentangled representation learning has received widespread attention. In this paper, we first propose a new flows that can incorporate causal structure information into the model, called causal flows. Based on the variational autoencoders(VAE) commonly used in disentangled representation learning, we design a new model, CF-VAE, which enhances the disentanglement ability of the VAE encoder by utilizing the causal flows. By further introducing the supervision of ground-truth factors, we demonstrate the disentanglement identifiability of our model. Experimental results on both synthetic and real datasets show that CF-VAE can achieve causal disentanglement and perform intervention experiments. Moreover, C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#36890;&#20449;&#21463;&#38480;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#36164;&#28304;&#26377;&#38480;&#31561;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24180;&#40836;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#26696;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#27599;&#36718;&#32852;&#37030;&#23398;&#20064;&#30340;&#24635;&#26102;&#38388;&#28040;&#32791;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08996</link><description>&lt;p&gt;
&#22522;&#20110;&#24180;&#40836;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;NOMA&#32593;&#32476;&#19979;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Joint Age-based Client Selection and Resource Allocation for Communication-Efficient Federated Learning over NOMA Networks. (arXiv:2304.08996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#36890;&#20449;&#21463;&#38480;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#36164;&#28304;&#26377;&#38480;&#31561;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24180;&#40836;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#26696;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#27599;&#36718;&#32852;&#37030;&#23398;&#20064;&#30340;&#24635;&#26102;&#38388;&#28040;&#32791;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#21487;&#20197;&#20351;&#24471;&#23458;&#25143;&#31471;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;FL&#37096;&#32626;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#26102;&#65292;&#30001;&#20110;&#36890;&#20449;&#38142;&#36335;&#24046;&#21644;&#25910;&#25947;&#36895;&#24230;&#24930;&#65292;FL&#30340;&#24615;&#33021;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26080;&#32447;&#36164;&#28304;&#21463;&#38480;&#65292;&#20934;&#30830;&#36873;&#21462;&#23458;&#25143;&#31471;&#21644;&#25511;&#21046;&#36164;&#28304;&#20998;&#37197;&#23545;&#20110;&#25552;&#39640;FL&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#23458;&#25143;&#31471;&#36873;&#25321;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#22312;&#38750;&#27491;&#20132;&#22810;&#22336;&#65288;NOMA&#65289;&#26080;&#32447;&#32593;&#32476;&#19978;&#27599;&#36718;FL&#30340;&#24635;&#26102;&#38388;&#28040;&#32791;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#25910;&#21040;&#30340;&#26412;&#22320;FL&#27169;&#22411;&#30340;&#26032;&#26087;&#31243;&#24230;&#26469;&#35774;&#35745;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#24180;&#40836;&#26356;&#26032;&#65288;AoU&#65289;&#25351;&#26631;&#33719;&#24471;&#36164;&#28304;&#20998;&#37197;&#30340;&#38381;&#21512;&#24335;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising paradigm that enables distributed clients to collaboratively train a shared global model while keeping the training data locally. However, the performance of FL is often limited by poor communication links and slow convergence when FL is deployed over wireless networks. Besides, due to the limited radio resources, it is crucial to select clients and control resource allocation accurately for improved FL performance. Motivated by these challenges, a joint optimization problem of client selection and resource allocation is formulated in this paper, aiming to minimize the total time consumption of each round in FL over non-orthogonal multiple access (NOMA) enabled wireless network. Specifically, based on a metric termed the age of update (AoU), we first propose a novel client selection scheme by accounting for the staleness of the received local FL models. After that, the closed-form solutions of resource allocation are obtained by monotonicity analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Parcel3D&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#21333;&#24352;RGB&#22270;&#20687;&#30340;&#29289;&#27969;&#21253;&#35065;&#24418;&#29366;&#37325;&#24314;&#38382;&#39064;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;CubeRefine R-CNN&#26469;&#26816;&#27979;&#21253;&#35065;&#26159;&#21542;&#21463;&#21040;&#25439;&#22351;&#65292;&#35813;&#31639;&#27861;&#22312;Parcel3D&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08994</link><description>&lt;p&gt;
Parcel3D&#65306;&#38754;&#21521;&#36816;&#36755;&#29289;&#27969;&#21333;&#24352;RGB&#22270;&#20687;&#30340;&#24418;&#29366;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Parcel3D: Shape Reconstruction from Single RGB Images for Applications in Transportation Logistics. (arXiv:2304.08994v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Parcel3D&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#21333;&#24352;RGB&#22270;&#20687;&#30340;&#29289;&#27969;&#21253;&#35065;&#24418;&#29366;&#37325;&#24314;&#38382;&#39064;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;CubeRefine R-CNN&#26469;&#26816;&#27979;&#21253;&#35065;&#26159;&#21542;&#21463;&#21040;&#25439;&#22351;&#65292;&#35813;&#31639;&#27861;&#22312;Parcel3D&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23454;&#29616;&#22312;&#29289;&#27969;&#39046;&#22495;&#20013;&#30340;&#25439;&#22351;&#21644;&#31713;&#25913;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#23545;&#21487;&#33021;&#21463;&#25439;&#21253;&#35065;&#30340;&#19977;&#32500;&#24418;&#29366;&#37325;&#24314;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#21333;&#20010;RGB&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#36866;&#29992;&#20110;&#21482;&#26377;&#31616;&#21333;&#25163;&#25345;&#35774;&#22791;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#37038;&#36882;&#21592;&#22312;&#36882;&#36865;&#36807;&#31243;&#20013;&#25110;&#23458;&#25143;&#22312;&#36865;&#36135;&#26102;&#38656;&#35201;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Parcel3D&#30340;&#26032;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#23427;&#22522;&#20110;Google&#25195;&#25551;&#29289;&#21697;&#65288;GSO&#65289;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;13,000&#20010;&#24102;&#26377;&#23436;&#25972;&#19977;&#32500;&#27880;&#37322;&#30340;&#21253;&#35065;&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#23436;&#25972;&#30340;&#12289;&#21363;&#38271;&#26041;&#20307;&#24418;&#29366;&#12289;&#26410;&#25439;&#22351;&#30340;&#21253;&#35065;&#21644;&#36890;&#36807;&#27169;&#25311;&#29983;&#25104;&#30340;&#25439;&#22351;&#21253;&#35065;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;CubeRefine R-CNN&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#23558;3D&#36793;&#30028;&#26694;&#30340;&#20272;&#35745;&#19982;&#36845;&#20195;&#32593;&#26684;&#32454;&#21270;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#21253;&#35065;&#30340;&#20219;&#24847;&#25439;&#22351;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;Parcel3D&#21644;&#29616;&#26377;&#31435;&#26041;&#20307;&#24418;&#29366;&#21253;&#35065;&#30340;&#30495;&#23454;&#22330;&#26223;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;Parcel3D&#26469;&#35757;&#32451;&#21487;&#20197;&#23454;&#29616;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on enabling damage and tampering detection in logistics and tackle the problem of 3D shape reconstruction of potentially damaged parcels. As input we utilize single RGB images, which corresponds to use-cases where only simple handheld devices are available, e.g. for postmen during delivery or clients on delivery. We present a novel synthetic dataset, named Parcel3D, that is based on the Google Scanned Objects (GSO) dataset and consists of more than 13,000 images of parcels with full 3D annotations. The dataset contains intact, i.e. cuboid-shaped, parcels and damaged parcels, which were generated in simulations. We work towards detecting mishandling of parcels by presenting a novel architecture called CubeRefine R-CNN, which combines estimating a 3D bounding box with an iterative mesh refinement. We benchmark our approach on Parcel3D and an existing dataset of cuboid-shaped parcels in real-world scenarios. Our results show, that while training on Parcel3D enables transfer to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35270;&#21270;&#35299;&#37322;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#36716;&#25442;&#65288;&#22686;&#24378;&#65289;&#30340;&#21709;&#24212;&#65292;&#21457;&#29616;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#35777;&#26126;&#35299;&#37322;&#30456;&#36739;&#20110;&#20998;&#31867;&#24615;&#33021;&#26356;&#23481;&#26131;&#21463;&#21040;&#22686;&#24378;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.08984</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35270;&#21270;&#35299;&#37322;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#31283;&#20581;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Robustness of Visual Explanations to Common Data Augmentation. (arXiv:2304.08984v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35270;&#21270;&#35299;&#37322;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#36716;&#25442;&#65288;&#22686;&#24378;&#65289;&#30340;&#21709;&#24212;&#65292;&#21457;&#29616;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#35777;&#26126;&#35299;&#37322;&#30456;&#36739;&#20110;&#20998;&#31867;&#24615;&#33021;&#26356;&#23481;&#26131;&#21463;&#21040;&#22686;&#24378;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20102;&#35299;&#20854;&#34892;&#20026;&#21464;&#24471;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#20026;&#37325;&#35201;&#12290;&#21518;&#32493;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26159;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21463;&#21040;&#36136;&#30097;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#31350;&#21518;&#32493;&#21487;&#35270;&#21270;&#35299;&#37322;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#36716;&#25442;&#65288;&#36890;&#24120;&#31216;&#20026;&#22686;&#24378;&#65289;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#39044;&#35745;&#35299;&#37322;&#22312;&#26576;&#20123;&#36716;&#25442;&#19979;&#26159;&#19981;&#21464;&#30340;&#65292;&#20363;&#22914;&#26356;&#25913;&#39068;&#33394;&#26144;&#23556;&#65292;&#21516;&#26102;&#23545;&#20110;&#20687;&#24179;&#31227;&#12289;&#23545;&#35937;&#32553;&#25918;&#21644;&#26059;&#36716;&#36825;&#26679;&#30340;&#36716;&#25442;&#21017;&#21709;&#24212;&#21464;&#25442;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31283;&#20581;&#24615;&#30340;&#19981;&#21516;&#31243;&#24230;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#26576;&#20123;&#35299;&#37322;&#26041;&#27861;&#65288;&#20363;&#22914;LRP&#22797;&#21512;&#29289;&#21644;Guided Backprop&#65289;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#31283;&#23450;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#22521;&#35757;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#35299;&#37322;&#36890;&#24120;&#30456;&#36739;&#20998;&#31867;&#24615;&#33021;&#32780;&#35328;&#23545;&#22686;&#24378;&#30340;&#40065;&#26834;&#24615;&#36739;&#24046;&#65292;&#26080;&#35770;&#25968;&#25454;&#22686;&#24378;&#26159;&#21542;&#26126;&#30830;&#22320;&#21253;&#25324;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of deep neural networks continues to grow, understanding their behaviour has become more crucial than ever. Post-hoc explainability methods are a potential solution, but their reliability is being called into question. Our research investigates the response of post-hoc visual explanations to naturally occurring transformations, often referred to as augmentations. We anticipate explanations to be invariant under certain transformations, such as changes to the colour map while responding in an equivariant manner to transformations like translation, object scaling, and rotation. We have found remarkable differences in robustness depending on the type of transformation, with some explainability methods (such as LRP composites and Guided Backprop) being more stable than others. We also explore the role of training with data augmentation. We provide evidence that explanations are typically less robust to augmentation than classification performance, regardless of whether data augm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23545;ChatGPT&#22312;&#36890;&#29992;&#38382;&#31572;&#22330;&#26223;&#20013;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#21457;&#29616;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#21487;&#38752;&#24615;&#26377;&#25152;&#24046;&#24322;&#65292;&#23588;&#20854;&#22312;&#27861;&#24459;&#21644;&#31185;&#23398;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#65292;&#23545;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.08979</link><description>&lt;p&gt;
ChatGPT&#21487;&#38752;&#24615;&#30340;&#27979;&#37327;&#19982;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT. (arXiv:2304.08979v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;ChatGPT&#22312;&#36890;&#29992;&#38382;&#31572;&#22330;&#26223;&#20013;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#21457;&#29616;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#21487;&#38752;&#24615;&#26377;&#25152;&#24046;&#24322;&#65292;&#23588;&#20854;&#22312;&#27861;&#24459;&#21644;&#31185;&#23398;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#65292;&#23545;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#30340;&#20986;&#29616;&#65292;&#29992;&#25143;&#33719;&#21462;&#20449;&#24687;&#30340;&#26041;&#24335;&#27491;&#22312;&#21457;&#29983;&#33539;&#24335;&#36716;&#21464;&#12290;&#19982;&#20256;&#32479;&#30340;&#25628;&#32034;&#24341;&#25806;&#19981;&#21516;&#65292;ChatGPT&#20174;&#27169;&#22411;&#26412;&#36523;&#26816;&#32034;&#30693;&#35782;&#24182;&#20026;&#29992;&#25143;&#29983;&#25104;&#31572;&#26696;&#12290;ChatGPT&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38382;&#31572;&#33021;&#21147;&#21560;&#24341;&#20102;&#36229;&#36807;1&#20159;&#29992;&#25143;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#20851;&#20110;&#20854;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#36890;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;5695&#20010;&#38382;&#39064;&#36328;&#36234;&#21313;&#20010;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#39046;&#22495;&#65292;&#39318;&#27425;&#23545;ChatGPT&#22312;&#36890;&#29992;&#38382;&#31572;&#22330;&#26223;&#20013;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27979;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;ChatGPT&#30340;&#21487;&#38752;&#24615;&#22240;&#19981;&#21516;&#39046;&#22495;&#32780;&#24322;&#65292;&#23588;&#20854;&#22312;&#27861;&#24459;&#21644;&#31185;&#23398;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;OpenAI&#35774;&#35745;&#30340;&#31995;&#32479;&#35282;&#33394;&#21487;&#20197;&#24433;&#21709;ChatGPT&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;ChatGPT&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#65292;&#21363;&#20351;&#26159;&#21333;&#20010;&#23383;&#31526;&#30340;&#26356;&#25913;&#20063;&#20250;&#23545;&#20854;&#21487;&#38752;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;ChatGPT&#21487;&#38752;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#23545;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The way users acquire information is undergoing a paradigm shift with the advent of ChatGPT. Unlike conventional search engines, ChatGPT retrieves knowledge from the model itself and generates answers for users. ChatGPT's impressive question-answering (QA) capability has attracted more than 100 million users within a short period of time but has also raised concerns regarding its reliability. In this paper, we perform the first large-scale measurement of ChatGPT's reliability in the generic QA scenario with a carefully curated set of 5,695 questions across ten datasets and eight domains. We find that ChatGPT's reliability varies across different domains, especially underperforming in law and science questions. We also demonstrate that system roles, originally designed by OpenAI to allow users to steer ChatGPT's behavior, can impact ChatGPT's reliability. We further show that ChatGPT is vulnerable to adversarial examples, and even a single character change can negatively affect its reli
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#20844;&#20247;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#30446;&#21069;&#22823;&#37096;&#20998;&#26816;&#27979;&#24037;&#20855;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#23481;&#26131;&#24494;&#35843;&#19988;&#38590;&#20197;&#34987;&#20854;&#20182;LLMs&#26816;&#27979;&#21040;&#12290;</title><link>http://arxiv.org/abs/2304.08968</link><description>&lt;p&gt;
&#38543;&#26426;&#40550;&#40521;&#23547;&#25214;&#38543;&#26426;&#40550;&#40521;&#65306;LLMs&#26131;&#20110;&#24494;&#35843;&#19988;&#38590;&#20197;&#34987;&#20854;&#20182;LLMs&#26816;&#27979;&#21040;
&lt;/p&gt;
&lt;p&gt;
Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs. (arXiv:2304.08968v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08968
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#20844;&#20247;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#30446;&#21069;&#22823;&#37096;&#20998;&#26816;&#27979;&#24037;&#20855;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#23481;&#26131;&#24494;&#35843;&#19988;&#38590;&#20197;&#34987;&#20854;&#20182;LLMs&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#27880;&#24847;&#21147;&#38761;&#21629;&#20351;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#24471;&#20197;&#25193;&#23637;&#24182;&#23454;&#29616;&#36234;&#26469;&#36234;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#31216;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#26368;&#36817;&#30001;&#20110;&#23545;&#35805;&#24494;&#35843;&#32780;&#22312;&#20844;&#20247;&#20013;&#33719;&#24471;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20174;&#32780;&#20351;&#20854;&#34892;&#20026;&#31526;&#21512;&#20844;&#20247;&#23545;&#20110;AI&#30340;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31361;&#20986;&#20063;&#21152;&#22823;&#20102;&#20851;&#27880;LLMs&#35823;&#29992;&#30340;&#20808;&#21069;&#25285;&#24551;&#65292;&#24182;&#23548;&#33268;&#20986;&#29616;&#35768;&#22810;&#22312;&#37326;&#22806;&#26816;&#27979;LLMs&#30340;&#24037;&#20855;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#36825;&#26679;&#30340;&#24037;&#20855;&#37117;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#22522;&#20110;&#24494;&#35843;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26816;&#27979;LLMs&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#35828;&#26126;&#25105;&#20204;&#30340;&#24037;&#20316;&#28041;&#21450;&#30340;&#37325;&#35201;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-attention revolution allowed generative language models to scale and achieve increasingly impressive abilities. Such models - commonly referred to as Large Language Models (LLMs) - have recently gained prominence with the general public, thanks to conversational fine-tuning, putting their behavior in line with public expectations regarding AI. This prominence amplified prior concerns regarding the misuse of LLMs and led to the emergence of numerous tools to detect LLMs in the wild.  Unfortunately, most such tools are critically flawed. While major publications in the LLM detectability field suggested that LLMs were easy to detect with fine-tuned autoencoders, the limitations of their results are easy to overlook. Specifically, they assumed publicly available generative models without fine-tunes or non-trivial prompts. While the importance of these assumptions has been demonstrated, until now, it remained unclear how well such detection could be countered.  Here, we show that a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#20316;&#20026;&#24418;&#29366;&#34920;&#31034;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#25152;&#24471;&#65292;&#23545;&#26059;&#36716;&#20855;&#26377;&#31561;&#21464;&#24615;&#65292;&#26469;&#29983;&#25104;&#36924;&#30495;&#30340;&#27963;&#32454;&#32990;&#27169;&#22411;&#65292;&#20026;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#32454;&#32990;&#36319;&#36394;&#21644;&#20998;&#21106;&#26041;&#27861;&#25552;&#20379;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.08960</link><description>&lt;p&gt;
&#20351;&#29992;SO(3)-&#31561;&#21464;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#29983;&#25104;&#27963;&#32454;&#32990;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative modeling of living cells with SO(3)-equivariant implicit neural representations. (arXiv:2304.08960v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#20316;&#20026;&#24418;&#29366;&#34920;&#31034;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#25152;&#24471;&#65292;&#23545;&#26059;&#36716;&#20855;&#26377;&#31561;&#21464;&#24615;&#65292;&#26469;&#29983;&#25104;&#36924;&#30495;&#30340;&#27963;&#32454;&#32990;&#27169;&#22411;&#65292;&#20026;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#32454;&#32990;&#36319;&#36394;&#21644;&#20998;&#21106;&#26041;&#27861;&#25552;&#20379;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20013;&#22522;&#20110;&#25968;&#25454;&#30340;&#32454;&#32990;&#36319;&#36394;&#21644;&#20998;&#21106;&#26041;&#27861;&#38656;&#35201;&#22810;&#26679;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#24403;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#26102;&#65292;&#21487;&#20197;&#20351;&#29992;&#21512;&#25104;&#30340;&#35745;&#31639;&#26426;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#36825;&#20123;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#38656;&#35201;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#32454;&#32990;&#24418;&#29366;&#20197;&#21450;&#30456;&#24212;&#30340;&#26174;&#24494;&#38236;&#22270;&#20687;&#12290;&#20026;&#20102;&#21512;&#25104;&#36924;&#30495;&#30340;&#27963;&#32454;&#32990;&#24418;&#24577;&#65292;&#29983;&#25104;&#27169;&#22411;&#20351;&#29992;&#30340;&#24418;&#29366;&#34920;&#31034;&#24212;&#33021;&#22815;&#20934;&#30830;&#34920;&#31034;&#32454;&#33410;&#21644;&#25299;&#25169;&#21464;&#21270;&#65292;&#36825;&#22312;&#32454;&#32990;&#20013;&#24456;&#24120;&#35265;&#12290;&#36825;&#20123;&#35201;&#27714;&#24182;&#19981;&#36866;&#29992;&#20110;3D&#20307;&#32032;&#25513;&#27169;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#20998;&#36776;&#29575;&#38480;&#21046;&#65292;&#20063;&#19981;&#36866;&#29992;&#20110;&#22810;&#36793;&#24418;&#32593;&#26684;&#65292;&#22240;&#20026;&#26080;&#27861;&#26131;&#20110;&#27169;&#25311;&#32454;&#32990;&#22686;&#38271;&#21644;&#26377;&#19997;&#20998;&#35010;&#31561;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDFs&#65289;&#30340;&#27700;&#24179;&#38598;&#26469;&#34920;&#31034;&#27963;&#32454;&#32990;&#24418;&#29366;&#65292;&#36825;&#20123;&#27700;&#24179;&#38598;&#30001;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#24471;&#20986;&#65292;&#32780;&#19988;&#23545;&#26059;&#36716;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#27492;&#34920;&#31034;&#36716;&#25442;&#20026;&#32593;&#26684;&#21644;RGB&#22270;&#20687;&#20197;&#36827;&#34892;&#21487;&#35270;&#21270;&#21644;&#29992;&#20110;&#19979;&#28216;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven cell tracking and segmentation methods in biomedical imaging require diverse and information-rich training data. In cases where the number of training samples is limited, synthetic computer-generated data sets can be used to improve these methods. This requires the synthesis of cell shapes as well as corresponding microscopy images using generative models. To synthesize realistic living cell shapes, the shape representation used by the generative model should be able to accurately represent fine details and changes in topology, which are common in cells. These requirements are not met by 3D voxel masks, which are restricted in resolution, and polygon meshes, which do not easily model processes like cell growth and mitosis. In this work, we propose to represent living cell shapes as level sets of signed distance functions (SDFs) which are estimated by neural networks. We optimize a fully-connected neural network to provide an implicit representation of the SDF value at any p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#23376;&#35789;&#20998;&#35789;&#25216;&#26415;&#22312;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#20855;&#32467;&#26500;&#30340;&#38899;&#20048;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;BPE&#26041;&#27861;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#21487;&#34892;&#19988;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.08953</link><description>&lt;p&gt;
&#20174;&#21333;&#35789;&#21040;&#38899;&#20048;&#65306;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#23376;&#35789;&#20998;&#35789;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Words to Music: A Study of Subword Tokenization Techniques in Symbolic Music Generation. (arXiv:2304.08953v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#23376;&#35789;&#20998;&#35789;&#25216;&#26415;&#22312;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#20855;&#32467;&#26500;&#30340;&#38899;&#20048;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;BPE&#26041;&#27861;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#21487;&#34892;&#19988;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#35789;&#20998;&#35789;&#22312;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#38543;&#30528;Transformer&#27169;&#22411;&#22312;&#31526;&#21495;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#25506;&#31350;&#23376;&#35789;&#20998;&#35789;&#22312;&#31526;&#21495;&#38899;&#20048;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#23376;&#35789;&#20998;&#35789;&#25216;&#26415;&#65288;&#22914;&#23383;&#33410;&#23545;&#32534;&#30721;BPE&#65289;&#21450;&#20854;&#23545;&#25152;&#29983;&#25104;&#27468;&#26354;&#25972;&#20307;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22522;&#20110;&#19977;&#31181;MIDI&#25968;&#25454;&#38598;&#65306;&#21333;&#38899;&#36712;&#26059;&#24459;&#12289;&#22810;&#36712;&#21333;&#20048;&#22120;&#21644;&#22810;&#36712;&#22810;&#20048;&#22120;&#12290;&#25105;&#20204;&#22312;&#38899;&#20048;&#26631;&#35760;&#21270;&#20043;&#21518;&#24212;&#29992;&#23376;&#35789;&#20998;&#35789;&#26041;&#26696;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#22312;&#30456;&#21516;&#26102;&#38388;&#20869;&#29983;&#25104;&#26356;&#38271;&#30340;&#27468;&#26354;&#65292;&#24182;&#19988;&#22312;&#32467;&#26500;&#25351;&#26631;&#65288;SI&#65289;&#12289;&#38899;&#39640;&#32423;&#21035;&#20449;&#24687;&#29109;&#31561;&#23458;&#35266;&#25351;&#26631;&#26041;&#38754;&#25913;&#21892;&#20102;&#29983;&#25104;&#30340;&#38899;&#20048;&#25972;&#20307;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#20004;&#31181;&#23376;&#35789;&#20998;&#35789;&#26041;&#27861;&#65292;BPE&#21644;&#19968;&#31181;&#22522;&#20110;&#38899;&#39640;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BPE&#26041;&#27861;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#21487;&#34892;&#19988;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subword tokenization has been widely successful in text-based natural language processing (NLP) tasks with Transformer-based models. As Transformer models become increasingly popular in symbolic music-related studies, it is imperative to investigate the efficacy of subword tokenization in the symbolic music domain. In this paper, we explore subword tokenization techniques, such as byte-pair encoding (BPE), in symbolic music generation and its impact on the overall structure of generated songs. Our experiments are based on three types of MIDI datasets: single track-melody only, multi-track with a single instrument, and multi-track and multi-instrument. We apply subword tokenization on post-musical tokenization schemes and find that it enables the generation of longer songs at the same time and improves the overall structure of the generated music in terms of objective metrics like structure indicator (SI), Pitch Class Entropy, etc. We also compare two subword tokenization methods, BPE a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#22870;&#21169;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#20154;&#26426;&#20132;&#20114;&#26469;&#25351;&#23450;&#20219;&#21153;&#30340;&#22870;&#21169;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#22312;&#25928;&#29575;&#19978;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.08944</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#22870;&#21169;&#23398;&#20064;&#23454;&#29616;&#21487;&#35777;&#26126;&#21453;&#39304;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning. (arXiv:2304.08944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#22870;&#21169;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#20154;&#26426;&#20132;&#20114;&#26469;&#25351;&#23450;&#20219;&#21153;&#30340;&#22870;&#21169;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#22312;&#25928;&#29575;&#19978;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#21512;&#36866;&#30340;&#22870;&#21169;&#20989;&#25968;&#23545;&#20110;&#26126;&#30830;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#21363;&#20351;&#23545;&#20110;&#31616;&#21333;&#30340;&#20219;&#21153;&#32780;&#35328;&#65292;&#35774;&#35745;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#20063;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20154;&#26426;&#20132;&#20114;&#24378;&#21270;&#23398;&#20064;&#65288;HiL RL&#65289;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#25552;&#20379;&#21508;&#31181;&#31867;&#22411;&#30340;&#21453;&#39304;&#26469;&#21521;RL&#20195;&#29702;&#20256;&#36798;&#22797;&#26434;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#32463;&#39564;&#25104;&#21151;&#65292;&#20294;HiL RL&#36890;&#24120;&#38656;&#35201;&#22826;&#22810;&#20154;&#31867;&#25945;&#24072;&#30340;&#21453;&#39304;&#65292;&#24182;&#19988;&#29702;&#35770;&#29702;&#35299;&#19981;&#36275;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#21453;&#39304;&#39640;&#25928;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#20197;&#37319;&#29992;&#20154;&#26426;&#20132;&#20114;&#26469;&#25351;&#23450;&#32473;&#23450;&#20219;&#21153;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;RL&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#39318;&#20808;&#22312;&#19981;&#25351;&#23450;&#22870;&#21169;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#29615;&#22659;&#65292;&#28982;&#21518;&#20165;&#35810;&#38382;&#20154;&#31867;&#25945;&#24072;&#26377;&#20851;&#26576;&#20123;&#29366;&#24577;&#21160;&#20316;&#23545;&#30340;&#20219;&#21153;&#22870;&#21169;&#30340;&#23569;&#37327;&#26597;&#35810;&#12290;&#20043;&#21518;&#65292;&#35813;&#31639;&#27861;&#23398;&#20064;&#20855;&#26377;&#21487;&#35777;&#26126;&#36951;&#25022;&#30028;&#38480;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;&#19982;&#20808;&#21069;&#30340;HiL RL&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22823;&#22823;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#22312;&#25928;&#29575;&#19978;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
An appropriate reward function is of paramount importance in specifying a task in reinforcement learning (RL). Yet, it is known to be extremely challenging in practice to design a correct reward function for even simple tasks. Human-in-the-loop (HiL) RL allows humans to communicate complex goals to the RL agent by providing various types of feedback. However, despite achieving great empirical successes, HiL RL usually requires too much feedback from a human teacher and also suffers from insufficient theoretical understanding. In this paper, we focus on addressing this issue from a theoretical perspective, aiming to provide provably feedback-efficient algorithmic frameworks that take human-in-the-loop to specify rewards of given tasks. We provide an active-learning-based RL algorithm that first explores the environment without specifying a reward function and then asks a human teacher for only a few queries about the rewards of a task at some state-action pairs. After that, the algorith
&lt;/p&gt;</description></item><item><title>ProGAP&#26159;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;GNN&#27169;&#22411;&#65292;&#37319;&#29992;&#36880;&#27493;&#35757;&#32451;&#26041;&#26696;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#36890;&#36807;&#23558;GNN&#20998;&#25104;&#19968;&#31995;&#21015;&#37325;&#21472;&#30340;&#23376;&#27169;&#22411;&#26469;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20445;&#25252;&#38544;&#31169;&#24182;&#20801;&#35768;&#26377;&#25928;&#23398;&#20064;&#22270;&#24418;&#32467;&#26500;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.08928</link><description>&lt;p&gt;
ProGAP: &#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#28176;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ProGAP: Progressive Graph Neural Networks with Differential Privacy Guarantees. (arXiv:2304.08928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08928
&lt;/p&gt;
&lt;p&gt;
ProGAP&#26159;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;GNN&#27169;&#22411;&#65292;&#37319;&#29992;&#36880;&#27493;&#35757;&#32451;&#26041;&#26696;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#36890;&#36807;&#23558;GNN&#20998;&#25104;&#19968;&#31995;&#21015;&#37325;&#21472;&#30340;&#23376;&#27169;&#22411;&#26469;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20445;&#25252;&#38544;&#31169;&#24182;&#20801;&#35768;&#26377;&#25928;&#23398;&#20064;&#22270;&#24418;&#32467;&#26500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#23398;&#20064;&#22270;&#24418;&#25968;&#25454;&#30340;&#24120;&#29992;&#24037;&#20855;&#65292;&#20294;&#24191;&#27867;&#20351;&#29992;&#24341;&#21457;&#20102;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#22270;&#24418;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#20010;&#20154;&#25110;&#25935;&#24863;&#20449;&#24687;&#12290;&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#24182;&#20801;&#35768;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#26377;&#25928;&#22788;&#29702;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;GNN&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22270;&#30340;&#22266;&#26377;&#32467;&#26500;&#36830;&#25509;&#24615;&#65292;GNN&#22312;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProGAP&#30340;&#26032;&#22411;&#24046;&#20998;&#38544;&#31169;GNN&#65292;&#37319;&#29992;&#36880;&#27493;&#35757;&#32451;&#26041;&#26696;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#32467;&#21512;&#32858;&#21512;&#25200;&#21160;&#25216;&#26415;&#20197;&#30830;&#20445;&#24046;&#20998;&#38544;&#31169;&#65292;ProGAP&#23558;GNN&#20998;&#25104;&#19968;&#31995;&#21015;&#37325;&#21472;&#30340;&#23376;&#27169;&#22411;&#65292;&#36880;&#27493;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#31532;&#19968;&#20010;&#23376;&#27169;&#22411;&#25193;&#23637;&#21040;&#23436;&#25972;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27599;&#20010;&#23376;&#27169;&#22411;&#37117;&#26159;&#22522;&#20110;&#36890;&#36807;&#31169;&#26377;&#32858;&#21512;&#23398;&#20064;&#21644;&#32531;&#23384;&#30340;&#33410;&#28857;&#23884;&#20837;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become a popular tool for learning on graphs, but their widespread use raises privacy concerns as graph data can contain personal or sensitive information. Differentially private GNN models have been recently proposed to preserve privacy while still allowing for effective learning over graph-structured datasets. However, achieving an ideal balance between accuracy and privacy in GNNs remains challenging due to the intrinsic structural connectivity of graphs. In this paper, we propose a new differentially private GNN called ProGAP that uses a progressive training scheme to improve such accuracy-privacy trade-offs. Combined with the aggregation perturbation technique to ensure differential privacy, ProGAP splits a GNN into a sequence of overlapping submodels that are trained progressively, expanding from the first submodel to the complete model. Specifically, each submodel is trained over the privately aggregated node embeddings learned and cached by the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20844;&#20849;&#20113;&#29615;&#22659;&#19979;DNN&#35757;&#32451;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#27969;&#31243;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#39044;&#22788;&#29702;&#26159;&#35757;&#32451;&#30340;&#29942;&#39048;&#65292;&#25552;&#20986;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.08925</link><description>&lt;p&gt;
&#20102;&#35299;&#25968;&#25454;&#39044;&#22788;&#29702;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31471;&#21040;&#31471;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Understand Data Preprocessing for Effective End-to-End Training of Deep Neural Networks. (arXiv:2304.08925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20844;&#20849;&#20113;&#29615;&#22659;&#19979;DNN&#35757;&#32451;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#27969;&#31243;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#39044;&#22788;&#29702;&#26159;&#35757;&#32451;&#30340;&#29942;&#39048;&#65292;&#25552;&#20986;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#20844;&#20849;&#20113;&#29615;&#22659;&#19979;DNN&#35757;&#32451;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#27969;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#23454;&#39564;&#27979;&#35797;&#65292;&#20197;&#27979;&#35797;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#25110;&#35760;&#24405;&#25991;&#20214;&#30340;&#20004;&#31181;&#20027;&#35201;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21551;&#29992;&#20102;NVIDIA DALI&#65288;&#39640;&#24230;&#20248;&#21270;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#24211;&#65289;&#30340;&#26368;&#39640;&#25928;&#36719;&#20214;&#21644;&#30828;&#20214;&#37197;&#32622;&#65292;&#25968;&#25454;&#39044;&#22788;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#26126;&#26174;&#30340;&#29942;&#39048;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#28508;&#22312;&#30340;&#21407;&#22240;&#65292;&#36816;&#29992;&#20102;&#21508;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#20026;&#8220;&#25968;&#25454;&#23384;&#20648;&#12289;&#21152;&#36733;&#31649;&#36947;&#8221;&#21644;&#8220;&#35757;&#32451;&#26694;&#26550;&#8221;&#30340;&#26032;&#21327;&#21516;&#35774;&#35745;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#28789;&#27963;&#36164;&#28304;&#37197;&#32622;&#25552;&#20379;&#21551;&#31034;&#65292;&#20197;&#20415;&#20805;&#20998;&#21033;&#29992;&#36164;&#28304;&#24182;&#26368;&#22823;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we primarily focus on understanding the data preprocessing pipeline for DNN Training in the public cloud. First, we run experiments to test the performance implications of the two major data preprocessing methods using either raw data or record files. The preliminary results show that data preprocessing is a clear bottleneck, even with the most efficient software and hardware configuration enabled by NVIDIA DALI, a high-optimized data preprocessing library. Second, we identify the potential causes, exercise a variety of optimization methods, and present their pros and cons. We hope this work will shed light on the new co-design of ``data storage, loading pipeline'' and ``training framework'' and flexible resource configurations between them so that the resources can be fully exploited and performance can be maximized.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#37327;&#23376;&#31639;&#27861;&#35299;&#20915;&#21333;&#24352;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#37319;&#29992;&#37327;&#23376;&#36864;&#28779;&#20248;&#21270;&#31639;&#27861;&#22312;&#20302;&#20998;&#36776;&#29575;&#31354;&#38388;&#20013;&#23547;&#25214;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2304.08924</link><description>&lt;p&gt;
&#37327;&#23376;&#36864;&#28779;&#29992;&#20110;&#21333;&#24352;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Quantum Annealing for Single Image Super-Resolution. (arXiv:2304.08924v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#37327;&#23376;&#31639;&#27861;&#35299;&#20915;&#21333;&#24352;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#37319;&#29992;&#37327;&#23376;&#36864;&#28779;&#20248;&#21270;&#31639;&#27861;&#22312;&#20302;&#20998;&#36776;&#29575;&#31354;&#38388;&#20013;&#23547;&#25214;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#21333;&#24352;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SISR&#65289;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;&#37327;&#23376;&#35745;&#31639;&#31639;&#27861;&#24212;&#29992;&#20110;&#36825;&#19968;&#37325;&#35201;&#30340;&#22270;&#20687;&#22686;&#24378;&#38382;&#39064;&#20013;&#65292;&#21363;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#21033;&#29992;AQC&#23376;&#31867;&#36136;&#23376;&#36864;&#28779;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#22312;&#20302;&#20998;&#36776;&#29575;&#31354;&#38388;&#20013;&#23547;&#25214;&#25152;&#38656;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26469;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a quantum computing-based algorithm to solve the single image super-resolution (SISR) problem. One of the well-known classical approaches for SISR relies on the well-established patch-wise sparse modeling of the problem. Yet, this field's current state of affairs is that deep neural networks (DNNs) have demonstrated far superior results than traditional approaches. Nevertheless, quantum computing is expected to become increasingly prominent for machine learning problems soon. As a result, in this work, we take the privilege to perform an early exploration of applying a quantum computing algorithm to this important image enhancement problem, i.e., SISR. Among the two paradigms of quantum computing, namely universal gate quantum computing and adiabatic quantum computing (AQC), the latter has been successfully applied to practical computer vision problems, in which quantum parallelism has been exploited to solve combinatorial optimization efficiently. This work demonst
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23039;&#24577;&#32422;&#26463;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#19968;&#33268;&#30340;&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#65292;&#24182;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#28145;&#24230;&#19981;&#19968;&#33268;&#24615;&#21644;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08916</link><description>&lt;p&gt;
&#23039;&#24577;&#32422;&#26463;&#29992;&#20110;&#19968;&#33268;&#30340;&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Pose Constraints for Consistent Self-supervised Monocular Depth and Ego-motion. (arXiv:2304.08916v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08916
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23039;&#24577;&#32422;&#26463;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#19968;&#33268;&#30340;&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#65292;&#24182;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#28145;&#24230;&#19981;&#19968;&#33268;&#24615;&#21644;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#19981;&#20165;&#23384;&#22312;&#23610;&#24230;&#27495;&#20041;&#65292;&#32780;&#19988;&#22312;&#23610;&#24230;&#26041;&#38754;&#30340;&#28145;&#24230;&#22270;&#25512;&#26029;&#19981;&#19968;&#33268;&#12290;&#23613;&#31649;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28040;&#38500;&#23610;&#24230;&#27495;&#20041;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#26159;&#26377;&#19968;&#33268;&#30340;&#23610;&#24230;&#39044;&#27979;&#21487;&#20197;&#22312;&#25512;&#26029;&#26399;&#38388;&#35745;&#31639;&#19968;&#27425;&#65292;&#28982;&#21518;&#22312;&#26102;&#38388;&#20869;&#20351;&#29992;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24341;&#20837;&#20102;&#19968;&#32452;&#26102;&#38388;&#19968;&#33268;&#24615;&#25439;&#22833;&#65292;&#20197;&#22312;&#26102;&#38388;&#19978;&#26368;&#23567;&#21270;&#23039;&#24577;&#19981;&#19968;&#33268;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#24341;&#20837;&#36825;&#20123;&#32422;&#26463;&#19981;&#20165;&#20943;&#23569;&#20102;&#28145;&#24230;&#19981;&#19968;&#33268;&#24615;&#65292;&#32780;&#19988;&#25552;&#39640;&#20102;&#28145;&#24230;&#21644;&#33258;&#25105;&#36816;&#21160;&#39044;&#27979;&#30340;&#22522;&#32447;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised monocular depth estimation approaches suffer not only from scale ambiguity but also infer temporally inconsistent depth maps w.r.t. scale. While disambiguating scale during training is not possible without some kind of ground truth supervision, having scale consistent depth predictions would make it possible to calculate scale once during inference as a post-processing step and use it over-time. With this as a goal, a set of temporal consistency losses that minimize pose inconsistencies over time are introduced. Evaluations show that introducing these constraints not only reduces depth inconsistencies but also improves the baseline performance of depth and ego-motion prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;DGP&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#24494;&#20998;&#31526;&#21495;&#26641;&#26500;&#24314;&#36951;&#20256;&#32534;&#31243;&#26641;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#39640;&#32500;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.08915</link><description>&lt;p&gt;
&#39640;&#32500;&#31526;&#21495;&#22238;&#24402;&#30340;&#21487;&#24494;&#20998;&#36951;&#20256;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Differentiable Genetic Programming for High-dimensional Symbolic Regression. (arXiv:2304.08915v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;DGP&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#24494;&#20998;&#31526;&#21495;&#26641;&#26500;&#24314;&#36951;&#20256;&#32534;&#31243;&#26641;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#39640;&#32500;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#20174;&#25968;&#23398;&#34920;&#36798;&#24335;&#20013;&#21457;&#29616;&#25968;&#25454;&#38388;&#38544;&#34255;&#20851;&#31995;&#30340;&#36807;&#31243;&#65292;&#34987;&#35270;&#20026;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#25928;&#26041;&#24335;&#12290;&#36951;&#20256;&#32534;&#31243;&#26159;&#35299;&#20915;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#20256;&#32479;&#30340;&#36951;&#20256;&#32534;&#31243;&#30340;&#38543;&#26426;&#36827;&#21270;&#24615;&#36136;&#36896;&#25104;&#20854;&#22312;&#35299;&#20915;&#39640;&#32500;&#23454;&#38469;&#38382;&#39064;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;iable&#30340;&#26041;&#27861;&#8212;&#8212;DGP&#65292;&#39318;&#27425;&#26500;&#24314;&#20102;&#29992;&#20110;&#39640;&#32500;&#31526;&#21495;&#22238;&#24402;&#30340;&#36951;&#20256;&#32534;&#31243;&#26641;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#21487;&#24494;&#20998;&#31526;&#21495;&#26641;&#30340;&#26032;&#25968;&#25454;&#32467;&#26500;&#65292;&#23558;&#31163;&#25955;&#32467;&#26500;&#26494;&#24347;&#21040;&#36830;&#32493;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#26469;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#28040;&#38500;&#30001;&#27492;&#26494;&#24347;&#24341;&#36215;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#26377;&#25928;&#30340;&#31526;&#21495;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) is the process of discovering hidden relationships from data with mathematical expressions, which is considered an effective way to reach interpretable machine learning (ML). Genetic programming (GP) has been the dominator in solving SR problems. However, as the scale of SR problems increases, GP often poorly demonstrates and cannot effectively address the real-world high-dimensional problems. This limitation is mainly caused by the stochastic evolutionary nature of traditional GP in constructing the trees. In this paper, we propose a differentiable approach named DGP to construct GP trees towards high-dimensional SR for the first time. Specifically, a new data structure called differentiable symbolic tree is proposed to relax the discrete structure to be continuous, thus a gradient-based optimizer can be presented for the efficient optimization. In addition, a sampling method is proposed to eliminate the discrepancy caused by the above relaxation for valid sym
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#20551;&#35774;&#65292;&#21457;&#29616;&#20102;Grassmannian Frame&#32467;&#26500;&#21644;&#23545;&#31216;&#27867;&#21270;&#29616;&#35937;&#65292;&#36825;&#23545;&#29305;&#24449;&#36873;&#25321;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#37117;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.08914</link><description>&lt;p&gt;
&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#30340;&#30740;&#31350;&#65306;Grassmannian Frame&#12289;&#23545;&#31216;&#24615;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Study of Neural Collapse Phenomenon: Grassmannian Frame, Symmetry, Generalization. (arXiv:2304.08914v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#20551;&#35774;&#65292;&#21457;&#29616;&#20102;Grassmannian Frame&#32467;&#26500;&#21644;&#23545;&#31216;&#27867;&#21270;&#29616;&#35937;&#65292;&#36825;&#23545;&#29305;&#24449;&#36873;&#25321;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#37117;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#24191;&#20041;&#31070;&#32463;&#23849;&#28291;&#20551;&#35774;&#25512;&#24191;&#20102;&#21407;&#22987;&#30340;&#31070;&#32463;&#23849;&#28291;&#29616;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#31867;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#24471;&#21040;&#20102;Grassmannian Frame&#32467;&#26500;&#12290;&#35813;&#32467;&#26500;&#22312;&#29699;&#38754;&#19978;&#26368;&#22823;&#21270;&#22320;&#20998;&#31163;&#20102;&#27599;&#20004;&#20010;&#31867;&#21035;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#19968;&#20010;&#26356;&#22823;&#30340;&#29305;&#24449;&#32500;&#24230;&#12290;&#20986;&#20110;&#23545;Grassmannian Frame&#23545;&#31216;&#24615;&#30340;&#22909;&#22855;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25506;&#32034;&#19981;&#21516;Grassmannian Frame&#27169;&#22411;&#26159;&#21542;&#20250;&#20135;&#29983;&#19981;&#21516;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#25105;&#20204;&#21457;&#29616;&#20102;&#23545;&#31216;&#27867;&#21270;&#29616;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#32622;&#25442;&#23545;&#31216;&#27867;&#21270;&#30340;&#23450;&#29702;&#12290;&#28982;&#32780;&#65292;&#20026;&#20160;&#20040;&#29305;&#24449;&#30340;&#19981;&#21516;&#26041;&#21521;&#20250;&#23548;&#33268;&#22914;&#27492;&#19981;&#21516;&#30340;&#27867;&#21270;&#29616;&#35937;&#30340;&#38382;&#39064;&#20173;&#28982;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we extends original Neural Collapse Phenomenon by proving Generalized Neural Collapse hypothesis. We obtain Grassmannian Frame structure from the optimization and generalization of classification. This structure maximally separates features of every two classes on a sphere and does not require a larger feature dimension than the number of classes. Out of curiosity about the symmetry of Grassmannian Frame, we conduct experiments to explore if models with different Grassmannian Frames have different performance. As a result, we discover the Symmetric Generalization phenomenon. We provide a theorem to explain Symmetric Generalization of permutation. However, the question of why different directions of features can lead to such different generalization is still open for future investigation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#65292;&#22312;&#20445;&#35777;&#30828;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#24037;&#31243;&#24037;&#20316;&#65292;&#38477;&#20302;&#24314;&#27169;&#20559;&#24046;&#65292;&#24182;&#36991;&#20813;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.08897</link><description>&lt;p&gt;
&#24102;&#26377;&#33258;&#25105;&#25913;&#36827;&#30828;&#32422;&#26463;&#30340;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning with self-improving hard constraints for multi-energy management systems. (arXiv:2304.08897v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#65292;&#22312;&#20445;&#35777;&#30828;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#24037;&#31243;&#24037;&#20316;&#65292;&#38477;&#20302;&#24314;&#27169;&#20559;&#24046;&#65292;&#24182;&#36991;&#20813;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#30828;&#32422;&#26463;&#20445;&#35777;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26159;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20013;&#26368;&#26377;&#21069;&#36884;&#30340;&#26368;&#20248;&#25511;&#21046;&#26041;&#21521;&#12290;&#23427;&#21482;&#38656;&#35201;&#22312;&#29615;&#22659;&#29305;&#23450;&#30340;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#19978;&#39044;&#20808;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#27169;&#22411;&#65288;&#21363;&#26893;&#29289;&#65292;&#24178;&#25200;&#21644;&#22122;&#22768;&#27169;&#22411;&#65292;&#20197;&#21450;&#26410;&#21253;&#25324;&#22312;&#26893;&#29289;&#27169;&#22411;&#20013;&#30340;&#29366;&#24577;&#30340;&#39044;&#27979;&#27169;&#22411; - &#20363;&#22914;&#38656;&#27714;&#65292;&#22825;&#27668;&#21644;&#20215;&#26684;&#39044;&#27979;&#65289;&#12290;&#22240;&#27492;&#65292;&#21487;&#20943;&#23569;&#39033;&#30446;&#29305;&#23450;&#30340;&#21069;&#26399;&#21644;&#25345;&#32493;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#20173;&#21487;&#20197;&#23398;&#20064;&#26356;&#22909;&#22320;&#34920;&#31034;&#22522;&#30784;&#31995;&#32479;&#21160;&#24577;&#65292;&#24182;&#20351;&#24314;&#27169;&#20559;&#24046;&#26368;&#23567;&#21270;&#65288;&#26080;&#22522;&#20110;&#27169;&#22411;&#30340;&#30446;&#26631;&#20989;&#25968;&#65289;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#20165;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#26377;&#26102;&#20063;&#19981;&#24635;&#26159;&#23481;&#26131;&#25552;&#20379;&#20934;&#30830;&#30340;&#20808;&#39564;&#65288;&#20363;&#22914;&#33021;&#37327;&#24179;&#34913;&#32422;&#26463;&#38656;&#35201;&#35814;&#32454;&#30830;&#23450;&#25152;&#26377;&#33021;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#65289;&#65292;&#20174;&#32780;&#23548;&#33268;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#36827;&#23637;&#65306;&#65288;I&#65289;&#23558;Optlayer&#21644;SafeFallback&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21629;&#21517;&#20026;O
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning (RL) with hard constraint guarantees is a promising optimal control direction for multi-energy management systems. It only requires the environment-specific constraint functions itself a prior and not a complete model (i.e. plant, disturbance and noise models, and prediction models for states not included in the plant model - e.g. demand, weather, and price forecasts). The project-specific upfront and ongoing engineering efforts are therefore still reduced, better representations of the underlying system dynamics can still be learned and modeling bias is kept to a minimum (no model-based objective function). However, even the constraint functions alone are not always trivial to accurately provide in advance (e.g. an energy balance constraint requires the detailed determination of all energy inputs and outputs), leading to potentially unsafe behavior. In this paper, we present two novel advancements: (I) combining the Optlayer and SafeFallback method, named O
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#32452;&#19981;&#21516;&#25968;&#25454;&#26679;&#26412;&#30340;&#27169;&#22411;&#31867;&#65292;&#36890;&#36807;&#35843;&#25972;&#36739;&#23569;&#30340;&#21442;&#25968;&#21363;&#21487;&#20026;&#26032;&#30340;&#12289;&#29305;&#23450;&#30340;&#38382;&#39064;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36164;&#20135;&#31649;&#29702;&#21644;&#38134;&#34892;&#19994;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.08883</link><description>&lt;p&gt;
&#37329;&#34701;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Parameterized Neural Networks for Finance. (arXiv:2304.08883v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#32452;&#19981;&#21516;&#25968;&#25454;&#26679;&#26412;&#30340;&#27169;&#22411;&#31867;&#65292;&#36890;&#36807;&#35843;&#25972;&#36739;&#23569;&#30340;&#21442;&#25968;&#21363;&#21487;&#20026;&#26032;&#30340;&#12289;&#29305;&#23450;&#30340;&#38382;&#39064;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36164;&#20135;&#31649;&#29702;&#21644;&#38134;&#34892;&#19994;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#21644;&#20998;&#26512;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#32452;&#19981;&#21516;&#25968;&#25454;&#26679;&#26412;&#30340;&#27169;&#22411;&#31867;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#26679;&#26412;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;&#20174;&#36825;&#20010;&#24847;&#20041;&#19978;&#35762;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#22240;&#20026;&#22312;&#23398;&#20064;&#20102;&#36825;&#31181;&#27169;&#22411;&#31867;&#20043;&#21518;&#65292;&#20165;&#38656;&#35201;&#35843;&#25972;&#23569;&#37327;&#21442;&#25968;&#21363;&#21487;&#20026;&#26032;&#30340;&#12289;&#29305;&#23450;&#30340;&#38382;&#39064;&#24314;&#27169;&#12290;&#22312;&#29702;&#35770;&#20998;&#26512;&#21644;&#19981;&#21516;&#19968;&#32500;&#38382;&#39064;&#30340;&#22238;&#24402;&#23454;&#20363;&#20043;&#21518;&#65292;&#25105;&#20204;&#26368;&#32456;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#36164;&#20135;&#31649;&#29702;&#20844;&#21496;&#21644;&#38134;&#34892;&#38754;&#20020;&#30340;&#26631;&#20934;&#38382;&#39064;&#20043;&#19968;&#65306;&#35843;&#25972;&#25910;&#30410;&#29575;&#26354;&#32447;&#12290;&#21576;&#29616;&#30340;&#32467;&#26524;&#28165;&#26970;&#22320;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#24212;&#29992;&#23545;&#37329;&#34701;&#20174;&#19994;&#20154;&#21592;&#29305;&#21035;&#24863;&#20852;&#36259;&#65292;&#22240;&#20026;&#20960;&#20046;&#25152;&#26377;&#25317;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#36164;&#20135;&#31649;&#29702;&#20844;&#21496;&#21644;&#38134;&#34892;&#22312;ESG&#35780;&#32423;&#26041;&#38754;&#20063;&#38656;&#35201;&#35843;&#25972;&#25110;&#29978;&#33267;&#25913;&#21464;&#20182;&#20204;&#24403;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss and analyze a neural network architecture, that enables learning a model class for a set of different data samples rather than just learning a single model for a specific data sample. In this sense, it may help to reduce the overfitting problem, since, after learning the model class over a larger data sample consisting of such different data sets, just a few parameters need to be adjusted for modeling a new, specific problem. After analyzing the method theoretically and by regression examples for different one-dimensional problems, we finally apply the approach to one of the standard problems asset managers and banks are facing: the calibration of spread curves. The presented results clearly show the potential that lies within this method. Furthermore, this application is of particular interest to financial practitioners, since nearly all asset managers and banks which are having solutions in place may need to adapt or even change their current methodologies when ESG ratings
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#22810;&#20013;&#24515;MRI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#27531;&#30041;&#32959;&#30244;&#20998;&#21106;&#65292;&#21487;&#25552;&#39640;&#27531;&#30041;&#32959;&#30244;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.08881</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26089;&#26399;&#22810;&#27169;&#24577; MRI &#20013;&#20998;&#21106;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;
&lt;/p&gt;
&lt;p&gt;
Segmentation of glioblastomas in early post-operative multi-modal MRI with deep neural networks. (arXiv:2304.08881v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08881
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#22810;&#20013;&#24515;MRI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#27531;&#30041;&#32959;&#30244;&#20998;&#21106;&#65292;&#21487;&#25552;&#39640;&#27531;&#30041;&#32959;&#30244;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#21518;&#20999;&#38500;&#31243;&#24230;&#26159;&#35786;&#26029;&#20026;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#24739;&#32773;&#30340;&#20027;&#35201;&#39044;&#21518;&#22240;&#32032;&#20043;&#19968;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20934;&#30830;&#22320;&#23558;&#26415;&#21518; MR &#22270;&#20687;&#20013;&#30340;&#27531;&#30041;&#32959;&#30244;&#36827;&#34892;&#20998;&#21106;&#21644;&#20998;&#31867;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#29992;&#20110;&#20272;&#35745;&#27531;&#30041;&#32959;&#30244;&#30340;&#26631;&#20934;&#26041;&#27861;&#23384;&#22312;&#39640;&#24230;&#30340;&#24615;&#38388;&#21644;&#24615;&#20869;&#35780;&#20998;&#20154;&#21592;&#24046;&#24322;&#24615;&#65292;&#32780;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#21487;&#22312;&#26089;&#26399;&#26415;&#21518; MRI &#20013;&#23454;&#29616;&#27531;&#30041;&#32959;&#30244;&#30340;&#20998;&#21106;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#20999;&#38500;&#31243;&#24230;&#12290;&#26412;&#30740;&#31350;&#35757;&#32451;&#20102;&#20004;&#31181;&#26368;&#26032;&#30340;&#39044;&#25805;&#20316;&#20998;&#21106;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20197;&#23436;&#25104;&#27492;&#20219;&#21153;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#28085;&#30422;&#27431;&#27954;&#21644;&#32654;&#22269;12&#23478;&#21307;&#38498;&#30340;&#36817;1000&#21517;&#24739;&#32773;&#30340;&#22810;&#20013;&#24515;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#24191;&#27867;&#39564;&#35777;&#12290;&#26368;&#20339;&#24615;&#33021;&#36798;&#21040;&#20102;61\%&#30340;Dice&#20998;&#25968;&#21644;&#32422;80\%&#30340;&#22343;&#34913;&#20934;&#30830;&#24615;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#21307;&#38498;&#20043;&#38388;&#26377;&#25928;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26368;&#20339;&#27169;&#22411;&#30340;&#20998;&#21106;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extent of resection after surgery is one of the main prognostic factors for patients diagnosed with glioblastoma. To achieve this, accurate segmentation and classification of residual tumor from post-operative MR images is essential. The current standard method for estimating it is subject to high inter- and intra-rater variability, and an automated method for segmentation of residual tumor in early post-operative MRI could lead to a more accurate estimation of extent of resection. In this study, two state-of-the-art neural network architectures for pre-operative segmentation were trained for the task. The models were extensively validated on a multicenter dataset with nearly 1000 patients, from 12 hospitals in Europe and the United States. The best performance achieved was a 61\% Dice score, and the best classification performance was about 80\% balanced accuracy, with a demonstrated ability to generalize across hospitals. In addition, the segmentation performance of the best models w
&lt;/p&gt;</description></item><item><title>NPS&#26159;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31243;&#24207;&#37319;&#26679;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#25191;&#34892;&#23884;&#20837;&#24182;&#24555;&#36895;&#29983;&#25104;&#20195;&#34920;&#24615;&#27169;&#25311;&#28857;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24494;&#22788;&#29702;&#22120;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.08880</link><description>&lt;p&gt;
NPS: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#31934;&#20934;&#31243;&#24207;&#37319;&#26679;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NPS: A Framework for Accurate Program Sampling Using Graph Neural Network. (arXiv:2304.08880v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08880
&lt;/p&gt;
&lt;p&gt;
NPS&#26159;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31243;&#24207;&#37319;&#26679;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#25191;&#34892;&#23884;&#20837;&#24182;&#24555;&#36895;&#29983;&#25104;&#20195;&#34920;&#24615;&#27169;&#25311;&#28857;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24494;&#22788;&#29702;&#22120;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25705;&#23572;&#23450;&#24459;&#32467;&#26463;&#65292;&#29616;&#20195;&#22788;&#29702;&#22120;&#65288;&#22914;RISC-V&#33258;&#23450;&#20041;&#25193;&#23637;&#65289;&#38656;&#35201;&#24555;&#36895;&#30340;&#26550;&#26500;&#21019;&#26032;&#26469;&#32500;&#25345;&#24615;&#33021;&#22686;&#38271;&#65292;&#31243;&#24207;&#37319;&#26679;&#26159;&#24494;&#22788;&#29702;&#22120;&#35774;&#35745;&#20013;&#20851;&#38190;&#30340;&#19968;&#27493;&#65292;&#22240;&#20026;&#23427;&#36873;&#25321;&#24037;&#20316;&#37327;&#27169;&#25311;&#30340;&#20195;&#34920;&#24615;&#27169;&#25311;&#28857;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#31243;&#24207;&#37319;&#26679;&#65288;NPS&#65289;&#65292;&#23427;&#20351;&#29992;&#21160;&#24577;&#24555;&#29031;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25191;&#34892;&#23884;&#20837;&#12290;NPS&#37319;&#29992;AssemblyNet&#36827;&#34892;&#23884;&#20837;&#29983;&#25104;&#65292;&#21033;&#29992;&#24212;&#29992;&#31243;&#24207;&#30340;&#20195;&#30721;&#32467;&#26500;&#21644;&#36816;&#34892;&#26102;&#29366;&#24577;&#65292;&#23558;AssemblyNet&#20316;&#20026;NPS&#30340;&#22270;&#27169;&#22411;&#21644;&#31070;&#32463;&#26550;&#26500;&#65292;&#25429;&#33719;&#31243;&#24207;&#30340;&#34892;&#20026;&#65292;&#22312;&#25968;&#25454;&#35745;&#31639;&#12289;&#20195;&#30721;&#36335;&#24452;&#21644;&#25968;&#25454;&#27969;&#31561;&#26041;&#38754;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;NPS&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;SimPoint&#65292;&#22312;&#25552;&#39640;&#24037;&#20316;&#36127;&#36733;&#20195;&#34920;&#24615;&#29575;&#30340;&#21516;&#26102;&#65292;&#23558;&#27169;&#25311;&#26102;&#38388;&#32553;&#30701;&#39640;&#36798;60%&#12290;
&lt;/p&gt;
&lt;p&gt;
With the end of Moore's Law, there is a growing demand for rapid architectural innovations in modern processors, such as RISC-V custom extensions, to continue performance scaling. Program sampling is a crucial step in microprocessor design, as it selects representative simulation points for workload simulation. While SimPoint has been the de-facto approach for decades, its limited expressiveness with Basic Block Vector (BBV) requires time-consuming human tuning, often taking months, which impedes fast innovation and agile hardware development. This paper introduces Neural Program Sampling (NPS), a novel framework that learns execution embeddings using dynamic snapshots of a Graph Neural Network. NPS deploys AssemblyNet for embedding generation, leveraging an application's code structures and runtime states. AssemblyNet serves as NPS's graph model and neural architecture, capturing a program's behavior in aspects such as data computation, code path, and data flow. AssemblyNet is trained
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#21033;&#29992;&#22823;&#35268;&#27169;&#36716;&#20889;&#26469;&#25552;&#21319;&#22823;&#22411;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22788;&#29702;&#20302;&#36164;&#28304;&#21644;&#26410;&#30693;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;UROMAN&#36716;&#20889;&#24037;&#20855;&#30340;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#39640;&#25928;&#30340;&#31574;&#30053;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;&#35821;&#35328;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.08865</link><description>&lt;p&gt;
&#22522;&#20110;&#32599;&#39532;&#21270;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#22823;&#35268;&#27169;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Romanization-based Large-scale Adaptation of Multilingual Language Models. (arXiv:2304.08865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#21033;&#29992;&#22823;&#35268;&#27169;&#36716;&#20889;&#26469;&#25552;&#21319;&#22823;&#22411;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22788;&#29702;&#20302;&#36164;&#28304;&#21644;&#26410;&#30693;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;UROMAN&#36716;&#20889;&#24037;&#20855;&#30340;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#39640;&#25928;&#30340;&#31574;&#30053;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;&#35821;&#35328;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;mPLMs&#65289;&#24050;&#25104;&#20026;&#36328;&#35821;&#35328;NLP&#20013;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;&#35768;&#22810;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#26041;&#38754;&#21463;&#21040;&#35832;&#22810;&#38480;&#21046;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#12289;&#35789;&#27719;&#37327;&#22686;&#21152;&#21644;&#21442;&#25968;&#39044;&#31639;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#22686;&#24378;mPLMs&#22788;&#29702;&#20302;&#36164;&#28304;&#21644;&#26410;&#30693;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25506;&#32034;&#22823;&#35268;&#27169;&#21033;&#29992;&#36716;&#20889;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#32034;UROMAN&#36716;&#20889;&#24037;&#20855;&#30340;&#28508;&#21147;&#65292;&#35813;&#24037;&#20855;&#20026;&#25152;&#26377;&#20070;&#20889;&#31995;&#32479;&#25552;&#20379;&#20102;&#20174;UTF-8&#21040;&#25289;&#19969;&#23383;&#31526;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20960;&#20046;&#20219;&#20309;&#35821;&#35328;&#30340;&#24265;&#20215;&#32599;&#39532;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;UROMAN&#30456;&#23545;&#20110;&#20854;&#20182;&#35821;&#35328;&#29305;&#23450;&#21644;&#25163;&#21160;&#31574;&#21010;&#30340;&#36716;&#20889;&#24037;&#20855;&#22312;&#36866;&#24212;&#22810;&#35821;&#35328;PLMs&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#24182;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#31574;&#30053;&#65292;&#20197;&#36866;&#24212;&#32599;&#39532;&#21270;&#21644;&#38750;&#32599;&#39532;&#21270;&#30340;14&#31181;&#19981;&#21516;&#20197;&#19978;&#35821;&#35328;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multilingual pretrained language models (mPLMs) have become the de facto state of the art for cross-lingual transfer in NLP. However, their large-scale deployment to many languages, besides pretraining data scarcity, is also hindered by the increase in vocabulary size and limitations in their parameter budget. In order to boost the capacity of mPLMs to deal with low-resource and unseen languages, we explore the potential of leveraging transliteration on a massive scale. In particular, we explore the UROMAN transliteration tool, which provides mappings from UTF-8 to Latin characters for all the writing systems, enabling inexpensive romanization for virtually any language. We first focus on establishing how UROMAN compares against other language-specific and manually curated transliterators for adapting multilingual PLMs. We then study and compare a plethora of data- and parameter-efficient strategies for adapting the mPLMs to romanized and non-romanized corpora of 14 diverse low-r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#39564;&#35780;&#20272;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#26681;&#25454;&#23454;&#39564;&#20998;&#26512;&#65292;&#38543;&#26426;&#26862;&#26519;&#26159;&#19968;&#31181;&#23545;&#21327;&#21464;&#37327;&#31227;&#20301;&#36739;&#19981;&#25935;&#24863;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.08855</link><description>&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#21306;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#40065;&#26834;&#24615;&#23545;&#21327;&#21464;&#37327;&#20559;&#31227;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Domain-Region Based Evaluation of ML Performance Robustness to Covariate Shift. (arXiv:2304.08855v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#39564;&#35780;&#20272;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#26681;&#25454;&#23454;&#39564;&#20998;&#26512;&#65292;&#38543;&#26426;&#26862;&#26519;&#26159;&#19968;&#31181;&#23545;&#21327;&#21464;&#37327;&#31227;&#20301;&#36739;&#19981;&#25935;&#24863;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37117;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#30340;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#31181;&#31283;&#23450;&#24615;&#36890;&#24120;&#19981;&#33021;&#28385;&#36275;&#65292;&#23548;&#33268;&#20102;&#25152;&#23398;&#27169;&#22411;&#22312;&#37096;&#32626;&#26102;&#20986;&#29616;&#20102;&#24847;&#22806;&#30340;&#34920;&#29616;&#12290;&#24403;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36755;&#20837;&#30340;&#27010;&#29575;&#20998;&#24067;&#19981;&#21516;&#65292;&#20294;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#20445;&#25345;&#19981;&#21464;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#21327;&#21464;&#37327;&#36716;&#31227;&#12290;&#26412;&#25991;&#23454;&#39564;&#35780;&#20272;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#22495;&#36827;&#34892;&#20998;&#35299;&#65292;&#36827;&#34892;&#20102;&#22522;&#20110;&#21306;&#22495;&#30340;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20998;&#31867;&#22120;&#22312;&#27599;&#20010;&#22495;&#21306;&#22495;&#30340;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#20108;&#32500;&#20998;&#31867;&#38382;&#39064;&#20013;&#27169;&#25311;&#20102;&#20998;&#24067;&#21464;&#21270;&#65292;&#38543;&#21518;&#36827;&#34892;&#20102;&#26356;&#39640;&#32500;&#24230;&#30340;&#22235;&#32500;&#23454;&#39564;&#12290;&#26681;&#25454;&#23454;&#39564;&#20998;&#26512;&#65292;&#38543;&#26426;&#26862;&#26519;&#26159;&#19968;&#31181;&#23545;&#21327;&#21464;&#37327;&#31227;&#20301;&#36739;&#19981;&#25935;&#24863;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most machine learning methods assume that the input data distribution is the same in the training and testing phases. However, in practice, this stationarity is usually not met and the distribution of inputs differs, leading to unexpected performance of the learned model in deployment. The issue in which the training and test data inputs follow different probability distributions while the input-output relationship remains unchanged is referred to as covariate shift. In this paper, the performance of conventional machine learning models was experimentally evaluated in the presence of covariate shift. Furthermore, a region-based evaluation was performed by decomposing the domain of probability density function of the input data to assess the classifier's performance per domain region. Distributional changes were simulated in a two-dimensional classification problem. Subsequently, a higher four-dimensional experiments were conducted. Based on the experimental analysis, the Random Forests
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32858;&#28966;&#20110;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;BadVFL&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#21518;&#38376;&#27880;&#20837;VFL&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25104;&#21151;&#29575;&#39640;&#24182;&#19988;&#35823;&#20998;&#31867;&#29575;&#24456;&#20302;&#12290;</title><link>http://arxiv.org/abs/2304.08847</link><description>&lt;p&gt;
BadVFL: &#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BadVFL: Backdoor Attacks in Vertical Federated Learning. (arXiv:2304.08847v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32858;&#28966;&#20110;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;BadVFL&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#21518;&#38376;&#27880;&#20837;VFL&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25104;&#21151;&#29575;&#39640;&#24182;&#19988;&#35823;&#20998;&#31867;&#29575;&#24456;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#21442;&#19982;&#32773;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#20854;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#20316;&#22320;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65307;&#30456;&#21453;&#65292;&#20182;&#20204;&#22312;&#26412;&#22320;&#35757;&#32451;&#33258;&#24049;&#30340;&#27169;&#22411;&#65292;&#24182;&#23558;&#26356;&#26032;&#21457;&#36865;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#32858;&#21512;&#12290;&#26681;&#25454;&#25968;&#25454;&#22312;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20998;&#24067;&#26041;&#24335;&#65292;FL&#21487;&#20197;&#20998;&#20026;&#27700;&#24179;&#65288;HFL&#65289;&#21644;&#31446;&#30452;&#65288;VFL&#65289;&#12290;&#22312;VFL&#20013;&#65292;&#21442;&#19982;&#32773;&#20849;&#20139;&#30456;&#21516;&#30340;&#35757;&#32451;&#23454;&#20363;&#38598;&#65292;&#20294;&#20165;&#25176;&#31649;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#30340;&#19981;&#21516;&#21644;&#38750;&#37325;&#21472;&#23376;&#38598;&#12290;&#32780;&#22312;HFL&#20013;&#65292;&#27599;&#20010;&#21442;&#19982;&#32773;&#20849;&#20139;&#30456;&#21516;&#30340;&#29305;&#24449;&#38598;&#65292;&#32780;&#35757;&#32451;&#38598;&#34987;&#20998;&#20026;&#26412;&#22320;&#25317;&#26377;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#12290;&#23613;&#31649;VFL&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#31561;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#20294;&#24456;&#23569;&#26377;&#24037;&#20316;&#20998;&#26512;&#20854;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;VFL&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25163;&#35797;&#22270;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25805;&#32437;&#32858;&#21512;&#27169;&#22411;&#20197;&#35302;&#21457;&#38169;&#35823;&#20998;&#31867;&#12290;&#22312;VFL&#19978;&#25191;&#34892;&#21518;&#38376;&#25915;&#20987;&#21487;&#20197;&#21019;&#24314;&#20005;&#37325;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20801;&#35768;&#25915;&#20987;&#32773;&#26377;&#38024;&#23545;&#24615;&#22320;&#25511;&#21046;&#27169;&#22411;&#39044;&#27979;&#30340;&#32467;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;VFL&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65292;&#31216;&#20026;BadVFL&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#21518;&#38376;&#27880;&#20837;VFL&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#36824;&#36866;&#24212;&#20110;VFL&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#20013;&#24515;&#24230;&#37327;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#30340;&#20302;&#22833;&#30495;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple parties to collaboratively train a machine learning model without sharing their data; rather, they train their own model locally and send updates to a central server for aggregation. Depending on how the data is distributed among the participants, FL can be classified into Horizontal (HFL) and Vertical (VFL). In VFL, the participants share the same set of training instances but only host a different and non-overlapping subset of the whole feature space. Whereas in HFL, each participant shares the same set of features while the training set is split into locally owned training data subsets.  VFL is increasingly used in applications like financial fraud detection; nonetheless, very little work has analyzed its security. In this paper, we focus on robustness in VFL, in particular, on backdoor attacks, whereby an adversary attempts to manipulate the aggregate model during the training process to trigger misclassifications. Performing backdoor attack
&lt;/p&gt;</description></item><item><title>&#21487;&#34892;&#24615;&#31574;&#30053;&#36845;&#20195; (FPI) &#26159;&#19968;&#20010;&#38388;&#25509;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#19978;&#19968;&#20010;&#31574;&#30053;&#30340;&#21487;&#34892;&#22495;&#26469;&#36845;&#20195;&#22320;&#38480;&#21046;&#24403;&#21069;&#31574;&#30053;&#12290;&#21487;&#34892;&#24615;&#31574;&#30053;&#25913;&#36827;&#26159;&#20854;&#26680;&#24515;&#65292;&#23427;&#22312;&#21487;&#34892;&#22495;&#20869;&#26368;&#22823;&#21270;&#22238;&#25253;&#65292;&#22312;&#21487;&#34892;&#22495;&#22806;&#26368;&#23567;&#21270;&#32422;&#26463;&#34928;&#20943;&#20989;&#25968; (CDF).</title><link>http://arxiv.org/abs/2304.08845</link><description>&lt;p&gt;
&#21487;&#34892;&#24615;&#31574;&#30053;&#36845;&#20195;
&lt;/p&gt;
&lt;p&gt;
Feasible Policy Iteration. (arXiv:2304.08845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08845
&lt;/p&gt;
&lt;p&gt;
&#21487;&#34892;&#24615;&#31574;&#30053;&#36845;&#20195; (FPI) &#26159;&#19968;&#20010;&#38388;&#25509;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#19978;&#19968;&#20010;&#31574;&#30053;&#30340;&#21487;&#34892;&#22495;&#26469;&#36845;&#20195;&#22320;&#38480;&#21046;&#24403;&#21069;&#31574;&#30053;&#12290;&#21487;&#34892;&#24615;&#31574;&#30053;&#25913;&#36827;&#26159;&#20854;&#26680;&#24515;&#65292;&#23427;&#22312;&#21487;&#34892;&#22495;&#20869;&#26368;&#22823;&#21270;&#22238;&#25253;&#65292;&#22312;&#21487;&#34892;&#22495;&#22806;&#26368;&#23567;&#21270;&#32422;&#26463;&#34928;&#20943;&#20989;&#25968; (CDF).
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#22312;&#23433;&#20840;&#32422;&#26463;&#19979;&#35299;&#20915;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340; $\textit{&#30452;&#25509;}$ &#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20250;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#19968;&#30452;&#20351;&#29992;&#21407;&#22987;&#32422;&#26463;&#12290;&#23427;&#20204;&#25110;&#32773;&#32570;&#20047;&#31574;&#30053;&#36845;&#20195;&#26399;&#38388;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#25110;&#32773;&#36973;&#36935;&#19981;&#21487;&#34892;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;&#21487;&#34892;&#24615;&#31574;&#30053;&#36845;&#20195;&#65288;FPI&#65289;&#30340; $\textit{&#38388;&#25509;}$ &#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#26368;&#21518;&#19968;&#20010;&#31574;&#30053;&#30340;&#21487;&#34892;&#22495;&#26469;&#36845;&#20195;&#22320;&#38480;&#21046;&#24403;&#21069;&#31574;&#30053;&#12290;&#21487;&#34892;&#22495;&#30001;&#19968;&#20010;&#21483;&#20570;&#32422;&#26463;&#34928;&#20943;&#20989;&#25968;&#65288;CDF&#65289;&#30340;&#21487;&#34892;&#24615;&#20989;&#25968;&#34920;&#31034;&#12290;FPI &#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#21483;&#20570;&#21487;&#34892;&#24615;&#31574;&#30053;&#25913;&#36827;&#30340;&#21306;&#22495;&#24615;&#31574;&#30053;&#26356;&#26032;&#35268;&#21017;&#65292;&#23427;&#22312;&#21487;&#34892;&#22495;&#20869;&#26368;&#22823;&#21270;&#22238;&#25253;&#65292;&#22312;&#21487;&#34892;&#22495;&#22806;&#26368;&#23567;&#21270; CDF&#12290;&#36825;&#20010;&#26356;&#26032;&#35268;&#21017;&#24635;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#30830;&#20445;&#21487;&#34892;&#22495;&#21333;&#35843;&#22320;&#25193;&#23637;&#65292;&#29366;&#24577;&#20540;&#20989;&#25968;&#21333;&#35843;&#22320;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe reinforcement learning (RL) aims to solve an optimal control problem under safety constraints. Existing $\textit{direct}$ safe RL methods use the original constraint throughout the learning process. They either lack theoretical guarantees of the policy during iteration or suffer from infeasibility problems. To address this issue, we propose an $\textit{indirect}$ safe RL method called feasible policy iteration (FPI) that iteratively uses the feasible region of the last policy to constrain the current policy. The feasible region is represented by a feasibility function called constraint decay function (CDF). The core of FPI is a region-wise policy update rule called feasible policy improvement, which maximizes the return under the constraint of the CDF inside the feasible region and minimizes the CDF outside the feasible region. This update rule is always feasible and ensures that the feasible region monotonically expands and the state-value function monotonically increases inside 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;UDTIRI&#65292;&#21253;&#21547;&#20102;&#26631;&#35760;&#40784;&#20840;&#30340;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#21487;&#20197;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.08842</link><description>&lt;p&gt;
UDTIRI:&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
UDTIRI: An Open-Source Road Pothole Detection Benchmark Suite. (arXiv:2304.08842v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08842
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;UDTIRI&#65292;&#21253;&#21547;&#20102;&#26631;&#35760;&#40784;&#20840;&#30340;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#21487;&#20197;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30475;&#21040;&#22312;&#22478;&#24066;&#25968;&#23383;&#23402;&#29983;&#39046;&#22495;&#20013;&#21033;&#29992;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#22312;&#26234;&#33021;&#36947;&#36335;&#26816;&#26597;&#39046;&#22495;&#65292;&#30446;&#21069;&#30740;&#31350;&#21644;&#25968;&#25454;&#26377;&#38480;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;Urban Digital Twins Intelligent Road Inspection (UDTIRI)&#25968;&#25454;&#38598;&#30340;&#26631;&#35760;&#40784;&#20840;&#30340;&#36947;&#36335;&#22353;&#27934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#25968;&#25454;&#38598;&#33021;&#22815;&#35753;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#35753;&#31639;&#27861;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#22330;&#26223;&#24182;&#26368;&#22823;&#21270;&#20854;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#25293;&#25668;&#20110;&#19981;&#21516;&#30340;&#24773;&#22659;&#20013;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20809;&#29031;&#21644;&#28287;&#24230;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#24847;&#22270;&#26159;&#23558;&#36825;&#20010;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#33457;&#36153;&#20102;&#22823;&#37327;&#31934;&#21147;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#24182;&#23545;UDTIRI&#25968;&#25454;&#38598;&#30340;&#19968;&#20123;&#20195;&#34920;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is seen that there is enormous potential to leverage powerful deep learning methods in the emerging field of urban digital twins. It is particularly in the area of intelligent road inspection where there is currently limited research and data available. To facilitate progress in this field, we have developed a well-labeled road pothole dataset named Urban Digital Twins Intelligent Road Inspection (UDTIRI) dataset. We hope this dataset will enable the use of powerful deep learning methods in urban road inspection, providing algorithms with a more comprehensive understanding of the scene and maximizing their potential. Our dataset comprises 1000 images of potholes, captured in various scenarios with different lighting and humidity conditions. Our intention is to employ this dataset for object detection, semantic segmentation, and instance segmentation tasks. Our team has devoted significant effort to conducting a detailed statistical analysis, and benchmarking a selection of represent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22270;&#21453;&#38382;&#39064;&#20013;&#30340;&#28304;&#23450;&#20301;&#30340;&#20004;&#38454;&#27573;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#36845;&#20195;&#21435;&#22122;&#65292;&#22312;&#20445;&#35777;&#31934;&#24230;&#30340;&#21516;&#26102;&#36171;&#20104;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.08841</link><description>&lt;p&gt;
&#29992;&#20110;&#22270;&#21453;&#38382;&#39064;&#20013;&#28304;&#23450;&#20301;&#30340;&#20004;&#38454;&#27573;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Two-stage Denoising Diffusion Model for Source Localization in Graph Inverse Problems. (arXiv:2304.08841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22270;&#21453;&#38382;&#39064;&#20013;&#30340;&#28304;&#23450;&#20301;&#30340;&#20004;&#38454;&#27573;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#36845;&#20195;&#21435;&#22122;&#65292;&#22312;&#20445;&#35777;&#31934;&#24230;&#30340;&#21516;&#26102;&#36171;&#20104;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#23450;&#20301;&#26159;&#22270;&#20449;&#24687;&#20256;&#25773;&#30340;&#21453;&#38382;&#39064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20449;&#24687;&#20256;&#25773;&#20013;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#32780;&#28304;&#23450;&#20301;&#38382;&#39064;&#30340;&#30149;&#24577;&#20351;&#36825;&#20123;&#25361;&#25112;&#26356;&#21152;&#20005;&#23803;&#12290;&#26368;&#36817;&#65292;&#29305;&#21035;&#26159;&#21463;&#32463;&#20856;&#38750;&#24179;&#34913;&#28909;&#21147;&#23398;&#21551;&#21457;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#22312;&#35299;&#20915;&#21453;&#38382;&#39064;&#21644;&#20135;&#29983;&#39640;&#36136;&#37327;&#37325;&#24314;&#26041;&#38754;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30452;&#25509;&#23558;&#20854;&#24212;&#29992;&#20110;&#28304;&#23450;&#20301;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#36825;&#26159;&#22240;&#20026;&#26377;&#20004;&#20010;&#21407;&#22240;&#12290;&#39318;&#20808;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#19978;&#35745;&#31639;&#21518;&#39564;&#20256;&#25773;&#32467;&#26524;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#37319;&#26679;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#36825;&#23558;&#23548;&#33268;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20854;&#27425;&#65292;&#22312;&#29616;&#26377;&#30340;&#26041;&#27861;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#26412;&#36523;&#26159;&#30149;&#24577;&#30340;&#65288;&#22810;&#23545;&#19968;&#65289;&#65307;&#22240;&#27492;&#65292;&#31616;&#21333;&#22320;&#23558;&#25193;&#25955;&#27169;&#22411;&#36716;&#31227;&#21040;&#27492;&#39046;&#22495;&#20250;&#23548;&#33268;&#20196;&#20154;&#19981;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#23545;&#21518;&#39564;&#20256;&#25773;&#22270;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#26469;&#35299;&#20915;&#22270;&#19978;&#28304;&#23450;&#20301;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#20248;&#28857;&#19982;&#21435;&#22122;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#26356;&#39640;&#25928;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#20811;&#26381;&#20102;&#35745;&#31639;&#19978;&#30340;&#19981;&#21487;&#34892;&#24615;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Source localization is the inverse problem of graph information dissemination and has broad practical applications.  However, the inherent intricacy and uncertainty in information dissemination pose significant challenges, and the ill-posed nature of the source localization problem further exacerbates these challenges. Recently, deep generative models, particularly diffusion models inspired by classical non-equilibrium thermodynamics, have made significant progress. While diffusion models have proven to be powerful in solving inverse problems and producing high-quality reconstructions, applying them directly to the source localization is infeasible for two reasons. Firstly, it is impossible to calculate the posterior disseminated results on a large-scale network for iterative denoising sampling, which would incur enormous computational costs. Secondly, in the existing methods for this field, the training data itself are ill-posed (many-to-one); thus simply transferring the diffusion mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35266;&#27979;&#22120;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#21644;&#38548;&#31163;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#20256;&#24863;&#22120;&#25925;&#38556;&#65292;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#33258;&#20027;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#36890;&#36807;&#23398;&#20064;&#23454;&#29616;Lueneberger&#35266;&#23519;&#22120;&#30340;&#35774;&#35745;&#65292;&#36890;&#36807;&#27531;&#30041;&#29983;&#25104;&#26816;&#27979;&#20256;&#24863;&#22120;&#25925;&#38556;&#24182;&#23454;&#29616;&#25925;&#38556;&#38548;&#31163;&#12290;</title><link>http://arxiv.org/abs/2304.08837</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#35266;&#27979;&#22120;&#30340;&#33258;&#20027;&#38750;&#32447;&#24615;&#31995;&#32479;&#20256;&#24863;&#22120;&#25925;&#38556;&#26816;&#27979;&#19982;&#38548;&#31163;
&lt;/p&gt;
&lt;p&gt;
Sensor Fault Detection and Isolation in Autonomous Nonlinear Systems Using Neural Network-Based Observers. (arXiv:2304.08837v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35266;&#27979;&#22120;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#26816;&#27979;&#21644;&#38548;&#31163;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#20256;&#24863;&#22120;&#25925;&#38556;&#65292;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#33258;&#20027;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#36890;&#36807;&#23398;&#20064;&#23454;&#29616;Lueneberger&#35266;&#23519;&#22120;&#30340;&#35774;&#35745;&#65292;&#36890;&#36807;&#27531;&#30041;&#29983;&#25104;&#26816;&#27979;&#20256;&#24863;&#22120;&#25925;&#38556;&#24182;&#23454;&#29616;&#25925;&#38556;&#38548;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#27979;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#38548;&#31163;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#20256;&#24863;&#22120;&#25925;&#38556;&#12290;&#32771;&#34385;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#20256;&#24863;&#22120;&#25925;&#38556;&#65306;&#23436;&#20840;&#25925;&#38556;&#21644;&#20256;&#24863;&#22120;&#21155;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#33258;&#20027;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#32780;&#19981;&#38656;&#23545;&#20854;&#19977;&#35282;&#24418;&#21644;/&#25110;&#27491;&#24120;&#24418;&#24335;&#36827;&#34892;&#20219;&#20309;&#20551;&#35774;&#65292;&#36825;&#36890;&#24120;&#22312;&#35266;&#23519;&#32773;&#35774;&#35745;&#25991;&#29486;&#20013;&#32771;&#34385;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;Lueneberger&#35266;&#23519;&#22120;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#35774;&#35745;&#65292;&#20854;&#20013;&#28041;&#21450;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#36716;&#21270;&#20026;&#20855;&#26377;&#36755;&#20986;&#27880;&#20837;&#30340;&#31283;&#23450;&#32447;&#24615;&#31995;&#32479;&#30340;&#21333;&#23556;&#26144;&#23556;&#12290;&#36825;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;Lueneberger&#35266;&#23519;&#22120;&#20934;&#30830;&#20272;&#35745;&#20102;&#31995;&#32479;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#36890;&#36807;&#27531;&#30041;&#29983;&#25104;&#23454;&#29616;&#20256;&#24863;&#22120;&#25925;&#38556;&#30340;&#26816;&#27979;&#12290;&#27531;&#24046;&#26159;&#36890;&#36807;&#35745;&#31639;&#31995;&#32479;&#27979;&#37327;&#36755;&#20986;&#21644;&#35266;&#23519;&#32773;&#39044;&#27979;&#36755;&#20986;&#21521;&#37327;&#20043;&#38388;&#24046;&#20540;&#30340;&#33539;&#25968;&#32780;&#24471;&#20986;&#30340;&#12290;&#25925;&#38556;&#38548;&#31163;&#26159;&#36890;&#36807;&#23558;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#36755;&#20986;&#19982;&#27531;&#24046;&#20449;&#21495;&#36827;&#34892;&#27604;&#36739;&#26469;&#23454;&#29616;&#30340;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#36890;&#36807;&#23545;&#21452;&#32592;&#31995;&#32479;&#30340;&#27169;&#25311;&#32467;&#26524;&#36827;&#34892;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new observer-based approach to detect and isolate faulty sensors in industrial systems. Two types of sensor faults are considered: complete failure and sensor deterioration. The proposed method is applicable to general autonomous nonlinear systems without making any assumptions about its triangular and/or normal form, which is usually considered in the observer design literature. The key aspect of our approach is a learning-based design of the Luenberger observer, which involves using a neural network to approximate the injective map that transforms the nonlinear system into a stable linear system with output injection. This learning-based Luenberger observer accurately estimates the system's state, allowing for the detection of sensor faults through residual generation. The residual is computed as the norm of the difference between the system's measured output and the observer's predicted output vectors. Fault isolation is achieved by comparing each sensor's meas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TTIDA&#30340;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#21487;&#25511;&#30340;&#36924;&#30495;&#26631;&#35760;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2304.08821</link><description>&lt;p&gt;
TTIDA: &#36890;&#36807;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#21487;&#25511;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models. (arXiv:2304.08821v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TTIDA&#30340;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#21487;&#25511;&#30340;&#36924;&#30495;&#26631;&#35760;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#22686;&#34917;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#26377;&#29992;&#20449;&#24687;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#22914;&#22122;&#22768;&#27880;&#20837;&#21644;&#22270;&#20687;&#21464;&#25442;&#65292;&#24050;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65288;GDA&#65289;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#20135;&#29983;&#26356;&#22810;&#26679;&#21270;&#21644;&#28789;&#27963;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#32463;&#24120;&#29992;&#20110;GDA&#65292;&#20294;&#19982;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20204;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TTIDA&#65288;&#25991;&#26412;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#65289;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#65288;T2T&#65289;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#23558;T2I&#27169;&#22411;&#30340;&#26465;&#20214;&#35774;&#32622;&#20026;T2T&#27169;&#22411;&#29983;&#25104;&#30340;&#35814;&#32454;&#25551;&#36848;&#65292;&#25105;&#20204;&#33021;&#22815;&#20197;&#28789;&#27963;&#21644;&#21487;&#25511;&#30340;&#26041;&#24335;&#29983;&#25104;&#36924;&#30495;&#30340;&#26631;&#35760;&#22270;&#20687;&#12290;&#22312;&#39046;&#22495;&#20869;&#20998;&#31867;&#12289;&#36328;&#39046;&#22495;&#20998;&#31867;&#21644;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#23637;&#31034;&#20102;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation has been established as an efficacious approach to supplement useful information for low-resource datasets. Traditional augmentation techniques such as noise injection and image transformations have been widely used. In addition, generative data augmentation (GDA) has been shown to produce more diverse and flexible data. While generative adversarial networks (GANs) have been frequently used for GDA, they lack diversity and controllability compared to text-to-image diffusion models. In this paper, we propose TTIDA (Text-to-Text-to-Image Data Augmentation) to leverage the capabilities of large-scale pre-trained Text-to-Text (T2T) and Text-to-Image (T2I) generative models for data augmentation. By conditioning the T2I model on detailed descriptions produced by T2T models, we are able to generate photo-realistic labeled images in a flexible and controllable manner. Experiments on in-domain classification, cross-domain classification, and image captioning tasks show consis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#32500;&#24230;&#24182;&#22312;&#22270;&#20687;&#24207;&#21015;&#19978;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#38469;&#39550;&#39542;&#25968;&#25454;&#30340;&#27169;&#25311;&#21644;&#21019;&#24847;&#20869;&#23481;&#21019;&#20316;&#30340;&#30340;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.08818</link><description>&lt;p&gt;
&#23558;&#28508;&#21464;&#37327;&#23545;&#40784;&#65306;&#20351;&#29992;&#28508;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. (arXiv:2304.08818v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#32500;&#24230;&#24182;&#22312;&#22270;&#20687;&#24207;&#21015;&#19978;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#38469;&#39550;&#39542;&#25968;&#25454;&#30340;&#27169;&#25311;&#21644;&#21019;&#24847;&#20869;&#23481;&#21019;&#20316;&#30340;&#30340;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#36890;&#36807;&#22312;&#21387;&#32553;&#30340;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#21512;&#25104;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#36807;&#22810;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#26412;&#25991;&#23558;LDM&#24212;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#29983;&#25104;&#65292;&#36825;&#26159;&#19968;&#39033;&#29305;&#21035;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#21333;&#29420;&#30340;&#22270;&#20687;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#22312;&#28508;&#31354;&#38388;&#25193;&#25955;&#27169;&#22411;&#20013;&#24341;&#20837;&#26102;&#38388;&#32500;&#24230;&#65292;&#24182;&#22312;&#32534;&#30721;&#30340;&#22270;&#20687;&#24207;&#21015;&#65288;&#21363;&#35270;&#39057;&#65289;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23558;&#29983;&#25104;&#22120;&#20174;&#22270;&#20687;&#29983;&#25104;&#22120;&#36716;&#25442;&#20026;&#35270;&#39057;&#29983;&#25104;&#22120;&#12290;&#21516;&#26679;&#65292;&#25105;&#20204;&#22312;&#26102;&#38388;&#19978;&#23545;&#40784;&#25193;&#25955;&#27169;&#22411;&#19978;&#37319;&#26679;&#22120;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#20851;&#27880;&#20004;&#20010;&#30456;&#20851;&#30340;&#23454;&#38469;&#24212;&#29992;&#65306;&#37326;&#22806;&#39550;&#39542;&#25968;&#25454;&#30340;&#27169;&#25311;&#21644;&#25991;&#26412;&#21040;&#35270;&#39057;&#24314;&#27169;&#30340;&#21019;&#24847;&#20869;&#23481;&#21019;&#20316;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#20998;&#36776;&#29575;&#20026;512 x 1024&#30340;&#30495;&#23454;&#39550;&#39542;&#35270;&#39057;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35270;&#39057;LDM&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512 x 1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#35821;&#38899;&#35782;&#21035;&#30340;&#23545;&#25239;&#26679;&#26412;&#20256;&#36882;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#38543;&#26426;&#26799;&#24230;&#38598;&#25104;&#21644;&#21160;&#24577;&#26799;&#24230;&#21152;&#26435;&#38598;&#25104;&#36825;&#20004;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#35821;&#38899;&#21644;&#22270;&#20687;&#22312;&#23545;&#25239;&#25915;&#20987;&#20256;&#36882;&#19978;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.08811</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#26041;&#27861;&#23454;&#29616;&#21487;&#20256;&#36882;&#38899;&#39057;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Towards the Transferable Audio Adversarial Attack via Ensemble Methods. (arXiv:2304.08811v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#35821;&#38899;&#35782;&#21035;&#30340;&#23545;&#25239;&#26679;&#26412;&#20256;&#36882;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#38543;&#26426;&#26799;&#24230;&#38598;&#25104;&#21644;&#21160;&#24577;&#26799;&#24230;&#21152;&#26435;&#38598;&#25104;&#36825;&#20004;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#35821;&#38899;&#21644;&#22270;&#20687;&#22312;&#23545;&#25239;&#25915;&#20987;&#20256;&#36882;&#19978;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#38754;&#37096;&#35782;&#21035;&#21644;&#35821;&#38899;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#20005;&#37325;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#24182;&#19981;&#20855;&#22791;&#36275;&#22815;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#21487;&#20256;&#36882;&#25915;&#20987;&#24050;&#25104;&#20026;&#40657;&#30418;&#25915;&#20987;&#30340;&#19968;&#31181;&#31361;&#20986;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24433;&#21709;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#38899;&#35782;&#21035;&#20013;&#23545;&#25239;&#26679;&#26412;&#65288;AEs&#65289;&#20256;&#36882;&#33021;&#21147;&#30340;&#28508;&#22312;&#22240;&#32032;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#33030;&#24369;&#24615;&#21644;&#20915;&#31574;&#36793;&#30028;&#30340;&#19981;&#35268;&#21017;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23545;&#25239;&#25915;&#20987;&#30340;&#20256;&#36882;&#33021;&#21147;&#26041;&#38754;&#65292;&#35821;&#38899;&#21644;&#22270;&#20687;&#20043;&#38388;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#22270;&#20687;&#30340;&#25968;&#25454;&#30456;&#20851;&#24615;&#36739;&#20302;&#32780;&#35821;&#38899;&#35782;&#21035;&#21017;&#30456;&#21453;&#12290;&#21463;&#21040;&#22522;&#20110;dropout&#30340;&#38598;&#25104;&#26041;&#27861;&#30340;&#28608;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38543;&#26426;&#26799;&#24230;&#38598;&#25104;&#21644;&#21160;&#24577;&#26799;&#24230;&#21152;&#26435;&#38598;&#25104;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;AEs&#20256;&#36882;&#33021;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning (DL) models have achieved significant progress in many domains, such as autonomous driving, facial recognition, and speech recognition. However, the vulnerability of deep learning models to adversarial attacks has raised serious concerns in the community because of their insufficient robustness and generalization. Also, transferable attacks have become a prominent method for black-box attacks. In this work, we explore the potential factors that impact adversarial examples (AEs) transferability in DL-based speech recognition. We also discuss the vulnerability of different DL systems and the irregular nature of decision boundaries. Our results show a remarkable difference in the transferability of AEs between speech and images, with the data relevance being low in images but opposite in speech recognition. Motivated by dropout-based ensemble approaches, we propose random gradient ensembles and dynamic gradient-weighted ensembles, and we evaluate the impact 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#38544;&#24335;&#34920;&#31034;&#26500;&#24314;&#20102;&#22330;&#26223;&#30456;&#20851;&#20808;&#39564;&#65292;&#20174;&#32780;&#22312;&#19981;&#35268;&#21017;&#29615;&#22659;&#20013;&#21033;&#29992;&#22522;&#20110;&#27169;&#25311;&#30340;&#39640;&#25928;&#36125;&#21494;&#26031;&#25512;&#29702;&#31639;&#27861;&#25104;&#21151;&#35782;&#21035;&#25235;&#21462;&#23039;&#24577;&#12290;</title><link>http://arxiv.org/abs/2304.08805</link><description>&lt;p&gt;
&#22522;&#20110;&#40654;&#26364;&#20960;&#20309;&#19982;&#38544;&#24335;&#34920;&#31034;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#26426;&#22120;&#20154;&#25235;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Implicit representation priors meet Riemannian geometry for Bayesian robotic grasping. (arXiv:2304.08805v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#38544;&#24335;&#34920;&#31034;&#26500;&#24314;&#20102;&#22330;&#26223;&#30456;&#20851;&#20808;&#39564;&#65292;&#20174;&#32780;&#22312;&#19981;&#35268;&#21017;&#29615;&#22659;&#20013;&#21033;&#29992;&#22522;&#20110;&#27169;&#25311;&#30340;&#39640;&#25928;&#36125;&#21494;&#26031;&#25512;&#29702;&#31639;&#27861;&#25104;&#21151;&#35782;&#21035;&#25235;&#21462;&#23039;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#22122;&#22768;&#29615;&#22659;&#19979;&#36827;&#34892;&#26426;&#22120;&#20154;&#25235;&#21462;&#38754;&#20020;&#30528;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#32570;&#20047;&#22330;&#26223;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30001;&#20110;&#20004;&#20010;&#21407;&#22240;&#65292;&#29992;&#36125;&#21494;&#26031;&#25512;&#29702;&#26469;&#35782;&#21035;&#33391;&#22909;&#30340;&#25235;&#21462;&#23039;&#24577;&#21464;&#24471;&#22256;&#38590;&#65306;i&#65289;&#20174;&#26080;&#20449;&#24687;&#30340;&#20808;&#39564;&#29983;&#25104;&#25968;&#25454;&#25928;&#29575;&#20302;&#19979;&#65292;ii&#65289;&#21518;&#39564;&#36890;&#24120;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#23450;&#20041;&#19968;&#20010;&#22797;&#26434;&#20998;&#24067;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#38544;&#24335;&#34920;&#31034;&#26500;&#24314;&#22330;&#26223;&#30456;&#20851;&#24615;&#20808;&#39564;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20351;&#24471;&#22312;&#19981;&#35268;&#21017;&#29615;&#22659;&#20013;&#24212;&#29992;&#22522;&#20110;&#27169;&#25311;&#30340;&#39640;&#25928;&#36125;&#21494;&#26031;&#25512;&#29702;&#31639;&#27861;&#26469;&#30830;&#23450;&#25104;&#21151;&#30340;&#25235;&#21462;&#23039;&#24577;&#25104;&#20026;&#21487;&#33021;&#12290;&#27169;&#25311;&#21644;&#29289;&#29702;&#22522;&#20934;&#27979;&#35797;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#39640;&#25104;&#21151;&#29575;&#21644;&#33391;&#22909;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic grasping in highly noisy environments presents complex challenges, especially with limited prior knowledge about the scene. In particular, identifying good grasping poses with Bayesian inference becomes difficult due to two reasons: i) generating data from uninformative priors proves to be inefficient, and ii) the posterior often entails a complex distribution defined on a Riemannian manifold. In this study, we explore the use of implicit representations to construct scene-dependent priors, thereby enabling the application of efficient simulation-based Bayesian inference algorithms for determining successful grasp poses in unstructured environments. Results from both simulation and physical benchmarks showcase the high success rate and promising potential of this approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#29615;&#20998;&#35299;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#21160;&#24577;&#32593;&#32476;&#30340;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#30340;LDN&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.08798</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#29615;&#20998;&#35299;&#30340;&#22823;&#35268;&#27169;&#21160;&#24577;&#32593;&#32476;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large-scale Dynamic Network Representation via Tensor Ring Decomposition. (arXiv:2304.08798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#29615;&#20998;&#35299;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#21160;&#24577;&#32593;&#32476;&#30340;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#30340;LDN&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#32852;&#32593;&#26102;&#20195;&#65292;&#22823;&#35268;&#27169;&#21160;&#24577;&#32593;&#32476;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#30340;&#21160;&#24577;&#24615;&#36136;&#25429;&#25417;&#20102;&#32593;&#32476;&#32467;&#26500;&#30340;&#28436;&#21270;&#21644;&#36793;&#26435;&#37325;&#30340;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#22240;&#27492;&#23545;&#25968;&#25454;&#20998;&#26512;&#21644;&#24314;&#27169;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#24352;&#37327;&#30340;&#28508;&#22312;&#20998;&#35299;&#65288;LFT&#65289;&#27169;&#22411;&#20026;LDN&#30340;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;LFT&#27169;&#22411;&#20960;&#20046;&#37117;&#26159;&#22522;&#20110;&#27491;&#20132;&#22810;&#39033;&#24335;&#20998;&#35299;&#65288;CPF&#65289;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#29615;&#65288;TR&#65289;&#20998;&#35299;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;LDN&#30340;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20316;&#32773;&#23558;&#21333;&#20010;&#28508;&#22312;&#22240;&#23376;&#20381;&#36182;&#24615;&#12289;&#38750;&#36127;&#24615;&#21644;&#20056;&#27861;&#26356;&#26032;&#65288;SLF-NMU&#65289;&#21407;&#21017;&#32435;&#20837;TR&#20998;&#35299;&#27169;&#22411;&#65292;&#24182;&#20998;&#26512;&#20102;TR&#20998;&#35299;&#30340;&#29305;&#27530;&#20559;&#32622;&#24418;&#24335;&#12290;&#20004;&#20010;&#30495;&#23454;LDN&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#27169;&#22411;&#36798;&#21040;&#20102;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Dynamic Networks (LDNs) are becoming increasingly important in the Internet age, yet the dynamic nature of these networks captures the evolution of the network structure and how edge weights change over time, posing unique challenges for data analysis and modeling. A Latent Factorization of Tensors (LFT) model facilitates efficient representation learning for a LDN. But the existing LFT models are almost based on Canonical Polyadic Factorization (CPF). Therefore, this work proposes a model based on Tensor Ring (TR) decomposition for efficient representation learning for a LDN. Specifically, we incorporate the principle of single latent factor-dependent, non-negative, and multiplicative update (SLF-NMU) into the TR decomposition model, and analyze the particular bias form of TR decomposition. Experimental studies on two real LDNs demonstrate that the propose method achieves higher accuracy than existing models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24211;&#23384;&#31649;&#29702;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#21253;&#25324;&#33258;&#23450;&#20041;GPU&#24182;&#34892;&#29615;&#22659;&#21644;&#20998;&#20139;&#22870;&#21169;&#26426;&#21046;&#65292;&#36890;&#36807;&#20998;&#25955;&#24335;Actor-Critic&#26041;&#27861;&#36827;&#34892;&#22521;&#35757;&#12290;&#22312;&#20379;&#24212;&#38142;&#22330;&#26223;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#21333;&#19968;&#26234;&#33021;&#20307;RL&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.08769</link><description>&lt;p&gt;
&#24211;&#23384;&#31649;&#29702;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-Agent Reinforcement Learning for Inventory Management. (arXiv:2304.08769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24211;&#23384;&#31649;&#29702;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#21253;&#25324;&#33258;&#23450;&#20041;GPU&#24182;&#34892;&#29615;&#22659;&#21644;&#20998;&#20139;&#22870;&#21169;&#26426;&#21046;&#65292;&#36890;&#36807;&#20998;&#25955;&#24335;Actor-Critic&#26041;&#27861;&#36827;&#34892;&#22521;&#35757;&#12290;&#22312;&#20379;&#24212;&#38142;&#22330;&#26223;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#21333;&#19968;&#26234;&#33021;&#20307;RL&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#24211;&#23384;&#31649;&#29702;&#65288;IM&#65289;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#23616;&#38480;&#20110;&#23454;&#29616;&#31616;&#21333;&#12289;&#32447;&#24615;&#29615;&#22659;&#65292;&#24182;&#20570;&#20986;&#20123;&#24494;&#35843;&#25972;&#20197;&#33719;&#24471;RL&#31639;&#27861;&#12290;&#23558;&#36825;&#20123;&#31616;&#21333;&#30340;&#29615;&#22659;&#25193;&#23637;&#21040;&#23454;&#38469;&#20379;&#24212;&#38142;&#31995;&#32479;&#20013;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#65306;&#38477;&#20302;&#29615;&#22659;&#35745;&#31639;&#37327;&#65292;&#23450;&#20041;&#20195;&#29702;&#31243;&#24207;&#37197;&#32622;&#20197;&#20195;&#34920;&#23454;&#38469;&#24215;&#38138;&#21644;&#20179;&#24211;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#20197;&#21450;&#25351;&#23450;&#22870;&#21169;&#26694;&#26550;&#20197;&#40723;&#21169;&#25972;&#20010;&#20379;&#24212;&#38142;&#20013;&#30340;&#33391;&#22909;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#33258;&#23450;&#20041;GPU&#24182;&#34892;&#29615;&#22659;&#30340;&#31995;&#32479;&#65292;&#35813;&#29615;&#22659;&#21253;&#25324;&#19968;&#20010;&#20179;&#24211;&#21644;&#22810;&#20010;&#21830;&#24215;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#29992;&#20110;&#20195;&#29702;-&#29615;&#22659;&#21160;&#24577;&#65292;&#21253;&#25324;&#22686;&#24378;&#29366;&#24577;&#21644;&#25805;&#20316;&#31354;&#38388;&#65292;&#20197;&#21450;&#20998;&#20139;&#22870;&#21169;&#26426;&#21046;&#65292;&#20197;&#20248;&#21270;&#22823;&#22411;&#38646;&#21806;&#21830;&#30340;&#20379;&#24212;&#38142;&#38656;&#27714;&#12290;&#20379;&#24212;&#38142;&#22270;&#20013;&#30340;&#27599;&#20010;&#39030;&#28857;&#37117;&#26159;&#19968;&#20010;&#29420;&#31435;&#30340;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#20998;&#25955;&#24335;Actor-Critic&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#20013;&#22830;&#35780;&#35770;&#23478;&#36890;&#20449;&#26469;&#23454;&#29616;&#21512;&#20316;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#21333;&#19968;&#26234;&#33021;&#20307;RL&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#20379;&#24212;&#38142;&#22330;&#26223;&#20013;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
With Reinforcement Learning (RL) for inventory management (IM) being a nascent field of research, approaches tend to be limited to simple, linear environments with implementations that are minor modifications of off-the-shelf RL algorithms. Scaling these simplistic environments to a real-world supply chain comes with a few challenges such as: minimizing the computational requirements of the environment, specifying agent configurations that are representative of dynamics at real world stores and warehouses, and specifying a reward framework that encourages desirable behavior across the whole supply chain. In this work, we present a system with a custom GPU-parallelized environment that consists of one warehouse and multiple stores, a novel architecture for agent-environment dynamics incorporating enhanced state and action spaces, and a shared reward specification that seeks to optimize for a large retailer's supply chain needs. Each vertex in the supply chain graph is an independent age
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; W-MAE &#30340;&#39044;&#35757;&#32451;&#22825;&#27668;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#37325;&#24314;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39044;&#27979;&#27668;&#35937;&#21464;&#37327;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20174;&#32780;&#23545;&#22825;&#27668;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2304.08754</link><description>&lt;p&gt;
W-MAE&#65306;&#20855;&#26377;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#22825;&#27668;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
W-MAE: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting. (arXiv:2304.08754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; W-MAE &#30340;&#39044;&#35757;&#32451;&#22825;&#27668;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#37325;&#24314;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39044;&#27979;&#27668;&#35937;&#21464;&#37327;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20174;&#32780;&#23545;&#22825;&#27668;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#39044;&#27979;&#26159;&#20855;&#26377;&#30452;&#25509;&#31038;&#20250;&#21644;&#32463;&#27982;&#24433;&#21709;&#30340;&#38271;&#26399;&#35745;&#31639;&#25361;&#25112;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#22823;&#37327;&#30340;&#36830;&#32493;&#25968;&#25454;&#25910;&#38598;&#65292;&#24182;&#22312;&#38271;&#26102;&#38388;&#20869;&#34920;&#29616;&#20986;&#20016;&#23500;&#30340;&#26102;&#31354;&#20381;&#36182;&#24615;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#23558;&#39044;&#35757;&#32451;&#25216;&#26415;&#24212;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#21464;&#37327;&#22825;&#27668;&#39044;&#27979;&#30340;&#20855;&#26377;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#30340;&#22825;&#27668;&#27169;&#22411;W-MAE&#12290;W-MAE&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#37325;&#24314;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#22312;&#26102;&#38388;&#23610;&#24230;&#19978;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;W-MAE&#20197;&#39044;&#27979;&#27668;&#35937;&#21464;&#37327;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20174;&#32780;&#23545;&#22825;&#27668;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#20351;&#29992;&#27599;&#20845;&#23567;&#26102;&#36873;&#25321;&#19968;&#27425;&#26679;&#26412;&#65292;&#20165;&#20351;&#29992;&#20004;&#24180;&#30340;ERA5&#25968;&#25454;&#65292;&#23545;W-MAE&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#23558;W-MAE&#19982;FourCastNet&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weather forecasting is a long-standing computational challenge with direct societal and economic impacts. This task involves a large amount of continuous data collection and exhibits rich spatiotemporal dependencies over long periods, making it highly suitable for deep learning models. In this paper, we apply pre-training techniques to weather forecasting and propose W-MAE, a Weather model with Masked AutoEncoder pre-training for multi-variable weather forecasting. W-MAE is pre-trained in a self-supervised manner to reconstruct spatial correlations within meteorological variables. On the temporal scale, we fine-tune the pre-trained W-MAE to predict the future states of meteorological variables, thereby modeling the temporal dependencies present in weather data. We pre-train W-MAE using the fifth-generation ECMWF Reanalysis (ERA5) data, with samples selected every six hours and using only two years of data. Under the same training data conditions, we compare W-MAE with FourCastNet, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#24102;&#25805;&#20316;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22810;&#31181;&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#20844;&#24320;GitHub&#20195;&#30721;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2304.08743</link><description>&lt;p&gt;
&#24102;&#25805;&#20316;&#32422;&#26463;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#19979;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Actor-Critic Deep Reinforcement Learning Algorithms for Robotics Control with Action Constraints. (arXiv:2304.08743v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#24102;&#25805;&#20316;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22810;&#31181;&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#20844;&#24320;GitHub&#20195;&#30721;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#25805;&#20316;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#25805;&#20316;&#21463;&#38480;&#30340;RL&#20013;&#65292;&#23398;&#20064;&#31995;&#32479;&#37319;&#21462;&#30340;&#27599;&#20010;&#25805;&#20316;&#37117;&#24517;&#39035;&#31526;&#21512;&#26576;&#20123;&#32422;&#26463;&#26465;&#20214;&#12290;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#23545;&#20110;&#30830;&#20445;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#25805;&#20316;&#30340;&#21487;&#34892;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#26426;&#22120;&#20154;&#25511;&#21046;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#29616;&#26377;&#31639;&#27861;&#21450;&#20854;&#26032;&#39062;&#30340;&#21464;&#20307;&#65292;&#28085;&#30422;&#22810;&#31181;&#25805;&#20316;&#38480;&#21046;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;&#28145;&#20837;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#20986;&#20154;&#24847;&#26009;&#30340;&#35265;&#35299;&#65292;&#21253;&#25324;&#22522;&#32447;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#20351;&#29992;&#30340;&#22522;&#20934;&#38382;&#39064;&#21644;&#30456;&#20851;&#20195;&#30721;&#21487;&#22312;github.com/omron-sinicx/action-constrained-RL-benchmark&#19978;&#22312;&#32447;&#33719;&#21462;&#65292;&#20197;&#36827;&#19968;&#27493;&#24320;&#23637;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a benchmark for evaluating action-constrained reinforcement learning (RL) algorithms. In action-constrained RL, each action taken by the learning system must comply with certain constraints. These constraints are crucial for ensuring the feasibility and safety of actions in real-world systems. We evaluate existing algorithms and their novel variants across multiple robotics control environments, encompassing multiple action constraint types. Our evaluation provides the first in-depth perspective of the field, revealing surprising insights, including the effectiveness of a straightforward baseline approach. The benchmark problems and associated code utilized in our experiments are made available online at github.com/omron-sinicx/action-constrained-RL-benchmark for further research and development.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#19979;&#28216;&#19987;&#23478;&#25968;&#25454;&#20174;&#31163;&#32447;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24615;&#22320;&#26597;&#35810;&#30456;&#20851;&#34892;&#20026;&#65288;&#21253;&#25324;&#35768;&#22810;&#27425;&#20248;&#34892;&#20026;&#65289;&#65292;&#23454;&#29616;&#20102;&#34892;&#20026;&#26816;&#32034;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.08742</link><description>&lt;p&gt;
&#34892;&#20026;&#26816;&#32034;&#65306;&#36890;&#36807;&#26597;&#35810;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#23454;&#29616;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets. (arXiv:2304.08742v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#19979;&#28216;&#19987;&#23478;&#25968;&#25454;&#20174;&#31163;&#32447;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24615;&#22320;&#26597;&#35810;&#30456;&#20851;&#34892;&#20026;&#65288;&#21253;&#25324;&#35768;&#22810;&#27425;&#20248;&#34892;&#20026;&#65289;&#65292;&#23454;&#29616;&#20102;&#34892;&#20026;&#26816;&#32034;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#20351;&#26426;&#22120;&#20154;&#23398;&#20064;&#26032;&#30340;&#35270;&#35273;&#21160;&#20316;&#25216;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#65292;&#26377;&#35768;&#22810;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#33539;&#24335;&#26159;&#21033;&#29992;&#22823;&#22411;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#35768;&#22810;&#34892;&#20026;&#65292;&#28982;&#21518;&#20351;&#29992;&#23569;&#37327;&#20219;&#21153;&#29305;&#23450;&#30340;&#20154;&#31867;&#30417;&#30563;&#65288;&#21363;&#20171;&#20837;&#25110;&#28436;&#31034;&#65289;&#26469;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;&#20294;&#26159;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#21033;&#29992;&#29421;&#31364;&#30340;&#20219;&#21153;&#29305;&#23450;&#30417;&#30563;&#24182;&#23558;&#20854;&#19982;&#31163;&#32447;&#25968;&#25454;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#22312;&#20110;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#19981;&#20165;&#20026;&#20195;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36824;&#21487;&#20197;&#20026;&#20195;&#29702;&#30340;&#23398;&#20064;&#25552;&#20379;&#26377;&#20851;&#20808;&#21069;&#25968;&#25454;&#31867;&#22411;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#19979;&#28216;&#19987;&#23478;&#25968;&#25454;&#20174;&#31163;&#32447;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24615;&#22320;&#26597;&#35810;&#30456;&#20851;&#34892;&#20026;&#65288;&#21253;&#25324;&#35768;&#22810;&#27425;&#20248;&#34892;&#20026;&#65289;&#12290;&#28982;&#21518;&#20195;&#29702;&#34987;&#32852;&#21512;&#35757;&#32451;&#22312;&#19987;&#23478;&#21644;&#26597;&#35810;&#25968;&#25454;&#19978;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;
&lt;/p&gt;
&lt;p&gt;
Enabling robots to learn novel visuomotor skills in a data-efficient manner remains an unsolved problem with myriad challenges. A popular paradigm for tackling this problem is through leveraging large unlabeled datasets that have many behaviors in them and then adapting a policy to a specific task using a small amount of task-specific human supervision (i.e. interventions or demonstrations). However, how best to leverage the narrow task-specific supervision and balance it with offline data remains an open question. Our key insight in this work is that task-specific data not only provides new data for an agent to train on but can also inform the type of prior data the agent should use for learning. Concretely, we propose a simple approach that uses a small amount of downstream expert data to selectively query relevant behaviors from an offline, unlabeled dataset (including many sub-optimal behaviors). The agent is then jointly trained on the expert and queried data. We observe that our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#12289;Radon&#21464;&#25442;&#21644;&#23383;&#20856;&#20272;&#31639;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;1-D&#36793;&#38469;&#36827;&#34892;&#37325;&#24314;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;&#22522;&#20110;&#23383;&#20856;&#30340;&#26041;&#27861;&#21644;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#12290;</title><link>http://arxiv.org/abs/2304.08740</link><description>&lt;p&gt;
&#29992;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#12289;Radon&#21464;&#25442;&#21644;&#23383;&#20856;&#20272;&#31639;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Estimating Joint Probability Distribution With Low-Rank Tensor Decomposition, Radon Transforms and Dictionaries. (arXiv:2304.08740v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#12289;Radon&#21464;&#25442;&#21644;&#23383;&#20856;&#20272;&#31639;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;1-D&#36793;&#38469;&#36827;&#34892;&#37325;&#24314;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;&#22522;&#20110;&#23383;&#20856;&#30340;&#26041;&#27861;&#21644;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#25968;&#25454;&#26679;&#26412;&#20013;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#30340;&#26041;&#27861;&#65292;&#20551;&#35774;&#24213;&#23618;&#20998;&#24067;&#33021;&#22815;&#20998;&#35299;&#20026;&#20960;&#20010;&#28151;&#21512;&#32452;&#20998;&#30340;&#20056;&#31215;&#23494;&#24230;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#20004;&#20010;&#20851;&#38190;&#24819;&#27861;&#65306;&#29992;&#20110;&#34920;&#31034;1-D&#23494;&#24230;&#30340;&#23383;&#20856;&#20197;&#21450;&#29992;&#20110;&#20272;&#31639;1-D&#36793;&#38469;&#30340;&#38543;&#26426;&#25237;&#24433;&#65292;&#25506;&#32034;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#30456;&#27604;&#22522;&#20110;&#23383;&#20856;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;1-D&#36793;&#38469;&#36827;&#34892;&#37325;&#24314;&#32780;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#22312;&#20272;&#31639;&#21512;&#25104;&#27010;&#29575;&#23494;&#24230;&#26041;&#38754;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#20197;&#21069;&#30340;&#22522;&#20110;&#23383;&#20856;&#30340;&#26041;&#27861;&#21644;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#25152;&#26377;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#20248;&#20110;&#36825;&#20123;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe a method for estimating the joint probability density from data samples by assuming that the underlying distribution can be decomposed as a mixture of product densities with few mixture components. Prior works have used such a decomposition to estimate the joint density from lower-dimensional marginals, which can be estimated more reliably with the same number of samples. We combine two key ideas: dictionaries to represent 1-D densities, and random projections to estimate the joint distribution from 1-D marginals, explored separately in prior work. Our algorithm benefits from improved sample complexity over the previous dictionary-based approach by using 1-D marginals for reconstruction. We evaluate the performance of our method on estimating synthetic probability densities and compare it with the previous dictionary-based approach and Gaussian Mixture Models (GMMs). Our algorithm outperforms these other approaches in all the experimental settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#25506;&#31350;&#20102;&#20154;&#26426;&#24863;&#30693;&#24046;&#24322;&#65292;&#21457;&#29616;&#21363;&#20351;&#20934;&#30830;&#29575;&#30456;&#20284;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#31572;&#26696;&#20998;&#24067;&#20063;&#21487;&#33021;&#19981;&#21516;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#20154;&#26426;&#21512;&#20316;&#26469;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.08733</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#26426;&#22120;&#26377;&#30456;&#21516;&#30340;&#30524;&#30555;&#21527;&#65311;&#22522;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#20154;&#26426;&#24863;&#30693;&#24046;&#24322;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do humans and machines have the same eyes? Human-machine perceptual differences on image classification. (arXiv:2304.08733v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#25506;&#31350;&#20102;&#20154;&#26426;&#24863;&#30693;&#24046;&#24322;&#65292;&#21457;&#29616;&#21363;&#20351;&#20934;&#30830;&#29575;&#30456;&#20284;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#31572;&#26696;&#20998;&#24067;&#20063;&#21487;&#33021;&#19981;&#21516;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#20154;&#26426;&#21512;&#20316;&#26469;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#33391;&#22909;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#27169;&#20223;&#20174;&#35757;&#32451;&#26631;&#31614;&#20013;&#23398;&#21040;&#30340;&#20154;&#31867;&#34892;&#20026;&#26469;&#35299;&#20915;&#35270;&#35273;&#20219;&#21153;&#12290;&#36817;&#26399;&#35270;&#35273;&#30740;&#31350;&#30340;&#22823;&#37096;&#20998;&#21162;&#21147;&#38598;&#20013;&#22312;&#20351;&#29992;&#26631;&#20934;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#27169;&#22411;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20102;&#35299;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#24863;&#30693;&#24046;&#24322;&#26041;&#38754;&#30340;&#24037;&#20316;&#36824;&#24456;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#37327;&#21270;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;&#26469;&#28304;&#38169;&#35823;&#30340;&#32479;&#35745;&#20998;&#24067;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#38590;&#24230;&#32423;&#21035;&#23545;&#20219;&#21153;&#36827;&#34892;&#25490;&#24207;&#65292;&#25506;&#35752;&#20154;&#31867;&#19982;&#26426;&#22120;&#19987;&#19994;&#30693;&#35782;&#30340;&#24046;&#24322;&#12290;&#21363;&#20351;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#30456;&#20284;&#65292;&#31572;&#26696;&#30340;&#20998;&#24067;&#20063;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#21033;&#29992;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#24863;&#30693;&#24046;&#24322;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#20102;&#19968;&#31181;&#21518;&#26399;&#20154;&#26426;&#21512;&#20316;&#65292;&#20854;&#34920;&#29616;&#27604;&#21333;&#29420;&#30340;&#20154;&#25110;&#26426;&#22120;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trained computer vision models are assumed to solve vision tasks by imitating human behavior learned from training labels. Most efforts in recent vision research focus on measuring the model task performance using standardized benchmarks. Limited work has been done to understand the perceptual difference between humans and machines. To fill this gap, our study first quantifies and analyzes the statistical distributions of mistakes from the two sources. We then explore human vs. machine expertise after ranking tasks by difficulty levels. Even when humans and machines have similar overall accuracies, the distribution of answers may vary. Leveraging the perceptual difference between humans and machines, we empirically demonstrate a post-hoc human-machine collaboration that outperforms humans or machines alone.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29992;EfficientNet&#31639;&#27861;&#20998;&#31867;&#19981;&#21516;&#31867;&#22411;&#30340;&#30284;&#30151;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20855;&#26377;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.08715</link><description>&lt;p&gt;
&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30284;&#30151;&#20998;&#31867;&#30340;EfficientNet&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
EfficientNet Algorithm for Classification of Different Types of Cancer. (arXiv:2304.08715v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29992;EfficientNet&#31639;&#27861;&#20998;&#31867;&#19981;&#21516;&#31867;&#22411;&#30340;&#30284;&#30151;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20855;&#26377;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39640;&#25928;&#22320;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30284;&#30151;&#23545;&#26089;&#26399;&#21457;&#29616;&#21644;&#26377;&#25928;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;EfficientNet&#31639;&#27861;&#20998;&#31867;&#33041;&#30244;&#12289;&#20083;&#33146;&#30284;&#20083;&#25151;X&#32447;&#25668;&#24433;&#12289;&#33016;&#37096;&#30284;&#21644;&#30382;&#32932;&#30284;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#24182;&#23545;&#22270;&#20687;&#36827;&#34892;&#39044;&#22788;&#29702;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#21644;&#21487;&#27604;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;EfficientNet&#31639;&#27861;&#22312;&#27599;&#20010;&#30284;&#30151;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#12289;&#39640;&#21484;&#22238;&#29575;&#21644;&#39640;F1&#20998;&#25968;&#65292;&#20248;&#20110;&#25991;&#29486;&#20013;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;EfficientNet&#31639;&#27861;&#30340;&#20248;&#32570;&#28857;&#21450;&#20854;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;EfficientNet&#31639;&#27861;&#38750;&#24120;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30284;&#30151;&#30340;&#20998;&#31867;&#65292;&#24182;&#21487;&#29992;&#20110;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and efficient classification of different types of cancer is critical for early detection and effective treatment. In this paper, we present the results of our experiments using the EfficientNet algorithm for classification of brain tumor, breast cancer mammography, chest cancer, and skin cancer. We used publicly available datasets and preprocessed the images to ensure consistency and comparability. Our experiments show that the EfficientNet algorithm achieved high accuracy, precision, recall, and F1 scores on each of the cancer datasets, outperforming other state-of-the-art algorithms in the literature. We also discuss the strengths and weaknesses of the EfficientNet algorithm and its potential applications in clinical practice. Our results suggest that the EfficientNet algorithm is well-suited for classification of different types of cancer and can be used to improve the accuracy and efficiency of cancer diagnosis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#31572;&#20102;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#27809;&#26377;&#19968;&#31181;&#21442;&#25968;&#21487;&#20197;&#21051;&#30011;&#20998;&#24067;&#31867;&#30340;PAC&#21487;&#23398;&#20064;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19981;&#23384;&#22312;&#19968;&#31181;&#21051;&#30011;&#21487;&#23398;&#20064;&#24615;&#30340;&#24615;&#36136;&#26469;&#28385;&#36275;&#20998;&#24067;&#31867;&#20197;&#21450;&#20854;&#20182;&#23398;&#20064;&#38382;&#39064;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.08712</link><description>&lt;p&gt;
&#19981;&#21487;&#33021;&#21051;&#30011;&#20998;&#24067;&#23398;&#20064;--&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Impossibility of Characterizing Distribution Learning -- a simple solution to a long-standing problem. (arXiv:2304.08712v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#31572;&#20102;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#27809;&#26377;&#19968;&#31181;&#21442;&#25968;&#21487;&#20197;&#21051;&#30011;&#20998;&#24067;&#31867;&#30340;PAC&#21487;&#23398;&#20064;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19981;&#23384;&#22312;&#19968;&#31181;&#21051;&#30011;&#21487;&#23398;&#20064;&#24615;&#30340;&#24615;&#36136;&#26469;&#28385;&#36275;&#20998;&#24067;&#31867;&#20197;&#21450;&#20854;&#20182;&#23398;&#20064;&#38382;&#39064;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#38271;&#26399;&#20197;&#26469;&#23384;&#22312;&#30340;&#19968;&#20010;&#38382;&#39064;&#65306;&#23547;&#25214;&#19968;&#31867;&#27010;&#29575;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#20197;&#21051;&#30011;&#23427;&#30340;PAC&#21487;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#24403;&#20196;&#20154;&#24778;&#35766;&#30340;&#31572;&#26696;&#8212;&#8212;&#27809;&#26377;&#36825;&#26679;&#30340;&#21442;&#25968;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#20351;&#25105;&#20204;&#33021;&#22815;&#23637;&#31034;&#31867;&#20284;&#32467;&#26524;&#30340;&#20960;&#20010;&#27010;&#24565;&#65292;&#20197;&#21450;&#20960;&#20010;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27809;&#26377;&#20219;&#20309;&#32500;&#24230;&#21487;&#20197;&#21051;&#30011;&#23398;&#20064;&#20998;&#24067;&#31867;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21482;&#21051;&#30011;&#21487;&#23398;&#20064;&#24615;&#65288;&#32780;&#19981;&#26159;&#37327;&#21270;&#26679;&#26412;&#22797;&#26434;&#24230;&#20989;&#25968;&#65289;&#30340;&#36739;&#24369;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#33258;&#28982;&#30340;&#35201;&#27714;&#65292;&#20197;&#20415;&#23545;&#36825;&#26679;&#19968;&#20010;&#21051;&#30011;&#36827;&#34892;&#26356;&#22909;&#30340;&#29702;&#35299;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19981;&#23384;&#22312;&#19968;&#31181;&#21051;&#30011;&#24615;&#36136;&#65292;&#20197;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#65292;&#23545;&#20110;&#20998;&#24067;&#31867;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#21508;&#31181;&#20854;&#20182;&#23398;&#20064;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27809;&#26377;&#20219;&#20309;&#32500;&#24230;&#21487;&#20197;&#21051;&#30011;&#65288;&#25110;&#21051;&#30011;&#21487;&#23398;&#20064;&#24615;&#65289;&#30340;&#27010;&#24565;&#65292;&#36866;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
We consider the long-standing question of finding a parameter of a class of probability distributions that characterizes its PAC learnability. We provide a rather surprising answer - no such parameter exists. Our techniques allow us to show similar results for several general notions of characterizing learnability and for several learning tasks. We show that there is no notion of dimension that characterizes the sample complexity of learning distribution classes. We then consider the weaker requirement of only characterizing learnability (rather than the quantitative sample complexity function). We propose some natural requirements for such a characterization and go on to show that there exists no characterization of learnability that satisfies these requirements for classes of distributions. Furthermore, we show that our results hold for various other learning problems. In particular, we show that there is no notion of dimension characterizing (or characterization of learnability) for
&lt;/p&gt;</description></item><item><title>LTC-SE&#26159;&#19968;&#31181;&#28082;&#24577;&#26102;&#24120;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#23558;&#22810;&#31181;&#31070;&#32463;&#20803;&#27169;&#22411;&#32479;&#19968;&#65292;&#20854;&#22686;&#24378;&#29256;&#19987;&#27880;&#20110;&#28789;&#27963;&#24615;&#12289;&#20860;&#23481;&#24615;&#21644;&#20195;&#30721;&#32452;&#32455;&#65292;&#28385;&#36275;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#24615;&#33021;&#35201;&#27714;&#65292;&#25193;&#23637;&#20102;&#28082;&#24577;&#31070;&#32463;&#32593;&#32476;&#22312;&#21487;&#25193;&#23637;&#20154;&#24037;&#26234;&#33021;&#21644;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.08691</link><description>&lt;p&gt;
LTC-SE: &#25193;&#23637;&#28082;&#24577;&#26102;&#24120;&#31070;&#32463;&#32593;&#32476;&#22312;&#21487;&#25193;&#23637;&#20154;&#24037;&#26234;&#33021;&#21644;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
LTC-SE: Expanding the Potential of Liquid Time-Constant Neural Networks for Scalable AI and Embedded Systems. (arXiv:2304.08691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08691
&lt;/p&gt;
&lt;p&gt;
LTC-SE&#26159;&#19968;&#31181;&#28082;&#24577;&#26102;&#24120;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#23558;&#22810;&#31181;&#31070;&#32463;&#20803;&#27169;&#22411;&#32479;&#19968;&#65292;&#20854;&#22686;&#24378;&#29256;&#19987;&#27880;&#20110;&#28789;&#27963;&#24615;&#12289;&#20860;&#23481;&#24615;&#21644;&#20195;&#30721;&#32452;&#32455;&#65292;&#28385;&#36275;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#24615;&#33021;&#35201;&#27714;&#65292;&#25193;&#23637;&#20102;&#28082;&#24577;&#31070;&#32463;&#32593;&#32476;&#22312;&#21487;&#25193;&#23637;&#20154;&#24037;&#26234;&#33021;&#21644;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LTC-SE&#65292;&#36825;&#26159;Hasani&#31561;&#20154;&#20110;2021&#24180;&#26368;&#21021;&#25552;&#20986;&#30340;&#28082;&#24577;&#26102;&#24120;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#30340;&#25913;&#36827;&#29256;&#26412;&#12290;&#35813;&#31639;&#27861;&#23558;&#28431;&#30005;&#31215;&#20998;-&#28779;&#31070;&#32463;&#20803;&#27169;&#22411;&#19982;&#36830;&#32493;&#26102;&#38388;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;CTRNN&#65289;&#12289;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#21644;&#37327;&#36523;&#23450;&#21046;&#30340;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#32479;&#19968;&#36215;&#26469;&#12290;LTC-SE&#30340;&#22686;&#24378;&#29256;&#19987;&#27880;&#20110;&#22686;&#24378;&#28789;&#27963;&#24615;&#12289;&#20860;&#23481;&#24615;&#21644;&#20195;&#30721;&#32452;&#32455;&#65292;&#20197;&#28385;&#36275;&#20855;&#26377;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#21644;&#20005;&#26684;&#24615;&#33021;&#35201;&#27714;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#29420;&#29305;&#32422;&#26463;&#12290;&#26356;&#26032;&#21518;&#30340;&#20195;&#30721;&#26159;&#19968;&#20010;&#19982;TensorFlow 2.x&#20860;&#23481;&#30340;&#32508;&#21512;&#31867;&#24211;&#65292;&#20026;LTCCell&#12289;CTRNN&#12289;NODE&#21644;CTGRU&#31867;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#37197;&#32622;&#36873;&#39033;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#20197;&#24448;&#30340;&#29256;&#26412;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#20248;&#21270;&#22312;&#29992;&#25143;&#20307;&#39564;&#12289;Keras&#20989;&#25968;&#20860;&#23481;&#24615;&#21644;&#20195;&#30721;&#28165;&#26224;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#36825;&#20123;&#25913;&#36827;&#25193;&#23637;&#20102;&#28082;&#24577;&#31070;&#32463;&#32593;&#32476;&#22312;&#21487;&#25193;&#23637;&#20154;&#24037;&#26234;&#33021;&#21644;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LTC-SE, an improved version of the Liquid Time-Constant (LTC) neural network algorithm originally proposed by Hasani et al. in 2021. This algorithm unifies the Leaky-Integrate-and-Fire (LIF) spiking neural network model with Continuous-Time Recurrent Neural Networks (CTRNNs), Neural Ordinary Differential Equations (NODEs), and bespoke Gated Recurrent Units (GRUs). The enhancements in LTC-SE focus on augmenting flexibility, compatibility, and code organization, targeting the unique constraints of embedded systems with limited computational resources and strict performance requirements. The updated code serves as a consolidated class library compatible with TensorFlow 2.x, offering comprehensive configuration options for LTCCell, CTRNN, NODE, and CTGRU classes. We evaluate LTC-SE against its predecessors, showcasing the advantages of our optimizations in user experience, Keras function compatibility, and code clarity. These refinements expand the applicability of liquid neural
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#25512;&#36827;&#26144;&#23556;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#24402;&#19968;&#21270;&#27969;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#24212;&#29992;&#31354;&#38388;&#12289;&#26679;&#26412;&#22806;&#25968;&#25454;&#28857;&#21487;&#24212;&#29992;&#24615;&#12289;&#23545;&#20004;&#20010;&#31354;&#38388;&#30340;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#24314;&#27169;&#31561;&#38382;&#39064;&#65292;&#21487;&#24212;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#20197;&#21450;&#20998;&#31867;&#27169;&#22411;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.08673</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#36716;&#25442;&#21644;&#33258;&#36866;&#24212;&#30340;&#25512;&#36827;&#23398;&#20064;&#21322;&#30417;&#30563;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Learning of Pushforwards For Domain Translation &amp; Adaptation. (arXiv:2304.08673v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#25512;&#36827;&#26144;&#23556;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#24402;&#19968;&#21270;&#27969;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#24212;&#29992;&#31354;&#38388;&#12289;&#26679;&#26412;&#22806;&#25968;&#25454;&#28857;&#21487;&#24212;&#29992;&#24615;&#12289;&#23545;&#20004;&#20010;&#31354;&#38388;&#30340;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#24314;&#27169;&#31561;&#38382;&#39064;&#65292;&#21487;&#24212;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#20197;&#21450;&#20998;&#31867;&#27169;&#22411;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24402;&#19968;&#21270;&#27969;&#26469;&#21442;&#25968;&#21270;&#26144;&#23556;&#30340;&#26032;&#39062;&#25512;&#36827;&#26144;&#23556;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#27010;&#29575;&#36317;&#31163;&#21644;&#24212;&#29992;&#29305;&#23450;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#36873;&#25321;&#25152;&#26377;&#21487;&#33021;&#26144;&#23556;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#24191;&#27867;&#24212;&#29992;&#31354;&#38388;&#12289;&#22312;&#26679;&#26412;&#22806;&#25968;&#25454;&#28857;&#19978;&#20855;&#26377;&#21487;&#24212;&#29992;&#24615;&#12289;&#23545;&#20004;&#20010;&#31354;&#38388;&#30340;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#24314;&#27169;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#21487;&#24212;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#20197;&#21450;&#20998;&#31867;&#27169;&#22411;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given two probability densities on related data spaces, we seek a map pushing one density to the other while satisfying application-dependent constraints. For maps to have utility in a broad application space (including domain translation, domain adaptation, and generative modeling), the map must be available to apply on out-of-sample data points and should correspond to a probabilistic model over the two spaces. Unfortunately, existing approaches, which are primarily based on optimal transport, do not address these needs. In this paper, we introduce a novel pushforward map learning algorithm that utilizes normalizing flows to parameterize the map. We first re-formulate the classical optimal transport problem to be map-focused and propose a learning algorithm to select from all possible maps under the constraint that the map minimizes a probability distance and application-specific regularizers; thus, our method can be seen as solving a modified optimal transport problem. Once the map 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#12289;&#20132;&#20114;&#24335;&#30340;&#25163;&#20889;&#33521;&#25991;&#25991;&#26412;&#27880;&#37322;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#25163;&#20889;&#25991;&#26412;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#27169;&#22411;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.08670</link><description>&lt;p&gt;
&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#12289;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25163;&#20889;&#33521;&#25991;&#25991;&#26412;&#27880;&#37322;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An end-to-end, interactive Deep Learning based Annotation system for cursive and print English handwritten text. (arXiv:2304.08670v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#12289;&#20132;&#20114;&#24335;&#30340;&#25163;&#20889;&#33521;&#25991;&#25991;&#26412;&#27880;&#37322;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#25163;&#20889;&#25991;&#26412;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#27169;&#22411;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#20204;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#20351;&#29992;&#35745;&#31639;&#35774;&#22791;&#21644;&#25968;&#23383;&#23186;&#20171;&#36827;&#34892;&#20219;&#21153;&#65292;&#23558;&#20197;&#21069;&#25163;&#21160;&#23436;&#25104;&#30340;&#20219;&#21153;&#36716;&#25442;&#20026;&#25968;&#23383;&#21270;&#29256;&#26412;&#30340;&#20219;&#20309;&#26041;&#27861;&#37117;&#20250;&#21463;&#21040;&#27426;&#36814;&#12290;&#23613;&#31649;&#20170;&#22825;&#21487;&#20197;&#22312;&#32447;&#23436;&#25104;&#35768;&#22810;&#25991;&#26723;&#20219;&#21153;&#65292;&#20294;&#20173;&#26377;&#35768;&#22810;&#24212;&#29992;&#21644;&#39046;&#22495;&#26080;&#27861;&#36991;&#20813;&#25163;&#20889;&#25991;&#26412;&#65292;&#36825;&#20351;&#25163;&#20889;&#25991;&#26723;&#30340;&#25968;&#23383;&#21270;&#25104;&#20026;&#19968;&#39033;&#38750;&#24120;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#65292;&#31163;&#32447;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#26368;&#36817;&#65292;&#22823;&#37096;&#20998;&#30340;&#23581;&#35797;&#24050;&#32463;&#36716;&#21521;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35774;&#35745;&#26356;&#22797;&#26434;&#21644;&#26356;&#28145;&#20837;&#30340;&#32593;&#32476;&#65292;&#24182;&#20445;&#35777;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#26377;&#26356;&#22810;&#30340;&#27880;&#37322;&#25968;&#25454;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#20170;&#22825;&#29992;&#20110;&#31163;&#32447;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#30340;&#22823;&#37096;&#20998;&#25968;&#25454;&#24211;&#37117;&#26159;&#25163;&#21160;&#25110;&#21322;&#33258;&#21160;&#27880;&#37322;&#30340;&#12290;&#36825;&#20123;&#25968;&#25454;&#24211;&#36890;&#24120;&#35268;&#27169;&#36739;&#23567;&#65292;&#26080;&#27861;&#28385;&#36275;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#35201;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#12289;&#20132;&#20114;&#24335;&#30340;&#27880;&#37322;&#31995;&#32479;&#65292;&#20197;&#35299;&#20915;&#25163;&#20889;&#25991;&#26412;&#25968;&#25454;&#30340;&#31232;&#32570;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#25968;&#25454;&#31649;&#29702;&#27169;&#22359;&#12289;&#27880;&#37322;&#27169;&#22359;&#21644;&#27169;&#22411;&#35757;&#32451;&#27169;&#22359;&#12290;&#27880;&#37322;&#27169;&#22359;&#29992;&#25143;&#21451;&#22909;&#65292;&#20801;&#35768;&#20132;&#20114;&#24335;&#27880;&#37322;&#33609;&#20889;&#21644;&#21360;&#21047;&#33521;&#35821;&#25163;&#20889;&#25991;&#26412;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#36824;&#21487;&#20197;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#65292;&#20855;&#26377;&#21512;&#29702;&#30340;&#27880;&#37322;&#26102;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#27169;&#22411;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the surging inclination towards carrying out tasks on computational devices and digital mediums, any method that converts a task that was previously carried out manually, to a digitized version, is always welcome. Irrespective of the various documentation tasks that can be done online today, there are still many applications and domains where handwritten text is inevitable, which makes the digitization of handwritten documents a very essential task. Over the past decades, there has been extensive research on offline handwritten text recognition. In the recent past, most of these attempts have shifted to Machine learning and Deep learning based approaches. In order to design more complex and deeper networks, and ensure stellar performances, it is essential to have larger quantities of annotated data. Most of the databases present for offline handwritten text recognition today, have either been manually annotated or semi automatically annotated with a lot of manual involvement. Thes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#26694;&#26550;&#65292;&#23558;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#22235;&#36275;&#26426;&#22120;&#20154;&#23398;&#20064;&#36830;&#32493;&#36339;&#36291;&#21160;&#20316;&#12290;&#36890;&#36807;&#23398;&#20064;&#21160;&#20316;&#21097;&#20313;&#20540;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#12289;&#36830;&#32493;&#30340;&#36339;&#36291;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2304.08663</link><description>&lt;p&gt;
&#23398;&#20064;&#21160;&#20316;&#21097;&#20313;&#20540;&#30340;&#36830;&#32493;&#22810;&#21151;&#33021;&#36339;&#36291;
&lt;/p&gt;
&lt;p&gt;
Continuous Versatile Jumping Using Learned Action Residuals. (arXiv:2304.08663v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#26694;&#26550;&#65292;&#23558;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#22235;&#36275;&#26426;&#22120;&#20154;&#23398;&#20064;&#36830;&#32493;&#36339;&#36291;&#21160;&#20316;&#12290;&#36890;&#36807;&#23398;&#20064;&#21160;&#20316;&#21097;&#20313;&#20540;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#22810;&#21151;&#33021;&#12289;&#36830;&#32493;&#30340;&#36339;&#36291;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36339;&#36291;&#23545;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#36890;&#36807;&#22256;&#38590;&#22320;&#24418;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#26694;&#26550;&#65292;&#32467;&#21512;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#20026;&#22235;&#36275;&#26426;&#22120;&#20154;&#23398;&#20064;&#36830;&#32493;&#36339;&#36291;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#23039;&#24577;&#25511;&#21046;&#22120;&#65292;&#23427;&#23558;&#25163;&#21160;&#35774;&#35745;&#30340;&#21152;&#36895;&#24230;&#25511;&#21046;&#22120;&#19982;&#23398;&#20064;&#21040;&#30340;&#21097;&#20313;&#31574;&#30053;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#30001;&#20110;&#21152;&#36895;&#24230;&#25511;&#21046;&#22120;&#20026;&#39640;&#25928;&#35757;&#32451;warm start&#31574;&#30053;&#65292;&#25152;&#20197;&#32463;&#36807;&#35757;&#32451;&#30340;&#31574;&#30053;&#20811;&#26381;&#20102;&#21152;&#36895;&#24230;&#25511;&#21046;&#22120;&#30340;&#23616;&#38480;&#24182;&#25552;&#39640;&#20102;&#36339;&#36291;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;&#20302;&#23618;&#20840;&#36523;&#25511;&#21046;&#22120;&#23558;&#23039;&#21183;&#25511;&#21046;&#22120;&#30340;&#36523;&#20307;&#23039;&#21183;&#21629;&#20196;&#36716;&#25442;&#25104;&#30005;&#26426;&#21629;&#20196;&#12290;&#32463;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#19979;&#30340;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#30452;&#25509;&#37096;&#32626;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#65292;&#25191;&#34892;&#22810;&#21151;&#33021;&#30340;&#36830;&#32493;&#36339;&#36291;&#21160;&#20316;&#65292;&#21253;&#25324;&#39640;&#36798;50cm&#12289;&#21521;&#21069;60cm&#30340;&#20840;&#21521;&#36339;&#36291;&#21644;&#39640;&#36798;90&#24230;&#30340;&#36339;&#36291;&#36716;&#21521;&#12290;&#35831;&#35775;&#38382;&#25105;&#20204;&#30340;&#32593;&#31449;&#20197;&#33719;&#21462;&#26356;&#22810;&#32467;&#26524;:https://sites.google.com/view/jumping-rl/home
&lt;/p&gt;
&lt;p&gt;
Jumping is essential for legged robots to traverse through difficult terrains. In this work, we propose a hierarchical framework that combines optimal control and reinforcement learning to learn continuous jumping motions for quadrupedal robots. The core of our framework is a stance controller, which combines a manually designed acceleration controller with a learned residual policy. As the acceleration controller warm starts policy for efficient training, the trained policy overcomes the limitation of the acceleration controller and improves the jumping stability. In addition, a low-level whole-body controller converts the body pose command from the stance controller to motor commands. After training in simulation, our framework can be deployed directly to the real robot, and perform versatile, continuous jumping motions, including omni-directional jumps at up to 50cm high, 60cm forward, and jump-turning at up to 90 degrees. Please visit our website for more results: https://sites.goo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#21644;AI&#26041;&#27861;&#39044;&#27979;DED&#25171;&#21360;&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#23454;&#26102;&#39044;&#27979;&#27599;&#20010;&#27779;&#20811;&#22622;&#23572;&#20013;&#30340;&#27668;&#23380;&#23384;&#22312;&#65292;&#26159;&#19968;&#20010;&#37325;&#22823;&#39134;&#36291;&#12290;</title><link>http://arxiv.org/abs/2304.08658</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#22312;DED&#25171;&#21360;SS316L&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
In-situ surface porosity prediction in DED (directed energy deposition) printed SS316L parts using multimodal sensor fusion. (arXiv:2304.08658v1 [physics.app-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#21644;AI&#26041;&#27861;&#39044;&#27979;DED&#25171;&#21360;&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#23454;&#26102;&#39044;&#27979;&#27599;&#20010;&#27779;&#20811;&#22622;&#23572;&#20013;&#30340;&#27668;&#23380;&#23384;&#22312;&#65292;&#26159;&#19968;&#20010;&#37325;&#22823;&#39134;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#22768;&#21457;&#23556;&#65288;AE&#65289;&#31561;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#26102;&#39057;&#27169;&#24335;&#19982;DED&#36807;&#31243;&#20013;&#30340;&#23380;&#38553;&#29575;&#24418;&#25104;&#36827;&#34892;&#39640;&#31354;&#38388;&#65288;0.5mm&#65289;&#21644;&#26102;&#38388;&#65288;&lt;1ms&#65289;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#20013;&#30340;LIME&#65288;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#38750;&#29305;&#23450;&#24615;&#35299;&#37322;&#65289;&#65292;&#23558;AE&#20013;&#30340;&#26576;&#20123;&#39640;&#39057;&#27874;&#24418;&#29305;&#24449;&#24402;&#22240;&#20110;DED&#36807;&#31243;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#23380;&#38553;&#24418;&#25104;&#36884;&#24452;&#65306;&#39134;&#28293;&#20107;&#20214;&#21644;&#20302;&#28909;&#37327;&#36755;&#20837;&#19979;&#30456;&#37051;&#25171;&#21360;&#36712;&#36857;&#30340;&#19981;&#20805;&#20998;&#29076;&#21512;&#12290;&#35813;&#26041;&#27861;&#20026;&#23454;&#26102;&#39044;&#27979;&#27599;&#20010;&#27779;&#20811;&#22622;&#23572;&#65288;0.5mm&#65289;&#20013;&#30340;&#27668;&#23380;&#23384;&#22312;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#26159;&#19982;&#20808;&#21069;&#21162;&#21147;&#30456;&#27604;&#30340;&#19968;&#20010;&#37325;&#22823;&#39134;&#36291;&#12290;&#22312;&#25171;&#21360;&#24182;&#38543;&#21518;&#21152;&#24037;SS316L&#26448;&#26009;&#26679;&#21697;&#26102;&#65292;&#21516;&#27493;&#37319;&#38598;&#20102;&#21253;&#25324;&#21147;&#65292;AE&#65292;&#25391;&#21160;&#21644;&#28201;&#24230;&#22312;&#20869;&#30340;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#29992;&#20110;&#35782;&#21035;&#20004;&#31181;&#23380;&#38553;&#24418;&#25104;&#36884;&#24452;&#30340;AE&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#36827;&#19968;&#27493;&#20998;&#26512;&#36825;&#20123;&#29305;&#24449;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#21644;AI&#26041;&#27861;&#39044;&#27979;DED&#25171;&#21360;&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to relate the time-frequency patterns of acoustic emission (AE) and other multi-modal sensor data collected in a hybrid directed energy deposition (DED) process to the pore formations at high spatial (0.5 mm) and time (&lt; 1ms) resolutions. Adapting an explainable AI method in LIME (Local Interpretable Model-Agnostic Explanations), certain high-frequency waveform signatures of AE are to be attributed to two major pathways for pore formation in a DED process, namely, spatter events and insufficient fusion between adjacent printing tracks from low heat input. This approach opens an exciting possibility to predict, in real-time, the presence of a pore in every voxel (0.5 mm in size) as they are printed, a major leap forward compared to prior efforts. Synchronized multimodal sensor data including force, AE, vibration and temperature were gathered while an SS316L material sample was printed and subsequently machined. A deep convolution neural network classifier was used to ide
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#23398;&#20064;&#26041;&#27861;&#22312;&#25552;&#39640;&#31070;&#32463;&#25688;&#35201;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#21644;&#29983;&#25104;&#25928;&#26524;&#26041;&#38754;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#23545;&#27604;&#65292;&#32467;&#26524;&#34920;&#26126;&#27010;&#29575;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#29983;&#25104;&#21644;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#25918;&#24323;&#20302;&#36136;&#37327;&#25688;&#35201;&#65292;&#19988;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#22833;&#25928;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2304.08653</link><description>&lt;p&gt;
&#20851;&#20110;&#27010;&#29575;&#31070;&#32463;&#25688;&#35201;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#21644;&#36873;&#25321;&#24615;&#29983;&#25104;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study. (arXiv:2304.08653v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#23398;&#20064;&#26041;&#27861;&#22312;&#25552;&#39640;&#31070;&#32463;&#25688;&#35201;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#21644;&#29983;&#25104;&#25928;&#26524;&#26041;&#38754;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#23545;&#27604;&#65292;&#32467;&#26524;&#34920;&#26126;&#27010;&#29575;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#29983;&#25104;&#21644;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#25918;&#24323;&#20302;&#36136;&#37327;&#25688;&#35201;&#65292;&#19988;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#22833;&#25928;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#25688;&#35201;&#27169;&#22411;&#22312;&#22522;&#20934;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20250;&#29983;&#25104;&#38169;&#35823;&#26657;&#20934;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#23545;&#36136;&#37327;&#36739;&#20302;&#30340;&#39044;&#27979;&#36171;&#20104;&#20102;&#39640;&#20449;&#24515;&#24230;&#65292;&#20174;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23548;&#33268;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24230;&#30340;&#38477;&#20302;&#12290;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26159;&#35299;&#20915;&#35823;&#26657;&#20934;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22797;&#26434;&#33258;&#22238;&#24402;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#30456;&#23545;&#26377;&#25928;&#24615;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24443;&#24213;&#35843;&#26597;&#20102;&#19981;&#21516;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#26041;&#27861;&#22312;&#25552;&#39640;&#31070;&#32463;&#25688;&#35201;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36328;&#36234;&#20102;&#19977;&#20010;&#38590;&#24230;&#19981;&#21516;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27010;&#29575;&#26041;&#27861;&#22987;&#32456;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#21644;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#65292;&#20174;&#32780;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#29983;&#25104;&#65288;&#21363;&#25918;&#24323;&#20302;&#36136;&#37327;&#25688;&#35201;&#65289;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#22833;&#25928;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep models for summarization attains impressive benchmark performance, but they are prone to generating miscalibrated predictive uncertainty. This means that they assign high confidence to low-quality predictions, leading to compromised reliability and trustworthiness in real-world applications. Probabilistic deep learning methods are common solutions to the miscalibration problem. However, their relative effectiveness in complex autoregressive summarization tasks are not well-understood. In this work, we thoroughly investigate different state-of-the-art probabilistic methods' effectiveness in improving the uncertainty quality of the neural summarization models, across three large-scale benchmarks with varying difficulty. We show that the probabilistic methods consistently improve the model's generation and uncertainty quality, leading to improved selective generation performance (i.e., abstaining from low-quality summaries) in practice. We also reveal notable failure patterns 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.08649</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#25216;&#26415;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65288;BERT&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65288;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#35789;&#24615;&#65288;POS&#65289;&#26631;&#35760;&#31561;&#65289;&#19978;&#20135;&#29983;&#20102;&#26368;&#26032;&#25216;&#26415;&#65288;SOTA&#65289;&#32467;&#26524;&#12290;&#24403;&#20998;&#31867;&#38271;&#25991;&#26723;&#65288;&#20363;&#22914;&#26469;&#33258;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#30340;&#25991;&#26723;&#65289;&#26102;&#65292;&#20351;&#29992;BERT&#27169;&#22411;&#21487;&#33021;&#27604;&#36739;&#22256;&#38590;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20960;&#31181;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#25216;&#26415;&#65292;&#29992;&#20110;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#20915;&#23450;&#25110;&#26368;&#39640;&#27861;&#38498;&#25968;&#25454;&#24211;&#65288;SCDB&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#21069;&#30340;SOTA&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#38024;&#23545;&#38271;&#25991;&#26723;&#30340;SOTA&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#65288;1&#65289;&#24191;&#27867;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;15&#20010;&#31867;&#21035;&#65307;&#65288;2&#65289;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;279&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#32467;&#26524;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#20135;&#29983;80&#65285;&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#20135;&#29983;60&#65285;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;TAP&#25968;&#25454;&#24211;,&#25552;&#20379;&#20102;&#20004;&#20010;&#20219;&#21153;:&#20107;&#25925;&#21457;&#29983;&#39044;&#27979;&#21644;&#20107;&#25925;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;,&#20197;&#29992;&#20110;&#21508;&#31181;&#20132;&#36890;&#30456;&#20851;&#30740;&#31350;&#65292;&#27604;&#36739;&#37325;&#35201;&#30340;&#21019;&#26032;&#26159;&#24341;&#20837;&#20102;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19981;&#20877;&#21482;&#30528;&#30524;&#20110;&#21333;&#20010;&#20301;&#32622;&#30340;&#20132;&#36890;&#20107;&#25925;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.08640</link><description>&lt;p&gt;
TAP&#65306;&#36947;&#36335;&#20132;&#36890;&#20107;&#25925;&#32508;&#21512;&#25968;&#25454;&#24211;&#30340;&#26500;&#24314;&#19982;&#30740;&#31350;&#65288;arXiv:2304.08640v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
TAP: A Comprehensive Data Repository for Traffic Accident Prediction in Road Networks. (arXiv:2304.08640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;TAP&#25968;&#25454;&#24211;,&#25552;&#20379;&#20102;&#20004;&#20010;&#20219;&#21153;:&#20107;&#25925;&#21457;&#29983;&#39044;&#27979;&#21644;&#20107;&#25925;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;,&#20197;&#29992;&#20110;&#21508;&#31181;&#20132;&#36890;&#30456;&#20851;&#30740;&#31350;&#65292;&#27604;&#36739;&#37325;&#35201;&#30340;&#21019;&#26032;&#26159;&#24341;&#20837;&#20102;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19981;&#20877;&#21482;&#30528;&#30524;&#20110;&#21333;&#20010;&#20301;&#32622;&#30340;&#20132;&#36890;&#20107;&#25925;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#23433;&#20840;&#26159;&#20840;&#29699;&#20844;&#20849;&#21355;&#29983;&#20851;&#27880;&#30340;&#37325;&#28857;&#12290;&#20934;&#30830;&#30340;&#20132;&#36890;&#20107;&#25925;&#39044;&#27979;&#22312;&#20943;&#23569;&#20132;&#36890;&#20107;&#25925;&#26041;&#38754;&#20855;&#26377;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#21482;&#30528;&#30524;&#20110;&#21333;&#20010;&#20301;&#32622;&#30340;&#20132;&#36890;&#20107;&#25925;&#39044;&#27979;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21040;&#36335;&#32593;&#20869;&#19981;&#21516;&#20107;&#25925;&#22320;&#28857;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#65292;&#36825;&#23601;&#38656;&#35201;&#24341;&#20837;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#20363;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21487;&#20197;&#33258;&#28982;&#22320;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23558;GNNs&#24212;&#29992;&#20110;&#20107;&#25925;&#39044;&#27979;&#38382;&#39064;&#38754;&#20020;&#30528;&#22914;&#20309;&#33719;&#21462;&#21512;&#36866;&#30340;&#22270;&#32467;&#26500;&#20132;&#36890;&#20107;&#25925;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#65292;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#20132;&#36890;&#20107;&#25925;&#39044;&#27979;&#65288;TAP&#65289;&#25968;&#25454;&#24211;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#20856;&#22411;&#20219;&#21153;&#65306;&#20107;&#25925;&#21457;&#29983;&#39044;&#27979;&#21644;&#20107;&#25925;&#20005;&#37325;&#31243;&#24230;&#39044;&#27979;&#12290;&#36825;&#20010;&#25968;&#25454;&#24211;&#35206;&#30422;&#20840;&#22269;&#65292;&#20855;&#26377;&#30495;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#21644;&#20016;&#23500;&#30340;&#22320;&#29702;&#31354;&#38388;&#29305;&#24449;&#65292;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#20132;&#36890;&#30456;&#20851;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road safety is a major global public health concern. Effective traffic crash prediction can play a critical role in reducing road traffic accidents. However, Existing machine learning approaches tend to focus on predicting traffic accidents in isolation, without considering the potential relationships between different accident locations within road networks. To incorporate graph structure information, graph-based approaches such as Graph Neural Networks (GNNs) can be naturally applied. However, applying GNNs to the accident prediction problem faces challenges due to the lack of suitable graph-structured traffic accident datasets. To bridge this gap, we have constructed a real-world graph-based Traffic Accident Prediction (TAP) data repository, along with two representative tasks: accident occurrence prediction and accident severity prediction. With nationwide coverage, real-world network topology, and rich geospatial features, this data repository can be used for a variety of traffic-
&lt;/p&gt;</description></item><item><title>pgmpy&#26159;&#19968;&#20010;Python&#24037;&#20855;&#21253;&#65292;&#23427;&#25552;&#20379;&#20102;&#29992;&#20110;&#22788;&#29702;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#30456;&#20851;&#27169;&#22411;&#30340;&#31639;&#27861;&#21644;&#24037;&#20855;&#65292;&#24182;&#20391;&#37325;&#20110;&#26131;&#20110;&#25193;&#23637;&#24615;&#65292;&#20351;&#24471;&#29992;&#25143;&#33021;&#22815;&#24555;&#36895;&#22320;&#20462;&#25913;&#12289;&#28155;&#21152;&#25110;&#23454;&#29616;&#26032;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.08639</link><description>&lt;p&gt;
pgmpy: &#19968;&#20010;&#29992;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340; Python &#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
pgmpy: A Python Toolkit for Bayesian Networks. (arXiv:2304.08639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08639
&lt;/p&gt;
&lt;p&gt;
pgmpy&#26159;&#19968;&#20010;Python&#24037;&#20855;&#21253;&#65292;&#23427;&#25552;&#20379;&#20102;&#29992;&#20110;&#22788;&#29702;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#30456;&#20851;&#27169;&#22411;&#30340;&#31639;&#27861;&#21644;&#24037;&#20855;&#65292;&#24182;&#20391;&#37325;&#20110;&#26131;&#20110;&#25193;&#23637;&#24615;&#65292;&#20351;&#24471;&#29992;&#25143;&#33021;&#22815;&#24555;&#36895;&#22320;&#20462;&#25913;&#12289;&#28155;&#21152;&#25110;&#23454;&#29616;&#26032;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BN&#65289;&#22312;&#22810;&#20010;&#39046;&#22495;&#29992;&#20110;&#24314;&#27169;&#12289;&#39044;&#27979;&#21644;&#20915;&#31574;&#21046;&#23450;&#12290;pgmpy&#26159;&#19968;&#20010;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#31639;&#27861;&#21644;&#24037;&#20855;&#26469;&#22788;&#29702;BNs&#21644;&#30456;&#20851;&#27169;&#22411;&#12290;&#23427;&#23454;&#29616;&#20102;&#32467;&#26500;&#23398;&#20064;&#12289;&#21442;&#25968;&#20272;&#35745;&#12289;&#36817;&#20284;&#21644;&#31934;&#30830;&#25512;&#29702;&#12289;&#22240;&#26524;&#25512;&#29702;&#21644;&#27169;&#25311;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#23454;&#29616;&#20391;&#37325;&#20110;&#27169;&#22359;&#21270;&#21644;&#26131;&#20110;&#25193;&#23637;&#65292;&#20801;&#35768;&#29992;&#25143;&#24555;&#36895;&#20462;&#25913;/&#28155;&#21152;&#29616;&#26377;&#31639;&#27861;&#65292;&#25110;&#20026;&#19981;&#21516;&#29992;&#20363;&#23454;&#29616;&#26032;&#31639;&#27861;&#12290;pgmpy&#22312;MIT&#35768;&#21487;&#35777;&#19979;&#21457;&#24067;&#65307;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/pgmpy/pgmpy&#25214;&#21040;&#65292;&#25991;&#26723;&#21487;&#22312;https://pgmpy.org&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Networks (BNs) are used in various fields for modeling, prediction, and decision making. pgmpy is a python package that provides a collection of algorithms and tools to work with BNs and related models. It implements algorithms for structure learning, parameter estimation, approximate and exact inference, causal inference, and simulations. These implementations focus on modularity and easy extensibility to allow users to quickly modify/add to existing algorithms, or to implement new algorithms for different use cases. pgmpy is released under the MIT License; the source code is available at: https://github.com/pgmpy/pgmpy, and the documentation at: https://pgmpy.org.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#20061;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#21457;&#29616;&#20854;&#20013;80&#65285;&#21253;&#21547;&#35760;&#24518;&#25968;&#25454;&#65292;&#20294;&#21253;&#21547;&#26368;&#22810;&#35760;&#24518;&#20869;&#23481;&#30340;&#36755;&#20986;&#26356;&#21487;&#33021;&#26159;&#39640;&#36136;&#37327;&#30340;&#12290;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#38477;&#20302;&#35760;&#24518;&#25991;&#26412;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.08637</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#35780;&#20272;&#65306;&#35805;&#35821;&#21644;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
An Evaluation on Large Language Model Outputs: Discourse and Memorization. (arXiv:2304.08637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08637
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#20061;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#21457;&#29616;&#20854;&#20013;80&#65285;&#21253;&#21547;&#35760;&#24518;&#25968;&#25454;&#65292;&#20294;&#21253;&#21547;&#26368;&#22810;&#35760;&#24518;&#20869;&#23481;&#30340;&#36755;&#20986;&#26356;&#21487;&#33021;&#26159;&#39640;&#36136;&#37327;&#30340;&#12290;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#38477;&#20302;&#35760;&#24518;&#25991;&#26412;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#20061;&#20010;&#26368;&#24191;&#27867;&#21487;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#21508;&#31181;&#36755;&#20986;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#25104;&#30340;&#24037;&#20855;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#19982;&#36755;&#20986;&#30149;&#24577;&#65288;&#20363;&#22914;&#65292;&#21453;&#20107;&#23454;&#21644;&#36923;&#36753;&#19978;&#30340;&#38169;&#35823;&#38472;&#36848;&#65289;&#20197;&#21450;&#19981;&#20445;&#25345;&#20027;&#39064;&#31561;&#26041;&#38754;&#30340;&#20851;&#31995;&#20013;&#65292;&#35760;&#24518;&#25991;&#26412;&#30334;&#20998;&#27604;&#12289;&#29420;&#29305;&#25991;&#26412;&#30334;&#20998;&#27604;&#21644;&#25972;&#20307;&#36755;&#20986;&#36136;&#37327;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;80.0&#65285;&#30340;&#36755;&#20986;&#21253;&#21547;&#35760;&#24518;&#25968;&#25454;&#65292;&#20294;&#21253;&#21547;&#26368;&#22810;&#35760;&#24518;&#20869;&#23481;&#30340;&#36755;&#20986;&#20063;&#26356;&#26377;&#21487;&#33021;&#34987;&#35748;&#20026;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#21644;&#35780;&#20272;&#20102;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#26174;&#31034;&#65292;&#22312;&#35780;&#20272;&#30340;&#27169;&#22411;&#20013;&#65292;&#36755;&#20986;&#30340;&#35760;&#24518;&#25991;&#26412;&#29575;&#26377;&#25152;&#38477;&#20302;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23601;&#23398;&#20064;&#12289;&#35760;&#24518;&#21644;&#35780;&#20272;&#20248;&#36136;&#25991;&#26412;&#30340;&#28508;&#22312;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an empirical evaluation of various outputs generated by nine of the most widely-available large language models (LLMs). Our analysis is done with off-the-shelf, readily-available tools. We find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic. Overall, 80.0% of the outputs evaluated contained memorized data, but outputs containing the most memorized content were also more likely to be considered of high quality. We discuss and evaluate mitigation strategies, showing that, in the models evaluated, the rate of memorized text being output is reduced. We conclude with a discussion on potential implications around what it means to learn, to memorize, and to evaluate quality text.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#30561;&#30496;&#34892;&#20026;&#29305;&#24449;&#22312;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#35774;&#32622;&#19979;&#20272;&#35745;&#31934;&#31070;&#30149;&#24739;&#32773;&#22797;&#21457;&#26085;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#30701;&#26102;&#30561;&#30496;&#34892;&#20026;&#29305;&#24449;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.08614</link><description>&lt;p&gt;
&#20449;&#21495;&#22788;&#29702;&#22823;&#25361;&#25112;2023&#8212;&#8212;&#30005;&#23376;&#39044;&#38450;&#65306;&#30561;&#30496;&#34892;&#20026;&#20316;&#20026;&#31934;&#31070;&#30149;&#24739;&#32773;&#22797;&#21457;&#25351;&#26631;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Signal Processing Grand Challenge 2023 -- e-Prevention: Sleep Behavior as an Indicator of Relapses in Psychotic Patients. (arXiv:2304.08614v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08614
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#30561;&#30496;&#34892;&#20026;&#29305;&#24449;&#22312;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#35774;&#32622;&#19979;&#20272;&#35745;&#31934;&#31070;&#30149;&#24739;&#32773;&#22797;&#21457;&#26085;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#30701;&#26102;&#30561;&#30496;&#34892;&#20026;&#29305;&#24449;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;USC SAIL&#25552;&#20986;&#30340;&#22312;&#20449;&#21495;&#22788;&#29702;&#22823;&#25361;&#25112;2023&#8212;&#8212;&#30005;&#23376;&#39044;&#38450;&#65288;&#20219;&#21153;2&#65289;&#20013;&#65292;&#26816;&#27979;&#31934;&#31070;&#30149;&#24739;&#32773;&#22797;&#21457;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;&#22797;&#21457;&#39044;&#27979;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20027;&#35201;&#30001;&#20110;&#19981;&#21516;&#20010;&#20307;&#30151;&#29366;&#21644;&#23545;&#27835;&#30103;&#30340;&#21453;&#24212;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#21033;&#29992;&#30561;&#30496;&#34892;&#20026;&#29305;&#24449;&#20272;&#35745;&#24322;&#24120;&#20540;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#35774;&#32622;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;&#37326;&#22806;&#37319;&#38598;&#30340;&#20154;&#31867;&#27963;&#21160;&#21644;&#24515;&#29575;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#24182;&#35780;&#20272;&#20102;&#21508;&#31181;&#29305;&#24449;&#31867;&#22411;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30701;&#26102;&#30561;&#30496;&#34892;&#20026;&#29305;&#24449;&#34920;&#29616;&#20986;&#27604;&#20854;&#28165;&#37266;&#30340;&#21516;&#34892;&#21644;&#26356;&#22823;&#30340;&#26102;&#38388;&#38388;&#38548;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#25552;&#20132;&#22312;&#20219;&#21153;&#30340;&#23448;&#26041;&#25490;&#34892;&#27036;&#19978;&#25490;&#21517;&#31532;&#19977;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#29305;&#24449;&#20316;&#20026;&#31934;&#31070;&#30149;&#22797;&#21457;&#30340;&#23458;&#35266;&#19988;&#38750;&#20405;&#20837;&#24615;&#30340;&#39044;&#27979;&#25351;&#26631;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the approach and results of USC SAIL's submission to the Signal Processing Grand Challenge 2023 - e-Prevention (Task 2), on detecting relapses in psychotic patients. Relapse prediction has proven to be challenging, primarily due to the heterogeneity of symptoms and responses to treatment between individuals. We address these challenges by investigating the use of sleep behavior features to estimate relapse days as outliers in an unsupervised machine learning setting. We extract informative features from human activity and heart rate data collected in the wild, and evaluate various combinations of feature types and time resolutions. We found that short-time sleep behavior features outperformed their awake counterparts and larger time intervals. Our submission was ranked 3rd in the Task's official leaderboard, demonstrating the potential of such features as an objective and non-invasive predictor of psychotic relapses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.08612</link><description>&lt;p&gt;
&#31163;&#25955;&#19982;&#21453;&#21521;&#20256;&#25773;&#30340;&#26725;&#26753;&#65306;&#30452;&#36890;&#27861;&#19982;&#20854;&#23427;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#20165;&#38480;&#20110;&#35745;&#31639;&#36830;&#32493;&#21464;&#37327;&#30340;&#26799;&#24230;&#65292;&#38480;&#21046;&#20102;&#28041;&#21450;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#23519;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340; Straight-Through&#65288;ST&#65289;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#20316;&#20026;&#26799;&#24230;&#30340;&#19968;&#38454;&#36817;&#20284;&#20540;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; ReinMax&#65292;&#23427;&#38598;&#25104;&#20102; Heun's Method&#65292;&#19968;&#31181;&#35299;ODE&#30340;&#20108;&#38454;&#25968;&#20540;&#26041;&#27861;&#65292;&#20197;&#36817;&#20284;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201; Hessian &#25110;&#20854;&#20182;&#20108;&#38454;&#23548;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#36755;&#20986;&#39044;&#27979;&#21644;&#26080;&#30417;&#30563;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;\ours &#22312;&#29616;&#26377;&#25216;&#26415;&#20013;&#24102;&#26469;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324; ST &#21644; Straight-Through Gum&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation, the cornerstone of deep learning, is limited to computing gradients solely for continuous variables. This limitation hinders various research on problems involving discrete latent variables. To address this issue, we propose a novel approach for approximating the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose a novel method called ReinMax, which integrates Heun's Method, a second-order numerical method for solving ODEs, to approximate the gradient. Our method achieves second-order accuracy without requiring Hessian or other second-order derivatives. We conduct experiments on structured output prediction and unsupervised generative modeling tasks. Our results show that \ours brings consistent improvements over the state of the art, including ST and Straight-Through Gum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#20197;&#20445;&#25252;&#28040;&#36153;&#32773;&#38544;&#31169;&#21644;&#25552;&#39640;&#25968;&#25454;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#33021;&#30340;&#25968;&#25454;&#20998;&#21306;&#12289;&#36890;&#20449;&#25299;&#25169;&#21644;&#23433;&#20840;&#26426;&#21046;&#20998;&#31867;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#35813;&#25216;&#26415;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.08602</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#19982;&#26234;&#33021;&#30005;&#32593;&#30340;&#20132;&#21449;&#36335;&#24452;&#65306;&#27010;&#36848;&#65292;&#25361;&#25112;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crossing Roads of Federated Learning and Smart Grids: Overview, Challenges, and Perspectives. (arXiv:2304.08602v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#20197;&#20445;&#25252;&#28040;&#36153;&#32773;&#38544;&#31169;&#21644;&#25552;&#39640;&#25968;&#25454;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#33021;&#30340;&#25968;&#25454;&#20998;&#21306;&#12289;&#36890;&#20449;&#25299;&#25169;&#21644;&#23433;&#20840;&#26426;&#21046;&#20998;&#31867;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#35813;&#25216;&#26415;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#20351;&#24471;&#28040;&#36153;&#32773;&#30340;&#38544;&#31169;&#25104;&#20026;&#26234;&#33021;&#30005;&#32593;&#65288;SGs&#65289;&#20013;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#23588;&#20854;&#26159;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#29992;&#20110;&#19981;&#21516;&#26381;&#21153;&#26102;&#12290;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#23558;&#35757;&#32451;&#25512;&#21521;&#36793;&#32536;&#65292;&#20026;&#38544;&#31169;&#20445;&#25252;&#21644;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#25552;&#20379;&#20102;&#24456;&#22909;&#30340;&#25240;&#34935;&#26041;&#26696;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;SGs&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20248;&#32570;&#28857;&#65292;&#20027;&#35201;&#21253;&#25324;&#36127;&#36733;&#39044;&#27979;&#65292;&#30005;&#21160;&#27773;&#36710;&#65292;&#25925;&#38556;&#35786;&#26029;&#65292;&#36127;&#36733;&#20998;&#35299;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#31561;&#12290;&#27492;&#22806;&#65292;&#36824;&#25506;&#35752;&#20102;&#25968;&#25454;&#20998;&#21306;&#12289;&#36890;&#20449;&#25299;&#25169;&#21644;&#23433;&#20840;&#26426;&#21046;&#31561;&#26041;&#38754;&#30340;&#20027;&#35201;&#35774;&#35745;&#36235;&#21183;&#21644;&#21487;&#33021;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#35813;&#25216;&#26415;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consumer's privacy is a main concern in Smart Grids (SGs) due to the sensitivity of energy data, particularly when used to train machine learning models for different services. These data-driven models often require huge amounts of data to achieve acceptable performance leading in most cases to risks of privacy leakage. By pushing the training to the edge, Federated Learning (FL) offers a good compromise between privacy preservation and the predictive performance of these models. The current paper presents an overview of FL applications in SGs while discussing their advantages and drawbacks, mainly in load forecasting, electric vehicles, fault diagnoses, load disaggregation and renewable energies. In addition, an analysis of main design trends and possible taxonomies is provided considering data partitioning, the communication topology, and security mechanisms. Towards the end, an overview of main challenges facing this technology and potential future directions is presented.
&lt;/p&gt;</description></item><item><title>eTOP&#26694;&#26550;&#21487;&#20197;&#22312;&#20219;&#20309;AutoML&#31995;&#32479;&#20043;&#19978;&#24037;&#20316;&#65292;&#24182;&#20915;&#23450;&#26159;&#21542;&#23558;&#25191;&#34892;&#31649;&#36947;&#21040;&#26368;&#21518;&#25110;&#22312;&#20013;&#38388;&#27493;&#39588;&#32456;&#27490;&#20197;&#26356;&#24555;&#22320;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.08597</link><description>&lt;p&gt;
eTOP&#65306;&#29992;&#20110;&#26356;&#24555;&#22320;&#35757;&#32451;AutoML&#31995;&#32479;&#30340;&#31649;&#36947;&#25552;&#21069;&#32456;&#27490;
&lt;/p&gt;
&lt;p&gt;
eTOP: Early Termination of Pipelines for Faster Training of AutoML Systems. (arXiv:2304.08597v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08597
&lt;/p&gt;
&lt;p&gt;
eTOP&#26694;&#26550;&#21487;&#20197;&#22312;&#20219;&#20309;AutoML&#31995;&#32479;&#20043;&#19978;&#24037;&#20316;&#65292;&#24182;&#20915;&#23450;&#26159;&#21542;&#23558;&#25191;&#34892;&#31649;&#36947;&#21040;&#26368;&#21518;&#25110;&#22312;&#20013;&#38388;&#27493;&#39588;&#32456;&#27490;&#20197;&#26356;&#24555;&#22320;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#21644;&#30828;&#20214;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;AI/ML&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#21040;&#26085;&#24120;&#24212;&#29992;&#20013;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25152;&#25552;&#20379;&#26381;&#21153;&#30340;&#36136;&#37327;&#12290;&#20294;&#26159;&#23545;&#20110;&#32473;&#23450;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#25214;&#21040;&#21512;&#36866;&#30340;AI/ML&#27169;&#22411;&#26159;&#19968;&#20010;&#22797;&#26434;&#32780;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#28041;&#21450;&#22810;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#27493;&#39588;&#65288;&#31216;&#20026;&#31649;&#36947;&#65289;&#65292;&#22914;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#24037;&#31243;&#12289;&#36873;&#25321;&#21644;&#27169;&#22411;&#35843;&#25972;&#30340;&#29983;&#25104;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#31561;&#12290;&#36825;&#20123;&#31649;&#36947;&#22312;&#32467;&#26500;&#19978;&#26159;&#22797;&#26434;&#30340;&#65292;&#22312;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#19978;&#37117;&#24456;&#26114;&#36149;&#65292;&#24182;&#19982;&#27599;&#20010;&#27493;&#39588;&#30456;&#20851;&#32852;&#12290;AutoML&#31995;&#32479;&#33258;&#21160;&#25628;&#32034;&#36825;&#20123;&#36229;&#21442;&#25968;&#65292;&#20294;&#36895;&#24230;&#24456;&#24930;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#31649;&#36947;&#30340;&#26368;&#32456;&#36755;&#20986;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;eTOP&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#22312;&#20219;&#20309;AutoML&#31995;&#32479;&#20043;&#19978;&#24037;&#20316;&#65292;&#24182;&#20915;&#23450;&#26159;&#21542;&#23558;&#31649;&#36947;&#25191;&#34892;&#21040;&#26368;&#21518;&#25110;&#22312;&#20013;&#38388;&#27493;&#39588;&#32456;&#27490;&#12290;&#22312;26&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#20197;&#21450;eTOP&#19982;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in software and hardware technologies have enabled the use of AI/ML models in everyday applications has significantly improved the quality of service rendered. However, for a given application, finding the right AI/ML model is a complex and costly process, that involves the generation, training, and evaluation of multiple interlinked steps (called pipelines), such as data pre-processing, feature engineering, selection, and model tuning. These pipelines are complex (in structure) and costly (both in compute resource and time) to execute end-to-end, with a hyper-parameter associated with each step. AutoML systems automate the search of these hyper-parameters but are slow, as they rely on optimizing the pipeline's end output. We propose the eTOP Framework which works on top of any AutoML system and decides whether or not to execute the pipeline to the end or terminate at an intermediate step. Experimental evaluation on 26 benchmark datasets and integration of eTOPwith 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#39044;&#27979;&#26041;&#27861;&#20934;&#30830;&#22320;&#23398;&#20064;&#36741;&#21161;&#20449;&#21495;&#30340;&#20316;&#29992;&#65292;&#20026;&#39044;&#27979;&#34880;&#31958;&#31561;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.08593</link><description>&lt;p&gt;
&#21033;&#29992;&#31232;&#30095;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65306;&#20197;&#39044;&#27979;&#34880;&#31958;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Forecasting with Sparse but Informative Variables: A Case Study in Predicting Blood Glucose. (arXiv:2304.08593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#39044;&#27979;&#26041;&#27861;&#20934;&#30830;&#22320;&#23398;&#20064;&#36741;&#21161;&#20449;&#21495;&#30340;&#20316;&#29992;&#65292;&#20026;&#39044;&#27979;&#34880;&#31958;&#31561;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#26410;&#26469;&#30340;&#30446;&#26631;&#20540;&#21487;&#33021;&#20250;&#21463;&#21040;&#20869;&#22312;&#21644;&#22806;&#22312;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#24403;&#39044;&#27979;&#34880;&#31958;&#26102;&#65292;&#20869;&#22312;&#20316;&#29992;&#21487;&#20197;&#20165;&#36890;&#36807;&#30446;&#26631;&#20449;&#21495;&#30340;&#21382;&#21490;&#35760;&#24405;&#65288;&#21363;&#34880;&#31958;&#65289;&#26469;&#25512;&#26029;&#65292;&#20294;&#31934;&#30830;&#24314;&#27169;&#22806;&#22312;&#24433;&#21709;&#30340;&#24433;&#21709;&#38656;&#35201;&#36741;&#21161;&#20449;&#21495;&#65292;&#20363;&#22914;&#25668;&#20837;&#30340;&#30899;&#27700;&#21270;&#21512;&#29289;&#37327;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#31232;&#30095;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#21464;&#37327;&#65288;SIVs&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#65288;i&#65289;&#23558;SIVs&#25928;&#26524;&#30340;&#23398;&#20064;&#31934;&#30830;&#22320;&#23398;&#20064;&#27599;&#20010;&#26102;&#38388;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In time-series forecasting, future target values may be affected by both intrinsic and extrinsic effects. When forecasting blood glucose, for example, intrinsic effects can be inferred from the history of the target signal alone (\textit{i.e.} blood glucose), but accurately modeling the impact of extrinsic effects requires auxiliary signals, like the amount of carbohydrates ingested. Standard forecasting techniques often assume that extrinsic and intrinsic effects vary at similar rates. However, when auxiliary signals are generated at a much lower frequency than the target variable (e.g., blood glucose measurements are made every 5 minutes, while meals occur once every few hours), even well-known extrinsic effects (e.g., carbohydrates increase blood glucose) may prove difficult to learn. To better utilize these \textit{sparse but informative variables} (SIVs), we introduce a novel encoder/decoder forecasting approach that accurately learns the per-timepoint effect of the SIV, by (i) is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#26041;&#26696;&#65292;&#36890;&#36807;&#36866;&#24212;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#20869;&#30340;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#21644;&#35745;&#31639;&#36127;&#36733;&#65292;&#20248;&#21270;&#25910;&#25947;&#36895;&#24230;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#36127;&#36733;&#12290;</title><link>http://arxiv.org/abs/2304.08589</link><description>&lt;p&gt;
&#24555;&#36895;&#24182;&#23481;&#38169;&#30340;&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#65292;&#38477;&#20302;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast and Straggler-Tolerant Distributed SGD with Reduced Computation Load. (arXiv:2304.08589v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#26041;&#26696;&#65292;&#36890;&#36807;&#36866;&#24212;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#20869;&#30340;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#21644;&#35745;&#31639;&#36127;&#36733;&#65292;&#20248;&#21270;&#25910;&#25947;&#36895;&#24230;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#20013;&#24515;&#33410;&#28857;&#23558;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#36816;&#31639;&#22806;&#21253;&#32473;&#22806;&#37096;&#30340;&#24037;&#20316;&#33410;&#28857;&#12290;&#20248;&#21270;&#36807;&#31243;&#30340;&#23646;&#24615;&#65292;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#65292;&#21487;&#20197;&#21033;&#29992;&#20197;&#20943;&#36731;&#19981;&#21709;&#24212;&#25110;&#36895;&#24230;&#24930;&#30340;&#24037;&#20154;&#65288;&#31216;&#20026;&#36831;&#38045;&#32773;&#65289;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#36825;&#20123;&#24773;&#20917;&#20250;&#38477;&#20302;&#35745;&#31639;&#22806;&#21253;&#30340;&#25910;&#30410;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#20165;&#31561;&#24453;&#27599;&#20010;&#31639;&#27861;&#36845;&#20195;&#20013;&#30340;&#19968;&#37096;&#20998;&#24037;&#20316;&#33410;&#28857;&#23436;&#25104;&#20854;&#35745;&#31639;&#26469;&#23454;&#29616;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#36866;&#24212;&#31561;&#24453;&#24037;&#20154;&#25968;&#37327;&#38543;&#31639;&#27861;&#28436;&#21270;&#20197;&#20248;&#21270;&#25910;&#25947;&#36895;&#24230;&#30340;&#26041;&#27861;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#29420;&#31435;&#30340;&#38543;&#26426;&#21464;&#37327;&#23545;&#36890;&#20449;&#21644;&#35745;&#31639;&#26102;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#26469;&#36866;&#24212;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#20869;&#30340;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#21644;&#35745;&#31639;&#36127;&#36733;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;SGD&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#26174;&#30528;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
In distributed machine learning, a central node outsources computationally expensive calculations to external worker nodes. The properties of optimization procedures like stochastic gradient descent (SGD) can be leveraged to mitigate the effect of unresponsive or slow workers called stragglers, that otherwise degrade the benefit of outsourcing the computation. This can be done by only waiting for a subset of the workers to finish their computation at each iteration of the algorithm. Previous works proposed to adapt the number of workers to wait for as the algorithm evolves to optimize the speed of convergence. In contrast, we model the communication and computation times using independent random variables. Considering this model, we construct a novel scheme that adapts both the number of workers and the computation load throughout the run-time of the algorithm. Consequently, we improve the convergence speed of distributed SGD while significantly reducing the computation load, at the ex
&lt;/p&gt;</description></item><item><title>CAM2&#26159;&#19968;&#20010;&#38754;&#21521;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#30340;&#19968;&#33268;&#24615;&#24863;&#30693;&#22810;&#20219;&#21153;&#25490;&#21517;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#24314;&#27169;&#31995;&#32479;&#22320;&#35299;&#24320;&#29992;&#25143;&#23545;&#27969;&#34892;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#19982;&#20182;&#20204;&#30495;&#27491;&#20852;&#36259;&#30340;&#32852;&#31995;&#65292;&#26469;&#28040;&#38500;&#21382;&#21490;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#24102;&#26469;&#30340;&#19968;&#33268;&#24615;&#20559;&#35265;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#24471;&#21040;&#26377;&#25928;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.08562</link><description>&lt;p&gt;
CAM2: &#38754;&#21521;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#30340;&#19968;&#33268;&#24615;&#24863;&#30693;&#22810;&#20219;&#21153;&#25490;&#21517;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CAM2: Conformity-Aware Multi-Task Ranking Model for Large-Scale Recommender Systems. (arXiv:2304.08562v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08562
&lt;/p&gt;
&lt;p&gt;
CAM2&#26159;&#19968;&#20010;&#38754;&#21521;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#30340;&#19968;&#33268;&#24615;&#24863;&#30693;&#22810;&#20219;&#21153;&#25490;&#21517;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#24314;&#27169;&#31995;&#32479;&#22320;&#35299;&#24320;&#29992;&#25143;&#23545;&#27969;&#34892;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#19982;&#20182;&#20204;&#30495;&#27491;&#20852;&#36259;&#30340;&#32852;&#31995;&#65292;&#26469;&#28040;&#38500;&#21382;&#21490;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#24102;&#26469;&#30340;&#19968;&#33268;&#24615;&#20559;&#35265;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#24471;&#21040;&#26377;&#25928;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21382;&#21490;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#25311;&#21512;&#21040;&#22823;&#35268;&#27169;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#20013;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#19968;&#33268;&#24615;&#20559;&#35265;&#65292;&#22240;&#20026;&#29992;&#25143;&#20852;&#36259;&#21487;&#33021;&#24456;&#38590;&#30830;&#23450;&#65292;&#32780;&#35768;&#22810;&#39033;&#30446;&#36890;&#24120;&#22522;&#20110;&#29983;&#24577;&#31995;&#32479;&#22240;&#32032;&#32780;&#19981;&#26159;&#19982;&#20010;&#20307;&#29992;&#25143;&#30456;&#20851;&#24615;&#20132;&#20114;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CAM2&#65292;&#36825;&#26159;&#19968;&#20010;&#19968;&#33268;&#24615;&#24863;&#30693;&#30340;&#22810;&#20219;&#21153;&#25490;&#21517;&#27169;&#22411;&#65292;&#26088;&#22312;&#20026;&#20854;&#20013;&#19968;&#20010;&#26368;&#22823;&#30340;&#24037;&#19994;&#25512;&#33616;&#24179;&#21488;&#21521;&#29992;&#25143;&#25552;&#20379;&#30456;&#20851;&#29289;&#21697;&#12290;CAM2&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#24314;&#27169;&#31995;&#32479;&#22320;&#35299;&#24320;&#29992;&#25143;&#23545;&#27969;&#34892;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#19982;&#20182;&#20204;&#30495;&#27491;&#20852;&#36259;&#30340;&#32852;&#31995;&#12290;&#36825;&#20010;&#26694;&#26550;&#26159;&#21487;&#25512;&#24191;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#25193;&#23637;&#20197;&#25903;&#25345;&#22312;&#20219;&#20309;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22810;&#20010;&#19968;&#33268;&#24615;&#21644;&#29992;&#25143;&#30456;&#20851;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#23454;&#36341;&#35265;&#35299;&#65292;&#24182;&#36890;&#36807;&#31163;&#32447;&#35780;&#20272;&#30340;&#25913;&#36827;&#26469;&#28436;&#31034;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning large-scale industrial recommender system models by fitting them to historical user interaction data makes them vulnerable to conformity bias. This may be due to a number of factors, including the fact that user interests may be difficult to determine and that many items are often interacted with based on ecosystem factors other than their relevance to the individual user. In this work, we introduce CAM2, a conformity-aware multi-task ranking model to serve relevant items to users on one of the largest industrial recommendation platforms. CAM2 addresses these challenges systematically by leveraging causal modeling to disentangle users' conformity to popular items from their true interests. This framework is generalizable and can be scaled to support multiple representations of conformity and user relevance in any large-scale recommender system. We provide deeper practical insights and demonstrate the effectiveness of the proposed model through improvements in offline evaluatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#23376;&#22270;&#37051;&#22495;&#27719;&#32858;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#23376;&#22270;&#20998;&#31867;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08556</link><description>&lt;p&gt;
&#38543;&#26426;&#23376;&#22270;&#37051;&#22495;&#27719;&#32858;&#29992;&#20110;&#23376;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Stochastic Subgraph Neighborhood Pooling for Subgraph Classification. (arXiv:2304.08556v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08556
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#23376;&#22270;&#37051;&#22495;&#27719;&#32858;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#23376;&#22270;&#20998;&#31867;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#22270;&#20998;&#31867;&#26159;&#22270;&#34920;&#24449;&#23398;&#20064;&#20013;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#20854;&#20219;&#21153;&#26159;&#23545;&#22270;&#20013;&#30340;&#19968;&#32452;&#33410;&#28857;&#65288;&#21363;&#23376;&#22270;&#65289;&#36827;&#34892;&#20998;&#31867;&#12290;&#23376;&#22270;&#20998;&#31867;&#20855;&#26377;&#39044;&#27979;&#19968;&#32452;&#34507;&#30333;&#36136;&#30340;&#32454;&#32990;&#21151;&#33021;&#25110;&#22312;&#32473;&#23450;&#34920;&#22411;&#38598;&#21512;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#32597;&#35265;&#30142;&#30149;&#31561;&#24212;&#29992;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#33410;&#28857;&#12289;&#38142;&#21644;&#22270;&#32423;&#20219;&#21153;&#30340;&#20107;&#23454;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22312;&#23376;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#21363;&#20351;&#26159;&#20026;&#22270;&#20998;&#31867;&#37327;&#36523;&#23450;&#21046;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20063;&#19981;&#33021;&#30452;&#25509;&#36716;&#21270;&#20026;&#23376;&#22270;&#20998;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#30053;&#20102;&#23376;&#22270;&#30340;&#22806;&#37096;&#25299;&#25169;&#32467;&#26500;&#65292;&#20174;&#32780;&#26080;&#27861;&#25429;&#25417;&#23376;&#22270;&#22312;&#22823;&#22270;&#20013;&#30340;&#20301;&#32622;&#12290;&#30446;&#21069;&#29992;&#20110;&#23376;&#22270;&#20998;&#31867;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#36890;&#36807;&#26631;&#31614;&#25216;&#24039;&#25110;&#22810;&#20010;&#28040;&#24687;&#20256;&#36882;&#36890;&#36947;&#26469;&#35299;&#20915;&#36825;&#20010;&#32570;&#38519;&#65292;&#20294;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20250;&#23545;&#35745;&#31639;&#26426;&#36896;&#25104;&#36127;&#25285;&#65292;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#24182;&#20445;&#25345;&#31934;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#23376;&#22270;&#37051;&#22495;&#27719;&#32858; (SSNP)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38543;&#26426;&#27719;&#32858;&#23376;&#22270;&#37051;&#22495;&#26469;&#21033;&#29992;&#22270;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;SSNP&#22312;&#20960;&#20010;&#23376;&#22270;&#20998;&#31867;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subgraph classification is an emerging field in graph representation learning where the task is to classify a group of nodes (i.e., a subgraph) within a graph. Subgraph classification has applications such as predicting the cellular function of a group of proteins or identifying rare diseases given a collection of phenotypes. Graph neural networks (GNNs) are the de facto solution for node, link, and graph-level tasks but fail to perform well on subgraph classification tasks. Even GNNs tailored for graph classification are not directly transferable to subgraph classification as they ignore the external topology of the subgraph, thus failing to capture how the subgraph is located within the larger graph. The current state-of-the-art models for subgraph classification address this shortcoming through either labeling tricks or multiple message-passing channels, both of which impose a computation burden and are not scalable to large graphs. To address the scalability issue while maintaining
&lt;/p&gt;</description></item><item><title>STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.08503</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08503
&lt;/p&gt;
&lt;p&gt;
STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;(STO)&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#26088;&#22312;&#21033;&#29992;&#20648;&#23384;&#22312;&#25968;&#25454;&#24211;&#20013;&#20197;&#21069;&#27714;&#35299;&#30340;&#20248;&#21270;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31639;&#27861;&#35774;&#35745;&#24050;&#26377;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;STO&#20013;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#24182;&#19981;&#23436;&#21892;&#12290;&#23427;&#20204;&#24448;&#24448;&#26159;&#30001;&#20854;&#20182;&#22522;&#20934;&#20989;&#25968;&#38543;&#26426;&#32452;&#21512;&#32780;&#25104;&#65292;&#36825;&#20123;&#22522;&#20934;&#20989;&#25968;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20339;&#20540;&#65292;&#25110;&#32773;&#29983;&#25104;&#33258;&#34920;&#29616;&#20986;&#26377;&#38480;&#21464;&#21270;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20013;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#25163;&#21160;&#37197;&#32622;&#30340;&#65292;&#22240;&#27492;&#21333;&#35843;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#34920;&#24449;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#31639;&#27861;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#21462;&#24471;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#20855;&#26377;&#39640;&#24230;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#38382;&#39064;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20123;&#34920;&#24449;STO&#38382;&#39064;&#30340;&#22522;&#26412;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential transfer optimization (STO), which aims to improve optimization performance by exploiting knowledge captured from previously-solved optimization tasks stored in a database, has been gaining increasing research attention in recent years. However, despite significant advancements in algorithm design, the test problems in STO are not well designed. Oftentimes, they are either randomly assembled by other benchmark functions that have identical optima or are generated from practical problems that exhibit limited variations. The relationships between the optimal solutions of source and target tasks in these problems are manually configured and thus monotonous, limiting their ability to represent the diverse relationships of real-world problems. Consequently, the promising results achieved by many algorithms on these problems are highly biased and difficult to be generalized to other problems. In light of this, we first introduce a few rudimentary concepts for characterizing STO pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#28608;&#20809;&#35825;&#23548;&#20987;&#31359;&#20809;&#35889;&#25216;&#26415;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;LSTM&#21644;GRU&#65292;&#25552;&#20379;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.08500</link><description>&lt;p&gt;
&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#28608;&#20809;&#35825;&#23548;&#20987;&#31359;&#20809;&#35889;&#25216;&#26415;&#20013;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A comparison between Recurrent Neural Networks and classical machine learning approaches In Laser induced breakdown spectroscopy. (arXiv:2304.08500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#28608;&#20809;&#35825;&#23548;&#20987;&#31359;&#20809;&#35889;&#25216;&#26415;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;LSTM&#21644;GRU&#65292;&#25552;&#20379;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31867;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#26102;&#38388;&#21160;&#21147;&#23398;&#20998;&#26512;&#20013;&#24314;&#31435;&#19981;&#21516;&#33410;&#28857;&#20043;&#38388;&#30340;&#36830;&#25509;&#24418;&#25104;&#26377;&#21521;&#25110;&#26080;&#21521;&#22270;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#28608;&#20809;&#35825;&#23548;&#20987;&#31359;&#20809;&#35889;&#25216;&#26415;&#23545;&#38109;&#21512;&#37329;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#22914;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;(LSTM)&#12289;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;(GRU)&#12289;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(Simple RNN)&#20197;&#21450;&#30001;&#21367;&#31215;-&#31616;&#21333;&#36882;&#24402;&#32593;&#32476;(Conv-SimpleRNN)&#12289;&#21367;&#31215;-LSTM(Conv-LSTM)&#21644;&#21367;&#31215;-GRU(Conv-GRU)&#32452;&#25104;&#30340;&#36882;&#24402;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#38109;&#26631;&#20934;&#26679;&#21697;&#27987;&#24230;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#23558;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#19982;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#26426;(SVR)&#12289;&#38543;&#26426;&#26862;&#26519;(RF)&#12289;k&#26368;&#36817;&#37051;(k-NN)&#21644;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;(MLR)&#31561;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;LSTM&#21644;GRU&#65292;&#25552;&#20379;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Networks are classes of Artificial Neural Networks that establish connections between different nodes form a directed or undirected graph for temporal dynamical analysis. In this research, the laser induced breakdown spectroscopy (LIBS) technique is used for quantitative analysis of aluminum alloys by different Recurrent Neural Network (RNN) architecture. The fundamental harmonic (1064 nm) of a nanosecond Nd:YAG laser pulse is employed to generate the LIBS plasma for the prediction of constituent concentrations of the aluminum standard samples. Here, Recurrent Neural Networks based on different networks, such as Long Short Term Memory (LSTM), Gated Recurrent Unit (GRU), Simple Recurrent Neural Network (Simple RNN), and as well as Recurrent Convolutional Networks comprising of Conv-SimpleRNN, Conv-LSTM and Conv-GRU are utilized for concentration prediction. Then a comparison is performed among prediction by classical machine learning methods of support vector regressor 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#20010;&#26032;&#39062;&#30340;&#24819;&#27861;&#65292;&#20998;&#21035;&#37319;&#29992;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#21644;&#20132;&#38169;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#35823;&#24046;&#21644;&#27169;&#22411;&#20869;&#37096;&#20559;&#35265;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08498</link><description>&lt;p&gt;
&#25490;&#21517;&#25439;&#22833;&#21644;&#20132;&#38169;&#23398;&#20064;&#20943;&#23569;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#25628;&#32034;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Ranking Loss and Sequestering Learning for Reducing Image Search Bias in Histopathology. (arXiv:2304.08498v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#20010;&#26032;&#39062;&#30340;&#24819;&#27861;&#65292;&#20998;&#21035;&#37319;&#29992;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#21644;&#20132;&#38169;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#35823;&#24046;&#21644;&#27169;&#22411;&#20869;&#37096;&#20559;&#35265;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#24212;&#29992;&#20013;&#26377;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#21253;&#25324;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#30340;&#22270;&#20687;&#25628;&#32034;&#12290;&#23613;&#31649;&#35745;&#31639;&#26426;&#35270;&#35273;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#30149;&#29702;&#23398;&#26723;&#26696;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#22823;&#38382;&#39064;&#12290;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#26159;AI&#20559;&#35265;&#21644;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#19968;&#20010;&#26356;&#29305;&#21035;&#30340;&#28145;&#24230;&#27169;&#22411;&#32570;&#28857;&#26159;&#23545;&#25628;&#32034;&#21151;&#33021;&#30340;&#26080;&#30693;&#12290;&#21069;&#32773;&#24433;&#21709;&#27599;&#20010;&#27169;&#22411;&#65292;&#21518;&#32773;&#21482;&#24433;&#21709;&#25628;&#32034;&#21644;&#21305;&#37197;&#12290;&#30001;&#20110;&#32570;&#20047;&#22522;&#20110;&#25490;&#21517;&#30340;&#23398;&#20064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24517;&#39035;&#22522;&#20110;&#20998;&#31867;&#35823;&#24046;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#25152;&#24471;&#30340;&#23884;&#20837;&#36827;&#34892;&#22270;&#20687;&#25628;&#32034;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#20351;&#29992;&#21508;&#31181;&#21307;&#38498;&#30340;&#22823;&#22411;&#22270;&#20687;&#24211;&#65292;&#28145;&#24230;&#27169;&#22411;&#20284;&#20046;&#20063;&#23481;&#26131;&#20135;&#29983;&#20869;&#37096;&#20559;&#35265;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#24819;&#27861;&#20197;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#26469;&#25351;&#23548;&#29305;&#24449;&#25552;&#21462;&#26397;&#21521;&#25628;&#32034;&#30340;&#21305;&#37197;&#23548;&#21521;&#24615;&#12290;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#23398;&#20064;&#25490;&#21517;&#65292;&#25105;&#20204;&#21487;&#20197;&#36991;&#20813;&#20998;&#31867;&#35823;&#24046;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#20132;&#38169;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#20943;&#23569;&#27169;&#22411;&#20869;&#37096;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning has started to play an essential role in healthcare applications, including image search in digital pathology. Despite the recent progress in computer vision, significant issues remain for image searching in histopathology archives. A well-known problem is AI bias and lack of generalization. A more particular shortcoming of deep models is the ignorance toward search functionality. The former affects every model, the latter only search and matching. Due to the lack of ranking-based learning, researchers must train models based on the classification error and then use the resultant embedding for image search purposes. Moreover, deep models appear to be prone to internal bias even if using a large image repository of various hospitals. This paper proposes two novel ideas to improve image search performance. First, we use a ranking loss function to guide feature extraction toward the matching-oriented nature of the search. By forcing the model to learn the ranking o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20026;&#20174;&#19994;&#20154;&#21592;&#25552;&#20379;&#25277;&#35937;&#23618;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#21644;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#65292;&#24182;&#25903;&#25345;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#65288;QFL&#65289;&#12290;</title><link>http://arxiv.org/abs/2304.08496</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#30340;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-Driven Quantum Federated Learning (QFL). (arXiv:2304.08496v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08496
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20026;&#20174;&#19994;&#20154;&#21592;&#25552;&#20379;&#25277;&#35937;&#23618;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#21644;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#65292;&#24182;&#25903;&#25345;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#65288;QFL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26377;&#22810;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#65288;QFL&#65289;&#30340;&#26694;&#26550;&#12290;&#20363;&#22914;&#65292;&#35895;&#27468;TensorFlow Quantum&#65288;TFQ&#65289;&#21644;TensorFlow Federated&#65288;TFF&#65289;&#24211;&#24050;&#32463;&#34987;&#29992;&#20110;&#23454;&#29616;QFL&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#32773;&#22823;&#22810;&#19981;&#29087;&#24713;&#37327;&#23376;&#35745;&#31639;&#65288;QC&#65289;&#24211;&#21644;&#26694;&#26550;&#12290;&#25552;&#20379;&#19968;&#20010;&#20026;QC&#21644;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24211;&#25552;&#20379;&#25277;&#35937;&#23618;&#30340;&#39046;&#22495;&#29305;&#23450;&#24314;&#27169;&#35821;&#35328;&#65288;DSML&#65289;&#26159;&#26377;&#30410;&#30340;&#12290;&#36825;&#21487;&#20197;&#20351;&#20174;&#19994;&#20154;&#21592;&#22312;&#37096;&#32626;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#26368;&#26032;&#25216;&#26415;&#30340;&#21516;&#26102;&#39640;&#25928;&#22320;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#21644;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#25193;&#23637;&#29616;&#26377;&#30340;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21551;&#29992;&#31995;&#32479;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#39537;&#21160;&#24037;&#31243;&#65288;MDE&#65289;&#24037;&#20855;&#65288;&#22914;MontiAnna&#12289;ML-Quadrat&#21644;GreyCat&#65289;&#20197;&#25903;&#25345;QFL&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, several studies have proposed frameworks for Quantum Federated Learning (QFL). For instance, the Google TensorFlow Quantum (TFQ) and TensorFlow Federated (TFF) libraries have been deployed for realizing QFL. However, developers, in the main, are not as yet familiar with Quantum Computing (QC) libraries and frameworks. A Domain-Specific Modeling Language (DSML) that provides an abstraction layer over the underlying QC and Federated Learning (FL) libraries would be beneficial. This could enable practitioners to carry out software development and data science tasks efficiently while deploying the state of the art in Quantum Machine Learning (QML). In this position paper, we propose extending existing domain-specific Model-Driven Engineering (MDE) tools for Machine Learning (ML) enabled systems, such as MontiAnna, ML-Quadrat, and GreyCat, to support QFL.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#24067;&#24335;&#25191;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21327;&#35843;&#25511;&#21046;&#22810;&#20010;&#26080;&#20154;&#26426;&#22312;&#33258;&#20027;&#31227;&#21160;&#25509;&#20837;&#24212;&#29992;&#20013;&#65292;&#26368;&#22823;&#21270;&#26381;&#21153;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.08493</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#32676;&#22312;&#33258;&#20027;&#31227;&#21160;&#25509;&#20837;&#24212;&#29992;&#20013;&#30340;&#21327;&#21516;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Coordinated Multi-Agent Reinforcement Learning for Unmanned Aerial Vehicle Swarms in Autonomous Mobile Access Applications. (arXiv:2304.08493v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#24067;&#24335;&#25191;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21327;&#35843;&#25511;&#21046;&#22810;&#20010;&#26080;&#20154;&#26426;&#22312;&#33258;&#20027;&#31227;&#21160;&#25509;&#20837;&#24212;&#29992;&#20013;&#65292;&#26368;&#22823;&#21270;&#26381;&#21153;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38598;&#20013;&#24335;&#35757;&#32451;&#21644;&#20998;&#24067;&#24335;&#25191;&#34892; (CTDE) &#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (MADRL) &#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#22810;&#20010;&#26080;&#20154;&#26426;&#22312;&#33258;&#20027;&#31227;&#21160;&#25509;&#20837;&#24212;&#29992;&#20013;&#12290;&#20026;&#27492;&#65292;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#38598;&#20013;&#24335;&#35757;&#32451;&#20013;&#29992;&#20110;&#21327;&#20316;&#22810;&#20010;&#26234;&#33021;&#20307;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#31227;&#21160;&#25509;&#20837;&#24212;&#29992;&#20013;&#30340;&#24635;&#26381;&#21153;&#36136;&#37327; (QoS)&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel centralized training and distributed execution (CTDE)-based multi-agent deep reinforcement learning (MADRL) method for multiple unmanned aerial vehicles (UAVs) control in autonomous mobile access applications. For the purpose, a single neural network is utilized in centralized training for cooperation among multiple agents while maximizing the total quality of service (QoS) in mobile access applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24120;&#35265;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#20869;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#23558;&#26368;&#23567;&#21518;&#38376;&#27010;&#24565;&#21644;&#21487;&#37197;&#32622;&#30340;&#30828;&#20214;&#26408;&#39532;&#32467;&#21512;&#20351;&#29992;&#65292;&#20174;&#32780;&#23545;&#30446;&#21069;&#30340;&#38450;&#24481;&#25514;&#26045;&#26500;&#25104;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.08411</link><description>&lt;p&gt;
&#26469;&#33258;&#20869;&#37096;&#30340;&#37034;&#24694;: &#36890;&#36807;&#30828;&#20214;&#26408;&#39532;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Evil from Within: Machine Learning Backdoors through Hardware Trojans. (arXiv:2304.08411v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24120;&#35265;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#20869;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#23558;&#26368;&#23567;&#21518;&#38376;&#27010;&#24565;&#21644;&#21487;&#37197;&#32622;&#30340;&#30828;&#20214;&#26408;&#39532;&#32467;&#21512;&#20351;&#29992;&#65292;&#20174;&#32780;&#23545;&#30446;&#21069;&#30340;&#38450;&#24481;&#25514;&#26045;&#26500;&#25104;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#20250;&#23545;&#26426;&#22120;&#23398;&#20064;&#36896;&#25104;&#20005;&#37325;&#23041;&#32961;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#30772;&#22351;&#23433;&#20840;&#20851;&#38190;&#30340;&#31995;&#32479;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#23436;&#20840;&#23621;&#20110;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24120;&#35265;&#30828;&#20214;&#21152;&#36895;&#22120;&#20869;&#65292;&#20174;&#32780;&#23545;&#24403;&#21069;&#38450;&#24481;&#25514;&#26045;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;&#36825;&#31181;&#25915;&#20987;&#23454;&#29992;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#30001;&#20110;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#30340;&#23384;&#20648;&#31354;&#38388;&#20005;&#37325;&#21463;&#38480;&#65292;&#22240;&#27492;&#25105;&#20204;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#26368;&#23567;&#21518;&#38376;&#27010;&#24565;&#65292;&#21482;&#25913;&#21464;&#23569;&#37327;&#27169;&#22411;&#21442;&#25968;&#21363;&#21487;&#28608;&#27963;&#21518;&#38376;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#37197;&#32622;&#30340;&#30828;&#20214;&#26408;&#39532;&#65292;&#21487;&#20197;&#19982;&#21518;&#38376;&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoors pose a serious threat to machine learning, as they can compromise the integrity of security-critical systems, such as self-driving cars. While different defenses have been proposed to address this threat, they all rely on the assumption that the hardware on which the learning models are executed during inference is trusted. In this paper, we challenge this assumption and introduce a backdoor attack that completely resides within a common hardware accelerator for machine learning. Outside of the accelerator, neither the learning model nor the software is manipulated, so that current defenses fail. To make this attack practical, we overcome two challenges: First, as memory on a hardware accelerator is severely limited, we introduce the concept of a minimal backdoor that deviates as little as possible from the original model and is activated by replacing a few model parameters only. Second, we develop a configurable hardware trojan that can be provisioned with the backdoor and p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#35774;&#35745;&#20986;&#19968;&#20010;&#22810;&#27169;&#24577;&#30701;&#35270;&#39057;&#35875;&#35328;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#22810;&#31181;&#29305;&#24449;&#30340;&#30701;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#19982;&#22806;&#37096;&#30693;&#35782;&#65292;&#33021;&#26377;&#25928;&#22320;&#21306;&#20998;&#30701;&#35270;&#39057;&#35875;&#35328;&#12290;</title><link>http://arxiv.org/abs/2304.08401</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#30701;&#35270;&#39057;&#35875;&#35328;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Multimodal Short Video Rumor Detection System Based on Contrastive Learning. (arXiv:2304.08401v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#35774;&#35745;&#20986;&#19968;&#20010;&#22810;&#27169;&#24577;&#30701;&#35270;&#39057;&#35875;&#35328;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#22810;&#31181;&#29305;&#24449;&#30340;&#30701;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#19982;&#22806;&#37096;&#30693;&#35782;&#65292;&#33021;&#26377;&#25928;&#22320;&#21306;&#20998;&#30701;&#35270;&#39057;&#35875;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30701;&#35270;&#39057;&#24179;&#21488;&#25104;&#20026;&#26032;&#38395;&#20998;&#20139;&#30340;&#37325;&#35201;&#28192;&#36947;&#20043;&#19968;&#65292;&#20013;&#22269;&#20027;&#35201;&#30701;&#35270;&#39057;&#24179;&#21488;&#36880;&#28176;&#25104;&#20026;&#34394;&#20551;&#26032;&#38395;&#30340;&#26032;&#28363;&#29983;&#22320;&#12290;&#30001;&#20110;&#30701;&#35270;&#39057;&#21253;&#21547;&#20102;&#22823;&#37327;&#20449;&#24687;&#21644;&#29305;&#24449;&#65292;&#20197;&#21450;&#35270;&#39057;&#20043;&#38388;&#30340;&#20005;&#37325;&#21516;&#36136;&#21270;&#21644;&#30456;&#20284;&#24615;&#65292;&#22240;&#27492;&#24456;&#38590;&#21306;&#20998;&#30701;&#35270;&#39057;&#35875;&#35328;&#12290;&#20026;&#20102;&#20943;&#36731;&#30701;&#35270;&#39057;&#35875;&#35328;&#30340;&#20256;&#25773;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;&#32771;&#34385;&#21040;&#27599;&#31181;&#31639;&#27861;&#30340;&#20248;&#32570;&#28857;&#65292;&#26500;&#24314;&#20102;&#22810;&#27169;&#24577;&#29305;&#24449;&#34701;&#21512;&#24182;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#30701;&#35270;&#39057;&#35875;&#35328;&#26816;&#27979;&#12290;&#26816;&#27979;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#65306;&#65288;1&#65289;&#21019;&#24314;&#25968;&#25454;&#38598;&#65306;&#26500;&#24314;&#20855;&#26377;&#22810;&#31181;&#29305;&#24449;&#30340;&#30701;&#35270;&#39057;&#25968;&#25454;&#38598;&#65307;&#65288;2&#65289;&#22810;&#27169;&#24577;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#65306;&#39318;&#20808;&#20351;&#29992;TSN&#35270;&#39057;&#32534;&#30721;&#27169;&#22411;&#25552;&#21462;&#35270;&#39057;&#29305;&#24449;&#65307;&#28982;&#21518;&#20351;&#29992;OCR&#21644;ASR&#25552;&#21462;&#35270;&#39057;&#30340;&#25991;&#26412;&#29305;&#24449;&#65307;&#26368;&#21518;&#65292;&#23558;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#34701;&#21512;&#26469;&#36827;&#34892;&#30701;&#35270;&#39057;&#35875;&#35328;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
With short video platforms becoming one of the important channels for news sharing, major short video platforms in China have gradually become new breeding grounds for fake news. However, it is not easy to distinguish short video rumors due to the great amount of information and features contained in short videos, as well as the serious homogenization and similarity of features among videos. In order to mitigate the spread of short video rumors, our group decides to detect short video rumors by constructing multimodal feature fusion and introducing external knowledge after considering the advantages and disadvantages of each algorithm. The ideas of detection are as follows: (1) dataset creation: to build a short video dataset with multiple features; (2) multimodal rumor detection model: firstly, we use TSN (Temporal Segment Networks) video coding model to extract video features; then, we use OCR (Optical Character Recognition) and ASR (Automatic Character Recognition) to extract video 
&lt;/p&gt;</description></item><item><title>&#23545;&#38472;&#31561;&#20154;&#21457;&#34920;&#22312;&#12298;&#33258;&#28982;&#8212;&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#12299;&#26434;&#24535;&#19978;&#30340;&#8220;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#25628;&#32034;&#8221;&#19968;&#25991;&#30340;&#35780;&#35770;&#21644;&#20851;&#20999;&#12290;</title><link>http://arxiv.org/abs/2304.08297</link><description>&lt;p&gt;
&#23545;&#8220;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#25628;&#32034;&#8221;&#30340;&#35780;&#35770;&#65288;arXiv: 2304.08297v2 [eess.IV] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Comments on 'Fast and scalable search of whole-slide images via self-supervised deep learning'. (arXiv:2304.08297v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08297
&lt;/p&gt;
&lt;p&gt;
&#23545;&#38472;&#31561;&#20154;&#21457;&#34920;&#22312;&#12298;&#33258;&#28982;&#8212;&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#12299;&#26434;&#24535;&#19978;&#30340;&#8220;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#25628;&#32034;&#8221;&#19968;&#25991;&#30340;&#35780;&#35770;&#21644;&#20851;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38472;&#31561;&#20154;&#65288;Chen2022&#65289;&#22312;&#12298;&#33258;&#28982;&#8212;&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#20102;&#39064;&#20026;&#8220;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#25628;&#32034;&#8221;&#30340;&#25991;&#31456;&#12290;&#35813;&#25991;&#31456;&#20316;&#32773;&#31216;&#20854;&#26041;&#27861;&#20026;&#8220;&#32452;&#32455;&#23398;&#33258;&#30417;&#30563;&#22270;&#20687;&#25628;&#32034;&#65292;&#31616;&#31216;SISH&#12290;&#8221;&#25105;&#20204;&#23545;SISH&#34920;&#31034;&#20102;&#20851;&#20999;&#65292;&#22240;&#20026;&#23427;&#26159;Yottixel&#30340;&#22686;&#37327;&#20462;&#25913;&#65292;&#20351;&#29992;&#20102;MinMax&#20108;&#20540;&#21270;&#20294;&#26410;&#24341;&#29992;&#21407;&#22987;&#20316;&#21697;&#65292;&#24182;&#22522;&#20110;&#19968;&#20010;&#35823;&#31216;&#8220;&#33258;&#30417;&#30563;&#22270;&#20687;&#25628;&#32034;&#8221;&#30340;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#38472;&#31561;&#20154;&#36827;&#34892;&#23454;&#39564;&#21644;&#27604;&#36739;&#26102;&#23384;&#22312;&#30340;&#20960;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chen et al. [Chen2022] recently published the article 'Fast and scalable search of whole-slide images via self-supervised deep learning' in Nature Biomedical Engineering. The authors call their method 'self-supervised image search for histology', short SISH. We express our concerns that SISH is an incremental modification of Yottixel, has used MinMax binarization but does not cite the original works, and is based on a misnomer 'self-supervised image search'. As well, we point to several other concerns regarding experiments and comparisons performed by Chen et al.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#36817;&#36793;&#32536;&#26696;&#20363;&#30340;&#38754;&#37096;&#39564;&#35777;&#38382;&#39064;&#65292;&#21457;&#29616;&#32467;&#21512;&#20154;&#26426;&#20915;&#31574;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08134</link><description>&lt;p&gt;
&#35299;&#20915;&#38754;&#37096;&#39564;&#35777;&#36793;&#32536;&#26696;&#20363;&#65306;&#28145;&#24230;&#20998;&#26512;&#21644;&#20154;&#26426;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tackling Face Verification Edge Cases: In-Depth Analysis and Human-Machine Fusion Approach. (arXiv:2304.08134v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#36817;&#36793;&#32536;&#26696;&#20363;&#30340;&#38754;&#37096;&#39564;&#35777;&#38382;&#39064;&#65292;&#21457;&#29616;&#32467;&#21512;&#20154;&#26426;&#20915;&#31574;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#38754;&#37096;&#35782;&#21035;&#31995;&#32479;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#24050;&#32463;&#36229;&#36807;&#20102;&#20154;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#26426;&#22120;&#26080;&#27861;&#27491;&#30830;&#20998;&#31867;&#30340;&#36793;&#32536;&#26696;&#20363;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#21644;&#20154;&#25805;&#20316;&#21592;&#22312;&#38754;&#37096;&#39564;&#35777;&#20219;&#21153;&#20013;&#30340;&#32452;&#21512;&#25928;&#24212;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#36793;&#32536;&#26696;&#20363;&#65292;&#20197;&#21457;&#29616;&#24120;&#35265;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#24615;&#35774;&#32622;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#36873;&#23450;&#20219;&#21153;&#20013;&#30340;60&#20010;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#31867;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#26426;&#22120;&#21644;&#20154;&#31867;&#20915;&#31574;&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;GitHub&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, face recognition systems surpass human performance on several datasets. However, there are still edge cases that the machine can't correctly classify. This paper investigates the effect of a combination of machine and human operators in the face verification task. First, we look closer at the edge cases for several state-of-the-art models to discover common datasets' challenging settings. Then, we conduct a study with 60 participants on these selected tasks with humans and provide an extensive analysis. Finally, we demonstrate that combining machine and human decisions can further improve the performance of state-of-the-art face verification systems on various benchmark datasets. Code and data are publicly available on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#25968;&#25454;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#22343;&#36136;&#21435;&#38654;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;NH-HAZE23&#25968;&#25454;&#38598;&#31561;&#38750;&#22343;&#36136;&#38654;&#22270;&#20687;&#26102;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#28385;&#36275;&#24314;&#27169;&#22343;&#36136;&#38654;&#25152;&#38656;&#30340;&#20551;&#35774;&#20043;&#19968;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25351;&#20986;&#20809;&#38752;&#25968;&#25454;&#22686;&#24191;&#24182;&#19981;&#33021;&#35299;&#20915;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#22788;&#29702;&#20998;&#24067;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.07874</link><description>&lt;p&gt;
&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30340;&#12289;&#22522;&#20110;Vision Transformer&#30340;&#38750;&#22343;&#36136;&#21435;&#38654;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Data-Centric Solution to NonHomogeneous Dehazing via Vision Transformer. (arXiv:2304.07874v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#25968;&#25454;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#22343;&#36136;&#21435;&#38654;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;NH-HAZE23&#25968;&#25454;&#38598;&#31561;&#38750;&#22343;&#36136;&#38654;&#22270;&#20687;&#26102;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#28385;&#36275;&#24314;&#27169;&#22343;&#36136;&#38654;&#25152;&#38656;&#30340;&#20551;&#35774;&#20043;&#19968;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25351;&#20986;&#20809;&#38752;&#25968;&#25454;&#22686;&#24191;&#24182;&#19981;&#33021;&#35299;&#20915;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#22788;&#29702;&#20998;&#24067;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#21435;&#38654;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#22343;&#36136;&#38654;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#22312;&#24212;&#29992;&#20110;&#23384;&#22312;&#38750;&#22343;&#36136;&#38654;&#30340;&#22270;&#20687;&#26102;&#20445;&#25345;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;NTIRE&#25361;&#25112;&#20171;&#32461;&#30340;NH-HAZE23&#25968;&#25454;&#38598;&#12290;&#20854;&#20013;&#19968;&#20010;&#22833;&#36133;&#30340;&#21407;&#22240;&#26159;&#38750;&#22343;&#36136;&#38654;&#19981;&#31526;&#21512;&#24314;&#27169;&#22343;&#36136;&#38654;&#25152;&#38656;&#30340;&#20551;&#35774;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#38656;&#35201;&#23545;&#22823;&#37327;&#30340;&#38750;&#22343;&#36136;&#38654;&#22270;&#20687;&#19982;&#20854;&#28165;&#26224;&#23545;&#24212;&#39033;&#36827;&#34892;&#37197;&#23545;&#65292;&#32780;NH-HAZE23&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#26159;&#26377;&#38480;&#30340;&#12290;&#23613;&#31649;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#38750;&#22343;&#36136;&#21435;&#38654;&#25968;&#25454;&#38598;&#25193;&#20805;NH-HAZE23&#25968;&#25454;&#38598;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#26377;&#24517;&#35201;&#35774;&#35745;&#19968;&#31181;&#36866;&#24403;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#20197;&#20943;&#23569;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an increased interest in image dehazing. Many deep learning methods have been proposed to tackle this challenge, and have made significant accomplishments dealing with homogeneous haze. However, these solutions cannot maintain comparable performance when they are applied to images with non-homogeneous haze, e.g., NH-HAZE23 dataset introduced by NTIRE challenges. One of the reasons for such failures is that non-homogeneous haze does not obey one of the assumptions that is required for modeling homogeneous haze. In addition, a large number of pairs of non-homogeneous hazy image and the clean counterpart is required using traditional end-to-end training approaches, while NH-HAZE23 dataset is of limited quantities. Although it is possible to augment the NH-HAZE23 dataset by leveraging other non-homogeneous dehazing datasets, we observe that it is necessary to design a proper data-preprocessing approach that reduces the distribution gaps between the target datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Deep Metric Learning&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;&#65292;&#33021;&#22815;&#26377;&#25928;&#30340;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07689</link><description>&lt;p&gt;
&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#29992;&#20110;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Empirical Bregman Divergence for Uncertain Distance Representation. (arXiv:2304.07689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Deep Metric Learning&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;&#65292;&#33021;&#22815;&#26377;&#25928;&#30340;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25216;&#26415;&#24050;&#24212;&#29992;&#20110;&#21508;&#31181;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#28145;&#24230;&#32593;&#32476;&#23398;&#20064;&#26679;&#26412;&#23884;&#20837;&#26469;&#36827;&#34892;&#35270;&#35273;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#26041;&#27861;&#37319;&#29992;&#22266;&#23450;&#36317;&#31163;&#24230;&#37327;&#20316;&#20026;&#20004;&#20010;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#21487;&#33021;&#23548;&#33268;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#20122;&#26368;&#20248;&#24615;&#33021;&#12290;Bregman&#25955;&#24230;&#27010;&#25324;&#20102;&#21508;&#31181;&#36317;&#31163;&#24230;&#37327;&#30340;&#24230;&#37327;&#65292;&#24182;&#22312;&#35768;&#22810;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#39046;&#22495;&#20013;&#20135;&#29983;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;Bregman&#25955;&#24230;&#33719;&#24471;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25439;&#22833;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35774;&#32622;&#23545;Bregman&#25955;&#24230;&#19979;&#30340;&#20984;&#20989;&#25968;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;SOTA&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20116;&#20010;&#27969;&#34892;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep metric learning techniques have been used for visual representation in various supervised and unsupervised learning tasks through learning embeddings of samples with deep networks. However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may lead to suboptimal performance for capturing the complex data distribution. The Bregman divergence generalizes measures of various distance metrics and arises throughout many fields of deep metric learning. In this paper, we first show how deep metric learning loss can arise from the Bregman divergence. We then introduce a novel method for learning empirical Bregman divergence directly from data based on parameterizing the convex function underlying the Bregman divergence with a deep learning setting. We further experimentally show that our approach performs effectively on five popular public datasets compared to other SOTA deep metric learning methods, particularly for pattern recognit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#38750;&#27954;&#22320;&#21306;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#30456;&#20851;&#24212;&#29992;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#35843;&#26597;&#21450;&#20851;&#38190;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#24403;&#21069;&#29616;&#29366;&#21450;&#26410;&#26469;&#36235;&#21183;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#21512;&#20316;&#30740;&#31350;&#21644;&#30693;&#35782;&#20132;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.07542</link><description>&lt;p&gt;
&#38750;&#27954;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#36235;&#21183;&#65306;30&#24180;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Research Trends in Africa: A 30 Years Overview with Bibliometric Analysis Review. (arXiv:2304.07542v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#38750;&#27954;&#22320;&#21306;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#30456;&#20851;&#24212;&#29992;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#35843;&#26597;&#21450;&#20851;&#38190;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#24403;&#21069;&#29616;&#29366;&#21450;&#26410;&#26469;&#36235;&#21183;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#21512;&#20316;&#30740;&#31350;&#21644;&#30693;&#35782;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38750;&#27954;&#22320;&#21306;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#30456;&#20851;&#24212;&#29992;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#35843;&#26597;&#65292;&#24182;&#36827;&#34892;&#20102;&#20851;&#38190;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#30740;&#31350;&#12290;&#35813;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#30740;&#31350;&#20849;&#25910;&#38598;&#20102;2761&#31687;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#25991;&#29486;&#65292;&#20854;&#20013;98&#65285;&#26159;&#21457;&#34920;&#22312;903&#20010;&#26399;&#21002;&#19978;&#33267;&#23569;&#26377;482&#27425;&#24341;&#29992;&#30340;&#25991;&#31456;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;&#36807;&#21435;&#30340;30&#24180;&#12290;&#21478;&#22806;&#65292;&#36825;&#20123;&#25991;&#29486;&#26159;&#20174;Science Citation Index EXPANDED&#26816;&#32034;&#20013;&#26469;&#33258;&#20110;1993&#24180;&#33267;2021&#24180;&#20043;&#38388;54&#20010;&#38750;&#27954;&#22269;&#23478;&#30340;&#30740;&#31350;&#20986;&#29256;&#29289;&#12290;&#25991;&#29486;&#35745;&#37327;&#30740;&#31350;&#26174;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#24403;&#21069;&#29616;&#29366;&#21644;&#26410;&#26469;&#36235;&#21183;&#30340;&#21487;&#35270;&#21270;&#65292;&#20197;&#20419;&#36827;&#38750;&#27954;&#22823;&#38470;&#20998;&#24067;&#22312;&#19981;&#21516;&#30740;&#31350;&#26426;&#26500;&#30340;&#20316;&#32773;&#20043;&#38388;&#36827;&#34892;&#26410;&#26469;&#30340;&#21512;&#20316;&#30740;&#31350;&#21644;&#30693;&#35782;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a critical bibliometric analysis study is conducted, coupled with an extensive literature survey on recent developments and associated applications in machine learning research with a perspective on Africa. The presented bibliometric analysis study consists of 2761 machine learning-related documents, of which 98% were articles with at least 482 citations published in 903 journals during the past 30 decades. Furthermore, the collated documents were retrieved from the Science Citation Index EXPANDED, comprising research publications from 54 African countries between 1993 and 2021. The bibliometric study shows the visualization of the current landscape and future trends in machine learning research and its application to facilitate future collaborative research and knowledge exchange among authors from different research institutions scattered across the African continent.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32806;&#21512;&#23383;&#20856;&#23398;&#20064;&#21644;&#36793;&#32536;&#21270;&#25439;&#22833;&#20989;&#25968;&#30340;&#23454;&#26102;&#22270;&#20687;&#27880;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26377;&#38480;&#25968;&#37327;&#30340;&#35270;&#35273;&#21407;&#22411;&#21644;&#30456;&#24212;&#30340;&#35821;&#20041;&#65292;&#24182;&#20445;&#25345;&#26631;&#31614;&#30340;&#31232;&#30095;&#19981;&#24179;&#34913;&#24615;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.06907</link><description>&lt;p&gt;
&#22522;&#20110;&#36793;&#32536;&#21270;&#32806;&#21512;&#23383;&#20856;&#23398;&#20064;&#30340;&#23454;&#26102;&#22270;&#20687;&#27880;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Toward Real-Time Image Annotation Using Marginalized Coupled Dictionary Learning. (arXiv:2304.06907v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32806;&#21512;&#23383;&#20856;&#23398;&#20064;&#21644;&#36793;&#32536;&#21270;&#25439;&#22833;&#20989;&#25968;&#30340;&#23454;&#26102;&#22270;&#20687;&#27880;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26377;&#38480;&#25968;&#37327;&#30340;&#35270;&#35273;&#21407;&#22411;&#21644;&#30456;&#24212;&#30340;&#35821;&#20041;&#65292;&#24182;&#20445;&#25345;&#26631;&#31614;&#30340;&#31232;&#30095;&#19981;&#24179;&#34913;&#24615;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#22270;&#20687;&#21253;&#21547;&#21508;&#31181;&#39640;&#23618;&#35821;&#20041;&#65292;&#34987;&#31216;&#20026;&#26631;&#31614;&#25110;&#27880;&#37322;&#12290;&#20960;&#20046;&#25152;&#26377;&#22788;&#29702;&#38750;&#22343;&#34913;&#26631;&#35760;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#27880;&#37322;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#25628;&#32034;&#30340;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#38750;&#24120;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32806;&#21512;&#23383;&#20856;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#23398;&#20064;&#26377;&#38480;&#25968;&#37327;&#30340;&#35270;&#35273;&#21407;&#22411;&#21644;&#30456;&#24212;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#26102;&#22270;&#20687;&#27880;&#37322;&#36807;&#31243;&#12290;&#26412;&#25991;&#30340;&#21478;&#19968;&#20010;&#36129;&#29486;&#26159;&#20351;&#29992;&#36793;&#32536;&#25439;&#22833;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#23545;&#20110;&#19981;&#24179;&#34913;&#26631;&#31614;&#30340;&#22270;&#20687;&#27880;&#37322;&#19981;&#21512;&#36866;&#30340;&#24179;&#26041;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#36793;&#32536;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#21033;&#29992;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21407;&#22411;&#26356;&#26032;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#22312;&#35821;&#20041;&#21407;&#22411;&#19978;&#24341;&#20837;&#20102;${\ell}_1$&#27491;&#21017;&#21270;&#65292;&#20197;&#20445;&#25345;&#23398;&#20064;&#35821;&#20041;&#21407;&#22411;&#20013;&#26631;&#31614;&#30340;&#31232;&#30095;&#19981;&#24179;&#34913;&#24615;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In most image retrieval systems, images include various high-level semantics, called tags or annotations. Virtually all the state-of-the-art image annotation methods that handle imbalanced labeling are search-based techniques which are time-consuming. In this paper, a novel coupled dictionary learning approach is proposed to learn a limited number of visual prototypes and their corresponding semantics simultaneously. This approach leads to a real-time image annotation procedure. Another contribution of this paper is that utilizes a marginalized loss function instead of the squared loss function that is inappropriate for image annotation with imbalanced labels. We have employed a marginalized loss function in our method to leverage a simple and effective method of prototype updating. Meanwhile, we have introduced ${\ell}_1$ regularization on semantic prototypes to preserve the sparse and imbalanced nature of labels in learned semantic prototypes. Finally, comprehensive experimental resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AMOGCN&#27169;&#22411;&#65292;&#23427;&#33258;&#21160;&#20174;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#36866;&#24212;&#32858;&#21512;&#20013;&#30740;&#31350;&#21253;&#21547;&#22810;&#36339;&#37051;&#23621;&#30340;&#20803;&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#33410;&#28857;&#23646;&#24615;&#35780;&#20215;&#30417;&#30563;&#12290;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#24322;&#26500;&#22270;&#20013;&#21457;&#29616;&#26377;&#21306;&#21035;&#30340;&#33410;&#28857;&#23884;&#20837;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.06336</link><description>&lt;p&gt;
&#22810;&#23646;&#24615;&#22810;&#38454;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24322;&#26500;&#22270;
&lt;/p&gt;
&lt;p&gt;
Attributed Multi-order Graph Convolutional Network for Heterogeneous Graphs. (arXiv:2304.06336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AMOGCN&#27169;&#22411;&#65292;&#23427;&#33258;&#21160;&#20174;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#36866;&#24212;&#32858;&#21512;&#20013;&#30740;&#31350;&#21253;&#21547;&#22810;&#36339;&#37051;&#23621;&#30340;&#20803;&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#33410;&#28857;&#23646;&#24615;&#35780;&#20215;&#30417;&#30563;&#12290;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#24322;&#26500;&#22270;&#20013;&#21457;&#29616;&#26377;&#21306;&#21035;&#30340;&#33410;&#28857;&#23884;&#20837;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#26088;&#22312;&#20174;&#22810;&#20851;&#31995;&#32593;&#32476;&#20013;&#21457;&#29616;&#26377;&#21306;&#21035;&#30340;&#33410;&#28857;&#23884;&#20837;&#21644;&#20851;&#31995;&#12290;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#35774;&#35745;&#21487;&#23398;&#20064;&#30340;&#20803;&#36335;&#24452;&#65292;&#23427;&#26174;&#30528;&#22320;&#24433;&#21709;&#20102;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#23646;&#24615;&#30340;&#22810;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;AMOGCN&#65289;&#65292;&#23427;&#33258;&#21160;&#20174;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#36866;&#24212;&#32858;&#21512;&#20013;&#30740;&#31350;&#21253;&#21547;&#22810;&#36339;&#37051;&#23621;&#30340;&#20803;&#36335;&#24452;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#20174;&#25163;&#21160;&#35774;&#35745;&#30340;&#33410;&#28857;&#36830;&#25509;&#20013;&#26500;&#24314;&#19981;&#21516;&#38454;&#25968;&#30340;&#37051;&#25509;&#30697;&#38453;&#12290;&#20043;&#21518;&#65292;&#20174;&#21508;&#31181;&#38454;&#25968;&#30340;&#37051;&#25509;&#30697;&#38453;&#30340;&#33258;&#21160;&#34701;&#21512;&#20013;&#38468;&#21152;&#19968;&#20010;&#23436;&#25972;&#30340;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#12290;&#36825;&#20010;&#36807;&#31243;&#30001;&#20174;&#33410;&#28857;&#21516;&#36136;&#24615;&#36890;&#36807;&#23646;&#24615;&#35780;&#20215;&#25552;&#21462;&#30340;&#33410;&#28857;&#35821;&#20041;&#20449;&#24687;&#30417;&#30563;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#23398;&#20064;&#21040;&#30340;&#22810;&#38454;&#37051;&#25509;&#30697;&#38453;&#30340;&#19968;&#23618;&#31616;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graph neural networks aim to discover discriminative node embeddings and relations from multi-relational networks.One challenge of heterogeneous graph learning is the design of learnable meta-paths, which significantly influences the quality of learned embeddings.Thus, in this paper, we propose an Attributed Multi-Order Graph Convolutional Network (AMOGCN), which automatically studies meta-paths containing multi-hop neighbors from an adaptive aggregation of multi-order adjacency matrices. The proposed model first builds different orders of adjacency matrices from manually designed node connections. After that, an intact multi-order adjacency matrix is attached from the automatic fusion of various orders of adjacency matrices. This process is supervised by the node semantic information, which is extracted from the node homophily evaluated by attributes. Eventually, we utilize a one-layer simplifying graph convolutional network with the learned multi-order adjacency matrix,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#65292;&#20854;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.05949</link><description>&lt;p&gt;
CMOS + &#38543;&#26426;&#32435;&#31859;&#30913;&#20307;&#65306;&#27010;&#29575;&#25512;&#29702;&#19982;&#23398;&#20064;&#24322;&#26500;&#35745;&#31639;&#26426;
&lt;/p&gt;
&lt;p&gt;
CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning. (arXiv:2304.05949v1 [cond-mat.mes-hall])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#65292;&#20854;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25705;&#23572;&#23450;&#24459;&#30340;&#25918;&#32531;&#65292;&#21033;&#29992;&#26032;&#20852;&#30340;&#32435;&#31859;&#25216;&#26415;&#65288;X&#65289;&#22686;&#24378;&#20114;&#34917;&#37329;&#23646;&#27687;&#21270;&#29289;&#21322;&#23548;&#20307;&#65288;CMOS&#65289;&#26230;&#20307;&#31649;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#38543;&#26426;&#30913;&#38567;&#36947;&#32467;&#65288;sMTJ&#65289;&#30340;&#27010;&#29575;&#27604;&#29305;&#65288;p&#20301;&#65289;&#19982;&#22810;&#21151;&#33021;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#33021;&#28304;&#39640;&#25928;&#30340;&#24322;&#26500;CMOS + X&#65288;X = sMTJ&#65289;&#21407;&#22411;&#12290;&#23613;&#31649;sMTJs&#35774;&#22791;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#25105;&#20204;&#30340;&#24322;&#26500;&#35745;&#31639;&#26426;&#25104;&#21151;&#22320;&#25191;&#34892;&#20102;&#27010;&#29575;&#25512;&#29702;&#21644;&#24322;&#27493;Boltzmann&#23398;&#20064;&#12290;&#20351;&#29992;CMOS&#39044;&#27979;&#27969;&#31243;&#35774;&#35745;&#22871;&#20214;&#65288;PDK&#65289;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25968;&#23383;CMOS-based p-bits&#27169;&#25311;&#39640;&#36136;&#37327;&#38543;&#26426;&#24615;&#38656;&#35201;&#36229;&#36807;10,000&#20010;&#26230;&#20307;&#31649;&#65292;&#27599;&#29983;&#25104;&#19968;&#20010;&#38543;&#26426;&#25968;&#30340;&#33021;&#37327;&#27604;&#20351;&#29992;&#21482;&#28040;&#32791;2fJ&#30340;sMTJ-based p-bits&#39640;&#32422;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#32553;&#25918;&#21644;&#38598;&#25104;&#29256;&#26412;&#21487;&#20197;&#26174;&#30528;&#25512;&#36827;&#27010;&#29575;&#24615;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the slowing down of Moore's law, augmenting complementary-metal-oxide semiconductor (CMOS) transistors with emerging nanotechnologies (X) is becoming increasingly important. In this paper, we demonstrate how stochastic magnetic tunnel junction (sMTJ)-based probabilistic bits, or p-bits, can be combined with versatile Field Programmable Gate Arrays (FPGA) to design an energy-efficient, heterogeneous CMOS + X (X = sMTJ) prototype. Our heterogeneous computer successfully performs probabilistic inference and asynchronous Boltzmann learning despite device-to-device variations in sMTJs. A comprehensive comparison using a CMOS predictive process design kit (PDK) reveals that digital CMOS-based p-bits emulating high-quality randomness use over 10,000 transistors with the energy per generated random number being roughly two orders of magnitude greater than the sMTJ-based p-bits that dissipate only 2 fJ. Scaled and integrated versions of our approach can significantly advance probabilistic 
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#35762;&#20041;&#27010;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#35270;&#35282;&#65292;&#24182;&#20171;&#32461;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#27531;&#24046;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#31561;&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#32473;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#38382;&#39064;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.05133</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Neural Network Architectures. (arXiv:2304.05133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05133
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35762;&#20041;&#27010;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#35270;&#35282;&#65292;&#24182;&#20171;&#32461;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#27531;&#24046;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#31561;&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#32473;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#38382;&#39064;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35762;&#20041;&#20174;&#25968;&#23398;&#35282;&#24230;&#27010;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#29305;&#21035;&#22320;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#34987;&#35270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#20171;&#32461;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#30784;&#30693;&#35782;&#20197;&#21450;&#19979;&#21015;&#20960;&#31181;&#26550;&#26500;&#65306;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#27531;&#24046;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.04133</link><description>&lt;p&gt;
&#22522;&#20110;NeRF&#25216;&#26415;&#30340;&#21355;&#26143;&#22270;&#20687;&#34920;&#38754;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
NeRF applied to satellite imagery for surface reconstruction. (arXiv:2304.04133v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Sat-NeRF&#27169;&#22411;&#65292;&#26159;&#23545;&#26368;&#36817;&#24341;&#20837;&#30340;S-NeRF&#27169;&#22411;&#30340;&#20462;&#25913;&#23454;&#29616;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#31232;&#30095;&#30340;&#21355;&#26143;&#22270;&#20687;&#38598;&#21512;&#20013;&#21512;&#25104;&#26032;&#30340;&#35270;&#35282;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#22270;&#29255;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#31934;&#30830;&#22320;&#20272;&#35745;&#22330;&#26223;&#34920;&#38754;&#30340;&#39640;&#31243;&#65292;&#36825;&#23545;&#21355;&#26143;&#35266;&#27979;&#24212;&#29992;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;S-NeRF&#26041;&#27861;&#25913;&#36827;&#20102;&#26631;&#20934;&#30340;NeRF&#26041;&#27861;&#65292;&#23558;&#36752;&#23556;&#24378;&#24230;&#32771;&#34385;&#20026;&#39640;&#21453;&#23556;&#29575;&#21644;&#20837;&#23556;&#36752;&#29031;&#24230;&#30340;&#20989;&#25968;&#12290;&#36825;&#20004;&#20010;&#37327;&#37117;&#26159;&#27169;&#22411;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26525;&#26465;&#30340;&#36755;&#20986;&#65292;&#32780;&#21518;&#32773;&#21017;&#34987;&#35270;&#20026;&#26469;&#33258;&#22826;&#38451;&#30340;&#30452;&#25509;&#20809;&#32447;&#21644;&#26469;&#33258;&#22825;&#31354;&#30340;&#28459;&#21453;&#23556;&#39068;&#33394;&#20989;&#25968;&#12290;&#35813;&#23454;&#29616;&#22522;&#20110;&#29992;&#32553;&#25918;-&#35009;&#21098;&#25216;&#26415;&#22686;&#24378;&#30340;&#21355;&#26143;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#23545;NeRF&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#30740;&#31350;&#65292;&#24471;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Sat-NeRF, a modified implementation of the recently introduced Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize novel views from a sparse set of satellite images of a scene, while accounting for the variation in lighting present in the pictures. The trained model can also be used to accurately estimate the surface elevation of the scene, which is often a desirable quantity for satellite observation applications. S-NeRF improves on the standard Neural Radiance Field (NeRF) method by considering the radiance as a function of the albedo and the irradiance. Both these quantities are output by fully connected neural network branches of the model, and the latter is considered as a function of the direct light from the sun and the diffuse color from the sky. The implementations were run on a dataset of satellite images, augmented using a zoom-and-crop technique. A hyperparameter study for NeRF was carried out, leading to intriguing observations on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#20559;&#12289;&#39640;&#25928;&#12289;&#36866;&#24403;&#30340;&#21333;&#35843;&#24067;&#23572;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#65292;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#20551;&#35774;&#30340;&#22823;&#23567;&#21644;&#35780;&#20272;&#26102;&#38388;&#37117;&#20026;$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$&#65292;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02700</link><description>&lt;p&gt;
&#26080;&#20559;&#20851;&#20110;&#22369;&#24230;&#20989;&#25968;&#30340;&#36866;&#24403;&#23398;&#20064;&#65306;&#36234;&#36807;&#40657;&#30418;&#20462;&#27491;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Agnostic proper learning of monotone functions: beyond the black-box correction barrier. (arXiv:2304.02700v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#20559;&#12289;&#39640;&#25928;&#12289;&#36866;&#24403;&#30340;&#21333;&#35843;&#24067;&#23572;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#65292;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#21644;&#20551;&#35774;&#30340;&#22823;&#23567;&#21644;&#35780;&#20272;&#26102;&#38388;&#37117;&#20026;$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$&#65292;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#20559;&#12289;&#39640;&#25928;&#12289;&#36866;&#24403;&#30340;&#21333;&#35843;&#24067;&#23572;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#12290;&#32473;&#23450;&#26410;&#30693;&#20989;&#25968;$f:\{\pm 1\}^n \rightarrow \{\pm 1\}$&#30340;$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$&#20010;&#22343;&#21248;&#38543;&#26426;&#26679;&#26412;&#65292;&#31639;&#27861;&#36755;&#20986;&#19968;&#20010;&#20551;&#35774;$g:\{\pm 1\}^n \rightarrow \{\pm 1\}$&#65292;&#35813;&#20551;&#35774;&#26159;&#21333;&#35843;&#30340;&#65292;&#24182;&#19988;&#19982;$f$&#30340;&#36317;&#31163;&#20026;$(\mathrm{opt} + \varepsilon)$&#65292;&#20854;&#20013;$\mathrm{opt}$&#26159;$f$&#19982;&#26368;&#36817;&#21333;&#35843;&#20989;&#25968;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#65288;&#22240;&#27492;&#20063;&#26159;&#20551;&#35774;&#30340;&#22823;&#23567;&#21644;&#35780;&#20272;&#26102;&#38388;&#65289;&#20063;&#26159;$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$&#65292;&#20960;&#20046;&#19982;Blais&#31561;&#20154;&#65288;RANDOM '15&#65289;&#30340;&#19979;&#30028;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#19968;&#20010;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#26410;&#30693;&#20989;&#25968;$f$&#21040;&#21333;&#35843;&#24615;&#30340;&#28155;&#21152;&#35823;&#24046;$\varepsilon$&#30340;&#36317;&#31163;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$&#12290;&#20197;&#21069;&#65292;&#38024;&#23545;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#24050;&#30693;&#26377;&#26679;&#26412;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#24182;&#19981;&#26159;&#36816;&#34892;&#26102;&#38388;&#26377;&#25928;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give the first agnostic, efficient, proper learning algorithm for monotone Boolean functions. Given $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$ uniformly random examples of an unknown function $f:\{\pm 1\}^n \rightarrow \{\pm 1\}$, our algorithm outputs a hypothesis $g:\{\pm 1\}^n \rightarrow \{\pm 1\}$ that is monotone and $(\mathrm{opt} + \varepsilon)$-close to $f$, where $\mathrm{opt}$ is the distance from $f$ to the closest monotone function. The running time of the algorithm (and consequently the size and evaluation time of the hypothesis) is also $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$, nearly matching the lower bound of Blais et al (RANDOM '15). We also give an algorithm for estimating up to additive error $\varepsilon$ the distance of an unknown function $f$ to monotone using a run-time of $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$. Previously, for both of these problems, sample-efficient algorithms were known, but these algorithms were not run-time efficient. Our work thus closes this g
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#32972;&#38376;&#25915;&#20987;&#65292;&#21457;&#29616;&#22312;&#26679;&#26412;&#30340;&#26368;&#19981;&#37325;&#35201;&#21306;&#22495;&#20013;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#32972;&#38376;&#25915;&#20987;&#25928;&#26524;&#26356;&#22909;&#65292;&#23545;&#35813;&#29616;&#35937;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.02277</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22270;&#24418;&#21518;&#38376;&#25915;&#20987;&#20013;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Trigger-injecting Position in Graph Backdoor Attack. (arXiv:2304.02277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02277
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#32972;&#38376;&#25915;&#20987;&#65292;&#21457;&#29616;&#22312;&#26679;&#26412;&#30340;&#26368;&#19981;&#37325;&#35201;&#21306;&#22495;&#20013;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#32972;&#38376;&#25915;&#20987;&#25928;&#26524;&#26356;&#22909;&#65292;&#23545;&#35813;&#29616;&#35937;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#20256;&#32479;&#30340;&#21518;&#38376;&#25915;&#20987;&#24847;&#22270;&#23558;&#21518;&#38376;&#21151;&#33021;&#27880;&#20837;&#27169;&#22411;&#20013;&#65292;&#20351;&#24471;&#24102;&#26377;&#39044;&#23450;&#20041;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#33021;&#22815;&#20986;&#29616;&#24322;&#24120;&#30340;&#34920;&#29616;&#65292;&#20294;&#26159;&#35813;&#27169;&#22411;&#22312;&#24178;&#20928;&#30340;&#36755;&#20837;&#19978;&#20173;&#28982;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#32972;&#38376;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#20294;&#26159;&#22312;&#22270;&#39046;&#22495;&#20013;&#30340;&#21518;&#38376;&#35302;&#21457;&#22120;&#22823;&#22810;&#34987;&#27880;&#20837;&#21040;&#26679;&#26412;&#30340;&#38543;&#26426;&#20301;&#32622;&#12290;&#23578;&#26410;&#26377;&#30740;&#31350;&#20998;&#26512;&#21644;&#35299;&#37322;&#22312;&#26679;&#26412;&#30340;&#26368;&#37325;&#35201;&#25110;&#26368;&#19981;&#37325;&#35201;&#30340;&#21306;&#22495;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#32972;&#38376;&#25915;&#20987;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#35302;&#21457;&#27880;&#20837;&#31574;&#30053;MIAS&#21644;LIAS&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#33324;&#26469;&#35828;&#65292;LIAS&#30340;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;LIAS&#21644;MIAS&#34920;&#29616;&#20043;&#38388;&#30340;&#24046;&#24322;&#21487;&#33021;&#26159;&#26174;&#33879;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#37322;&#35828;&#26126;&#20102;&#36825;&#20004;&#31181;&#31574;&#30053;&#30340;&#30456;&#20284;&#65288;&#26356;&#22909;&#65289;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks have been demonstrated as a security threat for machine learning models. Traditional backdoor attacks intend to inject backdoor functionality into the model such that the backdoored model will perform abnormally on inputs with predefined backdoor triggers and still retain state-of-the-art performance on the clean inputs. While there are already some works on backdoor attacks on Graph Neural Networks (GNNs), the backdoor trigger in the graph domain is mostly injected into random positions of the sample. There is no work analyzing and explaining the backdoor attack performance when injecting triggers into the most important or least important area in the sample, which we refer to as trigger-injecting strategies MIAS and LIAS, respectively. Our results show that, generally, LIAS performs better, and the differences between the LIAS and MIAS performance can be significant. Furthermore, we explain these two strategies' similar (better) attack performance through explanation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;3D-CNN&#65289;&#23545;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#39118;&#36895;&#39044;&#27979;&#30340;&#31934;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21608;&#22260;&#21306;&#22495;&#30340;&#31354;&#38388;&#25968;&#25454;&#36827;&#34892;3D-CNN&#35757;&#32451;&#21487;&#20197;&#27604;&#20165;&#20351;&#29992;&#21333;&#28857;&#20449;&#24687;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01545</link><description>&lt;p&gt;
&#21306;&#22495;&#39118;&#21147;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;CNN&#30340;&#39118;&#36895;&#39044;&#27979;&#65306;&#26469;&#33258;&#26102;&#31354;&#30456;&#20851;&#24615;&#20998;&#26512;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Regional Wind Characteristics Affect CNN-based wind predictions: Insights from Spatiotemporal Correlation Analysis. (arXiv:2304.01545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;3D-CNN&#65289;&#23545;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#39118;&#36895;&#39044;&#27979;&#30340;&#31934;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#21608;&#22260;&#21306;&#22495;&#30340;&#31354;&#38388;&#25968;&#25454;&#36827;&#34892;3D-CNN&#35757;&#32451;&#21487;&#20197;&#27604;&#20165;&#20351;&#29992;&#21333;&#28857;&#20449;&#24687;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26102;&#31354;&#25968;&#25454;&#32500;&#24230;&#23545;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#30340;&#39118;&#36895;&#39044;&#27979;&#27169;&#22411;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21152;&#20837;&#31354;&#38388;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#39118;&#36895;&#39044;&#27979;&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#19981;&#21516;&#31354;&#38388;&#23610;&#24230;&#25913;&#36827;&#30340;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#20339;&#26102;&#38388;&#38271;&#24230;&#30340;&#36755;&#20837;&#25968;&#25454;&#30340;&#30740;&#31350;&#20063;&#24456;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#22312;&#20351;&#29992;3D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;3D-CNN&#65289;&#39044;&#27979;&#39118;&#36895;&#26102;&#65292;&#37319;&#29992;&#20855;&#26377;&#19981;&#21516;&#26102;&#31354;&#32500;&#24230;&#30340;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#35780;&#20272;&#20854;&#39044;&#27979;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21608;&#22260;&#21306;&#22495;&#30340;&#31354;&#38388;&#25968;&#25454;&#36827;&#34892;3D-CNN&#35757;&#32451;&#21487;&#20197;&#27604;&#20165;&#20351;&#29992;&#21333;&#28857;&#20449;&#24687;&#33719;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22810;&#26102;&#38388;&#25968;&#25454;&#23545;&#39044;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the impact of spatiotemporal data dimensions on the precision of a wind forecasting model developed using an artificial neural network. Although previous studies have shown that incorporating spatial data can enhance the accuracy of wind forecasting models, few investigations have explored the extent of the improvement owing to different spatial scales in neural network-based predictive models. Additionally, there are limited studies on the optimal temporal length of the input data for these models. To address this gap, this study employs data with various spatiotemporal dimensions as inputs when forecasting wind using 3D-Convolutional Neural Networks (3D-CNN) and assesses their predictive performance. The results indicate that using spatial data of the surrounding area for 3D-CNN training can achieve better predictive performance than using only single-point information. Additionally, multi-time data had a more positive effect on the predictive performance than
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#32479;&#35745;&#21147;&#23398;&#26041;&#27861;&#30740;&#31350;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20302;&#25439;&#22833;&#21306;&#22495;&#23384;&#22312;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#24182;&#30001;&#38752;&#36817;&#20998;&#31867;&#20915;&#31574;&#36793;&#30028;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#20915;&#23450;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.01335</link><description>&lt;p&gt;
&#29992;&#28909;&#22122;&#22768;&#25551;&#32472;&#31070;&#32463;&#32593;&#32476;&#26223;&#35266;&#30340;&#22320;&#24418;
&lt;/p&gt;
&lt;p&gt;
Charting the Topography of the Neural Network Landscape with Thermal-Like Noise. (arXiv:2304.01335v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#32479;&#35745;&#21147;&#23398;&#26041;&#27861;&#30740;&#31350;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20302;&#25439;&#22833;&#21306;&#22495;&#23384;&#22312;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#24182;&#30001;&#38752;&#36817;&#20998;&#31867;&#20915;&#31574;&#36793;&#30028;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#20915;&#23450;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#39640;&#32500;&#12289;&#38750;&#20984;&#19988;&#22024;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#29702;&#35770;&#29702;&#35299;&#22312;&#24212;&#29992;&#35282;&#24230;&#21644;&#22522;&#30784;&#30740;&#31350;&#26041;&#38754;&#22343;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#37319;&#29992;&#26631;&#20934;&#30340;&#32479;&#35745;&#21147;&#23398;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;Langevin&#21160;&#24577;&#30456;&#31354;&#38388;&#25506;&#27979;&#26041;&#27861;&#30740;&#31350;&#36807;&#21442;&#25968;&#20840;&#36830;&#25509;&#32593;&#32476;&#22312;&#38543;&#26426;&#25968;&#25454;&#19978;&#25191;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#12290;&#36890;&#36807;&#20998;&#26512;&#28072;&#33853;&#32479;&#35745;&#25968;&#25454;&#65292;&#31867;&#27604;&#20110;&#20307;&#31995;&#22312;&#24658;&#23450;&#28201;&#24230;&#19979;&#30340;&#28909;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#25512;&#26029;&#20986;&#20102;&#19968;&#20010;&#28165;&#26224;&#30340;&#22320;&#24418;&#25551;&#36848;&#8212;&#8212;&#20302;&#25439;&#22833;&#21306;&#22495;&#26159;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#20854;&#32500;&#24230;&#21487;&#20197;&#36731;&#26131;&#22320;&#20174;&#27874;&#21160;&#24615;&#20013;&#33719;&#24471;&#12290;&#27492;&#22806;&#65292;&#35813;&#32500;&#24230;&#21463;&#21040;&#38752;&#36817;&#20998;&#31867;&#20915;&#31574;&#36793;&#30028;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#30340;&#25511;&#21046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#22235;&#38454;&#30456;&#20114;&#20316;&#29992;&#26159;&#20851;&#38190;&#30340;&#65292;&#32780;&#26631;&#20934;&#30340; Langevin &#26041;&#27861;&#19981;&#33021;&#20934;&#30830;&#25551;&#36848;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of neural networks is a complex, high-dimensional, non-convex and noisy optimization problem whose theoretical understanding is interesting both from an applicative perspective and for fundamental reasons. A core challenge is to understand the geometry and topography of the landscape that guides the optimization. In this work, we employ standard Statistical Mechanics methods, namely, phase-space exploration using Langevin dynamics, to study this landscape for an over-parameterized fully connected network performing a classification task on random data. Analyzing the fluctuation statistics, in analogy to thermal dynamics at a constant temperature, we infer a clear geometric description of the low-loss region. We find that it is a low-dimensional manifold whose dimension can be readily obtained from the fluctuations. Furthermore, this dimension is controlled by the number of data points that reside near the classification decision boundary. Importantly, we find that a quadra
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#37329;&#34701;&#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#37319;&#29992;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#21033;&#29992;&#32654;&#22269;&#24066;&#22330;&#20215;&#26684;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#39564;&#35777;&#20854;&#22312;Numerai-Signals&#30446;&#26631;&#19978;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16117</link><description>&lt;p&gt;
&#38754;&#21521;&#37329;&#34701;&#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Feature Engineering Methods on Multivariate Time-Series Data for Financial Data Science Competitions. (arXiv:2303.16117v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16117
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#37329;&#34701;&#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#37319;&#29992;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#65292;&#21033;&#29992;&#32654;&#22269;&#24066;&#22330;&#20215;&#26684;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#39564;&#35777;&#20854;&#22312;Numerai-Signals&#30446;&#26631;&#19978;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24212;&#29992;&#19981;&#21516;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#23545;&#32654;&#22269;&#24066;&#22330;&#20215;&#26684;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#27979;&#35797;&#27169;&#22411;&#22312;Numerai-Signals&#30446;&#26631;&#19978;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply different feature engineering methods for time-series to US market price data. The predictive power of models are tested against Numerai-Signals targets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20581;&#22766;&#30340;&#39118;&#38505;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20248;&#21270;&#26399;&#26435;&#23545;&#20914;&#31574;&#30053;&#65292;&#29305;&#21035;&#24212;&#29992;&#20110;&#30028;&#38480;&#26399;&#26435;&#23545;&#20914;&#65292;&#38543;&#30528;&#20195;&#29702;&#39118;&#38505;&#20559;&#22909;&#21464;&#21270;&#65292;&#23545;&#20914;&#31574;&#30053;&#21457;&#29983;&#25197;&#26354;&#65292;&#40065;&#26834;&#31574;&#30053;&#20248;&#20110;&#38750;&#40065;&#26834;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.15216</link><description>&lt;p&gt;
&#20581;&#22766;&#30340;&#39118;&#38505;&#24863;&#30693;&#26399;&#26435;&#23545;&#20914;
&lt;/p&gt;
&lt;p&gt;
Robust Risk-Aware Option Hedging. (arXiv:2303.15216v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20581;&#22766;&#30340;&#39118;&#38505;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20248;&#21270;&#26399;&#26435;&#23545;&#20914;&#31574;&#30053;&#65292;&#29305;&#21035;&#24212;&#29992;&#20110;&#30028;&#38480;&#26399;&#26435;&#23545;&#20914;&#65292;&#38543;&#30528;&#20195;&#29702;&#39118;&#38505;&#20559;&#22909;&#21464;&#21270;&#65292;&#23545;&#20914;&#31574;&#30053;&#21457;&#29983;&#25197;&#26354;&#65292;&#40065;&#26834;&#31574;&#30053;&#20248;&#20110;&#38750;&#40065;&#26834;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26435;&#23545;&#20914;/&#20132;&#26131;&#30340;&#30446;&#26631;&#19981;&#20165;&#20165;&#26159;&#20026;&#20102;&#20445;&#25252;&#19979;&#34892;&#39118;&#38505;&#65292;&#36824;&#24076;&#26395;&#23547;&#27714;&#25910;&#30410;&#65292;&#39537;&#21160;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20581;&#22766;&#30340;&#39118;&#38505;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;(RL)&#22312;&#20943;&#36731;&#19982;&#36335;&#24452;&#30456;&#20851;&#30340;&#37329;&#34701;&#34893;&#29983;&#21697;&#39118;&#38505;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;Jaimungal&#12289;Pesenti&#12289;Wang&#12289;Tatsat(2022)&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#20248;&#21270;&#20581;&#22766;&#30340;&#39118;&#38505;&#24863;&#30693;&#32489;&#25928;&#26631;&#20934;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#30028;&#38480;&#26399;&#26435;&#23545;&#20914;&#65292;&#24182;&#24378;&#35843;&#38543;&#30528;&#20195;&#29702;&#20174;&#39118;&#38505;&#35268;&#36991;&#36716;&#21464;&#20026;&#39118;&#38505;&#23547;&#27714;&#65292;&#26368;&#20248;&#23545;&#20914;&#31574;&#30053;&#20250;&#21457;&#29983;&#25197;&#26354;&#65292;&#20197;&#21450;&#20195;&#29702;&#22914;&#20309;&#24378;&#21270;&#20854;&#31574;&#30053;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#24403;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;(DGP)&#19982;&#35757;&#32451;DGP&#19981;&#21516;&#26102;&#65292;&#23545;&#20914;&#30340;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#20102;&#40065;&#26834;&#31574;&#30053;&#20248;&#20110;&#38750;&#40065;&#26834;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objectives of option hedging/trading extend beyond mere protection against downside risks, with a desire to seek gains also driving agent's strategies. In this study, we showcase the potential of robust risk-aware reinforcement learning (RL) in mitigating the risks associated with path-dependent financial derivatives. We accomplish this by leveraging the Jaimungal, Pesenti, Wang, Tatsat (2022) and their policy gradient approach, which optimises robust risk-aware performance criteria. We specifically apply this methodology to the hedging of barrier options, and highlight how the optimal hedging strategy undergoes distortions as the agent moves from being risk-averse to risk-seeking. As well as how the agent robustifies their strategy. We further investigate the performance of the hedge when the data generating process (DGP) varies from the training DGP, and demonstrate that the robust strategies outperform the non-robust ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#30340;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#26469;&#32531;&#35299;&#31169;&#20154;&#26631;&#27880;&#22270;&#20687;&#30340;&#30701;&#32570;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#12289;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#65292;&#36798;&#21040;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.14357</link><description>&lt;p&gt;
&#22788;&#29702;&#24322;&#26500;3D MR&#33181;&#20851;&#33410;&#22270;&#20687;&#65306;&#19968;&#31181;&#20855;&#26377;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#30340;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dealing With Heterogeneous 3D MR Knee Images: A Federated Few-Shot Learning Method With Dual Knowledge Distillation. (arXiv:2303.14357v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#30340;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#26469;&#32531;&#35299;&#31169;&#20154;&#26631;&#27880;&#22270;&#20687;&#30340;&#30701;&#32570;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#12289;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#65292;&#36798;&#21040;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#21307;&#23398;&#26426;&#26500;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#19981;&#27719;&#24635;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23458;&#25143;(&#22914;&#21307;&#38498;)&#20043;&#38388;&#30340;&#21327;&#20316;&#22521;&#35757;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20026;&#22823;&#22411;3D&#22270;&#20687;&#25968;&#25454;&#38598;&#21019;&#24314;&#27880;&#37322;&#30340;&#25104;&#26412;&#39640;&#26114;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#30103;&#26426;&#26500;&#26469;&#35828;&#65292;&#20182;&#20204;&#27809;&#26377;&#36275;&#22815;&#30340;&#30417;&#30563;&#25968;&#25454;&#26469;&#36827;&#34892;&#26412;&#22320;&#22521;&#35757;&#12290;&#22240;&#27492;&#65292;&#26377;&#38480;&#30417;&#30563;&#19979;&#30340;&#21512;&#20316;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#26426;&#26500;&#26377;&#36164;&#28304;&#26469;&#32534;&#21046;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#26631;&#31614;&#25968;&#25454;&#23384;&#20648;&#24211;&#12290;&#22240;&#27492;&#65292;&#20010;&#20307;&#23458;&#25143;&#21487;&#20197;&#21033;&#29992;&#20174;&#20844;&#20849;&#25968;&#25454;&#23384;&#20648;&#24211;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#26469;&#32531;&#35299;&#31169;&#26377;&#26631;&#27880;&#22270;&#20687;&#30340;&#30701;&#32570;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#30340;&#32852;&#37030;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#22312;&#23458;&#25143;&#20043;&#38388;&#36827;&#34892;&#26377;&#38480;&#26631;&#27880;&#30340;&#32852;&#21512;&#22521;&#35757;&#65292;&#32780;&#19981;&#20250;&#21361;&#23475;&#38544;&#31169;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#30417;&#30563;&#23398;&#20064;&#20174;&#27599;&#20010;&#23458;&#25143;&#30340;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#32780;&#26080;&#30417;&#30563;&#23398;&#20064;&#20174;&#20844;&#20849;&#25968;&#25454;&#23384;&#20648;&#24211;&#20013;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#21452;&#37325;&#30693;&#35782;&#33976;&#39311;&#20197;&#29983;&#25104;&#26356;&#22810;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;&#23545;&#33181;&#20851;&#33410;MR&#22270;&#20687;&#30340;&#24322;&#26500;&#20020;&#24202;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;0.87&#30340;AUC&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning has gained popularity among medical institutions since it enables collaborative training between clients (e.g., hospitals) without aggregating data. However, due to the high cost associated with creating annotations, especially for large 3D image datasets, clinical institutions do not have enough supervised data for training locally. Thus, the performance of the collaborative model is subpar under limited supervision. On the other hand, large institutions have the resources to compile data repositories with high-resolution images and labels. Therefore, individual clients can utilize the knowledge acquired in the public data repositories to mitigate the shortage of private annotated images. In this paper, we propose a federated few-shot learning method with dual knowledge distillation. This method allows joint training with limited annotations across clients without jeopardizing privacy. The supervised learning of the proposed method extracts features from limited lab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21344;&#25454;&#26680;PCA&#26041;&#27861;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11138</link><description>&lt;p&gt;
&#22522;&#20110;&#21344;&#25454;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#25925;&#38556;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fault Detection via Occupation Kernel Principal Component Analysis. (arXiv:2303.11138v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21344;&#25454;&#26680;PCA&#26041;&#27861;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31995;&#32479;&#30340;&#21487;&#38752;&#25805;&#20316;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#26816;&#27979;&#22522;&#30784;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#25925;&#38556;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#25925;&#38556;&#26816;&#27979;&#65292;&#20294;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#22240;&#20854;&#26131;&#20110;&#37096;&#32626;&#21644;&#23545;&#19987;&#23478;&#30693;&#35782;&#38656;&#27714;&#26368;&#23567;&#30340;&#29305;&#28857;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21344;&#25454;&#26680;&#36827;&#34892;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#21344;&#25454;&#26680;&#20135;&#29983;&#30340;&#29305;&#24449;&#26144;&#23556;&#36866;&#29992;&#20110;&#27979;&#37327;&#25968;&#25454;&#65292;&#30001;&#20110;&#20351;&#29992;&#31215;&#20998;&#20855;&#26377;&#20869;&#22312;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21033;&#29992;&#38271;&#24230;&#21487;&#21464;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#31995;&#32479;&#36712;&#36857;&#36827;&#34892;PCA&#12290;&#21344;&#25454;&#26680;PCA&#26041;&#27861;&#34987;&#29992;&#20110;&#24320;&#21457;&#19968;&#31181;&#37325;&#26500;&#35823;&#24046;&#26041;&#27861;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reliable operation of automatic systems is heavily dependent on the ability to detect faults in the underlying dynamical system. While traditional model-based methods have been widely used for fault detection, data-driven approaches have garnered increasing attention due to their ease of deployment and minimal need for expert knowledge. In this paper, we present a novel principal component analysis (PCA) method that uses occupation kernels. Occupation kernels result in feature maps that are tailored to the measured data, have inherent noise-robustness due to the use of integration, and can utilize irregularly sampled system trajectories of variable lengths for PCA. The occupation kernel PCA method is used to develop a reconstruction error approach to fault detection and its efficacy is validated using numerical simulations.
&lt;/p&gt;</description></item><item><title>&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488; (IDC) &#26088;&#22312;&#20419;&#36827;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65292;&#25552;&#20379;&#20844;&#20849;&#24211;&#21644;&#20113;&#31471;&#25216;&#26415;&#25903;&#25345;&#65292;&#26041;&#20415;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30284;&#30151;&#32452;&#32455;&#20998;&#31867;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.09354</link><description>&lt;p&gt;
&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#65306;&#35745;&#31639;&#30149;&#29702;&#23398;&#21487;&#37325;&#22797;&#30740;&#31350;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
The NCI Imaging Data Commons as a platform for reproducible research in computational pathology. (arXiv:2303.09354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09354
&lt;/p&gt;
&lt;p&gt;
&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488; (IDC) &#26088;&#22312;&#20419;&#36827;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65292;&#25552;&#20379;&#20844;&#20849;&#24211;&#21644;&#20113;&#31471;&#25216;&#26415;&#25903;&#25345;&#65292;&#26041;&#20415;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30284;&#30151;&#32452;&#32455;&#20998;&#31867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#21487;&#37325;&#22797;&#24615;&#23545;&#20110;&#23558;&#35745;&#31639;&#30149;&#29702;&#23398;&#65288;CompPath&#65289;&#20013;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#36716;&#21270;&#20026;&#23454;&#36341;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#25253;&#21578;&#38590;&#20197;&#37325;&#22797; ML &#32467;&#26524;&#30340;&#22256;&#38590;&#12290;&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#65288;IDC&#65289;&#26159;&#19968;&#20010;&#20844;&#20849;&#24211;&#65292;&#21253;&#21547; &gt;120 &#20010;&#30284;&#30151;&#22270;&#20687;&#25910;&#38598;&#65292;&#21253;&#25324; &gt;38,000 &#24352;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#65292;&#26088;&#22312;&#19982;&#20113;&#31471; ML &#26381;&#21153;&#19968;&#36215;&#20351;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102; IDC &#20419;&#36827; CompPath &#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#30340;&#28508;&#21147;&#12290; &#26448;&#26009;&#21644;&#26041;&#27861;&#65306;IDC &#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65306;&#25152;&#26377;&#22270;&#20687;&#37117;&#26681;&#25454; DICOM &#26631;&#20934;&#36827;&#34892;&#32534;&#30721;&#65292;&#20855;&#26377;&#25345;&#20037;&#21270;&#26631;&#35782;&#31526;&#12289;&#21487;&#36890;&#36807;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#36827;&#34892;&#21457;&#29616;&#65292;&#24182;&#21487;&#36890;&#36807;&#24320;&#25918;&#24335;&#24037;&#20855;&#35775;&#38382;&#12290;&#20511;&#27492;&#20248;&#21183;&#65292;&#25105;&#20204;&#22312; IDC &#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20004;&#20010;&#23454;&#39564;&#65292;&#38024;&#23545;&#32954;&#30284;&#32452;&#32455;&#20998;&#31867;&#30340;&#19968;&#31181;&#20195;&#34920;&#24615;&#22522;&#20110; ML &#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;/&#25110;&#35780;&#20272;&#12290;&#20026;&#35780;&#20272;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#39564;&#34987;&#22810;&#27425;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Reproducibility is critical for translating machine learning-based (ML) solutions in computational pathology (CompPath) into practice. However, an increasing number of studies report difficulties in reproducing ML results. The NCI Imaging Data Commons (IDC) is a public repository of &gt;120 cancer image collections, including &gt;38,000 whole-slide images (WSIs), that is designed to be used with cloud-based ML services. Here, we explore the potential of the IDC to facilitate reproducibility of CompPath research.  Materials and Methods: The IDC realizes the FAIR principles: All images are encoded according to the DICOM standard, persistently identified, discoverable via rich metadata, and accessible via open tools. Taking advantage of this, we implemented two experiments in which a representative ML-based method for classifying lung tumor tissue was trained and/or evaluated on different datasets from the IDC. To assess reproducibility, the experiments were run multiple times with i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; DISDE &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27169;&#22411;&#22312;&#19981;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;&#35813;&#26041;&#27861;&#23558;&#24615;&#33021;&#19979;&#38477;&#20998;&#35299;&#20026;&#19977;&#20010;&#26041;&#38754;&#65306;&#38590;&#24230;&#26356;&#22823;&#20294;&#26356;&#39057;&#32321;&#20986;&#29616;&#30340;&#31034;&#20363;&#22686;&#21152;&#12289;&#29305;&#24449;&#21644;&#32467;&#26524;&#20043;&#38388;&#20851;&#31995;&#30340;&#21464;&#21270;&#21644;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#39057;&#32321;&#25110;&#26410;&#35265;&#36807;&#30340;&#31034;&#20363;&#24615;&#33021;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.02011</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#35786;&#26029;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Diagnosing Model Performance Under Distribution Shift. (arXiv:2303.02011v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; DISDE &#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27169;&#22411;&#22312;&#19981;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;&#35813;&#26041;&#27861;&#23558;&#24615;&#33021;&#19979;&#38477;&#20998;&#35299;&#20026;&#19977;&#20010;&#26041;&#38754;&#65306;&#38590;&#24230;&#26356;&#22823;&#20294;&#26356;&#39057;&#32321;&#20986;&#29616;&#30340;&#31034;&#20363;&#22686;&#21152;&#12289;&#29305;&#24449;&#21644;&#32467;&#26524;&#20043;&#38388;&#20851;&#31995;&#30340;&#21464;&#21270;&#21644;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#39057;&#32321;&#25110;&#26410;&#35265;&#36807;&#30340;&#31034;&#20363;&#24615;&#33021;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#27169;&#22411;&#22312;&#19981;&#21516;&#20110;&#35757;&#32451;&#20998;&#24067;&#30340;&#30446;&#26631;&#20998;&#24067;&#19979;&#36816;&#34892;&#26102;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#20123;&#25805;&#20316;&#22833;&#36133;&#27169;&#24335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#31216;&#20026; DIstribution Shift DEcomposition&#65288;DISDE&#65289;&#65292;&#23558;&#24615;&#33021;&#19979;&#38477;&#24402;&#22240;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#24615;&#33021;&#19979;&#38477;&#20998;&#35299;&#20026;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#65306;1&#65289;&#26469;&#33258;&#35757;&#32451;&#30340;&#26356;&#38590;&#20294;&#26356;&#39057;&#32321;&#30340;&#31034;&#20363;&#22686;&#21152;&#65307;2&#65289;&#29305;&#24449;&#21644;&#32467;&#26524;&#20043;&#38388;&#20851;&#31995;&#30340;&#21464;&#21270;&#65307;3&#65289;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#39057;&#32321;&#25110;&#26410;&#35265;&#36807;&#30340;&#31034;&#20363;&#24615;&#33021;&#24046;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#22266;&#23450; $X$ &#30340;&#20998;&#24067;&#30340;&#21516;&#26102;&#25913;&#21464; $Y \mid X$ &#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#25110;&#22312;&#22266;&#23450; $Y \mid X$ &#30340;&#26465;&#20214;&#20998;&#24067;&#30340;&#21516;&#26102;&#25913;&#21464; $X$ &#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#19968;&#20010;&#20851;&#20110; $X$ &#30340;&#20551;&#35774;&#20998;&#24067;&#65292;&#20854;&#20013;&#21253;&#21547;&#35757;&#32451;&#21644;&#30446;&#26631;&#20013;&#20849;&#21516;&#30340;&#20540;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#27604;&#36739; $Y \mid X$ &#24182;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction models can perform poorly when deployed to target distributions different from the training distribution. To understand these operational failure modes, we develop a method, called DIstribution Shift DEcomposition (DISDE), to attribute a drop in performance to different types of distribution shifts. Our approach decomposes the performance drop into terms for 1) an increase in harder but frequently seen examples from training, 2) changes in the relationship between features and outcomes, and 3) poor performance on examples infrequent or unseen during training. These terms are defined by fixing a distribution on $X$ while varying the conditional distribution of $Y \mid X$ between training and target, or by fixing the conditional distribution of $Y \mid X$ while varying the distribution on $X$. In order to do this, we define a hypothetical distribution on $X$ consisting of values common in both training and target, over which it is easy to compare $Y \mid X$ and thus predictive
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#28436;&#21270;&#31639;&#27861;&#65292;&#37319;&#29992;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#36873;&#21462;&#30334;&#19975;&#20687;&#32032;&#30149;&#29702;&#22270;&#29255;&#20013;&#30340;&#29305;&#24449;&#65292;&#26500;&#24314;&#39057;&#32321;&#29305;&#24449;&#30452;&#26041;&#22270;&#65288;FFH&#65289;WSI&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;WSI&#22270;&#20687;&#22788;&#29702;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.00943</link><description>&lt;p&gt;
&#21160;&#24577;&#28436;&#21270;&#31639;&#27861;&#22312;&#30334;&#19975;&#20687;&#32032;&#30149;&#29702;&#22270;&#29255;&#28145;&#24230;&#23884;&#20837;&#29305;&#24449;&#36873;&#25321;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Computation in Action: Feature Selection for Deep Embedding Spaces of Gigapixel Pathology Images. (arXiv:2303.00943v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#28436;&#21270;&#31639;&#27861;&#65292;&#37319;&#29992;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#36873;&#21462;&#30334;&#19975;&#20687;&#32032;&#30149;&#29702;&#22270;&#29255;&#20013;&#30340;&#29305;&#24449;&#65292;&#26500;&#24314;&#39057;&#32321;&#29305;&#24449;&#30452;&#26041;&#22270;&#65288;FFH&#65289;WSI&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;WSI&#22270;&#20687;&#22788;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#25968;&#23383;&#30149;&#29702;&#23398;&#25216;&#26415;&#20173;&#38754;&#20020;&#30528;&#39640;&#32500;&#24230;&#25968;&#23383;&#30149;&#29702;&#20999;&#29255;&#30340;&#25361;&#25112;&#65292;&#21363;&#22914;&#20309;&#39640;&#25928;&#22788;&#29702;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#12290;&#20026;&#20102;&#21152;&#36895;&#22270;&#20687;&#20998;&#26512;&#24182;&#20026;&#30149;&#29702;&#23398;&#32467;&#26524;&#30340;&#21487;&#35270;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#24110;&#21161;&#65292;&#38656;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24341;&#20837;&#32039;&#20945;&#30340;WSI&#34920;&#31034;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#23884;&#20837;&#24335;WSI&#34920;&#31034;&#30340;&#21160;&#24577;&#28436;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#20174;&#22522;&#20110;&#34917;&#19969;&#30340;&#37319;&#26679;&#24320;&#22987;&#65292;&#21033;&#29992;KimiaNet&#31561;&#32452;&#32455;&#30149;&#29702;&#23398;&#19987;&#29992;&#28145;&#24230;&#32593;&#32476;&#26469;&#25552;&#21462;&#22823;&#37327;&#29305;&#24449;&#21521;&#37327;&#12290;&#22312;&#31895;&#31961;&#30340;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#38454;&#27573;&#65292;&#37319;&#29992;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#29305;&#24449;&#25968;&#37327;&#20316;&#20026;&#25351;&#23548;&#65292;&#20351;&#29992;&#32553;&#23567;&#25628;&#32034;&#31354;&#38388;&#30340;&#31574;&#30053;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#36890;&#36807;&#22810;&#27425;&#31895;&#30053;&#30340;&#28436;&#21270;&#31639;&#27861;&#20248;&#21270;&#24471;&#21040;&#30456;&#23545;&#36739;&#20248;&#30340;&#29305;&#24449;&#38598;&#65292;&#26500;&#24314;&#20102;&#39057;&#32321;&#29305;&#24449;&#30452;&#26041;&#22270;&#65288;FFH&#65289;WSI&#34920;&#31034;&#12290;&#26368;&#32456;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#24471;&#21040;&#20102;WIS&#30340;&#32039;&#20945;&#23884;&#20837;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;WSI&#22270;&#20687;&#22788;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the main obstacles of adopting digital pathology is the challenge of efficient processing of hyperdimensional digitized biopsy samples, called whole slide images (WSIs). Exploiting deep learning and introducing compact WSI representations are urgently needed to accelerate image analysis and facilitate the visualization and interpretability of pathology results in a postpandemic world. In this paper, we introduce a new evolutionary approach for WSI representation based on large-scale multi-objective optimization (LSMOP) of deep embeddings. We start with patch-based sampling to feed KimiaNet , a histopathology-specialized deep network, and to extract a multitude of feature vectors. Coarse multi-objective feature selection uses the reduced search space strategy guided by the classification accuracy and the number of features. In the second stage, the frequent features histogram (FFH), a novel WSI representation, is constructed by multiple runs of coarse LSMOP. Fine evolutionary fea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#30340;&#26799;&#24230;&#35745;&#31639;&#27425;&#25968; $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2302.08766</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#23618;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#30340;&#19979;&#30028;&#21644;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk Minimization. (arXiv:2302.08766v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#30340;&#26799;&#24230;&#35745;&#31639;&#27425;&#25968; $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#26368;&#20248;&#21270;&#38382;&#39064;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#19978;&#23618;&#21644;&#19979;&#23618;&#30446;&#26631;&#23545;&#24212;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#22240;&#27492;&#20855;&#26377;&#24635;&#21644;&#32467;&#26500;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33879;&#21517;&#30340;SARAH&#31639;&#27861;&#30340;&#21452;&#23618;&#25193;&#23637;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#38656;&#35201;$\mathcal {O}((n+m)^{\frac{1}{2}}\varepsilon ^{-1})$&#27425;&#26799;&#24230;&#35745;&#31639;&#25165;&#33021;&#23454;&#29616;$\varepsilon$&#31283;&#23450;&#24615;&#65292;&#20854;&#20013;$n+m$&#26159;&#26679;&#26412;&#24635;&#25968;&#65292;&#36825;&#27604;&#20808;&#21069;&#25152;&#26377;&#30340;&#21452;&#23618;&#31639;&#27861;&#37117;&#35201;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#29992;&#20110;&#24471;&#21040;&#21452;&#23618;&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#25152;&#38656;&#30340;oracle&#35843;&#29992;&#27425;&#25968;&#12290;&#36825;&#20010;&#19979;&#30028;&#27491;&#26159;&#25105;&#20204;&#30340;&#31639;&#27861;&#25152;&#36798;&#21040;&#30340;&#65292;&#22240;&#27492;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization problems, which are problems where two optimization problems are nested, have more and more applications in machine learning. In many practical cases, the upper and the lower objectives correspond to empirical risk minimization problems and therefore have a sum structure. In this context, we propose a bilevel extension of the celebrated SARAH algorithm. We demonstrate that the algorithm requires $\mathcal{O}((n+m)^{\frac12}\varepsilon^{-1})$ gradient computations to achieve $\varepsilon$-stationarity with $n+m$ the total number of samples, which improves over all previous bilevel algorithms. Moreover, we provide a lower bound on the number of oracle calls required to get an approximate stationary point of the objective function of the bilevel problem. This lower bound is attained by our algorithm, which is therefore optimal in terms of sample complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#22238;&#24402;&#25216;&#26415;&#24314;&#27169;&#26234;&#33021;&#26448;&#26009;&#28382;&#21518;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#23545;&#21387;&#30005;&#26448;&#26009;&#30340;&#28382;&#21518;&#29616;&#35937;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#12290;&#21516;&#26102;&#22312;&#30913;&#24615;&#26448;&#26009;&#26041;&#38754;&#25552;&#20379;&#20102;&#31232;&#30095;&#30333;&#30418;&#24314;&#27169;&#28382;&#21518;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.05313</link><description>&lt;p&gt;
&#26234;&#33021;&#26448;&#26009;&#20013;&#31232;&#30095;&#28382;&#21518;&#27169;&#22411;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Discovering sparse hysteresis models for smart materials. (arXiv:2302.05313v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#22238;&#24402;&#25216;&#26415;&#24314;&#27169;&#26234;&#33021;&#26448;&#26009;&#28382;&#21518;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#23545;&#21387;&#30005;&#26448;&#26009;&#30340;&#28382;&#21518;&#29616;&#35937;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#12290;&#21516;&#26102;&#22312;&#30913;&#24615;&#26448;&#26009;&#26041;&#38754;&#25552;&#20379;&#20102;&#31232;&#30095;&#30333;&#30418;&#24314;&#27169;&#28382;&#21518;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#22238;&#24402;&#25216;&#26415;&#24314;&#27169;&#26234;&#33021;&#26448;&#26009;&#65288;&#23588;&#20854;&#26159;&#21387;&#30005;&#26448;&#26009;&#65289;&#28382;&#21518;&#30340;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#37319;&#29992;&#20102;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#21644;&#39034;&#24207;&#38408;&#20540;&#26041;&#27861;&#23545;&#36127;&#36131;&#28382;&#21518;&#30340;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#65292;&#24471;&#21040;&#20102;&#31616;&#27905;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#27169;&#25311;&#21644;&#23454;&#39564;&#21387;&#30005;&#26448;&#26009;&#25968;&#25454;&#30340;&#28382;&#21518;&#29616;&#35937;&#12290;&#25991;&#31456;&#36824;&#27169;&#25311;&#20102;&#19981;&#21516;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#21253;&#25324;&#23398;&#20064;&#34676;&#34678;&#24418;&#28382;&#21518;&#12289;&#23545;&#21387;&#30005;&#33268;&#21160;&#22120;&#30340;&#30495;&#23454;&#28382;&#21518;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#36824;&#36890;&#36807;&#20197;&#38750;&#21462;&#21521;&#30005;&#24037;&#38050;&#20026;&#20363;&#65292;&#25552;&#20379;&#20102;&#23545;&#30913;&#24615;&#26448;&#26009;&#31232;&#30095;&#30333;&#30418;&#24314;&#27169;&#28382;&#21518;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents an approach for modelling hysteresis in smart materials, specifically piezoelectric materials, that leverages recent advancements in machine learning, particularly in sparse-regression techniques. While sparse regression has previously been used to model various scientific and engineering phenomena, its application to nonlinear hysteresis modelling in piezoelectric materials has yet to be explored. The study employs the least-squares algorithm with a sequential threshold to model the dynamic system responsible for hysteresis, resulting in a concise model that accurately predicts hysteresis for both simulated and experimental piezoelectric material data. Several numerical experiments are performed, including learning butterfly-shaped hysteresis and modelling real-world hysteresis data for a piezoelectric actuator. Additionally, insights are provided on sparse white-box modelling of hysteresis for magnetic materials taking non-oriented electrical steel as an example
&lt;/p&gt;</description></item><item><title>&#21019;&#26032;&#28857;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#22122;&#22768;&#36807;&#31243;&#30340;&#26143;&#24418;&#38477;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#24191;&#27867;&#36866;&#29992;&#20110;&#25351;&#25968;&#26063;&#20013;&#30340;&#22810;&#31181;&#20998;&#24067;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#32422;&#26463;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2302.05259</link><description>&lt;p&gt;
&#26143;&#24418;&#38477;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Star-Shaped Denoising Diffusion Probabilistic Models. (arXiv:2302.05259v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05259
&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#28857;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#22122;&#22768;&#36807;&#31243;&#30340;&#26143;&#24418;&#38477;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#24191;&#27867;&#36866;&#29992;&#20110;&#25351;&#25968;&#26063;&#20013;&#30340;&#22810;&#31181;&#20998;&#24067;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#32422;&#26463;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38477;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#20013;&#26080;&#22788;&#19981;&#22312;&#30340;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22823;&#22810;&#23616;&#38480;&#20110;&#39640;&#26031;&#21644;&#31163;&#25955;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26143;&#24418;&#38477;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;SS-DDPM&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#22122;&#22768;&#36807;&#31243;&#30340;&#27169;&#22411;&#12290;&#22312;&#39640;&#26031;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#27169;&#22411;&#31561;&#25928;&#20110;&#39532;&#23572;&#21487;&#22827;DDPM&#12290;&#28982;&#32780;&#65292;&#23427;&#21487;&#20197;&#23450;&#20041;&#21644;&#36866;&#29992;&#20110;&#20219;&#24847;&#22122;&#22768;&#20998;&#24067;&#65292;&#24182;&#19988;&#23545;&#20110;&#33853;&#22312;&#25351;&#25968;&#26063;&#20013;&#30340;&#24191;&#27867;&#20998;&#24067;&#65292;&#23427;&#37319;&#29992;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#37197;&#26041;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;Beta&#65292;von Mises-Fisher&#65292;Dirichlet&#65292;Wishart&#31561;&#20998;&#24067;&#30340;&#25193;&#25955;&#26679;&#24335;&#27169;&#22411;&#65292;&#24403;&#25968;&#25454;&#20301;&#20110;&#32422;&#26463;&#27969;&#24418;&#19978;&#26102;&#29305;&#21035;&#26377;&#29992;&#65292;&#20363;&#22914;&#21333;&#20301;&#29699;&#65292;&#27491;&#21322;&#23450;&#30697;&#38453;&#30340;&#31354;&#38388;&#65292;&#27010;&#29575;&#21333;&#32431;&#24418;&#31561;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#24456;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods based on Denoising Diffusion Probabilistic Models (DDPM) became a ubiquitous tool in generative modeling. However, they are mostly limited to Gaussian and discrete diffusion processes. We propose Star-Shaped Denoising Diffusion Probabilistic Models (SS-DDPM), a model with a non-Markovian diffusion-like noising process. In the case of Gaussian distributions, this model is equivalent to Markovian DDPMs. However, it can be defined and applied with arbitrary noising distributions, and admits efficient training and sampling algorithms for a wide range of distributions that lie in the exponential family. We provide a simple recipe for designing diffusion-like models with distributions like Beta, von Mises--Fisher, Dirichlet, Wishart and others, which can be especially useful when data lies on a constrained manifold such as the unit sphere, the space of positive semi-definite matrices, the probabilistic simplex, etc. We evaluate the model in different settings and find it competitive 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#27169;&#22411;&#65288;NCN&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#26469;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#35299;&#20915;&#38142;&#36335;&#19981;&#23436;&#25972;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.00890</link><description>&lt;p&gt;
&#20855;&#26377;&#23436;&#25104;&#21151;&#33021;&#30340;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Neural Common Neighbor with Completion for Link Prediction. (arXiv:2302.00890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#27169;&#22411;&#65288;NCN&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#26469;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#35299;&#20915;&#38142;&#36335;&#19981;&#23436;&#25972;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;vanilla&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#22312;&#21508;&#31181;&#22270;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#36890;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#21482;&#20351;&#29992;&#20004;&#20010;&#21333;&#29420;&#30446;&#26631;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#24182;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#12290;&#20026;&#20102;&#25429;&#33719;&#25104;&#23545;&#20851;&#31995;&#65292;&#19968;&#20123;&#27169;&#22411;&#23558;&#25163;&#21160;&#21151;&#33021;&#28155;&#21152;&#21040;&#36755;&#20837;&#22270;&#20013;&#65292;&#24182;&#20351;&#29992;MPNN&#30340;&#36755;&#20986;&#26469;&#29983;&#25104;&#25104;&#23545;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#20854;&#20182;&#20154;&#30452;&#25509;&#23558;&#25163;&#21160;&#21151;&#33021;&#29992;&#20316;&#25104;&#23545;&#34920;&#31034;&#12290;&#23613;&#31649;&#27492;&#31616;&#21270;&#36991;&#20813;&#20102;&#23558;GNN&#36880;&#20010;&#38142;&#25509;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#38142;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#30001;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#21644;&#19981;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#29305;&#24449;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#26377;&#24456;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#20445;&#25345;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#65288;NCN&#65289;&#65292;&#23427;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;NCN&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26410;&#35266;&#23519;&#21040;&#30340;&#38142;&#25509;&#38382;&#39064;&#12290;&#22270;&#30340;&#19981;&#23436;&#25972;&#24615;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#24182;&#23548;&#33268;&#20998;&#24067;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Despite its outstanding performance in various graph tasks, vanilla Message Passing Neural Network (MPNN) usually fails in link prediction tasks, as it only uses representations of two individual target nodes and ignores the pairwise relation between them. To capture the pairwise relations, some models add manual features to the input graph and use the output of MPNN to produce pairwise representations. In contrast, others directly use manual features as pairwise representations. Though this simplification avoids applying a GNN to each link individually and thus improves scalability, these models still have much room for performance improvement due to the hand-crafted and unlearnable pairwise features. To upgrade performance while maintaining scalability, we propose Neural Common Neighbor (NCN), which uses learnable pairwise representations. To further boost NCN, we study the unobserved link problem. The incompleteness of the graph is ubiquitous and leads to distribution shifts between
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#27010;&#29575;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;MTP-GO&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22330;&#26223;&#65292;&#37319;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23454;&#29616;&#36816;&#21160;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#23454;&#29616;&#22810;&#27169;&#24577;&#27010;&#29575;&#39044;&#27979;&#65292;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.00735</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#27010;&#29575;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;MTP-GO&#19982;&#31070;&#32463;ODE
&lt;/p&gt;
&lt;p&gt;
MTP-GO: Graph-Based Probabilistic Multi-Agent Trajectory Prediction with Neural ODEs. (arXiv:2302.00735v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#27010;&#29575;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;MTP-GO&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22330;&#26223;&#65292;&#37319;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23454;&#29616;&#36816;&#21160;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#23454;&#29616;&#22810;&#27169;&#24577;&#27010;&#29575;&#39044;&#27979;&#65292;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#24377;&#24615;&#30340;&#33258;&#20027;&#36335;&#24452;&#35268;&#21010;&#38656;&#35201;&#23545;&#21608;&#22260;&#36947;&#36335;&#29992;&#25143;&#26410;&#26469;&#34892;&#20026;&#20570;&#20986;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#20026;&#21709;&#24212;&#27492;&#38656;&#27714;&#21450;&#30456;&#20851;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#21517;&#20026;MTP-GO&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#22330;&#26223;&#36827;&#34892;&#32534;&#30721;&#65292;&#29983;&#25104;&#24213;&#23618;&#36816;&#21160;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36816;&#21160;&#27169;&#22411;&#37319;&#29992;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#20854;&#20013;&#30340;&#29366;&#24577;&#36716;&#31227;&#20989;&#25968;&#23558;&#21644;&#20854;&#20182;&#37096;&#20998;&#19968;&#36215;&#36827;&#34892;&#23398;&#20064;&#12290;&#32467;&#21512;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#33719;&#24471;&#22810;&#27169;&#24577;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#33021;&#21147;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enabling resilient autonomous motion planning requires robust predictions of surrounding road users' future behavior. In response to this need and the associated challenges, we introduce our model titled MTP-GO. The model encodes the scene using temporal graph neural networks to produce the inputs to an underlying motion model. The motion model is implemented using neural ordinary differential equations where the state-transition functions are learned with the rest of the model. Multimodal probabilistic predictions are obtained by combining the concept of mixture density networks and Kalman filtering. The results illustrate the predictive capabilities of the proposed model across various data sets, outperforming several state-of-the-art methods on a number of metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#26500;&#24314;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19978;&#38745;&#27490;&#39640;&#26031;&#36807;&#31243;&#30340;&#23454;&#29992;&#25216;&#26415;&#65292;&#33021;&#22815;&#23545;&#23450;&#20041;&#22312;&#36825;&#20123;&#31354;&#38388;&#19978;&#30340;&#20808;&#39564;&#21644;&#21518;&#39564;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#23454;&#38469;&#37319;&#26679;&#21644;&#35745;&#31639;&#21327;&#26041;&#24046;&#26680;&#12290;</title><link>http://arxiv.org/abs/2301.13088</link><description>&lt;p&gt;
Lie &#32676;&#21644;&#23427;&#20204;&#30340;&#40784;&#27425;&#31354;&#38388;&#19978;&#30340;&#38745;&#27490;&#26680;&#21644;&#39640;&#26031;&#36807;&#31243; II&#65306;&#38750;&#32039;&#23545;&#31216;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces II: non-compact symmetric spaces. (arXiv:2301.13088v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#26500;&#24314;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19978;&#38745;&#27490;&#39640;&#26031;&#36807;&#31243;&#30340;&#23454;&#29992;&#25216;&#26415;&#65292;&#33021;&#22815;&#23545;&#23450;&#20041;&#22312;&#36825;&#20123;&#31354;&#38388;&#19978;&#30340;&#20808;&#39564;&#21644;&#21518;&#39564;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#23454;&#38469;&#37319;&#26679;&#21644;&#35745;&#31639;&#21327;&#26041;&#24046;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26102;&#31354;&#27169;&#22411;&#20043;&#19968;&#65292;&#23427;&#21487;&#20197;&#32534;&#30721;&#26377;&#20851;&#24314;&#27169;&#20989;&#25968;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#24182;&#21487;&#29992;&#20110;&#31934;&#30830;&#25110;&#36817;&#20284;&#36125;&#21494;&#26031;&#23398;&#20064;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#29289;&#29702;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65292;&#20197;&#21450;&#22320;&#36136;&#32479;&#35745;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#31561;&#39046;&#22495;&#65292;&#23545;&#23545;&#31216;&#24615;&#30340;&#19981;&#21464;&#24615;&#26159;&#21487;&#20197;&#32771;&#34385;&#30340;&#26368;&#22522;&#26412;&#24418;&#24335;&#20043;&#19968;&#12290;&#39640;&#26031;&#36807;&#31243;&#21327;&#26041;&#24046;&#23545;&#36825;&#20123;&#23545;&#31216;&#24615;&#30340;&#19981;&#21464;&#24615;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#31354;&#38388;&#30340;&#24179;&#31283;&#24615;&#27010;&#24565;&#30340;&#26368;&#33258;&#28982;&#30340;&#25512;&#24191;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#24314;&#31435;&#38745;&#27490;&#39640;&#26031;&#36807;&#31243;&#30340;&#26500;&#36896;&#24615;&#21644;&#23454;&#29992;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#23545;&#31216;&#24615;&#32972;&#26223;&#19979;&#20986;&#29616;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#38750;&#24120;&#22823;&#30340;&#31867;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#20351;&#24471;&#33021;&#22815;&#65288;i&#65289;&#35745;&#31639;&#21327;&#26041;&#24046;&#26680;&#21644;&#65288;ii&#65289;&#20174;&#36825;&#20123;&#31354;&#38388;&#19978;&#23450;&#20041;&#30340;&#20808;&#39564;&#21644;&#21518;&#39564;&#39640;&#26031;&#36807;&#31243;&#20013;&#23454;&#38469;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are arguably the most important class of spatiotemporal models within machine learning. They encode prior information about the modeled function and can be used for exact or approximate Bayesian learning. In many applications, particularly in physical sciences and engineering, but also in areas such as geostatistics and neuroscience, invariance to symmetries is one of the most fundamental forms of prior information one can consider. The invariance of a Gaussian process' covariance to such symmetries gives rise to the most natural generalization of the concept of stationarity to such spaces. In this work, we develop constructive and practical techniques for building stationary Gaussian processes on a very large class of non-Euclidean spaces arising in the context of symmetries. Our techniques make it possible to (i) calculate covariance kernels and (ii) sample from prior and posterior Gaussian processes defined on such spaces, both in a practical manner. This work is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35760;&#24518;&#25216;&#26415;&#21152;&#36895;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Transformer&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20462;&#25913;&#27169;&#22411;&#26550;&#26500;&#25110;&#20351;&#29992;&#29305;&#27530;&#30828;&#20214;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#65292;&#24182;&#21487;&#20197;&#20351;&#25512;&#29702;&#24310;&#36831;&#38477;&#20302;22%&#12290;</title><link>http://arxiv.org/abs/2301.09262</link><description>&lt;p&gt;
AttMEMO: &#22312;&#22823;&#20869;&#23384;&#31995;&#32479;&#19978;&#21033;&#29992;&#35760;&#24518;&#21270;&#21152;&#36895;Transformers
&lt;/p&gt;
&lt;p&gt;
AttMEMO : Accelerating Transformers with Memoization on Big Memory Systems. (arXiv:2301.09262v2 [cs.PF] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35760;&#24518;&#25216;&#26415;&#21152;&#36895;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Transformer&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20462;&#25913;&#27169;&#22411;&#26550;&#26500;&#25110;&#20351;&#29992;&#29305;&#27530;&#30828;&#20214;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#65292;&#24182;&#21487;&#20197;&#20351;&#25512;&#29702;&#24310;&#36831;&#38477;&#20302;22%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22240;&#20854;&#20248;&#36234;&#30340;&#25512;&#29702;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#21534;&#21520;&#37327;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;Transformer&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#65292;&#23548;&#33268;&#25512;&#29702;&#26102;&#38388;&#36739;&#38271;&#12290;&#29616;&#26377;&#30340;Transformer&#25512;&#29702;&#21152;&#36895;&#24037;&#20316;&#23384;&#22312;&#38480;&#21046;&#65292;&#36825;&#20123;&#38480;&#21046;&#35201;&#20040;&#26159;&#30001;&#20110;&#20462;&#25913;Transformer&#26550;&#26500;&#65292;&#35201;&#20040;&#26159;&#38656;&#35201;&#19987;&#38376;&#30340;&#30828;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#35760;&#24518;&#21270;&#21152;&#36895;Transformer&#27169;&#22411;&#30340;&#26426;&#20250;&#65292;&#32780;&#19981;&#28041;&#21450;&#20197;&#19978;&#38480;&#21046;&#12290;&#22522;&#20110;&#36825;&#26679;&#30340;&#29420;&#29305;&#35266;&#23519;&#65292;&#21363;&#22312;&#25512;&#29702;&#24207;&#21015;&#20869;Attention&#35745;&#31639;&#20013;&#23384;&#22312;&#20016;&#23500;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21033;&#29992;&#26032;&#20852;&#30340;&#22823;&#20869;&#23384;&#31995;&#32479;&#30340;&#35760;&#24518;&#21270;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#25216;&#26415;&#26469;&#26597;&#25214;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#36755;&#20837;&#65292;&#20197;&#35782;&#21035;&#35745;&#31639;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#22914;&#20869;&#23384;&#26144;&#23556;&#21644;&#36873;&#25321;&#24615;&#35760;&#24518;&#21270;&#65292;&#20197;&#36991;&#20813;&#20869;&#23384;&#22797;&#21046;&#21644;&#19981;&#24517;&#35201;&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#20351;&#24471;&#25512;&#29702;&#24310;&#36831;&#38477;&#20302;&#20102;22%&#65292;&#32780;&#19988;&#20869;&#23384;&#38656;&#27714;&#36866;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36866;&#29992;&#20110;&#21508;&#31181;Transformer&#27169;&#22411;&#65292;&#32780;&#19988;&#26080;&#38656;&#36827;&#34892;&#37325;&#35201;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models gain popularity because of their superior inference accuracy and inference throughput. However, the transformer is computation-intensive, causing a long inference time. The existing works on transformer inference acceleration have limitations caused by either the modification of transformer architectures or the need of specialized hardware. In this paper, we identify the opportunities of using memoization to accelerate the self-attention mechanism in transformers without the above limitations. Built upon a unique observation that there is rich similarity in attention computation across inference sequences, we build a memoization database that leverages the emerging big memory system. We introduce a novel embedding technique to find semantically similar inputs to identify computation similarity. We also introduce a series of techniques such as memory mapping and selective memoization to avoid memory copy and unnecessary overhead. We enable 22% inference-latency reduct
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23618;&#32423;&#32465;&#23450;&#21644;&#32852;&#24819;&#26816;&#32034;&#21464;&#20026;&#30701;&#26399;&#21644;&#38271;&#26399;&#22768;&#26126;&#24615;&#35760;&#24518;&#30340;&#22312;&#32447;&#39044;&#27979;&#22788;&#29702;&#31995;&#32479;&#21487;&#33021;&#20250;&#24863;&#30693;&#21040;&#33258;&#24049;&#20855;&#26377;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2301.07016</link><description>&lt;p&gt;
&#24847;&#35782;&#26159;&#23398;&#20064;&#30340;&#36807;&#31243;&#65306;&#36890;&#36807;&#32465;&#23450;&#23398;&#20064;&#30340;&#39044;&#27979;&#22788;&#29702;&#31995;&#32479;&#21487;&#33021;&#20250;&#23558;&#33258;&#24049;&#24863;&#30693;&#20026;&#26377;&#24847;&#35782;&#30340;
&lt;/p&gt;
&lt;p&gt;
Consciousness is learning: predictive processing systems that learn by binding may perceive themselves as conscious. (arXiv:2301.07016v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07016
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23618;&#32423;&#32465;&#23450;&#21644;&#32852;&#24819;&#26816;&#32034;&#21464;&#20026;&#30701;&#26399;&#21644;&#38271;&#26399;&#22768;&#26126;&#24615;&#35760;&#24518;&#30340;&#22312;&#32447;&#39044;&#27979;&#22788;&#29702;&#31995;&#32479;&#21487;&#33021;&#20250;&#24863;&#30693;&#21040;&#33258;&#24049;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#29305;&#23450;&#22797;&#26434;&#39046;&#22495;&#23454;&#29616;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#65292;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#39640;&#25928;&#22320;&#27867;&#21270;&#20173;&#28982;&#26159;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#22312;&#20154;&#31867;&#36523;&#19978;&#65292;&#36825;&#31181;&#23398;&#20064;&#36890;&#36807;&#22768;&#26126;&#24615;&#23384;&#20648;&#36807;&#31243;&#36827;&#34892;&#65292;&#24182;&#19988;&#19982;&#24847;&#35782;&#23494;&#20999;&#30456;&#20851;&#12290;&#39044;&#27979;&#22788;&#29702;&#34987;&#25512;&#24191;&#20026;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#29702;&#26694;&#26550;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#30382;&#36136;&#22914;&#20309;&#23454;&#29616;&#28145;&#24230;&#29983;&#25104;&#24863;&#30693;&#27169;&#22411;&#65292;&#29992;&#20110;&#24863;&#23448;&#25968;&#25454;&#21644;&#34892;&#20026;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#22788;&#29702;&#23545;&#20110;&#24555;&#36895;&#32452;&#25104;&#24335;&#23398;&#20064;&#25110;&#24847;&#35782;&#20043;&#35868;&#25552;&#20379;&#20102;&#24456;&#23569;&#30340;&#30452;&#25509;&#35265;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#65292;&#36890;&#36807;&#36890;&#36807;&#32465;&#23450;&#39044;&#27979;&#20013;&#30340;&#23618;&#27425;&#27169;&#22411;&#26469;&#23454;&#29616;&#22312;&#32447;&#23398;&#20064;&#65292;&#39044;&#27979;&#22788;&#29702;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#20174;&#21333;&#20010;&#31034;&#20363;&#20013;&#20026;&#24863;&#30693;&#21644;&#34892;&#21160;&#24418;&#25104;&#24037;&#20316;&#35760;&#24518;&#65292;&#22312;&#26032;&#24773;&#20917;&#19979;&#28789;&#27963;&#27867;&#21270;&#65292;&#36825;&#21487;&#36890;&#36807;&#32852;&#24819;&#26816;&#32034;&#21464;&#20026;&#30701;&#26399;&#21644;&#38271;&#26399;&#30340;&#22768;&#26126;&#24615;&#35760;&#24518;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#22312;&#32447;&#23618;&#32423;&#39044;&#27979;&#32465;&#23450;&#8221;&#65292;&#20063;&#21487;&#33021;&#26159;&#31995;&#32479;&#24863;&#30693;&#33258;&#24049;&#20855;&#26377;&#24847;&#35782;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#20851;&#20110;&#24863;&#30693;&#30340;&#12289;&#36816;&#21160;&#30340;&#12289;&#35748;&#30693;&#30340;&#21644;&#24773;&#24863;&#30340;&#24847;&#35782;&#30340;&#32479;&#19968;&#35299;&#37322;&#65292;&#24182;&#20855;&#26377;&#36827;&#21270;&#21644;&#21457;&#32946;&#29983;&#29289;&#23398;&#30340;&#28145;&#21051;&#26681;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms have achieved superhuman performance in specific complex domains. Yet learning online from few examples and efficiently generalizing across domains remains elusive. In humans such learning proceeds via declarative memory formation and is closely associated with consciousness. Predictive processing has been advanced as a principled Bayesian inference framework for understanding the cortex as implementing deep generative perceptual models for both sensory data and action control. However, predictive processing offers little direct insight into fast compositional learning or the mystery of consciousness. Here we propose that through implementing online learning by hierarchical binding of unpredicted inferences, a predictive processing system may flexibly generalize in novel situations by forming working memories for perceptions and actions from single examples, which can become short- and long-term declarative memories retrievable by associative recall. We argu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;LQR&#25511;&#21046;&#22120;&#65292;&#22312;&#20960;&#20046;&#24517;&#28982;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377; $\tilde{ \mathcal{O}}(\sqrt{T})$ &#21518;&#24724;&#19978;&#38480;&#35777;&#26126;&#65292;&#19988;&#20855;&#26377;&#26029;&#30005;&#26426;&#21046;&#20445;&#35777;&#23433;&#20840;&#24182;&#23545;&#24615;&#33021;&#24433;&#21709;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2301.05537</link><description>&lt;p&gt;
&#33258;&#36866;&#24212; LQR &#31639;&#27861;&#30340;&#36817;&#20046;&#24517;&#28982; $\sqrt{T}$ &#21518;&#24724;&#19978;&#38480;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Almost Surely $\sqrt{T}$ Regret Bound for Adaptive LQR. (arXiv:2301.05537v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;LQR&#25511;&#21046;&#22120;&#65292;&#22312;&#20960;&#20046;&#24517;&#28982;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377; $\tilde{ \mathcal{O}}(\sqrt{T})$ &#21518;&#24724;&#19978;&#38480;&#35777;&#26126;&#65292;&#19988;&#20855;&#26377;&#26029;&#30005;&#26426;&#21046;&#20445;&#35777;&#23433;&#20840;&#24182;&#23545;&#24615;&#33021;&#24433;&#21709;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26410;&#30693;&#31995;&#32479;&#21442;&#25968;&#30340;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#38382;&#39064;(LQR)&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#33267;&#20170;&#20173;&#19981;&#28165;&#26970;&#26159;&#21542;&#33021;&#20960;&#20046;&#24517;&#28982;&#22320;&#36798;&#21040; $\tilde{ \mathcal{O}}(\sqrt{T})$ &#30340;&#21518;&#24724;&#19978;&#38480;&#65292;&#32780;&#26412;&#25991;&#21017;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;LQR&#25511;&#21046;&#22120;&#65292;&#22312;&#20960;&#20046;&#24517;&#28982;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377; $\tilde{ \mathcal{O}}(\sqrt{T})$ &#21518;&#24724;&#19978;&#38480;&#30340;&#35777;&#26126;&#12290;&#35813;&#25511;&#21046;&#22120;&#20855;&#26377;&#26029;&#30005;&#26426;&#21046;&#65292;&#21487;&#20197;&#32469;&#36807;&#28508;&#22312;&#30340;&#23433;&#20840;&#38544;&#24739;&#24182;&#30830;&#20445;&#31995;&#32479;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#24615;&#65292;&#20294;&#34987;&#35777;&#26126;&#21482;&#20250;&#26377;&#26377;&#38480;&#27425;&#35302;&#21457;&#65292;&#24182;&#23545;&#25511;&#21046;&#22120;&#30340;&#28176;&#36817;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#36890;&#36807;&#22312;&#30000;&#32435;&#35199;&#20234;&#22763;&#26364;(Tennessee Eastman)&#24037;&#33402;&#20013;&#36827;&#34892;&#20223;&#30495;&#39564;&#35777;&#20102;&#35813;&#25511;&#21046;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Linear-Quadratic Regulation (LQR) problem with unknown system parameters has been widely studied, but it has remained unclear whether $\tilde{ \mathcal{O}}(\sqrt{T})$ regret, which is the best known dependence on time, can be achieved almost surely. In this paper, we propose an adaptive LQR controller with almost surely $\tilde{ \mathcal{O}}(\sqrt{T})$ regret upper bound. The controller features a circuit-breaking mechanism, which circumvents potential safety breach and guarantees the convergence of the system parameter estimate, but is shown to be triggered only finitely often and hence has negligible effect on the asymptotic performance of the controller. The proposed controller is also validated via simulation on Tennessee Eastman Process~(TEP), a commonly used industrial process example.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#38454;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#35299;&#20915;&#32422;&#26463;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20854;&#25805;&#20316;&#22797;&#26434;&#24230;&#20026; ${\cal O}(\varepsilon^{-4}\log\varepsilon^{-1})$&#12290;</title><link>http://arxiv.org/abs/2301.02060</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32422;&#26463;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#38454;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A first-order augmented Lagrangian method for constrained minimax optimization. (arXiv:2301.02060v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#38454;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#35299;&#20915;&#32422;&#26463;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20854;&#25805;&#20316;&#22797;&#26434;&#24230;&#20026; ${\cal O}(\varepsilon^{-4}\log\varepsilon^{-1})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#32422;&#26463;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#38454;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20854;&#23376;&#38382;&#39064;&#34987;&#21457;&#29616;&#26159;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#32467;&#26500;&#21270;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20316;&#32773;&#22312; [26] &#20013;&#26368;&#36817;&#24320;&#21457;&#30340;&#19968;&#38454;&#26041;&#27861;&#26469;&#36866;&#24403;&#22320;&#35299;&#20915;&#12290;&#22312;&#19968;&#20123;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#65292;&#20026;&#20102;&#25214;&#21040;&#32422;&#26463;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#19968;&#20010; $\varepsilon$-KKT &#35299;&#65292;&#35813;&#26041;&#27861;&#30340;&#25805;&#20316;&#22797;&#26434;&#24230;&#20026; ${\cal O}(\varepsilon^{-4}\log\varepsilon^{-1})$&#65292;&#35813;&#22797;&#26434;&#24230;&#26159;&#30001;&#22522;&#26412;&#25805;&#20316;&#27979;&#37327;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we study a class of constrained minimax problems. In particular, we propose a first-order augmented Lagrangian method for solving them, whose subproblems turn out to be a much simpler structured minimax problem and are suitably solved by a first-order method recently developed in [26] by the authors. Under some suitable assumptions, an \emph{operation complexity} of ${\cal O}(\varepsilon^{-4}\log\varepsilon^{-1})$, measured by its fundamental operations, is established for the first-order augmented Lagrangian method for finding an $\varepsilon$-KKT solution of the constrained minimax problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#20854;&#36866;&#29992;&#24615;&#26356;&#24191;&#19988;&#19981;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2301.00752</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications. (arXiv:2301.00752v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#20854;&#36866;&#29992;&#24615;&#26356;&#24191;&#19988;&#19981;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#65288;mmWave&#65289;&#36890;&#20449;&#30340;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#22270;&#20687;&#30340;&#26102;&#38388;&#24207;&#21015;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27573;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65292;&#20197;&#32531;&#35299;&#34892;&#20154;&#38459;&#25377;&#22240;&#32032;&#23545;mmWave&#36890;&#20449;&#30340;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#36825;&#20123;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;&#25668;&#20687;&#22836;&#22270;&#20687;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;mmWave&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#34892;&#24615;&#12290;&#28857;&#20113;&#23558;&#19977;&#32500;&#31354;&#38388;&#34920;&#31034;&#20026;&#28857;&#38598;&#65292;&#20854;&#31354;&#38388;&#24615;&#36136;&#26356;&#21152;&#31232;&#30095;&#65292;&#19981;&#22826;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65292;&#24182;&#19988;&#36824;&#25552;&#20379;&#20102;3D&#20301;&#32622;&#21644;&#36816;&#21160;&#20449;&#24687;&#65292;&#36825;&#23545;&#20102;&#35299;&#28041;&#21450;&#34892;&#20154;&#30340;&#26080;&#32447;&#30005;&#20256;&#25773;&#29615;&#22659;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study demonstrates the feasibility of point cloud-based proactive link quality prediction for millimeter-wave (mmWave) communications. Previous studies have proposed machine learning-based methods to predict received signal strength for future time periods using time series of depth images to mitigate the line-of-sight (LOS) path blockage by pedestrians in mmWave communication. However, these image-based methods have limited applicability due to privacy concerns as camera images may contain sensitive information. This study proposes a point cloud-based method for mmWave link quality prediction and demonstrates its feasibility through experiments. Point clouds represent three-dimensional (3D) spaces as a set of points and are sparser and less likely to contain sensitive information than camera images. Additionally, point clouds provide 3D position and motion information, which is necessary for understanding the radio propagation environment involving pedestrians. This study designs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#29702;&#35770;&#20013;&#26377;&#20851;&#29702;&#35770;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#20013;&#31639;&#27861;&#31283;&#23450;&#24615;&#30340;&#20132;&#20114;&#65292;&#36890;&#36807;&#31639;&#27861;&#24615;&#36136;&#21462;&#20195;&#20102;&#26080;&#38480;&#65292;&#37325;&#35775;&#20102;Shelah&#38395;&#21517;&#30340;&#19981;&#31283;&#23450;&#20844;&#24335;&#23450;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#33021;&#26368;&#32456;&#27491;&#30830;&#23398;&#20064;&#27169;&#22411;&#65292;&#34920;&#24449;&#20102;Littlestone&#65288;&#31283;&#23450;&#65289;&#31867;&#65292;&#36879;&#36807;&#27169;&#22411;&#35770;&#20013;&#31867;&#22411;&#30340;&#21487;&#23450;&#20041;&#24615;&#24418;&#24335;&#21270;&#20102;Littlestone&#31867;&#30340;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2212.05050</link><description>&lt;p&gt;
&#36890;&#36807;&#31639;&#27861;&#37325;&#35775;&#19981;&#31283;&#23450;&#20844;&#24335;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
The unstable formula theorem revisited via algorithms. (arXiv:2212.05050v2 [math.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#29702;&#35770;&#20013;&#26377;&#20851;&#29702;&#35770;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#20013;&#31639;&#27861;&#31283;&#23450;&#24615;&#30340;&#20132;&#20114;&#65292;&#36890;&#36807;&#31639;&#27861;&#24615;&#36136;&#21462;&#20195;&#20102;&#26080;&#38480;&#65292;&#37325;&#35775;&#20102;Shelah&#38395;&#21517;&#30340;&#19981;&#31283;&#23450;&#20844;&#24335;&#23450;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#33021;&#26368;&#32456;&#27491;&#30830;&#23398;&#20064;&#27169;&#22411;&#65292;&#34920;&#24449;&#20102;Littlestone&#65288;&#31283;&#23450;&#65289;&#31867;&#65292;&#36879;&#36807;&#27169;&#22411;&#35770;&#20013;&#31867;&#22411;&#30340;&#21487;&#23450;&#20041;&#24615;&#24418;&#24335;&#21270;&#20102;Littlestone&#31867;&#30340;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26377;&#20851;&#27169;&#22411;&#29702;&#35770;&#20013;&#20851;&#20110;&#29702;&#35770;&#31283;&#23450;&#24615;&#30340;&#22522;&#30784;&#32467;&#26524;&#19982;&#23398;&#20064;&#20013;&#31639;&#27861;&#31283;&#23450;&#24615;&#20043;&#38388;&#24778;&#20154;&#30340;&#20132;&#20114;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#31639;&#27861;&#24615;&#36136;&#21462;&#20195;&#26080;&#38480;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Shelah&#38395;&#21517;&#30340;&#19981;&#31283;&#23450;&#20844;&#24335;&#23450;&#29702;&#30340;&#23436;&#25972;&#31639;&#27861;&#31867;&#27604;&#12290;&#36825;&#20854;&#20013;&#28041;&#21450;&#20102;&#20960;&#20010;&#26032;&#23450;&#29702;&#20197;&#21450;&#26368;&#36817;&#30340;&#30740;&#31350;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#8220;&#21487;&#33021;&#26368;&#32456;&#27491;&#30830;&#8221;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36825;&#20010;&#27169;&#22411;&#34920;&#24449;&#20102;Littlestone&#65288;&#31283;&#23450;&#65289;&#31867;&#65307;&#24182;&#36890;&#36807;&#27169;&#22411;&#35770;&#20013;&#31867;&#22411;&#30340;&#21487;&#23450;&#20041;&#24615;&#31867;&#27604;&#25551;&#36848;&#20102;Littlestone&#31867;&#30340;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is about the surprising interaction of a foundational result from model theory about stability of theories, which seems to be inherently about the infinite, with algorithmic stability in learning. Specifically, we develop a complete algorithmic analogue of Shelah's celebrated Unstable Formula Theorem, with algorithmic properties taking the place of the infinite. This draws on several new theorems as well as much recent work. In particular we introduce a new ``Probably Eventually Correct'' learning model, of independent interest, and characterize Littlestone (stable) classes in terms of this model; and we describe Littlestone classes via approximations, by analogy to definability of types in model theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#35745;&#31639;&#30149;&#29702;&#23398;&#25152;&#38656;&#30340;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#38382;&#39064;&#65292;&#36827;&#34892;&#20102;&#26368;&#22823;&#35268;&#27169;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#35268;&#27169;&#39046;&#22495;&#23545;&#40784;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#30149;&#29702;&#23398;&#20013;&#22987;&#32456;&#20248;&#20110;ImageNet&#19978;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#22871;&#39046;&#22495;&#29305;&#23450;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#26376;&#32463;&#21608;&#26399;&#30149;&#29702;&#25968;&#25454;&#38598;&#20013;&#20063;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.04690</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#30149;&#29702;&#25968;&#25454;&#38598;&#19978;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;&#27604;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Self-Supervised Learning on Diverse Pathology Datasets. (arXiv:2212.04690v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#35745;&#31639;&#30149;&#29702;&#23398;&#25152;&#38656;&#30340;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#38382;&#39064;&#65292;&#36827;&#34892;&#20102;&#26368;&#22823;&#35268;&#27169;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#35268;&#27169;&#39046;&#22495;&#23545;&#40784;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#30149;&#29702;&#23398;&#20013;&#22987;&#32456;&#20248;&#20110;ImageNet&#19978;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#22871;&#39046;&#22495;&#29305;&#23450;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#26376;&#32463;&#21608;&#26399;&#30149;&#29702;&#25968;&#25454;&#38598;&#20013;&#20063;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#30149;&#29702;&#23398;&#21487;&#20197;&#25405;&#25937;&#20154;&#31867;&#29983;&#21629;&#65292;&#20294;&#26159;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#30149;&#29702;&#22270;&#20687;&#38750;&#24120;&#26114;&#36149;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#26159;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20854;&#22312;&#30149;&#29702;&#23398;&#20013;&#30340;&#24212;&#29992;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#21892;&#20854;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#19968;&#39033;&#22522;&#20110;&#21407;&#21017;&#30340;&#30740;&#31350;&#26469;&#27604;&#36739;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24182;&#35752;&#35770;&#22914;&#20309;&#36866;&#24212;&#30149;&#29702;&#23398;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#35268;&#27169;&#26368;&#22823;&#30340;&#19968;&#39033;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30149;&#29702;&#22270;&#20687;&#25968;&#25454;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;4&#31181;&#20195;&#34920;&#24615;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#23545;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#30149;&#29702;&#23398;&#20013;&#22823;&#35268;&#27169;&#39046;&#22495;&#23545;&#40784;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#26631;&#20934;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65288;&#22914;&#32447;&#24615;&#21644;&#24494;&#35843;&#35780;&#20272;&#65289;&#20197;&#21450;&#22312;&#20302;&#26631;&#31614;&#29615;&#22659;&#19979;&#22987;&#32456;&#20248;&#20110;&#22312;ImageNet&#19978;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#39046;&#22495;&#29305;&#23450;&#25216;&#26415;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#36716;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#26376;&#32463;&#21608;&#26399;&#30149;&#29702;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#65292;&#25105;&#20204;&#24050;&#32463;&#23637;&#31034;&#22823;&#35268;&#27169;&#39046;&#22495;&#23545;&#40784;&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35813;&#39046;&#22495;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational pathology can lead to saving human lives, but models are annotation hungry and pathology images are notoriously expensive to annotate. Self-supervised learning has shown to be an effective method for utilizing unlabeled data, and its application to pathology could greatly benefit its downstream tasks. Yet, there are no principled studies that compare SSL methods and discuss how to adapt them for pathology. To address this need, we execute the largest-scale study of SSL pre-training on pathology image data, to date. Our study is conducted using 4 representative SSL methods on diverse downstream tasks. We establish that large-scale domain-aligned pre-training in pathology consistently out-performs ImageNet pre-training in standard SSL settings such as linear and fine-tuning evaluations, as well as in low-label regimes. Moreover, we propose a set of domain-specific techniques that we experimentally show leads to a performance boost. Lastly, for the first time, we apply SSL t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28151;&#21512;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;LSTM&#39044;&#27979;COVID-19&#30149;&#20363;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#25968;&#25454;&#33258;&#36866;&#24212;&#24615;&#20915;&#23450;&#27169;&#22411;&#22359;&#30340;&#30456;&#23545;&#36129;&#29486;&#65292;&#22312;&#20840;&#38754;&#25968;&#20540;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.17014</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28151;&#21512;&#39044;&#27979;&#27169;&#22411;&#39044;&#27979;COVID-19&#30149;&#20363;&#25968;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;LSTM
&lt;/p&gt;
&lt;p&gt;
An Interpretable Hybrid Predictive Model of COVID-19 Cases using Autoregressive Model and LSTM. (arXiv:2211.17014v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28151;&#21512;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;LSTM&#39044;&#27979;COVID-19&#30149;&#20363;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#25968;&#25454;&#33258;&#36866;&#24212;&#24615;&#20915;&#23450;&#27169;&#22411;&#22359;&#30340;&#30456;&#23545;&#36129;&#29486;&#65292;&#22312;&#20840;&#38754;&#25968;&#20540;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2019&#20896;&#29366;&#30149;&#27602;&#30149;&#65288;COVID-19&#65289;&#23545;&#20840;&#29699;&#20581;&#24247;&#21644;&#32463;&#27982;&#20135;&#29983;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#26500;&#24314;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#20197;&#25913;&#21892;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;AR&#65289;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#38271;&#30701;&#26102;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;&#65288;LSTM&#65289;&#30340;&#39044;&#27979;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#35813;&#28151;&#21512;&#27169;&#22411;&#34987;&#27491;&#24335;&#24314;&#27169;&#20026;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#32467;&#26500;&#36830;&#25509;&#20004;&#20010;&#32452;&#25104;&#27169;&#22411;&#22359;&#65292;&#36825;&#20004;&#20010;&#22359;&#30340;&#30456;&#23545;&#36129;&#29486;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#25968;&#25454;&#33258;&#36866;&#24212;&#24615;&#20915;&#23450;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#25968;&#25454;&#28304;&#30340;&#20840;&#38754;&#25968;&#20540;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28151;&#21512;&#27169;&#22411;&#22312;&#20854;&#20004;&#20010;&#32452;&#25104;&#27169;&#22411;&#20197;&#21450;&#20854;&#20182;&#27969;&#34892;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Coronavirus Disease 2019 (COVID-19) has a profound impact on global health and economy, making it crucial to build accurate and interpretable data-driven predictive models for COVID-19 cases to improve policy making. The extremely large scale of the pandemic and the intrinsically changing transmission characteristics pose great challenges for effective COVID-19 case prediction. To address this challenge, we propose a novel hybrid model in which the interpretability of the Autoregressive model (AR) and the predictive power of the long short-term memory neural networks (LSTM) join forces. The proposed hybrid model is formalized as a neural network with an architecture that connects two composing model blocks, of which the relative contribution is decided data-adaptively in the training procedure. We demonstrate the favorable performance of the hybrid model over its two component models as well as other popular predictive models through comprehensive numerical studies on two data sour
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#26102;&#38388;&#19981;&#21464;&#30340;&#27668;&#20505;&#27169;&#24335;&#27169;&#25311;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#27668;&#20505;&#27169;&#24335;&#27169;&#25311;&#30340;&#26497;&#31471;&#24773;&#20917;&#26469;&#39044;&#27979;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.16367</link><description>&lt;p&gt;
&#27668;&#20505;&#27169;&#24335;&#38598;&#21512;&#39044;&#27979;&#20013;&#30340;&#23616;&#37096;&#26102;&#38388;&#19981;&#21464;&#37327;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
A locally time-invariant metric for climate model ensemble predictions of extreme risk. (arXiv:2211.16367v3 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16367
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#26102;&#38388;&#19981;&#21464;&#30340;&#27668;&#20505;&#27169;&#24335;&#27169;&#25311;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#27668;&#20505;&#27169;&#24335;&#27169;&#25311;&#30340;&#26497;&#31471;&#24773;&#20917;&#26469;&#39044;&#27979;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#30456;&#20851;&#39044;&#27979;&#36890;&#24120;&#26159;&#36890;&#36807;&#32467;&#21512;&#22810;&#31181;&#27668;&#20505;&#27169;&#24335;&#27169;&#25311;&#26469;&#33719;&#24471;&#30340;&#12290;&#22312;&#39640;&#24433;&#21709;&#26497;&#31471;&#20107;&#20214;&#30340;&#32972;&#26223;&#19979;&#65292;&#22522;&#20110;&#24615;&#33021;&#30340;&#38598;&#21512;&#21152;&#26435;&#26041;&#26696;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23616;&#37096;&#26102;&#38388;&#19981;&#21464;&#30340;&#27668;&#20505;&#27169;&#24335;&#27169;&#25311;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#35780;&#20272;&#27668;&#20505;&#27169;&#24335;&#27169;&#25311;&#30340;&#26497;&#31471;&#24773;&#20917;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#20869;&#32599;&#27605;&#26497;&#31471;&#39640;&#28201;&#22825;&#27668;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#20843;&#20010;&#39069;&#22806;&#22478;&#24066;&#30340;&#27604;&#36739;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptation-relevant predictions of climate change are often derived by combining climate model simulations in a multi-model ensemble. Model evaluation methods used in performance-based ensemble weighting schemes have limitations in the context of high-impact extreme events. We introduce a locally time-invariant method for evaluating climate model simulations with a focus on assessing the simulation of extremes. We explore the behaviour of the proposed method in predicting extreme heat days in Nairobi and provide comparative results for eight additional cities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;Q-learning&#31639;&#27861;&#22312;&#22810;&#20219;&#21153;Atari&#28216;&#25103;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#21512;&#36866;&#30340;&#36873;&#25321;&#33021;&#22815;&#25552;&#39640;&#25193;&#23637;&#24615;&#21644;&#25512;&#24191;&#24615;&#65292;&#20351;&#27169;&#22411;&#24615;&#33021;&#19982;&#23481;&#37327;&#27491;&#30456;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#25968;&#25454;&#38598;&#24615;&#33021;&#20043;&#22806;&#12290;</title><link>http://arxiv.org/abs/2211.15144</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#22810;&#26679;&#24615;&#22810;&#20219;&#21153;&#25968;&#25454;&#19978;&#36827;&#34892;&#65292;&#20855;&#26377;&#25193;&#23637;&#24615;&#21644;&#25512;&#24191;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Q-Learning on Diverse Multi-Task Data Both Scales And Generalizes. (arXiv:2211.15144v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;Q-learning&#31639;&#27861;&#22312;&#22810;&#20219;&#21153;Atari&#28216;&#25103;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#21512;&#36866;&#30340;&#36873;&#25321;&#33021;&#22815;&#25552;&#39640;&#25193;&#23637;&#24615;&#21644;&#25512;&#24191;&#24615;&#65292;&#20351;&#27169;&#22411;&#24615;&#33021;&#19982;&#23481;&#37327;&#27491;&#30456;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#25968;&#25454;&#38598;&#24615;&#33021;&#20043;&#22806;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#24456;&#39640;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#39640;&#23481;&#37327;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#24191;&#20041;&#26234;&#33021;&#20307;&#65292;&#31867;&#20284;&#20110;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#31867;&#20284;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#25193;&#23637;&#27169;&#22411;&#23481;&#37327;&#26041;&#38754;&#36935;&#21040;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#36825;&#20123;&#24037;&#20316;&#24471;&#20986;&#30340;&#32467;&#35770;&#37325;&#26032;&#23457;&#35270;&#20197;&#21069;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#36866;&#24403;&#30340;&#36873;&#25321;&#65292;&#31163;&#32447;Q-learning&#31639;&#27861;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#21644;&#23481;&#37327;&#25193;&#23637;&#12290;&#25105;&#20204;&#20197;&#22810;&#20219;&#21153;Atari&#20316;&#20026;&#25193;&#23637;&#21644;&#25512;&#24191;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;&#39640;&#36798;8000&#19975;&#20010;&#21442;&#25968;&#32593;&#32476;&#22312;40&#20010;&#28216;&#25103;&#19978;&#35757;&#32451;&#21333;&#20010;&#31574;&#30053;&#65292;&#23454;&#29616;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#27169;&#22411;&#24615;&#33021;&#19982;&#23481;&#37327;&#30340;&#20851;&#31995;&#21576;&#27491;&#27604;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#19981;&#21516;&#65292;&#21363;&#20351;&#23436;&#20840;&#22312;&#22823;&#22411;&#65288;400M&#36807;&#28193;&#65289;&#20294;&#39640;&#24230;&#38590;&#20197;&#25277;&#35937;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#25512;&#24191;&#21040;&#25968;&#25454;&#38598;&#24615;&#33021;&#20043;&#22806;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of offline reinforcement learning (RL) is that high-capacity models trained on large, heterogeneous datasets can lead to agents that generalize broadly, analogously to similar advances in vision and NLP. However, recent works argue that offline RL methods encounter unique challenges to scaling up model capacity. Drawing on the learnings from these works, we re-examine previous design choices and find that with appropriate choices: ResNets, cross-entropy based distributional backups, and feature normalization, offline Q-learning algorithms exhibit strong performance that scales with model capacity. Using multi-task Atari as a testbed for scaling and generalization, we train a single policy on 40 games with near-human performance using up-to 80 million parameter networks, finding that model performance scales favorably with capacity. In contrast to prior work, we extrapolate beyond dataset performance even when trained entirely on a large (400M transitions) but highly subop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22788;&#29702;&#24402;&#19968;&#21270;&#27969;&#25299;&#25169;&#38382;&#39064;&#12289;&#23558;&#36793;&#30028;&#26465;&#20214;&#24212;&#29992;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#25216;&#26415;&#12289;&#24341;&#20837;&#20102;&#21487;&#20197;&#34987;&#21046;&#20316;&#25104;&#20219;&#24847;&#27425;&#21487;&#24494;&#30340; I-Spline &#21452;&#23556;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#26415;&#29992;&#20110;&#21019;&#24314;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#36153;&#31859;&#27874;&#20989;&#25968; Ansatz&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2211.14839</link><description>&lt;p&gt;
Waveflow&#65306;&#23558;&#36793;&#30028;&#26465;&#20214;&#24212;&#29992;&#20110;&#24179;&#28369;&#24402;&#19968;&#21270;&#27969;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#20197;&#36153;&#31859;&#27874;&#20989;&#25968;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Waveflow: Enforcing boundary conditions in smooth normalizing flows with application to fermionic wave functions. (arXiv:2211.14839v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22788;&#29702;&#24402;&#19968;&#21270;&#27969;&#25299;&#25169;&#38382;&#39064;&#12289;&#23558;&#36793;&#30028;&#26465;&#20214;&#24212;&#29992;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#25216;&#26415;&#12289;&#24341;&#20837;&#20102;&#21487;&#20197;&#34987;&#21046;&#20316;&#25104;&#20219;&#24847;&#27425;&#21487;&#24494;&#30340; I-Spline &#21452;&#23556;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#26415;&#29992;&#20110;&#21019;&#24314;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#36153;&#31859;&#27874;&#20989;&#25968; Ansatz&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#20010;&#20027;&#35201;&#30340;&#21019;&#26032;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#24402;&#19968;&#21270;&#27969;&#25299;&#25169;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#24402;&#19968;&#21270;&#27969;&#19978;&#24378;&#21046;&#26045;&#21152;&#29305;&#23450;&#31867;&#21035;&#36793;&#30028;&#26465;&#20214;&#30340;&#25216;&#26415;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; I-Spline &#21452;&#23556;&#65292;&#23427;&#20687;&#20043;&#21069;&#30340;&#24037;&#20316;&#19968;&#26679;&#21033;&#29992;&#20102;&#26679;&#26465;&#26354;&#32447;&#65292;&#20294;&#19982;&#36825;&#20123;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#21487;&#20197;&#34987;&#21046;&#20316;&#25104;&#20219;&#24847;&#27425;&#21487;&#24494;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#21019;&#24314;&#20102; Waveflow&#65292;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#19968;&#32500;&#22810;&#31890;&#23376;&#36153;&#31859;&#27874;&#20989;&#25968;&#30340; Ansatz&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21464;&#20998;&#37327;&#23376;&#33945;&#29305;&#21345;&#32599;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#26080;&#38656; MCMC &#25110;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#12290;&#20026;&#20102;&#24378;&#21046;&#36153;&#31859;&#27874;&#20989;&#25968;&#25152;&#38656;&#30340;&#21453;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#20165;&#22312;&#25490;&#21015;&#32676;&#30340;&#22522;&#26412;&#22495;&#19978;&#35757;&#32451;&#24402;&#19968;&#21270;&#27969;&#65292;&#36825;&#26377;&#25928;&#22320;&#23558;&#20854;&#20943;&#23569;&#20026;&#19968;&#20010;&#36793;&#30028;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce four main novelties: First, we present a new way of handling the topology problem of normalizing flows. Second, we describe a technique to enforce certain classes of boundary conditions onto normalizing flows. Third, we introduce the I-Spline bijection, which, similar to previous work, leverages splines but, in contrast to those works, can be made arbitrarily often differentiable. And finally, we use these techniques to create Waveflow, an Ansatz for the one-space-dimensional multi-particle fermionic wave functions in real space based on normalizing flows, that can be efficiently trained with Variational Quantum Monte Carlo without the need for MCMC nor estimation of a normalization constant. To enforce the necessary anti-symmetry of fermionic wave functions, we train the normalizing flow only on the fundamental domain of the permutation group, which effectively reduces it to a boundary value problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;PFG&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;RKHS&#33539;&#25968;&#30340;&#20989;&#25968;&#27491;&#21017;&#39033;&#23454;&#29616;&#26356;&#22823;&#30340;&#20989;&#25968;&#31867;&#21644;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#65292;&#35299;&#20915;&#20102;RKHS&#35201;&#27714;&#38480;&#21046;&#20989;&#25968;&#31867;&#21644;&#31639;&#27861;&#28789;&#27963;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;KL&#25955;&#24230;&#19978;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#36830;&#32493;&#26102;&#38388;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2211.13954</link><description>&lt;p&gt;
&#22522;&#20110;&#31890;&#23376;&#30340;&#39044;&#22788;&#29702;&#20989;&#25968;&#26799;&#24230;&#27969;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Particle-based Variational Inference with Preconditioned Functional Gradient Flow. (arXiv:2211.13954v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;PFG&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;RKHS&#33539;&#25968;&#30340;&#20989;&#25968;&#27491;&#21017;&#39033;&#23454;&#29616;&#26356;&#22823;&#30340;&#20989;&#25968;&#31867;&#21644;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#65292;&#35299;&#20915;&#20102;RKHS&#35201;&#27714;&#38480;&#21046;&#20989;&#25968;&#31867;&#21644;&#31639;&#27861;&#28789;&#27963;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;KL&#25955;&#24230;&#19978;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#36830;&#32493;&#26102;&#38388;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31890;&#23376;&#30340;&#21464;&#20998;&#25512;&#26029;&#36890;&#36807;&#26799;&#24230;&#27969;&#20272;&#35745;&#26368;&#23567;&#21270;&#27169;&#22411;&#26679;&#26412;&#19982;&#30446;&#26631;&#21518;&#39564;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#38543;&#30528;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#30340;&#27969;&#34892;&#65292;&#22522;&#20110;&#31890;&#23376;&#30340;VI&#31639;&#27861;&#30340;&#37325;&#28857;&#24050;&#32463;&#36716;&#21521;&#22312;&#37325;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#36924;&#36817;&#26799;&#24230;&#27969;&#30340;&#20989;&#25968;&#30340;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;RKHS&#30340;&#35201;&#27714;&#38480;&#21046;&#20102;&#20989;&#25968;&#31867;&#21644;&#31639;&#27861;&#30340;&#28789;&#27963;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;&#20102;RKHS&#33539;&#25968;&#30340;&#20989;&#25968;&#27491;&#21017;&#21270;&#39033;&#65292;&#25552;&#20379;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;VI&#31639;&#27861;&#65292;&#21483;&#20570;&#39044;&#22788;&#29702;&#20989;&#25968;&#26799;&#24230;&#27969;&#65288;PFG&#65289;&#12290;&#19982;SVGD&#30456;&#27604;&#65292;PFG&#20855;&#26377;&#26356;&#22823;&#30340;&#20989;&#25968;&#31867;&#65292;&#25913;&#36827;&#20102;&#22823;&#37327;&#31890;&#23376;&#22330;&#26223;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#26356;&#36866;&#24212;&#30149;&#24577;&#20998;&#24067;&#65292;&#24182;&#22312;KL&#25955;&#24230;&#19978;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#36830;&#32493;&#26102;&#38388;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#38750;&#32447;&#24615;&#20989;&#25968;&#35268;&#33539;&#20063;&#21487;&#20197;&#36731;&#26494;&#22320;&#24182;&#20837;&#21040;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Particle-based variational inference (VI) minimizes the KL divergence between model samples and the target posterior with gradient flow estimates. With the popularity of Stein variational gradient descent (SVGD), the focus of particle-based VI algorithms has been on the properties of functions in Reproducing Kernel Hilbert Space (RKHS) to approximate the gradient flow. However, the requirement of RKHS restricts the function class and algorithmic flexibility. This paper offers a general solution to this problem by introducing a functional regularization term that encompasses the RKHS norm as a special case. This allows us to propose a new particle-based VI algorithm called preconditioned functional gradient flow (PFG). Compared to SVGD, PFG has several advantages. It has a larger function class, improved scalability in large particle-size scenarios, better adaptation to ill-conditioned distributions, and provable continuous-time convergence in KL divergence. Additionally, non-linear fun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;&#65288;IFU&#65289;&#30340;&#26032;&#39062;&#32852;&#37030;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#23454;&#29616;&#26377;&#25928;&#19988;&#21487;&#37327;&#21270;&#30340;&#23458;&#25143;&#31471;&#28040;&#38500;&#35831;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#29575;&#36739;&#22522;&#26412;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;FU&#26041;&#27861;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2211.11656</link><description>&lt;p&gt;
&#39034;&#24207;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;&#65306;&#32852;&#37030;&#20248;&#21270;&#20013;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#30340;&#23458;&#25143;&#31471;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization. (arXiv:2211.11656v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;&#65288;IFU&#65289;&#30340;&#26032;&#39062;&#32852;&#37030;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#23454;&#29616;&#26377;&#25928;&#19988;&#21487;&#37327;&#21270;&#30340;&#23458;&#25143;&#31471;&#28040;&#38500;&#35831;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#25928;&#29575;&#36739;&#22522;&#26412;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;FU&#26041;&#27861;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#28040;&#38500;&#65288;MU&#65289;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;&#20174;&#35757;&#32451;&#36807;&#31243;&#20013;&#21024;&#38500;&#32473;&#23450;&#25968;&#25454;&#28857;&#30340;&#36129;&#29486;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#32852;&#37030;&#28040;&#38500;&#65288;FU&#65289;&#26159;&#23558;MU&#25193;&#23637;&#21040;&#20174;&#32852;&#21512;&#35757;&#32451;&#36807;&#31243;&#20013;&#28040;&#38500;&#32473;&#23450;&#23458;&#25143;&#31471;&#30340;&#36129;&#29486;&#12290;&#24403;&#21069;&#30340;FU&#26041;&#27861;&#36890;&#24120;&#19981;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#27809;&#26377;&#23545;&#28040;&#38500;&#25928;&#26524;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#21512;&#29702;&#30340;&#29702;&#35770;&#37327;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30693;&#24773;&#32852;&#21512;&#28040;&#38500;(IFU)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#19988;&#21487;&#37327;&#21270;&#30340;FU&#26041;&#27861;&#12290;&#22312;&#25509;&#25910;&#21040;&#32473;&#23450;&#23458;&#25143;&#31471;&#30340;&#28040;&#38500;&#35831;&#27714;&#21518;&#65292;IFU&#36890;&#36807;&#38543;&#26426;&#25200;&#21160;&#26426;&#21046;&#30830;&#23450;&#20102;&#37325;&#26032;&#21021;&#22987;&#21270;FL&#25152;&#38656;&#30340;&#26368;&#20339;FL&#36845;&#20195;&#65292;&#21487;&#20197;&#33719;&#24471;&#28040;&#38500;&#20445;&#35777;&#12290;IFU&#30340;&#29702;&#35770;&#20063;&#21487;&#20197;&#25193;&#23637;&#20197;&#35299;&#20915;&#39034;&#24207;&#28040;&#38500;&#35831;&#27714;&#12290;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#26412;&#37325;&#26032;&#35757;&#32451;&#21644;&#26368;&#20808;&#36827;&#30340;FU&#26041;&#27861;&#30456;&#27604;&#65292;IFU&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#28040;&#38500;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of Machine Unlearning (MU) is to provide theoretical guarantees on the removal of the contribution of a given data point from a training procedure. Federated Unlearning (FU) consists in extending MU to unlearn a given client's contribution from a federated training routine. Current FU approaches are generally not scalable, and do not come with sound theoretical quantification of the effectiveness of unlearning. In this work we present Informed Federated Unlearning (IFU), a novel efficient and quantifiable FU approach. Upon unlearning request from a given client, IFU identifies the optimal FL iteration from which FL has to be reinitialized, with unlearning guarantees obtained through a randomized perturbation mechanism. The theory of IFU is also extended to account for sequential unlearning requests. Experimental results on different tasks and dataset show that IFU leads to more efficient unlearning procedures as compared to basic re-training and state-of-the-art FU approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;(pFedMB)&#65292;&#36890;&#36807;&#22810;&#20998;&#25903;&#32467;&#26500;&#23454;&#29616;&#20010;&#24615;&#21270;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#32858;&#21512;&#26041;&#27861;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#21644;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#22343;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#23458;&#25143;&#31471;&#20855;&#26377;&#26469;&#33258;&#22797;&#26434;&#20998;&#24067;&#30340;&#25968;&#25454;&#19988;&#19981;&#33021;&#30830;&#23450;&#24444;&#27492;&#30340;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20419;&#36827;&#25317;&#26377;&#30456;&#20284;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#36827;&#34892;&#26356;&#22810;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2211.07931</link><description>&lt;p&gt;
&#22810;&#20998;&#25903;&#32467;&#26500;&#19979;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Personalized Federated Learning with Multi-branch Architecture. (arXiv:2211.07931v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;(pFedMB)&#65292;&#36890;&#36807;&#22810;&#20998;&#25903;&#32467;&#26500;&#23454;&#29616;&#20010;&#24615;&#21270;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#32858;&#21512;&#26041;&#27861;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#21644;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#22343;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#23458;&#25143;&#31471;&#20855;&#26377;&#26469;&#33258;&#22797;&#26434;&#20998;&#24067;&#30340;&#25968;&#25454;&#19988;&#19981;&#33021;&#30830;&#23450;&#24444;&#27492;&#30340;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20419;&#36827;&#25317;&#26377;&#30456;&#20284;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#36827;&#34892;&#26356;&#22810;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20351;&#24471;&#22810;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#23458;&#25143;&#31471;&#30456;&#20114;&#25581;&#31034;&#20854;&#21407;&#22987;&#25968;&#25454;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;FL&#35757;&#32451;&#21333;&#19968;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#34920;&#29616;&#22312;&#23458;&#25143;&#31471;&#30340;&#24179;&#22343;&#24615;&#33021;&#65292;&#20294;&#26159;&#36328;&#23458;&#25143;&#31471;&#30340;&#32479;&#35745;&#25968;&#25454;&#24322;&#36136;&#24615;&#20419;&#20351;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#30340;&#21457;&#23637;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#20010;&#24615;&#21270;&#30340;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;PFL&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#20855;&#26377;&#26469;&#33258;&#22797;&#26434;&#20998;&#24067;&#30340;&#25968;&#25454;&#19988;&#19981;&#33021;&#30830;&#23450;&#24444;&#27492;&#30340;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#20419;&#36827;&#25317;&#26377;&#30456;&#20284;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#36827;&#34892;&#26356;&#22810;&#21327;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20998;&#25903;&#32467;&#26500;&#30340;&#26032;&#22411;PFL&#26041;&#27861;(pFedMB)&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#27599;&#20010;&#23618;&#20998;&#25104;&#22810;&#20010;&#20998;&#25903;&#24182;&#20026;&#27599;&#20010;&#20998;&#25903;&#20998;&#37197;&#23458;&#25143;&#31471;&#29305;&#23450;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#20010;&#24615;&#21270;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;pFedMB&#30340;&#36890;&#20449;&#25928;&#29575;&#24182;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;pFedMB&#22312;&#20010;&#24615;&#21270;&#21644;&#36890;&#20449;&#25928;&#29575;&#26041;&#38754;&#37117;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a decentralized machine learning technique that enables multiple clients to collaboratively train models without requiring clients to reveal their raw data to each other. Although traditional FL trains a single global model with average performance among clients, statistical data heterogeneity across clients has resulted in the development of personalized FL (PFL), which trains personalized models with good performance on each client's data. A key challenge with PFL is how to facilitate clients with similar data to collaborate more in a situation where each client has data from complex distribution and cannot determine one another's distribution. In this paper, we propose a new PFL method (pFedMB) using multi-branch architecture, which achieves personalization by splitting each layer of a neural network into multiple branches and assigning client-specific weights to each branch. We also design an aggregation method to improve the communication efficiency and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#32422;&#31616;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;SINDy&#26041;&#27861;&#26500;&#24314;&#20302;&#32500;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#21160;&#24577;&#29305;&#24615;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#39044;&#27979;&#21442;&#25968;&#21464;&#21270;&#26102;&#30340;&#20840;&#26102;&#38388;&#35299;&#12290;</title><link>http://arxiv.org/abs/2211.06786</link><description>&lt;p&gt;
&#22522;&#20110;Autoencoder&#21644;SINDy&#26041;&#27861;&#30340;&#21442;&#25968;&#21270;&#31995;&#32479;&#32422;&#31616;&#24314;&#27169;&#65306;&#21608;&#26399;&#35299;&#30340;&#24310;&#32493;
&lt;/p&gt;
&lt;p&gt;
Reduced order modeling of parametrized systems through autoencoders and SINDy approach: continuation of periodic solutions. (arXiv:2211.06786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#32422;&#31616;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;SINDy&#26041;&#27861;&#26500;&#24314;&#20302;&#32500;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#21160;&#24577;&#29305;&#24615;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#39044;&#27979;&#21442;&#25968;&#21464;&#21270;&#26102;&#30340;&#20840;&#26102;&#38388;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#25511;&#21046;&#30340;&#22797;&#26434;&#29616;&#35937;&#36827;&#34892;&#39640;&#31934;&#24230;&#27169;&#25311;&#36890;&#24120;&#38656;&#35201;&#31934;&#32454;&#24178;&#28041;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#33021;&#38754;&#20020;&#30528;&#26114;&#36149;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24403;&#29992;&#20110;&#22810;&#31181;&#25511;&#21046;&#21442;&#25968;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;PDE&#31283;&#24577;&#35299;&#30340;&#36817;&#20284;&#26102;&#65292;&#36825;&#31181;&#24773;&#20917;&#21487;&#33021;&#20250;&#21464;&#24471;&#31105;&#27490;&#12290;&#22240;&#27492;&#65292;&#26500;&#24314;&#39640;&#25928;&#30340;&#32422;&#31616;&#27169;&#22411;&#65288;ROM&#65289;&#65292;&#20351;&#20854;&#22312;&#21442;&#25968;&#21464;&#21270;&#26102;&#33021;&#22815;&#20934;&#30830;&#32780;&#24555;&#36895;&#22320;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#29289;&#29702;&#29616;&#35937;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#26694;&#26550;&#65292;&#23558;ROM&#26500;&#24314;&#19982;&#38477;&#32500;&#21160;&#21147;&#23398;&#35782;&#21035;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23569;&#37327;&#20840;&#24207;&#35299;&#65292;&#21033;&#29992;&#21442;&#25968;&#31232;&#30095;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65288;SINDy&#65289;&#21644;&#33258;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#20302;&#32500;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#29992;&#20110;&#39640;&#25928;&#35745;&#31639;&#26032;&#30340;&#20840;&#26102;&#38388;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Highly accurate simulations of complex phenomena governed by partial differential equations (PDEs) typically require intrusive methods and entail expensive computational costs, which might become prohibitive when approximating steady-state solutions of PDEs for multiple combinations of control parameters and initial conditions. Therefore, constructing efficient reduced order models (ROMs) that enable accurate but fast predictions, while retaining the dynamical characteristics of the physical phenomenon as parameters vary, is of paramount importance. In this work, a data-driven, non-intrusive framework which combines ROM construction with reduced dynamics identification, is presented. Starting from a limited amount of full order solutions, the proposed approach leverages autoencoder neural networks with parametric sparse identification of nonlinear dynamics (SINDy) to construct a low-dimensional dynamical model. This model can be queried to efficiently compute full-time solutions at new
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20027;&#21160;&#20219;&#21153;&#38543;&#26426;&#21270;&#65288;ATR&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#29983;&#25104;&#20219;&#21153;&#26469;&#23398;&#20064;&#40065;&#26834;&#25216;&#33021;&#65292;&#35813;&#26041;&#27861;&#36873;&#25321;&#36866;&#21512;&#23398;&#20064;&#40065;&#26834;&#25216;&#33021;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#24179;&#34913;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#26469;&#39044;&#27979;&#20219;&#21153;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#21442;&#25968;&#21270;&#31243;&#24207;&#29983;&#25104;&#20219;&#21153;&#65292;&#20174;&#32780;&#20801;&#35768;&#40065;&#26834;&#22320;&#22788;&#29702;&#20219;&#21153;&#30340;&#21508;&#31181;&#24773;&#20917;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2211.06134</link><description>&lt;p&gt;
&#20027;&#21160;&#20219;&#21153;&#38543;&#26426;&#21270;&#65306;&#36890;&#36807;&#26080;&#30417;&#30563;&#29983;&#25104;&#22810;&#26679;&#21644;&#21487;&#34892;&#20219;&#21153;&#23398;&#20064;&#40065;&#26834;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Task Randomization: Learning Robust Skills via Unsupervised Generation of Diverse and Feasible Tasks. (arXiv:2211.06134v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20027;&#21160;&#20219;&#21153;&#38543;&#26426;&#21270;&#65288;ATR&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#29983;&#25104;&#20219;&#21153;&#26469;&#23398;&#20064;&#40065;&#26834;&#25216;&#33021;&#65292;&#35813;&#26041;&#27861;&#36873;&#25321;&#36866;&#21512;&#23398;&#20064;&#40065;&#26834;&#25216;&#33021;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#24179;&#34913;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#26469;&#39044;&#27979;&#20219;&#21153;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#21442;&#25968;&#21270;&#31243;&#24207;&#29983;&#25104;&#20219;&#21153;&#65292;&#20174;&#32780;&#20801;&#35768;&#40065;&#26834;&#22320;&#22788;&#29702;&#20219;&#21153;&#30340;&#21508;&#31181;&#24773;&#20917;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25805;&#32437;&#20219;&#21153;&#38656;&#35201;&#26426;&#22120;&#20154;&#20855;&#22791;&#36866;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;&#30340;&#25216;&#33021;&#24211;&#12290;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#33719;&#24471;&#36825;&#31181;&#25216;&#33021;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#33719;&#21462;&#35206;&#30422;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#38750;&#24120;&#22797;&#26434;&#30340;&#25163;&#21160;&#21171;&#21160;&#21644;&#39046;&#22495;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20027;&#21160;&#20219;&#21153;&#38543;&#26426;&#21270;&#65288;ATR&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#29983;&#25104;&#35757;&#32451;&#20219;&#21153;&#26469;&#23398;&#20064;&#40065;&#26834;&#30340;&#25216;&#33021;&#12290; ATR&#36890;&#36807;&#24179;&#34913;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#36873;&#25321;&#36866;&#21512;&#23398;&#20064;&#40065;&#26834;&#25216;&#33021;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#21253;&#25324;&#21021;&#22987;&#29615;&#22659;&#29366;&#24577;&#21644;&#25805;&#20316;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20849;&#21516;&#23398;&#20064;&#32039;&#20945;&#20219;&#21153;&#34920;&#31034;&#26469;&#39044;&#27979;&#20219;&#21153;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#21442;&#25968;&#21270;&#22312;&#27169;&#25311;&#20013;&#31243;&#24207;&#29983;&#25104;&#25152;&#36873;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#35757;&#32451;&#20219;&#21153;&#30340;&#20027;&#21160;&#36873;&#25321;&#20351;&#24471;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#35757;&#32451;&#30340;&#25216;&#33021;&#31574;&#30053;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#25110;&#39046;&#22495;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#40065;&#26834;&#22320;&#22788;&#29702;&#20219;&#21153;&#30340;&#21508;&#31181;&#24773;&#20917;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving real-world manipulation tasks requires robots to have a repertoire of skills applicable to a wide range of circumstances. When using learning-based methods to acquire such skills, the key challenge is to obtain training data that covers diverse and feasible variations of the task, which often requires non-trivial manual labor and domain knowledge. In this work, we introduce Active Task Randomization (ATR), an approach that learns robust skills through the unsupervised generation of training tasks. ATR selects suitable tasks, which consist of an initial environment state and manipulation goal, for learning robust skills by balancing the diversity and feasibility of the tasks. We propose to predict task diversity and feasibility by jointly learning a compact task representation. The selected tasks are then procedurally generated in simulation using graph-based parameterization. The active selection of these training tasks enables skill policies trained with our framework to robus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#23545;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23384;&#22312;&#36127;&#38754;&#24433;&#21709;&#65292;&#38480;&#21046;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#25552;&#20986;&#20102;FedTP&#65292;&#22312;&#23398;&#20064;&#23458;&#25143;&#31471;&#20010;&#24615;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#21516;&#26102;&#65292;&#23558;&#20854;&#20182;&#21442;&#25968;&#32858;&#21512;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#12290;</title><link>http://arxiv.org/abs/2211.01572</link><description>&lt;p&gt;
FedTP: &#32852;&#37030;&#23398;&#20064;&#20013;&#30340;Transformer&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
FedTP: Federated Learning by Transformer Personalization. (arXiv:2211.01572v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#23545;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23384;&#22312;&#36127;&#38754;&#24433;&#21709;&#65292;&#38480;&#21046;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#25552;&#20986;&#20102;FedTP&#65292;&#22312;&#23398;&#20064;&#23458;&#25143;&#31471;&#20010;&#24615;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#21516;&#26102;&#65292;&#23558;&#20854;&#20182;&#21442;&#25968;&#32858;&#21512;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#21327;&#20316;&#22320;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12290;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#20811;&#26381;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#26377;&#19968;&#20123;&#23581;&#35797;&#23558;Transformer&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#23545;&#33258;&#27880;&#24847;&#21147;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#20851;&#31995;&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#23384;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#26102;&#65292;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#23454;&#38469;&#19978;&#20250;&#23545;&#33258;&#27880;&#24847;&#21147;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36825;&#20123;&#24433;&#21709;&#38480;&#21046;&#20102;Transformer&#27169;&#22411;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedTP&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#23558;&#20854;&#20182;&#21442;&#25968;&#32858;&#21512;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#32780;&#19981;&#26159;&#20351;&#29992;&#32431;&#20010;&#24615;&#21270;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an emerging learning paradigm where multiple clients collaboratively train a machine learning model in a privacy-preserving manner. Personalized federated learning extends this paradigm to overcome heterogeneity across clients by learning personalized models. Recently, there have been some initial attempts to apply Transformers to federated learning. However, the impacts of federated learning algorithms on self-attention have not yet been studied. This paper investigates this relationship and reveals that federated averaging algorithms actually have a negative impact on self-attention where there is data heterogeneity. These impacts limit the capabilities of the Transformer model in federated learning settings. Based on this, we propose FedTP, a novel Transformer-based federated learning framework that learns personalized self-attention for each client while aggregating the other parameters among the clients. Instead of using a vanilla personalization mechanism th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#22522;&#20110;&#20223;&#30495;&#25512;&#26029;&#30340;&#21512;&#25104;&#20284;&#28982;&#26041;&#27861;&#65292;&#20351;&#29992;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#29983;&#25104;&#27169;&#25311;&#25968;&#25454;&#65292;&#23398;&#20064;&#26465;&#20214;&#33021;&#37327;&#27169;&#22411;(EBM)&#30340; likelihood&#65292;&#32467;&#21512;&#20808;&#39564;&#20272;&#35745;&#21518;&#39564;&#20998;&#24067;&#65292;&#21487;&#20197;&#20351;&#29992;MCMC&#25277;&#21462;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#26356;&#21152;&#28789;&#27963;&#21644;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2210.14756</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#20223;&#30495;&#25512;&#26029;&#30340;&#38750;&#24402;&#19968;&#21270;&#27169;&#22411;&#30340;&#26368;&#22823;&#20284;&#28982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Likelihood Learning of Unnormalized Models for Simulation-Based Inference. (arXiv:2210.14756v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#22522;&#20110;&#20223;&#30495;&#25512;&#26029;&#30340;&#21512;&#25104;&#20284;&#28982;&#26041;&#27861;&#65292;&#20351;&#29992;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#29983;&#25104;&#27169;&#25311;&#25968;&#25454;&#65292;&#23398;&#20064;&#26465;&#20214;&#33021;&#37327;&#27169;&#22411;(EBM)&#30340; likelihood&#65292;&#32467;&#21512;&#20808;&#39564;&#20272;&#35745;&#21518;&#39564;&#20998;&#24067;&#65292;&#21487;&#20197;&#20351;&#29992;MCMC&#25277;&#21462;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#26356;&#21152;&#28789;&#27963;&#21644;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#29992;&#20110;&#22522;&#20110;&#20223;&#30495;&#25512;&#26029;&#65288;SBI&#65289;&#30340;&#21512;&#25104;&#20284;&#28982;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#23384;&#22312;&#26102;&#20174;&#23454;&#39564;&#35266;&#27979;&#20013;&#36827;&#34892;&#20998;&#25674;&#25110;&#26377;&#38024;&#23545;&#24615;&#30340;&#25512;&#26029;&#12290;&#20004;&#31181;&#26041;&#27861;&#22343;&#20351;&#29992;&#20174;&#25552;&#35758;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#21442;&#25968;&#25152;&#29983;&#25104;&#30340;&#27169;&#25311;&#25968;&#25454;&#26469;&#23398;&#20064; likelihood &#30340;&#26465;&#20214;&#33021;&#37327;&#27169;&#22411;(EBM)&#12290;&#28982;&#21518;&#21487;&#20197;&#23558;&#23398;&#20064;&#21040;&#30340; likelihood &#19982;&#20219;&#20309;&#20808;&#39564;&#32452;&#21512;&#20197;&#33719;&#24471;&#21518;&#39564;&#20272;&#35745;&#65292;&#38543;&#21518;&#21487;&#20197;&#20351;&#29992; MCMC &#20174;&#20013;&#25277;&#21462;&#26679;&#26412;&#12290;&#19982;&#20854;&#20182;&#21512;&#25104;&#20284;&#28982;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;&#28789;&#27963;&#30340;&#33021;&#37327;&#27169;&#22411;&#21644; KL &#25439;&#22833;&#30340;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce two synthetic likelihood methods for Simulation-Based Inference (SBI), to conduct either amortized or targeted inference from experimental observations when a high-fidelity simulator is available. Both methods learn a conditional energy-based model (EBM) of the likelihood using synthetic data generated by the simulator, conditioned on parameters drawn from a proposal distribution. The learned likelihood can then be combined with any prior to obtain a posterior estimate, from which samples can be drawn using MCMC. Our methods uniquely combine a flexible Energy-Based Model and the minimization of a KL loss: this is in contrast to other synthetic likelihood methods, which either rely on normalizing flows, or minimize score-based objectives; choices that come with known pitfalls. We demonstrate the properties of both methods on a range of synthetic datasets, and apply them to a neuroscience model of the pyloric network in the crab, where our method outperforms prior art for a 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#22238;&#39038;&#20102;&#32593;&#32476;&#20449;&#21495;&#21644;&#20449;&#24687;&#22788;&#29702;&#26041;&#38754;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#36825;&#31181;&#36827;&#23637;&#20351;&#24471;&#20915;&#31574;&#12289;&#20248;&#21270;&#12289;&#25511;&#21046;&#21644;&#23398;&#20064;&#31561;&#26041;&#38754;&#33021;&#22815;&#25193;&#23637;&#21040;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#36234;&#26469;&#36234;&#26222;&#36941;&#30340;&#29615;&#22659;&#20013;&#65292;&#32780;&#19988;&#36890;&#36807;&#21512;&#20316;&#21644;&#20849;&#20139;&#65292;&#32593;&#32476;&#26234;&#33021;&#20307;&#33021;&#22815;&#21305;&#37197;&#20113;&#25110;&#32852;&#21512;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.13767</link><description>&lt;p&gt;
&#32593;&#32476;&#20449;&#21495;&#21644;&#20449;&#24687;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Networked Signal and Information Processing. (arXiv:2210.13767v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13767
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#22238;&#39038;&#20102;&#32593;&#32476;&#20449;&#21495;&#21644;&#20449;&#24687;&#22788;&#29702;&#26041;&#38754;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#36825;&#31181;&#36827;&#23637;&#20351;&#24471;&#20915;&#31574;&#12289;&#20248;&#21270;&#12289;&#25511;&#21046;&#21644;&#23398;&#20064;&#31561;&#26041;&#38754;&#33021;&#22815;&#25193;&#23637;&#21040;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#36234;&#26469;&#36234;&#26222;&#36941;&#30340;&#29615;&#22659;&#20013;&#65292;&#32780;&#19988;&#36890;&#36807;&#21512;&#20316;&#21644;&#20849;&#20139;&#65292;&#32593;&#32476;&#26234;&#33021;&#20307;&#33021;&#22815;&#21305;&#37197;&#20113;&#25110;&#32852;&#21512;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#22238;&#39038;&#20102;&#32593;&#32476;&#20449;&#21495;&#21644;&#20449;&#24687;&#22788;&#29702;&#20013;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#22312;&#36807;&#21435;25&#24180;&#20013;&#65292;&#36825;&#20123;&#36827;&#23637;&#20351;&#24471;&#20915;&#31574;&#21046;&#23450;&#12289;&#25512;&#29702;&#12289;&#20248;&#21270;&#12289;&#25511;&#21046;&#21644;&#23398;&#20064;&#33021;&#22815;&#25193;&#23637;&#21040;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#36234;&#26469;&#36234;&#26222;&#36941;&#30340;&#29615;&#22659;&#20013;&#12290;&#24403;&#36825;&#20123;&#26234;&#33021;&#20307;&#30456;&#20114;&#21512;&#20316;&#26102;&#65292;&#26032;&#30340;&#38598;&#20307;&#34892;&#20026;&#20174;&#23616;&#37096;&#20915;&#31574;&#21644;&#34892;&#21160;&#20013;&#20986;&#29616;&#12290;&#27492;&#22806;&#65292;&#29702;&#35770;&#21644;&#24212;&#29992;&#34920;&#26126;&#65292;&#36890;&#36807;&#21512;&#20316;&#21644;&#20849;&#20139;&#65292;&#32593;&#32476;&#26234;&#33021;&#20307;&#33021;&#22815;&#21305;&#37197;&#20113;&#25110;&#32852;&#21512;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#25552;&#39640;&#38544;&#31169;&#12289;&#22686;&#24378;&#38887;&#24615;&#21644;&#33410;&#32422;&#36164;&#28304;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The article reviews significant advances in networked signal and information processing, which have enabled in the last 25 years extending decision making and inference, optimization, control, and learning to the increasingly ubiquitous environments of distributed agents. As these interacting agents cooperate, new collective behaviors emerge from local decisions and actions. Moreover, and significantly, theory and applications show that networked agents, through cooperation and sharing, are able to match the performance of cloud or federated solutions, while offering the potential for improved privacy, increasing resilience, and saving resources.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24212;&#29992;XAI&#26041;&#27861;&#25581;&#31034;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#22823;&#37327;&#39118;&#21147;&#28065;&#36718;&#26426;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#21628;&#21505;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#26356;&#21152;&#31361;&#20986;&#22320;&#37319;&#29992;XAI&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35299;&#37322;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.12104</link><description>&lt;p&gt;
XAI&#29992;&#20110;&#36879;&#26126;&#30340;&#39118;&#21147;&#28065;&#36718;&#26426;&#21151;&#29575;&#26354;&#32447;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
XAI for transparent wind turbine power curve models. (arXiv:2210.12104v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24212;&#29992;XAI&#26041;&#27861;&#25581;&#31034;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#22823;&#37327;&#39118;&#21147;&#28065;&#36718;&#26426;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#21628;&#21505;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#26356;&#21152;&#31361;&#20986;&#22320;&#37319;&#29992;XAI&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35299;&#37322;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#39118;&#21147;&#28065;&#36718;&#26426;&#21151;&#29575;&#26354;&#32447;&#27169;&#22411;&#23545;&#20110;&#39118;&#33021;&#21457;&#30005;&#34892;&#19994;&#30340;&#35268;&#27169;&#25193;&#22823;&#21644;&#23454;&#29616;&#20854;&#22312;&#20840;&#29699;&#33021;&#28304;&#36716;&#22411;&#20013;&#30340;&#25311;&#35758;&#35282;&#33394;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#21442;&#25968;&#21270;&#12289;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26041;&#27861;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22240;&#20026;&#19981;&#36879;&#26126;&#30340;&#8220;&#40657;&#21283;&#23376;&#8221;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24212;&#29992;&#27969;&#34892;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#8212;&#8212;Shapley&#20540;&#20197;&#21450;&#26368;&#26032;&#30340;XAI&#22238;&#24402;&#27169;&#22411;&#21457;&#29616;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#25805;&#20316;&#39118;&#21147;&#28065;&#36718;&#26426;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#30001;&#20110;&#19987;&#27880;&#20110;&#27979;&#35797;&#38598;&#34920;&#29616;&#65292;&#36234;&#26469;&#36234;&#22823;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#36235;&#21183;&#21487;&#33021;&#23548;&#33268;&#29289;&#29702;&#19978;&#19981;&#21487;&#34892;&#30340;&#27169;&#22411;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21628;&#21505;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#26356;&#21152;&#31361;&#20986;&#22320;&#37319;&#29992;XAI&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35299;&#37322;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#25214;&#20986;&#39118;&#21147;&#28065;&#36718;&#26426;&#24037;&#19994;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate wind turbine power curve models, which translate ambient conditions into turbine power output, are crucial for wind energy to scale and fulfill its proposed role in the global energy transition. While machine learning (ML) methods have shown significant advantages over parametric, physics-informed approaches, they are often criticised for being opaque 'black boxes', which hinders their application in practice. We apply Shapley values, a popular explainable artificial intelligence (XAI) method, and the latest findings from XAI for regression models, to uncover the strategies ML models have learned from operational wind turbine data. Our findings reveal that the trend towards ever larger model architectures, driven by a focus on test set performance, can result in physically implausible model strategies. Therefore, we call for a more prominent role of XAI methods in model selection. Moreover, we propose a practical approach to utilize explanations for root cause analysis in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Self-ViT-MIL&#65292;&#22522;&#20110;&#24187;&#28783;&#29255;&#32423;&#21035;&#30340;&#27880;&#37322;&#23545;&#30284;&#32454;&#32990;&#21306;&#22495;&#36827;&#34892;&#20998;&#31867;&#21644;&#23450;&#20301;&#65292;&#28040;&#38500;&#20102;&#38656;&#35201;&#20687;&#32032;&#32423;&#21035;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2210.09021</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;Transformer&#21644;&#24369;&#26631;&#31614;&#30340;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Histopathological Image Classification based on Self-Supervised Vision Transformer and Weak Labels. (arXiv:2210.09021v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Self-ViT-MIL&#65292;&#22522;&#20110;&#24187;&#28783;&#29255;&#32423;&#21035;&#30340;&#27880;&#37322;&#23545;&#30284;&#32454;&#32990;&#21306;&#22495;&#36827;&#34892;&#20998;&#31867;&#21644;&#23450;&#20301;&#65292;&#28040;&#38500;&#20102;&#38656;&#35201;&#20687;&#32032;&#32423;&#21035;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#20998;&#26512;&#26159;&#20419;&#36827;&#32452;&#32455;&#26679;&#26412;&#30284;&#30151;&#35786;&#26029;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#33258;&#21160;&#21270;&#35786;&#26029;&#23384;&#22312;&#22810;&#31181;&#38382;&#39064;&#65292;&#26368;&#20027;&#35201;&#30340;&#38382;&#39064;&#26159;&#30001;&#20110;&#24040;&#22823;&#30340;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#26377;&#38480;&#30340;&#27880;&#37322;&#32780;&#24341;&#36215;&#30340;&#12290;WSI&#36890;&#24120;&#23637;&#31034;100Kx100K&#20687;&#32032;&#30340;&#20998;&#36776;&#29575;&#12290;&#22312;&#20687;&#32032;&#32423;&#21035;&#19978;&#27880;&#37322;WSI&#20013;&#30340;&#30284;&#32454;&#32990;&#21306;&#22495;&#26159;&#36153;&#21147;&#19988;&#38656;&#35201;&#39640;&#27700;&#24179;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#12290;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#20943;&#36731;&#20102;&#26114;&#36149;&#30340;&#20687;&#32032;&#32423;&#21035;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#22312;MIL&#20013;&#65292;&#23398;&#20064;&#26159;&#22522;&#20110;&#24187;&#28783;&#29255;&#32423;&#21035;&#30340;&#26631;&#31614;&#23637;&#24320;&#30340;&#65292;&#20854;&#20013;&#30149;&#29702;&#23398;&#23478;&#25552;&#20379;&#20851;&#20110;&#24187;&#28783;&#29255;&#20013;&#26159;&#21542;&#21253;&#21547;&#30284;&#32454;&#32990;&#32452;&#32455;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Self-ViT-MIL&#65292;&#23427;&#22522;&#20110;&#24187;&#28783;&#29255;&#32423;&#21035;&#30340;&#27880;&#37322;&#23545;&#30284;&#32454;&#32990;&#21306;&#22495;&#36827;&#34892;&#20998;&#31867;&#21644;&#23450;&#20301;&#65292;&#28040;&#38500;&#20102;&#38656;&#35201;&#20687;&#32032;&#32423;&#21035;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;Self-ViT-MIL&#22312;&#26080;&#38656;&#20381;&#38752;&#26631;&#31614;&#30340;&#33258;&#30417;&#30563;&#35774;&#32622;&#19979;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#23398;&#20064;&#20016;&#23500;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whole Slide Image (WSI) analysis is a powerful method to facilitate the diagnosis of cancer in tissue samples. Automating this diagnosis poses various issues, most notably caused by the immense image resolution and limited annotations. WSIs commonly exhibit resolutions of 100Kx100K pixels. Annotating cancerous areas in WSIs on the pixel level is prohibitively labor-intensive and requires a high level of expert knowledge. Multiple instance learning (MIL) alleviates the need for expensive pixel-level annotations. In MIL, learning is performed on slide-level labels, in which a pathologist provides information about whether a slide includes cancerous tissue. Here, we propose Self-ViT-MIL, a novel approach for classifying and localizing cancerous areas based on slide-level annotations, eliminating the need for pixel-wise annotated training data. Self-ViT- MIL is pre-trained in a self-supervised setting to learn rich feature representation without relying on any labels. The recent Vision Tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26377;&#25439;&#34920;&#31034;&#31354;&#38388;&#19979;&#30340;&#23376;&#30446;&#26631;&#25351;&#23548;&#22312;&#32447;&#24494;&#35843;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20174;&#24191;&#27867;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#32780;&#26469;&#30340;&#26377;&#25439;&#34920;&#31034;&#26469;&#35268;&#21010;&#19968;&#31995;&#21015;&#23376;&#30446;&#26631;&#65292;&#20998;&#35299;&#21407;&#22987;&#20219;&#21153;&#65292;&#24182;&#24378;&#35843;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#20174;&#32780;&#38477;&#20302;&#27867;&#21270;&#36807;&#31243;&#20013;&#20887;&#20313;&#20869;&#23481;&#30340;&#24178;&#25200;&#65292;&#20197;&#27492;&#24212;&#23545;&#22914;&#20309;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#26032;&#39046;&#22495;&#19979;&#28216;&#20219;&#21153;&#30340;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2210.06601</link><description>&lt;p&gt;
&#25439;&#22833;&#29109;&#26426;&#20250;&#19979;&#30340;&#27867;&#21270;&#65306;&#21033;&#29992;&#24191;&#27867;&#30340;&#31163;&#32447;&#25968;&#25454;&#23398;&#20064;&#35270;&#35273;&#36816;&#21160;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization with Lossy Affordances: Leveraging Broad Offline Data for Learning Visuomotor Tasks. (arXiv:2210.06601v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26377;&#25439;&#34920;&#31034;&#31354;&#38388;&#19979;&#30340;&#23376;&#30446;&#26631;&#25351;&#23548;&#22312;&#32447;&#24494;&#35843;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20174;&#24191;&#27867;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#32780;&#26469;&#30340;&#26377;&#25439;&#34920;&#31034;&#26469;&#35268;&#21010;&#19968;&#31995;&#21015;&#23376;&#30446;&#26631;&#65292;&#20998;&#35299;&#21407;&#22987;&#20219;&#21153;&#65292;&#24182;&#24378;&#35843;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#20174;&#32780;&#38477;&#20302;&#27867;&#21270;&#36807;&#31243;&#20013;&#20887;&#20313;&#20869;&#23481;&#30340;&#24178;&#25200;&#65292;&#20197;&#27492;&#24212;&#23545;&#22914;&#20309;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#26032;&#39046;&#22495;&#19979;&#28216;&#20219;&#21153;&#30340;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#25968;&#25454;&#38598;&#30340;&#21033;&#29992;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#21508;&#20010;&#39046;&#22495;&#30340;&#27867;&#21270;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#26032;&#39046;&#22495;&#19979;&#28216;&#20219;&#21153;&#30340;&#23398;&#20064;&#20173;&#28982;&#26159;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#19968;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#24191;&#27867;&#30340;&#25968;&#25454;&#19978;&#33719;&#21462;&#26410;&#35265;&#36807;&#30340;&#26102;&#38388;&#24310;&#36831;&#20219;&#21153;&#30340;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#65292;&#21516;&#26102;&#32467;&#21512;&#23398;&#20064;&#21040;&#30340;&#26377;&#25439;&#22833;&#34920;&#31034;&#31354;&#38388;&#19979;&#30340;&#23376;&#30446;&#26631;&#25351;&#23548;&#22312;&#32447;&#24494;&#35843;&#12290;&#24403;&#38754;&#23545;&#26032;&#30340;&#20219;&#21153;&#30446;&#26631;&#26102;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#26426;&#20250;&#27169;&#22411;&#26469;&#35268;&#21010;&#19968;&#31995;&#21015;&#26377;&#25439;&#34920;&#31034;&#20316;&#20026;&#23376;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#20219;&#21153;&#20998;&#35299;&#25104;&#26356;&#23481;&#26131;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#26377;&#25439;&#34920;&#31034;&#20174;&#24191;&#27867;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#32780;&#26469;&#65292;&#23427;&#20204;&#24378;&#35843;&#26377;&#20851;&#29366;&#24577;&#21644;&#30446;&#26631;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#21516;&#26102;&#25277;&#35937;&#20986;&#22952;&#30861;&#27867;&#21270;&#30340;&#20887;&#20313;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#20026;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#25552;&#20379;&#23376;&#30446;&#26631;&#35268;&#21010;&#65292;&#20026;&#31574;&#30053;&#25552;&#20379;&#32039;&#20945;&#30340;&#36755;&#20837;&#65292;&#20063;&#20026;&#22870;&#21169;&#25552;&#20379;&#20102;&#36739;&#22909;&#30340;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of broad datasets has proven to be crucial for generalization for a wide range of fields. However, how to effectively make use of diverse multi-task data for novel downstream tasks still remains a grand challenge in robotics. To tackle this challenge, we introduce a framework that acquires goal-conditioned policies for unseen temporally extended tasks via offline reinforcement learning on broad data, in combination with online fine-tuning guided by subgoals in learned lossy representation space. When faced with a novel task goal, the framework uses an affordance model to plan a sequence of lossy representations as subgoals that decomposes the original task into easier problems. Learned from the broad data, the lossy representation emphasizes task-relevant information about states and goals while abstracting away redundant contexts that hinder generalization. It thus enables subgoal planning for unseen tasks, provides a compact input to the policy, and facilitates reward
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20307;&#28040;&#35299;&#31639;&#27861;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#20026;PatentsView&#30340;&#29992;&#25143;&#25552;&#20379;&#21487;&#38752;&#30340;&#25968;&#25454;&#21644;&#27604;&#36739;&#19981;&#21516;&#28040;&#35299;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.01230</link><description>&lt;p&gt;
&#36890;&#36807;PatentsView.org&#23398;&#21040;&#30340;&#32463;&#39564;&#65306;&#35780;&#20272;&#23454;&#20307;&#28040;&#35299;&#31639;&#27861;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Estimating the Performance of Entity Resolution Algorithms: Lessons Learned Through PatentsView.org. (arXiv:2210.01230v2 [cs.DL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20307;&#28040;&#35299;&#31639;&#27861;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#20026;PatentsView&#30340;&#29992;&#25143;&#25552;&#20379;&#21487;&#38752;&#30340;&#25968;&#25454;&#21644;&#27604;&#36739;&#19981;&#21516;&#28040;&#35299;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#23454;&#20307;&#28040;&#35299;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26159;&#22522;&#20110;PatentsView.org&#65292;&#19968;&#20010;&#32654;&#22269;&#19987;&#21033;&#21644;&#21830;&#26631;&#21150;&#20844;&#23460;&#30340;&#19987;&#21033;&#25968;&#25454;&#25506;&#27979;&#24037;&#20855;&#65292;&#20351;&#29992;&#23454;&#20307;&#28040;&#35299;&#31639;&#27861;&#26469;&#21306;&#20998;&#19987;&#21033;&#21457;&#26126;&#20154;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#29305;&#23450;&#30340;&#24615;&#33021;&#35780;&#20272;&#22120;&#65292;&#32771;&#34385;&#21040;&#37319;&#26679;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21333;&#12289;&#23454;&#29992;&#21644;&#21407;&#21017;&#24615;&#30340;&#65292;&#36825;&#20123;&#26159;&#20351;&#25105;&#20204;&#33021;&#22815;&#25551;&#32472;PatentsView&#30340;&#23454;&#20307;&#28040;&#35299;&#24615;&#33021;&#30340;&#31532;&#19968;&#20010;&#20195;&#34920;&#24615;&#22270;&#20687;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#29992;&#26469;&#21578;&#30693;PatentsView&#30340;&#29992;&#25143;&#25968;&#25454;&#30340;&#21487;&#38752;&#24615;&#24182;&#20801;&#35768;&#27604;&#36739;&#31454;&#20105;&#30340;&#28040;&#35299;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel evaluation methodology for entity resolution algorithms. It is motivated by PatentsView.org, a U.S. Patents and Trademarks Office patent data exploration tool that disambiguates patent inventors using an entity resolution algorithm. We provide a data collection methodology and tailored performance estimators that account for sampling biases. Our approach is simple, practical and principled -- key characteristics that allow us to paint the first representative picture of PatentsView's disambiguation performance. This approach is used to inform PatentsView's users of the reliability of the data and to allow the comparison of competing disambiguation algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#31867;&#21035;&#29305;&#24449;&#30340;&#26799;&#24230;&#20272;&#35745;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#19978;&#30340;&#39640;&#25928;&#24615;&#33021;&#20197;&#21450;&#25552;&#20379;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;&#21311;&#21517;&#21270;&#38646;&#21806;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2209.03771</link><description>&lt;p&gt;
&#38024;&#23545;&#31867;&#21035;&#29305;&#24449;&#30340;&#26799;&#24230;&#20272;&#35745;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent with gradient estimator for categorical features. (arXiv:2209.03771v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#31867;&#21035;&#29305;&#24449;&#30340;&#26799;&#24230;&#20272;&#35745;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#19978;&#30340;&#39640;&#25928;&#24615;&#33021;&#20197;&#21450;&#25552;&#20379;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;&#21311;&#21517;&#21270;&#38646;&#21806;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#25968;&#25454;&#20986;&#29616;&#22312;&#35832;&#22914;&#20581;&#24247;&#21644;&#20379;&#24212;&#38142;&#31561;&#20851;&#38190;&#39046;&#22495;&#65292;&#36825;&#20123;&#25968;&#25454;&#38656;&#35201;&#29305;&#27530;&#22788;&#29702;&#12290;&#20026;&#20102;&#23558;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#27492;&#31867;&#25968;&#25454;&#65292;&#38656;&#35201;&#36827;&#34892;&#32534;&#30721;&#12290;&#20026;&#20102;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#29420;&#28909;&#32534;&#30721;&#20173;&#28982;&#26159;&#19968;&#20010;&#38750;&#24120;&#22909;&#30340;&#36873;&#25321;&#65292;&#20294;&#27492;&#31867;&#32534;&#30721;&#20250;&#20135;&#29983;&#31232;&#30095;&#25968;&#25454;&#12290;&#26799;&#24230;&#20272;&#35745;&#22120;&#19981;&#36866;&#29992;&#20110;&#31232;&#30095;&#25968;&#25454;&#65306;&#26799;&#24230;&#20027;&#35201;&#34987;&#35748;&#20026;&#26159;&#38646;&#65292;&#32780;&#23454;&#38469;&#19978;&#24182;&#19981;&#24635;&#26159;&#23384;&#22312;&#65292;&#22240;&#27492;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#20272;&#35745;&#22120;&#22312;&#29702;&#35770;&#19978;&#26368;&#23567;&#21270;&#30340;&#20869;&#23481;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#27169;&#22411;&#26550;&#26500;&#19978;&#23637;&#31034;&#20102;&#23427;&#30340;&#25928;&#29575;&#12290;&#22312;&#31867;&#20284;&#35774;&#32622;&#19979;&#65292;&#36825;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#34920;&#29616;&#27604;&#24120;&#35265;&#20272;&#35745;&#22120;&#26356;&#22909;&#12290;&#22312;&#21311;&#21517;&#21270;&#21518;&#65292;&#25105;&#20204;&#36824;&#20844;&#24320;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;&#38646;&#21806;&#25968;&#25454;&#38598;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#26088;&#22312;&#28145;&#20837;&#32771;&#34385;&#31867;&#21035;&#25968;&#25454;&#24182;&#20351;&#27169;&#22411;&#21644;&#20248;&#21270;&#22120;&#36866;&#24212;&#36825;&#20123;&#20851;&#38190;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Categorical data are present in key areas such as health or supply chain, and this data require specific treatment. In order to apply recent machine learning models on such data, encoding is needed. In order to build interpretable models, one-hot encoding is still a very good solution, but such encoding creates sparse data. Gradient estimators are not suited for sparse data: the gradient is mainly considered as zero while it simply does not always exists, thus a novel gradient estimator is introduced. We show what this estimator minimizes in theory and show its efficiency on different datasets with multiple model architectures. This new estimator performs better than common estimators under similar settings. A real world retail dataset is also released after anonymization. Overall, the aim of this paper is to thoroughly consider categorical data and adapt models and optimizers to these key features.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22312;&#32447;&#21644;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#32500;&#25252;&#35745;&#21010;&#26694;&#26550;&#65292;&#20197;&#30830;&#23450;&#26368;&#20248;&#24674;&#22797;&#31574;&#30053;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#27604;&#26631;&#20934;&#30340;&#39044;&#38450;&#24615;&#12289;&#32416;&#27491;&#24615;&#21644;&#36138;&#23146;&#24335;&#35745;&#21010;&#26041;&#26696;&#26377;&#25152;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2208.00808</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#22312;&#32447;&#21644;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#32500;&#25252;&#35745;&#21010;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Maintenance Planning Framework using Online and Offline Deep Reinforcement Learning. (arXiv:2208.00808v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22312;&#32447;&#21644;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#32500;&#25252;&#35745;&#21010;&#26694;&#26550;&#65292;&#20197;&#30830;&#23450;&#26368;&#20248;&#24674;&#22797;&#31574;&#30053;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#27604;&#26631;&#20934;&#30340;&#39044;&#38450;&#24615;&#12289;&#32416;&#27491;&#24615;&#21644;&#36138;&#23146;&#24335;&#35745;&#21010;&#26041;&#26696;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#26412;&#25928;&#30410;&#30340;&#36164;&#20135;&#31649;&#29702;&#26159;&#21508;&#20010;&#34892;&#19994;&#20851;&#27880;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#38024;&#23545;&#19981;&#26029;&#24694;&#21270;&#30340;&#27700;&#31649;&#24320;&#21457;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#33258;&#21160;&#30830;&#23450;&#26368;&#20248;&#24674;&#22797;&#31574;&#30053;&#12290;&#25105;&#20204;&#37319;&#29992;&#22312;&#32447;&#21644;&#31163;&#32447;DRL&#35774;&#32622;&#26469;&#35299;&#20915;&#32500;&#20462;&#35745;&#21010;&#38382;&#39064;&#12290;&#22312;&#22312;&#32447;DRL&#20013;&#65292;&#26234;&#33021;&#20307;&#19982;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#12289;&#26448;&#26009;&#21644;&#22833;&#25928;&#29575;&#29305;&#24449;&#30340;&#22810;&#20010;&#27700;&#31649;&#30340;&#27169;&#25311;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;Q&#23398;&#20064;&#65288;DQN&#65289;&#23545;&#26234;&#33021;&#20307;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23398;&#20064;&#20855;&#26377;&#26368;&#23567;&#24179;&#22343;&#25104;&#26412;&#21644;&#38477;&#20302;&#22833;&#25928;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#31163;&#32447;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#20351;&#29992;&#38745;&#24577;&#25968;&#25454;&#65288;&#22914;DQN&#37325;&#25918;&#25968;&#25454;&#65289;&#36890;&#36807;&#20445;&#23432;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#36827;&#19968;&#27493;&#20132;&#20114;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;DRL&#30340;&#31574;&#30053;&#27604;&#26631;&#20934;&#30340;&#39044;&#38450;&#24615;&#12289;&#32416;&#27491;&#24615;&#21644;&#36138;&#23146;&#24335;&#35745;&#21010;&#26041;&#26696;&#26377;&#25152;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#33021;&#22815;&#20174;&#22266;&#23450;&#30340;DQN&#37325;&#25918;&#25968;&#25454;&#20013;&#23398;&#20064;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cost-effective asset management is an area of interest across several industries. Specifically, this paper develops a deep reinforcement learning (DRL) solution to automatically determine an optimal rehabilitation policy for continuously deteriorating water pipes. We approach the problem of rehabilitation planning in an online and offline DRL setting. In online DRL, the agent interacts with a simulated environment of multiple pipes with distinct lengths, materials, and failure rate characteristics. We train the agent using deep Q-learning (DQN) to learn an optimal policy with minimal average costs and reduced failure probability. In offline learning, the agent uses static data, e.g., DQN replay data, to learn an optimal policy via a conservative Q-learning algorithm without further interactions with the environment. We demonstrate that DRL-based policies improve over standard preventive, corrective, and greedy planning alternatives. Additionally, learning from the fixed DQN replay data
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#37051;&#22495;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#23454;&#29616;&#20102;&#25152;&#26377;&#20256;&#24863;&#22120;&#38388;&#30340;&#36890;&#35759;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#35270;&#35273;&#23548;&#33322;&#20013;&#32570;&#20047;&#20840;&#23616;&#23450;&#20301;&#20449;&#24687;&#30340;&#38382;&#39064;&#24182;&#23454;&#29616;&#20102;&#39640;&#25928;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2208.00759</link><description>&lt;p&gt;
&#30475;&#35265;&#26426;&#22120;&#20154;&#30475;&#19981;&#21040;&#30340;&#19996;&#35199;&#65306;&#23398;&#20064;&#21327;&#20316;&#24863;&#30693;&#36827;&#34892;&#35270;&#35273;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
See What the Robot Can't See: Learning Cooperative Perception for Visual Navigation. (arXiv:2208.00759v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00759
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#37051;&#22495;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#23454;&#29616;&#20102;&#25152;&#26377;&#20256;&#24863;&#22120;&#38388;&#30340;&#36890;&#35759;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#35270;&#35273;&#23548;&#33322;&#20013;&#32570;&#20047;&#20840;&#23616;&#23450;&#20301;&#20449;&#24687;&#30340;&#38382;&#39064;&#24182;&#23454;&#29616;&#20102;&#39640;&#25928;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#26410;&#30693;&#29615;&#22659;&#20013;&#21033;&#29992;&#35270;&#35273;&#20256;&#24863;&#22120;&#24341;&#23548;&#26426;&#22120;&#20154;&#21040;&#36798;&#30446;&#30340;&#22320;&#30340;&#38382;&#39064;&#12290;&#22312;&#32570;&#20047;&#20840;&#23616;&#23450;&#20301;&#20449;&#24687;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#20256;&#24863;&#22120;&#32534;&#30721;&#21644;&#20256;&#36882;&#30456;&#20851;&#30340;&#35270;&#35282;&#20449;&#24687;&#32473;&#26426;&#22120;&#20154;&#65292;&#35753;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20351;&#29992;&#31532;&#19968;&#35270;&#35282;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#23613;&#21487;&#33021;&#39640;&#25928;&#22320;&#23548;&#33322;&#21040;&#30446;&#26631;&#12290;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#22522;&#20110;&#37051;&#22495;&#30340;&#29305;&#24449;&#32858;&#21512;&#27169;&#22359;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#20811;&#26381;&#20102;&#35753;&#25152;&#26377;&#20256;&#24863;&#22120;&#29978;&#33267;&#26080;&#27861;&#30452;&#25509;&#30475;&#21040;&#30446;&#26631;&#30340;&#20256;&#24863;&#22120;&#39044;&#27979;&#36890;&#21521;&#30446;&#26631;&#26368;&#30701;&#36335;&#26041;&#21521;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of navigating a mobile robot towards a target in an unknown environment that is endowed with visual sensors, where neither the robot nor the sensors have access to global positioning information and only use first-person-view images. In order to overcome the need for positioning, we train the sensors to encode and communicate relevant viewpoint information to the mobile robot, whose objective it is to use this information to navigate as efficiently as possible to the target. We overcome the challenge of enabling all the sensors (even those that cannot directly see the target) to predict the direction along the shortest path to the target by implementing a neighborhood-based feature aggregation module using a Graph Neural Network (GNN) architecture. In our experiments, we first demonstrate generalizability to previously unseen environments with various sensor layouts. Our results show that by using communication between the sensors and the robot, we achieve up to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21487;&#24494;&#20998;&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#39640;&#24230;&#20934;&#30830;&#30340;&#26041;&#24335;&#28385;&#36275;&#37096;&#20998;&#24494;&#20998;&#26041;&#31243;&#32422;&#26463;&#65292;&#24182;&#19988;&#22312;&#25512;&#26029;&#26102;&#21487;&#20197;&#25552;&#20379;&#36830;&#32493;&#30340;&#35299;&#26469;&#28385;&#36275;&#25152;&#38656;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27979;&#35797;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2207.08675</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#24494;&#20998;&#27714;&#35299;&#22120;&#20197;&#22788;&#29702;&#24102;&#30828;&#32422;&#26463;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning differentiable solvers for systems with hard constraints. (arXiv:2207.08675v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21487;&#24494;&#20998;&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#39640;&#24230;&#20934;&#30830;&#30340;&#26041;&#24335;&#28385;&#36275;&#37096;&#20998;&#24494;&#20998;&#26041;&#31243;&#32422;&#26463;&#65292;&#24182;&#19988;&#22312;&#25512;&#26029;&#26102;&#21487;&#20197;&#25552;&#20379;&#36830;&#32493;&#30340;&#35299;&#26469;&#28385;&#36275;&#25152;&#38656;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27979;&#35797;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#39640;&#24230;&#20934;&#30830;&#30340;&#26041;&#24335;&#28385;&#36275;&#37096;&#20998;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#32422;&#26463;&#65292;&#20197;&#21450;&#36798;&#21040;&#25152;&#38656;&#30340;&#23481;&#24046;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;PDE&#21463;&#32422;&#26463;&#23618;&#65292;&#21487;&#20197;&#34701;&#20837;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#21487;&#24494;&#20998;&#20248;&#21270;&#21644;&#38544;&#24335;&#20989;&#25968;&#23450;&#29702;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;&#29289;&#29702;&#32422;&#26463;&#12290;&#21463;&#21040;&#23383;&#20856;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23398;&#20064;&#20102;&#19968;&#20010;&#20989;&#25968;&#26063;&#65292;&#20854;&#20013;&#27599;&#20010;&#20989;&#25968;&#37117;&#23450;&#20041;&#20102;&#20174;PDE&#21442;&#25968;&#21040;PDE&#35299;&#30340;&#26144;&#23556;&#12290;&#22312;&#25512;&#26029;&#26102;&#65292;&#27169;&#22411;&#36890;&#36807;&#27714;&#35299;PDE&#21463;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25214;&#21040;&#23398;&#20064;&#21040;&#30340;&#20989;&#25968;&#26063;&#20013;&#30340;&#26368;&#20248;&#32447;&#24615;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24863;&#20852;&#36259;&#30340;&#22495;&#19978;&#25552;&#20379;&#20102;&#36830;&#32493;&#30340;&#35299;&#65292;&#20934;&#30830;&#22320;&#28385;&#36275;&#25152;&#38656;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#30828;&#32422;&#26463;&#30452;&#25509;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#19982;&#22312;&#26080;&#32422;&#26463;&#30446;&#26631;&#20989;&#25968;&#19978;&#35757;&#32451;&#30456;&#27604;&#21487;&#20197;&#23454;&#29616;&#26356;&#20302;&#30340;&#27979;&#35797;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a practical method to enforce partial differential equation (PDE) constraints for functions defined by neural networks (NNs), with a high degree of accuracy and up to a desired tolerance. We develop a differentiable PDE-constrained layer that can be incorporated into any NN architecture. Our method leverages differentiable optimization and the implicit function theorem to effectively enforce physical constraints. Inspired by dictionary learning, our model learns a family of functions, each of which defines a mapping from PDE parameters to PDE solutions. At inference time, the model finds an optimal linear combination of the functions in the learned family by solving a PDE-constrained optimization problem. Our method provides continuous solutions over the domain of interest that accurately satisfy desired physical constraints. Our results show that incorporating hard constraints directly into the NN architecture achieves much lower test error when compared to training on an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning&#65292;&#36890;&#36807;&#24341;&#20837;&#23567;q&#20989;&#25968;&#20316;&#20026;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q-learning&#29702;&#35770;&#65292;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.00713</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning
&lt;/p&gt;
&lt;p&gt;
q-Learning in Continuous Time. (arXiv:2207.00713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;q-Learning&#65292;&#36890;&#36807;&#24341;&#20837;&#23567;q&#20989;&#25968;&#20316;&#20026;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q-learning&#29702;&#35770;&#65292;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#29109;&#27491;&#21017;&#21270;&#30340;&#25506;&#32034;&#24615;&#25193;&#25955;&#36807;&#31243;&#30340;Q-learning&#22312;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#23567;q&#20989;&#25968;&#8221;&#20316;&#20026;&#22823;Q&#20989;&#25968;&#30340;&#19968;&#38454;&#36817;&#20284;&#65292;&#30740;&#31350;&#20102;q&#20989;&#25968;&#30340;q-learning&#29702;&#35770;&#65292;&#24182;&#24212;&#29992;&#20110;&#35774;&#35745;&#19981;&#21516;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the continuous-time counterpart of Q-learning for reinforcement learning (RL) under the entropy-regularized, exploratory diffusion process formulation introduced by Wang et al. (2020). As the conventional (big) Q-function collapses in continuous time, we consider its first-order approximation and coin the term ``(little) q-function". This function is related to the instantaneous advantage rate function as well as the Hamiltonian. We develop a ``q-learning" theory around the q-function that is independent of time discretization. Given a stochastic policy, we jointly characterize the associated q-function and value function by martingale conditions of certain stochastic processes, in both on-policy and off-policy settings. We then apply the theory to devise different actor-critic algorithms for solving underlying RL problems, depending on whether or not the density function of the Gibbs measure generated from the q-function can be computed explicitly. One of our algorithms inter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#27169;&#22411;HyGNN&#65292;&#21487;&#20197;&#22522;&#20110;&#33647;&#29289;&#30340;SMILES&#23383;&#31526;&#20018;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.12747</link><description>&lt;p&gt;
HyGNN: &#22522;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
HyGNN: Drug-Drug Interaction Prediction via Hypergraph Neural Network. (arXiv:2206.12747v4 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#27169;&#22411;HyGNN&#65292;&#21487;&#20197;&#22522;&#20110;&#33647;&#29289;&#30340;SMILES&#23383;&#31526;&#20018;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;(DDIs)&#21487;&#33021;&#20250;&#24433;&#21709;&#33647;&#29289;&#21151;&#33021;&#65292;&#22312;&#26368;&#31967;&#31957;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#33391;&#33647;&#29289;&#21453;&#24212;(ADRs)&#12290;&#39044;&#27979;&#25152;&#26377;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#20851;&#38190;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;(HyGNN)&#27169;&#22411;&#65292;&#20165;&#22522;&#20110;&#33647;&#29289;&#30340;SMILES&#23383;&#31526;&#20018;&#65292;&#20026;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#25429;&#25417;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#20174;SMILES&#23383;&#31526;&#20018;&#20013;&#25552;&#21462;&#33647;&#29289;&#30340;&#21270;&#23398;&#20122;&#32467;&#26500;&#21019;&#24314;&#36229;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;HyGNN&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20851;&#27880;&#24230;&#30340;&#36229;&#22270;&#36793;&#32534;&#30721;&#22120;&#65292;&#20197;&#33719;&#21462;&#33647;&#29289;&#30340;&#36229;&#36793;&#34920;&#31034;&#65292;&#20197;&#21450;&#19968;&#20010;&#35299;&#30721;&#22120;&#65292;&#20197;&#39044;&#27979;&#33647;&#29289;&#23545;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36229;&#22270;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20122;&#32467;&#26500;&#21644;&#36229;&#36793;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;HyGNN&#30340;&#24615;&#33021;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug-Drug Interactions (DDIs) may hamper the functionalities of drugs, and in the worst scenario, they may lead to adverse drug reactions (ADRs). Predicting all DDIs is a challenging and critical problem. Most existing computational models integrate drug-centric information from different sources and leverage them as features in machine learning classifiers to predict DDIs. However, these models have a high chance of failure, especially for the new drugs when all the information is not available. This paper proposes a novel Hypergraph Neural Network (HyGNN) model based on only the SMILES string of drugs, available for any drug, for the DDI prediction problem. To capture the drug similarities, we create a hypergraph from drugs' chemical substructures extracted from the SMILES strings. Then, we develop HyGNN consisting of a novel attention-based hypergraph edge encoder to get the representation of drugs as hyperedges and a decoder to predict the interactions between drug pairs. Furthermo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#40065;&#26834;&#25439;&#22833;&#30340;&#23398;&#20064;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#38797;&#28857;&#20248;&#21270;&#38382;&#39064;&#30340;&#35265;&#35299;&#65292;&#24182;&#22312;&#22312;&#32447;&#33073;&#26426;&#39044;&#27979;&#21644;&#25511;&#21046;&#35774;&#32622;&#20013;&#65292;&#25512;&#23548;&#20102;&#31283;&#20581;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.08464</link><description>&lt;p&gt;
&#23398;&#20064;&#20540;&#20989;&#25968;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Robust Losses for Learning Value Functions. (arXiv:2205.08464v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#40065;&#26834;&#25439;&#22833;&#30340;&#23398;&#20064;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#38797;&#28857;&#20248;&#21270;&#38382;&#39064;&#30340;&#35265;&#35299;&#65292;&#24182;&#22312;&#22312;&#32447;&#33073;&#26426;&#39044;&#27979;&#21644;&#25511;&#21046;&#35774;&#32622;&#20013;&#65292;&#25512;&#23548;&#20102;&#31283;&#20581;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22823;&#22810;&#25968;&#20540;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#22522;&#20110;&#22343;&#26041;&#65288;&#25237;&#24433;&#65289;&#36125;&#23572;&#26364;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#20108;&#27425;&#35823;&#24046;&#23545;&#31163;&#32676;&#20540;&#24456;&#25935;&#24863;&#65292;&#19981;&#20165;&#20250;&#20559;&#31227;&#30446;&#26631;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36824;&#20250;&#23548;&#33268;&#20855;&#26377;&#39640;&#25391;&#24133;&#21644;&#39640;&#26041;&#24046;&#30340;&#26799;&#24230;&#12290;&#20026;&#20102;&#25511;&#21046;&#36825;&#20123;&#39640;&#25391;&#24133;&#26356;&#26032;&#65292;RL&#20013;&#30340;&#20856;&#22411;&#31574;&#30053;&#21253;&#25324;&#21098;&#20999;&#26799;&#24230;&#12289;&#21098;&#20999;&#22870;&#21169;&#12289;&#37325;&#26032;&#32553;&#25918;&#22870;&#21169;&#25110;&#21098;&#20999;&#35823;&#24046;&#12290;&#34429;&#28982;&#36825;&#20123;&#31574;&#30053;&#20284;&#20046;&#19982;Huber loss&#31561;&#40065;&#26834;&#25439;&#22833;&#26377;&#20851;&#65292;&#20294;&#23427;&#20204;&#24314;&#31435;&#22312;&#21322;&#26799;&#24230;&#26356;&#26032;&#35268;&#21017;&#19978;&#65292;&#19981;&#20250;&#26368;&#23567;&#21270;&#24050;&#30693;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#26368;&#36817;&#23545;&#23558;&#24179;&#26041;&#36125;&#23572;&#26364;&#35823;&#24046;&#37325;&#26500;&#20026;&#38797;&#28857;&#20248;&#21270;&#38382;&#39064;&#30340;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;Huber&#36125;&#23572;&#26364;&#35823;&#24046;&#21644;&#32477;&#23545;&#36125;&#23572;&#26364;&#35823;&#24046;&#30340;&#38797;&#28857;&#37325;&#26500;&#12290;&#25105;&#20204;&#20174;&#40065;&#26834;&#25439;&#22833;&#30340;&#24418;&#24335;&#21270;&#24320;&#22987;&#65292;&#28982;&#21518;&#25512;&#23548;&#20986;&#31283;&#20581;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#22312;&#32447;&#33073;&#26426;&#39044;&#27979;&#21644;&#25511;&#21046;&#35774;&#32622;&#20013;&#26368;&#23567;&#21270;&#36825;&#20123;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most value function learning algorithms in reinforcement learning are based on the mean squared (projected) Bellman error. However, squared errors are known to be sensitive to outliers, both skewing the solution of the objective and resulting in high-magnitude and high-variance gradients. To control these high-magnitude updates, typical strategies in RL involve clipping gradients, clipping rewards, rescaling rewards, or clipping errors. While these strategies appear to be related to robust losses -- like the Huber loss -- they are built on semi-gradient update rules which do not minimize a known loss. In this work, we build on recent insights reformulating squared Bellman errors as a saddlepoint optimization problem and propose a saddlepoint reformulation for a Huber Bellman error and Absolute Bellman error. We start from a formalization of robust losses, then derive sound gradient-based approaches to minimize these losses in both the online off-policy prediction and control settings. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PTP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#23618;&#35268;&#21010;&#22120;&#21644;&#28508;&#31354;&#38388;&#20013;&#30340;&#26465;&#20214;&#29983;&#25104;&#22120;&#26469;&#20998;&#35299;&#30446;&#26631;&#25104;&#23376;&#30446;&#26631;&#65292;&#24182;&#22312;&#20197;&#21069;&#30340;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#23376;&#30446;&#26631;&#29983;&#25104;&#22120;&#21644;&#31574;&#30053;&#65292;&#28982;&#21518;&#22312;&#32447;&#24494;&#35843;&#20197;&#36866;&#24212;&#26032;&#30340;&#30446;&#26631;&#65292;&#20174;&#32780;&#35757;&#32451;&#30446;&#26631;&#23548;&#21521;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#38271;&#26399;&#30446;&#26631;&#23548;&#21521;&#30340;&#32463;&#39564;&#37319;&#38598;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2205.08129</link><description>&lt;p&gt;
&#35745;&#21010;&#21040;&#23454;&#36341;&#65306;&#22312;&#28508;&#31354;&#38388;&#20013;&#32452;&#21512;&#30446;&#26631;&#30340;&#39640;&#25928;&#22312;&#32447;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space. (arXiv:2205.08129v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PTP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#23618;&#35268;&#21010;&#22120;&#21644;&#28508;&#31354;&#38388;&#20013;&#30340;&#26465;&#20214;&#29983;&#25104;&#22120;&#26469;&#20998;&#35299;&#30446;&#26631;&#25104;&#23376;&#30446;&#26631;&#65292;&#24182;&#22312;&#20197;&#21069;&#30340;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#23376;&#30446;&#26631;&#29983;&#25104;&#22120;&#21644;&#31574;&#30053;&#65292;&#28982;&#21518;&#22312;&#32447;&#24494;&#35843;&#20197;&#36866;&#24212;&#26032;&#30340;&#30446;&#26631;&#65292;&#20174;&#32780;&#35757;&#32451;&#30446;&#26631;&#23548;&#21521;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#38271;&#26399;&#30446;&#26631;&#23548;&#21521;&#30340;&#32463;&#39564;&#37319;&#38598;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#34892;&#20026;&#26469;&#23436;&#25104;&#29616;&#23454;&#19990;&#30028;&#20013;&#26080;&#32467;&#26500;&#29615;&#22659;&#19979;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#33719;&#24471;&#21487;&#20197;&#22312;&#21629;&#20196;&#19979;&#36798;&#26102;&#21040;&#36798;&#21487;&#37197;&#32622;&#30446;&#26631;&#30340;&#31574;&#30053;&#65292;&#23436;&#25104;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#30446;&#26631;&#23548;&#21521;&#31574;&#30053;&#38750;&#24120;&#38590;&#20197;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Planning to Practice&#65288;PTP&#65289;&#65292;&#19968;&#31181;&#23454;&#29616;&#35757;&#32451;&#30446;&#26631;&#23548;&#21521;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38656;&#35201;&#22810;&#20010;&#19981;&#21516;&#31867;&#22411;&#20132;&#20114;&#25165;&#33021;&#35299;&#20915;&#30340;&#38271;&#26102;&#31243;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#37325;&#35201;&#30340;&#24605;&#24819;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#23618;&#22320;&#20998;&#35299;&#20102;&#21040;&#36798;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#26465;&#20214;&#23376;&#30446;&#26631;&#29983;&#25104;&#22120;&#22312;&#20302;&#32423;&#26080;&#27169;&#22411;&#31574;&#30053;&#20013;&#30340;&#28508;&#31354;&#38388;&#20013;&#35774;&#32622;&#20013;&#38388;&#23376;&#30446;&#26631;&#30340;&#39640;&#32423;&#35745;&#21010;&#22120;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#21363;&#39318;&#20808;&#22312;&#20197;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#23376;&#30446;&#26631;&#29983;&#25104;&#22120;&#21644;&#31574;&#30053;&#65292;&#28982;&#21518;&#22312;&#32447;&#24494;&#35843;&#20197;&#36866;&#24212;&#26032;&#30340;&#30446;&#26631;&#65292;&#20197;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PTP&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#24456;&#39640;&#25928;&#22320;&#37319;&#38598;&#38271;&#26399;&#30446;&#26631;&#23548;&#21521;&#30340;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
General-purpose robots require diverse repertoires of behaviors to complete challenging tasks in real-world unstructured environments. To address this issue, goal-conditioned reinforcement learning aims to acquire policies that can reach configurable goals for a wide range of tasks on command. However, such goal-conditioned policies are notoriously difficult and time-consuming to train from scratch. In this paper, we propose Planning to Practice (PTP), a method that makes it practical to train goal-conditioned policies for long-horizon tasks that require multiple distinct types of interactions to solve. Our approach is based on two key ideas. First, we decompose the goal-reaching problem hierarchically, with a high-level planner that sets intermediate subgoals using conditional subgoal generators in the latent space for a low-level model-free policy. Second, we propose a hybrid approach which first pre-trains both the conditional subgoal generator and the policy on previously collected
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#20851;&#20110;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#36866;&#24212;&#24178;&#39044;&#20013;&#30340;&#31532;&#19968;&#20221;&#32479;&#19968;&#35843;&#26597;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#21644;&#31227;&#21160;&#20581;&#24247;&#20013;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#36825;&#20004;&#20010;&#39046;&#22495;&#20013;&#37117;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#22312;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#21644;&#19981;&#21516;&#20043;&#22788;&#38656;&#35201;&#32771;&#34385;&#65292;&#24182;&#19988;&#36825;&#37324;&#23384;&#22312;&#24040;&#22823;&#30340;&#21512;&#20316;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2203.02605</link><description>&lt;p&gt;
&#29616;&#20195;&#29983;&#29289;&#32479;&#35745;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#26500;&#24314;&#26368;&#20248;&#33258;&#36866;&#24212;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning in Modern Biostatistics: Constructing Optimal Adaptive Interventions. (arXiv:2203.02605v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#20851;&#20110;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#36866;&#24212;&#24178;&#39044;&#20013;&#30340;&#31532;&#19968;&#20221;&#32479;&#19968;&#35843;&#26597;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#21644;&#31227;&#21160;&#20581;&#24247;&#20013;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#36825;&#20004;&#20010;&#39046;&#22495;&#20013;&#37117;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#22312;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#21644;&#19981;&#21516;&#20043;&#22788;&#38656;&#35201;&#32771;&#34385;&#65292;&#24182;&#19988;&#36825;&#37324;&#23384;&#22312;&#24040;&#22823;&#30340;&#21512;&#20316;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#24207;&#21015;&#24615;&#20915;&#31574;&#20013;&#21344;&#25454;&#20102;&#37325;&#35201;&#22320;&#20301;&#65292;&#25104;&#20026;&#20132;&#20184;&#33258;&#36866;&#24212;&#24178;&#39044;&#65288;AIs&#65289;&#30340;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#20294;&#20854;&#29616;&#23454;&#24212;&#29992;&#20173;&#28982;&#21463;&#21040;&#38480;&#21046;&#65292;&#37096;&#20998;&#26159;&#30001;&#20110;&#26041;&#27861;&#35770;&#21644;&#24212;&#29992;&#31038;&#21306;&#20043;&#38388;&#30340;&#21327;&#21516;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#23398;&#20064;AIs&#30340;RL&#26041;&#27861;&#30340;&#31532;&#19968;&#20221;&#32479;&#19968;&#35843;&#26597;&#65292;&#21033;&#29992;RL&#30340;&#36890;&#29992;&#26041;&#27861;&#35770;&#20254;&#26469;&#26725;&#25509;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#21644;&#31227;&#21160;&#20581;&#24247;&#20013;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#36825;&#20004;&#20010;AI&#39046;&#22495;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#36825;&#20004;&#20010;AI&#39046;&#22495;&#20043;&#38388;&#30340;&#24322;&#21516;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#23545;&#20351;&#29992;RL&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#24049;&#22312;&#20004;&#20010;&#39046;&#22495;&#20013;&#35774;&#35745;&#26696;&#20363;&#30740;&#31350;&#30340;&#32463;&#39564;&#65292;&#35828;&#26126;&#20102;&#22312;AIs&#39046;&#22495;&#20013;&#65292;&#32479;&#35745;&#23398;&#12289;RL&#21644;&#21307;&#30103;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#24040;&#22823;&#21512;&#20316;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, reinforcement learning (RL) has acquired a prominent position in the space of health-related sequential decision-making, becoming an increasingly popular tool for delivering adaptive interventions (AIs). However, despite potential benefits, its real-life application is still limited, partly due to a poor synergy between the methodological and the applied communities. In this work, we provide the first unified survey on RL methods for learning AIs, using the common methodological umbrella of RL to bridge the two AI areas of dynamic treatment regimes and just-in-time adaptive interventions in mobile health. We outline similarities and differences between these two AI domains and discuss their implications for using RL. Finally, we leverage our experience in designing case studies in both areas to illustrate the tremendous collaboration opportunities between statistical, RL, and healthcare researchers in the space of AIs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212; (CATE) &#30340;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#32570;&#22833;&#27835;&#30103;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#32570;&#22833;&#27835;&#30103;&#34920;&#31034;&#32593;&#32476; (MTRNet)&#65292;&#36890;&#36807;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#21327;&#21464;&#37327;&#30340;&#24179;&#34913;&#34920;&#31034;&#26469;&#35299;&#20915;&#21327;&#21464;&#37327;&#36716;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2203.01422</link><description>&lt;p&gt;
&#32570;&#22833;&#27835;&#30103;&#20449;&#24687;&#30340;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimating Conditional Average Treatment Effects with Missing Treatment Information. (arXiv:2203.01422v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212; (CATE) &#30340;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#32570;&#22833;&#27835;&#30103;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#32570;&#22833;&#27835;&#30103;&#34920;&#31034;&#32593;&#32476; (MTRNet)&#65292;&#36890;&#36807;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#21327;&#21464;&#37327;&#30340;&#24179;&#34913;&#34920;&#31034;&#26469;&#35299;&#20915;&#21327;&#21464;&#37327;&#36716;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212; (CATE) &#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#27835;&#30103;&#20449;&#24687;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#36825;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20294;&#32570;&#22833;&#27835;&#30103;&#30340; CATE &#20272;&#35745;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#32570;&#22833;&#27835;&#30103;&#24773;&#20917;&#19979;&#30340; CATE &#20272;&#35745;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#21327;&#21464;&#37327;&#20559;&#31227;&#12290;&#25105;&#20204;&#35748;&#23450;&#20102;&#25105;&#20204;&#30340;&#24773;&#20917;&#20013;&#23384;&#22312;&#20004;&#31181;&#21327;&#21464;&#37327;&#36716;&#31227;&#65306;(i) &#27835;&#30103;&#32452;&#21644;&#23545;&#29031;&#32452;&#20043;&#38388;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#65292;&#20197;&#21450;(ii) &#35266;&#23519;&#21040;&#30340;&#27835;&#30103;&#32452;&#21644;&#32570;&#22833;&#27835;&#30103;&#32452;&#20043;&#38388;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36825;&#20123;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#20026;&#25105;&#20204;&#30340;&#32570;&#22833;&#27835;&#30103;&#24773;&#20917;&#19979;&#30340; CATE &#20272;&#35745;&#23548;&#20986;&#19968;&#20010;&#27867;&#21270;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#21463;&#21040;&#25105;&#20204;&#30340;&#30028;&#38480;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32570;&#22833;&#27835;&#30103;&#34920;&#31034;&#32593;&#32476; (MTRNet)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340; CATE &#20272;&#35745;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#21327;&#21464;&#37327;&#30340;&#24179;&#34913;&#34920;&#31034;&#12290;&#36890;&#36807;&#20351;&#29992;&#24179;&#34913;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;MTRNet&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Estimating conditional average treatment effects (CATE) is challenging, especially when treatment information is missing. Although this is a widespread problem in practice, CATE estimation with missing treatments has received little attention. In this paper, we analyze CATE estimation in the setting with missing treatments where unique challenges arise in the form of covariate shifts. We identify two covariate shifts in our setting: (i) a covariate shift between the treated and control population; and (ii) a covariate shift between the observed and missing treatment population. We first theoretically show the effect of these covariate shifts by deriving a generalization bound for estimating CATE in our setting with missing treatments. Then, motivated by our bound, we develop the missing treatment representation network (MTRNet), a novel CATE estimation algorithm that learns a balanced representation of covariates using domain adaptation. By using balanced representations, MTRNet provid
&lt;/p&gt;</description></item><item><title>&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#65292;&#22914;&#20309;&#35753;&#30693;&#35782;&#25277;&#21462;&#26356;&#22909;&#22320;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#65311;&#26412;&#25991;&#35843;&#30740;&#20102;&#19977;&#31181;&#35299;&#20915;&#33539;&#24335;&#65306;&#39640;&#36164;&#28304;&#25968;&#25454;&#12289;&#26356;&#24378;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2202.08063</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#30340;&#30693;&#35782;&#25277;&#21462;&#65306;&#35843;&#30740;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08063
&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;&#65292;&#22914;&#20309;&#35753;&#30693;&#35782;&#25277;&#21462;&#26356;&#22909;&#22320;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#65311;&#26412;&#25991;&#35843;&#30740;&#20102;&#19977;&#31181;&#35299;&#20915;&#33539;&#24335;&#65306;&#39640;&#36164;&#28304;&#25968;&#25454;&#12289;&#26356;&#24378;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#25277;&#21462;&#65288;KE&#65289;&#26088;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#65292;&#36890;&#24120;&#36973;&#21463;&#25968;&#25454;&#21294;&#20047;&#21644;&#20986;&#29616;&#26410;&#35265;&#31867;&#22411;&#65288;&#20302;&#36164;&#28304;&#24773;&#22659;&#65289;&#30340;&#22256;&#25200;&#12290;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#24050;&#24191;&#27867;&#30740;&#31350;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#23545;&#20302;&#36164;&#28304;&#24773;&#22659;&#19979;KE&#36827;&#34892;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23558;&#29616;&#26377;&#30340;&#24037;&#20316;&#31995;&#32479;&#24615;&#22320;&#20998;&#20026;&#19977;&#31181;&#33539;&#24335;&#65306;&#65288;1&#65289;&#21033;&#29992;&#39640;&#36164;&#28304;&#25968;&#25454;&#65292;&#65288;2&#65289;&#21033;&#29992;&#26356;&#24378;&#30340;&#27169;&#22411;&#65292;&#65288;3&#65289;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20123;&#28508;&#22312;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35843;&#30740;&#21487;&#20197;&#24110;&#21161;&#23398;&#26415;&#21644;&#24037;&#19994;&#30028;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#21019;&#24847;&#65292;&#25552;&#21319;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Extraction (KE), aiming to extract structural information from unstructured texts, often suffers from data scarcity and emerging unseen types, i.e., low-resource scenarios. Many neural approaches to low-resource KE have been widely investigated and achieved impressive performance. In this paper, we present a literature review towards KE in low-resource scenarios, and systematically categorize existing works into three paradigms: (1) exploiting higher-resource data, (2) exploiting stronger models, and (3) exploiting data and models together. In addition, we highlight promising applications and outline some potential directions for future research. We hope that our survey can help both the academic and industrial communities to better understand this field, inspire more ideas, and boost broader applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24179;&#34913;&#30340;&#31574;&#30053;&#35299;&#20915;&#20102;&#25209;&#35268;&#33539;&#21270;&#22312;&#22522;&#20110;&#26679;&#20363;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2201.12559</link><description>&lt;p&gt;
&#20026;&#22522;&#20110;&#26679;&#20363;&#30340;&#22686;&#37327;&#23398;&#20064;&#37325;&#26032;&#24179;&#34913;&#25209;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
Rebalancing Batch Normalization for Exemplar-based Class-Incremental Learning. (arXiv:2201.12559v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24179;&#34913;&#30340;&#31574;&#30053;&#35299;&#20915;&#20102;&#25209;&#35268;&#33539;&#21270;&#22312;&#22522;&#20110;&#26679;&#20363;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#35268;&#33539;&#21270;&#21450;&#20854;&#21464;&#31181;&#24050;&#32463;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#30456;&#23545;&#36739;&#23569;&#30340;&#24037;&#20316;&#19987;&#38376;&#30740;&#31350;&#20102;BN&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#25928;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#38024;&#23545;&#22522;&#20110;&#26679;&#20363;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;BN&#26356;&#26032;&#26041;&#27861;&#12290;BN&#22312;CIL&#20013;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#24403;&#21069;&#20219;&#21153;&#21644;&#36807;&#21435;&#20219;&#21153;&#20043;&#38388;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#65292;&#36825;&#20351;&#24471;BN&#30340;&#32463;&#39564;&#22343;&#20540;&#21644;&#26041;&#24046;&#20197;&#21450;&#21487;&#23398;&#20064;&#20223;&#23556;&#21464;&#25442;&#21442;&#25968;&#22312;&#24403;&#21069;&#20219;&#21153;&#20013;&#20005;&#37325;&#20559;&#32622;&#65292;&#20174;&#32780;&#23548;&#33268;&#24536;&#35760;&#36807;&#21435;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#19968;&#31181;BN&#21464;&#31181;&#26159;&#20026;&#8220;&#22312;&#32447;&#8221;CIL&#24320;&#21457;&#30340;&#65292;&#20854;&#20013;&#35757;&#32451;&#26159;&#22312;&#21333;&#20010;&#26102;&#26399;&#20869;&#23436;&#25104;&#30340;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#20182;&#20204;&#30340;&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#23545;&#8220;&#31163;&#32447;&#8221;CIL&#24102;&#26469;&#25910;&#30410;&#65292;&#20854;&#20013;&#22312;&#19981;&#24179;&#34913;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#22810;&#27425;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#22521;&#35757;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#26080;&#25928;&#30340;&#20027;&#35201;&#21407;&#22240;&#22312;&#20110;&#20854;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#20559;&#32622;&#65292;&#32780;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21017;&#26159;&#36890;&#36807;&#37319;&#29992;&#37325;&#26032;&#24179;&#34913;&#30340;&#31574;&#30053;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#24471;BN&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#22686;&#37327;&#23398;&#20064;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#37117;&#21462;&#24471;&#20102;&#26356;&#20248;&#31168;&#30340;&#34920;&#29616;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20445;&#25345;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Normalization (BN) and its variants has been extensively studied for neural nets in various computer vision tasks, but relatively little work has been dedicated to studying the effect of BN in continual learning. To that end, we develop a new update patch for BN, particularly tailored for the exemplar-based class-incremental learning (CIL). The main issue of BN in CIL is the imbalance of training data between current and past tasks in a mini-batch, which makes the empirical mean and variance as well as the learnable affine transformation parameters of BN heavily biased toward the current task -contributing to the forgetting of past tasks. While one of the recent BN variants has been developed for "online" CIL, in which the training is done with a single epoch, we show that their method does not necessarily bring gains for "offline" CIL, in which a model is trained with multiple epochs on the imbalanced training data. The main reason for the ineffectiveness of their method lies 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#31639;&#27861;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#21151;&#29575;&#20998;&#37197;&#30340;&#21152;&#26435;&#33021;&#25928;&#27604;&#26368;&#22823;&#21270;&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#27169;&#22359;&#21270;&#32467;&#26500;&#21644;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#35299;&#20915;&#38382;&#39064;&#30340;&#38750;&#20984;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#32622;&#25442;&#31561;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.11799</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#33021;&#37327;&#24863;&#30693;&#21151;&#29575;&#20998;&#37197;&#30340;&#22522;&#20110;&#22270;&#31639;&#27861;&#30340;&#23637;&#24320;
&lt;/p&gt;
&lt;p&gt;
Graph-based Algorithm Unfolding for Energy-aware Power Allocation in Wireless Networks. (arXiv:2201.11799v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#31639;&#27861;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#21151;&#29575;&#20998;&#37197;&#30340;&#21152;&#26435;&#33021;&#25928;&#27604;&#26368;&#22823;&#21270;&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#27169;&#22359;&#21270;&#32467;&#26500;&#21644;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#35299;&#20915;&#38382;&#39064;&#30340;&#38750;&#20984;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#32622;&#25442;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#31639;&#27861;&#30340;&#21487;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#21151;&#29575;&#20998;&#37197;&#30340;&#21152;&#26435;&#33021;&#25928;&#27604;&#65288;WSEE&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#38382;&#39064;&#30340;&#38750;&#20984;&#24615;&#65292;&#35813;&#26041;&#27861;&#30001;&#21463;&#32463;&#20856;&#36845;&#20195;&#27425;&#20248;&#26041;&#27861;&#21551;&#21457;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#32452;&#25104;&#65292;&#24182;&#22686;&#24378;&#20102;&#23398;&#20064;&#32452;&#20214;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#20984;&#36924;&#36817;&#65288;SCA&#65289;&#26041;&#27861;&#30340;&#28145;&#23618;&#23637;&#24320;&#12290;&#22312;&#25105;&#20204;&#23637;&#24320;&#30340;SCA&#65288;USCA&#65289;&#26694;&#26550;&#20013;&#65292;&#21407;&#26469;&#39044;&#35774;&#30340;&#21442;&#25968;&#29616;&#22312;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#22810;&#29992;&#25143;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#20316;&#20026;&#24213;&#23618;&#22270;&#37051;&#25509;&#30697;&#38453;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCNs&#65289;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#24314;&#31569;&#30340;&#32622;&#25442;&#31561;&#21464;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#24212;&#29992;&#20110;&#26080;&#32447;&#32593;&#32476;&#25968;&#25454;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;&#36890;&#36807;&#28176;&#36827;&#35757;&#32451;&#31574;&#30053;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#35757;&#32451;USCA&#26694;&#26550;&#12290;&#26080;&#30417;&#30563;&#25439;&#22833;&#34987;&#31934;&#24515;&#35774;&#35745;&#20026;&#32771;&#34385;&#22810;&#20010;&#20301;&#32622;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel graph-based trainable framework to maximize the weighted sum energy efficiency (WSEE) for power allocation in wireless communication networks. To address the non-convex nature of the problem, the proposed method consists of modular structures inspired by a classical iterative suboptimal approach and enhanced with learnable components. More precisely, we propose a deep unfolding of the successive concave approximation (SCA) method. In our unfolded SCA (USCA) framework, the originally preset parameters are now learnable via graph convolutional neural networks (GCNs) that directly exploit multi-user channel state information as the underlying graph adjacency matrix. We show the permutation equivariance of the proposed architecture, which is a desirable property for models applied to wireless network data. The USCA framework is trained through a stochastic gradient descent approach using a progressive training strategy. The unsupervised loss is carefully devised to featu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#21307;&#23398;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#24565;&#38388;&#30340;&#20851;&#31995;&#26500;&#24314;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#37322;&#32974;&#20799;&#36229;&#22768;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#25512;&#29702;&#32467;&#26524;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2201.07798</link><description>&lt;p&gt;
&#22522;&#20110;&#21307;&#23398;&#27010;&#24565;&#30340;&#32974;&#20799;&#36229;&#22768;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35748;&#30693;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Explainer for Fetal ultrasound images classifier Based on Medical Concepts. (arXiv:2201.07798v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#21307;&#23398;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#24565;&#38388;&#30340;&#20851;&#31995;&#26500;&#24314;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#37322;&#32974;&#20799;&#36229;&#22768;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#30340;&#25512;&#29702;&#32467;&#26524;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20108;&#32500;&#23381;&#26399;&#26816;&#26597;&#20013;&#65292;&#32974;&#20799;&#26631;&#20934;&#25195;&#25551;&#24179;&#38754;&#30340;&#26816;&#27979;&#26159;&#19968;&#39033;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#24191;&#27867;&#30340;&#21307;&#23398;&#30693;&#35782;&#21644;&#22810;&#24180;&#30340;&#22521;&#35757;&#12290;&#34429;&#28982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#21487;&#20197;&#21327;&#21161;&#32463;&#39564;&#19981;&#36275;&#30340;&#21307;&#29983;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#21307;&#23398;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#26694;&#26550;&#65292;&#20174;&#20020;&#24202;&#21307;&#29983;&#30340;&#35748;&#30693;&#35282;&#24230;&#25552;&#20379;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#27010;&#24565;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(GCN)&#26500;&#24314;&#20851;&#38190;&#21307;&#23398;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23545;&#19968;&#20010;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20102;&#26131;&#20110;&#29702;&#35299;&#30340;&#25512;&#29702;&#32467;&#26524;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fetal standard scan plane detection during 2-D mid-pregnancy examinations is a highly complex task, which requires extensive medical knowledge and years of training. Although deep neural networks (DNN) can assist inexperienced operators in these tasks, their lack of transparency and interpretability limit their application. Despite some researchers have been committed to visualizing the decision process of DNN, most of them only focus on the pixel-level features and do not take into account the medical prior knowledge. In this work, we propose an interpretable framework based on key medical concepts, which provides explanations from the perspective of clinicians' cognition. Moreover, we utilize a concept-based graph convolutional neural(GCN) network to construct the relationships between key medical concepts. Extensive experimental analysis on a private dataset has shown that the proposed method provides easy-to-understand insights about reasoning results for clinicians.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;DQN&#21644;Rainbow&#20004;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;Atari&#28216;&#25103;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22312;&#22312;&#32447;&#32593;&#32476;&#21644;&#30446;&#26631;&#32593;&#32476;&#20043;&#38388;&#24341;&#20837;&#19968;&#23450;&#30340;&#25509;&#36817;&#24230;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.05848</link><description>&lt;p&gt;
&#20351;&#29992;&#36739;&#24930;&#30340;&#22312;&#32447;&#32593;&#32476;&#23454;&#29616;&#26356;&#24555;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Faster Deep Reinforcement Learning with Slower Online Network. (arXiv:2112.05848v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.05848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;DQN&#21644;Rainbow&#20004;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;Atari&#28216;&#25103;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22312;&#22312;&#32447;&#32593;&#32476;&#21644;&#30446;&#26631;&#32593;&#32476;&#20043;&#38388;&#24341;&#20837;&#19968;&#23450;&#30340;&#25509;&#36817;&#24230;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#20351;&#29992;&#20004;&#20010;&#32593;&#32476;&#36827;&#34892;&#20215;&#20540;&#20989;&#25968;&#20248;&#21270;&#65306;&#19968;&#20010;&#22312;&#32447;&#32593;&#32476;&#21644;&#19968;&#20010;&#30446;&#26631;&#32593;&#32476;&#65292;&#21518;&#32773;&#24102;&#26377;&#19968;&#23450;&#30340;&#24310;&#36831;&#12290;&#20351;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;&#32593;&#32476;&#20351;&#24471;&#26234;&#33021;&#20307;&#33021;&#22815;&#23545;&#25239;&#21551;&#21457;&#24335;&#24341;&#23548;&#26102;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;&#21363;DQN&#21644;Rainbow&#65289;&#20013;&#24341;&#20837;&#20102;&#26356;&#26032;&#31639;&#27861;&#65292;&#20197;&#28608;&#21169;&#22312;&#32447;&#32593;&#32476;&#20445;&#25345;&#19982;&#30446;&#26631;&#32593;&#32476;&#30340;&#25509;&#36817;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#23384;&#22312;&#22122;&#22768;&#26356;&#26032;&#26102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#20195;&#29702;&#31216;&#20026;DQN Pro&#21644;Rainbow Pro&#65292;&#23427;&#20204;&#22312;Atari&#22522;&#20934;&#27979;&#35797;&#19978;&#30456;&#23545;&#20110;&#21407;&#22987;&#31639;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31616;&#21333;&#24605;&#24819;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20195;&#30721;&#21487;&#22312;Github.com/amazon-research/fast-rl-with-slow-updates &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms often use two networks for value function optimization: an online network, and a target network that tracks the online network with some delay. Using two separate networks enables the agent to hedge against issues that arise when performing bootstrapping. In this paper we endow two popular deep reinforcement learning algorithms, namely DQN and Rainbow, with updates that incentivize the online network to remain in the proximity of the target network. This improves the robustness of deep reinforcement learning in presence of noisy updates. The resultant agents, called DQN Pro and Rainbow Pro, exhibit significant performance improvements over their original counterparts on the Atari benchmark demonstrating the effectiveness of this simple idea in deep reinforcement learning. The code for our paper is available here: Github.com/amazon-research/fast-rl-with-slow-updates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#26862;&#26519;&#39044;&#27979;&#20986;&#20855;&#26377;&#22823;&#30913;&#30697;&#30340;&#26448;&#26009;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#22823;&#30913;&#30697;&#26448;&#26009;&#65292;&#36825;&#21487;&#20197;&#20026;&#20174;&#29616;&#26377;&#26448;&#26009;&#35774;&#35745;&#39640;&#24615;&#33021;&#30913;&#38081;&#20197;&#21450;&#21457;&#29616;&#26032;&#22411;&#22823;&#30913;&#30697;&#21270;&#21512;&#29289;&#25552;&#20379;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.14712</link><description>&lt;p&gt;
&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#26862;&#26519;&#39044;&#27979;&#20855;&#26377;&#22823;&#30913;&#30697;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
Prediction of Large Magnetic Moment Materials With Graph Neural Networks and Random Forests. (arXiv:2111.14712v4 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.14712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#26862;&#26519;&#39044;&#27979;&#20986;&#20855;&#26377;&#22823;&#30913;&#30697;&#30340;&#26448;&#26009;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#22823;&#30913;&#30697;&#26448;&#26009;&#65292;&#36825;&#21487;&#20197;&#20026;&#20174;&#29616;&#26377;&#26448;&#26009;&#35774;&#35745;&#39640;&#24615;&#33021;&#30913;&#38081;&#20197;&#21450;&#21457;&#29616;&#26032;&#22411;&#22823;&#30913;&#30697;&#21270;&#21512;&#29289;&#25552;&#20379;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#24615;&#26448;&#26009;&#26159;&#35768;&#22810;&#21487;&#25512;&#21160;&#29983;&#24577;&#36807;&#28193;&#30340;&#25216;&#26415;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#30005;&#21160;&#26426;&#65292;&#39118;&#21147;&#21457;&#30005;&#26426;&#21644;&#30913;&#24615;&#21046;&#20919;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#21457;&#29616;&#20855;&#26377;&#22823;&#30913;&#30697;&#30340;&#26448;&#26009;&#25104;&#20026;&#20102;&#26085;&#30410;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25195;&#25551;&#25968;&#21313;&#19975;&#20010;&#29616;&#26377;&#26448;&#26009;&#30340;&#26080;&#26426;&#26230;&#20307;&#32467;&#26500;&#25968;&#25454;&#24211;&#65288;ICSD&#65289;&#65292;&#20197;&#25214;&#21040;&#20855;&#26377;&#38081;&#30913;&#24615;&#21644;&#22823;&#30913;&#30697;&#30340;&#26448;&#26009;&#12290;&#37319;&#29992;&#26448;&#26009;&#35745;&#21010;&#25968;&#25454;&#24211;&#19978;&#30340;&#26230;&#20307;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CGCNN&#65289;&#65292;&#26448;&#26009;&#22270;&#32593;&#32476;&#65288;MEGNet&#65289;&#21644;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#24211;&#21253;&#21547;&#39640;&#36890;&#37327;DFT&#39044;&#27979;&#32467;&#26524;&#12290;&#23545;&#20110;&#38543;&#26426;&#26862;&#26519;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#21270;&#23398;&#25104;&#20998;&#21644;&#26230;&#20307;&#32467;&#26500;&#30340;&#36817;100&#20010;&#25551;&#36848;&#31526;&#26469;&#36873;&#25321;&#30456;&#20851;&#23646;&#24615;&#12290;&#36825;&#32473;&#20986;&#30340;&#32467;&#26524;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;&#36825;&#20123;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#23427;&#20204;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#20855;&#26377;&#22823;&#30913;&#30697;&#30340;&#26448;&#26009;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#20351;&#29992;&#29616;&#26377;&#26448;&#26009;&#35774;&#35745;&#39640;&#24615;&#33021;&#30913;&#38081;&#20197;&#21450;&#21457;&#29616;&#20855;&#26377;&#22823;&#30913;&#30697;&#30340;&#26032;&#21270;&#21512;&#29289;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Magnetic materials are crucial components of many technologies that could drive the ecological transition, including electric motors, wind turbine generators and magnetic refrigeration systems. Discovering materials with large magnetic moments is therefore an increasing priority. Here, using state-of-the-art machine learning methods, we scan the Inorganic Crystal Structure Database (ICSD) of hundreds of thousands of existing materials to find those that are ferromagnetic and have large magnetic moments. Crystal graph convolutional neural networks (CGCNN), materials graph network (MEGNet) and random forests are trained on the Materials Project database that contains the results of high-throughput DFT predictions. For random forests, we use a stochastic method to select nearly one hundred relevant descriptors based on chemical composition and crystal structure. This gives results that are comparable to those of neural networks. The comparison between these different machine learning appr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#20302;&#36136;&#37327;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20855;&#22791;&#31070;&#32463;&#35843;&#21046;&#29305;&#24449;&#65292;&#36890;&#36807;&#19968;&#20010;&#39069;&#22806;&#30340;&#36755;&#20837;&#30340;&#20989;&#25968;&#26367;&#25442;&#20102;&#20840;&#36830;&#25509;&#23618;&#30340;&#22266;&#23450;&#26435;&#37325;&#65292;&#20351;&#24471;&#22312;&#27979;&#35797;&#20013;&#20855;&#26377;&#35843;&#21046;&#23618;&#30340;&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#38477;&#35299;&#26356;&#21152;&#40065;&#26834;&#65292;&#21516;&#26102;&#20063;&#33021;&#22815;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#25554;&#34917;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2107.08574</link><description>&lt;p&gt;
&#19968;&#31181;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#30340;&#40065;&#26834;&#24615;&#30340;&#35843;&#21046;&#23618;
&lt;/p&gt;
&lt;p&gt;
A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues. (arXiv:2107.08574v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.08574
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#20302;&#36136;&#37327;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20855;&#22791;&#31070;&#32463;&#35843;&#21046;&#29305;&#24449;&#65292;&#36890;&#36807;&#19968;&#20010;&#39069;&#22806;&#30340;&#36755;&#20837;&#30340;&#20989;&#25968;&#26367;&#25442;&#20102;&#20840;&#36830;&#25509;&#23618;&#30340;&#22266;&#23450;&#26435;&#37325;&#65292;&#20351;&#24471;&#22312;&#27979;&#35797;&#20013;&#20855;&#26377;&#35843;&#21046;&#23618;&#30340;&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#38477;&#35299;&#26356;&#21152;&#40065;&#26834;&#65292;&#21516;&#26102;&#20063;&#33021;&#22815;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#25554;&#34917;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#32570;&#22833;&#21644;&#36136;&#37327;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12290;&#24320;&#21457;&#32773;&#36890;&#24120;&#21482;&#20351;&#29992;&#39640;&#36136;&#37327;&#25968;&#25454;&#31934;&#24515;&#31579;&#36873;&#20986;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65307;&#28982;&#32780;&#65292;&#36825;&#20250;&#38477;&#20302;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25928;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#27491;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#20302;&#36136;&#37327;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#21033;&#29992;&#19968;&#20010;&#39069;&#22806;&#30340;&#36755;&#20837;&#30340;&#20989;&#25968;&#26367;&#25442;&#20102;&#20840;&#36830;&#25509;&#23618;&#30340;&#22266;&#23450;&#26435;&#37325;&#12290;&#36825;&#21463;&#21551;&#21457;&#20110;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#35843;&#21046;&#65292;&#30382;&#36136;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#30340;&#21487;&#38752;&#24615;&#21644;&#20854;&#20182;&#25968;&#25454;&#30340;&#23384;&#22312;&#31243;&#24230;&#19978;&#19979;&#35843;&#33410;&#36755;&#20837;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;&#21487;&#38752;&#24615;&#24471;&#20998;&#20316;&#20026;&#35843;&#21046;&#20449;&#21495;&#65292;&#21457;&#29616;&#20855;&#26377;&#35843;&#21046;&#23618;&#30340;&#27169;&#22411;&#23545;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#38477;&#35299;&#65288;&#21253;&#25324;&#39069;&#22806;&#30340;&#32570;&#22833;&#25968;&#25454;&#65289;&#26356;&#21152;&#40065;&#26834;&#12290;&#36825;&#20123;&#27169;&#22411;&#20248;&#20110;&#25554;&#34917;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#23436;&#20840;&#36339;&#36807;&#25554;&#34917;&#36807;&#31243;&#33410;&#30465;&#20102;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#25554;&#34917;&#38169;&#35823;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data missingness and quality are common problems in machine learning, especially for high-stakes applications such as healthcare. Developers often train machine learning models on carefully curated datasets using only high quality data; however, this reduces the utility of such models in production environments. We propose a novel neural network modification to mitigate the impacts of low quality and missing data which involves replacing the fixed weights of a fully-connected layer with a function of an additional input. This is inspired from neuromodulation in biological neural networks where the cortex can up- and down-regulate inputs based on their reliability and the presence of other data. In testing, with reliability scores as a modulating signal, models with modulating layers were found to be more robust against degradation of data quality, including additional missingness. These models are superior to imputation as they save on training time by completely skipping the imputatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#32467;&#21512;&#32447;&#24615;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#34394;&#25311;&#20256;&#24863;&#12290;&#36890;&#36807;&#23545;&#30130;&#21171;&#35797;&#39564;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2107.03645</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#30340;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#65306;&#30130;&#21171;&#35797;&#39564;&#26550;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessment of hybrid machine learning models for non-linear system identification of fatigue test rigs. (arXiv:2107.03645v3 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.03645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#32467;&#21512;&#32447;&#24615;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#34394;&#25311;&#20256;&#24863;&#12290;&#36890;&#36807;&#23545;&#30130;&#21171;&#35797;&#39564;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32473;&#23450;&#30130;&#21171;&#35797;&#39564;&#21488;&#39537;&#21160;&#20449;&#21495;&#30340;&#31995;&#32479;&#21709;&#24212;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#37319;&#29992;&#32447;&#24615;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#27169;&#22411;&#12290;&#20026;&#20102;&#32771;&#34385;&#38750;&#32447;&#24615;&#29616;&#35937;&#65292;&#24314;&#35758;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#22686;&#21152;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#34917;&#20805;&#12290;&#35813;&#26041;&#27861;&#30340;&#38468;&#21152;&#34394;&#25311;&#20256;&#24863;&#24212;&#29992;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#20351;&#29992;&#26469;&#33258;&#20282;&#26381;&#28082;&#21387;&#35797;&#39564;&#21488;&#30340;&#38750;&#32447;&#24615;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#20844;&#24320;&#20102;&#35813;&#25968;&#25454;&#38598;&#12290;&#22312;&#35780;&#20272;&#20013;&#37319;&#29992;&#20102;&#21508;&#31181;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#25351;&#26631;&#20197;&#21450;&#21464;&#24133;&#19979;&#30340;&#30130;&#21171;&#24378;&#24230;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of system responses for a given fatigue test bench drive signal is a challenging task, for which linear frequency response function models are commonly used. To account for non-linear phenomena, a novel hybrid model is suggested, which augments existing approaches using Long Short-Term Memory networks. Additional virtual sensing applications of this method are demonstrated. The approach is tested using non-linear experimental data from a servo-hydraulic test rig and this dataset is made publicly available. A variety of metrics in time and frequency domains, as well as fatigue strength under variable amplitudes, are employed in the evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23376;&#37319;&#26679;&#26694;&#26550;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#28857;&#30340;&#20449;&#24687;&#22686;&#30410;&#37327;&#26469;&#25351;&#23548;&#25506;&#32034;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#26356;&#26032;RL&#31639;&#27861;&#30340;&#31574;&#30053;&#27425;&#25968;&#22823;&#22823;&#20943;&#23569;&#65292;&#20294;&#20173;&#20445;&#25345;&#36739;&#23567;&#30340;&#36817;&#20284;&#26368;&#20248;&#36951;&#25022;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2106.07203</link><description>&lt;p&gt;
&#22522;&#20110;&#26222;&#36866;&#20989;&#25968;&#36924;&#36817;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#23376;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Online Sub-Sampling for Reinforcement Learning with General Function Approximation. (arXiv:2106.07203v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#23376;&#37319;&#26679;&#26694;&#26550;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#28857;&#30340;&#20449;&#24687;&#22686;&#30410;&#37327;&#26469;&#25351;&#23548;&#25506;&#32034;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#26356;&#26032;RL&#31639;&#27861;&#30340;&#31574;&#30053;&#27425;&#25968;&#22823;&#22823;&#20943;&#23569;&#65292;&#20294;&#20173;&#20445;&#25345;&#36739;&#23567;&#30340;&#36817;&#20284;&#26368;&#20248;&#36951;&#25022;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26222;&#36866;&#20989;&#25968;&#36924;&#36817;&#65288;FA&#65289;&#26041;&#27861;&#37117;&#19987;&#27880;&#20110;&#29702;&#35299;&#32479;&#35745;&#22797;&#26434;&#24615;&#25110;&#36951;&#25022;&#36793;&#30028;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#36828;&#26410;&#24471;&#21040;&#29702;&#35299;&#8212;&#8212;&#20107;&#23454;&#19978;&#65292;&#20989;&#25968;&#31867;&#19978;&#30340;&#31616;&#21333;&#20248;&#21270;&#38382;&#39064;&#21487;&#33021;&#21516;&#26679;&#38590;&#20197;&#22788;&#29702;&#12290;&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#23376;&#37319;&#26679;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#27979;&#37327;RL&#31639;&#27861;&#25910;&#38598;&#30340;&#25968;&#25454;&#28857;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#24182;&#20351;&#29992;&#35813;&#27979;&#37327;&#25351;&#23548;&#25506;&#32034;&#12290;&#23545;&#20110;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#21644;&#22797;&#26434;&#24230;&#26377;&#30028;&#30340;&#20989;&#25968;&#31867;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31574;&#30053;&#21482;&#38656;&#35201;&#26356;&#26032;$\propto\operatorname{poly}\log(K)$ &#27425;&#65292;&#23601;&#21487;&#20197;&#36816;&#34892; $K$ &#27425;RL&#31639;&#27861;&#32780;&#20173;&#28982;&#23454;&#29616;&#36739;&#23567;&#30340;&#36817;&#20284;&#26368;&#20248;&#36951;&#25022;&#36793;&#30028;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#26356;&#26032;&#31574;&#30053;&#33267;&#23569;&#35201; $\Omega(K)$ &#27425;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#20943;&#23569;&#20102;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20248;&#21270;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the existing works for reinforcement learning (RL) with general function approximation (FA) focus on understanding the statistical complexity or regret bounds. However, the computation complexity of such approaches is far from being understood -- indeed, a simple optimization problem over the function class might be as well intractable. In this paper, we tackle this problem by establishing an efficient online sub-sampling framework that measures the information gain of data points collected by an RL algorithm and uses the measurement to guide exploration. For a value-based method with complexity-bounded function class, we show that the policy only needs to be updated for $\propto\operatorname{poly}\log(K)$ times for running the RL algorithm for $K$ episodes while still achieving a small near-optimal regret bound. In contrast to existing approaches that update the policy for at least $\Omega(K)$ times, our approach drastically reduces the number of optimization calls in solving 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#30340;&#29702;&#35770;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21482;&#38656;&#22312;RKHS&#19978;&#23450;&#20041;&#31264;&#23494;Koopman&#31639;&#23376;&#30340;DMD&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#39640;&#26031;&#24452;&#21521;&#22522;&#26680;&#20989;&#25968;&#30340;RKHS&#21482;&#25903;&#25345;&#20223;&#23556;&#21160;&#21147;&#23398;&#30340;&#26377;&#30028;Koopman&#31639;&#23376;&#12290;</title><link>http://arxiv.org/abs/2106.00106</link><description>&lt;p&gt;
&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#30340;&#20869;&#26680;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
The kernel perspective on dynamic mode decomposition. (arXiv:2106.00106v3 [math.FA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.00106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#30340;&#29702;&#35770;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21482;&#38656;&#22312;RKHS&#19978;&#23450;&#20041;&#31264;&#23494;Koopman&#31639;&#23376;&#30340;DMD&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#39640;&#26031;&#24452;&#21521;&#22522;&#26680;&#20989;&#25968;&#30340;RKHS&#21482;&#25903;&#25345;&#20223;&#23556;&#21160;&#21147;&#23398;&#30340;&#26377;&#30028;Koopman&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20851;&#20110;Koopman&#31639;&#23376;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;DMD&#65289;&#30340;&#29702;&#35770;&#20551;&#35774;&#65292;&#21253;&#25324;&#29305;&#24449;&#20989;&#25968;&#30340;&#26684;&#23376;&#23384;&#22312;&#24615;&#12289;Koopman&#31639;&#23376;&#20043;&#38388;&#30340;&#20849;&#21516;&#29305;&#24449;&#20989;&#25968;&#12289;Koopman&#31639;&#23376;&#30340;&#26377;&#30028;&#24615;&#21644;&#32039;&#24615;&#12290;&#38024;&#23545;&#27599;&#20010;&#20551;&#35774;&#25552;&#20379;&#20102;&#35828;&#26126;&#20854;&#38480;&#21046;&#24615;&#30340;&#21453;&#20363;&#12290;&#29305;&#21035;&#22320;&#65292;&#26412;&#25991;&#35777;&#26126;&#20102;&#39640;&#26031;&#24452;&#21521;&#22522;&#26680;&#20989;&#25968;&#30340;&#26412;&#22320;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#21482;&#25903;&#25345;&#20223;&#23556;&#21160;&#21147;&#23398;&#30340;&#26377;&#30028;Koopman&#31639;&#23376;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;DMD&#26694;&#26550;&#65292;&#21482;&#38656;&#22312;RKHS&#19978;&#23450;&#20041;&#31264;&#23494;Koopman&#31639;&#23376;&#21363;&#21487;&#65292;&#36890;&#36807;&#25968;&#20540;&#23454;&#20363;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This manuscript revisits theoretical assumptions concerning dynamic mode decomposition (DMD) of Koopman operators, including the existence of lattices of eigenfunctions, common eigenfunctions between Koopman operators, and boundedness and compactness of Koopman operators. Counterexamples that illustrate restrictiveness of the assumptions are provided for each of the assumptions. In particular, this manuscript proves that the native reproducing kernel Hilbert space (RKHS) of the Gaussian RBF kernel function only supports bounded Koopman operators if the dynamics are affine. In addition, a new framework for DMD, that requires only densely defined Koopman operators over RKHSs is introduced, and its effectiveness is demonstrated through numerical examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#20540;&#22330;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;(MF-AIRL)&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#23637;&#31034;&#34892;&#20026;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#27169;&#25311;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#22870;&#21169;&#24674;&#22797;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2104.14654</link><description>&lt;p&gt;
&#23545;&#20110;&#22343;&#20540;&#22330;&#21338;&#24328;&#30340;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adversarial Inverse Reinforcement Learning for Mean Field Games. (arXiv:2104.14654v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.14654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#20540;&#22330;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;(MF-AIRL)&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#23637;&#31034;&#34892;&#20026;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#27169;&#25311;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#22870;&#21169;&#24674;&#22797;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22343;&#20540;&#22330;&#21338;&#24328;(MFGs)&#26159;&#21033;&#29992;&#22343;&#20540;&#22330;&#29702;&#35770;&#31616;&#21270;&#26234;&#33021;&#20307;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#19968;&#31181;&#21487;&#25968;&#23398;&#22788;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#36890;&#36807;&#20174;&#23637;&#31034;&#34892;&#20026;&#20013;&#24674;&#22797;&#22870;&#21169;&#20449;&#21495;&#65292;&#23427;&#20351;&#24471;&#24212;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;(IRL)&#26469;&#39044;&#27979;&#22823;&#32676;&#20307;&#30340;&#34892;&#20026;&#21464;&#24471;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MFGs&#20013;&#30340;IRL&#26041;&#27861;&#26080;&#27861;&#25512;&#26029;&#20986;&#21508;&#20010;&#26234;&#33021;&#20307;&#23637;&#31034;&#34892;&#20026;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22343;&#20540;&#22330;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;(MF-AIRL)&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#23637;&#31034;&#34892;&#20026;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#26500;&#24314;MF-AIRL&#65292;&#22522;&#20110;&#26368;&#22823;&#29109;IRL&#21644;&#19968;&#20010;&#26032;&#30340;&#22343;&#34913;&#27010;&#24565;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#19981;&#23436;&#32654;&#28436;&#31034;&#30340;&#27169;&#25311;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MF-AIRL&#22312;&#22870;&#21169;&#24674;&#22797;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mean field games (MFGs) provide a mathematically tractable framework for modelling large-scale multi-agent systems by leveraging mean field theory to simplify interactions among agents. It enables applying inverse reinforcement learning (IRL) to predict behaviours of large populations by recovering reward signals from demonstrated behaviours. However, existing IRL methods for MFGs are powerless to reason about uncertainties in demonstrated behaviours of individual agents. This paper proposes a novel framework, Mean-Field Adversarial IRL (MF-AIRL), which is capable of tackling uncertainties in demonstrations. We build MF-AIRL upon maximum entropy IRL and a new equilibrium concept. We evaluate our approach on simulated tasks with imperfect demonstrations. Experimental results demonstrate the superiority of MF-AIRL over existing methods in reward recovery.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;HINT&#65292;&#26088;&#22312;&#26816;&#39564;&#26426;&#22120;&#23398;&#20064;&#36890;&#29992;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35821;&#27861;&#21644;&#35821;&#20041;&#19977;&#20010;&#23618;&#27425;&#12290;&#20026;&#20102;&#26816;&#39564;&#27169;&#22411;&#30340;&#25554;&#20540;&#21644;&#22806;&#25512;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20116;&#20493;&#20132;&#21449;&#27979;&#35797;&#38598;&#12290;&#36890;&#36807;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#36827;&#19968;&#27493;&#25506;&#31350;&#20854;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2103.01403</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31995;&#32479;&#21270;&#24863;&#30693;&#12289;&#35821;&#27861;&#21644;&#35821;&#20041;&#30340;&#36890;&#29992;&#25968;&#25454;&#38598;&#30340;&#26497;&#31616;&#20027;&#20041;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics. (arXiv:2103.01403v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.01403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;HINT&#65292;&#26088;&#22312;&#26816;&#39564;&#26426;&#22120;&#23398;&#20064;&#36890;&#29992;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35821;&#27861;&#21644;&#35821;&#20041;&#19977;&#20010;&#23618;&#27425;&#12290;&#20026;&#20102;&#26816;&#39564;&#27169;&#22411;&#30340;&#25554;&#20540;&#21644;&#22806;&#25512;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20116;&#20493;&#20132;&#21449;&#27979;&#35797;&#38598;&#12290;&#36890;&#36807;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#36827;&#19968;&#27493;&#25506;&#31350;&#20854;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#25484;&#25569;&#31639;&#26415;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26032;&#38382;&#39064;&#30340;&#20363;&#22806;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;Handwritten arithmetic with INTegers&#65288;HINT&#65289;&#26469;&#26816;&#39564;&#26426;&#22120;&#23398;&#20064;&#36890;&#29992;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35821;&#27861;&#21644;&#35821;&#20041;&#19977;&#20010;&#23618;&#27425;&#12290;&#22312;HINT&#20013;&#65292;&#26426;&#22120;&#34987;&#36171;&#20104;&#23398;&#20064;&#22914;&#20309;&#20174;&#21407;&#22987;&#20449;&#21495;&#65288;&#22914;&#22270;&#20687;&#65289;&#20013;&#24863;&#30693;&#27010;&#24565;&#65288;&#21363;&#24863;&#30693;&#65289;&#65292;&#22914;&#20309;&#23558;&#22810;&#20010;&#27010;&#24565;&#32467;&#26500;&#21270;&#32452;&#21512;&#20197;&#24418;&#25104;&#26377;&#25928;&#34920;&#36798;&#24335;&#65288;&#21363;&#35821;&#27861;&#65289;&#65292;&#20197;&#21450;&#22914;&#20309;&#23454;&#29616;&#27010;&#24565;&#20197;&#25903;&#25345;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#65288;&#21363;&#35821;&#20041;&#65289;&#65292;&#20840;&#37096;&#22312;&#24369;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#31995;&#32479;&#21270;&#36890;&#29992;&#33021;&#21147;&#65292;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#20116;&#20493;&#20132;&#21449;&#27979;&#35797;&#38598;&#65292;&#20197;&#35780;&#20272;&#20851;&#20110;&#19977;&#23618;&#32423;&#21035;&#30340;&#23398;&#20064;&#27010;&#24565;&#30340;&#25554;&#20540;&#21644;&#22806;&#25512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#23398;&#20064;&#20998;&#21106;&#65292;&#20197;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26032;&#27010;&#24565;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#20102;&#35299;&#29616;&#26377;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#36824;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;HINT&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by humans' exceptional ability to master arithmetic and generalize to new problems, we present a new dataset, Handwritten arithmetic with INTegers (HINT), to examine machines' capability of learning generalizable concepts at three levels: perception, syntax, and semantics. In HINT, machines are tasked with learning how concepts are perceived from raw signals such as images (i.e., perception), how multiple concepts are structurally combined to form a valid expression (i.e., syntax), and how concepts are realized to afford various reasoning tasks (i.e., semantics), all in a weakly supervised manner. Focusing on systematic generalization, we carefully design a five-fold test set to evaluate both the interpolation and the extrapolation of learned concepts w.r.t. the three levels. Further, we design a few-shot learning split to determine whether or not models can rapidly learn new concepts and generalize them to more complex scenarios. To comprehend existing models' limitations, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35889;&#29702;&#35770;&#35745;&#31639;Riemannian Mat&#233;rn&#39640;&#26031;&#36807;&#31243;&#22312;&#32039;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#26680;&#65292;&#20351;&#20854;&#21487;&#20197;&#36890;&#36807;&#26631;&#20934;&#30340;&#21487;&#25193;&#23637;&#25216;&#26415;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#23558;&#25512;&#21160;Mat&#233;rn&#39640;&#26031;&#36807;&#31243;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2006.10160</link><description>&lt;p&gt;
Matern&#39640;&#26031;&#36807;&#31243;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Mat\'ern Gaussian processes on Riemannian manifolds. (arXiv:2006.10160v6 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.10160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35889;&#29702;&#35770;&#35745;&#31639;Riemannian Mat&#233;rn&#39640;&#26031;&#36807;&#31243;&#22312;&#32039;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#26680;&#65292;&#20351;&#20854;&#21487;&#20197;&#36890;&#36807;&#26631;&#20934;&#30340;&#21487;&#25193;&#23637;&#25216;&#26415;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#23558;&#25512;&#21160;Mat&#233;rn&#39640;&#26031;&#36807;&#31243;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#27169;&#22411;&#31867;&#65292;&#29305;&#21035;&#26159;&#22312;&#31934;&#30830;&#34920;&#31034;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24456;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#12290;&#21463;&#29289;&#29702;&#31185;&#23398;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#26368;&#36817;&#23558;&#24191;&#27867;&#20351;&#29992;&#30340;Mat&#233;rn&#39640;&#26031;&#36807;&#31243;&#25512;&#24191;&#21040;&#27169;&#25311;&#23450;&#20041;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#20989;&#25968;&#65292;&#36890;&#36807;&#23558;&#36825;&#20123;&#36807;&#31243;&#37325;&#26032;&#34920;&#31034;&#20026;&#38543;&#26426;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#25289;&#26222;&#25289;&#26031;-&#36125;&#23572;&#29305;&#25289;&#31859;&#31639;&#23376;&#30340;&#35889;&#29702;&#35770;&#22312;&#32039;&#40654;&#26364;&#27969;&#24418;&#19978;&#35745;&#31639;&#36825;&#20123;&#36807;&#31243;&#30340;&#26680;&#24515;&#25216;&#26415;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#26631;&#20934;&#21487;&#25193;&#23637;&#25216;&#26415;&#65288;&#20363;&#22914;&#35825;&#23548;&#28857;&#26041;&#27861;&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#23558;&#35813;&#25512;&#24191;&#20174;Mat&#233;rn&#25512;&#24191;&#21040;&#24191;&#27867;&#20351;&#29992;&#30340;&#24179;&#26041;&#25351;&#25968;&#39640;&#26031;&#36807;&#31243;&#12290;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20247;&#25152;&#21608;&#30693;&#30340;&#25216;&#26415;&#23545;Riemannian Mat&#233;rn&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20351;&#23427;&#20204;&#33021;&#22815;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are an effective model class for learning unknown functions, particularly in settings where accurately representing predictive uncertainty is of key importance. Motivated by applications in the physical sciences, the widely-used Mat\'ern class of Gaussian processes has recently been generalized to model functions whose domains are Riemannian manifolds, by re-expressing said processes as solutions of stochastic partial differential equations. In this work, we propose techniques for computing the kernels of these processes on compact Riemannian manifolds via spectral theory of the Laplace-Beltrami operator in a fully constructive manner, thereby allowing them to be trained via standard scalable techniques such as inducing point methods. We also extend the generalization from the Mat\'ern to the widely-used squared exponential Gaussian process. By allowing Riemannian Mat\'ern Gaussian processes to be trained using well-understood techniques, our work enables their use i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#19968;&#31867;&#38750;&#20984;&#24378;&#20984;min-max&#38382;&#39064;&#30340;&#38543;&#26426;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36817;&#31471;&#38454;&#27573;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#20854;&#20013;&#23884;&#20837;&#20102;&#35768;&#22810;&#20247;&#25152;&#21608;&#30693;&#30340;&#38543;&#26426;&#26356;&#26032;&#65292;&#24555;&#36895;&#25910;&#25947;&#24615;&#24471;&#21040;&#20102;&#24314;&#31435;&#12290;</title><link>http://arxiv.org/abs/2006.06889</link><description>&lt;p&gt;
&#24555;&#36895;&#38754;&#21521;&#20855;&#26377;PL&#26465;&#20214;&#30340;&#38750;&#20984;&#24378;&#20984;min-max&#38382;&#39064;&#30340;&#30446;&#26631;&#21644;&#23545;&#20598;&#24046;&#25910;&#25947;(arXiv:2006.06889v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Fast Objective &amp; Duality Gap Convergence for Non-Convex Strongly-Concave Min-Max Problems with PL Condition. (arXiv:2006.06889v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.06889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#19968;&#31867;&#38750;&#20984;&#24378;&#20984;min-max&#38382;&#39064;&#30340;&#38543;&#26426;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36817;&#31471;&#38454;&#27573;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#20854;&#20013;&#23884;&#20837;&#20102;&#35768;&#22810;&#20247;&#25152;&#21608;&#30693;&#30340;&#38543;&#26426;&#26356;&#26032;&#65292;&#24555;&#36895;&#25910;&#25947;&#24615;&#24471;&#21040;&#20102;&#24314;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#20110;&#35299;&#20915;&#24179;&#28369;&#38750;&#20984;&#24378;&#20984;min-max&#38382;&#39064;&#30340;&#38543;&#26426;&#26041;&#27861;&#65292;&#35813;&#38382;&#39064;&#30001;&#20110;&#20854;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65288;&#22914;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65292;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65289;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#36739;&#24930;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#20998;&#26512;&#22260;&#32469;&#25910;&#25947;&#20110;&#25509;&#36817;&#31283;&#24577;&#28857;&#23637;&#24320;&#12290;&#25105;&#20204;&#32771;&#34385;&#21033;&#29992;Polyak-Lojasiewicz&#65288;PL&#65289;&#26465;&#20214;&#26469;&#35774;&#35745;&#26356;&#24555;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#26356;&#24378;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#34429;&#28982;PL&#26465;&#20214;&#24050;&#32463;&#34987;&#29992;&#20110;&#35774;&#35745;&#35768;&#22810;&#38543;&#26426;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#38750;&#20984;&#26497;&#23567;&#21270;&#26368;&#22823;&#21270;&#20248;&#21270;&#30340;&#24212;&#29992;&#20173;&#28982;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#20010;&#27867;&#21270;&#30340;&#22522;&#20110;&#36817;&#31471;&#38454;&#27573;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#20854;&#20013;&#23884;&#20837;&#20102;&#35768;&#22810;&#20247;&#25152;&#21608;&#30693;&#30340;&#38543;&#26426;&#26356;&#26032;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;&#21407;&#22987;&#30446;&#26631;&#38388;&#38553;&#21644;&#23545;&#20598;&#38388;&#38553;&#30340;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#30456;&#27604;&#65292;&#65288;i&#65289;&#25105;&#20204;&#30340;&#20998;&#26512;&#26159;...
&lt;/p&gt;
&lt;p&gt;
This paper focuses on stochastic methods for solving smooth non-convex strongly-concave min-max problems, which have received increasing attention due to their potential applications in deep learning (e.g., deep AUC maximization, distributionally robust optimization). However, most of the existing algorithms are slow in practice, and their analysis revolves around the convergence to a nearly stationary point.We consider leveraging the Polyak-Lojasiewicz (PL) condition to design faster stochastic algorithms with stronger convergence guarantee. Although PL condition has been utilized for designing many stochastic minimization algorithms, their applications for non-convex min-max optimization remain rare. In this paper, we propose and analyze a generic framework of proximal stage-based method with many well-known stochastic updates embeddable. Fast convergence is established in terms of both the primal objective gap and the duality gap. Compared with existing studies, (i) our analysis is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#22810;&#21464;&#37327;&#22823;&#25968;&#25454;&#20998;&#26512;&#65288;MBDA&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25512;&#23548;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#21644;&#20132;&#20114;&#24335;&#27169;&#22411;&#30340;&#20248;&#21183;&#20197;&#21450;&#24182;&#34892;&#22788;&#29702;&#30340;&#33021;&#21147;&#65292;&#24212;&#29992;&#20110;&#32593;&#32476;&#30417;&#27979;&#21644;&#35786;&#26029;&#65292;&#26368;&#32456;&#22312;UGR'16&#21644;Dartmouth'18&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/1907.02677</link><description>&lt;p&gt;
&#22810;&#21464;&#37327;&#22823;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#23398;&#20064;&#29992;&#20110;&#32593;&#32476;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpretable Learning in Multivariate Big Data Analysis for Network Monitoring. (arXiv:1907.02677v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1907.02677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#22810;&#21464;&#37327;&#22823;&#25968;&#25454;&#20998;&#26512;&#65288;MBDA&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25512;&#23548;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#21644;&#20132;&#20114;&#24335;&#27169;&#22411;&#30340;&#20248;&#21183;&#20197;&#21450;&#24182;&#34892;&#22788;&#29702;&#30340;&#33021;&#21147;&#65292;&#24212;&#29992;&#20110;&#32593;&#32476;&#30417;&#27979;&#21644;&#35786;&#26029;&#65292;&#26368;&#32456;&#22312;UGR'16&#21644;Dartmouth'18&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20197;&#35780;&#20272;&#36890;&#20449;&#32593;&#32476;&#24615;&#33021;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#65292;&#27604;&#22914;&#32593;&#32476;&#30417;&#27979;&#21644;&#25925;&#38556;&#25490;&#38500;&#65292;&#22914;&#26524;&#19981;&#33021;&#34987;&#20154;&#31867;&#25805;&#20316;&#21592;&#35299;&#37322;&#65292;&#25968;&#25454;&#27169;&#22411;&#23601;&#27809;&#22810;&#22823;&#29992;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#21464;&#37327;&#22823;&#25968;&#25454;&#20998;&#26512;&#65288;MBDA&#65289;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36825;&#26159;&#19968;&#31181;&#36817;&#26399;&#25552;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#12290;&#22312;&#36825;&#20010;&#25193;&#23637;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#25512;&#23548;&#29305;&#24449;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#26159;&#24403;&#25968;&#25454;&#37327;&#24222;&#22823;&#26102;&#24212;&#29992;MBDA&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#25152;&#24471;&#21040;&#30340;&#32593;&#32476;&#30417;&#27979;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#26816;&#27979;&#21644;&#35786;&#26029;&#19981;&#21516;&#30340;&#32593;&#32476;&#24322;&#24120;&#65292;&#37319;&#29992;&#19968;&#31181;&#23558;&#21487;&#35299;&#37322;&#24615;&#21644;&#20132;&#20114;&#24335;&#27169;&#22411;&#30340;&#20248;&#21183;&#19982;&#24182;&#34892;&#22788;&#29702;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20316;&#27969;&#12290;&#25105;&#20204;&#23558;&#25193;&#23637;&#30340;MBDA&#24212;&#29992;&#20110;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65306;UGR'16&#65292;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#20934;&#27969;&#37327;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;Dartmouth'18&#65292;&#26368;&#38271;&#21644;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an increasing interest in the development of new data-driven models useful to assess the performance of communication networks. For many applications, like network monitoring and troubleshooting, a data model is of little use if it cannot be interpreted by a human operator. In this paper, we present an extension of the Multivariate Big Data Analysis (MBDA) methodology, a recently proposed interpretable data analysis tool. In this extension, we propose a solution to the automatic derivation of features, a cornerstone step for the application of MBDA when the amount of data is massive. The resulting network monitoring approach allows us to detect and diagnose disparate network anomalies, with a data-analysis workflow that combines the advantages of interpretable and interactive models with the power of parallel processing. We apply the extended MBDA to two case studies: UGR'16, a benchmark flow-based real-traffic dataset for anomaly detection, and Dartmouth'18, the longest and l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#22909;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#20919;&#28448;&#20559;&#22909;&#39034;&#24207;&#30340;&#38382;&#39064;&#20197;&#21450;&#22810;&#26631;&#31614;&#25490;&#21517;&#38382;&#39064;&#12290;&#36890;&#36807;&#26032;&#30340;&#24179;&#28369;&#38454;&#26799;&#24418;&#28608;&#27963;&#20989;&#25968;&#65292;PNN&#22312;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#22312;&#20005;&#26684;&#26631;&#31614;&#25490;&#21517;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20116;&#31181;&#20808;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/1904.02345</link><description>&lt;p&gt;
&#20559;&#22909;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Preference Neural Network. (arXiv:1904.02345v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1904.02345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#22909;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#20919;&#28448;&#20559;&#22909;&#39034;&#24207;&#30340;&#38382;&#39064;&#20197;&#21450;&#22810;&#26631;&#31614;&#25490;&#21517;&#38382;&#39064;&#12290;&#36890;&#36807;&#26032;&#30340;&#24179;&#28369;&#38454;&#26799;&#24418;&#28608;&#27963;&#20989;&#25968;&#65292;PNN&#22312;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#22312;&#20005;&#26684;&#26631;&#31614;&#25490;&#21517;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20116;&#31181;&#20808;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20559;&#22909;&#31070;&#32463;&#32593;&#32476;&#65288;PNN&#65289;&#65292;&#37319;&#29992;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#35299;&#20915;&#20102;&#20919;&#28448;&#20559;&#22909;&#25490;&#24207;&#30340;&#38382;&#39064;&#12290;PNN&#36824;&#35299;&#20915;&#20102;&#22810;&#26631;&#31614;&#25490;&#21517;&#38382;&#39064;&#65292;&#20854;&#20013;&#26631;&#31614;&#21487;&#33021;&#20855;&#26377;&#20919;&#28448;&#20559;&#22909;&#39034;&#24207;&#25110;&#23376;&#32452;&#24179;&#31561;&#25490;&#21517;&#12290;PNN&#37319;&#29992;&#22810;&#23618;&#21069;&#39304;&#32467;&#26500;&#65292;&#20855;&#26377;&#23436;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#20803;&#12290;&#27599;&#20010;&#31070;&#32463;&#20803;&#37117;&#21253;&#21547;&#22522;&#20110;&#21916;&#22909;&#39034;&#24207;&#30340;&#26032;&#22411;&#24179;&#28369;&#38454;&#26799;&#24418;&#28608;&#27963;&#20989;&#25968;&#12290;PNN&#36755;&#20837;&#34920;&#31034;&#25968;&#25454;&#29305;&#24449;&#65292;&#36755;&#20986;&#31070;&#32463;&#20803;&#34920;&#31034;&#26631;&#31614;&#32034;&#24341;&#12290;&#35813;PNN&#20351;&#29992;&#20102;&#21253;&#21547;&#26410;&#32463;&#36807;&#23454;&#39564;&#30340;&#37325;&#22797;&#26631;&#31614;&#20540;&#30340;&#26032;&#20559;&#22909;&#25366;&#25496;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#39640;&#35745;&#31639;&#25928;&#29575;&#19979;&#65292;PNN&#22312;&#20005;&#26684;&#26631;&#31614;&#25490;&#21517;&#30340;&#20934;&#30830;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#25552;&#20986;&#30340;&#20116;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a preference neural network (PNN) to address the problem of indifference preferences orders with new activation function. PNN also solves the Multi-label ranking problem, where labels may have indifference preference orders or subgroups are equally ranked. PNN follows a multi-layer feedforward architecture with fully connected neurons. Each neuron contains a novel smooth stairstep activation function based on the number of preference orders. PNN inputs represent data features and output neurons represent label indexes. The proposed PNN is evaluated using new preference mining dataset that contains repeated label values which have not experimented before. PNN outperforms five previously proposed methods for strict label ranking in terms of accurate results with high computational efficiency.
&lt;/p&gt;</description></item></channel></rss>